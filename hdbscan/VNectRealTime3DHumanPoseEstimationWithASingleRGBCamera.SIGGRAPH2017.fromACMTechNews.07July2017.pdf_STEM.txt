









































vnect: real-tim 3D human pose estim with a singl rgb 
camera 

dushyant mehta1,2, srinath sridhar1, oleksandr sotnychenko1, helg rhodin1, mo- 
hammad shafiei1,2, hans-pet seidel1, weipeng xu1, dan casas3, christian theobalt1 
1max planck institut for informatics, 2saarland university, 3universidad rey juan carlo 

fig. 1. We recov the full global 3D skeleton pose in real-tim from a singl rgb camera, even wireless captur be possibl by stream from a smartphon 
(left). It enabl applic such a control a game character, embodi vr, sport motion analysi and reconstruct of commun video (right). 
commun video (cc by) courtesi of real madrid c.f. [2016] and rusfencing-tv [2017]. 

We present the first real-tim method to captur the full global 3D skelet 
pose of a human in a stable, tempor consist manner use a singl 
rgb camera. our method combin a new convolut neural network 
(cnn) base pose regressor with kinemat skeleton fitting. our novel fully- 
convolut pose formul regress 2D and 3D joint posit jointli 
in real time and do not requir tightli crop input frames. A real-tim 
kinemat skeleton fit method us the cnn output to yield tempor 
stabl 3D global pose reconstruct on the basi of a coher kinemat 
skeleton. thi make our approach the first monocular rgb method usabl 
in real-tim applic such a 3D charact control—thu far, the onli 
monocular method for such applic employ special rgb-d cam- 
eras. our method’ accuraci be quantit on par with the best offlin 
3D monocular rgb pose estim methods. our result be qualit 
compar to, and sometim good than, result from monocular rgb-d 
approaches, such a the kinect. however, we show that our approach be 
more broadli applic than rgb-d solutions, i.e., it work for outdoor 
scenes, commun videos, and low qualiti commod rgb cameras. 

cc concepts: • comput methodolog →motion capture; 

addit key word and phrases: bodi pose, monocular, real time 

thi work be be fund by the erc start grant project capreal (335545). dan 
casa be support by a mari curi individu fellow, grant agreement 707326. 

© 2017 copyright held by the owner/author(s). public right licens to acm. 
thi be the author’ version of the work. It be post here for your person use. not for 
redistribution. the definit version of record be publish in acm transact on 
graphics, https://doi.org/http://dx.doi.org/10.1145/3072959.3073596. 

acm refer format: 
dushyant mehta, srinath sridhar, oleksandr sotnychenko, helg rhodin, 
mohammad shafiei, hans-pet seidel,weipeng xu, dancasa and christian 
theobalt. 2017. vnect: real-tim 3D human pose estim with a singl 
rgb camera. acm trans. graph. 36, 4, articl 44 (juli 2017), 14 pages. 
doi: http://dx.doi.org/10.1145/3072959.3073596 

1 introduct 
optic skelet motion captur of human be wide use in applica- 
tion such a charact anim for movi and games, sport and 
biomechanics, and medicine. To overcom the usabl constraint 
impos by commerci system requir marker suit [menach 
2000], research develop marker-less motion captur meth- 
od that estim motion in more gener scene use multi-view 
video [moeslund et al. 2006], with recent solut be real-tim 
[stoll et al. 2011]. the swell in popular of applic such a real- 
time motion-driven 3D game charact control, self-immers in 
3D virtual and augment reality, and human–comput interaction, 
have lead to new real-tim full-bodi motion estim techniqu us- 
ing onli a single, easi to install, depth camera, such a the microsoft 
kinect [microsoft corpor 2010, 2013, 2015]. rgb-d camera 
provid valuabl depth data which greatli simplifi monocular 
pose reconstruction. however, rgb-d camera often fail in gen- 
eral outdoor scene (due to sunlight interference), be bulkier, have 
high power consumption, have low resolut and limit range, 
and be not a wide and cheapli avail a color cameras. 
skelet pose estim from a singl color camera be a much 

more challeng and sever underconstrain problem. monocu- 
lar rgb bodi pose estim in 2D have be wide researched, but 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



44:2 • D. mehta et. al. 

estim onli the 2D skelet pose [bourdev andmalik 2009; felzen- 
szwalb et al. 2010; felzenszwalb and huttenloch 2005; ferrari et al. 
2009; pishchulin et al. 2013; wei et al. 2016]. learning-bas discrim- 
in methods, in particular deep learn method [insafutdinov 
et al. 2016; lifshitz et al. 2016; newel et al. 2016; tompson et al. 2014], 
repres the current state of the art in 2D pose estimation, with 
some of these method demonstr real-tim perform [cao 
et al. 2016; wei et al. 2016]. monocular rgb estim of the 3D 
skelet pose be a much harder challeng tackl by rel few 
method [bogo et al. 2016; tekin et al. 2016b,c; zhou et al. 2015, 
2016, 2015b]. unfortunately, these method be typic offline, and 
they often reconstruct 3D joint posit individu per image, 
which be tempor unstable, and do not enforc constant bone 
lengths. most approach also captur local 3D pose rel to a 
bound box, and not the full global 3D pose. thi make them 
unsuit for applic such a real-tim 3D charact control. 
In thi paper, we present the first method that captur tempo- 

ralli consist global 3D human pose—in term of joint angl of a 
single, stabl kinemat skeleton—in real-tim (30 hz) from a singl 
rgb video in a gener environment. our approach build upon 
the top perform singl rgb 3D pose estim method use 
convolut neural network (cnns) [mehta et al. 2016; pavlako 
et al. 2016]. high accuraci requir train compar deep net- 
work which be harder to run in real-time, partli due to addit 
preprocess step such a bound box extraction. mehta et al. 
[2016] use a 100-layer architectur to predict 2D and 3D joint posi- 
tion simultaneously, but be unsuit for real-tim execution. To 
improv runtime, we use a shallow 50-layer network. however, 
for best qualiti at real-tim frame rates, we do not mere use a 
shallow variant, but extend it to a novel fully-convolut for- 
mulation. thi enabl high accuraci 2D and 3D pose regression, 
in particular of end effector (hands, feet), in real-time. In contrast 
to exist solut our approach allow oper on non-crop 
images, and where run-tim be a concern, it can be use to bootstrap 
a simpl bound box tracker. We also combin the cnn-base 
joint posit regress with an effici optim step to fit 
a 3D skeleton to these reconstruct in a tempor stabl way, 
yield the global pose and joint angl of the skeleton. In summary, 
we contribut by propos the first real-tim method to captur 
global 3D kinemat skeleton pose from singl rgb video. To strike a 
good compromis between comput complex and accuracy, 
our method combines: 

• A new real-time, fully-convolut 3D bodi pose formu- 
lation use cnn that yield 2D and 3D joint posit 
simultan and forgo the need to perform expens 
bound box computations. 

• model-bas kinemat skeleton fit against the 2d/3d 
pose predict to produc tempor stabl joint angl 
of a metric global 3D skeleton, in real time. 

our real-tim method achiev state-of-the-art accuraci compa- 
rabl to the best offlin rgb pose estim method on standard 
3D human bodi pose benchmarks, particularli for end effector posi- 
tion (section 5.2). our result be qualit compar to, and 
sometim good than, state-of-the-art singl rgb-d method [gir- 
shick et al. 2011], even commerci one [microsoft corpor 

2015]. We experiment show that thi make our the first single- 
rgb method usabl for similar real-tim 3D applications—so far 
onli feasibl with rgb-d input—such a game charact control or 
immers first person virtual realiti (vr). We further show that our 
method succeed in set where exist rgb-d method would 
not, such a outdoor scenes, commun videos, and even with low 
qualiti video stream from ubiquit mobil phone cameras. 

2 relat work 
our goal be stabl 3D skelet motion captur from (1) a singl 
camera (2) in real-time. We focu the discuss of relat work 
on approach from the larg bodi of marker-less motion captur 
research that contribut to attain either of these properties. 

multi-view: with multi-view setup markerless motion-captu solu- 
tion attain high accuracy. track of a manual initi actor 
model from frame to framewith a gener imag formationmodel 
be common. see [moeslund et al. 2006] for a complet overview. most 
method target high qualiti with offlin comput [bregler and 
malik 1998; how et al. 1999; loper and black 2014; sidenbladh 
et al. 2000; starck and hilton 2003]. real-tim perform can be 
attain by repres the actor with gaussian [rhodin et al. 
2015; stoll et al. 2011; wren et al. 1997] and other approxim 
[ma and Wu 2014], in addit to formul allow model-to- 
imag fitting. however, these tracking-bas approach often lose 
track in local minimum of the non-convex fit function they opti- 
mize and requir separ initialization, e.g. use [bogo et al. 2016; 
rhodin et al. 2016b; sminchisescu and trigg 2001]. robust 
could be increas with a combin of gener and discrimina- 
tive estim [elhayek et al. 2016], even from a singl input view 
[rosal and sclaroff 2006; sminchisescu et al. 2006], and egocentr 
perspect [rhodin et al. 2016a]. We util gener track 
compon to ensur tempor stability, but avoid model projec- 
tion through a full imag format model to speed up estimation. 
instead, we combin discrimin pose estim with kinemat 
fit to succeed in our underconstrain setting. 

monocular depth-based: the addit depth channel provid by 
rgb-d sensor have lead to robust real-tim pose estim solut 
[baak et al. 2011; ganapathi et al. 2012; Ma and Wu 2014; shotton 
et al. 2013; wei et al. 2012; Ye and yang 2014] and the availabil- 
iti of low-cost devic have enabl a rang of new applications. 
even real-tim track of gener deform object [zollhöfer 
et al. 2014] and template-fre reconstruct [dou et al. 2016; inn- 
mann et al. 2016; newcomb et al. 2015; orts-escolano et al. 2016] 
have be demonstrated. rgb-d inform overcom forward- 
backward ambigu in monocular pose estimation. our goal be a 
video solut that overcom depth ambigu without reli 
on a special activ sensor. 

monocular rgb: monocular gener motion captur have onli 
be show for short clip and when pair with strong motion 
prior [urtasun et al. 2006] or in combin with discrimin 
re-initi [rosal and sclaroff 2006; sminchisescu et al. 2006], 
sinc gener reconstruct be fundament underconstrained. 
use photo-realist templat model for model fit enabl more 
robust monocular track of simpl motions, but requir more 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



vnect: real-tim 3D human pose estim with a singl rgb camera • 44:3 

fig. 2. overview. given a full-siz imag It at frame t , the person-cent crop Bt be effici extract by bound box tracking, use the previou 
frame’ keypoint kt−1. from the crop, the cnn jointli predict 2D heatmap hj,t and our novel 3D location-map xj,t , yj,t and zj,t for all joint j . the 2D 
keypoint Kt be retriev from hj,t and, after filtering, be use to read off 3D pose plt from xj,t , yj,t and zj,t . these per-fram estim be combin to 
stabl global pose pgt by skeleton fitting. inform from frame t − 1 be mark in gray-dashed. 

expens offlin comput [de La gorc et al. 2008]. sampling- 
base method avoid local minimum [balan et al. 2005; Bo and smin- 
chisescu 2010; deutscher and reid 2005; gall et al. 2010]. however, 
real-tim variant can not guarante global converg due to a 
limit number of samples, such a particl swarm optim 
techniqu [oikonomidi et al. 2011]. structure-from-mot tech- 
niqu exploit motion cue in a batch of frame [garg et al. 2013], 
and have also be appli to human motion estim [gotardo 
and martinez 2011; lee et al. 2013; park and sheikh 2011; zhu et al. 
2011]. however, batch optim do not appli to our real-tim 
setting, where frame be stream sequentially. for some appli- 
cation manual annot and correct of frame be suitable, 
for instanc to enabl movi actor reshap [jain et al. 2010] and 
garment replac in video [rogg et al. 2014]. In combin 
with physic constraints, highli accur reconstruct be pos- 
sibl from monocular video [wei and chai 2010]. vondrak et al. 
[2012] succeed without manual annot by simul biped- 
controllers, but requir batch-optimization. while these method 
can yield high-qual reconstructions, interact and expens 
optim preclud live applications. 
discrimin 2D human pose estim be often an interme- 

diat step to monocular 3D pose estimation. pictori structur 
approach infer bodi part locat by messag pass over a 
huge set of pose-st [agarw and trigg 2006; andriluka et al. 
2009; bourdev and malik 2009; felzenszwalb and huttenloch 2005; 
ferrari et al. 2009; johnson and everingham 2010] and have be 
extend to 3D pose estim [amin et al. 2013; balan et al. 2007; 
belagianni et al. 2014; sigal et al. 2012]. recent approach outper- 
form these method in comput time and accuraci by leverag 
larg imag databas with 2D joint locat annotation, which en- 
abl high accuraci predict with deep cnn [belagianni and 
zisserman 2016; Hu et al. 2016; insafutdinov et al. 2016; pishchulin 
et al. 2016; wei et al. 2016], on multipl gpus, even at real-tim 
rate [cao et al. 2016]. given 2D joint locations, lift them to 3D 
pose be challenging. exist approach use bone length and depth 
order constraint [mori and malik 2006; taylor 2000], sparsiti as- 
sumption [wang et al. 2014; zhou et al. 2015,a], joint limit [akhter 
and black 2015], inter-penetr constraint [bogo et al. 2016], 
tempor depend [rhodin et al. 2016b], and regress [yasin 
et al. 2016]. treat 3D pose a a hidden variabl in 2D estim 

be an altern [brau and jiang 2016]. however, the spars set 
of 2D locat lose imag evidence, e.g. on forward-backward 
orient of limbs, which lead to erron estim in ambigu- 
ou cases. To overcom these ambiguities, discrimin method 
have be propos that learn implicit depth featur for 3D pose 
directli from more express imag representations. rosal and 
sclaroff regress 3D pose from silhouett imag with the special 
map architectur [2000], agarw and trigg with linear re- 
gression [2006], and elgamm and lee through a joint emb 
of imag and 3D pose [2004]. sminchisescu further util tem- 
poral consist to propag pose probabl with a bayesian 
mixtur of expert markov model [2007]. reli on the recent ad- 
vanc in machin learn techniqu and comput capabilities, 
approach for direct 3D pose regress from the input imag have 
be proposed, use structur learn of latent pose [li et al. 
2015a; tekin et al. 2016a], joint predict of 2D and 3D pose [li and 
chan 2014; tekin et al. 2016b; yasin et al. 2016], transfer of featur 
from 2D dataset [mehta et al. 2016], novel pose space formula- 
tion [pavlako et al. 2016] and classif over exampl pose 
[pons-mol et al. 2014; rogez and schmid 2016]. rel per-bon 
predict [li and chan 2014], kinemat skeleton model [zhou 
et al. 2016], or root center joint posit [ionescu et al. 2014a] be 
use a the eventu output space. such direct 3D pose regress 
method captur depth relat well, but 3D estim usual do 
not accur match the true 2D locat when re-project to the 
image, becaus estim be do in crop imag that lose 
camera perspect effects, use a canon height, and minim 
3D loss instead of project to 2d. furthermore, they onli deliv 
joint positions, be tempor unstable, and none have show real- 
time performance. We propos a method to combin 2D and 3D 
estim in real-tim along with tempor tracking. It be inspir 
by the method of tekin et al. [2016c], where batch of frame be 
process offlin after motion compensation, and be relat to the 
recent propos per-fram combin of 2D and 3D pose [tekin 
et al. 2016b]. 

notably, onli fewmethod target real-timemonocular reconstruc- 
tion. except be the regress of 3D pose from haar featur 
by bissacco et al. [2007] and detect of a set of discret pose 
from edg direct histogram in the vicin of the previou frame 
pose [taycher et al. 2006]. both onli obtain tempor unstable, 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



44:4 • D. mehta et. al. 

coars pose, not directli usabl in our applications. chai and hod- 
gin obtain suffici qualiti to drive virtual avatar in real-time, but 
requir visual marker [chai and hodgin 2005]. the use of cnn in 
real time have be explor for variant of the object detect prob- 
lem, for instanc bound box detect and pedestrian detect 
method have leverag applic specif architectur [angelova 
et al. 2015; liu et al. 2016; redmon et al. 2015] and preprocess 
step [ren et al. 2015]. 

In a similar vein, we propos a 3D pose estim approach that 
leverag a novel fully-convolut cnn formul to predict 2D 
and 3D pose jointly. In combin with inexpens preprocess 
and an optim base skelet fit method, it enabl high 
accuraci pose estimation, while run at more than 30 hz. 

3 overview 
our system be capabl of obtain a tempor consistent, full 
3D skelet pose of a human from a monocular rgb camera. esti- 
mate 3D pose from a singl rgb camera be a challenging, under- 
constrain problem with inher ambiguities. figur 2 provid 
an overview of our method to tackl thi challeng problem. It 
consist of two primari components. the first be a convolut 
neural network (cnn) to regress 2D and 3D joint posit under 
the ill-pos monocular captur conditions. It be train on anno- 
tat 3D human pose dataset [ionescu et al. 2014b; mehta et al. 
2016], addit leverag annot 2D human pose dataset 
[andriluka et al. 2014; johnson and everingham 2010] for improv 
in-the-wild performance. the second compon combin the re- 
gress joint posit with a kinemat skeleton fit method to 
produc a tempor stable, camera-relative, full 3D skelet pose. 

cnn pose regression: the core of our method be a cnn that predict 
both 2d, and root (pelvis) rel 3D joint posit in real-time. the 
new propos fully-convolut pose formul lead to result 
on par with the state-of-the-art offlin method in 3D joint posit 
accuraci (see section 5.2 for details). be fully-convolutional, it 
can oper in the absenc of tight crop around the subject. the 
cnn be capabl of predict joint posit for a divers class of 
activ regardless of the scene settings, provid a strong basi 
for further pose refin to produc tempor consist full-3d 
pose paramet 

kinemat skeleton fitting: the 2D and the 3D predict from the 
cnn, togeth with the tempor histori of the sequence, can be 
leverag to obtain tempor consist full-3d skelet pose, with 
the skelet root (pelvis) local in camera space. our approach 
us an optim function that: (1) combin the predict 2D 
and 3D joint posit to fit a kinemat skeleton in a least squar 
sense, (2) ensur tempor smooth track over time. We further 
improv the stabil of the track pose by appli filter step 
at differ stages. 

skeleton initi (optional): the system be set up with a default 
skeleton which work well out of the box for most humans. for more 
accur estimates, the rel bodi proport of the underli 
skeleton can be adapt to that of the subject, by averag cnn 
predict for a few frame at the beginning. sinc monocular 
reconstruct be ambigu without a scale reference, the cnn 

predict height normal 3D joint positions. user onli need to 
provid their height (distanc from head to toe) once, so that we 
can track the 3D pose in true metric space. 

4 real-tim monocular 3D pose estim 
In thi section, we describ in detail the differ compon of our 
method to estim a tempor consist 3D skelet motion from 
monocular rgb input sequences. As input, we assum a continu 
stream of monocular rgb imag {..., it−1, It }. for frame t in the 
input stream, the final output of our approach be pgt which be the 
full global 3D skelet pose of the person be tracked. becaus 
thi output be alreadi tempor consist and in global 3D space, 
it can be readili use in applic such a charact control. 

We use the follow notat for the output in the intermedi 
compon of our method. the cnn pose regressor jointli esti- 
mate the 2D joint posit Kt and root-rel 3D joint posit 
plt (section 4.1). the 3D skeleton fit compon combin the 
2D and 3D joint posit predict to estim a smooth, tempo- 
ralli consist pose pgt (θ , d), which be parameter by the global 
posit d in camera space, and joint angl θ of the kinemat skele- 
ton S . J indic the number of joints. We drop the frame-numb 
subscript t in certain section to aid readability. 

4.1 cnn pose regress 
the goal of cnn pose regress be to obtain joint positions, both, 
in 2D imag space and 3d. for 2D pose estim with neural nets, 
the chang of formul from direct regress of x ,y body-joint 
coordin [toshev and szegedi 2014] to a heatmap base body- 
joint detect formul [tompson et al. 2014] have be the key 
driver behind the recent develop in 2D pose estimation. the 
heatmap base formul natur tie imag evid to pose 
estim by predict a confid heatmap hj,t over the imag 
plane for each joint j ∈ {1..j }. 
exist approach to 3D pose estim lack such an image- 

to-predict association, often directli regress the root-rel 
joint locat [ionescu et al. 2014a], lead to predict pose 
whose extent of articul doesn’t reflect that of the person in the 
image. see figur 9. treat pose a a vector of joint locat also 
caus a natur gravit toward network with fully-connect 
formul [mehta et al. 2016; rogez and schmid 2016; tekin et al. 
2016a; Yu et al. 2016], restrict the input to tight crop at a fix 
resolution, a limit that need to be overcome. these method 
assum the avail of tight bound boxes, which necessit 
supplement with separ bound box estim for actual 
usage, which further add to the run-tim of these methods. the 
fully-convolut formul of pavlako et al. [pavlako et al. 
2016] seek to allevi some of these issues, but be limit by the 
expens per-joint volumetr formulation, which still reli on 
crop input and do not scale well to larg imag sizes. 

We overcom these limit through our new formulation, by 
extend the 2D heatmap formul to 3D use three addit 
location-map Xj ,yj ,zj per joint j, captur the root-rel lo- 
cation x j , yj and zj respectively. To have the 3D pose predict 
link more strongli to the 2D appear in the image, the x j , 
yj and zj valu be read off from their respect location-map 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



vnect: real-tim 3D human pose estim with a singl rgb camera • 44:5 

fig. 3. schema of the fully-convolut formul for predict root rel- 
ativ joint locations. for each joint j , the 3D coordin be predict from 
their respect location-map Xj , Yj , Zj at the posit of the maximum 
in the correspond 2D heatmap Hj . the structur observ here in the 
location-map emerg due to the spatial loss formulation. see section 4.1. 

at the posit of the maximum of the correspond joint’ 2D 
heatmap Hj , and store in PL = {x, y, z}, where x ∈ r1×j be a vec- 
tor that store the coordin x locat of each joint maximum. 
the pose formul be visual in figur 3. network use thi 
fully-convolut formul be not constrain in input imag 
size, and can work without tight crops. additionally, the network 
provid 2D and 3D joint locat estim without addit over- 
head, which we exploit in subsequ step for real-tim estimation. 
section 5.2 show the improv afford by thi formulation. 

loss term: To enforc the fact that we be onli interest in x j , 
yj and zj from their respect map at joint j’ 2D location, the 
joint location-map loss be weight strong around the joint’ 2D 
location. We use the L2 loss. for x j be the loss formul be 

loss(x j ) = ∥hgtj ⊙ (xj − X 
GT 
j )∥2, (1) 

where GT indic ground truth and ⊙ be the hadamard product. 
the locat map be weight with the respect ground truth 2D 
heatmap hgtj , which in turn have confid equal to a gaussian 
with a small support local at joint j’ 2D location. note that 
no structur be impos on the location-maps. the structur that 
emerg in the predict location-map be indic of the correla- 
tion of x j and yj with root rel locat of joint j in the imag 
plane. see figur 3. 

network details: We use the propos formul to adapt the 
resnet50 network architectur of He et al. [2016]. We replac the 
layer of resnet50 from res5a onward with the architectur de- 
pict in figur 5, produc the heatmap and location-map for all 
joint j ∈ {1..j }. after training, the batch normal [ioff and 
szegedi 2015] layer be merg with the weight of their preced 
convolut layer to improv the speed of the forward pass. 

intermedi supervision: We predict the 2D heatmap and 3D 
location-map from the featur at res4d and res5a, taper down 
the weight of intermedi loss with increas iter count. 
additionally, similar to the root-rel location-map Xj , Yj and 
Zj , we predict kinemat parent-rel location-map ∆xj , ∆yj 
and ∆zj from the featur at res5b and comput bone length-map 

fig. 4. repres train frame fromhuman3.6m andmpi-inf-3dhp 
3D pose datasets. also show be the background, cloth and occlud 
augment do on mpi-inf-3dhp train data. 

as: 
blj = 

√ 
∆xj ⊙ ∆xj + ∆yj ⊙ ∆yj + ∆zj ⊙ ∆zj . (2) 

these intermedi predict be subsequ concaten with 
the intermedi features, to give the network an explicit notion of 
bone length to guid the predictions. see figur 5. 

experi show that the deeper variant of resnet offer onli 
small gain for a substanti increas (1.5×) in comput time, 
prompt u to choos resnet50 to enabl real-time, yet highli 
accur joint locat estim with the propos formulation. 

training: the network be pretrain for 2D pose estim on 
mpii [andriluka et al. 2014] and lsp [johnson and everingham 
2010, 2011] to allow superior in-the-wild performance, a propos 
by mehta et al. [mehta et al. 2016]. for 3D pose, we use mpi-inf- 
3dhp [mehta et al. 2016] and human3.6m [ionescu et al. 2014b]. 
We take train sequenc for all subject except S9 and s11 from 
human3.6m.w sampl frame a per [ionescu et al. 2014a]. formpi- 
inf-3dhp, we consid all 8 train subjects. We choos sequenc 
from all 5 chest-high cameras, 2 head-high camera (angl down) 
and 1 knee-high camera (angl up) to learn some degre of invari- 
anc to the camera viewpoint. the sampl frame have at least 
one joint move by > 200mm between them. We use variou combi- 
nation of background, occlud (chair), upper-bodi cloth and 
lower-bodi cloth augment for 70% of the select frames. 
We train with person center crops, and use imag scale augmen- 
tation at 2 scale (0.7×, 1.0×), result in 75k train sampl for 
human3.6m and 100k train sampl for mpi-inf-3dhp. figur 
4 show a few repres frame of train data. In addit 
to the 17 joint typic considered, we use foot tip positions. the 
ground truth joint posit be with respect to a height normal 
skeleton (knee–neck height 92 cm). We make use of the caff [2014] 
framework for training, and use the adadelta [zeiler 2012] solver 
with learn rate taper down with increas iterations. 

bound box tracker : exist offlin solut process each frame 
in a separ person-loc and bound box (bb) crop 
step [mehta et al. 2016; tekin et al. 2016c] or assum bound 
box be avail [li and chan 2014; Li et al. 2015b; pavlako 
et al. 2016; tekin et al. 2016a; zhou et al. 2016]. although our fully- 
convolut formul allow the cnn to work without requir- 
ing cropping, the run-tim of the cnn be highli depend on the 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



44:6 • D. mehta et. al. 

fig. 5. network structure. the structur abov be preced by resnet50/100 till level 4. We use kinemat parent rel 3D joint locat predict ∆x, 
∆y, ∆Z a well a bone length map BL construct from these a auxiliari tasks.th network predict 2D locat heatmap H and root rel 3D joint 
locat X, Y, Z. refer to section 4.1. 

input imag size. additionally, the cnn be train for subject size 
in the rang of 250–340 px in the frame, requir averag of 
predict at multipl imag scale per frame (scale space search) if 
process the full frame at each time step. guarante real-tim 
rate necessit restrict the size of the input to the network 
and track the scale of the person in the imag to avoid search 
the scale space in each frame. We do thi in an integr way. the 
2D pose predict from the cnn at each frame be use to deter- 
mine the BB for the next frame through a slightli larg box around 
the predictions. the small rectangl contain the keypoint 
K be comput and augment with a buffer area 0.2× the height 
vertic and 0.4× the width horizontally. To stabil the estimates, 
the BB be shift horizont to the centroid of the 2D predictions, 
and it corner be filter with a weight averag with the pre- 
viou frame’ BB use a momentum of 0.75. To normal scale, 
the BB crop be resiz to 368x368 px. the BB tracker start with 
(slow) multi-scal predict on the full imag for the first few 
frames, and hone in on the person in the imag make use of the 
bb-agnost predict from the fulli convolut network. the 
BB track be easi to implement and without runtim overhead, 
sinc the propos fully-convolut network output 2D and 3D 
pose jointli and oper on arbitrari input sizes. 

4.2 kinemat skeleton fit 
appli per-fram pose estim techniqu on a video do 
not exploit and ensur tempor consist of motion, and small 
pose inaccuraci lead to tempor jitter, an unaccept artifact 
for most graphic applications. We combin the 2D and 3D joint 
posit in a joint optim framework, along with tempor 
filter and smoothing, to obtain an accurate, tempor stabl 
and robust result. first, the 2D predict Kt be tempor filter 
[casiez et al. 2012] and use to obtain the 3D coordin of each 
joint from the location-map predictions, give u plt . To ensur 
skelet stability, the bone length inher to plt be replac by 
the bone length of the underli skeleton in a simpl retarget 
step that preserv the bone direct of plt . the result 2D and 

3D predict be combin by minim the object energi 

etotal(θ , d) = eik(θ , d) + eproj(θ , d) 
+ esmooth(θ , d) + edepth(θ , d), (3) 

for skelet joint angl θ and the root joint’ locat in camera 
space d. the 3D invers kinemat term eik determin the overal 
pose by similar to the 3D cnn output plt . the project term 
eproj determin global posit d and correct the 3D pose by 
re-project onto the detect 2D keypoit kt. both term be 
implement with the L2 loss, 

eproj = ∥π(pgt ) − Kt ∥2 and eik = ∥(pgt − d) − plt ∥2, (4) 

where Π be the project function from 3D to the imag plane, 
and pgt = P 

G 
t (θ , d). We assum the pinhol project model. If the 

camera calibr be unknown a vertic field of view of 54 degre 
be assumed. tempor stabil be enforc with smooth prior 
esmooth = ∥p̂gt ∥2, penal the acceler p̂gt . To counteract the 
strong depth uncertainti in monocular reconstruction, we penal 
larg variat in depth addit with edepth = ∥[p̃gt ]z ∥2 where 
[p̃gt ]z be the z compon of 3D veloc p̃gt . finally, the 3D pose be 
also filter with the 1 euro filter [casiez et al. 2012]. 

parameters: the energi term eik,eproj,esmooth and edepth be 
weight with ωik = 1,ωproj = 44,ωsmooth = 0.07 and ωdepth = 
0.11, respectively. the paramet of the 1 euro filter [casiez et al. 
2012] be empir set to fcmin = 1.7, β = 0.3 for filter Kt , to 
fcmin = 0.8, β = 0.4 for plt , and to fcmin = 20, β = 0.4 for filter 
pgt . our implement us the levenberg-marquardt algorithm 
from the cere librari [agarw et al. 2017]. 

5 result 
We show live applic of our system at 30 hz. the reconstruct 
qualiti be high and we demonstr the use of our method 
3D charact control, embodi virtual reality, and pose track 
from low qualiti smartphon camera streams. see section 5.3 and 
figur 1. result be best observ in motion in the supplement 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



vnect: real-tim 3D human pose estim with a singl rgb camera • 44:7 

video. the import of the step toward enabl these applica- 
tion with a video solut be thoroughli evalu in more than 
10 sequences. result be compar in qualiti to depth-camera 
base solut like the kinect [microsoft corpor 2013] and 
significantli outperform exist monocular video-bas solutions. 
As the qualit baselin we choos the state-of-the-art 2D to 3D 
lift approach of zhou et al. [2015] and the 3D regress approach 
of mehta et al. [2016], which estim joint-posit offline. the 
accuraci improv be further quantit valid on the 
establish h3.6m dataset [ionescu et al. 2014b] and the mpi-inf- 
3dhp dataset [mehta et al. 2016]. the robust to divers persons, 
cloth and scene be demonstr on sever real-tim exampl 
and commun videos. pleas see our project webpag for more 
result and details1. 

comput be perform on a 6-core xeon cpu, 3.8 ghz and 
a singl titan X (pascal architecture) gpu. the cnn comput 
take ≈18 ms, the skeleton fit ≈7–10 ms, and preprocess and 
filter 5 ms. 

5.1 comparison with activ depth sensor (kinect) 
We synchron record video from an rgb camera and a co- 
locat kinect sensor in a live room scenario. figur 6 show rep- 
resent frames. although the depth sensor provid addit 
information, our reconstruct from just rgb be of a similar qual- 
ity. the kinect result be of compar stabil to ours, but yield 
erron reconstruct when limb be close to scene objects, 
such a when sit down. our rgb solution, however, succeed 
in thi case, although be slightli less reliabl in depth estimation. 
A challeng case for both method be the tight cross of legs. 
pleas see the supplement video for a visual comparison. 
the video solut succeed also in situat with direct sun- 

light (figur 7), where ir-bas depth camera be inoperable. more- 
over, rgb camera can simpli be equip with larg field-of-view 
(fov) len and, despit strong distortions, success track hu- 
man [rhodin et al. 2016a]. On the other hand, exist activ sen- 
sor be limit to rel small fovs, which sever limit the 
track volume. 

5.2 comparison with video solut 

qualit evaluation: We qualit compar against the 3D 
pose regress method of mehta et al. [2016] and zhou et al. [2015] 
on sequenc 6 (outdoor) of mpi-inf-3dhp test set. our result be 
compar to the qualiti of these offlin method (see figur 10). 
however, the per frame estim of these offlin method exhibit 
jitter over time, a drawback of most exist solutions. our full pose 
result be tempor stabl and be comput at real-tim frame 
rate of 30 hz. 

the kinemat skeleton fit estim global translat d. fig- 
ure 8 demonstr that estim be drift-free, the foot posit 
match with the same refer point after perform a circular 
walk. the smooth constraint in depth direct limit slide 
of the charact away from the character, a pictur in the supple- 
mental video sequences. 

1http://gvv.mpi-inf.mpg.de/projects/vnect/ 

fig. 6. side-by-sid pose comparison with our method (top) and kinect 
(bottom). overal estim pose be of similar qualiti (first two frames). 
both the kinect (third and fourth frames) and our approach (fourth and 
fifth frames) occasion predict erron poses. 

fig. 7. our approach succeed in strong illumin and sunlight (center 
right and right), while the ir-bas depth estim of the microsoft kinect 
be erron (left) and depth-bas track fail (center left). 

fig. 8. the estim 3D pose be drift-free. the motion of the person start 
and end at the mark point (orange), both in the real world and in our 
reconstruction. 

quantit evaluation: We compar our method with the state- 
of-the-art approach of mehta et al. [2016] on the mpi-inf-3dhp 
dataset, use the more robust percentag of correct keypoint 
metric (3d pck @150mm) on the 14 joint span by head, neck, 
shoulders, elbow, wrist, hips, knee and ankles. We train both, our 
model, a well a that of mehta et al. on the same data (human3.6m 
+ mpi-inf-3dhp), a detail in section 4.1, to be compat in 
term of the camera viewpoint selected, and use resnet100 a the 
base architectur for a fair comparison. tabl 1 show the result of 
the raw 3D predict from our network on ground-truth bound 
box crop frames. We see that the result be compar to that 
of mehta et al. the slight increas in accuraci on go to a 50-layer 
network be possibl due to the good gradient estim come 
from larg mini-batch that can be fit into memori while training, 
on account of the small size of the network. evid that our 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



44:8 • D. mehta et. al. 

tabl 1. comparison of our network against state of the art on mpi-inf-3dhp test set, use ground-truth bound boxes. We report the percentag of 
correct keypoint measur in 3d, and the area under the curv for the same, a propos by mpi-inf-3dhp. We addit report the mean per joint 
posit error in mm. higher pck and auc be better, and low mpjpe be better. 

stand/ sit On crouch/ On the 
network scale walk exercis chair reach floor sport misc. total 

pck pck pck pck pck pck pck pck auc mpjpe(mm) 
our 

(resnet 100) 
0.7, 1.0 87.6 76.4 71.4 71.6 47.8 82.5 78.9 75.0 39.5 127.8 
1.0 86.4 72.3 68.0 65.4 40.7 80.5 76.3 71.4 36.9 142.8 

our 
(resnet 50) 

0.7, 1.0 87.7 77.4 74.7 72.9 51.3 83.3 80.1 76.6 40.4 124.7 
1.0 86.7 73.9 69.8 66.1 44.7 82.0 79.4 73.3 37.8 138.7 

[mehta et al. 2016] 
(resnet 100) 

0.7, 1.0 86.6 75.3 74.8 73.7 52.2 82.1 77.5 75.7 39.3 117.6 
1.0 86.3 72.4 71.5 67.6 49.2 81.0 76.2 73.2 37.8 126.6 

fig. 9. A visual look at the direct 3D predict result from our fully- 
convolut formul v mehta et al. our formul allow the pre- 
diction to be more strongli tie to imag evidence, lead to overal good 
pose quality, particular for the end effectors. the red arrow point to mis- 
predictions. 

fig. 10. side-by-sid comparison of our full method (left), against the of- 
fline joint-posit estim method of mehta et al. [2016] (middle) and 
zhou et al. [2015] (right). our real-tim result be of a compar qualiti 
to these offlin methods. 2D joint posit for zhou et al. be gener 
use convolut pose machin [2016]. 

fig. 11. joint-wis breakdown of the accuraci of mehta et al. and our 
resnet100 base cnn predict on mpi-inf-3dhp test set. 

method tie the estim 3D posit strongli to imag appear- 
anc than previou method can also be glean from the fact that 
our approach perform significantli good for activ class such 
a standing/walking, sport and miscellan without signific 
self-occlusions. We do lose some perform on activ class 
with signific self-occlus such a sitting/li on the floor. We 
addit report the mean per joint posit error (mpjpe) num- 
ber in mm. note that mpjpe be not a robust measure, and be heavili 
influenc by larg outliers, and henc the bad perform on 
the mpjpe measur (124.7mm v 117.6mm) despit the good 3D 
pck result (76.6% v 75.7%). We further investig the natur of 
error of our method. We first look at the joint-wis breakup of 
accuraci of our fully-convolut resnet100 cnn predict v 
mehta et al. ’s formul with fully-connect layers. figur 11 
show that the accuraci of ankl for our formul be significantli 
better, while the accuraci of the head be markedli worse. 

In figur 9, we visual compar the two methods, further demon- 
strate the strong tie-in to imag appear that our formul 
affords, and the downsid of the strong tie-in. We also show that 
our method be prone to occasion larg mispredict when the 
bodi joint 2D locat detector misfires. It be these larg outlier that 
obfusc the report mpjpe numbers. figur 12, which plot the 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



vnect: real-tim 3D human pose estim with a singl rgb camera • 44:9 

fig. 12. fraction of joint incorrectli predict on mpi-inf-3dhp test 
set, a determin by the distanc between the predict joint locat 
and the ground truth joint locat be great than the error threshold. 
the dot line mark the threshold for which the 3D pck number be 
reported. At bottom right we see that our method have larg occasion 
mispredictions, which result in high mpjpe number despit otherwis 
similar performance. 

tabl 2. result on mpi-inf-3dhp test set with the bound box corner 
randomli jitter between +/- 40px to emul nois from a BB estimator. 
our fully-convolut formul be more robust than a compar 
fully-connect formulation. the evalu be at a singl scale (1.0). 

stand/ sit On crouch/ On the 
network walk exerc. chair reach floor sport misc. total 

pck pck pck pck pck pck pck pck auc 
our (resnet 100) 86.0 71.0 65.0 61.1 37.4 78.9 75.5 69.5 35.8 
our (resnet 50) 84.9 69.4 65.1 61.9 40.8 78.6 77.6 70.1 35.7 
[mehta et al. 2016] 81.2 64.2 67.1 62.1 43.5 76.0 71.1 67.8 34.0 

fraction of mispredict joint vs. the error threshold on mpi-inf- 
3dhp test set show that our method have a high fraction of per- 
joint mispredict beyond 300mm. It explain the high mpjpe 
number compar to mehta et al. despit equival pck perfor- 
mance. the variou filter stage employ in the full pipelin 
amelior these larg mispredictions. 

forhuman3.6m,w follow the protocol a in earli work [pavlako 
et al. 2016; tekin et al. 2016b,c], and evalu on all action and cam- 
era for subject number 9 and 11, and report mean per joint posit 
error (mm) for root rel 3D joint posit from our network. 
see tabl 3. note that despit the occasion larg outlier affect 
the mpjpe measure, our predict be still good than most of the 
exist methods. 
the accuraci attain from singl view method be still below 

that of real-tim multi-view methods, which can achiev a mean 
accuraci of the order of 10mm [stoll et al. 2011]. 

gener to differ person and scenes: We test our method 
on a varieti of actors, it succeed for differ bodi shapes, gender 
and skin tone. see supplement video. To further valid the robust- 
ness we appli the method to commun video from youtube, 
see figur 1. It gener well to the differ background and 
camera types. 

fig. 13. fraction of joint correctli predict on the ts1 sequenc of mpi- 
inf-3dhp test set, a determin by the distanc between the predict 
joint locat and the ground truth joint locat be below the error 
threshold. the dot line mark the 150mm threshold for which the 3D 
pck number be reported. We see that onli use the 2D predict 
a constraint for skeleton fit (blue) perform significantli bad than 
use both 2D and 3D predict a constraint (red). though add 1 euro 
filter (purple) visual improv the results, the slightli high error here 
be due to the sluggish recoveri from track failures. the 3D predict 
from the cnn (green) be also shown. 

model components: To demonstr that our fully-convolut 
pose formul be less sensit to inexact crop than network 
use a fully-connect formulation, we emul a noisi BB estima- 
tor by jitter the ground-truth bound box corner of mpi-inf- 
3dhp test set uniformli at random in the rang of +/- 40 px. thi 
also captur scenario where one or more end effector be not in 
the frame, so a loss in accuraci be expect for all methods. tabl 2 
show that the fully-connect formul of mehta et al. suffer a 
bad hit in accuraci than our approach, go down by 7.9 pck, 
while our compar network go down by onli 5.5 pck. 

We show the effect of the variou compon of our full pipelin 
on the ts1 sequenc of mpi-inf-3dhp test set in figur 13. without 
the eik compon of etot the track accuraci go down to a 
pck of 46.1% compar to a pck of 81.7% when eik be used. the raw 
cnn 3D predict in conjunct with the BB tracker result in a 
pck of 80.3%. use eik in etot produc consist good result 
for all threshold low than 150 mm. thi show the improv 
brought about by our skeleton fit term. additionally, a show 
in the supplementari video, use 1 euro filter produc qualita- 
tive good results, but the overal pck decreas slightli (79.7%) 
due to slow recoveri from track failures. the influenc of 
the smooth and filter step on the tempor consist be 
further analyz in the supplement video. 

5.3 applic 
our approach be suitabl for variou interact applic sinc 
it be real-time, tempor stable, fulli automatic, and export data 
directli in a format amen to 3D charact control. 

charact control: real-tim motion captur solut provid a 
natur interfac for game charact and virtual avatars, which 
go beyond classic mous and gamepad control. We appli our 
method on motion common in activ like tennis, dance, and 
juggling, see figur 1 and 14. the swing of the arm and leg motion 
be nice captur and could, for instance, be use in a casual sport 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



44:10 • D. mehta et. al. 

tabl 3. result of our raw cnn predict on human3.6m, evalu on the ground truth bound box crop for all frame of subject 9 and 11. our cnn 
use onli human3.6m a the 3D train set, and be pretrain for 2D pose prediction. the error measur use be mean per joint posit error (mpjpe) in 
millimeters. note again that the error measur use be not robust, and subject to obfusc from occasion larg mispredictions, such a those exhibit by 
our raw cnn predictions. 

sit take walk walk 
method direct discuss eat greet phone pose purch. sit down smoke photo wait walk dog pair all 

[zhou et al. 2015b] 87.4 109.3 87.1 103.2 116.2 106.9 99.8 124.5 199.2 107.4 143.3 118.1 79.4 114.2 97.7 113.0 
[tekin et al. 2016c] 102.4 147.7 88.8 125.3 118.0 112.3 129.2 138.9 224.9 118.4 182.7 138.8 55.1 126.3 65.8 125.0 
[yu et al. 2016] 85.1 112.7 104.9 122.1 139.1 105.9 166.2 117.5 226.9 120.0 135.9 117.7 137.4 99.3 106.5 126.5 
[ionescu et al. 2014b] 132.7 183.6 132.4 164.4 162.1 150.6 171.3 151.6 243.0 162.1 205.9 170.7 96.6 177.1 127.9 162.1 
[zhou et al. 2016] 91.8 102.4 97.0 98.8 113.4 90.0 93.8 132.2 159.0 106.9 125.2 94.4 79.0 126.0 99.0 107.3 
[pavlako et al. 2016] 58.6 64.6 63.7 62.4 66.9 57.7 62.5 76.8 103.5 65.7 70.7 61.6 69.0 56.4 59.5 66.9 
[mehta et al. 2016] 52.6 63.8 55.4 62.3 71.8 52.6 72.2 86.2 120.6 66.0 79.8 64.0 48.9 76.8 53.7 68.6 
[tekin et al. 2016b] 85.0 108.8 84.4 98.9 119.4 98.5 93.8 73.8 170.4 85.1 95.7 116.9 62.1 113.7 94.8 100.1 
our (resnet 100) 61.7 77.8 64.6 70.3 90.5 61.9 79.8 113.2 153.1 80.9 94.4 75.1 54.9 83.5 61.0 82.5 
our (resnet 50) 62.6 78.1 63.4 72.5 88.3 63.1 74.8 106.6 138.7 78.8 93.8 73.9 55.8 82.0 59.6 80.5 

and danc game, but also for motion analysi of profession 
athlet to optim their motion patterns. We also show success 
result in non front-fac motion such a turn and write on 
a wall, a well a squatting. 

virtual reality: the recent avail of cheap head-mount 
display have spark a rang of new applications. mani product 
use handheld devic to track the user’ hand posit for inter- 
action. our solut enabl them from a singl consum color 
camera. beyond interaction, our marker-less full-bodi solut en- 
abl embodi virtual reality, see figur 1. A rich immers feel 
be creat by pose a virtual avatar of the user exactli to their own 
real pose. with our solut the real and virtual pose be align 
such that user perceiv the virtual bodi a their own. 

ubiquit motion captur with smartphones: real-tim monocular 
3D pose estim lend itself to applic on low qualiti smart- 
phone video streams. By stream the video to a machin with 
suffici capabl for our algorithm, one can turn ani smart- 
phone into a lightweight, fully-automatic, handheld motion captur 
sensor, see figur 15 and the accompani video. sinc smart- 
phone be widespread, it enabl the aforement applic 
for casual user without requir addit sens devices. 

6 limit 
depth estim from a monocular imag be sever ill posed, 
slight inaccuraci in the estim can lead to larg differ 
depth estimates, which manifest also in our result in slight tem- 
poral jitter. We claim improv stabil and tempor consist 
compar to exist monocular rgb 3D pose estim methods. 
thi uncertainti could be further reduc with domain specif 
knowledge, e.g., foot-contact constraint when the floor locat 
be known, and head-pos stabil with the posit of head- 
mounted-display in VR applications, which be readili obtain with 
imu-sensors. 
A downsid of our cnn predict formul be that mispre- 

diction of 2D joint locat result in implaus 3D poses. thi be 
amelior in the tracker through skeleton retarget and pose 

fig. 14. applic to entertainment. the real-tim 3D pose estim 
method provid a natur motion interface, e.g. for sport games. 

filtering. thi could be address directli in the cnn through im- 
posit of strong interdepend between predictions. addi- 
tionally, the perform on pose with signific amount of self 
occlus remain a challenge. 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



vnect: real-tim 3D human pose estim with a singl rgb camera • 44:11 

fig. 15. handheld record with a readili avail smartphon camera 
(left) and our estim pose (right), stream to and process by a gpu 
enabl pc. 

further, veri fast motion can exceed the converg radiu of 
our IK optimization, but the integr of per frame 2D and 3D 
pose yield quick recoveri from erron poses. initi experi 
with 256 × 256 px input to the cnn show that much high frame 
rate be possibl with no loss in accuracy. 

7 discuss 
the avail of suffici annot 3D pose train data re- 
main an issue. even the most recent annot real 3D pose data 
sets, or combin real/synthet data set [chen et al. 2016; ionescu 
et al. 2014b; mehta et al. 2016] be a subset of real world human 
pose, shape, appear and background distributions. recent top 
perform method explicitli address thi data sparsiti by train 
similarli deep networks, but with architectur chang enabl 
improv intermedi train supervis [mehta et al. 2016]. 
our implement onli support a singl person, although 

the propos fully-convolut formul could be scale to 
multipl persons. such an extens be current preclud due to 
the lack of multi-person datasets, requir to train multi-person 3D 
pose regressors. one possibl approach be to adapt the multi-person 
2D pose method of insafutdinov et al. [2016] and cao et al. [2016]. 
We also analyz the impact of 2D joint posit mispredict 

on the 3D joint posit predict from our fully-convolut 
formulation. We decoupl the 3D predict from the 2D predic- 
tion by look up the 3D joint posit from their location-map 
use the ground truth 2D joint positions. see tabl 4. We see a 3D 
pck improv of 2.8, which be congruent with the notion of a 
strong tie-in of the predict joint posit with the imag plane, 
which caus the 3D joint predict to be erron when 2D 
joint detect misfires. the upsid of thi be that the 3D predict 
can be improv through improv to 2D joint posit predic- 
tion. alternatively, optim formul that directli oper 
on the heatmap and the location-map could be constructed. our 

tabl 4. result onmpi-inf-3dhp test set with the 3D joint posit lookup 
in the location-map do use the ground truth 2D locat rather the 
predict 2D locations. We see that the locat map have captur good 
3D pose information, which can perhap be extract through optim 
method oper directli on heatmap and location-maps. the evalu 
us 2 scale (0.7, 1.0). 

stand/ sit On crouch/ On the 
network walk exerc. chair reach floor sport misc. total 

pck pck pck pck pck pck pck pck auc 
our (resnet 100) 88.1 80.9 74.0 76.1 56.3 82.9 80.2 77.8 41.0 
our (resnet 50) 88.0 81.8 78.6 77.4 59.3 82.8 81.2 79.4 41.6 

[mehta et al. 2016] 86.6 75.3 74.8 73.7 52.2 82.1 77.5 75.7 39.3 

fully-convolut formul can also benefit from iter re- 
finement, akin to heatmap-bas 2D pose estim approach 
[hu et al. 2016; newel et al. 2016]. 

8 conclus 
We have present the first method that estim the 3D kinemat 
pose of a human, includ global position, in a stable, tempor 
consist manner from a singl rgb video stream at 30 hz. our 
approach combin a fully-convolut cnn that regress 2D and 
3D joint posit and a kinemat skeleton fit method, produc- 
ing a real-tim tempor stabl 3D reconstruct of the motion. 
In contrast to most exist approaches, our network can oper 
without strict bound boxes, and facilit inexpens bound 
box tracking. We have show result in a varieti of challeng 
real-tim scenarios, includ live stream from a smartphon 
camera, a well a in commun videos. A number of applic 
have be presented, such a embodi VR and interact charact 
control for comput games. 

qualit and quantit evalu demonstr that our ap- 
proach compar to offlin state-of-the-art monocular rgb method 
and approach the qualiti of real-tim rgb-d methods. hence, we 
believ our method be a signific step forward to democrat 
3D human pose estimation, lift both the need for special camera 
such a the ir-bas depth cameras, a well a the long and heavi 
process times. 

refer 
ankur agarw and bill triggs. 2006. recov 3D human pose from monocular 

images. ieee transact on pattern analysi and machin intellig (pami) 28, 1 
(2006), 44–58. 

sameer agarwal, keir mierle, and others. 2017. cere solver. http://ceres-solver.org. 
(2017). 

ijaz akhter and michael J black. 2015. pose-condit joint angl limit for 3D human 
pose reconstruction. In ieee confer on comput vision and pattern recognit 
(cvpr). 1446–1455. 

sikandar amin, mykhaylo andriluka, marcu rohrbach, and bernt schiele. 2013. multi- 
view pictori structur for 3D human pose estimation. In bmvc. 

mykhaylo andriluka, leonid pishchulin, peter gehler, and bernt schiele. 2014. 2D 
human pose estimation: new benchmark and state of the art analysis. In ieee 
confer on comput vision and pattern recognit (cvpr). 

mykhaylo andriluka, stefan roth, and bernt schiele. 2009. pictori structur revisited: 
peopl detect and articul pose estimation. In ieee confer on comput 
vision and pattern recognit (cvpr). 1014–1021. 

anelia angelova, alex krizhevsky, vincent vanhoucke, abhijit ogale, and dave fer- 
guson. 2015. real-tim pedestrian detect with deep network cascades. In 
proceed of bmvc 2015. 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



44:12 • D. mehta et. al. 

andrea baak, meinard müller, gaurav bharaj, hans-pet seidel, and christian 
theobalt. 2011. A data-driven approach for real-tim full bodi pose recon- 
struction from a depth camera. In ieee intern confer on comput vision 
(iccv). 

alexandru O balan, leonid sigal, and michael J black. 2005. A quantit evalu 
of video-bas 3D person tracking. In 2005 ieee intern workshop on visual 
surveil and perform evalu of track and surveillance. ieee, 349–356. 

alexandru O balan, leonid sigal, michael J black, jame E davis, and horst W 
haussecker. 2007. detail human shape and pose from images. In ieee confer 
on comput vision and pattern recognit (cvpr). 1–8. 

vasileio belagiannis, sikandar amin, mykhaylo andriluka, bernt schiele, nassir navab, 
and slobodan ilic. 2014. 3D pictori structur for multipl human pose estimation. 
In ieee confer on comput vision and pattern recognit (cvpr). 1669–1676. 

vasileio belagianni and andrew zisserman. 2016. recurr human pose estimation. 
arxiv preprint arxiv:1605.02914 (2016). 

alessandro bissacco, ming-hsuan yang, and stefano soatto. 2007. fast human pose 
estim use appear and motion via multi-dimension boost regression. 
In 2007 ieee confer on comput vision and pattern recognition. ieee, 1–8. 

liefeng Bo and cristian sminchisescu. 2010. twin gaussian process for structur 
prediction. intern journal of comput vision 87, 1-2 (2010), 28–52. 

federica bogo, angjoo kanazawa, christoph lassner, peter gehler, javier romero, and 
michael J. black. 2016. keep it smpl: automat estim of 3D human pose and 
shape from a singl image. In european confer on comput vision (eccv). 

lubomir bourdev and jitendra malik. 2009. poselets: bodi part detector train use 
3d human pose annotations. In ieee intern confer on comput vision 
(iccv). 1365–1372. 

ernesto brau and hao jiang. 2016. 3D human pose estim via deep learn from 
2D annotations. In intern confer on 3D vision (3dv). 

christoph bregler and jitendramalik. 1998. track peopl with twist and exponenti 
maps. In confer on comput vision and pattern recognition. 8–15. 

zhe cao, toma simon, shih-enwei, and yaser sheikh. 2016. realtim multi-person 2D 
pose estim use part affin fields. arxiv preprint arxiv:1611.08050 (2016). 

géri casiez, nicola roussel, and daniel vogel. 2012. 1âćň filter: a simpl speed-bas 
low-pass filter for noisi input in interact systems. In proceed of the sigchi 
confer on human factor in comput systems. acm, 2527–2530. 

jinxiang chai and jessica K hodgins. 2005. perform anim from low- 
dimension control signals. acm transact on graphic (tog) 24, 3 (2005), 
686–696. 

wenzheng chen, huan wang, yangyan li, hao su, zhenhua wang, changh tu, dani 
lischinski, daniel cohen-or, and baoquan chen. 2016. synthes train 
imag for boost human 3D pose estimation. In intern confer on 3D 
vision (3dv). 

martin de La gorce, niko paragios, and david J fleet. 2008. model-bas hand track 
with texture, shade and self-occlusions. In comput vision and pattern recognition, 
2008. cvpr 2008. ieee confer on. ieee, 1–8. 

jonathan deutscher and ian reid. 2005. articul bodi motion captur by stochast 
search. intern journal of comput vision 61, 2 (2005), 185–205. 

mingsong dou, sameh khamis, yuri degtyarev, philip davidson, sean ryan fanello, 
adarsh kowdle, sergio ort escolano, christoph rhemann, david kim, jonathan 
taylor, and others. 2016. fusion4d: real-tim perform captur of challeng 
scenes. acm transact on graphic (tog) 35, 4 (2016), 114. 

ahm elgamm and chan-su lee. 2004. infer 3D bodi pose from silhouett 
use activ manifold learning. In comput vision and pattern recognition, 2004. 
cvpr 2004. proceed of the 2004 ieee comput societi confer on, vol. 2. ieee, 
ii–681. 

ahm elhayek, edilson de aguiar, arjun jain, jonathan tompson, leonid pishchulin, 
mykhaylo andriluka, christoph bregler, bernt schiele, and christian theobalt. 
2016. marconi - convnet-bas marker-less motion captur in outdoor and 
indoor scenes. ieee transact on pattern analysi and machin intellig (pami) 
(2016). 

pedro F felzenszwalb, ross B girshick, david mcallester, and deva ramanan. 2010. ob- 
ject detect with discrimin train part-bas models. In ieee transact 
on pattern analysi and machin intelligence. ieee, 1627–1645. 

pedro F felzenszwalb and daniel P huttenlocher. 2005. pictori structur for object 
recognition. intern journal of comput vision (ijcv) 61, 1 (2005), 55–79. 

vittorio ferrari, manuel marin-jimenez, and andrew zisserman. 2009. pose search: 
retriev peopl use their pose. In ieee confer on comput vision and pattern 
recognit (cvpr). 1–8. 

juergen gall, bodo rosenhahn, thoma brox, and hans-pet seidel. 2010. optim 
and filter for human motion capture. intern journal of comput vision 
(ijcv) 87, 1–2 (2010), 75–92. 

varun ganapathi, christian plagemann, daphn koller, and sebastian thrun. 2012. 
real-tim human pose track from rang data. In european confer on comput 
vision. springer, 738–751. 

ravi garg, anastasio roussos, and lourd agapito. 2013. dens variat recon- 
struction of non-rigid surfac from monocular video. In proceed of the ieee 

confer on comput vision and pattern recognition. 1272–1279. 
ross girshick, jami shotton, pushmeet kohli, antonio criminisi, and andrew fitzgib- 

bon. 2011. effici regress of general-act human pose from depth images. 
In comput vision (iccv), 2011 ieee intern confer on. ieee, 415–422. 

paulo FU gotardo and aleix M martinez. 2011. comput smooth time trajectori 
for camera and deform shape in structur from motion with occlusion. ieee 
transact on pattern analysi and machin intellig 33, 10 (2011), 2051–2065. 

kaim he, xiangyu zhang, shaoq ren, and jian sun. 2016. deep residu learn 
for imag recognition. In ieee confer on comput vision and pattern recognit 
(cvpr). 

nichola R howe, michael E leventon, and william T freeman. 1999. bayesian re- 
construct of 3D human motion from single-camera video.. In nips, vol. 99. 
820–6. 

peiyun hu, deva ramanan, jia jia, sen wu, xiaohui wang, lianhong cai, and jie tang. 
2016. bottom-up and top-down reason with hierarch rectifi gaussians. 
In ieee confer on comput vision and pattern recognit (cvpr). 

matthia innmann, michael zollhöfer, matthia nießner, christian theobalt, and marc 
stamminger. 2016. volumedeform: real-tim volumetr non-rigid reconstruction. 
(octob 2016), 17. 

eldar insafutdinov, leonid pishchulin, bjoern andres, mykhaylo andriluka, and bernt 
schiele. 2016. deepercut: A deeper, stronger, and faster multi-person pose estima- 
tion model. In european confer on comput vision (eccv). 

sergey ioff and christian szegedy. 2015. batch normalization: acceler deep 
network train by reduc intern covari shift. In proceed of the 32nd 
intern confer on machin learning. 448–456. 

catalin ionescu, joao carreira, and cristian sminchisescu. 2014a. iter second- 
order label sensit pool for 3d human pose estimation. In ieee confer on 
comput vision and pattern recognit (cvpr). 1661–1668. 

catalin ionescu, drago papava, vlad olaru, and cristian sminchisescu. 2014b. hu- 
man3.6m: larg scale dataset and predict method for 3d human sens in 
natur environments. ieee transact on pattern analysi and machin intellig 
(pami) 36, 7 (2014), 1325–1339. 

arjun jain, thorsten thormählen, hans-pet seidel, and christian theobalt. 2010. 
moviereshape: track and reshap of human in videos. acm transact 
on graphic 29, 5 (2010). doi:https://doi.org/10.1145/1866158.1866174 

yangq jia, evan shelhamer, jeff donahue, sergey karayev, jonathan long, ross 
girshick, sergio guadarrama, and trevor darrell. 2014. caffe: convolut ar- 
chitectur for fast featur embedding. In proceed of the 22nd acm intern 
confer on multimedia. 675–678. 

sam johnson and mark everingham. 2010. cluster pose and nonlinear appear 
model for human pose estimation. In british machin vision confer (bmvc). 
doi:10.5244/c.24.12. 

sam johnson and mark everingham. 2011. learn effect human pose estim 
from inaccur annotation. In proceed of ieee confer on comput vision 
and pattern recognition. 

minsik lee, jungchan cho, chong-ho choi, and songhwai oh. 2013. procrustean 
normal distribut for non-rigid structur from motion. In proceed of the ieee 
confer on comput vision and pattern recognition. 1280–1287. 

sijin Li and antoni B chan. 2014. 3d human pose estim from monocular imag 
with deep convolut neural network. In asian confer on comput vision 
(accv). 332–347. 

sijin li, weichen zhang, and antoni B chan. 2015a. maximum-margin structur 
learn with deep network for 3d human pose estimation. In ieee intern 
confer on comput vision (iccv). 2848–2856. 

sijin li, weichen zhang, and antoni B chan. 2015b. maximum-margin structur 
learn with deep network for 3d human pose estimation. In ieee intern 
confer on comput vision (iccv). 2848–2856. 

ita lifshitz, ethan fetaya, and shimon ullman. 2016. human pose estim use 
deep consensu voting. In european confer on comput vision (eccv). 

wei liu, dragomir anguelov, dumitru erhan, christian szegedy, and scott E. reed. 
2016. ssd: singl shot multibox detector. In european confer on comput 
vision (eccv). 

matthew M loper and michael J black. 2014. opendr: An approxim differenti 
renderer. In european confer on comput vision. springer, 154–169. 

ziyang Ma and enhua wu. 2014. real-tim and robust hand track with a singl 
depth camera. the visual comput 30, 10 (2014), 1133–1144. 

dushyant mehta, helg rhodin, dan casas, oleksandr sotnychenko, weipeng xu, and 
christian theobalt. 2016. monocular 3D human pose estim In the wild use 
improv cnn supervision. arxiv preprint arxiv:1611.09813v2 (2016). 

alberto menache. 2000. understand motion captur for comput anim and video 
games. morgan kaufmann. 

microsoft corporation. 2010. kinect for xbox 360. http://www.xbox.com/en-us/ 
xbox-360/accessories/kinect. (2010). 

microsoft corporation. 2013. kinect for xbox one. http://www.xbox.com/en-us/ 
xbox-one/accessories/kinect. (2013). 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



vnect: real-tim 3D human pose estim with a singl rgb camera • 44:13 

microsoft corporation. 2015. kinect sdk. https://developer.microsoft.com/en-us/ 
windows/kinect. (2015). 

thoma B. moeslund, adrian hilton, and volker krãijger. 2006. A survey of advanc 
in vision-bas humanmot captur and analysis. cviu 104, 2–3 (2006), 90–126. 

greg mori and jitendra malik. 2006. recov 3d human bodi configur use 
shape contexts. ieee transact on pattern analysi and machin intellig 
(tpami) 28, 7 (2006), 1052–1062. 

richard A. newcombe, dieter fox, and steven M. seitz. 2015. dynamicfusion: recon- 
struction and track of non-rigid scene in real-time. In the ieee confer 
on comput vision and pattern recognit (cvpr). 

alejandro newell, kaiyu yang, and jia deng. 2016. stack hourglass network for 
human pose estimation. In european confer on comput vision (eccv). 

iason oikonomidis, nikolao kyriazis, and antoni A argyros. 2011. effici model- 
base 3D track of hand articul use kinect.. In bmvc, vol. 1. 3. 

sergio orts-escolano, christoph rhemann, sean fanello, wayn chang, adarsh kowdle, 
yuri degtyarev, david kim, philip L davidson, sameh khamis, mingsong dou, and 
others. 2016. holoportation: virtual 3D teleport in real-time. In proceed 
of the 29th annual symposium on user interfac softwar and technology. acm, 
741–754. 

hyun soo park and yaser sheikh. 2011. 3D reconstruct of a smooth articul tra- 
jectori from a monocular imag sequence. In intern confer on comput 
vision (iccv). 201–208. 

georgio pavlakos, xiaowei zhou, konstantino G derpanis, and kosta daniilidis. 
2016. coarse-to-fin volumetr predict for single-imag 3D human pose. arxiv 
preprint arxiv:1611.07828 (2016). 

leonid pishchulin, mykhaylo andriluka, peter gehler, and bernt schiele. 2013. strong 
appear and express spatial model for human pose estimation. In proceed 
of the ieee intern confer on comput vision. 3487–3494. 

leonid pishchulin, eldar insafutdinov, siyu tang, bjoern andres, mykhaylo andriluka, 
peter gehler, and bernt schiele. 2016. deepcut: joint subset partit and label 
for multi person pose estimation. In ieee confer on comput vision and pattern 
recognit (cvpr). 

gerard pons-moll, david J fleet, and bodo rosenhahn. 2014. posebit for monocular hu- 
man pose estimation. In ieee confer on comput vision and pattern recognit 
(cvpr). 2337–2344. 

real madrid c.f. 2016. cristiano ronaldo and coentrao continu their recovery. https: 
//www.youtube.com/watch?v=xqipux_buoo. (2016). 

joseph redmon, santosh divvala, ross girshick, and ali farhadi. 2015. you onli look 
once: unified, real-tim object detection. arxiv preprint arxiv:1506.02640 (2015). 

shaoq ren, kaim he, ross girshick, and jian sun. 2015. faster r-cnn: toward 
real-tim object detect with region propos networks. In advanc in neural 
inform process systems. 91–99. 

helg rhodin, christian richardt, dan casas, eldar insafutdinov, mohammad shafiei, 
hans-pet seidel, bernt schiele, and christian theobalt. 2016a. egocap: egocentr 
marker-less motion captur with two fishey cameras. acm trans. graph. (proc. 
siggraph asia) (2016). 

helg rhodin, nadia robertini, dan casas, christian richardt, hans-pet seidel, and 
christian theobalt. 2016b. gener automat human shape and motion captur 
use volumetr contour cues. In european confer on comput vision (eccv). 
springer, 509–526. 

helg rhodin, nadia robertini, christian richardt, hans-pet seidel, and christian 
theobalt. 2015. A versatil scene model with differenti visibl appli to 
gener pose estimation. In iccv. 

grégori rogez and cordelia schmid. 2016. mocap-guid data augment for 3D 
pose estim in the wild. arxiv preprint arxiv:1607.02046 (2016). 

lorenz rogge, felix klose, michael stengel, martin eisemann, and marcu magnor. 
2014. garment replac in monocular video sequences. acm transact on 
graphic (tog) 34, 1 (2014), 6. 

rómer rosal and stan sclaroff. 2000. special map and the estim of 
human bodi pose from a singl image. inhumanmotion, 2000. proceedings. workshop 
on. ieee, 19–24. 

rómer rosal and stan sclaroff. 2006. combin gener and discrimin model 
in a framework for articul pose estimation. intern journal of comput 
vision 67, 3 (2006), 251–276. 

rusfencing-tv. 2017. the most beauti strike / saber woman (translat from 
russian). https://www.youtube.com/watch?v=0gocmswukcu. (2017). 

jami shotton, tobi sharp, alex kipman, andrew fitzgibbon, mark finocchio, andrew 
blake, mat cook, and richard moore. 2013. real-tim human pose recognit in 
part from singl depth images. commun. acm 56, 1 (2013), 116–124. 

hedvig sidenbladh, michael J black, and david J fleet. 2000. stochast track of 3D 
human figur use 2D imag motion. In european confer on comput vision. 
springer, 702–718. 

leonid sigal, michael isard, horst haussecker, and michael J black. 2012. loose- 
limb people: estim 3D human pose and motion use non-parametr belief 
propagation. intern journal of comput vision (ijcv) 98, 1 (2012), 15–48. 

cristian sminchisescu, atul kanaujia, and dimitri metaxas. 2006. learn joint 
top-down and bottom-up process for 3D visual inference. In ieee confer on 
comput vision and pattern recognit (cvpr). 1743–1752. 

cristian sminchisescu, atul kanaujia, and dimitri N metaxas. 2007. bm3e: discrimina- 
tive densiti propag for visual tracking. ieee transact on pattern analysi 
and machin intellig 29, 11 (2007), 2030–2044. 

cristian sminchisescu and bill triggs. 2001. covari scale sampl for monocular 
3D bodi tracking. In ieee confer on comput vision and pattern recognit 
(cvpr), vol. 1. ieee, i–447. 

jonathan starck and adrian hilton. 2003. model-bas multipl view reconstruct of 
people. In ieee intern confer on comput vision (iccv). 915–922. 

carsten stoll, nil hasler, juergen gall, hans-pet seidel, and christian theobalt. 2011. 
fast articul motion track use a sum of gaussian bodi model. In ieee 
intern confer on comput vision (iccv). 951–958. 

leonid taycher, david demirdjian, trevor darrell, and gregori shakhnarovich. 2006. 
condit random people: track human with crf and grid filters. In 2006 ieee 
comput societi confer on comput vision and pattern recognit (cvpr’06), 
vol. 1. ieee, 222–229. 

camillo J taylor. 2000. reconstruct of articul object from point correspond 
in a singl uncalibr image. In ieee confer on comput vision and pattern 
recognit (cvpr), vol. 1. 677–684. 

bugra tekin, isinsu katircioglu, mathieu salzmann, vincent lepetit, and pascal fua. 
2016a. structur predict of 3D human pose with deep neural networks. In 
british machin vision confer (bmvc). 

bugra tekin, pablo márquez-neila, mathieu salzmann, and pascal fua. 2016b. fuse 
2D uncertainti and 3D cue for monocular bodi pose estimation. arxiv preprint 
arxiv:1611.05708 (2016). 

bugra tekin, artem rozantsev, vincent lepetit, and pascal fua. 2016c. direct predict 
of 3D bodi pose from motion compens sequences. In ieee confer on 
comput vision and pattern recognit (cvpr). 

jonathan J tompson, arjun jain, yann lecun, and christoph bregler. 2014. joint train 
of a convolut network and a graphic model for human pose estimation. In 
advanc in neural inform process system (nips). 1799–1807. 

alexand toshev and christian szegedy. 2014. deeppose: human pose estim via 
deep neural networks. In confer on comput vision and pattern recognit 
(cvpr). 1653–1660. 

raquel urtasun, david J fleet, and pascal fua. 2006. tempor motion model for 
monocular and multiview 3d human bodi tracking. comput vision and imag 
understand 104, 2 (2006), 157–177. 

marek vondrak, leonid sigal, jessica hodgins, and odest jenkins. 2012. video-bas 
3D motion captur through bipe control. acm transact On graphic (tog) 31, 
4 (2012), 27. 

chunyu wang, yizhou wang, zhouchen lin, alan L yuille, and wen gao. 2014. robust 
estim of 3d human pose from a singl image. In ieee confer on comput 
vision and pattern recognit (cvpr). 2361–2368. 

shih-enwei, varun ramakrishna, takeo kanade, and yaser sheikh. 2016. convolut 
pose machines. In confer on comput vision and pattern recognit (cvpr). 

xiaolin wei and jinxiang chai. 2010. videomocap: model physic realist human 
motion from monocular video sequences. In acm transact on graphic (tog), 
vol. 29. acm, 42. 

xiaolin wei, peizhao zhang, and jinxiang chai. 2012. accur realtim full-bodi 
motion captur use a singl depth camera. acm transact on graphic (tog) 
31, 6 (2012), 188. 

christoph richard wren, ali azarbayejani, trevor darrell, and alex paul pentland. 
1997. pfinder: real-tim track of the human body. ieee transact on pattern 
analysi and machin intellig (pami) 19, 7 (1997), 780–785. 

hashim yasin, umar iqbal, björn krüger, andrea weber, and juergen gall. 2016. A 
dual-sourc approach for 3D pose estim from a singl image. In confer 
on comput vision and pattern recognit (cvpr). 

mao Ye and ruigang yang. 2014. real-tim simultan pose and shape estim for 
articul object use a singl depth camera. In proceed of the ieee confer 
on comput vision and pattern recognition. 2345–2352. 

yongkang yu, feilinand yonghao, zhen yilin, and weidong mohan. 2016. marker-less 
3D human motion captur with monocular imag sequenc and height-maps. In 
european confer on comput vision (eccv). 

matthew D zeiler. 2012. adadelta: an adapt learn rate method. arxiv preprint 
arxiv:1212.5701 (2012). 

xiaowei zhou, spyridon leonardos, xiaoyan hu, and kosta daniilidis. 2015. 3D shape 
estim from 2D landmarks: A convex relax approach. In ieee confer 
on comput vision and pattern recognit (cvpr). 4447–4455. 

xingyi zhou, xiao sun, wei zhang, shuang liang, and yichen wei. 2016. deep kine- 
matic pose regression. eccv worktp on geometri meet deep learning. 

xiaowei zhou, menglong zhu, spyridon leonardos, and kosta daniilidis. 2015a. spars 
represent for 3D shape estimation: A convex relax approach. arxiv 
preprint arxiv:1509.04309 (2015). 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 



44:14 • D. mehta et. al. 

xiaowei zhou, menglong zhu, spyridon leonardos, kosta derpanis, and kosta dani- 
ilidis. 2015b. spars meet deepness: 3D human pose estim from monoc- 
ular video. In ieee confer on comput vision and pattern recognit (cvpr). 

yingi zhu, mark cox, and simon lucey. 2011. 3D motion reconstruct for real- 
world camera motion. In comput vision and pattern recognit (cvpr), 2011 ieee 
confer on. ieee, 1–8. 

michael zollhöfer, matthia nießner, shahram izadi, christoph rhemann, christoph 
zach, matthew fisher, chenglei wu, andrew fitzgibbon, charl loop, christian 
theobalt, and marc stamminger. 2014. real-tim non-rigid reconstruct use 
an rgb-d camera. acm transact on graphic (tog) 33, 4 (2014). 

acm transact on graphics, vol. 36, no. 4, articl 44. public date: juli 2017. 


