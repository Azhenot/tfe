
















































machin learn a bias view of women | wire 













machin taught by photo learn a sexist view of women 





subscrib 

busi 
cultur 
design 
gear 
scienc 
secur 
transport 
photo 
video 
backchannel 

search 


busi 
cultur 
design 
gear 
scienc 
secur 
transport 
photo 
video 
backchannel 

photo 
video 
backchannel 
magazin 
wire insid 








get our newslett 
wired’ big stori deliv to your inbox. 

submit 





we'r on 
pinterest 
see what' inspir us. 
follow 


















author: tom simonitetom simonit 

busi 

08.21.17 
09:00 be 

machin taught by photo learn a sexist view of women 


hotlittlepotato 

share 

share1801 

tweet 

comment 

email 








author: tom simonitetom simonit 

busi 

08.21.17 
09:00 be 

machin taught by photo learn a sexist view of women 


hotlittlepotato 

last fall, univers of 
virginia comput scienc professor vicent ordóñez notic a pattern 
in some of the guess make by image-recognit softwar he be 
building. “it would see a pictur of a kitchen and more often than not 
associ it with women, not men,” he says. 
that 
get ordóñez wonder whether he and other research be 
unconsci inject bia into their software. So he team up with 
colleagu to test two larg collect of label photo use to 
“train” image-recognit software. 
their 
result be illuminating. two promin research-imag 
collections—includ one support by microsoft and facebook—display a 
predict gender bia in their depict of activ such a cook 
and sports. imag of shop and wash be link to women, for 
example, while coach and shoot be tie to men. 
machine-learn 
softwar train on the dataset didn’t just mirror those biases, it 
amplifi them. If a photo set gener associ woman with cooking, 
softwar train by studi those photo and their label creat an 
even strong association. 
mark 
yatskar, a research at the allen institut for artifici 
intelligence, say that thi phenomenon could also amplifi other bia 
in data, for exampl relat to race. “thi could work to not onli 
reinforc exist social bia but actual make them worse,” say 
yatskar, who work with ordóñez and other on the project while at the 
univers of washington. 
As 
sophist machine-learn program proliferate, such distort 
matter. In the researchers' tests, peopl pictur in kitchens, for 
example, becom even more like to be label “woman” than reflect 
the train data. the researchers’ paper includ a photo of a man at a stove label “woman.” 
If 
replic in tech companies, these problem could affect photo-storag 
services, in-hom assist with camera like the amazon look, or 
tool that use social medium photo to discern consum preferences. 
googl accident demonstr the danger of inappropri imag 
softwar in 2015, when it photo servic tag black peopl a gorillas. 


relat stori 





tom simonit 
when govern rule by software, citizen are left in the dark 







brian barrett 
googl map Is racist becaus the internet Is racist 







megan garcia 
how to keep your AI from turn into a racist monster 




As 
ai-bas system take on more complex tasks, the stake will becom 
higher. yatskar describ a futur robot that when unsur of what 
someon be do in the kitchen offer a man a beer and a woman help 
wash dishes. "A system that take action that can be clearli 
attribut to gender bia cannot effect function with people," he 
says. 
tech 
compani have come to lean heavili on softwar that learn from pile 
of data, after breakthrough in machin learn roughli five year ago. 
more recently, research have begin to show how techniqu consid 
cold and clinic can pick up unsavori biases. 
last summer, research from boston univers and microsoft show that softwar 
train on text collect from googl new reproduc gender bia 
well document in humans. when they ask softwar to complet the 
statement “man be to comput programm a woman be to x,” it replied, “homemaker.” 
the 
new studi show that gender bia be built into two big set of photos, 
releas to help softwar good understand the content of images. the 
research look at imsitu, creat by the univers of washington, and coco, 
initi coordin by microsoft, and now also cosponsor by 
facebook and startup mightyai. each collect contain more than 
100,000 imag of complex scene drawn from the web, label with 
descriptions. 







both 
dataset contain mani more imag of men than women, and the object 
and activ depict with differ gender show what the research 
call “significant” gender bias. In the coco dataset, kitchen object 
such a spoon and fork be strongli associ with women, while 
outdoor sport equip such a snowboard and tenni racket be 
strongli associ with men. 
when 
image-recognit softwar be “trained” by examin these datasets, 
the bia be amplified. A system train on the coco dataset associ 
men with keyboard and comput mous even more strongli than the dataset 
itself. 
the 
research devis a way to neutral thi amplif 
phenomenon—effect forc learn softwar to reflect it train 
data. but it requir a research to be look for bia in the first 
place, and to specifi what he or she want to correct. and the correct 
softwar still reflect the gender bia bake into the origin data. 
eric 
horvitz, director of microsoft research, say he hope other adopt 
such tool a they build softwar power by machin learning. the 
compani have an intern ethic committe dedic to keep AI in the 
company' product in line. “I and microsoft a a whole celebr 
effort identifi and address bia and gap in dataset and system 
creat out of them,” horvitz says. research and engin work 
with coco and other dataset should be look for sign of bia in 
their own work and others’ he says. 
away 
from computers, book and other educ materi for child 
often be tweak to show an ideal world, with equal number of men 
and woman construct workers, for example. horvitz say it may be 
worth consid a similar approach in some case for materi use to 
teach softwar about the world. “it’ a realli import question–when 
should we chang realiti to make our system perform in an aspir 
way?” he says. 
other 
studi bia in machin learn aren't so sure. If there realli be 
more male construct workers, image-recognit program should be 
allow to see that, say aylin caliskan, a research at princeton. 
step can be take afterward to measur and adjust ani bia if needed. 
“we risk lose essenti information,” she says. “the dataset need to 
reflect the real statist in the world.” 
one 
point of agreement in the field be that use machin learn to solv 
problem be more complic than mani peopl previous thought. “work 
like thi be correct the illus that algorithm can be blindli 
appli to solv problems,” say suresh venkatasubramanian, a professor 
at the univers of utah. 

relat video 




businesspresid barack obama on how artifici intellig will affect job 
wire 
guest editor presid barack obama, wire editor in chief scott dadich 
and mit media lab director joi ito discu how artifici intellig 
might up-end economi and how societi can adapt. 


#machin learn 
#artifici intellig 
#gender 






most popular 






sciencehow to watch the total solar eclips without glass 
author: rhett allainrhett allain 







sciencetot solar eclips 2017: follow live from coast to coast 
author: wire staffwir staff 











backchannelth solar eclips Is coming—here' exactli when it'll happen 
author: stephen wolframstephen wolfram 



more stori 









hide comment 


sponsor stori 
power By outbrain 
greencardorganization.comcheck if you be elig for a u. green card 


save smarter online1 trick you should use everi time you turn On your pc... 


genomeweb908 devic rais $20m 


mansion globaltak A peek At thi insan super yacht 


healthnewstips.todaywhi doctor will No longer prescrib metformin (watch) 


blinkist appno time To read? get through 4 book In 1 day with the blinkist app 




more busi 





busi 
defin 'hate speech' onlin Is imperfect art a much a scienc 
author: davey albadavey alba 









busi 
sorry, ban ‘killer robots’ just isn’t practic 
author: tom simonitetom simonit 








busi 
fcc pledg open -- just don't ask To see complaint 
author: klint finleyklint finley 









busi 
propos california law target sexual harass in VC 
author: nitasha tikunitasha tiku 








video 
instagram' ceo on free speech, ai, and internet addict 
author: nichola thompsonnichola thompson 










busi 
when govern rule by software, citizen are left in the dark 
author: tom simonitetom simonit 






We recommend 
power By outbrain wire staffour 12 favorit laptops, from macbook to chromebook 


scott rosenbergbitcoin make even smart peopl feel dumb | backchannel 


jack stewartth latest fli car concept seems—dar We say it—seri 


david pierceth new imac pro Is apple’ most bonker supercomput ever 


sponsoredinvest in the source: autom technolog are the next big thing 
googl 












get our newslett 
wired’ big stori deliv to your inbox. 

submit 





we'r on 
pinterest 
see what' inspir us. 
follow 













login 
subscrib 
advertis 
site map 
press center 
faq 
access help 
custom care 
contact Us 
securedrop 
t-shirt collect 
newslett 
wire staff 
job 
rss 

cnmn collect 
use of thi site constitut accept of our user agreement (effect 3/21/12) and privaci polici (effect 3/21/12). affili link policy. your california privaci rights. 
the materi on thi site may not be reproduced, distributed, 
transmitted, cach or otherwis used, except with the prior write permiss of condé nast. 






















