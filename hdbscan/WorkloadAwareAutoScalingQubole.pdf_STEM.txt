









































workload-awar 
auto-sc 
A new paradigm for 
big data workload 



white paper 

execut summari 
auto-sc have becom a common concept with 
the advent of the public cloud. It be one of the 
first techniqu that allow applic to exploit 
the elast of the cloud. howev - a the cloud 
gain popular and more complex applic 
move to the cloud – first gener auto-sc 
technolog fell behind in serv the requir 
of such applications. 

In thi document we describ workload-awar 
auto-scaling. thi be an altern architectur 
approach to auto-sc that be good suit for 
new class of applic like hadoop, spark 
and presto that have now becom commonplac 
in the cloud. We show that tradit auto-sc 
technolog be ill-suit for big data applic 
and that workload-awar auto-sc technolog 
such a that offer by qubol be vastli superior. 
these technolog result in signific benefit to 
reliability, cost and respons for big data 
applications. 

http://aws.amazon.com/autosc 


white paper 

auto-sc – a short histori 
aw introduc auto-sc group in 20091. In it introduction, the blog notes: 

auto-sc let you defin scale polici driven by metric collect by amazon cloudwatch. your amazon 
ec2 instanc will scale automat base on actual system load and perform but you won’t be spend 
money to keep idl instanc running. 

auto-sc defin in thi manner be larg target for stateless applic – like web server – where the 
state be store on extern databas & caches. real-tim metric like cpu and memori util be use by 
applic to dynam add or remov node – a show in the figur below: 

simpl strategi like thi work fairli well for web applications. some salient characterist of web applic be 
relev to the way these auto-sc system be designed: 

• they be stateless 

• everi applic request (say a http request) be usual veri short live 

• applic workload be driven by extern client and not know in advanc 

• usual applic want to minim respons latenc (a oppos to optim cost) 

• all node be usual symmetr from the point of view of cpu/memori usag 

• An applic (host on a singl auto-sc group) be homogen 

• applic workload chang be often smooth (say increas gradual dure work hour and declin 
thereafter) 

2 

figur 1: auto-sc use cpu util in aw 



white paper 

A. aw emr2 

hortonwork hdp-aws3 

figur 2: auto-sc use memori util in aw emr 

figur 3: configur auto-sc in hortonwork data cloud 

3 

rule name 

add 

If 

Is 

for 

cooldown period 

myscalingrul 

1 

yarnmemoryavailablepercentag 

less than or equal to 15 percent 

1 

1 

�ve-minut period 

�ve-minut period 

instanc 

rule name 

scale 
adjust 

evalu 
period 

cooldown 
comparison 

oper 

threshold 

cloudwatch 
metric 

enter big-data 
As aw be introduc auto-sc group in 2009 – big data be just come into be – with hadoop and late 
spark and presto becom commonli use to wrangl with larg data sets. the public cloud be a match make in 
heaven for big data. larg data set be much more easili store in cloud storag system like S3 – and larg scale 
and bursti comput requir of big data applic be ideal suit for the larg and elast pool of 
comput resourc avail in these clouds. 

the auto-sc polici describ abov be easi to comprehend and it be not surpris that the same architectur get 
co-opt for run big data workloads. We see a similar approach in commerci cloud hadoop offer (aw emr 
and hortonworks) for scale a hadoop cluster in 2017 a we saw for scale web applic in 2009. 



white paper 

hadoop be not a web server! 
big data workload be a complet contrast to standard applications. A singl cluster (the rough equival of an 
auto-sc group) be submit multipl simultan discret applications. each of these applic can compris 
up to hundr of thousand of tasks. some of the differ characterist of these applic be a follows: 

4 

state servers: most big data applic store state on each node while they be running. 
remov node without account for thi state can caus workload and even the entir 
cluster to fail. the variou kind of state that can be store in each node include: 

• data belong to hdf 

• data belong to a distribut cach (like rdd in spark) 

• intermedi data produc by task that be need by subsequ task in the 
applic (for example: map output in map-reduc parlance) 

non-uniform 
server load: 

node in a big data cluster often have wide vari load factors. 
some node have more memori intens task and some have more cpu intens ones. 
run task can be of wide vari time durations. the amount of data store on each 
node can also vari wide depend on what applic it have be part of. 

long run 
requests: 

individu task compris a big data applic can run for hours. 
some task (like reducers) run for long time gather data from previou stage – or they can 
be long run simpli becaus they be process too much data (say due to skews). 

workload 
awareness: 

profil of big data applic run in a cluster be know up-front. 
unlik web applic where the request be gener from extern client – in a big 
data cluster – task unit be gener by an applic that be submit to a coordin 
daemon4 in the cluster. As such the characterist of the applic - the number of task it 
will gener (or even control over the same), the data it will read and the comput it will 
perform on it – be all know to the coordinator. 

In mani case – applic be repeated. for exampl the same report queri may run 
frequently, or the same etl job may run period in the night. thi further help a smart 
coordin anticip in advanc the natur of the workload submit to it. 



util 
sensitive: 

workload 
burstiness: 

big-data workload be usual veri cost sensitive. 
A big subset be the etl applic that care more about reliabl and cost (which translat 
into a desir for high cluster utilization). anoth big subset be ad-hoc queri and analysi that 
be latenc sensit (but be also somewhat cost sensitive). 

A big data cluster can go from idl to run 100k task in an instant. 
thi be contrast to usual web traffic where traffic usual go up rel smoothli even in the 
bad of days. 

white paper 

5 



white paper 

all these complet upturn the assumpt that underli old world auto-sc technolog and make it a veri poor 
fit. consid these immedi observations: 

• remov node while downscal be hard: both becaus of long run task a well a accumul state. 

• downscal algorithm need to pick node carefully: node be no longer uniformli load – neither do 
they have equal amount of applic state. 

• same auto-sc polici cannot be appli to all workloads: some workload want low latency, some high 
utilization. some may have sla constraint and some may have budget caps. 

• usag of pre-empt nodes(lik aw ec2 spot instances) be hard: primarili becaus node be stateful. 
pre-empt node can even lead to cluster failure. At the same time – usag of pre-empt resourc becom 
extrem import to reduc cost – particularli for etl workloads. 

• cluster scale have to take applic characterist into account: a the most trivial exampl - one cannot 
repeatedli upscal by a small step function to satisfi a 100k task application. that may take a veri long time. 

6 

these differ can be summar thus: 

auto-sc type 

applic characterist 

state server 

uniform load on server 

long run request 

latenc sensit high variabl 

util sensit low high 

workload bursti moder extrem high 

workload awar 

standard applic big data applic 

tabl 1: standard versu big-data applic 



when we start build auto-sc technolog at qubole, we evalu and rejected4 exist approach to auto- 
scale a be insuffici for build a truli cloud-n big data solution. instead we built auto-sc into hadoop 
and spark where it have access to the detail of the big data applic and the detail state of the cluster nodes. 

be workload awar have make a dramat differ to our abil to orchestr hadoop and spark in the cloud. the 
differ way in which we have use thi awar include: 

white paper 

workload awar auto-sc 

7 

upscaling: qubol manag cluster look at a varieti of criterion - beyond resourc util - to come up 
with upscal decisions. some examples: 

• parallelism-aware: If applic have limit parallel (say a job can onli use 10 
cores) - then upscal will not scale the cluster beyond that number (even though the 
cluster may exhibit high resourc utilization) 

• sla-aware: qubol monitor job for estim complet time and add comput 
resourc if they can help meet sla. If a job can be predict to complet in it requir 
sla then no upscal be trigger on it behalf (even though resourc util may 
be high). A larg job with thousand of task will trigger a much strong upscal 
respons than a small job. 

• workload awar scale limits5: If an applic be limit in the number of cpu 
resourc it can use (say becaus of limit put in by the administrator) - then it will not 
trigger upscal if it be alreadi use the maximum resourc allowed. 

• recommissioning: ani upscal requir be first attempt to be fulfil use 
ani node that be current in the process of grace downscaling. 

furthermor a composit cluster upscal decis be take depend on the requir 
of each of the job run in the cluster. 



white paper 

8 

downscaling: • smart victim selection: task run on each node and the amount of state on each 
node be consid in addit to the node launch time to determin which node be 
safe and optim to remov from the cluster when down-scaling. 

• grace downscaling: all state from a node be copi elsewher befor remov it from 
the cluster. thi includ hdf decommiss and log archiv – and in case of forc 
downscal – also involv offload intermedi data to cloud storage. 

• contain packing6 : schedul algorithm insid yarn be modifi to pack task into a 
small set of node that allow more node to be avail for downscaling. 

composit health 
checks: 

spot loss 
notification: 

spot 
rebalancing7: 

We period check run node in a cluster against their membership and health statu in 
hdf (distribut storag system) and yarn (distribut comput system). node that don’t 
pa such composit health check be automat remov from the cluster. 

yarn base hadoop and spark cluster in qubol be abl to deal with spot loss 
notif provid by aws. the cluster manag softwar immedi shut off task 
schedul on such nodes, stop further hdf write to such nodes, backs-up contain log 
on these node and tri it best to replic ani state left on such node to other surviv 
node in the system 

We be not onli abl to downscal node that be free - but abl to evalu which node 
have the most accumul state/task and may be the easi to retire. In most case we 
can even estim the amount of time requir to retir a node. thi sophist allow 
u to build featur like spot rebalanc where a cluster with excess on-demand node can 
retir them when it find that spot node have becom avail in the spot market. 



white paper 

cloud-awar workload manag 

9 

just like auto-sc technolog benefit enorm by be workload-awar – the dual be also true. workload 
manag technolog insid big data engines– like hadoop, spark and presto – benefit enorm from be 
cloud aware. A few exampl be in order: 

spot awareness8: 

task estimation: 

heterogen 
clusters9: 

hdf and job schedul in qubole’ hadoop/spark/presto cluster be awar of which node 
be preemptibl spot node (and henc unreliable) and which node be regular ones. thi 
knowledg allow u care placement of data and task to allow applic to run reliabl in 
the presenc of spot losses: 

• hdf data block are, by default, replic to spot and on-demand node 

• import task - like applic master and qubol control job (shell commands) 
be not place on spot node (and thi limit be factor into auto-sc logic) 

A key step in all big data technolog be divid process into small chunk that can be 
perform in parallel. the maximum comput resourc avail to an applic can be 
use to dynam comput such parallel (thi be now dynam and configur where it 
be previous static). 

In heterogen cluster –ani one of differ type of node can be chosen for upscaling. 
the knowledg of workload requir at ani instant can allow the cluster manag 
softwar to choos the right instanc for cluster upscal or downscaling. moreov the 
knowledg of differ heterogen instanc type can be use to automat come up 
with optim configur for a specif job. 



white paper 

the tabl below summar the abov technolog differ between tradit and workload-awar auto-sc 
technologies: 

auto-sc type 

featur 

load monitor 

simpl health check 

parallel awar 

sla awar 
upscal 

downscal 

spot node 

recommiss 

workload specif scale limit 

smart victim select 

grace downscal 

contain pack 

composit health check 

(spot) node rebalanc 

(spot) node loss notif handl 

spot awar schedul 

heterogen cluster 

featur group 
tradit workload-awar 

tabl 2: tradit versu workload-awar auto-sc 

10 



white paper 

conclus 
We have show comprehens how the natur of big data applic differ substanti from simpl onlin 
applic like web servers. To truli take advantag of the cloud – one have to integr auto-sc deep into the big 
data stack so that it be workload-aware. A true cloud nativ implement also make the big data stack awar of the 
cloud resourc and help it adapt workload and data manag in respons to it. 

the describ technolog be already, or soon plan to be, part of the qubol big data platform offering. 

11 

appendix – qubol tco save in practic 
the workload awar auto-sc white paper have describ whi gener approach to auto-sc be ineffici and 
costli for big data use case in the cloud. qubol have pioneer workload awar auto-sc for big data over the last 
sever year and deliv the technolog into a gener avail product servic in 2017. By work with over 
200 big data custom of all size and in multipl industries, we have also be abl to construct model that quantifi 
the financi impact of util workload-awar auto-sc in real life environments. thi appendix round out the 
technolog paper with the financi insights. 

first, the cost of ownership save of use qubol a a data platform in the cloud be 80% overal a measur 
in typic custom environ whether the comparison be to cloud or on-premis big data! qubol 
custom have save $140m in cost sinc 2016 (without count our larg custom who could distort the 
save upwards). the cost save measur primarili deriv from 3 autom effici qubol bring to bear 
with autom agent technology. 100% of the save be not due to workload awar auto-sc (55% are), but 
100% of the save document here across more than 200 custom be avail to ani busi use the qubol 
service. the 3 primari driver of cost save are: 

1. cluster life cycl manag (clcm) – amount save by automat termin a cluster when it be inact 
vs. allow it to continu to run at a minimum capac in the absenc of clcm. 

2. workload-awar auto-sc – amount save by predict adjust the number of node to meet demand 
vs. allow cluster to run at full capac for the durat of the instanc in the absenc of the agent. 

3. spot shopper save – amount save by use spot instanc at an assum 80% discount over on-demand 
instanc price thank to the qubol agent. 

save by qubol autom featur ($140m in comput costs) 



white paper 

about qubol 
qubol be passion about make data-driven insight easili access to anyone. qubol custom current process nearli an exabyt of data everi month, 
make u the lead cloud-agnost big-data-as-a-servic provider. custom have chosen qubol becaus we creat the industry’ first autonom data 
platform. thi cloud-bas data platform self-manages, self-optim and learn to improv automat and a a result deliv unbeat agility, flexibility, and 
tco. qubol custom focu on their data, not their data platform. qubol investor includ crv, lightspe ventur partners, norwest ventur partner and 
ivp. for more inform visit www.qubole.com 

for more information: 

contact: tri qd for free: 
sales@qubole.com https://www.qubole.com/products/pricing/ 

469 El camino real, suit 205 
santa clara, CA 95050 

(855) 423-6674 | info@qubole.com 

www.qubole.com 

end note 

page 2 - 1 new featur for amazon ec2: elast load balancing, auto scaling, and amazon cloudwatch - jeff barr, aw blog, may 18, 2009 

page 3 - 2 use automat scale in amazon emr - aw emr document 

page 3 3 ec2 spot note - aw emr document 

page 7 4auto-sc hortonwork data cloud - hdp aw document 

page 7 5 industry’ first auto-sc hadoop cluster - joydeep sen sarma qubol blog, jun 17, 2012 

page 8 6 hdf decommiss - apach hadoop 2.7.2 document 

page 8 7 rebalanc hadoop cluster for higher spot util - hariharan iyer, qubol blog, jun 9, 2015 

page 9 8 ride the spot eleph - mayank ahuja, qubol blog, No 12, 2015 

page 9 9 qubol announc heterogen cluster on aw - hariharan iyer, qubol blog, nov 30, 2016 

spot shopper, 
$25.12 (18%) 

cluster 
lifecycl 

management, 
$35.83 (26%) 

workload awar 
autoscaling, 
$77.73 (56%) 

https://www.qubole.com/products/pricing/ 
https://aws.amazon.com/blogs/aws/new-aws-load-balancing-automatic-scaling-and-cloud-monitoring-services/ 
http://docs.aws.amazon.com/emr/latest/managementguide/emr-automatic-scaling.html 
https://aws.amazon.com/ec2/spot/ 
https://hortonworks.github.io/hdp-aws/auto-scaling/index.html 
https://www.qubole.com/blog/industrys-first-auto-scaling-hadoop-clusters/ 
https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/federation.html#decommiss 
https://www.qubole.com/blog/rebalancing-hadoop-higher-spot-utilization/ 
https://www.qubole.com/blog/riding-the-spotted-elephant/ 
https://www.qubole.com/blog/qubole-announces-heterogeneous-clusters-on-aws-reduce-costs-up-to-90-with-spot-fleet/ 

