


















































match network for one shot learn 

oriol vinyal 
googl deepmind 

vinyals@google.com 

charl blundel 
googl deepmind 

cblundell@google.com 

timothi lillicrap 
googl deepmind 

countzero@google.com 

koray kavukcuoglu 
googl deepmind 

korayk@google.com 

daan wierstra 
googl deepmind 

wierstra@google.com 

abstract 

learn from a few exampl remain a key challeng in machin learning. 
despit recent advanc in import domain such a vision and language, the 
standard supervis deep learn paradigm do not offer a satisfactori solut 
for learn new concept rapidli from littl data. In thi work, we employ idea 
from metric learn base on deep neural featur and from recent advanc 
that augment neural network with extern memories. our framework learn a 
network that map a small label support set and an unlabel exampl to it 
label, obviat the need for fine-tun to adapt to new class types. We then defin 
one-shot learn problem on vision (use omniglot, imagenet) and languag 
tasks. our algorithm improv one-shot accuraci on imagenet from 87.6% to 
93.2% and from 88.0% to 93.8% on omniglot compar to compet approaches. 
We also demonstr the use of the same model on languag model by 
introduc a one-shot task on the penn treebank. 

1 introduct 

human learn new concept with veri littl supervis – e.g. a child can gener the concept 
of “giraffe” from a singl pictur in a book – yet our best deep learn system need hundr or 
thousand of examples. thi motiv the set we be interest in: “one-shot” learning, which 
consist of learn a class from a singl label example. 

deep learn have make major advanc in area such a speech [7], vision [13] and languag [16], 
but be notori for requir larg datasets. data augment and regular techniqu allevi 
overfit in low data regimes, but do not solv it. furthermore, learn be still slow and base on 
larg datasets, requir mani weight updat use stochast gradient descent. this, in our view, be 
mostli due to the parametr aspect of the model, in which train exampl need to be slowli learnt 
by the model into it parameters. 

In contrast, mani non-parametr model allow novel exampl to be rapidli assimilated, whilst not 
suffer from catastroph forgetting. some model in thi famili (e.g., near neighbors) do not 
requir ani train but perform depend on the chosen metric [1]. previou work on metric 
learn in non-parametr setup [18] have be influenti on our model, and we aim to incorpor 
the best characterist from both parametr and non-parametr model – namely, rapid acquisit 
of new exampl while provid excel generalis from common examples. 

the novelti of our work be twofold: at the model level, and at the train procedure. We propos 
match net (mn), a neural network which us recent advanc in attent and memori that 
enabl rapid learning. secondly, our train procedur be base on a simpl machin learn 
principle: test and train condit must match. thu to train our network to do rapid learning, we 

ar 
X 

iv 
:1 

60 
6. 

04 
08 

0v 
1 

[ 
c 

.L 
G 

] 
1 

3 
Ju 

n 
20 

16 



figur 1: match network architectur 

train it by show onli a few exampl per class, switch the task from minibatch to minibatch, 
much like how it will be test when present with a few exampl of a new task. 

besid our contribut in defin a model and train criterion amen for one-shot learning, 
we contribut by the definit of task that can be use to benchmark other approach on both 
imagenet and small scale languag modeling. We hope that our result will encourag other to work 
on thi challeng problem. 

We organ the paper by first defin and explain our model whilst link it sever compo- 
nent to relat work. then in the follow section we briefli elabor on some of the relat work 
to the task and our model. In section 4 we describ both our gener setup and the experi we 
performed, demonstr strong result on one-shot learn on a varieti of task and setups. 

2 model 

our non-parametr approach to solv one-shot learn be base on two compon which we 
describ in the follow subsections. first, our model architectur follow recent advanc in neural 
network augment with memori (a discuss in section 3). given a (small) support set S, our 
model defin a function cS (or classifier) for each S, i.e. a map S → cs(.). second, we employ 
a train strategi which be tailor for one-shot learn from the support set S. 

2.1 model architectur 

In recent years, mani group have investig way to augment neural network architectur with 
extern memori and other compon that make them more “computer-like”. We draw inspir 
from model such a sequenc to sequenc (seq2seq) with attent [2], memori network [29] and 
pointer network [27]. 

In all these models, a neural attent mechanism, often fulli differentiable, be defin to access (or 
read) a memori matrix which store use inform to solv the task at hand. typic us of 
thi includ machin translation, speech recognition, or question answering. more generally, these 
architectur model P (b|a) where A and/or B can be a sequenc (like in seq2seq models), or, more 
interestingli for us, a set [26]. 

our contribut be to cast the problem of one-shot learn within the set-to-set framework [26]. 
the key point be that when trained, match network be abl to produc sensibl test label for 
unobserv class without ani chang to the network. more precisely, we wish to map from a 
(small) support set of k exampl of image-label pair S = {(xi, yi)}ki=1 to a classifi cs(x̂) which, 
give a test exampl x̂, defin a probabl distribut over output ŷ. We defin the map 
S → cs(x̂) to be P (ŷ|x̂, S) where P be parameteris by a neural network. thus, when give a 

2 



new support set of exampl S′ from which to one-shot learn, we simpli use the parametr neural 
network defin by P to make predict about the appropri label ŷ for each test exampl x̂: 
P (ŷ|x̂, s′). In general, our predict output class for a give input unseen exampl x̂ and a support 
set S becom argmaxi P (y|x̂, s). 
our model in it simplest form comput ŷ a follows: 

ŷ = 

k∑ 
i=1 

a(x̂, xi)yi (1) 

where xi, yi be the sampl and label from the support set S = {(xi, yi)}ki=1, and a be an attent 
mechan which we discu below. note that eq. 1 essenti describ the output for a new class a 
a linear combin of the label in the support set. where the attent mechan a be a kernel on 
X ×X , then (1) be akin to a kernel densiti estimator. where the attent mechan be zero for the 
b furthest xi from x̂ accord to some distanc metric and an appropri constant otherwise, then 
(1) be equival to ‘k − b’-nearest neighbour (although thi requir an extens to the attent 
mechan that we describ in section 2.1.2). thu (1) subsum both kde and knn methods. 
anoth view of (1) be where a act a an attent mechan and the yi act a memori bound to 
the correspond xi. In thi case we can understand thi a a particular kind of associ memori 
where, give an input, we “point” to the correspond exampl in the support set, retriev it label. 
however, unlik other attent memori mechan [2], (1) be non-parametr in nature: a the 
support set size grows, so do the memori used. henc the function form defin by the classifi 
cs(x̂) be veri flexibl and can adapt easili to ani new support set. 

2.1.1 the attent kernel 

equat 1 reli on choos a(., .), the attent mechanism, which fulli specifi the classi- 
fier. the simplest form that thi take (and which have veri tight relationship with common 
attent model and kernel functions) be to use the softmax over the cosin distanc c, i.e., 
a(x̂, xi) = e 

c(f(x̂),g(xi))/ 
∑k 
j=1 e 

c(f(x̂),g(xj)) with emb function f and g be appropri- 
ate neural network (potenti with f = g) to emb x̂ and xi. In our experi we shall see 
exampl where f and g be parameteris various a deep convolut network for imag 
task (a in vgg[22] or inception[24]) or a simpl form word emb for languag task (see 
section 4). 

We note that, though relat to metric learning, the classifi defin by equat 1 be discriminative. 
for a give support set S and sampl to classifi x̂, it be enough for x̂ to be suffici align with 
pair (x′, y′) ∈ S such that y′ = y and misalign with the rest. thi kind of loss be also relat to 
method such a neighborhood compon analysi (nca) [18], triplet loss [9] or larg margin 
near neighbor [28]. 

however, the object that we be tri to optim be precis align with multi-way, one-shot 
classification, and thu we expect it to perform good than it counterparts. additionally, the loss be 
simpl and differenti so that one can find the optim paramet in an “end-to-end” fashion. 

2.1.2 full context embed 

the main novelti of our model lie in reinterpret a well studi framework (neural network with 
extern memories) to do one-shot learning. close relat to metric learning, the emb func- 
tion f and g act a a lift to featur space X to achiev maximum accuraci through the classif 
function describ in eq. 1. 

despit the fact that the classif strategi be fulli condit on the whole support set through 
P (.|x̂, s), the embed on which we appli the cosin similar to “attend”, “point” or simpli 
comput the near neighbor be myopic in the sens that each element xi get emb by g(xi) 
independ of other element in the support set S. furthermore, S should be abl to modifi how 
we emb the test imag x̂ through f . 

We propos emb the element of the set through a function which take a input the full set 
S in addit to xi, i.e. g becom g(xi, s). thus, a a function of the whole support set S, g can 
modifi how to emb xi. thi could be use when some element xj be veri close to xi, in which 

3 



case it may be benefici to chang the function with which we emb xi – some evid of thi 
be discuss in section 4. We use a bidirect long-short term memori (lstm) [8] to encod 
xi in the context of the support set S, consid a a sequenc (see appendix for a more precis 
definition). 

the second issu can be fix via an lstm with read-attent over the whole set S, whose input 
be equal to x: 

f(x̂, S) = attlstm(f ′(x̂), g(s),k) 
where f ′(x̂) be the featur (e.g., deriv from a cnn) which be input to the lstm (constant at 
each time step). K be the fix number of unrol step of the lstm, and g(s) be the set over which 
we attend, emb with g. thi allow for the model to potenti ignor some element in the 
support set S, and add “depth” to the comput of attent (see appendix for more details). 

2.2 train strategi 

In the previou subsect we describ match network which map a support set to a classif 
function, S → c(x̂). We achiev thi via a modif of the set-to-set paradigm augment with 
attention, with the result map be of the form pθ(.|x̂, s), note that θ be the paramet 
of the model (i.e. of the emb function f and g describ previously). 

the train procedur have to be chosen care so a to match infer at test time. our model 
have to perform well with support set S′ which contain class never see dure training. 

more specifically, let u defin a task T a distribut over possibl label set L. typic we 
consid T to uniformli weight all data set of up to a few uniqu class (e.g., 5), with a few 
exampl per class (e.g., up to 5). In thi case, a label set L sampl from a task T , L ∼ T , will 
typic have 5 to 25 examples. 

To form an “episode” to comput gradient and updat our model, we first sampl L from T (e.g., 
L could be the label set {cats, dogs}). We then use L to sampl the support set S and a batch B 
(i.e., both S and B be label exampl of cat and dogs). the match net be then train to 
minimis the error predict the label in the batch B condit on the support set S. thi be a 
form of meta-learn sinc the train procedur explicitli learn to learn from a give support set 
to minimis a loss over a batch. more precisely, the match net train object be a follows: 

θ = argmax 
θ 
el∼t 

es∼l,b∼l 
 ∑ 
(x,y)∈b 

logpθ (y|x, S) 

 . (2) 
train θ with eq. 2 yield a model which work well when sampl S′ ∼ T ′ from a differ 
distribut of novel labels. crucially, our model do not need ani fine tune on the class it have 
never see due to it non-parametr nature. obviously, a T ′ diverg far from the T from which we 
sampl to learn θ, the model will not work – we belabor thi point further in section 4.1.2. 

3 relat work 

3.1 memori augment neural network 

A recent surg of model which go beyond “static” classif of fix vector onto their class 
have reshap current research and industri applic alike. thi be most notabl in the massiv 
adopt of lstm [8] in a varieti of task such a speech [7], translat [23, 2] or learn program 
[4, 27]. A key compon which allow for more express model be the introduct of “content” 
base attent in [2], and “computer-like” architectur such a the neural ture machin [4] or 
memori network [29]. our work take the metalearn paradigm of [21], where an lstm learnt 
to learn quickli from data present sequentially, but we treat the data a a set. the one-shot learn 
task we defin on the penn treebank [15] relat to evalu techniqu and model present in 
[6], and we discu thi in section 4. 

3.2 metric learn 

As discuss in section 2, there be mani link between content base attention, kernel base near 
neighbor and metric learn [1]. the most relev work be neighborhood compon analysi 

4 



(nca) [18], and the follow up non-linear version [20]. the loss be veri similar to ours, except we 
use the whole support set S instead of pair-wis comparison which be more amen to one-shot 
learning. follow-up work in the form of deep convolut siames [11] network includ much 
more power non-linear mappings. other loss which includ the notion of a set (but use less 
power metrics) be propos in [28]. 

lastly, the work in one-shot learn in [14] be inspir and also provid u with the invalu 
omniglot dataset – refer to a the “transpose” of mnist. other work use zero-shot learn on 
imagenet, e.g. [17]. however, there be not much one-shot literatur on imagenet, which we hope to 
amend via our benchmark and task definit in the follow section. 

4 experi 

In thi section we describ the result of mani experiments, compar our match network 
model against strong baselines. all of our experi revolv around the same basic task: an N -way 
k-shot learn task. each method be provid with a set of k label exampl from each of N 
class that have not previous be train upon. the task be then to classifi a disjoint batch of 
unlabel exampl into one of these N classes. thu random perform on thi task stand at 
1/n . We compar a number of altern models, a baselines, to match networks. 

let u introduc some notation. L′ denot the held-out subset of label which we onli use for 
one-shot. thus, unless otherwis specified, train be alway on 6=l′, and test in one-shot mode on 
l′. 

We ran one-shot experi on three data sets: two imag classif set (omniglot [14] and 
imagenet [19, ilsvrc-2012]) and one languag model (penn treebank). the experi on 
the three data set compris a divers set of qualiti in term of complexity, sizes, and modalities. 

4.1 imag classif result 

for vision problems, we consid four kind of baselines: match on raw pixels, match on 
discrimin featur from a state-of-the-art classifi (baselin classifier), mann [21], and our 
reimplement of the convolut siames net [11]. the baselin classifi be train to 
classifi an imag into one of the origin class present in the train data set, but exclud the 
N class so a not to give it an unfair advantag (i.e., train to classifi class in 6=l′). We then 
take thi network and use the featur from the last layer (befor the softmax) for near neighbour 
matching, a strategi commonli use in comput vision [3] which have achiev excel result 
across mani tasks. follow [11], the convolut siames net be train on a same-or-differ 
task of the origin train data set and then the last layer be use for near neighbour matching. 

We also tri further fine tune the featur use onli the support set S′ sampl from l′. thi 
yield massiv overfitting, but give that our network be highli regularized, can yield extra gains. 
note that, even when fine tuning, the setup be still one-shot, a onli a singl exampl per class from 
L′ be used. 

model match Fn fine tune 5-way acc 20-way acc1-shot 5-shot 1-shot 5-shot 

pixel cosin N 41.7% 63.2% 26.7% 42.6% 
baselin classifi cosin N 80.0% 95.0% 69.5% 89.1% 
baselin classifi cosin Y 82.3% 98.4% 70.6% 92.0% 
baselin classifi softmax Y 86.0% 97.6% 72.9% 92.3% 

mann (no conv) [21] cosin N 82.8% 94.9% – – 
convolut siames net [11] cosin N 96.7% 98.4% 88.0% 96.5% 
convolut siames net [11] cosin Y 97.3% 98.4% 88.1% 97.0% 

match net (ours) cosin N 98.1% 98.9% 93.8% 98.5% 
match net (ours) cosin Y 97.9% 98.7% 93.5% 98.7% 

tabl 1: result on the omniglot dataset. 

5 



S’ 

matchnet 

incept 

figur 2: exampl of two 5-way problem instanc on imagenet. the imag in the set S′ contain 
class never see dure training. our model make far less mistak than the incept baseline. 

4.1.1 omniglot 

omniglot [14] consist of 1623 charact from 50 differ alphabets. each of these be hand drawn 
by 20 differ people. the larg number of class (characters) with rel few data per class 
(20), make thi an ideal data set for test small-scal one-shot classification. the N -way omniglot 
task setup be a follows: pick N unseen charact classes, independ of alphabet, a L. provid 
the model with one draw of each of the N charact a S ∼ L and a batch B ∼ L. follow 
[21], we augment the data set with random rotat by multipl of 90 degre and use 1200 
charact for training, and the remain charact class for evaluation. 

We use a simpl yet power cnn a the emb function – consist of a stack of modules, 
each of which be a 3 × 3 convolut with 64 filter follow by batch normal [10], a relu 
non-linear and 2× 2 max-pooling. We resiz all the imag to 28× 28 so that, when we stack 4 
modules, the result featur map be 1× 1× 64, result in our emb function f(x). A fulli 
connect layer follow by a softmax non-linear be use to defin the baselin classifier. 

result compar the baselin to our model on omniglot be show in tabl 1. for both 1-shot 
and 5-shot, 5-way and 20-way, our model outperform the baselines. there be no major surpris in 
these results: use more exampl for k-shot classif help all models, and 5-way be easi than 
20-way. We note that the baselin classifi improv a bit when fine tune on s′, and use cosin 
distanc versu train a small softmax from the small train set (thu requir fine tuning) also 
perform well. siames net fare well versu our match net when use 5 exampl per class, 
but their perform degrad rapidli in one-shot. fulli condit embed (fce) do not 
seem to help much and be left out of the tabl due to space constraints. 

like the author in [11], we also test our method train on omniglot on a complet disjoint task – 
one-shot, 10 way mnist classification. the baselin classifi do about 63% accuraci wherea 
(a report in their paper) the siames net do 70%. our model achiev 72%. 

4.1.2 imagenet 

our experi follow the same setup a omniglot for testing, but we consid a rand and a 
dog (harder) setup. In the rand setup, we remov 118 label at random from the train set, then 
test onli on these 118 class (which we denot a lrand). for the dog setup, we remov all 
class in imagenet descend from dog (total 118) and train on all non-dog classes, then 
test on dog class (ldogs). imagenet be a notori larg data set which can be quit a feat of 
engin and infrastructur to run experi upon it, requir mani resources. thus, a well a 
use the full imagenet data set, we devis a new data set – miniimagenet – consist of 60, 000 
colour imag of size 84 × 84 with 100 classes, each have 600 examples. thi dataset be more 
complex than cifar10 [12], but fit in memori on modern machines, make it veri conveni for 
rapid prototyp and experimentation. We use 80 class for train and test on the remain 
20 classes. In total, thus, we have randimagenet, dogsimagenet, and miniimagenet. 

the result of the miniimagenet experi be show in tabl 2. As with omniglot, match 
network outperform the baselines. however, miniimagenet be a much harder task than omniglot 
which allow u to evalu full contextu embed (fce) sensibl (on omniglot it make no 
difference). As we an see, fce improv the perform of match networks, with and without 
fine tuning, typic improv perform by around two percentag points. 

6 



tabl 2: result on miniimagenet. 

model match Fn fine tune 5-way acc1-shot 5-shot 

pixel cosin N 23.0% 26.6% 
baselin classifi cosin N 36.6% 46.0% 
baselin classifi cosin Y 36.2% 52.2% 
baselin classifi softmax Y 38.4% 51.2% 

match net (ours) cosin N 41.2% 56.2% 
match net (ours) cosin Y 42.4% 58.0% 
match net (ours) cosin (fce) N 44.2% 57.0% 
match net (ours) cosin (fce) Y 46.6% 60.0% 

tabl 3: result on full imagenet on rand and dog one-shot tasks. note that 6=lrand and 6=ldog 
be set of class which be see dure training, but be provid for completeness. 

model match Fn fine tune imagenet 5-way 1-shot acc 
lrand 6=lrand ldog 6=ldog 

pixel cosin N 42.0% 42.8% 41.4% 43.0% 
incept classifi cosin N 87.6% 92.6% 59.8% 90.0% 

match net (ours) cosin (fce) N 93.2% 97.0% 58.8% 96.4% 

incept oracl softmax (full) Y (full) ≈ 99% ≈ 99% ≈ 99% ≈ 99% 

next we turn to experi base upon full size, full scale imagenet. our baselin classifi for 
thi data set be incept [25] train to classifi on all class except those in the test set of class 
(for randimagenet) or those concern dog (for dogsimagenet). We also compar to featur from 
an incept oracl classifi train on all class in imagenet, a an upper bound. our baselin 
classifi be one of the strong publish imagenet model at 79% top-1 accuraci on the standard 
imagenet valid set. instead of train match network from scratch on these larg tasks, we 
initialis their featur extractor f and g with the paramet from the incept classifi (pretrain 
on the appropri subset of the data) and then further train the result network on random 5-way 
1-shot task from the train data set, incorpor full context embed and our match 
network and train strategy. 

the result of the randimagenet and dogsimagenet experi be show in tabl 3. the incept 
oracl (train on all classes) perform almost perfectli when restrict to 5 class only, which be 
not too surpris give it impress top-1 accuracy. when train sole on 6=lrand, match 
net improv upon incept by almost 6% when test on lrand, halv the errors. figur 2 show 
two instanc of 5-way one-shot learning, where incept fails. look at all the errors, incept 
appear to sometim prefer an imag abov all other (these imag tend to be clutter like the 
exampl in the second column, or more constant in color). match nets, on the other hand, manag 
to recov from these outlier that sometim appear in the support set s′. 

match net manag to improv upon incept on the complementari subset 6=ldog (although 
thi setup be not one-shot, a the featur extract have be train on these labels). however, on the 
much more challeng ldog subset, our model degrad by 1%. We hypothes thi to the fact 
that the sampl set dure training, S, come from a random distribut of label (from 6=ldogs), 
wherea the test support set S′ from ldog contain similar classes, more akin to fine grain 
classification. thus, we believ that if we adapt our train strategi to sampl S from fine grain 
set of label instead of sampl uniformli from the leaf of the imagenet class tree, improv 
could be attained. We leav thi a futur work. 

4.1.3 one-shot languag model 

We also introduc a new one-shot languag task which be analog to those examin for images. 
the task be a follows: give a queri sentenc with a miss word in it, and a support set of sentenc 
which each have a miss word and a correspond 1-hot label, choos the label from the support 

7 



set that best match the queri sentence. here we show a singl example, though note that the word 
on the right be not provid and the label for the set be give a 1-hot-of-5 vectors. 
1. an experiment vaccin can alter the immun respons of peopl infect with the aid viru a 
<blank_token> u.s. scientist said. 

promin 

2. the show one of five new nbc <blank_token> be the second casualti of the three network so far 
thi fall. 

seri 

3. howev sinc eastern first file for chapter N protect march N it have consist promis 
to pay creditor N cent on the <blank_token>. 

dollar 

4. we have a lot of peopl who threw in the <blank_token> today say <unk> elli a partner in 
benjamin jacobson & son a specialist in trade ual stock on the big board. 

towel 

5. it’ not easi to roll out someth that <blank_token> and make it pay mr. jacob says. comprehens 
query: in late new york trade yesterday the <blank_token> be quot at N mark down from N 
mark late friday and at N yen down from N yen late friday. 

dollar 

sentenc be take from the penn treebank dataset [15]. On each trial, we make sure that the set 
and batch be popul with sentenc that be non-overlapping. thi mean that we do not use 
word with veri low frequenc counts; e.g. if there be onli a singl sentenc for a give word we do 
not use thi data sinc the sentenc would need to be in both the set and the batch. As with the imag 
tasks, each trial consist of a 5 way choic between the class avail in the set. We use a batch 
size of 20 throughout the sentenc match task (smt) and vari the set size across k=1,2,3. We 
ensur that the same number of sentenc be avail for each class in the set. 

We split the word into a randomli sampl 9000 for train and 1000 for testing. As well, we 
train and test use sentenc take from the standard ptb train set and test with sentenc 
from the standard test set. thus, neither the word nor the sentenc use dure test time have be 
see dure training. 

We compar our one-shot match model to an oracl lstm languag model (lstm-lm) [30] 
train on all the words. In thi setup, the lstm have an unfair advantag a it be not do one-shot 
learn but see all the data – thus, thi should be take a an upper bound. To do so, we examin 
a similar setup wherein a sentenc be present to the model with a singl word fill in with 5 
differ possibl word (includ the correct answer). for each of these 5 sentenc the model give 
a log-likelihood and the max of these be take to be the choic of the model. 

As with the other 5 way choic tasks, chanc perform on thi task be 20%. the lstm languag 
model oracl achiev an upper bound of 72.8% accuraci on the test set. match network 
with a simpl encod model achiev 32.4%, 36.1%, 38.2% accuraci on the task with k = 1, 2, 3 
exampl in the set, respectively. futur work should explor combin parametr model such a 
an lstm-lm with non-parametr compon such a the match network explor here. 

two relat task be the cnn QA test of entiti predict from news articl [5], and the children’ 
book test (cbt) [6]. In the cbt for example, a sequenc of sentenc from a book be provid 
a context. In the final sentenc one of the words, which have appear in a previou sentence, be 
missing. the task be to choos the correct word to fill in thi blank from a small set of word give 
a possibl answers, all of which occur in the preced sentences. In our sentenc match task 
the sentenc provid in the set be randomli drawn from the ptb corpu and be relat to the 
sentenc in the queri batch onli by the fact that they share a word. In contrast to cbt and cnn 
dataset, they provid onli a gener rather than specif sequenti context. 

5 conclus 

In thi paper we introduc match networks, a new neural architectur that, by way of it 
correspond train regime, be capabl of state-of-the-art perform on a varieti of one-shot 
classif tasks. there be a few key insight in thi work. firstly, one-shot learn be much 
easi if you train the network to do one-shot learning. secondly, non-parametr structur in a 
neural network make it easi for network to rememb and adapt to new train set in the same 
tasks. combin these observ togeth yield match networks. further, we have defin 
new one-shot task on imagenet, a reduc version of imagenet (for rapid experimentation), and a 
languag model task. An obviou drawback of our model be the fact that, a the support set S grow 
in size, the comput for each gradient updat becom more expensive. although there be spars 
and sampling-bas method to allevi this, much of our futur effort will concentr around thi 
limitation. further, a exemplifi in the imagenet dog subtask, when the label distribut have 
obviou bia (such a be fine grained), our model suffers. We feel thi be an area with excit 
challeng which we hope to keep improv in futur work. 

8 



acknowledg 

We would like to thank nal kalchbrenn for brainstorm around the design of the function g, and 
sander dieleman and sergio guadarrama for their help set up imagenet. We would also like 
thank simon osindero for use discuss around the task discuss in thi paper, and theophan 
weber and remi muno for follow some earli developments. karen simonyan and david silver 
help with the manuscript, a well a mani at googl deepmind. thank also to geoff hinton and 
alex toshev for discuss about our results. 

refer 
[1] C atkeson, A moore, and S schaal. local weight learning. artifici intellig review, 1997. 
[2] D bahdanau, K cho, and Y bengio. neural machin translat by jointli learn to align and translate. 

iclr, 2014. 
[3] J donahue, Y jia, O vinyals, J hoffman, N zhang, E tzeng, and T darrell. decaf: A deep convolut 

activ featur for gener visual recognition. In icml, 2014. 
[4] A graves, G wayne, and I danihelka. neural ture machines. arxiv preprint arxiv:1410.5401, 2014. 
[5] K hermann, T kocisky, E grefenstette, L espeholt, W kay, M suleyman, and P blunsom. teach 

machin to read and comprehend. In nips, 2015. 
[6] F hill, A bordes, S chopra, and J weston. the goldilock principle: read children’ book with explicit 

memori representations. arxiv preprint arxiv:1511.02301, 2015. 
[7] G hinton et al. deep neural network for acoust model in speech recognition: the share view of 

four research groups. signal process magazine, ieee, 2012. 
[8] S hochreit and J schmidhuber. long short-term memory. neural computation, 1997. 
[9] E hoffer and N ailon. deep metric learn use triplet network. similarity-bas pattern recognition, 

2015. 
[10] S ioff and C szegedy. batch normalization: acceler deep network train by reduc intern 

covari shift. arxiv preprint arxiv:1502.03167, 2015. 
[11] G koch, R zemel, and R salakhutdinov. siames neural network for one-shot imag recognition. In 

icml deep learn workshop, 2015. 
[12] A krizhevski and G hinton. convolut deep belief network on cifar-10. unpublished, 2010. 
[13] A krizhevsky, I sutskever, and G hinton. imagenet classif with deep convolut neural networks. 

In nips, 2012. 
[14] BM lake, R salakhutdinov, J gross, and J tenenbaum. one shot learn of simpl visual concepts. In 

cogsci, 2011. 
[15] MP marcus, MA marcinkiewicz, and B santorini. build a larg annot corpu of english: the penn 

treebank. comput linguistics, 1993. 
[16] T mikolov, M karafiát, L burget, J cernockỳ, and S khudanpur. recurr neural network base languag 

model. In interspeech, 2010. 
[17] M norouzi, T mikolov, S bengio, Y singer, J shlens, A frome, G corrado, and J dean. zero-shot learn 

by convex combin of semant embeddings. arxiv preprint arxiv:1312.5650, 2013. 
[18] S roweis, G hinton, and R salakhutdinov. neighbourhood compon analysis. nips, 2004. 
[19] O russakovsky, J deng, H su, J krause, S satheesh, S ma, Z huang, A karpathy, A khosla, M bernstein, 

A berg, and L fei-fei. imagenet larg scale visual recognit challenge. ijcv, 2015. 
[20] R salakhutdinov and G hinton. learn a nonlinear emb by preserv class neighbourhood 

structure. In aistats, 2007. 
[21] A santoro, S bartunov, M botvinick, D wierstra, and T lillicrap. meta-learn with memory-aug 

neural networks. In icml, 2016. 
[22] K simonyan and A zisserman. veri deep convolut network for large-scal imag recognition. arxiv 

preprint arxiv:1409.1556, 2014. 
[23] I sutskever, O vinyals, and QV le. sequenc to sequenc learn with neural networks. In nips, 2014. 
[24] C szegedy, W liu, Y jia, P sermanet, S reed, D anguelov, D erhan, V vanhoucke, and A rabinovich. 

go deeper with convolutions. In cvpr, 2015. 
[25] C szegedy, V vanhoucke, S ioffe, J shlens, and Z wojna. rethink the incept architectur for 

comput vision. arxiv preprint arxiv:1512.00567, 2015. 
[26] O vinyals, S bengio, and M kudlur. order matters: sequenc to sequenc for sets. arxiv preprint 

arxiv:1511.06391, 2015. 
[27] O vinyals, M fortunato, and N jaitly. pointer networks. In nips, 2015. 
[28] K weinberg and L saul. distanc metric learn for larg margin near neighbor classification. jmlr, 

2009. 
[29] J weston, S chopra, and A bordes. memori networks. iclr, 2014. 
[30] W zaremba, I sutskever, and O vinyals. recurr neural network regularization. arxiv preprint 

arxiv:1409.2329, 2014. 

9 



appendix 

A model descript 

In thi section we fulli specifi the model which condit the emb function f and g on the 
whole support set S. much previou work have fulli describ similar mechanisms, which be whi we 
left the precis detail for thi appendix. 

a.1 the fulli condit embed f 

As describ in section 2.1.2, the emb function for an exampl x̂ in the batch B be a follows: 

f(x̂, S) = attlstm(f ′(x̂), g(s),k) 
where f ′ be a neural network (e.g., vgg or inception, a describ in the main text). We defin K 
to be the number of “processing” step follow work from [26] from their “process” block. g(s) 
repres the emb function g appli to each element xi from the set S. 

thus, the state after k process step be a follows: 

ĥk, ck = lstm(f ′(x̂), [hk−1, rk−1], ck−1) (3) 

hk = ĥk + f 
′(x̂) (4) 

rk−1 = 

|s|∑ 
i=1 

a(hk−1, g(xi))g(xi) (5) 

a(hk−1, g(xi)) = softmax(htk−1g(xi)) (6) 

note that lstm(x, h, c) follow the same lstm implement defin in [23] with x the input, 
h the output (i.e., cell after the output gate), and c the cell. a be commonli refer to a “content” 
base attention, and the softmax in eq. 6 normal w.r.t. g(xi). the read-out rk−1 from g(s) be 
concaten to hk−1. sinc we do K step of “reads”, attlstm(f ′(x̂), g(s),k) = hK where hk 
be a describ in eq. 3. 

a.2 the fulli condit embed g 

In section 2.1.2 we describ the encod function for the element in the support set S, g(xi, s), 
a a bidirect lstm. more precisely, let g′(xi) be a neural network (similar to f ′ above, e.g. a 
vgg or incept model). then we defin g(xi, S) = ~hi + ~hi + g′(xi) with: 

~hi,~ci = lstm(g′(xi),~hi−1,~ci−1) 
~hi, ~ci = lstm(g′(xi), ~hi+1, ~ci+1) 

where, a in above, lstm(x, h, c) follow the same lstm implement defin in [23] with x 
the input, h the output (i.e., cell after the output gate), and c the cell. note that the recurs for ~h 
start from i = |s|. As in eq. 3, we add a skip connect between input and outputs. 

B imagenet class split 

here we defin the two class split use in our full imagenet experi – these class be 
exclud for train dure our one-shot experi describ in section 4.1.2. 

lrand = 
n01498041, n01537544, n01580077, n01592084, n01632777, n01644373, n01665541, n01675722, n01688243, n01729977, n01775062, 
n01818515, n01843383, n01883070, n01950731, n02002724, n02013706, n02092339, n02093256, n02095314, n02097130, n02097298, 

10 



n02098413, n02101388, n02106382, n02108089, n02110063, n02111129, n02111500, n02112350, n02115913, n02117135, n02120505, 
n02123045, n02125311, n02134084, n02167151, n02190166, n02206856, n02231487, n02256656, n02398521, n02480855, n02481823, 
n02490219, n02607072, n02666196, n02672831, n02704792, n02708093, n02814533, n02817516, n02840245, n02843684, n02870880, 
n02877765, n02966193, n03016953, n03017168, n03026506, n03047690, n03095699, n03134739, n03179701, n03255030, n03388183, 
n03394916, n03424325, n03467068, n03476684, n03483316, n03627232, n03658185, n03710193, n03721384, n03733131, n03785016, 
n03786901, n03792972, n03794056, n03832673, n03843555, n03877472, n03899768, n03930313, n03935335, n03954731, n03995372, 
n04004767, n04037443, n04065272, n04069434, n04090263, n04118538, n04120489, n04141975, n04152593, n04154565, n04204347, 
n04208210, n04209133, n04258138, n04311004, n04326547, n04367480, n04447861, n04483307, n04522168, n04548280, n04554684, 
n04597913, n04612504, n07695742, n07697313, n07697537, n07716906, n12998815, n13133613 

ldog = 

n02085620, n02085782, n02085936, n02086079, n02086240, n02086646, n02086910, n02087046, n02087394, n02088094, n02088238, 
n02088364, n02088466, n02088632, n02089078, n02089867, n02089973, n02090379, n02090622, n02090721, n02091032, n02091134, 
n02091244, n02091467, n02091635, n02091831, n02092002, n02092339, n02093256, n02093428, n02093647, n02093754, n02093859, 
n02093991, n02094114, n02094258, n02094433, n02095314, n02095570, n02095889, n02096051, n02096177, n02096294, n02096437, 
n02096585, n02097047, n02097130, n02097209, n02097298, n02097474, n02097658, n02098105, n02098286, n02098413, n02099267, 
n02099429, n02099601, n02099712, n02099849, n02100236, n02100583, n02100735, n02100877, n02101006, n02101388, n02101556, 
n02102040, n02102177, n02102318, n02102480, n02102973, n02104029, n02104365, n02105056, n02105162, n02105251, n02105412, 
n02105505, n02105641, n02105855, n02106030, n02106166, n02106382, n02106550, n02106662, n02107142, n02107312, n02107574, 
n02107683, n02107908, n02108000, n02108089, n02108422, n02108551, n02108915, n02109047, n02109525, n02109961, n02110063, 
n02110185, n02110341, n02110627, n02110806, n02110958, n02111129, n02111277, n02111500, n02111889, n02112018, n02112137, 
n02112350, n02112706, n02113023, n02113186, n02113624, n02113712, n02113799, n02113978 

C ptb class split 

here we defin the two class split use in our ptb experi – these class be exclud for 
train dure our one-shot languag experi describ in section 4.1.3. 

lrand = 

’s, 12-year, 190.58-point, 1930s, 26-week, a.c., abortion, absorbed, accelerating, acceptable, accords, accusations, achieve, acquires, actively, 
adapted, addition, adequate, admitting, adopt, adopted, adopting, advised, advisers, advises, advising, aer, affidavits, afternoon, ag, aged, ages, 
agreements, airport, akzo, alaska, alcohol, alert, alliance, allied-signal, ally, altman, ambrosiano, american, amgen, amount, amounts, an, andy, 
angry, animals, annuities, antitrust, anybody, anyway, appointed, approaching, approvals, arabs, arafat, arbitration, argentina, arranged, arrest, 
artists, assembled, associations, assume, assumptions, atoms, attitudes, audio, authorities, authority, away, balls, bally, banknote, banks, ban- 
ning, barely, barred, barriers, bass, battery, baum, bears, bell, belt, best, best-known, billion, binge, blamed, blanket, bloc, block, blocking, boat, 
bodies, boesel, bolstered, bonuses, boston, bowed, boys, bozell, bradstreet, brains, breakers, breaks, briefly, brink, brisk, broad-based, broken, 
bronx, brother, bsn, built, buried, burmah, burned, bursts, bush, businessland, businessman, buys, calculate, calculated, caltrans, campbell, can- 
dlestick, capitalism, captured, careers, carpeting, carried, carry-forward, casting, castle, catholic, caught, ceiling, cells, centuries, chair, chairs, 
challenged, chances, chandler, characters, charts, cheating, checks, cherry, chiron, cie, cie., cincinnati, circuit, civic, clara, classroom, clean- 
air, climate, closer, cms, cnw, coast, coats, cocom, cold, collected, comes, commercial, commerzbank, commissioned, committed, commute, 
complains, completing, computer, confirm, confiscated, confronted, conn, conn., consisting, consortium, constitute, consultant, consumer, 
consumers, contemporary, contra, contraceptive, contributing, convinced, cost-cutting, count, counterparts, counties, courses, cover, cracks, 
craft, crane, create, creating, crossing, crumbling, crusade, crusaders, cubic, curtail, curve, cushion, cut, cynthia, dairy, dam, david, davis, day, 
deal, dealerships, debentures, debut, deceptive, decided, decision, decisions, deck, defended, defenders, defenses, definitely, delivering, della, 
demonstrated, department, departure, depress, designated, desk, desktop, detailing, devaluation, develops, devoe, di, dialogue, dictator, die, 
diesel, differ, digs, diluted, diminished, direct-mail, disappointing, discount, discrepancies, discuss, disease, disney, disruption, distributed, dis- 
tributor, dive, diversified, divided, dividends, dodge, doing, domestic, dominant, domination, double-a, downgraded, downgrading, downtown, 
drives, drought, drunk, dunkin, earn, earthquakes, edisto, editions, educate, eggs, elaborate, elite, embarrassing, emerges, emerging, emigra- 
tion, employers, empty, enactment, encourages, endorsement, enemies, engelken, enhanced, entertaining, enthusiastic, epicenter, equipped, era, 
erosion, esselte, est, ethical, ethiopia, eurodollar, events, everyone, exchanges, exciting, exclusively, executed, executing, executive, executives, 
exempt, expertise, explicit, explosion, expressed, expression, extending, extraordinary, faculty, failed, failure, fallout, faltered, fanfare, fare, 
farm, fast-growing, fasteners, fastest, fax, fazio, february, federated, fee, field, fifth, fighting, filipino, film, final, financiers, finished, finland, 
firmed, fiscal, fits, fitzwater, five-cent, fixed-income, fla, flamboyant, fleets, fleming, fletcher, flight, flights, flowers, focus, folk, following, 
foot, forecasting, found, fox, fray, freeway, freeways, freeze, frequency, freshman, fromstein, frustrating, fur, galileo, game, gandhi, garbage, 
gathered, gave, gear, gene, generale, genuine, gerard, giant, girl, gloomy, goes, golden, goodman, gov., governing, government-owned, gover- 
nor, grave, greenhouse, gridlock, grim, guerrilla, guild, gun, h&r, half-hour, handicapped, handy, hanging, happening, happy, harold, haunts, 
headed, heating, heavier, heavily, hedges, heights, heller, helping, helps, hepatitis, hess, high-definition, high-technology, hiring, hoffman, hold, 
hole, homeless, honduras, hooker, horizon, hot-dipped, houses, how, hubbard, hurricane, hydro-quebec, hyman, idaho, ill, illness, illustrated, 
immune, impeachment, implicit, impose, impression, impressive, increase, incredible, incurred, indexing, indiana, indicates, indications, influ- 
ences, influx, inherent, inquiry, intensive, intentions, internationally, involves, irish, ironically, isler, itel, itt, j., jackson, jaguar, jazz, jefferson, 
jittery, jolted, july, jump, jury, justifies, karen, kean, keating, kent, kgb, khan, killing, knocked, knocking, koch, l.j., labs, lasts, lately, latest, 
lawrence, league, lean, least, leave, legitimacy, lehman, leisure, lend, leo, life, lighter, lights, linda, line, literature, live, living, longstanding, 
looking, looks, loral, lord, lose, lotus, louisville, lower, ltd., luis, lumpur, made, madrid, malcolm, male, manage, management, manic, manville, 
marcos, marked, market-makers, market-share, markets, mary, mass-market, mayor, mccormick, mcdonald, md., measured, member, members, 
memorandum, merabank, mercury, merely, merged, mergers, message, mich., mile, millions, mining, ministers, ministry, minorities, minutes, 
missile, mission, mitterrand, modified, monitor, months, moody, moore, mothers, motorola, movie, mr., much, multibillion-dollar, multiples, 
mundane, municipalities, muscle, mutual, mutual-fund, named, names, namibia, nashua, nathan, ncr, near-term, nec, necessary, necessity, 
negligence, negotiating, negotiation, nervousness, newcomers, newspaper, nice, noise, northrop, norton, nose, nothing, noticed, notification, 
notified, notwithstanding, november, nurses, nutritional, observed, oddly, off, offerings, official, oh, oklahoma, oldest, olympic, ones, open- 
ing, opera, optical, optimistic, or, original, orthodox, ortiz, ousted, outfit, outlook, outside, oversees, owen, oy, pachinko, packaged, painted, 
park, parker, part, particular, partly, patrick, patterson, payable, pc, peace, peaked, peddling, pegged, pepsico, perception, perfect, perfectly, 
pfizer, pharmaceutical, phelan, philippine, philippines, phony, photographic, physicians, picking, pigs, pittsburgh, place, plagued, plan, planes, 
planet, pleasure, poles, pool, portable, portfolio, ports, post-crash, pound, poured, poverty, precedent, preclude, pregnant, prescribed, presents, 
pretty, priced, privileges, procurement, products, profit-taking, projections, prominent, promise, promotional, prompted, proper, proponents, 

11 



propose, prosecuted, protein, prototype, prove, proved, published, publisher, pull, pulled, pumped, pumping, pushing, quebec, quickview, 
quist, quite, radical, radio, rain, ranging, rank, rebates, rebel, rebound, rebuild, recent, recital, recognizes, recognizing, recorded, recorders, 
reduce, reduced, refinery, refrigerators, registered, regret, reinvest, rejected, rejecting, rejection, relations, relatively, relying, remark, remics, 
reorganization, repaired, repeatedly, reports, represent, repurchase, resembles, reserved, resisted, resolved, resort, rest, restraints, restrictions, 
restructured, restructuring, result, rican, right, ring, rise, robbed, robinson, robots, robust, roh, role, rolled, rose, rothschild, rough, royal, 
ruled, rushing, s.c, sale, salesmen, salespeople, salmonella, salvage, saul, says, scheduled, school, schwarz, seagram, second, sector, securities, 
seek, segment, seismic, seldom, selected, semel, sending, sentences, sentencing, session, settlement, seventh, shed, shell, sheraton, shifting, 
shocks, short, showed, shy, sigh, sights, signals, sir, site, sites, sitting, skinner, slashed, snapped, so-called, soldiers, solely, solo, somehow, 
sotheby, speak, specialist, specialize, specializing, specifically, specifications, speculate, speculated, spencer, sperry, spreading, spur, stake, 
standardized, standing, statistics, steady, stemmed, stern, stevens, stock-index, stockholm, straight, strategists, stream, strength, stress-related, 
strict, subscriber, suggestions, surplus, surprise, surprises, surrounding, syrian, taiwanese, tall, tap, tapped, task, taxation, taxed, tci, technicians, 
televised, temptation, testing, texans, theatre, third, this, thomas, those, thoughts, thriving, tickets, ties, tiger, tighter, tire, tisch, together, toronto- 
based, toshiba, towers, toxin, traditional, trains, transit, trap, treated, trecker, tribune, trigger, triggering, trillion, tube, tune, turn, turnaround, 
typically, u.k., u.n., uncertain, underlying, underwear, underwrite, underwriter, underwriting, undo, unfortunately, unidentified, unilab, unisys, 
unit, unknown, unlawful, unless, unused, upheld, upon, upside, urge, usia, uv-b, valid, van, vendors, very, victim, vienna, violations, virginia, 
vision, visit, voluntary, w., wade, wait, wanting, ward, warner, wars, wary, wash., wealthy, wednesday, when-issued, whether, white-collar, 
wholly, widening, will, willingness, wilmington, win, winnebago, winners, wish, wolf, words, work, working, worse, would, yard, yards, 
yearly, yielding, youth, z, zone 

12 


