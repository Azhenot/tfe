





































fairness-awar learn through regular approach 

toshihiro kamishima∗,shotaro akaho∗, and jun sakuma† 
∗nation institut of advanc industri scienc and technolog (aist), 

aist tsukuba central 2, umezono 1–1–1, tsukuba, ibaraki, 305–8568 japan, 
email: mail@kamishima.net (http://www.kamishima.net/) and s.akaho@aist.go.jp 

†univers of tsukuba; and japan scienc and technolog agency, 
1-1-1 tennodai, tsukuba, japan; and 4-1-8, honcho, kawaguchi, saitama, 332-0012 japan 

email: jun@cs.tsukuba.ac.jp 

abstract—with the spread of data mine technolog and 
the accumul of social data, such technolog and data be 
be use for determin that serious affect people’ 
lives. for example, credit score be frequent determin 
base on the record of past credit data togeth with statist 
predict techniques. needless to say, such determin 
must be social and legal fair from a viewpoint of social 
responsibility; namely, it must be unbias and nondiscrimi- 
natori in sensit features, such a race, gender, religion, and 
so on. sever research have recent begin to attempt the 
develop of analysi techniqu that be awar of social fair- 
ness or discrimination. they have show that simpli avoid 
the use of sensit featur be insuffici for elimin bia 
in determinations, due to the indirect influenc of sensit 
information. from a privacy-preserv viewpoint, thi can be 
interpret a hide sensit inform when classif 
result be observed. In thi paper, we first discu three 
caus of unfair in machin learning. We then propos 
a regular approach that be applic to ani predict 
algorithm with probabilist discrimin models. We further 
appli thi approach to logist regress and empir show 
it effect and efficiency. 

keywords-fairness, discrimination, privacy, classification, lo- 
gistic regression, inform theori 

I. introduct 

data mine techniqu be be increasingli use for 
seriou determin such a credit, insur rates, em- 
ployment applications, and so on. their emerg have 
be make possibl by the accumul of vast store of 
digit person data, such a demograph information, 
financi transactions, commun logs, tax payments, 
and so on. additionally, the spread of off-the-shelf mine 
tool have make it easi to analyz these store data. 
such determin often affect people’ life seriously. 
for example, credit score be frequent determin base 
on the record of past credit data togeth with statist 
predict techniques. 

needless to say, such seriou determin must be 
social and legal fair from a viewpoint of social responsi- 
bility; that is, they must be unbias and nondiscriminatori 
in relat to sensit featur such a race, gender, religion, 
and so on. blind to such factor must be ensur 
in determin that affect people’ life directly. thus, 

sensit featur must be care treat in the process 
and algorithm for machin learning. 

In some cases, some featur must be care process 
for reason other than avoid discrimination. one such 
reason would be contract between servic provid and 
customers. consid the case in which person inform 
about custom demograph be collect to recommend 
item at an e-commerc site. If the site collect these data 
under a privaci polici that restrict the use of the data for 
the purpos of recommendation, person inform must 
not be use for the select of custom to be provid 
person discount coupons. In thi case, the use of 
unrestrict data would be problematic. becaus purchas 
log be influenc by recommend base on person 
information, care consider would be requir for the 
use of such data. 

sever research have recent begin to attempt the 
develop of analyt techniqu that be awar of social 
fair or discrimin [1], [2]. they have show that the 
simpl elimin of sensit featur from calcul 
be insuffici for avoid inappropri determin pro- 
cesses, due to the indirect influenc of sensit information. 
for example, when determin credit scoring, the featur 
of race be not used. however, if peopl of a specif race 
live in a specif area and address be use a a featur for 
train a predict model, the train model might make 
unfair determin even though the race featur be not 
explicitli used. such a phenomenon be call a red-lin 
effect [2] or indirect discrimin [1], and we describ 
it in detail in section ii-a. new analyt techniqu have 
be devis to deal with fairness. for example, calder 
and verwer propos a naiv bay that be modifi so a 
to be less discriminatori [2], and pedreschi et al. discuss 
discriminatori associ rule [1]. 

In thi paper, we formul caus of unfair in 
machin learning, develop wide applic and effici 
techniqu to enhanc fairness, and evalu the effective- 
ness and effici of our techniques. first, we discu 
the caus of unfair in machin learning. In previou 
works, sever notion of fair have be propos and 
success exploited. though these work focu on 

toshihiro kamishima 
T. kamishima, S. akaho, and J. sakuma “fairness-awar learn through regular approach” proceed of the 3rd ieee intern workshop on privaci aspect on data mine (2011) 
unoffici updat version 



result unfairness, we consid unfair in term of 
it causes. We describ three type of cause: prejudice, 
underestimation, and neg legacy. prejudic involv a 
statist depend between sensit featur and other 
information; underestim be the state in which a classifi 
have not yet converged; and neg legaci refer to the 
problem of unfair sampl or label in the train data. 
We also propos measur to quantifi the degre of these 
caus use mutual inform and the helling distance. 

second, we then focu on indirect prejudic and develop 
a techniqu to reduc it. thi techniqu be implement 
a regular that restrict the learners’ behaviors. thi 
approach can be appli to ani predict algorithm with 
discrimin probabilist models, such a logist regres- 
sion. In solv classif problem that pay attent 
to sensit information, we have to consid the trade- 
off between the classif accuraci and the degre of 
result fairness. our method provid a way to control 
thi trade-off by adjust the regular parameter. We 
propos a prejudic remov regularizer, which enforc a 
determination’ independ from sensit information. As 
we demonstrate, such a regular can be built into a logist 
regress model. 

finally, we perform experi to test the effect 
and effici of our methods. We compar our method 
with the two-naive-bay on a real data set use in a previ- 
ou studi [2]. We evalu the effect of our approach 
and examin the balanc between predict accuraci and 
fairness. 

note that in the previou work, a learn algorithm that be 
awar of social discrimin be call discrimination-awar 
mining. however, we hereaft use the terms, ‘unfairness’ / 
‘unfair’, instead of the ‘discrimination’ / ‘discriminatory’ for 
two reasons. first, a describ above, these technolog 
can be use for compli with laws, regulations, or con- 
tract that be irrelev to discrimination. second, becaus 
the term discrimin be frequent use for the mean 
of classif in the machin learn literature, use thi 
term becom highli confusing. wors yet, in thi paper, we 
target a discrimin model, i.e., logist regression. 

We discu caus of unfair in section II and propos 
our method for enhanc fair in section iii. our 
method be empir compar with two-naive-bay in 
section iv. section V show relat work, and section VI 
summar our conclusions. 

ii. fair IN data analysi 

after introduc an exampl of the difficulti in fairness- 
awar learning, we show three caus of unfair and 
quantit measur for the degre of these causes. 

A. illustr of the difficulti in fairness-awar learn 

We here introduc an exampl from the literatur to 
show the difficulti in fairness-awar learn [2], which 

be a simpl analyt result for the data set describ in 
section iv-b. the research perform a classif 
problem to predict whether the incom of an individu 
would be high or low. 

the sensit feature, S, be gender, which take a value, 
male or female, and the target class, Y, indic whether 
his/her incom be high or low. the sensit feature, S, 
be gender, which take a value, male or female, and the 
target class, Y , indic whether his/her incom be high 
or low. there be some other non-sensit features, X . 
the ratio of femal record compris about 1/3 of the data 
set; that is, the number of femal record be much small 
than that of male records. additionally, while about 30% of 
male record be classifi into the high class, onli 11% 
of femal record were. therefore, female–high record 
be the minor in thi data set. 

In thi data set, we describ how femal record tend 
to be classifi into the low class unfairly. calder and 
verwer defin a discrimin score (hereaft refer to 
a the calders-verw score (cv score) by subtract the 
condit probabl of the posit class give a sensit 
valu from that give a non-sensit value. In thi example, 
a CV score be defin a 

pr[i =high|s=male]− pr[i =high|s=female]. 

the CV score calcul directli from the origin data be 
0.19. after train a naiv bay classifi from data involv- 
ing a sensit feature, the CV score on the predict class 
increas to about 0.34. thi show that femal record be 
more frequent misclassifi to the low class than male 
records; and thus, female–high individu be consid 
to be unfairli treated. thi phenomenon be mainli caus 
by an occam’ razor principle, which be commonli adopt 
in classifiers. becaus infrequ and specif pattern tend 
to be discard to gener observ in data, minor 
record can be unfairli neglected. even if the sensit 
featur be remov from the train data for a naiv bay 
classifier, the result CV score be 0.28, which still show 
an unfair treatment for minorities. thi be caus by the 
indirect influenc of sensit features. thi event be call 
by a red-lin effect [2], a term that origin from the 
histor practic of draw red line on a map around 
neighborhood in which larg number of minor be 
know to dwell. consequently, simpli remov sensit 
featur be insufficient, and affirm action have to be 
adopt to correct the unfair in machin learning. 

B. three caus of unfair 

In thi section, we discu the social fair in data 
analysis. previou work [1], [2] have focu on unfair 
in the result determinations. To look more care 
at the problem of fair in machin learning, we shall 
examin the underli caus or sourc of unfairness. 
We suppos that there be at least three possibl causes: 



prejudice, underestimation, and neg legacy. note that 
these be not mutual exclusive, and two or more caus 
may composit lead to unfair treatments. 

befor present these three caus of unfairness, we 
must introduc sever notations. here, we discu super- 
vise learning, such a classif and regression, which 
be awar of unfairness. Y be a target random variabl to 
be predict base on the instanc valu of features. the 
sensit variable, S, and non-sensit variable, X , corre- 
spond to sensit and non-sensit features, respectively. 
We further introduc a predict modelm[i |x, s], which 
model a condit distribut of Y give X and S. with 
thi model and a true distribut over X and S, pr∗[x, s], 
we defin 

pr[y, X, S] =m[i |x, s]pr∗[x, s]. (1) 

appli margin and/or bayes’ rule to thi equa- 
tion, we can calcul other distributions, such a pr[y, S] 
or pr[i |x]. We use p̃r[·] to denot sampl distributions. 
p̂r[y, X, S] be defin by replac a true distribut in (1) 
with it correspond sampl distribution: 

p̂r[y, X, S] =m[i |x, s]p̃r[x, s], (2) 

and induc distribut from p̂r[y,x, S] be denot by 
use p̂r[·]. 

1) prejudice: prejudic mean a statist depend 
between a sensit variable, S, and the target variable, Y , 
or a non-sensit variable, X . there be three type of 
prejudices: direct prejudice, indirect prejudice, and latent 
prejudice. 

the first type be direct prejudice, which be the use of a 
sensit variabl in a predict model. If a model with a 
direct prejudic be use in classification, the classif 
result clearli depend on sensit features, therebi gen- 
erat a databas contain direct discrimin [1]. To 
remov thi type of prejudice, all that we have to do be 
simpli elimin the sensit variabl from the predict 
model. We then show a relat between such thi direct 
prejudic and statist dependence. after elimin the 
sensit variable, equat (1) can be rewrit a 

pr[y, X, S] =m[i |x]pr∗[s|x]pr∗[x]. 

thi equat state that S and Y be condit inde- 
pendent give X , i.e., Y ⊥⊥ S | X . hence, we can say that 
when the condit Y 6⊥⊥ S |X be not satisfied, the predict 
model have a direct prejudice. 

the second type be an indirect prejudice, which be statis- 
tical depend between a sensit variabl and a target 
variable. even if a predict model lack a direct prejudice, 
the model can have an indirect prejudic and can make an 
unfair determination. We give a simpl example. consid 
the case that all Y , X , and S be real scalar variables, and 
these variabl satisfi the equations: 

Y = X + εY and S = X + εS , 

where εY and εS be mutual independ 
random variables. becaus pr[y, X, S] be equal to 
pr[i |x] pr[s|x] pr[x], these variabl satisfi the 
condit Y ⊥⊥ S | X , but do not satisfi the condit 
y⊥⊥s. hence, the adopt predict model do not have 
a direct prejudice, but may have an indirect prejudice. If 
the varianc of εY and εS be small, Y and S becom 
highli correlated. In thi case, even if a model do not 
have a direct prejudice, the determin clearli depend 
on sensit information. such result determin 
be call indirect discrimin [1] or a red-lin effect 
[2] a describ in section ii-a. To remov thi indirect 
prejudice, we must use a predict model that satisfi the 
condit y⊥⊥s. 

We next show an index to quantifi the degre of indirect 
prejudice, which be straightforwardli defin a the mutual 
inform between Y and S. however, becaus a true 
distribut in (1) be unknown, we adopt sampl distribut 
in equat (2) over a give sampl set, D: 

PI = 
∑ 

(y,s)∈d 

p̂r[y, s] ln 
p̂r[y, s] 

p̂r[s]p̂r[s] 
. (3) 

We refer to thi index a a (indirect) prejudic index (pi for 
short). for convenience, the applic of the normal 
techniqu for mutual inform [3] lead to a normal 
prejudic index (npi for short): 

npi = pi/( 
√ 

h(i )h(s)), (4) 

where an entropi function h(x) be defin a 
− 

∑ 
x∈d p̂r[x] ln p̂r[x]. the rang of thi npi be [0, 1]. 

the third type of prejudic be latent prejudice, which be a 
statist depend between a sensit variable, S, and a 
non-sensit variable, X . consid an exampl that satisfi 
the equations: 

Y = X1 + εY , X = X1 + x2, and S = X2 + εS , 

where εy⊥⊥ε and x1⊥⊥x2. clearly, the condit 
Y ⊥⊥ S | X and y⊥⊥ be satisfied, but X and S be 
not mutual independent. thi depend doesn’t caus a 
sensit inform to influenc the final determination, but 
it would be exploit for train learners; thus, thi might 
violat some regul or laws. recal our exampl about 
person inform in section I. the use of raw purchas 
log may violat contract with customers, becaus the 
log be influenc by recommend base on person 
information, even if it be irrelev to the final select 
of customers. remov of potenti prejudic be achiev 
by make X and Y independ from S simultaneously. 
similar to a pi, the degre of a latent prejudic can be 
quantifi by the mutual inform between X and S. 

2) underestimation: underestim be the state in 
which a learn model be not fulli converg due to the 
finit of the size of a train data set. given a learn 



algorithm that can acquir a predict model without indi- 
rect prejudice, it will make a fair determin if infinit 
train exampl be available. however, if the size of the 
train data set be finite, the learn classifi may lead to 
more unfair determin than that observ in the train 
sampl distribution. though such determin be not 
intentional, they might awak suspicion of unfair treatment. 
In other words, though the notion of converg at infin 
be appropri in a mathemat sense, it might not be in a 
social sense. We can quantifi the degre of underestim 
by assess the result differ between the train 
sampl distribut over D, p̃r[·], and the distribut in- 
duce by a model, p̂r[·]. along thi line, we defin the 
underestim index (uei) use the helling distance: 

uei = 
(1 

2 

∑ 
y,s∈d 

(√ 
p̂r[y, s]− 

√ 
p̃r[y, s] 

)2)1/2 
= 

( 
1− 

∑ 
y,s∈d 

√ 
p̂r[y, s]p̃r[y, S] 

)1/2 
. (5) 

note that we do not adopt the kl-diverg becaus it can 
be infinit and thi properti be inconveni for an index. 

3) neg legacy: neg legaci be unfair sampl 
or label in the train data. for example, if a bank have 
be refus credit to minor peopl without assess 
them, the record of minor peopl be less sampl in 
a train data set. A sampl select bia be caus by 
such bia sampl depend on the featur of samples. 
It be know that the problem of a sampl select bia 
can be avoid by adopt specif type of classif 
algorithm [4]. however, it be not easi to detect the exist 
of a sampl select bia onli by observ train data. 
On the other hand, if a bank have be unfairli reject 
the loan of the peopl who should have be approved, 
the label in the train data would becom unfair. thi 
problem be seriou becaus it be hard to detect and correct. 
however, if other information, e.g., a small-siz fairli 
label data set, can be exploited, thi problem can be 
correct by techniqu such a transfer learn [5]. 

regul or law that demand the remov of potenti 
prejudic be rare. We investig uei in the experiment 
section of thi paper, but we don’t especi focu on 
underestimation. As describ above, avoid a neg 
legaci can be difficult if no addit inform be avail- 
able. We therefor focu on the develop of a method to 
remov indirect prejudice. 

iii. prejudic remov techniqu 

We here propos a techniqu to reduc indirect prejudice. 
becaus thi techniqu be implement a a regularizer, 
which we call a prejudic remover, it can be appli to 
wide varieti of predict algorithm with probabilist 
discrimin models. 

A. gener framework 

We focu on classif and built our regular 
into logist regress models. Y , X , and S be random 
variabl correspond to a class, non-sensit features, 
and a sensit feature, respectively. A train data set 
consist of the instanc of these random variables, i.e., 
D = {(y,x, s)}. the condit probabl of a class 
give non-sensit and sensit featur be model by 
m[i |x, s;θ], where Θ be the set of model parameters. 
these paramet be estim base on the maximum 
likelihood principle; that is, the paramet be tune so a 
to maxim the log-likelihood: 

`(d;θ) = 
∑ 

(yi,xi,si)∈d 

lnm[yi|xi, si;θ]. (6) 

We adopt two type of regularizers. the first regular- 
izer be a standard one to avoid over-fitting. We use an 
L2 regular ‖θ‖22. the second regularizer, r(d,θ), be 
introduc to enforc fair classification. We design thi 
regular to be easi to implement and to requir onli 
modest comput resources. By add these two regu- 
lariz to equat (6), the object function to minim 
be obtained: 

−`(d;θ) + ηr(d,θ) + λ 
2 
‖θ‖22, (7) 

where λ and η be posit regular parameters. 
We dealt with a classif problem in which the target 

valu Y be binari {0, 1}, X take a real vectors, x, and S 
take a discret value, s, in a domain S. We use a logist 
regress model a a predict model: 

m[y|x, s;θ] = yσ(x>ws) + (1− y)(1− σ(x>ws)), (8) 

where σ(·) be a sigmoid function, and the paramet be 
weight vector for x, Θ = {ws}s∈ . note that a constant 
term be includ in x without loss of generality. We next 
introduc a regular to reduc the indirect prejudice. 

B. prejudic remov 

A prejudic remov regular directli tri to reduc 
the prejudic index and be denot by rpr. recal that the 
prejudic index be defin a 

PI = 
∑ 
y, 

p̂r[y, S] ln 
p̂r[y, S] 

p̂r[s]p̂r[i ] 

= 
∑ 

y,x, 

m[i |x, s;θ]p̃r[x, S] ln p̂r[y, S] 
p̂r[s]p̂r[i ] 

. 

∑ 
x, p̃r[x, S] can be replac with 

∑ 
(xi,si)∈d, and the 

argument of logarithm can be rewrit a p̂r[i |si]/p̂r[i ], 
by reduc p̂r[s]. We obtain∑ 

(xi,si)∈d 

∑ 
y∈{0,1} 

m[y|xi, si;θ] ln 
p̂r[y|si] 
p̂r[y] 

. 



the straightforward way to comput p̂r[y|s] be to marginal- 
ize m[y|x, s;θ]p̂r[x, s] over X . however, if the domain 
of X be large, thi margin be comput heavy. 
We henc take a drastic simpl approach. We replac 
X with x̄s, which be a sampl mean vector of x over a set 
of train sampl whose correspond sensit featur be 
equal to s, {(yi,xi, si) ∈ D s.t. si = s}, and we get 

p̂r[y|s] =m[y|x̄s, s;θ], (9) 

p̂r[y] = 
∑ 
s∈ 

p̂r[s]m[y|x̄s, s;θ]. (10) 

finally, the prejudic remov regular rpr(d,θ) is∑ 
(xi,si)∈d 

∑ 
y∈{0,1} 

m[y|xi, si;θ] ln 
p̂r[y|si] 
p̂r[y] 

, (11) 

where p̂r[y|s] and p̂r[y] be equat (9) and (10), respec- 
tively. thi regular becom larg when a class be de- 
termin mainli base on sensit features; thus, sensit 
featur becom less influenti to the final determination. 
In the case of logist regression, the object function (7) 
to minim be rewrit a 

− 
∑ 

(yi,xi,si) 

lnm[yi|xi, si;θ]+rpr(d,θ)+ 
λ 

2 

∑ 
s∈ 

‖ws‖22, (12) 

where m[y|x, s;θ] be equat (8) and rpr(d,θ) be 
equat (11). In our experiment, thi object function be 
minim by a conjug gradient method start from the 
initi condit w = 0, ∀s ∈ S , and we obtain an optim 
paramet set, {w∗s}. 

the probabl of Y = 1 give a sampl without a class 
label, (xnew, snew) can be predict by 

pr[i =1|xnew, snew; {w∗s}] = σ(x>neww∗snew). 

iv. experi 

We compar our method with calder and verwer’ 
method on the real data set use in a previou studi [2]. 

A. calders-verwer’ 2-naive-bay 

We briefli introduc calder and verwer’ 2-naive-bay 
method (cv2nb), which be found to be the best method 
in the previou studi use the same dataset [2]. thi 
method target a binari classif problem. the number 
of sensit featur be one and the featur be binary. the 
gener model of thi method be 

pr[y,x, S] =m[y, S] 
∏ 

i 

m[xi|y, s]. (13) 

m[xi|y, S] model a condit distribut of Xi give 
Y and S, and the paramet of these model be estim 
by the similar way in the estim of paramet of a naiv 
bay model.m[y, S] model a joint distribut Y and S. 
becaus Y and S be not mutual independent, the final 

1 calcul a CV score, disc, of the predict class by the current model. 
2 while disc > 0 
3 numpo be the number of posit sampl classifi by the current model. 
4 if numpo < the number of posit sampl in D then 
5 n(i =1, s=0)← n(i =1, s=0) + ∆n(i =0, s=1) 
6 n(i =0, s=0)← n(i =0, s=0)−∆n(i =0, s=1) 
7 els 
8 n(i =0, s=1)← n(i =0, s=1) + ∆n(i =1, s=0) 
9 n(i =1, s=1)← n(i =1, s=1)−∆n(i =1, s=0) 
10 if ani of n(y, S) be neg then 

cancel the previou updat of n(y, S) and abort 
11 recalcul pr[i |s] and a CV score, disc base on updat n(y, S) 

figur 1. naiv bay modif algorithm 
note: n(i =y, s=s) denot the number of sampl in D, 
whose class and sensit featur be y and s, respectively. In 
our experiment, ∆ be set to 0.01 a in the origin paper. 

determin might be unfair. while each featur depend 
onli on a class in the case of the origin naiv bayes, everi 
non-sensit feature, xi, depend on both Y and S in the 
case of cv2nb. It be a if two naiv bay classifi be 
learn depend on each valu of the sensit feature; 
that be whi thi method be name by the 2-naive-bayes. 
To make the classifi fair, m[y, S] be initi by the 
sampl distribut p̃r[y, s], and thi model be modifi by 
the algorithm in figur 1. A model paramet m(y, s) 
be deriv by n(y, s)/ 

∑ 
y, n(i 

′, s′). thi algorithm be 
design so a to updat pr[y, S] gradual until a CV score 
becom positive. note that we slightli modifi the origin 
algorithm by add line 10 in figur 1, which guarante 
the parameters, n(y, s), to be non-negative, becaus the 
origin algorithm may fail to stop. 

B. experiment condit 

We summar our experiment conditions. We test a 
previous use real data set [2], a show in section ii-a. 
thi set includ 16281 data in an adult.test file of the 
adult/censu incom distribut at the uci repositori 
[6]. the target variabl indic whether or not incom be 
larg than 50m dollars, and the sensit featur be gender. 
thirteen non-sensit featur be discret by the pro- 
cedur in the origin paper. In the case of the naiv bayes, 
paramet of models,m[xi|y, s], be estim by a map 
estim with multinomi distribut and dirichlet priors. 
In the case of our logist regression, discret variabl be 
repres by 0/1 dummi variabl cod by a so-cal 
1-of-k scheme. the regular paramet for the L2 
regularizer, λ, be fix to 1, becaus the perform of 
pure logist regress be less affect by thi paramet in 
our preliminari experiments. We test six methods: logist 
regress with a sensit featur (lr), logist regress 
without a sensit featur (lrns), logist regress with 
a prejudic remov regular (pr), naiv bay with 
a sensit featur (nb), naiv bay without a sensit 
featur (nbns), and caldar and verwer’ 2-naive-bay 
(cv2nb). We show the mean of the statist obtain by 
the five-fold cross-validation. 



tabl I 
A summari OF experiment result 

method acc nmi npi uei cv PI / MI 

LR 0.851 0.267 5.21e-02 0.040 0.189 2.10e-01 
lrn 0.850 0.266 4.99e-04 0.036 -0.033 1.06e-03 
PR η=0 0.850 0.265 4.94e-02 0.038 0.185 2.01e-01 
PR η=0.1 0.850 0.264 4.11e-02 0.036 0.170 1.68e-01 
PR η=0.3 0.774 0.149 7.53e-03 0.127 -0.095 5.47e-02 
PR η=1 0.720 0.124 1.29e-05 0.148 -0.004 1.12e-04 
PR η=10 0.676 0.013 2.13e-01 0.259 -0.472 1.84e+01 

NB 0.822 0.246 1.12e-01 0.068 0.332 4.90e-01 
nbn 0.826 0.249 7.17e-02 0.043 0.267 3.11e-01 
cv2nb 0.813 0.191 3.64e-06 0.082 -0.002 2.05e-05 

note: 〈n1〉e〈n2〉 denot n1 × 10n2 . 

C. experiment result 

tabl I show accuraci (acc), npi and uei in section ii, 
and CV score (cvs). MI denot mutual inform be- 
tween sampl label and predict labels, nmi be obtain 
by normal thi MI in a process similar to npi. PI / MI 
quantifi a prejudic index that be sacrific by obtain a 
unit of inform about the correct label. thi can be use 
to measur the effici of the trade-off between predict 
accuraci and prejudic removal. A small PI / MI valu 
indic high effici in thi trade-off. 

We first compar the perform of our method with that 
of baselin in tabl I. compar with nbns, our method 
be superior both in accuraci and npi at η = 0.1. becaus 
lrn success remov prejudic without sacrific 
accuraci unlik nbns, our PR at η = 1 be good in 
PI / mi, but accuraci be fairli degraded. note that two 
methods, PR at η = 0 and lr, behav similarly, becaus 
our PR be almost equival to LR if the prejudic remov 
be elimin by set η = 0. 

We next move on to the influenc of the parameter, η, 
which control the degre of prejudic removal. We expect 
that the larg the η, the more prejudic would be removed, 
wherea accuraci might be sacrificed. accord to tabl I, 
a η increased, our PR gener becom degrad in accu- 
racy, but be also not fulli improv in prejudic removal. 

To further investig the chang of perform depend- 
ing on thi paramet η, we demonstr the variat in 
accuraci (acc), normal prejudic index (npi), and the 
trade-off effici between accuraci and prejudic remov 
(pi / mi) in figur 2. We focu on our PR method. overall, 
the chang be rather unstabl in all statistics. the reason 
for thi instabl would be the sub-optim in solut 
stem from the lack of convex of the object func- 
tion (12) and the approxim by replac the margin 
valu of X with their sampl means. the increas of η 
gener damag accuraci becaus a prejudic remov 
regular be design to remov prejudic by sacrific 
correct in prediction. npi peak at η = 1, though 

0.60 

0.65 

0.70 

0.75 

0.80 

0.85 

0.01 0.1 1 10 

(a) accuraci (acc) 

10 
−6 

10 
−5 

10 
−4 

10 
−3 

10 
−2 

10 
−1 

1 
0.01 0.1 1 10 

(b) normal prejudic index (npi) 

10 
−5 

10 
−4 

10 
−3 

10 
−2 

10 
−1 

1 

10 

0.01 0.1 1 10 

(c) trade-off effici between accuraci and prejudic remov 
(pi / mi) 

figur 2. the chang in perform accord to the paramet η 

note: horizont ax repres the paramet η, and vertic 
ax repres statist in each subtitle. solid, chain, dotted, 
and broken line indic the statist of pr, cv2nb, lrns, 
and nbns, respectively. 



we expect that more prejudic would be remov a η 
increased. We postul that thi would be due to the approx- 
imat in the margin of X; further investig be 
requir for thi point. the peak in trade-off effici be 
observ at η = 1, but accuraci be fairli damag at thi 
point. 

We next compar our PR with other methods. the 
perform of cv2nb be fairli good, and our PR be 
inferior except for accuraci at the low rang of η. when 
compar to the baselin lrns, by tune the paramet η, 
our PR could exceed in all statistics. however, it fail to 
exceed in both accuraci and prejudic remov at the same 
η. 

In summary, our PR could success reduc indirect 
prejudice, but accuraci be sacrific for thi reduction. We 
must further improv the effici in the trade-off between 
accuraci and prejudic removal. 

V. relat work 

sever analyt techniqu that be awar of fair or 
discrimin have recent receiv attention. pedreschi 
et al. emphas the unfair in associ rule whose 
consequ includ seriou determin [1], [7]. they 
advoc the notion of α-protection, which be the condit 
that associ rule be fair. given a rule whose conse- 
quent exhibit neg determination, it would be unfair if 
the confid of the rule substanti increas by add a 
condit relat to a sensit featur to the anteced part 
of the rule. the α-protect constrain the rule so that the 
ratio of thi increas be at most α. they also suggest the 
notion of direct discrimin and indirect discrimination. 
A direct discriminatori rule directli contain a sensit 
condit in it antecedent, and while an indirect discrimi- 
natori rule doesn’t directli contain a sensit condition, the 
rule be consid to be unfair in the context of background 
knowledg that includ sensit information. their work 
have sinc be extend [8]. variou kind of index for 
evalu discriminatori determin be propos and 
their statist signific have be discussed. A system for 
find such unfair rule have be propos [9]. calder and 
verwer propos sever method to modifi naiv bay 
for enhanc fair a describ in section iv-a [2]. 
luong et al. propos a notion of situat testing, wherein a 
determin be consid unfair if differ determin 
be make for two individu all of whose featur be equal 
except for sensit one [10]. such unfair be detect 
by compar the determin for record whose sensit 
featur be different, but be neighbor in non-sensit 
featur space. If a target determin differs, but non- 
sensit featur be complet equal, then a target variabl 
depend on a sensit variable. therefore, thi situat 
test have connect to our indirect prejudice. dwork et al. 
argu a data transform for the purpos of export 
data while keep awar of fair [11]. A data set held 

by a data owner be transform and pass to a vendor who 
classifi the transform data. the transform preserv 
the neighborhood relat of data and the equival 
between the expect of data map from sensit 
individu and from non-sensit ones. In a sens that 
consid the neighborhood relations, thi approach be 
relat to the abov notion of situat testing. becaus 
their proposit 2.2 impli that the classif result 
be roughli independ from the membership in a sensit 
group, their approach have relat to our idea of prejudice. 

In a broad sense, fairness-awar learn be a kind of 
cost-sensit learn [12]. that be to say, the cost of 
enhanc fair be take into account. fair in machin 
learn can be interpret a a sub-not of legitimacy, 
which mean that model can be deploy in the real 
world [13]. gondek and hofmann devis a method for 
find cluster that be not relev to a give group 
[14]. If a give group contain sensit information, thi 
method can be use for cluster data into fair clusters. 
independ compon analysi might be use to maintain 
the independ between featur [15]. 

the remov of prejudic be close relat to privacy- 
preserv data mine [16], which be a technolog for min- 
ing use inform without expos individu privat 
records. the privaci protect level be quantifi by mutual 
inform between the public and privat realm [17]. In 
our case, the degre of indirect prejudic be quantifi by mu- 
tual inform between classif result and sensit 
features. due to the similar of these two us of mutual 
information, the design goal of fairness-awar learn can 
be consid the protect of sensit inform when 
expos classif results. 

regard underestimation, the concept of anytim algo- 
rithm in plan or decis make [18] might be useful. 

As describ in section ii-b, the problem of neg 
legaci be close relat to transfer learning. transfer learn- 
ing be “the problem of retain and appli the knowledg 
learn in one or more task to effici develop an effec- 
tive hypothesi for a new task” [5]. among mani type of 
transfer learning, the problem of a sampl select bia [4] 
would be relat to the neg legaci problem. sampl 
select bia mean that the sampl be not at random, but 
bia depend on some featur valu of data. anoth 
relat approach to transfer learn be weight sampl 
accord the degre of use for the target task [19]. 
use these approaches, if give a small amount of fairli 
label data, other data set that might be unfairli label 
would be correctli processed. 

vi. conclus and futur work 

the contribut of thi paper be a follows. first, 
we propos three caus of unfairness: prejudice, under- 
estimation, and neg legacy. prejudic refer to the 
depend between sensit inform and the other 



information, either directli or indirectly. We further clas- 
sifi prejudic into three type and develop a way to 
quantifi them by mutual information. underestim be 
the state in which a classifi have not yet converged, therebi 
produc more unfair determin than those observ 
in a sampl distribution. neg legaci be the problem of 
unfair sampl or label in the train data. second, 
we develop techniqu to reduc indirect prejudice. We 
propos a prejudic remov regularizer, which enforc 
a classifier’ independ from sensit information. our 
method can be appli to ani algorithm with probabilist 
discrimin model and be simpl to implement. third, 
we show experiment result of logist regress with 
our prejudic remov regularizer. the experiment result 
show the effect and effici of our methods. We 
further propos a method to evalu the trade-off between 
the predict accuraci and fairness. 

research on fairness-awar learn be just beginning; 
thus, there be mani problem yet to be solved; for example, 
the definit of fair in data analysis, measur for 
fairness, and maintain other type of law or regulations. 
the type of analyt method be sever limit at present. 
our method can be easili appli to regression, but fairness- 
awar cluster and rank method be also needed. 

the use of data mine technolog in our societi 
will onli becom great with time. unfortunately, their 
result can occasion damag people’ life [20]. On the 
other hand, data analysi be crucial for enhanc public 
welfare. for example, exploit person inform have 
prove to be effect for reduc energi consumption, 
improv the effici of traffic control, prevent in- 
fectiou diseases, and so on. consequently, method of 
data exploit that do not damag people’ lives, such 
a fairness/discrimination-awar learning, privacy-preserv 
data mining, or adversari learning, togeth compris the 
notion of social respons mining, which it should be- 
come an import concept in the near future. 

acknowledg 

We wish to thank dr. sicco verwer for provid detail 
inform about hi work. thi work be support by the 
grant-in-aid 14658106, 16700157, and 21500154 of the 
japan societi for the promot of science. 

refer 

[1] D. pedreschi, S. ruggieri, and F. turini, “discrimination- 
awar data mining,” in proc. of the 14th int’l conf. on 
knowledg discoveri and data mining, 2008. 

[2] T. calder and S. verwer, “three naiv bay approach for 
discrimination-fre classification,” data mine and knowl- 
edg discovery, vol. 21, pp. 277–292, 2010. 

[3] A. strehl and J. ghosh, “cluster ensembl — a knowledg 
reus framework for combin multipl partitions,” journal 
of machin learn research, vol. 3, pp. 583–617, 2002. 

[4] B. zadrozny, “learn and evalu classifi under sam- 
ple select bias,” in proc. of the 21st int’l conf. on machin 
learning, 2004, pp. 903–910. 

[5] “nip workshop — induct transfer: 10 year later,” 2005, 
http://iitrl.acadiau.ca/itws05/. 

[6] A. frank and A. asuncion, “uci machin learn reposi- 
tory,” univers of california, irvine, school of inform 
and comput sciences, 2010, http://archive.ics.uci.edu/ml. 

[7] S. ruggieri, D. pedreschi, and F. turini, “data mine for 
discrimin discovery,” acm transact on knowledg 
discoveri from data, vol. 4, no. 2, 2010. 

[8] D. pedreschi, S. ruggieri, and F. turini, “measur discrim- 
inat in socially-sensit decis records,” in proc. of the 
siam int’l conf. on data mining, 2009, pp. 581–592. 

[9] S. ruggieri, D. pedreschi, and F. turini, “dcube: discrimina- 
tion discoveri in databases,” in proc of the acm sigmod 
int’l conf. on manag of data, 2010, pp. 1127–1130. 

[10] B. T. luong, S. ruggieri, and F. turini, “k-nn a an imple- 
mentat of situat test for discrimin discoveri and 
prevention,” in proc. of the 17th int’l conf. on knowledg 
discoveri and data mining, 2011, pp. 502–510. 

[11] C. dwork, M. hardt, T. pitassi, O. reingold, and R. zemel, 
“fair through awareness,” arxiv.org:1104.3913, 2011. 

[12] C. elkan, “the foundat of cost-sensit learning,” in 
proc. of the 17th int’l joint conf. on artifici intelligence, 
2001, pp. 973–978. 

[13] C. perlich, S. kaufman, and S. rosset, “leakag in data 
mining: formulation, detection, and avoidance,” in proc. of 
the 17th int’l conf. on knowledg discoveri and data 
mining, 2011, pp. 556–563. 

[14] D. gondek and T. hofmann, “non-redund data clustering,” 
in proc. of the 4th ieee int’l conf. on data mining, 2004, 
pp. 75–82. 

[15] R. S. sutton and A. G. barto, reinforc learning: An 
introduction. mit press, 1998. 

[16] C. C. aggarw and P. S. yu, eds., privacy-preserv data 
mining: model and algorithms. springer, 2008. 

[17] S. venkatasubramanian, “measur of anonimity,” in privacy- 
preserv data mining: model and algorithms, C. C. 
aggarw and P. S. yu, eds. springer, 2008, ch. 4. 

[18] S. zilberstein, “use anytim algorithm in intellig sys- 
tems,” AI magazine, vol. 17, no. 3, pp. 73–86, 1996. 

[19] W. dai, Q. yang, g.-r. xue, and Y. yu, “boost for 
transfer learning,” in proc. of the 24th int’l conf. on machin 
learning, 2007, pp. 193–200. 

[20] D. boyd, “privaci and public in the context of big data,” 
in keynot talk of the 19th int’l conf. on world wide web, 
2010. 




