


















































learn cross-mod embed for cook recip and food imag 

amaia salvador1∗ nichola hynes2∗ yusuf aytar2 javier marin2 ferda ofli3 

ingmar weber3 antonio torralba2 
1universitat politècnica de catalunya 2massachusett institut of technolog 

3qatar comput research institute, hbku 
amaia.salvador@upc.edu, nhynes@mit.edu, {yusuf,jmarin,torralba}@csail.mit.edu, {fofli,iweber}@qf.org.qa 

abstract 

In thi paper, we introduc recipe1m, a new large-scale, 
structur corpu of over 1m cook recip and 800k food 
images. As the larg publicli avail collect of recip 
data, recipe1m afford the abil to train high-capac 
model on aligned, multi-mod data. use these data, we 
train a neural network to find a joint emb of recip 
and imag that yield impress result on an image-recip 
retriev task. additionally, we demonstr that regulariza- 
tion via the addit of a high-level classif object 
both improv retriev perform to rival that of human 
and enabl semant vector arithmetic. We postul that 
these embed will provid a basi for further explor 
of the recipe1m dataset and food and cook in general. 
code, data and model be publicli available1. 

1. introduct 

there be few thing so fundament to the human expe- 
rienc a food. it consumpt be intric link to our 
health, our feel and our culture. even migrant start 
a new life in a foreign countri often hold on to their ethnic 
food longer than to their nativ language. vital a it be to 
our lives, food also offer new perspect on topic chal- 
leng in comput vision like find represent that 
be robust to occlus and deform (a occur dure 
ingredi processing). 

the profus of onlin recip collect with user- 
submit photo present the possibl of train ma- 
chine to automat understand food prepar by 
jointli analyz ingredi lists, cook instruct and 
food images. far beyond applic sole in the realm of 
culinari arts, such a tool may also be appli to the plethora 
of food imag share on social medium to achiev insight 
into the signific of food and it prepar on public 

∗contribut equally. 
1http://im2recipe.csail.mit.edu 

figur 1: learn cross-mod embed from recipe- 
imag pair collect from onlin resources. these enabl 
u to achiev in-depth understand of food from it ingre- 
dient to it preparation. 

health [4] and cultur heritag [14]. develop a tool for 
autom analysi requir larg and well-cur datasets. 

the emerg of massiv label dataset [19, 26] and 
deeply-learn represent [10, 20, 5] have redefin the 
state-of-the-art in object recognit and scene classification. 
moreover, the same techniqu have enabl progress in 
new domain like dens label and imag segmentation. 
perhap the introduct of a new large-scal food dataset– 
complet with it own intrins challenges–wil yield a simi- 
lar advanc of the field. for instance, categor an 
ingredient’ state (e.g., sliced, diced, raw, baked, grilled, or 
boiled) provid a uniqu challeng in attribut recognition– 
one that be not well pose by exist datasets. furthermore, 
the free-form natur of food suggest a departur from the 
concret task of classif in favor of a more nuanc 
object that integr variat in a recipe’ structure. 

exist work, however, have focu larg on the use 
of medium-scal dataset for perform categor 
[1, 8, 16, 13]. for instance, bossard et al. [1] introduc the 
food-101 visual classif dataset and set a baselin of 
50.8% accuracy. even with the impetu for food imag cat- 
egorization, subsequ work by [13], [16] and [17] could 
onli improv thi result to 77.4%, 79% and 80.9%, respec- 

1 

http://im2recipe.csail.mit.edu 


figur 2: dataset statistics. preval of cours categori and number of instruct and ingredi per recipe. 

partit # recip # imag 

train 720,639 619,508 
valid 155,036 133,860 
test 154,045 134,338 

total 1,029,720 887,706 

tabl 1: recipe1m dataset. number of sampl in training, 
valid and test sets. 

tively, which indic that the size of the dataset may be 
the limit factor. although myer et al. [16] build upon 
food-101 to tackl the novel challeng of estim a meal’ 
energi content, the segment and depth inform use 
in their work be not make avail for further exploration. 

In thi work, we address data limit by introduc 
the large-scal recipe1m dataset which contain one million 
structur cook recip and their images. additionally, to 
demonstr it utility, we present the im2recip retriev task 
which leverag the full dataset–imag and text–to solv 
the practic and social relev problem of demystifi 
the creation of a dish that can be see but not necessarili de- 
scribed. To thi end, we have develop a multi-mod neural 
model which jointli learn to emb imag and recip in 
a common space which be semant regular by the 
addit of a high-level classif task. the perform 
of the result embed be thoroughli evalu against 
baselin and humans, show remark improv 
over the former while fare compar to the latter. with 
the releas of recipe1m, we hope to spur advanc on 
not onli the im2recip task but also heretofor unimagin 
object which requir a deep understand of the domain 
and it modalities. 

2. dataset 

given the relev of understand recipes, it be surpris- 
ing that there be not a larg bodi of work on the topic. We 
estim that thi be due to the absenc of a large, gener 
collect of recip data. To our knowledge, virtual all of 
the readili avail food-rel dataset either contain onli 

categor imag [16, 1, 8, 24] or simpli recip text [11]. 
onli recent have a few dataset be releas that includ 
both recip and images. the first of which [23] have 101k 
imag divid equal among 101 categories; the recip 
for each be howev raw html. In a late work, chen and 
ngo [6] present a dataset contain 110,241 imag anno- 
tat with 353 ingredi label and 65,284 recipes, each 
with a brief introduction, ingredi list, and prepar 
instructions. Of note be that the dataset onli contain recip 
for chines cuisine. 

although the aforement dataset constitut a larg 
step toward learn richer recip representations, they be 
still limit in either gener or size. As the abil to 
learn effect represent be larg a function of the 
quantiti and qualiti of the avail data, we creat and 
releas publicli a new, large-scal corpu of structur recip 
data that includ over 1m recip and 800k images. In 
comparison to the current larg dataset in thi domain, 
recipe1m includ twice a mani recip a [11] and eight 
time a mani imag a [6]. In the follow subsect 
we outlin how the dataset be collect and organ and 
provid an analysi of it contents. 

2.1. data collect 

the recip be scrap from over two dozen popu- 
lar cook websit and process through a pipelin that 
extract relev text from the raw html, download 
link images, and assembl the data into a compact json 
schema in which each datum be uniqu identified. As 
part of the extract process, excess whitespace, html 
entities, and non-ascii charact be remov from the 
recip text. 

2.2. data structur 

the content of the recipe1m dataset may logic be 
group into two layers. the first contain basic inform 
includ title, a list of ingredients, and a sequenc of instruc- 
tion for prepar the dish; all of these data be provid a 
free text. the second layer build upon the first and includ 
ani imag with which the recip be associated–thes be 
provid a rgb in jpeg format. additionally, a subset of 



recip be annot with cours label (e.g., appetizer, side 
dish, dessert), the preval of which be summar in 
figur 2. 

2.3. analysi 

the averag recip in the dataset consist of nine ingre- 
dient which be transform over the cours of ten instruc- 
tions. approxim half of the recip have imag which, 
due to the natur of the data sources, depict the fulli pre- 
par dish. recipe1m includ approxim 0.4% dupli- 
cate recip and 2% duplic imag (differ recip may 
share same image). exclud those 0.4% recipes, 20% 
of recip have non-uniqu titl but symmetr differ 
by a median of 16 ingredients. 0.2% of recip share the 
same ingredi but be rel simpl (e.g., spaghetti, 
granola), have a median of six ingredients. regard 
our experiments, we care remov ani exact duplic 
or recip share the same imag in order to avoid over- 
lap between train and test subsets. As detail in 
tabl 1, around 70% of the data be label a training, and 
the remaind be split equal between the valid and test 
sets. 

In figur 2, one can easili observ that the distribut of 
data be heavi tailed. for instance, of the 16k uniqu ingredi- 
ent that have be identified, onli 4,000 account for 95% of 
occurrences. At the low end of instruct count–particularli 
those with one step–on will find the dread combin all 
ingredients. At the other end be lengthi recip and ingredi- 
ent list associ with recip that includ sub-recipes. A 
similar issu of outlier exist also for images: a sever of 
the includ recip collect curat user-submit images, 
popular recip like chocol chip cooki have order of 
magnitud more imag than the average. notably, 25% of 
imag be associ with 1% of recip while half of all 
imag belong to 10% of recipes; the size of the second layer 
in number of uniqu recip be 333k. 

3. learn embed 
In thi section we introduc our neural joint emb 

model. here we util the pair (recip and image) data 
in order to learn a common emb space a sketch in 
figur 1. next, we discu recip and imag represent 
and then we introduc our neural joint emb model 
that build upon recip and imag representations. 

3.1. represent of recip 

there be two major compon of a recipe: it ingredi- 
ent and cook instructions. We develop a suitabl repre- 
sentat for each of these components. 

ingredients. each recip contain a set of ingredi text a 
show in figur 1. for each ingredi we learn an ingre- 
dient level word2vec [15] representation. In order to do so, 

the actual ingredi name be extract from each ingre- 
dient text. for instanc in “2 tbsp of oliv oil” the oliv oil 
be extract a the ingredi name and treat a a singl 
word for word2vec computation. the initi ingredi name 
extract task be solv by a bi-direct lstm that per- 
form logist regress on each word in the ingredi text. 
train be perform on a subset of our train set for 
which we have the annot for actual ingredi names. 
ingredi name extract modul work with 99.5% accu- 
raci test on a held-out set. 

cook instructions. each recip also have a list of cook 
instructions. As the instruct be quit lengthi (averag 
208 words) a singl lstm be not well suit to their rep- 
resent a gradient be diminish over the mani time 
steps. instead we propos a two-stag lstm model which 
be design to encod a sequenc of sequences. first, each 
instruction/sent be repres a a skip-instruct vec- 
tor and then an lstm be train over the sequenc of these 
vector to obtain the represent of all instructions. the 
result fixed-length represent be fed into to our joint 
emb model (see instructions-encod in figur 3). 

skip-instructions. our cook instruct representation, 
refer a skip-instructions, be the product of a sequence- 
to-sequ model [21]. specifically, we build upon the 
techniqu of skip-thought [9] which encod a sentenc 
and us that encod a context when decoding/predict 
the previou and next sentences. our modif to thi 
method includ add start- and end-of-recip “instructions” 
and use an lstm instead of a gru. In either case, the 
represent of a singl instruct be the final output of 
the encoder. As before, thi be use a the instruct input 
to our emb model. 

3.2. represent of food imag 

for the imag represent we adopt two major state-of- 
the-art deep convolut networks, name vgg-16 [20] 
and resnet-50 [5] models. In particular, the deep resid- 
ual network have a proven record of success on a varieti 
of benchmark [5]. although [20] suggest train veri 
deep network with small convolut filters, deep residu 
network take it to anoth level use ubiquit ident 
map that enabl train of much deeper architectur 
(e.g., with 50, 101, 152 layers) with good performance. We 
incorpor these model by remov the last softmax classi- 
ficat layer and connect the rest to our joint emb 
model a show in the right side of figur 3. 

4. joint neural embed 
build upon the previous describ recip and im- 

age representations, we now introduc our joint emb 
method. the recip model, display in figur 3, includ 
two encoders: one for ingredi and one for instructions, 



figur 3: joint neural emb model with semant regularization. our model learn a joint emb space for food 
imag and cook recipes. 

the combin of which be design to learn a recip 
level representation. the ingredi encod combin the 
sequenc of ingredi word vectors. sinc the ingredi 
list be an unord set, we choos to util a bidirect 
lstm model, which consid both forward and backward 
orderings. the instruct encod be implement a a 
forward lstm model over skip-instruct vectors. the 
output of both encod be concaten and emb 
into a recipe-imag joint space. the imag represent 
be simpli project into thi space through a linear transfor- 
mation. the goal be to learn transform to make the 
embed for a give recipe-imag pair “close.” 

formally, assum that we be give a set of the recipe- 
imag pairs, (rk, vk) in which Rk be the kth recip 
and vk be the associ image. further, let Rk = 
({stk} 

nk 
t=1, {gtk} 

mk 
t=1, vk), where {stk} 

nk 
t=1 be the sequenc of 

nk cook instructions, {gtk} 
mk 
t=1 be the sequenc of mk in- 

gredient tokens. the object be to maxim the cosin 
similar between posit recipe-imag pairs, and mini- 
mize it between all non-match recipe-imag pairs, up to 
a specifi margin. 

the ingredi encod be implement use a bi- 
direct lstm: at each time step it take two ingredient- 
word2vec represent of gtk and g 

m−t+1 
k , and eventu 

it produc the fixed-length represent hgk for ingredi- 
ents. the instruct encod be implement through a 
regular lstm. At each time step it receiv an instruct 
represent from the skip-instruct encoder, and final 
it produc the fixed-length represent hsk. h 

g 
k and h 

s 
k 

be concaten in order to obtain the recip represent 
hrk . then the recip and imag represent be map 
into the joint emb space as: φR =wrhrk + b 

R and 
φv =W vvk+ b 

v , respectively. WR and W v be emb 

matrix which be also learned. final the complet model 
be train end-to-end with posit and neg recipe-imag 
pair (φr, φv) use the cosin similar loss with margin 
defin a follows: 

lcos((φ 
R, φv), y) = 

{ 
1 − cos(φr, φv), if y = 1 
max(0, cos(φr, φv) − α), if y = −1 

where cos(.) be the normal cosin similar and α be the 
margin. 

5. semant regular 
We incorpor addit regular on our embed- 

ding through solv the same high-level classif prob- 
lem in multipl modal with share high-level weights. 
We refer to thi method a semant regularization. the key 
idea be that if high-level discrimin weight be shared, 
then both of the modal (recip and imag embeddings) 
should util these weight in a similar way which bring 
anoth level of align base on discrimination. We 
optim thi object togeth with our joint emb 
loss. essenti the model also learn to classifi ani imag 
or recip emb into one of the food-rel semant 
categories. We limit the effect of semant regular a 
it be not the main problem that we aim to solve. 

semant categories. We start by assign food-101 cate- 
gori to those recip that contain them in their title. how- 
ever, after thi procedur we be onli abl to annot 13% 
of our dataset, which we argu be not enough label data 
for a good regularization. hence, we compos a larg set of 
semant categori pure extract from recip titles. We 
first obtain the top 2,000 most frequent bigram in recip 
titl from our train set. We manual remov those that 



im2recip recipe2im 

medr r@1 r@5 r@10 medr r@1 r@5 r@10 

random rank 500 0.001 0.005 0.01 500 0.001 0.005 0.01 
cca w/ skip-thought + word2vec (googlenews) + imag featur 25.2 0.11 0.26 0.35 37.0 0.07 0.20 0.29 
cca w/ skip-instruct + ingredi word2vec + imag featur 15.7 0.14 0.32 0.43 24.8 0.09 0.24 0.35 

joint emb. onli 7.2 0.20 0.45 0.58 6.9 0.20 0.46 0.58 
joint emb. + semant 5.2 0.24 0.51 0.65 5.1 0.25 0.52 0.65 

tabl 2: im2recip retriev comparisons. median rank and recal rate at top K be report for baselin and our method. 
note that the joint neural emb model consist outperform all the baselin methods. 

joint emb. method im2recip recipe2im 

medr-1k medr-5k medr-10k medr-1k medr-5k medr-10k 

vgg-16 
fix vision 15.3 71.8 143.6 16.4 76.8 152.8 
finetun (ft) 12.1 56.1 111.4 10.5 51.0 101.4 
ft + semant reg. 8.2 36.4 72.4 7.3 33.4 64.9 

resnet-50 
fix vision 7.9 35.7 71.2 9.3 41.9 83.1 
finetun (ft) 7.2 31.5 62.8 6.9 29.8 58.8 
ft + semant reg. 5.2 21.2 41.9 5.1 20.2 39.2 

tabl 3: ablat studies. effect of the differ model compon to the median rank (the low be better). 

contain unwant charact (e.g., n’, !, ? or &) and those 
that do not have discrimin food properti (e.g., best 
pizza, super easi or 5 minutes). We then assign each of the 
remain bigram a the semant categori to all recip 
that includ it in their title. By use bigram and food-101 
categori togeth we obtain a total of 1,047 categories, 
which cover 50% of the dataset. chicken salad, grill veg- 
etable, chocol cake and fri fish be some exampl 
among the categori we collect use thi procedure. all 
those recip without a semant categori be assign to an 
addit background class. although there be some overlap 
in the gener categories, 73% of the recip in our dataset 
(exclud those in the background class) belong to a singl 
categori (i.e., onli one of the gener class appear in 
their title). for recip where two or more categori appear 
in the title, the categori with high frequenc rate in the 
dataset be chosen. 

classification. To incorpor semant regular to the 
joint emb we use a singl fulli connect layer. given 
the embed φv and φr, class probabl be obtain 
with pr = W cφr and pv = W cφv follow by a softmax 
activation. W c be the matrix of learn weights, which be 
share between imag and recip embed to promot 
semant align between them. formally, we express the 
semant regular loss a lreg(φr, φv, cr, cv) where 
cr,cv be the semant categori label for recip and image, 
respectively. note that cr and cv be the same if (φr, φv) be 
a posit pair. then we can write the final object as: 

l(φr, φv, cr, cv, y) = lcos((φ 
r, φv), y)+ 

λlreg(φ 
r, φv, cr, cv) 

optimization. We follow a two-stag optim proce- 
dure while learn the model. If we updat both the recip 
encod and imag network at the same time, optimiza- 
tion becom oscillatori and even divergent. previou work 
on cross-mod train [2] suggest train model for 
differ modal separ and fine tune them jointli 
afterward to allow alignment. follow thi insight, we 
adopt a similar procedur when train our model. We 
first fix the weight of the imag network, which be found 
from pre-train on the imagenet object classif task, 
and learn the recip encodings. thi way the recip net- 
work learn to align itself to the imag represent and 
also learn semant regular paramet (W c). then 
we freez the recip encod and semant regular 
weights, and learn the imag network. thi two-stag pro- 
ce be crucial for success optim of the object 
function. after thi initi align stage, we releas all the 
weight to be learned. however, the result do not chang 
much in thi final, joint optimization. 
implement detail all the neural model be imple- 
ment use the torch7 framework2. the margin α be 

2http://torch.ch/ 

http://torch.ch/ 


figur 4: retriev examples. from left to right: (1) the 
queri image, (2) it associ ingredi list, (3) the re- 
triev ingredi and (4) the imag associ to the re- 
triev recipe. 

select a 0.1 in joint neural emb models. the reg- 
ular hyperparamet be set a λ = 0.02 in all our 
experiments. while optim the cosin loss we pick a 
posit recipe-imag pair with 20% probabl and a ran- 
dom neg recipe-imag pair with 80% probabl from 
the train set. the model be train on 4 nvidia titan 
X with 12gb of memori for three days. 

6. experi 
We begin with the evalu of our learn embed 

for the im2recip retriev task. We then studi the effect of 
each compon of our model and compar our final system 
against human performance. We also analyz the properti 
of our learn embed through unit visual and 
vector arithmet in the emb space. 

6.1. im2recip retriev 

We evalu all the recip represent for im2recip 
retrieval. given a food image, the task be to retriev it recip 
from a collect of test recipes. We also perform recipe2im 
retriev use the same setting. all result be report for 
the test set. 
comparison with the baselines. canon correl 
analysi (cca) be one of the strong statist model for 
learn joint embed for differ featur space when 
pair data be provided. We use cca over mani high-level 
recip and imag represent a our baseline. these 
cca embed be learn use recipe-imag pair from 
the train data. In each recipe, the ingredi be repre- 

sent with the mean word2vec across all it ingredi in 
the manner of [12]. the cook instruct be repres 
with mean skip-thought vector [9] across the cook in- 
structions. A recip be then repres a concaten of 
these two features. We also evalu cca over mean in- 
gredient word2vec and skip-instruct featur a anoth 
baseline. the imag featur util in the cca baselin 
be the resnet-50 featur befor the softmax layer. al- 
though they be learn for visual object categor task 
on imagenet dataset, these featur be wide adopt by 
the comput vision community, and they have be show 
to gener well to differ visual recognit task [3]. 

for evaluation, give a test queri image, we use cosin 
similar in the common space for rank the relev 
recip and perform im2recip retrieval. the recipe2im 
retriev set be evalu likewise. We adopt the test 
procedur from image2capt retriev task [7, 22]. We 
report result on a subset of randomli select 1,000 recipe- 
imag pair from the test set. We repeat the experi 
10 time and report the mean results. We report median 
rank (medr), and recal rate at top K (r@k) for all the 
retriev experiments. To clarify, r@5 in the im2recip task 
repres the percentag of all the imag queri where the 
correspond recip be retriev in the top 5, henc high 
be better. the quantit result for im2recip retriev be 
show in tabl 2. 

our model greatli outperform the cca baselin in all 
measures. As expected, cca over ingredi word2vec and 
skip-instruct perform good than cca over word2vec 
train on googlenew [15] and skip-thought vector that 
be learn over a large-scal book corpu [9]. In 65% of all 
evalu queries, our method can retriev the correct recip 
give a food image. the semant regular notabl 
improv the qualiti of our emb for im2recip task 
which be quantifi with the medr drop from 7.2 to 5.2 in ta- 
ble 2. the result for recipe2im task be also similar to those 
in the im2recip retriev setting. figur 4 compar the 
ingredi from the origin recip (true recipes) with the 
retriev recip (coupl with their correspond image) 
for differ imag queries. As can be observ in figur 4, 
our embed gener well and allow overal satisfac- 
tori recip retriev results. however, at the ingredi level, 
one can find that in some case our model retriev recip 
with miss ingredients. thi usual occur due to the lack 
of fine-grain featur (e.g., confus between shrimp 
and salmon) or simpli becaus the ingredi be not vis- 
ibl in the queri imag (e.g., blueberri in a smoothi or 
beef in a lasagna). 
ablat studies. We also analyz the effect of each com- 
ponent in our our model in sever optim stages. the 
result be report in tabl 3. note that here we also report 
medr with 1k, 5K and 10k random select to show how 
the result scale in larg retriev problems. As expected, 



figur 5: local unit activations. We find that ingredi detector emerg in differ unit in our embeddings, which 
be align across modal (e.g., unit 352: “cream”, unit 22: “spong cake” or unit 571: “steak”). 

visual featur from the resnet-50 model show a substan- 
tial improv in retriev perform when compar 
to vgg-16 features. even with “fix vision” network 
the joint emb achiev 7.9 medr use resnet-50 
architectur (see tabl 3). further “finetuning” of vision net- 
work slightli improv the results. although it becom a 
lot harder to decreas the medr in small numbers, addit 
“semant regularization” improv the medr in both cases. 

6.2. comparison with human perform 

In order to good ass the qualiti of our embed we 
also evalu the perform of human on the im2recip 
task. the experi be perform through amazon me- 
chanic turk (amt) service3. for qualiti purposes, we 
requir each amt worker to have at least 97% approv rate 
and have perform at least 500 task befor our experiment. 
In a singl evalu batch, we first randomli choos 10 
recip and their correspond images. We then ask an 
amt worker to choos the correct recipe, out of the 10 pro- 
vide recipes, for the give food image. thi multipl choic 
select task be perform 10 time for each food imag in 
the batch. the accuraci of an evalu batch be defin a 
the percentag of imag queri correctli assign to their 
correspond recipe. 

the evalu be perform for three level of diffi- 
culty. the batch (of 10 recipes) be randomli chosen 
from either all the test recip (easy), recip share the 
same cours (e.g., soup, salad, or beverage; medium), or 
recip share the name of the dish (e.g., salmon, pizza, 
or ravioli; hard). As expected–for our model a well a the 
amt workers–th accuraci decreas a task becom more 

3http://mturk.com 

specific. In both coars and fine-grain tests, our method 
perform compar to or good than the amt workers. As 
hypothesized, semant regular further improv the 
result (see tabl 4). 

In the “all recipes” condition, 25 random evalu 
batch (25× 10 individu task in total) be select from 
the entir test set. joint emb with semant regulariza- 
tion perform the best with 3.2 percentag point improve- 
ment over averag human accuracy. for the course-specif 
tests, 5 batch be randomli select within each give 
meal course. although, on average, our joint embedding’ 
perform be slightli low than the humans’, with seman- 
tic regular our joint emb surpass humans’ 
perform by 6.8 percentag points. In dish-specif tests, 
five random batch be select if they have the dish name 
(e.g., pizza) in their title. with slightli low accuraci in 
general, dish-specif result also show similar behavior. par- 
ticularli for the “beverage” and “smoothie” results, human 
perform be good than our method, possibl becaus 
detail analysi be need to elicit the homogen ingre- 
dient in drinks. similar behavior be also observ for the 
“sushi” result where fine-grain featur of the sushi roll’ 
center be crucial to identifi the correct sushi recipe. 

6.3. analysi of the learn emb 

To gain further insight into our neural embedding, we 
perform a seri of qualit analysi experiments. We 
explor whether ani semant concept emerg in the neuron 
activ and whether the emb space have certain 
arithmet properties. 

neuron visualizations. through neural activ visual- 
izat we investig if ani semant concept emerg in the 

http://mturk.com 


all recip course-specif recip dish-specif recip 

dessert salad bread beverag soup-stew course-mean pasta pizza steak salmon smoothi hamburg ravioli sushi dish-mean 

human 81.6 ± 8.9 52.0 70.0 34.0 58.0 56.0 54.0 ± 13.0 54.0 48.0 58.0 52.0 48.0 46.0 54.0 58.0 52.2 ± 04.6 
joint-emb. onli 83.6 ± 3.0 76.0 68.0 38.0 24.0 62.0 53.6 ± 21.8 58.0 58.0 58.0 64.0 38.0 58.0 62.0 42.0 54.8 ± 09.4 
joint-emb.+semant 84.8 ± 2.7 74.0 82.0 56.0 30.0 62.0 60.8 ± 20.0 52.0 60.0 62.0 68.0 42.0 68.0 62.0 44.0 57.2 ± 10.1 

tabl 4: comparison with human perform on im2recip task. the mean result be highlight a bold for good 
visualization. note that on averag our method with semant regular perform good than averag amt worker. 

- 

- 

- 

+ 

+ 

+ 

= 

= 

= 

(a) imag 

- 

- 

- 

+ 

+ 

+ 

= 

= 

= 

(b) recip 

- 

- 

- 

+ 

+ 

+ 

= 

= 

= 

(c) cross-mod 

figur 6: arithmet use imag embed (left), recip embed (middle) and cross-mod arithmet between 
imag and recip embed (right). We repres the averag vector of a queri with the imag from it 4 near neighbors. 
In the case of the arithmet result, we show the near neighbor only. 

neuron in our emb vector despit not be explicitli 
train for that purpose. We pick the top activ images, 
ingredi lists, and cook instruct for a give neuron. 
then we use the methodolog introduc by zhou et al. [25] 
to visual imag region that contribut the most to the 
activ of specif unit in our learn visual embeddings. 
We appli the same procedur on the recip side to also obtain 
those ingredi and recip instruct to which certain 
unit react the most. figur 5 show the result for the same 
unit in both the imag and recip embedding. We find that 
certain unit display local semant align between 
the embed of the two modalities. 

semant vector arithmetic. differ work in the lit- 
eratur [15, 18] have use simpl arithmet oper 
to demonstr the capabl of their learn represen- 
tations. In the context of food recipes, one would expect that 
v(“chicken pizza”)−v(“pizza”)+v(“salad”) = v(“chicken 
salad”), where v repres the map into the emb 
space. We investig whether our learn embed have 
such properti by appli the previou equat templat 
to the averag vector of recip that contain the queri 
word in their title. We appli thi procedur in the imag and 
recip emb space and show result in figur 6(a) 
and 6(b), respectively. our find suggest that the learn 
embed have semant properti that translat to simpl 
geometr transform in the learn space. 

finally, we appli the same arithmet oper to em- 
bed across modalities. In particular, we explor the 
case of modifi a recip by linearli combin it imag 
emb with a varieti of text-origin embeddings. for 

example, give an imag of a chocol cake, we tri to trans- 
form it into a chocol cupcak by remov and add the 
mean recip embed of cake and cupcake, respectively. 
figur 6(c) show the results, which we find to be compa- 
rabl to those use embed within the same modality. 
thi suggest that the recip and imag embed learn 
in our model be semant aligned, which broach the 
possibl of applic in recip modif (e.g., ingre- 
dient replacement, calori adjustment) or even cross-mod 
generation. 

7. conclus 
In thi paper, we present recipe1m, the larg structur 

recip dataset to date, the im2recip problem, and neural em- 
bed model with semant regular which achiev 
impress result for the im2recip task. more generally, 
the method present here could be gain appli to 
other “recipes” like assembl instructions, tutorials, and in- 
dustrial processes. further, we hope that our contribut 
will support the creation of autom tool for food and 
recip understand and open door for mani less explor 
aspect of learn such a composit creativ and pre- 
dict visual outcom of action sequences. 

8. acknowledg 
thi work have be support by csail-qcri collab- 

orat project and the framework of project tec2013- 
43935-r and tec2016-75976-r, financ by the spanish 
ministerio de economia y competitividad and the european 
region develop fund (erdf). 



refer 
[1] L. bossard, M. guillaumin, and L. van gool. food-101– 

mine discrimin compon with random forests. In 
european confer on comput vision, page 446–461. 
springer, 2014. 1, 2 

[2] L. castrejon, Y. aytar, C. vondrick, H. pirsiavash, and A. tor- 
ralba. learn align cross-mod represent from 
weakli align data. In comput vision and pattern recog- 
nition (cvpr), 2016 ieee confer on. ieee, 2016. 5 

[3] J. donahue, Y. jia, O. vinyals, J. hoffman, N. zhang, 
E. tzeng, and T. darrell. decaf: A deep convolut acti- 
vation featur for gener visual recognition. arxiv preprint 
arxiv:1310.1531, 2013. 6 

[4] V. R. K. garimella, A. alfayad, and I. weber. social medium 
imag analysi for public health. In chi, page 5543–5547, 
2016. 1 

[5] K. he, X. zhang, S. ren, and J. sun. deep residu learn 
for imag recognition. arxiv preprint arxiv:1512.03385, 
2015. 1, 3 

[6] c.-w. N. jing-j chen. deep-bas ingredi recognit 
for cook recip retrival. acm multimedia, 2016. 2 

[7] A. karpathi and L. fei-fei. deep visual-semant align 
for gener imag descriptions. In proceed of the ieee 
confer on comput vision and pattern recognition, 
page 3128–3137, 2015. 6 

[8] Y. kawano and K. yanai. foodcam: A real-tim food recog- 
nition system on a smartphone. multimedia tool and appli- 
cations, 74(14):5263–5287, 2015. 1, 2 

[9] R. kiros, Y. zhu, R. salakhutdinov, R. zemel, A. torralba, 
R. urtasun, and S. fidler. skip-thought vectors. In nips, 
page 3294–3302, 2015. 3, 6 

[10] A. krizhevsky, I. sutskever, and G. E. hinton. imagenet 
classif with deep convolut neural networks. In 
nips, 2012. 1 

[11] T. kusmierczyk, C. trattner, and K. norvag. understand 
and predict onlin food recip product patterns. In 
hypertext, 2016. 2 

[12] Q. V. Le and T. mikolov. distribut represent of sen- 
tenc and documents. arxiv preprint arxiv:1405.4053, 2014. 
6 

[13] C. liu, Y. cao, Y. luo, G. chen, V. vokkarane, and Y. ma. 
deepfood: deep learning-bas food imag recognit for 
computer-aid dietari assessment. In intern confer- 
enc on smart home and health telematics, page 37–48. 
springer, 2016. 1 

[14] Y. mejova, S. abbar, and H. haddadi. fetish food in 
digit age: #foodporn around the world. In icwsm, page 
250–258, 2016. 1 

[15] T. mikolov, K. chen, G. corrado, and J. dean. effici 
estim of word represent in vector space. corr, 
abs/1301.3781, 2013. 3, 6, 8 

[16] A. myers, N. johnston, V. rathod, A. korattikara, A. gorban, 
N. silberman, S. guadarrama, G. papandreou, J. huang, and 
K. murphy. im2calories: toward an autom mobil vision 
food diary. In iccv, page 1233–1241, 2015. 1, 2 

[17] F. ofli, Y. aytar, I. weber, R. hammouri, and A. torralba. Is 
saki #delicious? the food percept gap on instagram and it 

relat to health. In proceed of the 26th intern 
confer on world wide web. intern world wide 
web confer steer committee, 2017. 1 

[18] A. radford, L. metz, and S. chintala. unsupervis represen- 
tation learn with deep convolut gener adversari 
networks. arxiv preprint arxiv:1511.06434, 2015. 8 

[19] O. russakovsky, J. deng, H. su, J. krause, S. satheesh, S. ma, 
Z. huang, A. karpathy, A. khosla, M. bernstein, et al. ima- 
genet larg scale visual recognit challenge. intern 
journal of comput vision, 115(3):211–252, 2015. 1 

[20] K. simonyan and A. zisserman. veri deep convolut 
network for large-scal imag recognition. arxiv preprint 
arxiv:1409.1556, 2014. 1, 3 

[21] I. sutskever, O. vinyals, and Q. V. le. sequenc to sequenc 
learn with neural networks. In nips, page 3104–3112, 
2014. 3 

[22] O. vinyals, A. toshev, S. bengio, and D. erhan. show and tell: 
A neural imag caption generator. In proceed of the ieee 
confer on comput vision and pattern recognition, 
page 3156–3164, 2015. 6 

[23] X. wang, D. kumar, N. thome, M. cord, and F. precioso. 
recip recognit with larg multimod food dataset. In 
icm workshops, page 1–6, 2015. 2 

[24] R. xu, L. herranz, S. jiang, S. wang, X. song, and R. jain. 
geoloc model for dish recognition. ieee trans. 
multimedia, 17(8):1187–1199, 2015. 2 

[25] B. zhou, A. khosla, A. lapedriza, A. oliva, and A. torralba. 
object detector emerg in deep scene cnns. intern 
confer on learn representations, 2015. 8 

[26] B. zhou, A. lapedriza, J. xiao, A. torralba, and A. oliva. 
learn deep featur for scene recognit use place 
database. In advanc in neural inform process sys- 
tems, page 487–495, 2014. 1 


