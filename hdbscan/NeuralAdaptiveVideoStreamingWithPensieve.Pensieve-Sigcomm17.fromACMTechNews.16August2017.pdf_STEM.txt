




















































neural adapt video stream with pensiev 


neural adapt video stream with pensiev 
hongzi mao, ravi netravali, mohammad alizadeh 

mit comput scienc and artifici intellig laboratori 
{hongzi,ravinet,alizadeh}@mit.edu 

abstract 
client-sid video player employ adapt bitrat (abr) algorithm 
to optim user qualiti of experi (qoe). despit the abund 
of recent propos schemes, state-of-the-art abr algorithm suffer 
from a key limitation: they use fix control rule base on simplifi 
or inaccur model of the deploy environment. As a result, 
exist scheme inevit fail to achiev optim perform 
across a broad set of network condit and qoe objectives. 

We propos pensieve, a system that gener abr algorithm 
use reinforc learn (rl). pensiev train a neural network 
model that select bitrat for futur video chunk base on obser- 
vation collect by client video players. pensiev do not reli 
on pre-program model or assumpt about the environment. 
instead, it learn to make abr decis sole through observ 
of the result perform of past decisions. As a result, pensiev 
automat learn abr algorithm that adapt to a wide rang of 
environ and qoe metrics. We compar pensiev to state-of-the- 
art abr algorithm use trace-driven and real world experi 
span a wide varieti of network conditions, qoe metrics, and 
video properties. In all consid scenarios, pensiev outperform 
the best state-of-the-art scheme, with improv in averag qoe 
of 12%–25%. pensiev also gener well, outperform exist 
scheme even on network for which it be not explicitli trained. 

cc concepts: inform systems→multimedia streaming; network 
→ network resourc allocation; comput methodologies→ reinforce- 
ment learn 

keywords: bitrat adaptation, video streaming, reinforc learn 
acm refer format: hongzi mao, ravi netravali, mohammad alizadeh 
mit comput scienc and artifici intellig laboratory. 2017. neural 
adapt video stream with pensieve. In proceed of sigcomm ’17, 
august 21-25, 2017, lo angeles, ca, usa, 14 pags. 
doi: http://dx.doi.org/10.1145/3098822.3098843 

1 introduct 
recent year have see a rapid increas in the volum of http- 
base video stream traffic [7, 39]. concurr with thi increas 
have be a steadi rise in user demand on video quality. mani studi 
have show that user will quickli abandon video session if the 
qualiti be not sufficient, lead to signific loss in revenu for 

permiss to make digit or hard copi of all or part of thi work for person or 
classroom use be grant without fee provid that copi be not make or distribut 
for profit or commerci advantag and that copi bear thi notic and the full citat 
on the first page. copyright for compon of thi work own by other than the 
author(s) must be honored. abstract with credit be permitted. To copi otherwise, or 
republish, to post on server or to redistribut to lists, requir prior specif permiss 
and/or a fee. request permiss from permissions@acm.org. 
sigcomm ’17, lo angeles, ca, usa 
© 2017 copyright held by the owner/author(s). public right licens to acm. 
978-1-4503-4653-5/17/08. . . $15.00 
doi: http://dx.doi.org/10.1145/3098822.3098843 

content provid [12, 25]. nevertheless, content provid continu 
to struggl with deliv high-qual video to their viewers. 

adapt bitrat (abr) algorithm be the primari tool that con- 
tent provid use to optim video quality. these algorithm run 
on client-sid video player and dynam choos a bitrat for 
each video chunk (e.g., 4-second block). abr algorithm make bi- 
trate decis base on variou observ such a the estim 
network throughput and playback buffer occupancy. their goal be 
to maxim the user’ qoe by adapt the video bitrat to the 
underli network conditions. however, select the right bitrat 
can be veri challeng due to (1) the variabl of network through- 
put [18, 42, 49, 52, 53]; (2) the conflict video qoe requir 
(high bitrate, minim rebuffering, smoothness, etc.); (3) the cascad- 
ing effect of bitrat decis (e.g., select a high bitrat may 
drain the playback buffer to a danger level and caus rebuff 
in the future); and (4) the coarse-grain natur of abr decisions. 
We elabor on these challeng in §2. 

the major of exist abr algorithm (§7) develop fix con- 
trol rule for make bitrat decis base on estim network 
throughput (“rate-based” algorithm [21, 42]), playback buffer size 
(“buffer-based” scheme [19, 41]), or a combin of the two 
signal [26]. these scheme requir signific tune and do not 
gener to differ network condit and qoe objectives. the 
state-of-the-art approach, mpc [51], make bitrat decis by solv- 
ing a qoe maxim problem over a horizon of sever futur 
chunks. By optim directli for the desir qoe objective, mpc 
can perform good than approach that use fix heuristics. how- 
ever, mpc’ perform reli on an accur model of the system 
dynamics—particularly, a forecast of futur network throughput. 
As our experi show, thi make mpc sensit to throughput 
predict error and the length of the optim horizon (§3). 

In thi paper, we propos pensieve,1 a system that learn abr 
algorithm automatically, without use ani pre-program con- 
trol rule or explicit assumpt about the oper environment. 
pensiev us modern reinforc learn (rl) techniqu [27, 
30, 43] to learn a control polici for bitrat adapt pure through 
experience. dure training, pensiev start know noth about 
the task at hand. It then gradual learn to make good abr de- 
cision through reinforcement, in the form of reward signal that 
reflect video qoe for past decisions. 

pensieve’ learn techniqu mine inform about the actual 
perform of past choic to optim it control polici for the 
characterist of the network. for example, pensiev can learn how 
much playback buffer be necessari to mitig the risk of rebuff 
in a specif network, base on the network’ inher throughput 
variability. Or it can learn how much to reli on throughput versu 
buffer occup signals, or how far into the futur to plan it deci- 
sion automatically. By contrast, approach that use fix control 

1A pensiev be a devic use in harri potter [38] to review memories. 



sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa H. mao et al. 

rule or simplifi network model be unabl to optim their bi- 
trate decis base on all avail inform about the oper 
environment. 

pensiev repres it control polici a a neural network that 
map “raw” observ (e.g., throughput samples, playback buffer 
occupancy, video chunk sizes) to the bitrat decis for the next 
chunk. the neural network provid an express and scalabl way 
to incorpor a rich varieti of observ into the control policy.2 

pensiev train thi neural network use a3c [30], a state-of-the-art 
actor-crit RL algorithm. We describ the basic train algorithm 
and present extens that allow a singl neural network model to 
gener to video with differ properties, e.g., the number of 
encod and their bitrat (§4). 

To train it models, pensiev us simul over a larg corpu 
of network traces. pensiev us a fast and simpl chunk-level simu- 
lator. while pensiev could also train use packet-level simulations, 
emulations, or data collect from live video client (§6), the chunk- 
level simul be much faster and allow pensiev to “experience” 
100 hour of video download in onli 10 minutes. We show that pen- 
sieve’ simul faith model video stream with live video 
players, provid that the transport stack be configur to achiev 
close to the true network capac (§4.1). 

We evalu pensiev use a full system implement (§4.4). 
our implement deploy pensieve’ neural network model on an 
abr server, which video client queri to get the bitrat to use for the 
next chunk; client request includ observ about throughput, 
buffer occupancy, and video properties. thi design remov the 
burden of perform neural network comput on video clients, 
which may have limit comput power, e.g., tvs, mobil de- 
vices, etc. (§6). 

We compar pensiev to state-of-the-art abr algorithm use 
a broad set of network condit (both with trace-bas emul 
and in the wild) and qoe metric (§5.2). We find that in all con- 
sider scenarios, pensiev rival or outperform the best exist 
scheme, with averag qoe improv rang from 12%–25%. 
additionally, our result show pensieve’ abil to gener to 
unseen network condit and video properties. for example, on 
both broadband and hsdpa networks, pensiev be abl to outper- 
form all exist abr algorithm by train sole with a synthet 
dataset. finally, we present result which highlight pensieve’ low 
overhead and lack of sensit to system parameters, e.g., in the 
neural network (§5.4). 

2 background 
http-base adapt stream (standard a dash [2]) be the 
predomin form of video deliveri today. By transmit video 
use http, content provid be abl to leverag exist cdn 
infrastructur and maintain simplifi (stateless) backends. further, 
http be compat with a multitud of client-sid applic such 
a web browser and mobil applications. 

In dash systems, video be store on server a multipl chunks, 
each of which repres a few second of the overal video playback. 
each chunk be encod at sever discret bitrates, where a high 

2A few prior scheme [6, 8, 9, 47] have appli RL to video streaming. but these 
scheme use basic “tabular” RL approach [43]. As a result, they must reli on simplifi 
network model and perform poorli in real network conditions. We discu these 
scheme further in §5.4 and §7. 

video 
server 

cdn 

throughput 
predictor 

playback 
buffer 

abr 
control 

render 
video chunk 

chunk 
info 

throughput estim 

buffer occup 

video player 

figur 1: An overview of http adapt video streaming. 

bitrat impli a high qualiti and thu a larg chunk size. chunk 
across bitrat be align to support seamless qualiti transitions, 
i.e., a video player can switch to a differ bitrat at ani chunk 
boundari without fetch redund bit or skip part of the 
video. 

figur 1 illustr the end-to-end process of stream a video 
over http today. As shown, a player emb in a client applica- 
tion first send a token to a video servic provid for authentication. 
the provid respond with a manifest file that direct the client 
to a cdn host the video and list the avail bitrat for the 
video. the client then request video chunk one by one, use an 
adapt bitrat (abr) algorithm. these algorithm use a varieti of 
differ input (e.g., playback buffer occupancy, throughput mea- 
surements, etc.) to select the bitrat for futur chunks. As chunk 
be downloaded, they be play back to the client; note that play- 
back of a give chunk cannot begin until the entir chunk have be 
downloaded. 

challenges: the polici employ by abr algorithm heavili 
influenc video stream performance. however, these algorithm 
face four primari practic challenges: 
(1) network condit can fluctuat over time and can vari signifi- 

cantli across environments. thi complic bitrat select 
a differ scenario may requir differ weight for input 
signals. for example, on time-vari cellular links, throughput 
predict be often inaccur and cannot account for sudden fluc- 
tuation in network bandwidth—inaccur predict can lead 
to underutil network (lower video quality) or inflat down- 
load delay (rebuffering). To overcom this, abr algorithm 
must priorit more stabl input signal like buffer occup 
in these scenarios. 

(2) abr algorithm must balanc a varieti of qoe goal such a 
maxim video qualiti (i.e., high averag bitrate), mini- 
mize rebuff event (i.e., scenario where the client’ play- 
back buffer be empty), and maintain video qualiti smooth 
(i.e., avoid constant bitrat fluctuations). however, mani of 
these goal be inher conflict [3, 18, 21]. for example, 
on network with limit bandwidth, consist request 
chunk encod at the high possibl bitrat will maxim 
quality, but may increas rebuff rates. conversely, on vary- 
ing networks, choos the high bitrat that the network can 
support at ani time could lead to substanti qualiti fluctuation, 
and henc degrad smoothness. To further complic matters, 
prefer among these qoe factor vari significantli across 
user [23, 31, 32, 34]. 

(3) bitrat select for a give chunk can have cascad effect 
on the state of the video player. for example, select a high 
bitrat may deplet the playback buffer and forc subsequ 



neural adapt video stream with pensiev sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa 

buffer 

abr agent 

state 

neural network bitrat 
240p 
480p 
720p 
1080p 

qoe metricreward 

client-sid network and video player measur 

bandwidth 

bit rate 

720p 

figur 2: appli reinforc learn to bitrat adapta- 
tion. 

chunk to be download at low bitrat (to avoid rebuffering). 
additionally, a give bitrat select will directli influenc the 
next decis when smooth be considered—abr algorithm 
will be less inclin to chang bitrates. 

(4) the control decis avail to abr algorithm be coarse- 
grain a they be limit to the avail bitrat for a give 
video. thus, there exist scenario where the estim through- 
put fall just below one bitrate, but well abov the next avail 
bitrate. In these cases, the abr algorithm must decid whether 
to priorit high qualiti or the risk of rebuffering. 

3 learn abr algorithm 
In thi paper, we consid a learning-bas approach to gener 
abr algorithms. unlik approach which use preset rule in the 
form of fine-tun heuristics, our techniqu attempt to learn an 
abr polici from observations. specifically, our approach be base on 
reinforc learn (rl). RL consid a gener set in which 
an agent interact with an environment. At each time step t , the agent 
observ some state st , and choos an action at . after appli 
the action, the state of the environ transit to st+1 and the 
agent receiv a reward rt . the goal of learn be to maxim 
the expect cumul discount reward: E 

[∑∞ 
t=0 γ 

t rt 
] 
, where 

γ ∈ (0, 1] be a factor discount futur rewards. 
figur 2 summar how RL can be appli to bitrat adaptation. 

As shown, the decis polici guid the abr algorithm be not 
handcrafted. instead, it be deriv from train a neural network. the 
abr agent observ a set of metric includ the client playback 
buffer occupancy, past bitrat decisions, and sever raw network 
signal (e.g., throughput measurements) and feed these valu to the 
neural network, which output the action, i.e., the bitrat to use for 
the next chunk. the result qoe be then observ and pass back 
to the abr agent a a reward. the agent us the reward inform 
to train and improv it neural network model. more detail about 
the specif train algorithm we use be provid in §4.2. 

To motiv learning-bas abr algorithms, we now provid 
two exampl where exist techniqu that reli on fix heurist 
can perform poorly. We choos these exampl for illustr pur- 
poses. We do not claim that they be indic of the perform 
gain with learn in realist network scenarios. We perform thor- 
ough quantit evalu compar learning-gener abr 
algorithm to exist scheme in §5.2. 

In these examples, we compar rl-gener abr algorithm to 
mpc [51]. mpc us both throughput estim and observ 
about buffer occup to select bitrat that maxim a give qoe 
metric across a futur chunk horizon. here we consid robustmpc, 
a version of mpc that be configur to use a horizon of 5 chunks, 

and a conserv throughput estim which normal the de- 
fault throughput predict with the max predict error over the 
past 5 chunks. As the mpc paper shows, and our result validate, 
robustmpc’ conserv throughput predict significantli im- 
prof perform over default mpc, and achiev a high level 
of perform in most case (§5.2). however, heurist like ro- 
bustmpc’ throughput predict requir care tune and can 
backfir when their design assumpt be violated. 

exampl 1: the first exampl consid a scenario in which the 
network throughput be highli variable. figur 3a compar the net- 
work throughput specifi by the input trace with the throughput 
estim use by robustmpc. As shown, robustmpc’ estim 
be overli cautious, hover around 2 mbp instead of the averag 
network throughput of roughli 4.5 mbps. these inaccur through- 
put predict prevent robustmpc from reach high bitrat even 
though the occup of the playback buffer continu increases. 
In contrast, the rl-gener algorithm be abl to properli ass 
the high averag throughput (despit fluctuations) and switch to the 
high avail bitrat onc it have enough cushion in the playback 
buffer. the rl-gener algorithm consid here be train on 
a larg corpu of real network trace (§5.1), not the synthet trace 
in thi experiment. yet, it be abl to make the appropri decision. 

exampl 2: In our second example, both robustmpc and the rl- 
gener abr algorithm optim for a new qoe metric which be 
gear toward user who strongli prefer HD video. thi metric 
assign high reward to HD bitrat and low reward to all other bitrat 
(detail in tabl 1), while still favor smooth and penal 
rebuffering. To optim for thi metric, an abr algorithm should 
attempt to build the client’ playback buffer to a high enough level 
such that the player can switch up to and maintain an HD bitrat level. 
use thi approach, the video player can maxim the amount of 
time spent stream HD video, while minim rebuff time 
and bitrat transitions. however, perform well in thi scenario 
requir long term plan sinc at ani give instant, the penalti of 
select a high bitrat (hd or not) may be incur mani chunk 
in the futur when the buffer cannot support multipl HD downloads. 

figur 3b illustr the bitrat select make by each of these 
algorithms, and the effect that these decis have on the playback 
buffer. note that robustmpc and the rl-gener algorithm be 
both configur to optim for thi new qoe metric. As shown, 
robustmpc be unabl to appli the aforement policy. instead, 
robustmpc maintain a medium-s playback buffer and request 
chunk at bitrat that fall between the low level (300 kbps) 
and the low HD level (1850 kbps). the reason be that, despit 
be tune to consid a horizon of futur chunk at everi step, 
robustmpc fail to plan far enough into the future. In contrast, 
the rl-gener abr algorithm be abl to activ implement 
the polici outlin above. It quickli grow the playback buffer by 
request chunk at 300 kbps, and then immedi jump to the 
HD qualiti of 1850 kbps; it be abl to then maintain thi level for 
nearli 80 seconds, therebi ensur qualiti smoothness. 

summary: robustmpc have difficulti (1) factor throughput fluc- 
tuation and predict error into it decisions, and (2) choos the 
appropri optim horizon. these defici exist becaus 
mpc lack an accur model of network dynamics—thu it reli on 



sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa H. mao et al. 

0 

1 

2 

3 

4 
Bi 
t r 
at 
e 
(M 

bp 
s) 

pensiev robustmpc 

0 

20 

40 

0 3 0 6 0 9 0 1 2 0 

Bu 
ffe 

r s 
ize 

(s 
ec 
) pensiev robustmpc 

1 

3 

5 

7 

9 

0 30 60 90 120 

Th 
ro 
ug 
hp 

ut 
(M 

bp 
s) 

time (sec) 

true bandwidth robustmpc estim 

(a) synthet network. 

0 
0.4 
0.8 
1.2 
1.6 
2 

Bi 
t r 
at 
e 
(M 

bp 
s) 

pensiev robustmpc 

0 

20 

40 

0 4 0 8 0 1 2 0 1 6 0 

Bu 
ffe 

r s 
ize 

(s 
ec 
) pensiev robustmpc 

0.3 

0.6 

0.9 

1.2 

1.5 

0 40 80 120 160 

Th 
ro 
ug 
hp 

ut 
(M 

bp 
s) 

time (sec) 

true bandwidth robustmpc estim 

(b) hsdpa network. 

figur 3: profil bitrat selections, buffer occupancy, and throughput estim with robustmpc [51] and pensieve. 

simpl and sub-optim heurist such a conserv throughput 
predict and a small optim horizon. more generally, ani 
abr algorithm that reli on fix heurist or simplifi system 
model suffer from these limitations. By contrast, rl-gener 
algorithm learn from actual perform result from differ 
decisions. By incorpor thi inform into a flexibl neural 
network policy, rl-gener abr algorithm can automat 
optim for differ network characterist and qoe objectives. 

4 design 
In thi section, we describ the design and implement of pen- 
sieve, a system that gener rl-base abr algorithm and appli 
them to video stream sessions. We start by explain the train 
methodolog (§4.1) and algorithm (§4.2) underli pensieve. We 
then describ an enhanc to the basic train algorithm, which 
enabl pensiev to support differ video use a singl model 
(§4.3). finally, we explain the implement detail of pensiev 
and how it appli learn model to real stream session (§4.4). 

4.1 train methodolog 
the first step of pensiev be to gener an abr algorithm use 
RL (§3). To do this, pensiev run a train phase in which the 
learn agent explor a video stream environment. ideally, 
train would occur use actual video stream clients. however, 
emul the standard video stream environ entail use a 
web browser to continu download video chunks. thi approach 
be slow, a the train algorithm must wait until all of the chunk in 
a video be complet download befor updat it model. 

To acceler thi process, pensiev train abr algorithm in a 
simpl simul environ that faith model the dynam 
of video stream with real client applications. pensieve’ simul 
maintain an intern represent of the client’ playback buffer. 
for each chunk download, the simul assign a download time 
that be sole base on the chunk’ bitrat and the input network 

throughput traces. the simul then drain the playback buffer by 
the current chunk’ download time, to repres video playback dur- 
ing the download, and add the playback durat of the download 
chunk to the buffer. the simul care keep track of rebuffer- 
ing event that aris a the buffer occup changes, i.e., scenario 
where the chunk download time exce the buffer occup at 
the start of the download. In scenario where the playback buffer 
cannot accommod video from an addit chunk download, 
pensieve’ simul paus request for 500 m befor retrying.3 

after each chunk download, the simul pass sever state obser- 
vation to the RL agent for processing: the current buffer occupancy, 
rebuff time, chunk download time, size of the next chunk (at 
all bitrates), and the number of remain chunk in the video. We 
describ how thi input be use by the RL agent in more detail in 
§4.2. use thi chunk-level simulator, pensiev can “experience” 
100 hour of video download in onli 10 minutes. 

though model the applic layer semant of client video 
player be straightforward, faith simul be complic by 
intricaci at the transport layer. specifically, video player may 
not request futur chunk a soon a a chunk download completes, 
e.g., becaus the playback buffer be full. such delay can trigger 
the underli tcp connect to revert to slow start, a behavior 
know a slow-start-restart [4]. slow start may in turn prevent the 
video player from fulli use the avail bandwidth, particularli 
for small chunk size (low bitrates). thi behavior make simul 
challeng a it inher tie network throughput to the abr 
algorithm be used, e.g., scheme that fill buffer quickli will 
experi more slow start phase and thu less network utilization. 

To verifi thi behavior, we load the test video describ in §5.1 
over an emul 6 mbp link use four abr algorithms, each of 
which continu request chunk at a singl bitrate. We load the 
video with each scheme twice, both with slow-start-restart enabl 

3thi be the default request retri rate use by dash player [2]. 



neural adapt video stream with pensiev sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa 

3 

4 

5 

6 

0 30 60 90 

Th 
ro 
ug 
hp 

ut 
(M 

bp 
s) 

time (sec) 

2.85 mbp 1.2 mbp 0.75 mbp 0.3mbp 

(a) tcp slow start restart enabl 

5 

6 

0 30 60 90th 
ro 
ug 
hp 

ut 
(M 

bp 
s) 

time (sec) 

2.85 mbp 1.2 mbp 0.75 mbp 0.3 mbp 

(b) tcp slow start restart disabl 

figur 4: profil the throughput usag per-chunk of commod- 
iti video player with and without tcp slow start restart. 

and disabled.4 figur 4 show the throughput usag dure chunk 
download for each bitrat in both scenarios. As shown, with slow- 
start-restart enabled, the throughput depend on the bitrat of the 
chunk; abr algorithm use low bitrat (smaller chunk sizes) 
achiev less throughput per chunk. however, throughput be consist 
and match the avail bandwidth (6 mbps) for differ bitrat 
if we disabl slow-start-restart. 

pensieve’ simul assum that the throughput specifi by 
the trace be entir use by each chunk download. As the abov 
result show, thi can be achiev by disabl slow-start-restart on 
the video server. disabl slow-start-restart could increas traffic 
burstiness, but recent standard effort be tackl the same problem 
for video stream more grace by pace the initi burst from 
tcp follow an idl period [13, 17]. 

while it be possibl to use a more accur simul (e.g., packet- 
level) to train pensieve, in the end, no simul can captur all 
real world system artifact with 100% accuracy. however, we find 
that pensiev can learn veri high qualiti abr algorithm (§5.2) 
use imperfect simulations, a long a it experi a larg enough 
varieti of network condit dure training. thi be a consequ 
of pensieve’ strong gener abil (§5.3). 

4.2 basic train algorithm 
We now describ our train algorithms. As show in figur 5, 
pensieve’ train algorithm us a3c [30], a state-of-the-art actor- 
critic method which involv train two neural networks. the 
detail function of these network be explain below. 

inputs: after the download of each chunk t , pensieve’ learn 
agent take state input st = ( ~xt , ~τt , ~nt ,bt , ct , lt ) to it neural net- 
works. ~xt be the network throughput measur for the past k 
video chunks; ~τt be the download time of the past k video chunks, 
which repres the time interv of the throughput measurements; 
~nt be a vector of m avail size for the next video chunk; bt be 
the current buffer level; ct be the number of chunk remain in the 
video; and lt be the bitrat at which the last chunk be downloaded. 
4in linux, the net.ipv4.tcp_slow_start_after_idl paramet can be 
use to set thi configuration. 

xt xt-1 

n1 n2 nm 

bt 

ct 

lt 

past chunk throughput 

next chunk size 

current buffer size 

number of chunk left 

last chunk bit rate 

state st 
actor network 

critic network 

polici 
πθ(st, at) 

valu 
vπθ(st) 

1d-cnn 

1d-cnn 

1d-cnn 

1d-cnn 

τt τt-1 

past chunk download time 

1d-cnn 

1d-cnn 

xt-k+1 

τt-k+1 

figur 5: the actor-crit algorithm that pensiev us to gen- 
erat abr polici (describ in §4.4). 

policy: upon receiv st , pensieve’ RL agent need to take an 
action at that correspond to the bitrat for the next video chunk. 
the agent select action base on a policy, defin a a probabl 
distribut over action π : π (st ,at ) → [0, 1]. π (st ,at ) be the 
probabl that action at be take in state st . In practice, there be 
intract mani {state, action} pairs, e.g., throughput estim and 
buffer occup be continu real numbers. To overcom this, 
pensiev us a neural network (nn) [15] to repres the polici 
with a manag number of adjust parameters, θ , which we 
refer to a polici parameters. use θ , we can repres the polici 
a πθ (st ,at ). nn have recent be appli success to solv 
large-scal RL task [27, 29, 40]. An advantag of nn be that they 
do not need hand-craft featur and can be appli directli to 
“raw” observ signals. the actor network in figur 5 depict how 
pensiev us an NN to repres an abr policy. We describ how 
we design the specif architectur of the NN in §5.3. 

polici gradient training: after appli each action, the simul 
environ provid the learn agent with a reward rt for that 
chunk. recal from §3 that the primari goal of the RL agent be 
to maxim the expect cumul (discounted) reward that it 
receiv from the environment. thus, the reward be set to reflect the 
perform of each chunk download accord to the specif qoe 
metric we wish to optimize. see §5 for exampl of qoe metrics. 

the actor-crit algorithm use by pensiev to train it polici be 
a polici gradient method [44]. We highlight the key step of the 
algorithm, focu on the intuition. the key idea in polici gradient 
method be to estim the gradient of the expect total reward 
by observ the trajectori of execut obtain by follow 
the policy. the gradient of the cumul discount reward with 
respect to the polici parameters, θ , can be comput a [30]: 

∇θeπθ 
 

∞∑ 
t=0 

γ t rt 

 
= eπθ 

[ 
∇θ logπθ (s,a)aπθ (s,a) 

] 
. (1) 

aπθ (s,a) be the advantag function, which repres the differ 
in the expect total reward when we determinist pick action 
a in state s, compar with the expect reward for action drawn 
from polici πθ . the advantag function encod how much good a 
specif action be compar to the “averag action” take accord 
to the policy. 



sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa H. mao et al. 

In practice, the agent sampl a trajectori of bitrat decis and 
us the empir comput advantag a(st ,at ), a an unbias 
estim of aπθ (st ,at ). each updat of the actor network paramet 
θ follow the polici gradient, 

θ ← θ + α 
∑ 
t 
∇θ logπθ (st ,at )a(st ,at ), (2) 

where α be the learn rate. the intuit behind thi updat rule be a 
follows. the direct ∇θ logπθ (st ,at ) specifi how to chang the 
polici paramet in order to increas πθ (st ,at ) (i.e., the probabl 
of action at at state st ). equat 2 take a step in thi direction. the 
size of the step depend on the valu of the advantag for action at 
in state st . thus, the net effect be to reinforc action that empir 
lead to good returns. 

To comput the advantag a(st ,at ) for a give experience, we 
need an estim of the valu function, vπθ (s )—the expect total 
reward start at state s and follow the polici πθ . the role of 
the critic network in figur 5 be to learn an estim of vπθ (s ) from 
empir observ rewards. We follow the standard tempor 
differ method [43] to train the critic network paramet θv , 

θv ← θv − α ′ 
∑ 
t 
∇θv 

( 
rt + γV 

πθ (st+1;θv ) −V πθ (st ;θv ) 
)2 
, (3) 

where V πθ (·;θv ) be the estim of vπθ (·), output by the critic net- 
work, and α ′ be the learn rate for the critic. for an experi 
(st ,at , rt , st+1) (i.e., take action at in state st , receiv reward rt , and 
transit to st+1), the advantag a(st ,at ) can now be estim a 
rt + γV 

πθ (st+1;θv ) −V πθ (st ;θv ). see [24] for more details. 
It be import to note that the critic network mere help to train 

the actor network. post-training, onli the actor network be requir 
to execut the abr algorithm and make bitrat decisions. 

finally, we must ensur that the RL agent explor the action 
space adequ dure train to discov good policies. one 
common practic to encourag explor be to add an entropi 
regular term to the actor’ updat rule [30]; thi can be crit- 
ical in help the learn agent converg to a good polici [50]. 
concretely, we modifi equat 2 to be, 

θ ← θ + α 
∑ 
t 
∇θ logπθ (st ,at )a(st ,at ) + β∇θh (πθ (·|st )), (4) 

where H (·) be the entropi of the polici (the probabl distribut 
over actions) at each time step. thi term encourag explor 
by push θ in the direct of high entropy. the paramet β be 
set to a larg valu at the start of train (to encourag exploration) 
and decreas over time to emphas improv reward (§4.4). 

the detail deriv and pseudocod can be found in [30] (§4 
and algorithm s3). 

parallel training: To further enhanc and speed up training, pen- 
siev spawn multipl learn agent in parallel, a suggest by the 
a3c paper [30]. By default, pensiev us 16 parallel agents. each 
learn agent be configur to experi a differ set of input 
paramet (e.g., network traces). however, the agent continu 
send their {state, action, reward} tupl to a central agent, which 
aggreg them to gener a singl abr algorithm model. for 
each sequenc of tupl that it receives, the central agent us the 
actor-crit algorithm to comput a gradient and perform a gradient 
descent step (equat (3) and (4)). the central agent then updat 
the actor network and push out the new model to the agent which 

n1 
n2 
0 

n3 

actor network 

p1 
p2 
0 

p3 
0 

softm 
ax 

mask 
(1 1 0 1 0) 

0N 
ex 

t c 
hu 

nk 
s 

iz 
e 

xtbtctlt τt 

figur 6: modif to the state input and the softmax output 
to support multipl videos. 

sent that tuple. note that thi can happen asynchron among all 
agents, i.e., there be no lock between agent [36]. 

choic of algorithm: A varieti of differ algorithm could be 
use to train the learn agent in the abstract RL framework de- 
scribe abov (e.g., dqn [29], reinforc [44], etc.). In our 
design, we chose to use a3c [30] becaus (1) to the best of our 
knowledge, it be the state-of-art and it have be success appli 
to mani other concret learn problem [20, 48, 50]; and (2) in 
the video stream application, the asynchron parallel train 
framework support onlin train in which mani user concur- 
rentli send their experi feedback to the agent. We also compar 
pensiev with previou tabular q-learn scheme [6] in §5.4. 

4.3 enhanc for multipl video 
the basic algorithm describ in §4.2 have some practic issues. the 
primari challeng be that video can be encod at differ bitrat 
level and may have divers chunk size due to variabl bitrat en- 
cod [41], e.g., chunk size for 720p video be not ident across 
videos. handl thi variat would requir each neural network 
to take a variabl size set of input and produc a variabl size set 
of outputs. the naiv solut to support a broad rang of video 
be to train a model for each possibl set of video properties. unfortu- 
nately, thi solut be not scalable. To overcom this, we describ 
two enhanc to the basic algorithm that enabl pensiev to 
gener a singl model to handl multipl video (figur 6). 

first, we pick canon input and output format that span the 
maximum number of bitrat level we expect to see in practice. for 
example, a rang of 13 level cover the entir dash refer 
client video list [11]. then, to determin the input state for a specif 
video, we take the chunk size and map them to the index which have 
the closest bitrate. the remain input states, which pertain to the 
bitrat that the video do not support, be zero out. for example, 
in figur 6, chunk size (n1,n2,n3) be map to the correspond 
indices, while the remain input valu be fill with zeroes. 

the second chang pertain to how the output of the actor net- 
work be interpreted. for a give video, we appli a mask to the output 
of the final softmax [5] layer in the actor network, such that the 
output probabl distribut be onli over the bitrat that the video 
actual supports. formally, the mask be present by a 0-1 vec- 
tor [m1,m2, ...,mk ], and the modifi softmax for the NN output 
[z1, z2, ..., zk ] will be 

pi = 
mie 

zi∑ 
jmje 

zj , (5) 

where pi be the normal probabl for action i. with thi mod- 
ification, the output probabl be still a continu function of 



neural adapt video stream with pensiev sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa 

the network parameters. the reason be that the mask valu {mi } 
be independ of the network parameters, and be onli a function 
of the input video. As a result, the standard back-propag of 
the gradient in the NN still hold and the train techniqu estab- 
lish in §4.2 can be appli without modification. We evalu the 
effect of these modif in more detail in §5.4. 

4.4 implement 
To gener abr algorithms, pensiev pass k = 8 past bandwidth 
measur to a 1D convolut layer (cnn) with 128 filters, 
each of size 4 with stride 1. next chunk size be pass to anoth 
1d-cnn with the same shape. result from these layer be then 
aggreg with other input in a hidden layer that us 128 neuron 
to appli the softmax function (figur 5). the critic network us 
the same NN structure, but it final output be a linear neuron (with 
no activ function). dure training, we use a discount factor 
γ = 0.99, which impli that current action will be influenc by 
100 futur steps. the learn rate for the actor and critic be con- 
figur to be 10−4 and 10−3, respectively. additionally, the entropi 
factor β be control to decay from 1 to 0.1 over 105 iterations. We 
keep all these hyperparamet fix throughout our experiments. 
while some tune be useful, we found that pensiev perform well 
for a wide rang of hyperparamet values. thu we do not use 
sophist hyperparamet tune method [14]. We implement 
thi architectur use tensorflow [1]. for compatibility, we lever- 
age the tflearn deep learn library’ tensorflow api [46] to 
declar the neural network dure both train and testing. 

onc pensiev have gener an abr algorithm use it simula- 
tor, it must appli the model’ rule to real video stream sessions. 
To do this, pensiev run on a standalon abr server, implement 
use the python basehttpserver. client request be modifi 
to includ addit inform about the previou chunk down- 
load and the video be stream (§4.2). By collect inform 
through client requests, pensieve’ server and abr algorithm can 
remain stateless while still benefit from observ that can 
sole be collect in client video players. As client request for indi- 
vidual chunk arriv at the video server, pensiev feed the provid 
observ through it actor NN model and respond to the video 
client with the bitrat level to use for the next chunk download; the 
client then contact the appropri cdn to fetch the correspond 
chunk. It be import to note that pensieve’ abr algorithm could 
also oper directli insid video players. We evalu the overhead 
that a server-sid deploy have on video qoe in §5.4, and discu 
other deploy model in more detail in §6. 

5 evalu 
In thi section, we experiment evalu pensieve. our experi- 
ment cover a broad set of network condit (both trace-bas 
and in the wild) and qoe metrics. our result answer the follow 
questions: 
(1) how do pensiev compar to state-of-the-art abr algorithm 

in term of video qoe? We find that, in all of the consid 
scenarios, pensiev be abl to rival or outperform the best ex- 
ist scheme, with averag qoe improv rang from 
12.1%–24.6% (§5.2); figur 7 provid a summary. 

(2) Do model learn by pensiev gener to new network con- 
dition and videos? We find that pensieve’ abr algorithm be 
abl to maintain high level of perform both in the presenc 
of new network condit and new video properti (§5.3). 

(3) how sensit be pensiev to variou paramet such a the neu- 
ral network architectur and the latenc between the video client 
and abr server? our experi suggest that perform 
be larg unaffect by these paramet (tabl 2 and 3). for 
example, appli 100 m rtt valu between client and the 
pensiev server reduc averag qoe by onli 3.5% (§5.4). 

5.1 methodolog 

network traces: To evalu pensiev and state-of-the-art abr 
algorithm on realist network conditions, we creat a corpu of 
network trace by combin sever public datasets: a broadband 
dataset provid by the fcc [10] and a 3g/hsdpa mobil dataset 
collect in norway [37]. the fcc dataset contain over 1 million 
throughput traces, each of which log the averag throughput over 
2100 seconds, at a 5 second granularity. We gener 1000 trace 
for our corpus, each with a durat of 320 seconds, by concaten 
randomli select trace from the “web browsing” categori in the 
august 2016 collection. the hsdpa dataset compris 30 minut 
of throughput measurements, gener use mobil devic that 
be stream video while in transit (e.g., via bus, train, etc.). To 
match the durat of the fcc trace includ in our corpus, we 
gener 1000 trace (each span 320 seconds) use a slide 
window across the hsdpa dataset. To avoid scenario where bitrat 
select be trivial, i.e., situat where pick the maximum bitrat 
be alway the optim solution, or where the network cannot support 
ani avail bitrat for an extend period, we onli consid 
origin trace whose averag throughput be less than 6 mbps, and 
whose minimum throughput be abov 0.2 mbps. We reformat 
throughput trace from both dataset to be compat with the 
mahimahi [33] network emul tool. unless otherwis noted, 
we use a random sampl of 80% of our corpu a a train set 
for pensieve; we use the remain 20% a a test set for all abr 
algorithms. all in all, our test set compris of over 30 hour of 
network traces. 

adapt algorithms: We compar pensiev to the follow 
algorithm which collect repres the state-of-the-art in bitrat 
adaptation: 
(1) buffer-bas (bb): mimic the buffer-bas algorithm describ 

by huang et al. [19] which us a reservoir of 5 second and a 
cushion of 10 seconds, i.e., it select bitrat with the goal of 
keep the buffer occup abov 5 seconds, and automati- 
calli choos the high avail bitrat if the buffer occup 
exce 15 seconds. 

(2) rate-bas (rb): predict throughput use the harmon mean 
of the experienc throughput for the past 5 chunk downloads. 
It then select the high avail bitrat that be below the 
predict throughput. 

(3) bola [41]: us lyapunov optim to select bitrat sole 
consid buffer occup observations. We use the bola 
implement in dash.j [2]. 

(4) mpc [51]: us buffer occup observ and throughput 
predict (comput in the same way a rb) to select the 



sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa H. mao et al. 

bitrat which maxim a give qoe metric over a horizon of 
5 futur chunks. 

(5) robustmpc [51]: us the same approach a mpc, but account 
for error see between predict and observ throughput by 
normal throughput estim by the max error see in the 
past 5 chunks. 

note: mpc involv solv an optim problem for each bitrat 
decis which maxim the qoe metric over the next 5 video 
chunks. the mpc [51] paper describ a method, fastmpc, which 
precomput the solut to thi optim problem for a quan- 
tize set of input valu (e.g., buffer size, throughput prediction, etc.). 
becaus the implement of fastmpc be not publicli available, 
we implement mpc use our abr server a follows. for each 
bitrat decision, we solv the optim problem exactli on the 
abr server by enumer all possibl for the next 5 chunks. 
We found that the comput take at most 27 m for 6 bitrat 
level and have neglig impact on qoe. 

experiment setup: We modifi dash.j (version 2.4) [2] to sup- 
port each of the aforement state-of-the-art abr algorithms. 
for pensiev and both variant of mpc, dash.j be configur 
to fetch bitrat select decis from an abr server that im- 
plement the correspond algorithm. abr server ran on the 
same machin a the client, and request to these server be 
make use xmlhttprequests. all other algorithm ran directli 
in dash.js. the dash player be configur to have a playback 
buffer capac of 60 seconds. our evalu use the “envivio- 
dash3” video from the dash-246 javascript refer client [11]. 
thi video be encod by the h.264/mpeg-4 codec at bitrat 
in {300, 750, 1200, 1850, 2850, 4300} kbp (which pertain to video 
mode in {240, 360, 480, 720, 1080, 1440}p). additionally, the video 
be divid into 48 chunk and have a total length of 193 seconds. 
thus, each chunk repres approxim 4 second of video 
playback. In our setup, the client video player be a googl chrome 
browser (version 53) and the video server (apach version 2.4.7) 
ran on the same machin a the client. We use mahimahi [33] to 
emul the network condit from our corpu of network traces, 
along with an 80 m rtt, between the client and server. unless 
otherwis noted, all experi be perform on amazon ec2 
t2.2xlarg instances. 

qoe metrics: there exist signific varianc in user prefer 
for video stream qoe [23, 31, 32, 34]. thus, we consid a 
varieti of qoe metrics. We start with the gener qoe metric use 
by mpc [51], which be defin a 

qoe = 
N∑ 
n=1 

q(rn ) − µ 
N∑ 
n=1 

Tn − 
n−1∑ 
n=1 

����� 
q(rn+1) − q(rn ) 

����� 
(6) 

for a video with N chunks. Rn repres the bitrat of chunkn and 
q(rn ) map that bitrat to the qualiti perceiv by a user. Tn repre- 
sent the rebuff time that result from download chunkn at 
bitrat Rn , while the final term penal chang in video qualiti to 
favor smoothness. 

We consid three choic of q(rn ): 
(1) qoel in : q(rn ) = Rn . thi metric be use by mpc [51]. 
(2) qoeloд : q(rn ) = log(r/rmin ). thi metric captur the notion 

that, for some users, the margin improv in perceiv 
qualiti decreas at high bitrat and be use by bola [41]. 

name bitrat util (q (R )) rebuff 
penalti (µ) 

qoel in R 4.3 
qoeloд log (r/rmin ) 2.66 

qoehd 
0.3→1, 0.75→2, 1.2→3 81.85→12, 2.85→15, 4.3→20 

tabl 1: the qoe metric we consid in our evaluation. each 
metric be a variant of equat 6. 

(3) qoehd : thi metric favor high definit (hd) video. It as- 
sign a low qualiti score to non-hd bitrat and a high qualiti 
score to HD bitrates. 

the exact valu of q(rn ) for our baselin video be provid in 
tabl 1. In thi section, we report the averag qoe per chunk, i.e., 
the total qoe metric divid by the number of chunk in the video. 

5.2 pensiev vs. exist abr algorithm 
To evalu pensieve, we compar it with state-of-the-art abr 
algorithm on each qoe metric list in tabl 1. In each experiment, 
pensieve’ abr algorithm be train to optim for the consid 
qoe metric, use the entir train corpu describ in §5.1; both 
mpc variant be also modifi to optim for the consid 
qoe metric. for comparison, we also present result for the offlin 
optim scheme, which be comput use dynam program with 
complet futur throughput information. the offlin optim serf 
a an (unattainable) upper bound on the qoe that an omnisci 
polici with complet and perfect knowledg of the futur network 
throughput could achieve. 

figur 7 show the averag qoe that each scheme achiev on 
our entir test corpus. figur 8 and 9 provid more detail result 
in the form of full cdf for each network. there be three key take- 
away from these results. first, we find that pensiev either match 
or exce the perform of the best exist abr algorithm 
on each qoe metric and network considered. the closest compet- 
ing scheme be robustmpc; thi show the import of tuning, a 
without robustmpc’ conserv throughput estimates, mpc can 
becom too aggress (reli on the playback buffer) and perform 
bad than even a naiv rate-bas scheme. for qoel in , which be 
consid in the mpc paper [51], the averag qoe for pensiev be 
15.5% high than robustmpc on the fcc broadband network traces. 
the gap between pensiev and robustmpc widen to 18.9% and 
24.6% for qoeloд and qoehd . the result be qualit similar 
for the norway hsdpa network traces. 

second, we observ that the perform of exist abr algo- 
rithm struggl to optim for differ qoe objectives. the reason 
be that these algorithm employ fix control laws, even though op- 
timiz for differ qoe object requir inher differ 
abr strategies. for example, for qoeloд , sinc the margin im- 
provement in user-perceiv qualiti diminish at high bitrates, 
the optim strategi be to avoid jump to high bitrat level when 
the risk of rebuff be high. however, to optim for qoel in , 
the abr algorithm need to be more aggressive. pensiev be abl 
to automat learn these polici and thus, perform with 
pensiev remain consist high a condit change. 

the result for qoehd further illustr thi point. recal that 
qoehd favor HD video, assign the high util to the top three 
bitrat avail for our test video (see tabl 1). As discuss in 
§3, optim for qoehd requir longer term plan than the 



neural adapt video stream with pensiev sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa 

0 

0.2 

0.4 

0.6 

0.8 

1 

qoe_lin qoe_log qoe_hd 

No 
rm 

al 
ize 

d 
av 
er 
ag 
e 
Q 
oE 

buffer-bas rate-bas bola mpc robustmpc pensiev 

(a) fcc broadband dataset 

0 

0.2 

0.4 

0.6 

0.8 

1 

qoe_lin qoe_log qoe_hd 

No 
rm 

al 
ize 

d 
Av 
er 
ag 
e 
Q 
oE 

buffer-bas rate-bas bola mpc robustmpc pensiev 

(b) norway hsdpa dataset 

figur 7: compar pensiev with exist abr algorithm on broadband and 3g/hsdpa networks. the qoe metric consid 
be present in tabl 1. result be normal against the perform of pensieve. error bar span ± one standard deviat from 
the average. 

0 

0.5 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

averag qoe 

buffer-bas 
rate-bas 
bola 
mpc 
robustmpc 
pensiev 
offlin optim 

(a) qoel in 

0 

0.5 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

averag qoe 

buffer-bas 
rate-bas 
bola 
mpc 
robustmpc 
pensiev 
offlin optim 

(b) qoeloд 

0 

0.5 

1 

-1 2 5 8 11 14 

CD 
F 

averag qoe 

buffer-bas 
rate-bas 
bola 
mpc 
robustmpc 
pensiev 
offlin optim 

(c) qoehd 
figur 8: compar pensiev with exist abr algorithm on the qoe metric list in tabl 1. result be collect on the fcc 
broadband dataset. averag qoe valu be list for each abr algorithm. 

0 

0.5 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

averag qoe 

buffer-bas 
rate-bas 
bola 
mpc 
robustmpc 
pensiev 
offlin optim 

(a) qoel in 

0 

0.5 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

averag qoe 

buffer-bas 
rate-bas 
bola 
mpc 
robustmpc 
pensiev 
offlin optim 

(b) qoeloд 

0 

0.5 

1 

-1 2 5 8 11 14 

CD 
F 

averag qoe 

buffer-bas 
rate-bas 
bola 
mpc 
robustmpc 
pensiev 
offlin optim 

(c) qoehd 
figur 9: compar pensiev with exist abr algorithm on the qoe metric list in tabl 1. result be collect on the 
norway hsdpa dataset. averag qoe valu be list for each abr algorithm. 

other two qoe metrics. when network bandwidth be inadequate, 
the abr algorithm should build the playback buffer a quickli a 
possibl use the low avail bitrate. onc the buffer be larg 
enough, it should then make a direct transit to the low HD 
qualiti (bypass intermedi bitrates). however, build buffer 
to a level which circumv rebuff and maintain suffici 
smooth requir a lot of foresight. As illustr by the exampl 
in figur 3b, pensiev be abl to learn such a polici with zero tune 
or design involvement, while other scheme such a robustmpc 
have difficulti optim such long term strategies. 

finally, pensieve’ perform be within 9.6%–14.3% of the 
offlin optim scheme across all network trace and qoe metrics. 
recal that the offlin optim perform cannot be achiev in 
practic a it requir complet knowledg of futur throughput. 
thi show that there be like to be littl room for ani onlin algo- 
rithm (without futur knowledge) to improv over pensiev in these 
scenarios. We revisit the question of pensieve’ optim in §5.4. 

qoe breakdown: To good understand the qoe gain obtain 
by pensieve, we analyz pensieve’ perform on the individ- 
ual term in our gener qoe definit (equat 6). specifically, 
figur 10 compar pensiev to state-of-the-art abr algorithm in 
term of the util from the averag playback bitrate, the penalti 
from rebuffering, and the penalti from switch bitrat (i.e., the 
smooth penalty). In other words, a give scheme’ qoe can be 
comput by subtract the rebuff penalti and smooth 
penalti from the bitrat utility. In the interest of space, figur 10 
combin the result for the fcc broadband and hsdpa traces. 

As shown, a larg portion of pensieve’ perform gain come 
from it abil to limit rebuff across the differ network and 
qoe metric considered. pensiev reduc rebuff by 10.6%– 
32.8% across the three metric by build up suffici buffer to 
handl the network’ throughput fluctuations. additionally, figur 6 
illustr that pensiev do not outperform all state-of-the-art 
scheme on everi qoe factor. instead, pensiev be abl to balanc 



sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa H. mao et al. 

0 
0.2 
0.4 
0.6 
0.8 
1 

1.2 
1.4 

bitrat util rebuff penalti smooth penalti 

Av 
er 
ag 
e 
va 
lu 
e 

buffer-bas rate-bas bola mpc robustmpc pensiev 

(a) qoel in 

0 
0.2 
0.4 
0.6 
0.8 
1 

1.2 
1.4 

bitrat util rebuff penalti smooth penalti 

Av 
er 
ag 
e 
va 
lu 
e 

buffer-bas rate-bas bola mpc robustmpc pensiev 

(b) qoeloд 

0 
1 
2 
3 
4 
5 
6 
7 

bitrat util rebuff penalti smooth penalti 

Av 
er 
ag 
e 
va 
lu 
e 

buffer-bas rate-bas bola mpc robustmpc pensiev 

(c) qoehd 
figur 10: compar pensiev with exist abr algorithm 
by analyz their perform on the individu compon 
in the gener qoe definit (equat 6). result consid 
both the broadband and hsdpa networks. error bar span ± 
one standard deviat from the average. 

each factor in a way that optim the qoe metric. for example, to 
optim qoehd , pensiev achiev the best bitrat util by alway 
tri to download chunk at HD bitrates, while when optim for 
qoel in or qoeloд , pensiev focu on achiev suffici high 
bitrat with the small amount of rebuff and bitrat switches. 

5.3 gener 
In the experi above, pensiev be train with a set of trace 
collect on the same network that be use dure testing; note 
that no test trace be directli includ in the train set. how- 
ever, in practice, pensieve’ abr algorithm could encount new 
networks, with differ condit (and thus, with differ optim 
strategies). To evalu pensieve’ abil to gener to new net- 
work conditions, we conduct two experiments. first, we evalu 
pensiev in the wild on two real networks. second, we take general- 
iti to the extrem and show how pensiev can be train to perform 
well across multipl environ use a pure synthet dataset. 

real world experiments: We evalu pensiev and sever state- 
of-the-art abr algorithm in the wild use three differ networks: 
the verizon lte cellular network, a public wifi network at a lo- 
cal coffe shop, and the wide area network between shanghai and 
boston. In these experiments, a client, run on a macbook pro 

0 

0.5 

1 

1.5 

2 

2.5 

lte public wifi intern link 

Av 
er 
ag 
e 
Q 
oE 

bola robustmpc pensiev 

figur 11: compar pensiev with exist abr algorithm 
in the wild. result be for theqoel in metric and be collect 
on the verizon lte cellular network, a public wifi network, 
and the wide area network between shanghai and boston. bar 
list averag and error bar span ± one standard deviat from 
the average. 

laptop, contact a video server run on a desktop machin lo- 
cat in boston. We consid a subset of the abr algorithm list 
in §5.1: bola, robustmpc, and pensieve. On each network, we 
load our test video ten time with each scheme, randomli select 
the order among them. the pensiev abr algorithm evalu here 
be sole train use the broadband and hsdpa trace in our 
corpus. however, even on these new networks, pensiev be abl 
to outperform the other scheme on the qoel in metric (figur 11). 
experi with the other qoe metric show similar results. 

train with a synthet dataset: can we train pensiev without 
ani real network data? learn from synthet data alon would 
of cours be undesirable, but we use it a a challeng test of 
pensieve’ abil to generalize. 

We design a data set to cover a rel broad set of network 
conditions, with averag throughput rang from 0.2 mbp to 4.3 
mbps. specifically, the dataset be gener use a markovian 
model in which each state repres an averag throughput in the 
aforement range. state transit be perform at a 1 second 
granular and follow a geometr distribut (make it more 
like to transit to a nearbi averag throughput). each throughput 
valu be then drawn from a gaussian distribut center around 
the averag throughput for the current state, with varianc uniformli 
distribut between 0.05 and 0.5. 

We then use pensiev to compar two abr algorithm on the 
test dataset describ abov (i.e., a combin of the hsdpa and 
broadband datasets): one train sole use the synthet dataset, 
and anoth train explicitli on broadband and hsdpa network 
traces. figur 12 illustr our result for all three qoe metric 
list in tabl 1. As shown, pensieve’ abr algorithm that be 
train on the synthet dataset be abl to gener across these new 
networks, outperform robustmpc and achiev averag qoe 
valu within 1.6%–10.8% of the abr algorithm train directli 
on the test networks. these result suggest that, in practice, pen- 
siev will like be abl to gener to a broad rang of network 
condit encount by it clients. 

multipl videos: As a final test of generalization, we evalu pen- 
sieve’ abil to gener across multipl video properties. To do 
this, we train a singl abr model on 1,000 synthet video use 
the techniqu describ in §4.3. the number of avail bitrat 



neural adapt video stream with pensiev sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa 

0 

0.2 

0.4 

0.6 

0.8 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

averag qoe 

robustmpc 
pensiev (synthetic) 
pensiev 

(a) qoel in 

0 

0.2 

0.4 

0.6 

0.8 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

averag qoe 

robustmpc 
pensiev (synthetic) 
pensiev 

(b) qoeloд 

0 

0.2 

0.4 

0.6 

0.8 

1 

-1 2 5 8 11 14 

CD 
F 

averag qoe 

robustmpc 
pensiev (synthetic) 
pensiev 

(c) qoehd 
figur 12: compar two abr algorithm with pensiev on the broadband and hsdpa networks: one algorithm be train on 
synthet network traces, while the other be train use a set of trace directli from the broadband and hsdpa networks. result 
be aggreg across the two datasets. 

0 

0.2 

0.4 

0.6 

0.8 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

averag qoe 

multi-video pensiev 
single-video pensiev 

figur 13: compar abr algorithm train across multipl 
video with those train explicitli on the test video. the mea- 
sure metric be qoel in . 

for each video be randomli select from [3, 10],5 and the valu 
for each bitrat be then randomli chosen from {200, 300, 450, 750, 
1200, 1850, 2350, 2850, 3500, 4300} kbps. the number of video chunk 
for each video be randomli gener from [20, 100]; chunk size 
be comput by multipli the standard 4-second chunk size 
with gaussian nois ∼ N (1, 0.1). thus, these video diverg on nu- 
merou properti includ the bitrat option (both the number of 
option and valu of each), number of chunks, chunk size and video 
duration. importantly, we ensur that none of the gener train 
video have the exact same bitrat option a the test video. 

We compar thi newli train model to the origin model, which 
be train sole on the “enviviodash3” video describ in §5.1 
(the test video). our result measur qoel in on broadband and hs- 
dpa network trace and be depict in figur 13. As shown, the 
gener abr algorithm train across multipl video be abl 
to achiev averag qoel in valu within 3.2% of the model train 
explicitli on the test video. these result suggest that in practice, 
pensiev server can be configur to use a small number of abr 
algorithm to improv stream for a divers set of videos. 

5.4 pensiev deep dive 
In thi section, we describ microbenchmark that provid a deeper 
understand of pensiev and shed light on some practic concern 
with use rl-gener abr algorithms. We begin by compar 
pensieve’ RL algorithm to tabular RL schemes, which be use by 
some previou propos for appli RL to video streaming. We 
then analyz how robust pensiev be to vari system paramet 
(e.g., neural network hyperparameters, client-to-abr server latency) 
and evalu it train time. finally, we conduct experi to 
understand how close pensiev be to the optim scheme. 

5thi rang repres the two end of the spectrum for the number of bitrat support 
by the video provid by the dash refer client [11]. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 1 2 

CD 
F 

averag qoe 

tabular q-learn 
pensiev 1 past chunk 
pensiev 8 past chunk 
pensiev 16 past chunk 

figur 14: compar exist tabular RL scheme with vari- 
ant of pensiev that consid differ number of past 
throughout measurements. result be evalu with qoel in 
for the hsdpa network. 

comparison to tabular RL schemes: A few recent scheme [6, 
8, 9, 47] have appli “tabular” RL to video streaming. tabular 
method repres the model to be learn a a table, with separ 
entri for all state (e.g., client observations) and action (e.g., 
bitrat decisions). tabular method do not scale to larg state/act 
spaces. As a result, such scheme be forc to restrict the state 
space by make simplifi (and unrealistic) assumpt about 
network behavior. for example, the most recent tabular RL scheme 
for abr [6] assum network throughput be markovian, i.e., the 
futur bandwidth depend onli on the throughput observ in the 
last chunk download. 

To compar these approach with pensieve, we implement a 
tabular RL scheme with q-learn [29]. our implement be mod- 
ele after the design in [6]. the state space be the same a describ 
in §4.2 except that the past bandwidth measur be restrict 
to onli 1 sampl (a in [6]). the past bandwidth measur and 
buffer occup be quantiz with 0.5 mbp and 1 second granu- 
lariti respectively. our quantiz be more fine-grain than that 
use in [6]; we found that thi result in good perform in our 
experiments. (note that simul result in [6] use synthet 
gener network trace with the markov property.) 

figur 14 show a signific perform gap (46.3%) between 
the tabular scheme and pensieve. thi result show that simpl 
network model (e.g., markovian dynamics) fail to captur the intri- 
caci of real networks. unlik tabular RL methods, pensiev can 
incorpor a larg amount of throughput histori into it state space 
to optim for actual network characteristics. 

To good understand the import of throughput history, we 
tri to answer: how mani past chunk be necessari to includ in the 
state space? To do this, we gener three abr algorithm with pen- 
siev that consid differ number of throughput measurements: 



sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa H. mao et al. 

number of neuron and filter (each) averag qoehd 
4 3.850 ± 1.215 
16 4.681 ± 1.369 
32 5.106 ± 1.452 
64 5.496 ± 1.411 

128 5.489 ± 1.378 

tabl 2: sweep the number of cnn filter and hidden neu- 
ron in pensieve’ learn architecture. 

number of hidden layer averag qoehd 
1 5.489 ± 1.378 
2 5.396 ± 1.434 
5 4.253 ± 1.219 

tabl 3: sweep the number of hidden layer in pensieve’ 
learn architecture. 

1, 8, and 16 past video chunks. As show in figur 14, consid 
onli 1 past chunk do not provid enough inform to infer 
futur network characterist and hurt performance. consid 
the past 8 chunk allow pensiev to extract more inform and 
improv it policy. however, the benefit of addit throughput 
measur eventu plateau. for example, provid pensiev 
with measur for the past 16 chunk onli improv the aver- 
age qoe by 1% compar to use throughput measur for 
8 chunks. thi margin improv come at the cost of high 
burden dure training. 

neural network (nn) architecture: start with pensieve’ de- 
fault learn architectur (figur 5), we swept a rang of NN pa- 
ramet to understand the impact that each have on qoehd 6. first, 
use a singl fix hidden layer, we vari the number of filter in 
the 1d-cnn and the number of neuron in the hidden merg layer. 
these paramet be swept in tandem, i.e., when 4 filter be 
used, 4 neuron be used. result from thi sweep be present in 
tabl 2. As shown, perform begin to plateau onc the number 
of filter and neuron each exceed 32. additionally, notic that onc 
these valu reach 128 (pensieve’ default configuration), varianc 
level decreas while averag qoe valu remain stable. 

next, after fix the number of filter and hidden neuron to 128, 
we vari the number of hidden layer in pensieve’ architecture. 
the result qoehd valu be list in tabl 3. interestingly, we 
find that the shallowest network of 1 hidden layer yield the best per- 
formance; thi repres the default valu in pensieve. perform 
steadili degrad a we increas the number of hidden layers. how- 
ever, it be import to note that our sweep use a fix learn rate 
and number of train iterations. tune these paramet to cater 
to deeper network may improv performance, a these network 
gener take longer to train. 

client-to-abr server latency: recal that pensiev deploy the 
rl-gener abr model on an abr server (not the video stream- 
ing clients). under thi deploy model, client must first queri 
the pensieve’ abr server to determin the bitrat to use for the 
next chunk, befor download that chunk from a cdn server. To 
understand the overhead incur by thi addit round trip, we 
perform a sweep of the rtt between the client player and abr 

6qoehd be use for the paramet sweep experi a it highlight perform 
differ more clearly. 

rtt (ms) averag qoehd 
0 5.407 ± 1.820 

20 5.356 ± 1.768 
40 5.309 ± 1.768 
60 5.271 ± 1.773 
80 5.217 ± 1.742 
100 5.219 ± 1.748 

tabl 4: averag qoehd valu when differ rtt valu be 
impos between the client and pensieve’ abr server. 

0 

0.2 

0.4 

0.6 

0.8 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

averag qoe 

pensiev 
onlin optim 
offlin optim 

figur 15: compar pensiev with onlin and offlin optimal. 
the experi us the qoel in metric. 

server, consid valu from 0 ms–100 ms. thi experi use 
the same setup describ in §5.1, and measur the qoehd metric. 
tabl 4 list our results, highlight that the latenc from thi ad- 
dition rtt have minim impact on qoe: the averag qoehd with 
a 100 m latenc be within 3.5% of that when the latenc be 0 
ms. the reason be that the latenc incur from the addit round 
trip to pensieve’ abr server be mask by the playback buffer 
occup and chunk download time [18, 21]. 

train time: To measur the overhead of gener abr algo- 
rithm use rl, we profil pensieve’ train process. train 
a singl algorithm requir approxim 50,000 iterations, where 
each iter take 300 m and correspond to 16 agent updat 
their paramet in parallel (use the train approach describ in 
§4.2). thus, in total, train take approxim 4 hours. We note 
that thi cost be incur offlin and can be perform infrequ 
depend on environ stability. 

optimality: our result illustr that pensiev be abl to outperform 
exist abr algorithms. however, figur 8 and 9 show that there 
still exist a gap between pensiev and the offlin optimal. It be 
unclear to what extent thi gap can be close sinc the offlin optim 
scheme make decis with perfect knowledg of futur bandwidth 
(§5.1). A practic onlin algorithm would onli know the underli 
distribut of futur network throughput (rather than the precis 
throughput values). thu pensiev may in fact be much closer to the 
optim onlin scheme. 

Of course, we cannot comput the optim onlin algorithm for 
real network traces, a we do not know the stochast process 
underli these traces. thus, to understand how pensiev compar 
to the best onlin algorithm, we conduct a control experi 
where the download time for each chunk be gener accord to a 
know markov process. specifically, we simul the download time 
Tn of chunk n astn = tn−1 (rn/rn−1)+ϵ, where Rn be the bitrat of 
chunk n and ϵ ∼ N 

( 
0, σ 2 

) 
. for thi model, it be straightforward to 

comput the optim onlin decis use dynam programming. 
see [28] for details. 



neural adapt video stream with pensiev sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa 

To compar the optim onlin algorithm with pensieve, we set the 
video chunk length δ to be 4 seconds, mimic the “enviviodash3” 
video describ in §5.1. the initi download time T0 be set to 4 
second for bitrat R0 = 2 kbps, and the standard deviat σ of the 
gaussian nois be set to 0.5. both buffer occup and download 
time be quantiz to 0.1 second to run dynam programming. 

We use the same setup in §5.1 to train a pensiev agent in thi 
simul environment, and compar pensieve’ perform with 
the onlin and offlin optim schemes. our experi consid 
the qoel in metric and the result be depict in figur 15. As ex- 
pected, the offlin optim outperform the onlin optim by 9.1% 
on average. thi be compar to the perform gap between pen- 
siev and the offlin optim observ in §5.2. indeed, the averag 
qoe achiev by pensiev be within 0.2% of the onlin optimal. 

6 discuss 

deploy pensiev in practice: In our current implementation, 
pensieve’ abr server run on the server-sid of video stream 
applications. thi approach offer sever advantag over deploy- 
ment in client video players. first, a varieti of client-sid devic 
be use for video stream today, rang from multi-cor desktop 
machin to mobil devic to tvs. By use an abr server to 
simpli guid client bitrat selection, pensiev can easili support 
thi broad rang of video client without modif that may sac- 
rific performance. additionally, abr algorithm be tradit 
deploy on client which can quickli react to chang environ- 
ment [51]. however, a note in §4, pensiev preserv thi abil 
by have client includ observ about the environ in 
each request sent to the abr server. further, our result suggest that 
the addit latenc requir to contact pensieve’ abr server 
have neglig impact on qoe (§5.4). If direct deploy in client 
video player be preferred, pensiev could use compress neural 
network [16] or repres them in languag support by mani 
client applications, e.g., javascript [45]. 

period and onlin training: In thi paper, we primarili describ 
rl-base abr algorithm gener a an offlin task. that is, with 
pensieve, we assum that the abr algorithm be gener a priori 
(dure a train phase) and be then unmodifi after deployment. 
however, pensiev can natur support an approach in which an 
abr algorithm be gener or updat period a new data 
arrives. thi techniqu would enabl abr algorithm to further 
adapt to the exact condit that video client be experi at a 
give time. the extrem version of thi approach be to train onlin 
directli on the video client. however, onlin train on video client 
rais two challenges. first, it increas the comput overhead 
for the client. second, it requir algorithm that can learn from 
small amount of data and converg to a good polici quickly. 

retrain frequenc depend on how quickli new network be- 
havior emerg to which exist model do not generalize. while 
our gener result (§5.3) suggest that retrain frequent 
may not be necessary, techniqu to determin when to retrain and 
investig the tradeoff with onlin train be interest area 
for futur work. 

7 relat work 
the earli abr algorithm can be primarili group into two 
classes: rate-bas and buffer-based. rate-bas algorithm [21, 42] 
first estim the avail network bandwidth use past chunk 
downloads, and then request chunk at the high bitrat that the 
network be predict to support. for example, festiv [21] predict 
throughput to be the harmon mean of the experienc throughput 
for the past 5 chunk downloads. however, these method be hin- 
dere by the bia present when estim avail bandwidth 
on top of http [22, 26]. sever system aim to correct these 
throughput estim use smooth heurist and data aggrega- 
tion techniqu [42], but accur throughput predict remain a 
challeng in practic [53]. 

In contrast, buffer-bas approach [19, 41] sole consid the 
client’ playback buffer occup when decid the bitrat for 
futur chunks. the goal of these algorithm be to keep the buffer 
occup at a pre-configur level which balanc rebuff and 
video quality. the most recent buffer-bas approach, bola [41], 
optim for a specifi qoe metric use a lyapunov optim 
formulation. bola also support chunk download abandonment, 
wherebi a video player can restart a chunk download at a low 
bitrat level if it suspect that rebuff be imminent. 

each of these approach perform well in certain set but 
not in others. specifically, rate-bas approach be best at startup 
time and when link rate be stable, while buffer-bas approach 
be suffici and more robust in steadi state and in the presenc of 
time-vari network [19]. consequently, recent propos abr 
algorithm have also investig combin these two techniques. 
the state-of-the-art approach be mpc [51], which employ model 
predict control algorithm that use both throughput estim and 
buffer occup inform to select bitrat that be expect to 
maxim qoe over a horizon of sever futur chunks. however, 
mpc still reli heavili on accur throughput estim which be 
not alway available. when throughput predict be incorrect, 
mpc’ perform can degrad significantly. address thi issu 
requir heurist that make throughput predict more conser- 
vative. however, tune such heurist to perform well in differ 
environ be challenging. further, a we observ in §3, mpc be 
often unabl to plan far enough into the futur to appli the polici 
that would maxim perform in give settings. 

A separ line of work have propos appli RL to adapt 
video stream [6, 8, 9, 47]. all of these scheme appli RL in 
a “tabular form,” which store and learn the valu function for 
all state and action explicitly, rather than use function approx- 
imat (e.g., neural networks). As a result, these scheme do not 
scale to the larg state space necessari for good perform in 
real networks, and their evalu have be limit to simul 
with synthet network models. for example, the most recent tabu- 
lar scheme [6] reli on the fundament assumpt that network 
bandwidth be markovian, i.e., the futur bandwidth depend onli on 
the throughput observ in the last chunk download. thi assump- 
tion confin the state space to consid onli one past bandwidth 
measurement, make the tabular approach feasibl to implement. 
As we saw in §5.4, the inform contain in one past chunk be 
not suffici to accur infer the distribut of futur bandwidth. 
nevertheless, some of the techniqu use in the exist RL video 



sigcomm ’17, august 21-25, 2017, lo angeles, ca, usa H. mao et al. 

stream scheme (e.g., post-decis state [6, 35]) could be use 
to acceler learn in pensiev a well. 

8 conclus 
We present pensieve, a system which gener abr algorithm 
use reinforc learning. unlik abr algorithm that use fix 
heurist or inaccur system models, pensieve’ abr algorithm 
be gener use observ of the result perform of 
past decis across a larg number of video stream experiments. 
thi allow pensiev to optim it polici for differ network 
characterist and qoe metric directli from experience. over a 
broad set of network condit and qoe metrics, we found that 
pensiev outperform exist abr algorithm by 12%–25%. 

acknowledgments. We thank our shepherd, john byers, and the 
anonym sigcomm review for their valuabl feedback. We 
also thank te-yuan huang for her guidanc regard video stream- 
ing in practice, and jiam luo for fruit discuss regard 
the learn aspect of the design. thi work be fund in part by 
nsf grant cns-1617702, cns-1563826, and cns-1407470, the 
mit center for wireless network and mobil computing, and a 
qualcomm innov fellowship. 

refer 
[1] M. abadi et al. 2016. tensorflow: A system for large-scal machin learning. 

In osdi. usenix association. 
[2] akamai. 2016. dash.js. https://github.com/dash-industry-forum/dash.js/. (2016). 

[3] S. akhshabi, A. C. begen, and C. dovrolis. 2011. An experiment evalu of 
rate-adapt algorithm in adapt stream over http. In mmsys. 

[4] M. allman, V. paxson, and E. blanton. 2009. tcp congest control. rfc 5681. 

[5] C. M. bishop. 2006. pattern recognit and machin learning. springer. 
[6] F. chiariotti et al. 2016. onlin learn adapt strategi for dash clients. In 

proceed of the 7th intern confer on multimedia systems. acm, 8. 

[7] cisco. 2016. cisco visual network index: forecast and methodology, 
2015-2020. 

[8] M. claey et al. 2013. design of a q-learning-bas client qualiti select 
algorithm for http adapt video streaming. In adapt and learn agent 
workshop. 

[9] M. claey et al. 2014. design and optimis of a (fa) q-learning-bas http 
adapt stream client. connect scienc (2014). 

[10] feder commun commission. 2016. raw data - measur broadband 
america. (2016). https://www.fcc.gov/reports-research/reports/ 
measuring-broadband-america/raw-data-measuring-broadband-america-2016 

[11] dash industri form. 2016. refer client 2.4.0. http://mediapm.edgesuite. 
net/dash/public/nightly/samples/dash-if-reference-player/index.html. (2016). 

[12] F. dobrian et al. 2011. understand the impact of video qualiti on user 
engagement. In sigcomm. acm. 

[13] G. fairhurst et al. 2015. updat tcp to support rate-limit traffic. rfc 
7661 (2015). 

[14] xavier glorot and yoshua bengio. 2010. understand the difficulti of train 
deep feedforward neural networks.. In aistats, vol. 9. 249–256. 

[15] M. T. hagan, H. B. demuth, M. H. beale, and O. De jesús. 1996. neural 
network design. pw publish compani boston. 

[16] S. han, H. mao, and W. J. dally. 2015. deep compression: compress deep 
neural network with pruning, train quantiz and huffman coding. corr, 
abs/1510.00149 2 (2015). 

[17] M. handley, J. padhye, and S. floyd. 2000. tcp congest window validation. 
rfc 2861 (2000). 

[18] t.y. huang et al. 2012. confused, timid, and unstable: pick a video 
stream rate be hard. In proceed of the 2012 acm confer on internet 
measur confer (imc). acm. 

[19] t.y. huang et al. 2014. A buffer-bas approach to rate adaptation: evid 
from a larg video stream service. In sigcomm. acm. 

[20] M. jaderberg et al. 2017. reinforc learn with unsupervis auxiliari 
tasks. In iclr. 

[21] J. jiang, V. sekar, and H. zhang. 2012. improv fairness, efficiency, and 
stabil in http-base adapt video stream with festive. In conext. 

[22] J. jiang et al. 2016. cfa: A practic predict system for video qoe 
optimization. In nsdi. usenix association. 

[23] I. ketykó et al. 2010. qoe measur of mobil youtub video streaming. In 
proceed of the 3rd workshop on mobil video deliveri (movid). acm. 

[24] V. R konda and J. N. tsitsiklis. 2000. actor-crit algorithms. In advanc in 
neural inform process systems. 1008–1014. 

[25] S. S. krishnan and R. K. sitaraman. 2012. video stream qualiti impact viewer 
behavior: infer causal use quasi-experiment designs. In proceed 
of the 2012 acm confer on internet measur confer (imc). acm. 

[26] Z. Li et al. 2014. probe and adapt: rate adapt for http video stream 
At scale. ieee journal on select area in commun (2014). 

[27] H. mao, M. alizadeh, I. menache, and S. kandula. 2016. resourc manag 
with deep reinforc learning. In hotnets. acm. 

[28] H. mao, R. netravali, and M. alizadeh. 2017. neural adapt video stream 
with pensieve. (2017). 
http://web.mit.edu/pensieve/content/pensieve-tech-report.pdf 

[29] V. mnih et al. 2015. human-level control through deep reinforc learning. 
natur 518 (2015), 529–533. 

[30] V. mnih et al. 2016. asynchron method for deep reinforc learning. In 
intern confer on machin learning. 1928–1937. 

[31] R. k.p. mok, E. W. W. chan, X. luo, and R. k.c. chang. 2011. infer the 
qoe of http video stream from user-view activities. In proceed of 
the first acm sigcomm workshop on measur Up the stack (w-must). 

[32] R. K. P. mok, E. W. W. chan, and R. K. C. chang. 2011. measur the qualiti 
of experi of http video streaming. In 12th ifip/iee intern 
symposium on integr network manag (im 2011) and workshops. 

[33] R. netravali et al. 2015. mahimahi: accur record-and-replay for http. In 
proceed of usenix atc. 

[34] K. piamrat, C. viho, J. M. bonnin, and A. ksentini. 2009. qualiti of experi 
measur for video stream over wireless networks. In proceed of 
the 2009 sixth intern confer on inform technology: new 
gener (itng). ieee comput society. 

[35] W. B. powell. 2007. approxim dynam programming: solv the curs of 
dimensionality. vol. 703. john wiley & sons. 

[36] B. recht, C. re, S. wright, and F. niu. 2011. hogwild: A lock-fre approach to 
parallel stochast gradient descent. In advanc in neural inform 
process systems. 693–701. 

[37] H. riiser et al. 2013. commut path bandwidth trace from 3G networks: 
analysi and applications. In proceed of the 4th acm multimedia system 
confer (mmsys). acm. 

[38] J. K. rowling. 2000. harri potter and the goblet of fire. london: bloomsbury. 
[39] sandvine. 2015. global internet phenomena-latin american & north america. 
[40] D. silver et al. 2016. master the game of Go with deep neural network and 

tree search. natur 529 (2016), 484–503. 
[41] K. spiteri, R. urgaonkar, and R. K. sitaraman. 2016. bola: near-optim 

bitrat adapt for onlin videos. corr abs/1601.06748 (2016). 
[42] Y. sun et al. 2016. cs2p: improv video bitrat select and adapt with 

data-driven throughput prediction. In sigcomm. acm. 
[43] R. S. sutton and A. G. barto. 1998. reinforc learning: An introduction. 

mit press. 
[44] R. S. sutton et al. 1999. polici gradient method for reinforc learn with 

function approximation.. In nips, vol. 99. 1057–1063. 
[45] synaptic. 2016. synaptic.j – the javascript architecture-fre neural network 

librari for node.j and the browser. https://synaptic.juancazala.com/. (2016). 
[46] tflearn. 2017. tflearn: deep learn librari featur a higher-level api for 

tensorflow. http://tflearn.org/. (2017). 
[47] J. van der hooft et al. A learning-bas algorithm for improv 

bandwidth-awar of adapt stream clients. In 2015 ifip/iee 
intern symposium on integr network management. ieee. 

[48] A. S. vezhnevet et al. 2017. feudal network for hierarch reinforc 
learning. arxiv preprint arxiv:1703.01161 (2017). 

[49] K. winstein, A. sivaraman, and H. balakrishnan. stochast forecast achiev 
high throughput and low delay over cellular networks. In nsdi. 

[50] Y. Wu and Y. tian. 2017. train agent for first-person shooter game with 
actor-crit curriculum learning. In iclr. 

[51] X. yin, A. jindal, V. sekar, and B. sinopoli. 2015. A control-theoret approach 
for dynam adapt video stream over http. In sigcomm. acm. 

[52] Y. zaki et al. 2015. adapt congest control for unpredict cellular 
networks. In acm sigcomm comput commun review. acm. 

[53] X. K. zou. 2015. can accur predict improv video stream in cellular 
networks?. In hotmobile. acm. 

https://github.com/dash-industry-forum/dash.js/ 
https://www.fcc.gov/reports-research/reports/measuring-broadband-america/raw-data-measuring-broadband-america-2016 
https://www.fcc.gov/reports-research/reports/measuring-broadband-america/raw-data-measuring-broadband-america-2016 
http://mediapm.edgesuite.net/dash/public/nightly/samples/dash-if-reference-player/index.html 
http://mediapm.edgesuite.net/dash/public/nightly/samples/dash-if-reference-player/index.html 
http://web.mit.edu/pensieve/content/pensieve-tech-report.pdf 
https://synaptic.juancazala.com/ 
http://tflearn.org/ 

abstract 
1 introduct 
2 background 
3 learn abr algorithm 
4 design 
4.1 train methodolog 
4.2 basic train algorithm 
4.3 enhanc for multipl video 
4.4 implement 

5 evalu 
5.1 methodolog 
5.2 pensiev vs. exist abr algorithm 
5.3 gener 
5.4 pensiev deep dive 

6 discuss 
7 relat work 
8 conclus 
refer 

