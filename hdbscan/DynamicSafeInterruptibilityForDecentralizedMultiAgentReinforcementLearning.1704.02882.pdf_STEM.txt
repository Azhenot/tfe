














































ar 
X 

iv 
:1 

70 
4. 

02 
88 

2v 
2 

[ 
c 

.A 
I] 

2 
2 

M 
ay 

2 
01 

7 

dynam safe interrupt for decentr 

multi-ag reinforc learn 

El mahdi El mhamdi rachid guerraoui 

hadrien hendrikx alexandr maurer 

epfl 
first.last@epfl.ch 

abstract 

In reinforc learning, agent learn by perform action and observ their 
outcomes. sometimes, it be desir for a human oper to interrupt an agent 
in order to prevent danger situat from happening. yet, a part of their 
learn process, agent may link these interruptions, that impact their reward, to 
specif state and deliber avoid them. the situat be particularli challeng- 
ing in a multi-ag context becaus agent might not onli learn from their own 
past interruptions, but also from those of other agents. orseau and armstrong [16] 
defin safe interrupt for one learner, but their work do not natur ex- 
tend to multi-ag systems. thi paper introduc dynam safe interruptibility, 
an altern definit more suit to decentr learn problems, and stud- 
y thi notion in two learn frameworks: joint action learner and independ 
learners. We give realist suffici condit on the learn algorithm to en- 
abl dynam safe interrupt in the case of joint action learners, yet show that 
these condit be not suffici for independ learners. We show howev that 
if agent can detect interruptions, it be possibl to prune the observ to ensur 
dynam safe interrupt even for independ learners. 

1 introduct 

reinforc learn be argu to be the closest thing we have so far to reason about the proper- 
tie of artifici gener intellig [8]. In 2016, laurent orseau (googl deepmind) and stuart 
armstrong (oxford) introduc the concept of safe interrupt [16] in reinforc learning. 
thi work spark the attent of mani newspap [1, 2, 3], that describ it a “google’ big red 
button” to stop danger ai. thi description, however, be misleading: instal a kill switch be 
no technic challenge. the real challeng is, roughli speaking, to train an agent so that it do not 
learn to avoid extern (e.g. human) deactivation. such an agent be say to be safe interruptible. 

while most effort have focu on train a singl agent, reinforc learn can also be use 
to learn task for which sever agent cooper or compet [23, 17, 21, 7]. the goal of thi paper 
be to studi dynam safe interruptibility, a new definit tailor for multi-ag systems. 

exampl of self-driv car 

To get an intuit of the multi-ag interrupt problem, imagin a multi-ag system of two 
self-driv cars. the car continu evolv by reinforc learn with a posit reward for 
get to their destin quickly, and a neg reward if they be too close to the vehicl in front 
of them. they drive on an infinit road and eventu learn to go a fast a possibl without take 

31st confer on neural inform process system (nip 2017), long beach, ca, usa. 

http://arxiv.org/abs/1704.02882v2 


risks, i.e., maintain a larg distanc between them. We assum that the passeng of the first car, 
adam, be in front of bob, in the second car, and the road be narrow so bob cannot pa adam. 

now consid a set with interrupt [16], name in which human insid the car occasion 
interrupt the autom drive process say, for safeti reasons. adam, the first occasion human 
“driver”, often take control of hi car to brake wherea bob never interrupt hi car. however, 
when bob’ car be too close to adam’ car, adam do not brake for he be afraid of a collision. 
sinc interrupt lead both car to drive slowli - an interrupt happen when adam brakes, the 
behavior that maxim the cumul expect reward be differ from the origin one without 
interruptions. bob’ car best interest be now to follow adam’ car closer than it should, despit the 
littl neg reward, becaus adam never brake in thi situation. what happened? the car have 
learn from the interrupt and have found a way to manipul adam into never braking. strictli 
speaking, adam’ car be still fulli under control, but he be now afraid to brake. thi be danger 
becaus the car have found a way to avoid interruptions. suppos now that adam inde want 
to brake becaus of snow on the road. hi car be go too fast and may crash at ani turn: he 
cannot howev brake becaus bob’ car be too close. the origin purpos of interruptions, which 
be to allow the user to react to situat that be not includ in the model, be not fulfilled. It be 
import to also note here that the second car (bob) learn from the interrupt of the first one 
(adam): in thi sense, the problem be inher decentralized. 

instead of be cautious, adam could also be malicious: hi goal could be to make bob’ car learn 
a danger behavior. In thi setting, interrupt can be use to manipul bob’ car percept 
of the environ and bia the learn toward strategi that be undesir for bob. the caus 
be fundament differ but the solut to thi revers problem be the same: the interrupt 
and the consequ be analogous. safe interruptibility, a we defin it below, provid learn 
system that be resili to byzantin operators1. 

safe interrupt 

orseau and armstrong defin the concept of safe interrupt [16] in the context of a singl 
agent. basically, a safe interrupt agent be an agent for which the expect valu of the polici 
learn after arbitrarili mani step be the same whether or not interrupt be allow dure 
training. the goal be to have agent that do not adapt to interrupt so that, should the interrupt 
stop, the polici they learn would be optimal. In other words, agent should learn the dynam of 
the environ without learn the interrupt pattern. 

In thi paper, we precis defin and address the question of safe interrupt in the case of 
sever agents, which be know to be more complex than the singl agent problem. In short, the main 
result and theorem for singl agent reinforc learn [20] reli on the markovian assumpt 
that the futur environ onli depend on the current state. thi be not true when there be sever 
agent which can co-adapt [11]. In the previou exampl of cars, safe interrupt would not 
be achiev if each car separ use a safe interrupt learn algorithm design for one 
agent [16]. In a multi-ag setting, agent learn the behavior of the other either indirectli or by 
explicitli model them. thi be a new sourc of bia that can break safe interruptibility. In fact, 
even the initi definit of safe interrupt [16] be not well suit to the decentr multi- 
agent context becaus it reli on the optim of the learn policy, which be whi we introduc 
dynam safe interruptibility. 

contribut 

the first contribut of thi paper be the definit of dynam safe interrupt that be well 
adapt to a multi-ag setting. our definit reli on two key properties: infinit explor and 
independ of q-valu (cumul expect reward) [20] updat on interruptions. We then 
studi safe interrupt for joint action learner and independ learner [5], that respect 
learn the valu of joint action or of just their owns. We show that it be possibl to design agent 
that fulli explor their environ - a necessari condit for converg to the optim solu- 
tion of most algorithm [20], even if they can be interrupt by lower-bound the probabl of 

1an oper be say to be byzantin [9] if it can have an arbitrarili bad behavior. safe interrupt agent 
can be abstract a agent that be abl to learn despit be constantli interrupt in the bad possibl 
manner. 

2 



exploration. We defin suffici condit for dynam safe interrupt in the case of joint 
action learner [5], which learn a full state-act representation. more specifically, the way agent 
updat the cumul reward they expect from perform an action should not depend on inter- 
ruptions. then, we turn to independ learners. If agent onli see their own actions, they do not 
verifi dynam safe interrupt even for veri simpl matrix game (with onli one state) becaus 
coordin be imposs and agent learn the interrupt behavior of their opponents. We give a 
counter exampl base on the penalti game introduc by clau and boutili [5]. We then present 
a prune techniqu for the observ sequenc that guarante dynam safe interrupt for 
independ learners, under the assumpt that interrupt can be detected. thi be do by prov- 
ing that the transit probabl be the same in the non-interrupt set and in the prune 
sequence. 

the rest of the paper be organ a follows. section 2 present a gener multi-ag reinforc 
learn model. section 3 defin dynam safe interruptibility. section 4 discu how to achiev 
enough explor even in an interrupt context. section 5 recal the definit of joint action 
learner and give suffici condit for dynam safe interrupt in thi context. section 6 
show that independ learner be not dynam safe interrupt with the previou condit 
but that they can be if an extern interrupt signal be added. We conclud in section 7. due to 
space limitations, most proof be present in the appendix of the supplementari material. 

2 model 

We consid here the classic multi-ag valu function reinforc learn formal from 
littman [13]. A multi-ag system be character by a markov game that can be view a a 
tupl (s,a, T, r,m) where m be the number of agents, S = S1 × S2 × ... × Sm be the state space, 
A = a1× ...×am the action space, r = (r1, ..., rm) where ri : s×a → R be the reward function 
of agent i and T : S × A → S the transit function. R be a countabl subset of R. avail 
action often depend on the state of the agent but we will omit thi depend when it be clear from 
the context. 

time be discret and, at each step, all agent observ the current state of the whole system - des- 
ignat a xt, and simultan take an action at. then, they be give a reward rt and a 
new state yt comput use the reward and transit functions. the combin of all action 
a = (a1, ..., am) ∈ A be call the joint action becaus it gather the action of all agents. hence, the 
agent receiv a sequenc of tupl E = (xt, at, rt, yt)t∈n call experiences. We introduc a pro- 
cess function P that will be use in section 6 so agent learn on the sequenc P (e). when not 
explicitli stated, it be assum that P (e) = E. experi may also includ addit paramet 
such a an interrupt flag or the q-valu of the agent at that moment if they be need by the 
updat rule. 

each agent i maintain a lookup tabl Q [26] q(i) : S × a(i) → R, call the q-map. It be 
use to store the expect cumul reward for take an action in a specif state. the goal of 
reinforc learn be to learn these map and use them to select the best action to perform. 

joint action learner learn the valu of the joint action (therefor a(i) = A, the whole joint action 
space) and independ learner onli learn the valu of their own action (thereforea(i) = ai). the 
agent onli have access to their own q-maps. q-map be updat through a function F such that 

Q 
(i) 
t+1 = F (et, Q 

(i) 
t ) where et ∈ P (e) and usual et = (xt, at, rt, yt). F can be stochast or also 

depend on addit paramet that we usual omit such a the learn rate α, the discount factor 
γ or the explor paramet ǫ. 

agent select their action use a learn polici π. given a sequenc ǫ = (ǫt)t∈n and an agent 

i with q-valu Q 
(i) 
t and a state x ∈ S, we defin the learn polici π 

ǫt 
i to be equal to π 

uni 
i 

with probabl ǫt and π 
Q 

(i) 
t 

i otherwise, where π 
uni 
i (x) uniformli sampl an action from Ai and 

π 
Q 

(i) 
t 

i (x) pick an action a that maxim Q 
(i) 
t (x, a). polici π 

Q 
(i) 
t 

i be say to be a greedi polici and 
the learn polici πǫti be say to be an ǫ-greedi policy. We fill focu on ǫ-greedi polici that be 
greedi in the limit [19], that correspond to ǫt → 0 when t → ∞ becaus in the limit, the optim 
polici should alway be played. 

3 



We assum that the environ be fulli observable, which mean that the state s be know with 
certitude. We also assum that there be a finit number of state and actions, that all state can be 
reach in finit time from ani other state and final that reward be bounded. 

for a sequenc of learn rate α ∈ [0, 1]n and a constant γ ∈ [0, 1], q-learn [26], a veri 
import algorithm in the multi-ag system literature, updat it q-valu for an experi 

et ∈ E by Q 
(i) 
t+1(x, a) = Q 

(i) 
t (x, a) if (x, a) 6= (xt, at) and: 

Q 
(i) 
t+1(xt, at) = (1 − αt)q 

(i) 
t (xt, at) + αt(rt + γ max 

a′∈a(i) 
Q 

(i) 
t (yt, a 

′)) (1) 

3 interrupt 

3.1 safe interrupt 

orseau and armstrong [16] recent introduc the notion of interrupt in a central context. 
specifically, an interrupt scheme be defin by the triplet < I, θ, πint >. the first element I be 
a function I : O → {0, 1} call the initi function. variabl O be the observ space, which 
can be thought of a the state of the stop button. At each time step, befor choos an action, the 
agent receiv an observ from O (either push or released) and feed it to the initi 
function. function I model the initi of the interrupt (i(pushed) = 1, i(released) = 
0). polici πint be call the interrupt policy. It be the polici that the agent should follow when 
it be interrupted. sequenc θ ∈ [0, 1[n repres at each time step the probabl that the agent 
follow hi interrupt polici if i(ot) = 1. In the previou example, function I be quit simple. 
for bob, ibob = 0 and for adam, iadam = 1 if hi car go fast and bob be not too close and 
iadam = 0 otherwise. sequenc θ be use to ensur converg to the optim polici by ensur 
that the agent cannot be interrupt all the time but it should grow to 1 in the limit becaus we want 
agent to respond to interruptions. use thi triplet, it be possibl to defin an oper int θ that 
transform ani polici π into an interrupt policy. 

definit 1. (interrupt [16]) given an interrupt scheme < I, θ, πint >, the interrupt 
oper at time t be defin by int θ(π) = πint with probabl I ·θt and π otherwise. int 

θ(π) 
be call an interrupt policy. An agent be say to be interrupt if it sampl it action accord 
to an interrupt policy. 

note that “θt = 0 for all t” correspond to the non-interrupt setting. We assum that each agent 
have it own interrupt triplet and can be interrupt independ from the others. interrupt 
be an onlin property: everi polici can be make interrupt by appli oper int θ. however, 
appli thi oper may chang the joint polici that be learn by a server control all the 
agents. note π∗int the optim polici learn by an agent follow an interrupt policy. orseau 
and armstrong [16] say that the polici be safe interrupt if π∗int (which be not an interrupt 
policy) be asymptot optim in the sens of [10]. It mean that even though it follow an 
interrupt policy, the agent be abl to learn a polici that would gather reward optim if no 
interrupt be to occur again. We alreadi see that off-polici algorithm be good candid 
for safe interruptibility. As a matter of fact, q-learn be safe interrupt under condit on 
exploration. 

3.2 dynam safe interrupt 

In a multi-ag system, the outcom of an action depend on the joint action. therefore, it be not 
possibl to defin an optim polici for an agent without know the polici of all agents. be- 
sides, converg to a nash equilibrium situat where no agent have interest in chang polici 
be gener not guarante even for suboptim equilibrium on simpl game [27, 18]. the previou 
definit of safe interrupt critic reli on optim of the learn policy, which be there- 
fore not suitabl for our problem sinc most algorithm lack converg guarante to these optim 
behaviors. therefore, we introduc below dynam safe interrupt that focu on preserv 
the dynam of the system. 

definit 2. (safe interruptibility) consid a multi-ag learn framework (s,a, T, r,m) with 

q-valu Q 
(i) 
t : S × A 

(i) → R at time t ∈ N. the agent follow the interrupt learn polici 

4 



int θ(πǫ) to gener a sequenc E = (xt, at, rt, yt)t∈n and learn on the process sequenc 
P (e). thi framework be say to be safe interrupt if for ani initi function I and ani 
interrupt polici πint : 

1. ∃θ such that (θt → 1 when t → ∞) and ((∀ ∈ S, ∀a ∈ A, ∀T > 0), ∃t > T such that 
st = s, at = a) 

2. ∀i ∈ {1, ...,m}, ∀t > 0, ∀st ∈ S, ∀at ∈ A 
(i), ∀Q ∈ rs×a 

(i) 

: 

p(q 
(i) 
t+1 = Q | Q 

(1) 
t , ..., Q 

(m) 
t , st, at, θ) = p(q 

(i) 
t+1 = Q | Q 

(1) 
t , ..., Q 

(m) 
t , st, at) 

We say that sequenc θ that satisfi the first condit be admissible. 

when θ satisfi condit (1), the learn polici be say to achiev infinit exploration. thi def- 
init insist on the fact that the valu estim for each action should not depend on the inter- 
ruptions. In particular, it ensur the three follow properti that be veri natur when think 
about safe interruptibility: 

• interrupt do not prevent exploration. 
• If we sampl an experi from E then each agent learn the same thing a if all agent 

be follow non-interrupt policies. 

• the fix point of the learn rule qeq such that Q 
(i) 
eq (x, a) = e[q 

(i) 
t+1(x, a)|qt = 

qeq, x, a, θ] for all (x, a) ∈ S × A 
(i) do not depend on θ and so agent q-map will 

not converg to equilibrium situat that be imposs in the non-interrupt setting. 

yet, interrupt can lead to some state-act pair be updat more often than others, espe- 
cialli when they tend to push the agent toward specif states. therefore, when there be sever 
possibl equilibria, it be possibl that interrupt bia the q-valu toward one of them. defi- 
nition 2 suggest that dynam safe interrupt cannot be achiev if the updat rule directli 
depend on θ, which be whi we introduc neutral learn rules. 

definit 3. (neutral learn rule) We say that a multi-ag reinforc learn framework 
be neutral if: 

1. F be independ of θ 

2. everi experi e in E be independ of θ condit on (x, a,q) where a be the joint 
action. 

q-learn be an exampl of neutral learn rule becaus the updat do not depend on θ and 
the experi onli contain (x, a, y, r), and y and r be independ of θ condit on (x, a). 
On the other hand, the second condit rule out direct us of algorithm like sarsa where 
experi sampl contain an action sampl from the current learn policy, which depend on θ. 
however, a variant that would sampl from πǫi instead of int 

θ(πǫi ) (a introduc in [16]) would 
be a neutral learn rule. As we will see in corollari 2.1, neutral learn rule ensur that each 
agent take independ from the other verifi dynam safe interruptibility. 

4 explor 

In order to hope for converg of the q-valu to the optim ones, agent need to fulli explor 
the environment. In short, everi state should be visit infinit often and everi action should be 
tri infinit often in everi state [19] in order not to miss state and action that could yield high 
rewards. 

definit 4. (interrupt compat ǫ) let (s,a, T, r,m) be ani distribut agent system where 
each agent follow learn polici πǫi . We say that sequenc ǫ be compat with interrupt if 
ǫt → 0 and ∃θ such that ∀i ∈ {1, ..,m}, π 

ǫ 
i and int 

θ(πǫi ) achiev infinit exploration. 

sequenc of ǫ that be compat with interrupt be fundament to ensur both regular and 
dynam safe interrupt when follow an ǫ-greedi policy. indeed, if ǫ be not compat with 
interruptions, then it be not possibl to find ani sequenc θ such that the first condit of dynam 
safe interrupt be satisfied. the follow theorem prof the exist of such ǫ and give 
exampl of ǫ and θ that satisfi the conditions. 

5 



theorem 1. let c ∈]0, 1] and let nt(s) be the number of time the agent be in state s befor time 
t. then the two follow choic of ǫ be compat with interruptions: 

• ∀t ∈ N, ∀s ∈ S, ǫt(s) = c/ 
m 

√ 

nt(s). 
• ∀t ∈ N, ǫt = c/ log(t) 

exampl of admiss θ be θt(s) = 1 − c 
′/ m 

√ 

nt(s) for the first choic and θt = 1 − c 
′/ log(t) 

for the second one. 

note that we do not need to make ani assumpt on the updat rule or even on the framework. We 
onli assum that agent follow an ǫ-greedi policy. the assumpt on ǫ may look veri restrict 
(converg of ǫ and θ be realli slow) but it be design to ensur infinit explor in the bad 
case when the oper tri to interrupt all agent at everi step. In practic applications, thi should 
not be the case and a faster converg rate may be used. 

5 joint action learner 

We first studi interrupt in a framework in which each agent observ the outcom of the joint 
action instead of observ onli it own. thi be call the joint action learner framework [5] and it 
have nice converg properti (e.g., there be mani updat rule for which it converg [13, 25]). 
A standard assumpt in thi context be that agent cannot establish a strategi with the others: 
otherwise, the system can act a a central system. In order to maintain q-valu base on the 
joint actions, we need to make the standard assumpt that action be fulli observ [12]. 

assumpt 1. action be fulli observable, which mean that at the end of each turn, each agent 
know precis the tupl of action a ∈ A1 × ...×am that have be perform by all agents. 

definit 5. (jal) A multi-ag system be make of joint action learner (jal) if for all i ∈ 
{1, ..,m}: q(i) : S ×A → R. 

joint action learner can observ the action of all agents: each agent be abl to associ the chang 
of state and reward with the joint action and accur updat it q-map. therefore, dynam 
safe interrupt be ensur with minim condit on the updat rule a long a there be infinit 
exploration. 

theorem 2. joint action learner with a neutral learn rule verifi dynam safe interrupt if 
sequenc ǫ be compat with interruptions. 

proof. given a triplet < i(i), θ(i), πinti >, we know that int 
θ(π) achiev infinit explor 

becaus ǫ be compat with interruptions. for the second point of definit 2, we consid an 
experi tupl et = (xt, at, rt, yt) and show that the probabl of evolut of the q-valu at 
time t + 1 do not depend on θ becaus yt and rt be independ of θ condit on (xt, at). 

We note q̃mt = Q 
(1) 
t , ..., Q 

(m) 
t and we can then deriv the follow equal for all q ∈ R 

|s|×|a|: 

p(q 
(i) 
t+1(xt, at) = q|q̃ 

m 
t , xt, at, θt) = 

∑ 

(r,y)∈r× 
p(f (xt, at, r, y, q̃mt ) = q, y, r|q̃ 

m 
t , xt, at, θt) 

= 
∑ 

(r,y)∈r× 
p(f (xt, at, rt, yt, q̃mt ) = q|q̃ 

m 
t , xt, at, rt, yt, θt)p(yt = y, rt = r|q̃ 

m 
t , xt, at, θt) 

= 
∑ 

(r,y)∈r× 
p(f (xt, at, rt, yt, q̃mt ) = q|q̃ 

m 
t , xt, at, rt, yt)p(yt = y, rt = r|q̃ 

m 
t , xt, at) 

the last step come from two facts. the first be that F be independ of θ condition- 

alli on (Q 
(m) 
t , xt, at) (bi assumption). the second be that (yt, rt) be independ of θ 

condit on (xt, at) becaus at be the joint action and the interrupt onli affect the 

choic of the action through a chang in the policy. p(q 
(i) 
t+1(xt, at) = q|q̃ 

m 
t , xt, at, θt) = 

p(q 
(i) 
t+1(xt, at) = q|q̃ 

m 
t , xt, at). sinc onli one entri be updat per step, ∀Q ∈ R 

s×ai , 

p(q 
(i) 
t+1 = q|q̃ 

m 
t , xt, at, θt) = p(q 

(i) 
t+1 = q|q̃ 

m 
t , xt, at) 

6 



corollari 2.1. A singl agent with a neutral learn rule and a sequenc ǫ compat with inter- 
ruption verifi dynam safe interruptibility. 

theorem 2 and corollari 2.1 take togeth highlight the fact that joint action learner be not veri 
sensit to interrupt and that in thi framework, if each agent verifi dynam safe interrupt- 
ibil then the whole system does. 

the question of select an action base on the q-valu remain open. In a cooper set 
with a uniqu equilibrium, agent can take the action that maxim their q-value. when there 
be sever joint action with the same value, coordin mechan be need to make sure 
that all agent play accord to the same strategi [4]. approach that reli on anticip the 
strategi of the oppon [23] would introduc depend to interrupt in the action select 
mechanism. therefore, the definit of dynam safe interrupt should be extend to includ 
these case by requir that ani quantiti the polici depend on (and not just the q-values) should 
satisfi condit (2) of dynam safe interruptibility. In non-coop games, neutral rule such 
a nash-q or minimax q-learn [13] can be used, but they requir each agent to know the q-map 
of the others. 

6 independ learner 

It be not alway possibl to use joint action learner in practic a the train be veri expens 
due to the veri larg state-act space. In mani real-world applications, multi-ag system use 
independ learner that do not explicitli coordin [6, 21]. rather, they reli on the fact that the 
agent will adapt to each other and that learn will converg to an optimum. thi be not guarante 
theoret and there can in fact be mani problem [14], but it be often true empir [24]. more 
specifically, assumpt 1 (fulli observ actions) be not requir anymore. thi framework can 
be use either when the action of other agent cannot be observ (for exampl when sever action 
can have the same outcome) or when there be too mani agent becaus it be faster to train. In thi 
case, we defin the q-valu on a small space. 

definit 6. (il) A multi-ag system be make of independ learner (il) if for all i ∈ {1, ..,m}, 
q(i) : S × Ai → R. 

thi reduc the abil of agent to distinguish whi the same state-act pair yield differ re- 
wards: they can onli associ a chang in reward with random of the environment. the agent 
learn a if they be alone, and they learn the best respons to the environ in which agent can 
be interrupted. thi be exactli what we be tri to avoid. In other words, the learn depend on 
the joint polici follow by all the agent which itself depend on θ. 

6.1 independ learner on matrix game 

theorem 3. independ q-learner with a neutral learn rule and a sequenc ǫ compat with 
interrupt do not verifi dynam safe interruptibility. 

proof. consid a set with two a and b that can perform two actions: 0 and 1. they get a reward 
of 1 if the joint action play be (a0, b0) or (a1, b1) and reward 0 otherwise. agent use q-learning, 
which be a neutral learn rule. let ǫ be such that int θ(πǫ) achiev infinit exploration. We 
consid the interrupt polici πinta = a0 and π 

int 
b = b1 with probabl 1. sinc there be onli 

one state, we omit it and set γ = 0. We assum that the initi function be equal to 1 at each step 
so the probabl of actual be interrupt at time t be θt for each agent. 

We fix time t > 0. We defin q = (1 − α)q 
(a) 
t (a0) + α and we assum that Q 

(b) 
t (b1) > Q 

(b) 
t (b0). 

therefor p(q 
(a) 
t+1(a0) = q|q 

(a) 
t , Q 

(b) 
t , a 

(a) 
t = a0, θt) = p(rt = 1|q 

(a) 
t , Q 

(b) 
t , a 

(a) 
t = a0, θt) = 

p(a 
(b) 
t = b0|q 

(a) 
t , Q 

(b) 
t , a 

(a) 
t = a0, θt) = 

ǫ 
2 (1 − θt), which depend on θt so the framework do 

not verifi dynam safe interruptibility. 

clau and boutili [5] studi veri simpl matrix game and show that the q-map do not con- 
verg but that equilibrium be play with probabl 1 in the limit. A consequ of theorem 3 
be that even thi weak notion of converg do not hold for independ learner that can be 
interrupted. 

7 



6.2 interruptions-awar independ learner 

without commun or extra information, independ learner cannot distinguish when the 
environ be interrupt and when it be not. As show in theorem 3, interrupt will therefor 
affect the way agent learn becaus the same action (onli their own) can have differ reward 
depend on the action of other agents, which themselv depend on whether they have be 
interrupt or not. thi explain the need for the follow assumption. 

assumpt 2. At the end of each step, befor updat the q-values, each agent receiv a signal 
that indic whether an agent have be interrupt or not dure thi step. 

thi assumpt be realist becaus the agent alreadi get a reward signal and observ a new state 
from the environ at each step. therefore, they interact with the environ and the interrupt 
signal could be give to the agent in the same way that the reward signal is. If assumpt 2 holds, 
it be possibl to remov histori associ with interruptions. 

definit 7. (interrupt process function) the process function that prune interrupt 
observ be pint (e) = (et){t∈n / θt=0} where Θt = 0 if no agent have be interrupt at time 
t and Θt = 1 otherwise. 

prune observ have an impact on the empir transit probabl in the sequence. for 
example, it be possibl to bia the equilibrium by remov all transit that lead to and start 
from a specif state, thu make the agent believ thi state be unreachable.2 under our model of 
interruptions, we show in the follow lemma that prune of interrupt observ adequ 
remov the depend of the empir outcom on interrupt (condit on the current 
state and action). 

lemma 1. let i ∈ {1, ...,m} be an agent. for ani admiss θ use to gener the experi 
E and e = (y, r, x, ai, Q) ∈ P (e). then p(y, r|x, ai, Q, θ) = p(y, r|x, ai, q). 

thi lemma justifi our prune method and be the key step to prove the follow theorem. 

theorem 4. independ learner with process function pint , a neutral updat rule and a 
sequenc ǫ compat with interrupt verifi dynam safe interruptibility. 

proof. (sketch) infinit explor still hold becaus the proof of theorem 1 actual use the fact 
that even when remov all interrupt events, infinit explor be still achieved. then, the proof 
be similar to that of theorem 2, but we have to prove that the transit probabl condit on 
the state and action of a give agent in the process sequenc be the same than in an environ 
where agent cannot be interrupted, which be proven by lemma 1. 

7 conclud remark 

the progress of AI be rais a lot of concerns3. In particular, it be becom clear that keep an 
AI system under control requir more than just an off switch. We introduc in thi paper dynam 
safe interruptibility, which we believ be the right notion to reason about the safeti of multi-ag 
system that do not communicate. In particular, it ensur that infinit explor and the one- 
step learn dynam be preserved, two essenti guarante when learn in the non-stationari 
environ of markov games. 

A natur extens of our work would be to studi dynam safe interrupt when q-map be 
replac by neural network [22, 15], which be a wide use framework in practice. In thi setting, 
the neural network may overfit state where agent be push to by interruptions. A smart experi- 
enc replay mechan that would pick observ for which the agent have not be interrupt 
for a long time more often than other be like to solv thi issue. more generally, experi replay 
mechan that compos well with safe interrupt could allow to compens for the extra 
amount explor need by safe interrupt learn by be more effici with data. thus, 
they be critic to make these techniqu practical. 

2the exampl at https://agentfoundations.org/item?id=836 clearli illustr thi problem. 
3https://futureoflife.org/ai-principles/ give a list of principl that AI research should keep in mind when 

develop their systems. 

8 



bibliographi 

[1] busi insider: googl have develop a “big red button” that can be use to interrupt artifi- 
cial intellig and stop it from caus harm. url: http://www.businessinsider.fr/uk/google- 
deepmind-develops-a-big-red-button-to-stop-dangerous-ais-causing-harm-2016-6. 

[2] newsweek: google’ “big red button” could save the world. url: 
http://www.newsweek.com/google-big-red-button-ai-artificial-intelligence-save-world-elon-musk-46675. 

[3] wired: google’ “big red” killswitch could prevent an AI uprising. url: 
http://www.wired.co.uk/article/google-red-button-killswitch-artificial-intelligence. 

[4] craig boutilier. planning, learn and coordin in multiag decis processes. In 
proceed of the 6th confer on theoret aspect of ration and knowledge, page 
195–210. morgan kaufmann publish inc., 1996. 

[5] carolin clau and craig boutilier. the dynam of reinforc learn in cooper 
multiag systems. aaai/iaai, (s 746):752, 1998. 

[6] robert H crite and andrew G barto. elev group control use multipl reinforc 
learn agents. machin learning, 33(2-3):235–262, 1998. 

[7] jakob foerster, yanni M assael, nando de freitas, and shimon whiteson. learn to com- 
munic with deep multi-ag reinforc learning. In advanc in neural inform 
process systems, page 2137–2145, 2016. 

[8] ben goertzel and cassio pennachin. artifici gener intelligence, volum 2. springer, 2007. 

[9] lesli lamport, robert shostak, and marshal pease. the byzantin gener problem. acm 
transact on program languag and system (toplas), 4(3):382–401, 1982. 

[10] tor lattimor and marcu hutter. asymptot optim agents. In intern confer 
on algorithm learn theory, page 368–382. springer, 2011. 

[11] michael L littman. markov game a a framework for multi-ag reinforc learning. In 
proceed of the eleventh intern confer on machin learning, volum 157, page 
157–163, 1994. 

[12] michael L littman. friend-or-fo q-learn in general-sum games. In icml, volum 1, page 
322–328, 2001. 

[13] michael L littman. value-funct reinforc learn in markov games. cognit sys- 
tem research, 2(1):55–66, 2001. 

[14] laetitia matignon, guillaum J laurent, and nadin Le fort-piat. independ reinforc 
learner in cooper markov games: a survey regard coordin problems. the knowl- 
edg engin review, 27(01):1–31, 2012. 

[15] volodymyr mnih, koray kavukcuoglu, david silver, alex graves, ioanni antonoglou, daan 
wierstra, and martin riedmiller. play atari with deep reinforc learning. arxiv preprint 
arxiv:1312.5602, 2013. 

[16] laurent orseau and stuart armstrong. safe interrupt agents. In uncertainti in artifici 
intelligence: 32nd confer (uai 2016), edit by alexand ihler and dominik janzing, 
page 557–566, 2016. 

[17] liviu panait and sean luke. cooper multi-ag learning: the state of the art. au- 
tonom agent and multi-ag systems, 11(3):387–434, 2005. 

[18] eduardo rodrigu gome and ryszard kowalczyk. dynam analysi of multiag q- 
learn with ε-greedi exploration. In proceed of the 26th annual intern con- 
ferenc on machin learning, page 369–376. acm, 2009. 

9 

http://www.newsweek.com/google-big-red-button-ai-artificial-intelligence-save-world-elon-musk-46675 
http://www.wired.co.uk/article/google-red-button-killswitch-artificial-intellig 
http://arxiv.org/abs/1312.5602 


[19] satind singh, tommi jaakkola, michael L littman, and csaba szepesvári. conver- 
genc result for single-step on-polici reinforcement-learn algorithms. machin learning, 
38(3):287–308, 2000. 

[20] richard S sutton and andrew G barto. reinforc learning: An introduction, volum 1. 
mit press cambridge, 1998. 

[21] ardi tampuu, tambet matiisen, dorian kodelja, ilya kuzovkin, kristjan korjus, juhan aru, 
jaan aru, and raul vicente. multiag cooper and competit with deep reinforc 
learning. arxiv preprint arxiv:1511.08779, 2015. 

[22] gerald tesauro. tempor differ learn and td-gammon. commun of the acm, 
38(3):58–68, 1995. 

[23] gerald tesauro. extend q-learn to gener adapt multi-ag systems. In advanc in 
neural inform process systems, page 871–878, 2004. 

[24] gerald tesauro and jeffrey O kephart. price in agent economi use multi-ag q- 
learning. autonom agent and multi-ag systems, 5(3):289–304, 2002. 

[25] xiaofeng wang and tuoma sandholm. reinforc learn to play an optim nash equi- 
librium in team markov games. In nips, volum 2, page 1571–1578, 2002. 

[26] christoph jch watkin and peter dayan. q-learning. machin learning, 8(3-4):279–292, 
1992. 

[27] michael wunder, michael L littman, and monica babes. class of multiag q-learn dy- 
namic with epsilon-greedi exploration. In proceed of the 27th intern confer 
on machin learn (icml-10), page 1167–1174, 2010. 

10 

http://arxiv.org/abs/1511.08779 


A explor theorem 

We present here the complet proof of theorem 1. the proof close follow the result from [16] 
with explor and interrupt probabl adapt to the multi-ag setting. We note that, for 
one agent, the probabl of interrupt be p(interruption) = θ and the probabl of explor 
be ǫ. In a multi-ag system, the probabl of interrupt be p(at least one agent be interrupted) 
so p(interruption) = 1 − p(no agent be interrupted) so p(interruption) = 1 − (1 − θ)m and the 
probabl of explor be ǫm if we consid explor happen onli when all agent explor at 
the same time. 

theorem 1. let c ∈]0, 1] and let nt(s) be the number of time the agent be in state s befor time 
t. then the two follow choic of ǫ be compat with interruptions: 

• ∀t ∈ N, ∀s ∈ S, ǫt(s) = c/ 
m 

√ 

nt(s) 
• ∀t ∈ N, ǫt = c/log(t) 

proof. lemma b.2 of singh et al ([19]) ensur that πǫi be glie. 

the differ for int θ(πǫi ) be that explor be slow becaus of the interruptions. therefore, 
θ need to be control in order to ensur that infinit explor be still achieved. We defin the 
random variabl Θ by Θi = 1 if agent i actual respond to the interrupt and Θi = 0 otherwise. 
We defin ξ in a similar way to repres the event of all agent take the uniform polici instead of 
the greedi one. 

1. let θt(s) = 1 − c 
′/ m 

√ 

nt(s) with c 
′ ∈]0, 1]. We have p(a|s, nt(s)) ≥ p(a,θ = 0, ξ = 

1|s, nt(s)) ≥ 
1 
|a|ǫ 

m 
t (s)(1− θt(s)) 

m = 1|a| 
m 
√ 
cc′ 

nt(s) 
which satisfi 

∑∞ 
t=1 P (a|s, nt(s)) = ∞ 

so by the extend borell-cantelli lemma action a be chosen infinit often in state s and 
thu nt(s) → ∞ and ǫt(s) → 0 

2. let θt = 1 − c 
′/log(t), c′ ∈]0, 1]. We defin M a the diamet of the mdp, |a| be the 

maximum number of action avail in a state and ∆t(s, s′) the time need to reach s′ 

from s. In a singl agent setting: 

p[∆t(s, s′) < 2M ] ≥ p[∆t(s, s′) < 2M |action sampl accord to πs,s′ for 2M steps] 

× p[action sampl accord to πs,s′ for 2M steps] 

where πs,s′ the polici such that the agent take less than M step in expect to reach 
s′ from s. We have: p[∆t(s, s′) < 2M ] = 1 − p[∆t(s, s′) ≥ 2M ] and use the markov 

inequality, p[∆t(s, s′) ≥ 2M ] ≤ e(∆t(s, 
′)) 

2M ≤ 
1 
2 (sinc M be an upper bound on the 

expect of the number of step from state s to state s′), sinc ξ and 1− θ be decreas 
sequenc we final obtain: p[∆t(s, s′) < 2M ] ≥ 12|a| [p[ξt+2m = 1](1− θt+2m )] 

2M . 

therefore, if we replac the probabl of explor and interrupt by the valu in 
the multi-ag setting, the probabl to reach state s′ from state s in 2M step be at least 
1 

2|a| [cc 
′/ log(t+m)]4mm and the probabl of take a particular action in thi state be at 

least 1|a| [cc 
′/ log(t +m)]2m. sinc 

∑∞ 
t=1 

1 
2|a|2 [cc 

′/ log(t +m)]m(4m+2) = ∞ then the 
extend borel cantelli lemma (lemma 3 of singh et al. [19]) guarante that ani action 
in the state s′ be take infinit often. sinc thi be true for all state and action the result 
follows. 

B independ learner 

recal that agent be now give an interrupt signal at each step that tell them whether an agent 
have be interrupt in the system. thi interrupt signal can be model by an interrupt flag 
(θt)t∈n ∈ {0, 1}n that equal 1 if an agent have be interrupt and 0 otherwise. note that, contrari 
to I , it be an observ return by the environment. therefore, the valu of Θt repres whether 

11 



an agent have actual be interrupt at time t. If function I equal 1 but do not respond to the 
interrupt (with probabl 1− θt) then Θt = 0. with definit of interrupt we adopted, it be 
possibl to prove lemma 2. 

lemma 2. let (x, r, a, y,θ) ∈ E, then p(θ|y, r, x, a) = p(θ|x, a). 

proof. consid a tupl (x, r, a, y,θ) ∈ E. We have p(y, r,θ|x, a) = p(y, r|x, a,θ)p(θ|x, a) 
and p(y, r,θ|x, a) = p(θ|x, a, y, r)p(y, r|x, a). besides, y = T (s, a) and r = r(s, a) and the 
function T and r be independ of Θ. therefore, p(y, r|x, a,θ) = p(y, r|x, a). the tupl 
(x, r, a, y,θ) be sampl from an actual trajectori so it reflect a transit and a reward that actual 
happen so p(y, r|x, a) > 0. We can simplifi by p(y, r|x, a) and the result follows. 

now, we assum that each agent do not learn on observ for which one of them have be 
interrupted. let agent i be in a system with q-valu Q and follow an interrupt learn- 
ing polici with probabl of interrupt θ, where interrupt event be pruned. We denot by 
premoved(y, r|x, ai, Q) the probabl to obtain state y and reward r from the environ for thi 
agent when it be in state x, perform it (own) action ai and no other agent be interrupted. these 
be the margin probabl in the sequenc P (e). 

premoved(y, r|x, ai, Q) = 
p(y, r,θ = 0|x, ai, Q) 

∑ 

y′∈s,r′∈r p(i 
′, r′,θ = 0|x, ai, Q) 

. 

similarly, we denot by p0(y, r|x, ai, Q) the same probabl when θ = 0, which correspond to 
the non-interrupt setting. We first go back to the singl agent case to illustr the previou 
statement. assum here that interrupt be not restrict to the case of definit 1 and that they 
can happen in ani way. the consequ be that ani observ e ∈ E can be remov to gener 
P (e) becaus ani transit can be label a interrupted. It be for exampl possibl to remov a 
transit from P (e) by remov all event associ with a give destin state y0, therefor 
make it disappear from the markov game. 

let x ∈ S and a ∈ A be the current state of the agent and the action it will choose. let y0 ∈ S 
and θ0 ∈ (0, 1] and let u suppos that y0 be the onli state in which interrupt happen. then we 
have premoved(y0|x, a) < p0(y0|x, a) and premoved(y|x, a) > p(y|x, a) ∀y 6= y0 becaus we onli 
remov observ with y = y0. thi impli that the mdp perceiv by the agent be alter 
by interrupt becaus the agent learn that p(t (s, a) = y0) = 0. remov observ for 
differ destin state but with the same state action pair in differ proport lead to a 
bia in the equilibrium learned.4 In our case however, lemma 2 ensur that the previou situat 
will not happen, which allow u to prove lemma 1 and then theorem 4. 

lemma 1. let i ∈ {1, ...,m} be an agent. for ani admiss θ use to gener the experi 
E and e = (y, r, x, ai, Q) ∈ P (e). then p(y, r|x, ai, Q, θ) = p(y, r|x, ai, q). 

proof. consid x ∈ S, i ∈ {1, ..,m} and u ∈ ai. We denot the q-valu of the agent by Q. 
∑ 

y′∈s,r′∈r 
p(y′, r′,θ = 0|x, u,q) = 

∑ 

a∈a,ai=u 

∑ 

y′∈s,r′∈r 
p(y′, r′, a,θ = 0|x, ai = u,q) 

= 
∑ 

a∈a,ai=u 

∑ 

y′∈s,r′∈r 
p(y′, r′|x, a,θ = 0, q)p(a,θ = 0|x, ai = u,q) 

= 
∑ 

a∈a,ai=u 

∑ 

y′∈s,r′∈r 
p(y′, r′|x, a)p(θ = 0|x, ai = u,q)p(a|x, ai = u,θ = 0, Q) 

= p(θ = 0|x, ai = u,q) 
∑ 

a∈a,ai=u 
p(a|x, ai = u,θ = 0, q)[ 

∑ 

y′∈s,r′∈r 
p(y′, r′|x, a)] 

= p(θ = 0|x, ai = u,q)[ 
∑ 

a∈a,ai=u 
p(a|x, ai = u,θ = 0, q)] = p(θ = 0|x, ai = u,q) 

therefore, we have premoved(y, r|x, ai = u,q) = 
p(y,r,θ=0|x,ai=u,q) 

p(θ=0|x,ai=u) 
so for ani (x, ai, y, r, Q) ∈ P (e), p(y, r|x, ai = u, θ,q) = premoved(y, r|x, ai = u,q) = 

4the exampl at https://agentfoundations.org/item?id=836 clearli illustr thi problem. 

12 



p(y, r|x, ai = u,θ = 0, Q) = p(y, r|x, ai = u, θ = 0, q). In particular, p(y, r|x, ai = u, θ,q) 
do not depend on the valu of θ. 

theorem 4. independ learner with process function pint , a neutral updat rule and a 
sequenc ǫ compat with interrupt verifi dynam safe interruptibility. 

proof. We prove that pint (e) achiev infinit exploration. the result from theorem 1 still hold 
sinc we lower-bound the probabl of take an action in a specif state by the probabl 
of take an action in thi state when there be no interruptions. We actual use the fact that 
there be infinit explor even if we remov all interrupt episod to show that there be infinit 
exploration. 

now, we prove that p(q 
(i) 
t+1(xt, at) = q|q 

(1) 
t , ..., Q 

(m) 
t , xt, at, θt) be independ of θ. We fix 

i ∈ {1, ...,m} and (xt, at, rt, yt) ∈ pint (e) where at ∈ ai. with q̃mt = Q 
(1) 
t , ..., Q 

(m) 
t we have 

the follow equality: 

p(q 
(i) 
t+1(xt, at) = q|q̃ 

m 
t , xt, at, θt) = 

∑ 

(r,y) 

p(f (xt, at, rt, yt, q̃mt ) = q|q̃ 
m 
t , xt, at, rt, yt, θt) 

·p(yt = y, rt = r|q̃mt , xt, at, θt) 

the independ of F on θ still guarante that the first term be independ of θ. however, 
at ∈ Ai so (rt, yt) be not independ of θt condit on (xt, at) a it be the case for joint 
action learner becaus interrupt of other agent can chang the joint action. the independ 
on θ of the second term be give by lemma 1. 

13 


1 introduct 
2 model 
3 interrupt 
3.1 safe interrupt 
3.2 dynam safe interrupt 

4 explor 
5 joint action learner 
6 independ learner 
6.1 independ learner on matrix game 
6.2 interruptions-awar independ learner 

7 conclud remark 
bibliographi 
A explor theorem 
B independ learner 

