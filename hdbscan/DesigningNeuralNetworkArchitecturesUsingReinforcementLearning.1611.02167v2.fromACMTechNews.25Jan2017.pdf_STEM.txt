


















































under review a a confer paper at iclr 2017 

design neural network architectur 
use reinforc learn 

bowen baker, otkrist gupta, nikhil naik & ramesh raskar 
media laboratori 
massachusett institut of technolog 
cambridg MA 02139, usa 
{bowen, otkrist, naik, raskar}@mit.edu 

abstract 

At present, design convolut neural network (cnn) architectur requir 
both human expertis and labor. new architectur be handcraft by care 
experiment or modifi from a hand of exist networks. We propos a 
meta-model approach base on reinforc learn to automat gen- 
erat high-perform cnn architectur for a give learn task. the learn 
agent be train to sequenti choos cnn layer use q-learn with an �- 
greedi explor strategi and experi replay. the agent explor a larg 
but finit space of possibl architectur and iter discov design with im- 
prove perform on the learn task. On imag classif benchmarks, the 
agent-design network (consist of onli standard convolution, pooling, and 
fully-connect layers) beat exist network design with the same layer type 
and be competit against the state-of-the-art method that use more complex 
layer types. We also outperform exist meta-model approach for network 
design on imag classif tasks. 

1 introduct 

deep convolut neural network (cnns) have see great success in the past few year on a 
varieti of machin learn problem (lecun et al., 2015). A typic cnn architectur consist 
of sever convolution, pooling, and fulli connect layers. while construct a cnn, a network 
design have to make numer design choices: the number of layer of each type, the order of 
layers, and the design paramet for each type of layer, e.g., the recept field size, stride, and 
number of recept field for a convolut layer. the number of possibl choic make the design 
space of cnn architectur extrem larg and hence, infeas for an exhaust manual search. 
while there have be some work (pinto et al., 2009; bergstra et al., 2013; domhan et al., 2015) on 
autom or computer-aid neural network design, new cnn architectur or network design ele- 
ment be still primarili develop by research use new theoret insight or intuit gain 
from experimentation. 

In thi paper, we seek to autom the process of cnn architectur select through a meta- 
model procedur base on reinforc learning. We construct a novel q-learn agent whose 
goal be to discov cnn architectur that perform well on a give machin learn task with no 
human intervention. the learn agent be give the task of sequenti pick layer of a cnn 
model. By discret and limit the layer paramet to choos from, the agent be left with 
a finit but larg space of model architectur to search from. the agent learn through random 
explor and slowli begin to exploit it find to select high perform model use the �- 
greedi strategi (mnih et al., 2015). the agent receiv the valid accuraci on the give machin 
learn task a the reward for select an architecture. We expedit the learn process through 
repeat memori sampl use experi replay (lin, 1993). We refer to thi q-learn base 
meta-model method a metaqnn, which be summar in figur 1. 

We conduct experi with a space of model architectur consist of onli standard convolution, 
pooling, and fulli connect layer use three standard imag classif datasets: cifar-10, 
svhn, and mnist. the learn agent discov cnn architectur that beat all exist network 

1 

ar 
X 

iv 
:1 

61 
1. 

02 
16 

7v 
2 

[ 
c 

.L 
G 

] 
3 

0 
N 

ov 
2 

01 
6 



under review a a confer paper at iclr 2017 

agent sampl 

network topolog 

agent learn 

from memori 
train network 

store in 

replay memori 

R 
Q 

sampl 

memori 
updat 

q-valu 

conv 

conv 

pool 

softmax 

topology: 
c(64,5,1) 

c(128,3,1) 

p(2,2) 

sm(10) 

performance: 
93.3% 

R 

figur 1: design cnn architectur with q-learning: the agent begin by sampl a con- 
volut neural network (cnn) topolog condit on a predefin behavior distribut and 
the agent’ prior experi (left block). that cnn topolog be then train on a specif task; the 
topolog descript and performance, e.g. valid accuracy, be then store in the agent’ mem- 
ori (middl block). finally, the agent us it memori to learn about the space of cnn topolog 
through q-learn (right block). 

design onli with the same layer type (e.g., springenberg et al. (2014); srivastava et al. (2015)). 
In addition, their perform be competit against network design that includ complex layer 
type and train procedur (e.g., clevert et al. (2015); lee et al. (2016)). finally, the metaqnn 
select model comfort outperform previou autom network design method (stanley & 
miikkulainen, 2002; bergstra et al., 2013). the top network design discov by the agent on 
one dataset be also competit when train on other datasets, indic that they be suit for 
transfer learn tasks. moreover, we can gener not just one, but sever varied, well-perform 
network designs, which can be ensembl to further boost the predict performance. 

2 relat work 

design neural network architectures: research on autom neural network design go 
back to the 1980 when genet algorithm-bas approach be propos to find both architec- 
ture and weight (schaffer et al., 1992). however, to the best of our knowledge, network design 
with genet algorithms, such a those gener with the neat algorithm (stanley & miikkulainen, 
2002), have be unabl to match the perform of hand-craft network on standard bench- 
mark (verbancs & harguess, 2013). other biolog inspir idea have also be explored; 
motiv by screen method in genetics, pinto et al. (2009) propos a high-throughput network 
select approach where they randomli sampl thousand of architectur and choos promis 
one for further training. 

bayesian optim have also be use (shahriari et al., 2016) for automat select of network 
architectur (bergstra et al., 2013; domhan et al., 2015) and hyperparamet (snoek et al., 2012; 
swerski et al., 2013). notably, bergstra et al. (2013) propos a meta-model approach base 
on tree of parzen estim (tpe) (bergstra et al., 2011) to choos both the type of layer and 
hyperparamet of feed-forward networks; however, they fail to match the perform of hand- 
craft networks. 

reinforc learning: recent there have be much work at the intersect of reinforc 
learn and deep learning. for instance, method use cnn to approxim theq-learn util 
function (watkins, 1989) have be success in game-play agent (mnih et al., 2015; silver 
et al., 2016) and robot control (lillicrap et al., 2015; levin et al., 2016). these method reli on 
phase of exploration, where the agent tri to learn about it environ through sampling, and 
exploitation, where the agent us what it learn about the environ to find good paths. In 
tradit reinforc learn settings, over-explor can lead to slow converg times, yet 
over-exploit can lead to converg to local minimum (kaelbl et al., 1996). however, in the 
case of larg or continu state spaces, the �-greedi strategi of learn have be empir show 
to converg (vermorel & mohri, 2005). finally, when the state space be larg or explor be costly, 
the experi replay techniqu (lin, 1993) have prove use in experiment set (adam et al., 
2012; mnih et al., 2015). We incorpor these techniques—q-learning, the �-greedi strategi and 
experi replay—in our algorithm design. 

2 



under review a a confer paper at iclr 2017 

3 background 

our method reli on q-learning, a type of reinforc learning. We now summar the theoret- 
ical formul of q-learning, a adopt to our problem. consid the task of teach an agent 
to find optim path a a markov decis process (mdp) in a finite-horizon environment. con- 
strain the environ to be finite-horizon ensur that the agent will determinist termin 
in a finit number of time steps. In addition, we restrict the environ to have a discret and 
finit state space S a well a action space U . for ani state si ∈ S , there be a finit set of actions, 
u(si) ⊆ U , that the agent can choos from. In an environ with stochast transitions, an agent 
in state si take some action u ∈ u(si) will transit to state sj with probabl ps′|s,u(sj |si, u), 
which may be unknown to the agent. At each time step t, the agent be give a reward rt, depend 
on the transit from state s to s′ and action u. rt may also be stochast accord to a distribut 
pr|s′,s,u. the agent’ goal be to maxim the total expect reward over all possibl trajectories, i.e., 
maxti∈t rti , where the total expect reward for a trajectori Ti be 

rti = 
∑ 

(s,u,s′)∈ti er|s,u,s′ [r|s, u, s 
′]. (1) 

though we limit the agent to a finit state and action space, there be still a combinatori larg 
number of trajectories, which motiv the use of reinforc learning. We defin the maximiza- 
tion problem recurs in term of subproblem a follows. for ani state si ∈ S and subsequ 
action u ∈ u(si), we defin the maximum total expect reward to be q∗(si, u). q∗(·) be know a 
the action-valu function and individu q∗(si, u) be know a q-values. the recurs maximiza- 
tion equation, which be know a bellman’ equation, can be write a 

q∗(si, u) = esj |si,u 
[ 
er|si,u,sj [r|si, u, sj ] + γmaxu′∈u(sj)q∗(sj , u′) 

] 
. (2) 

In mani cases, it be imposs to analyt solv bellman’ equat (bertsekas, 2015), but it can 
be formul a an iter updat 

qt+1(si, u) = (1− α)qt(si, u) + α 
[ 
rt + γmaxu′∈u(sj)qt(sj , u 

′) 
] 
. (3) 

equat 3 be the simplest form of q-learn propos by watkin (1989). for well formul 
problems, limt→∞qt(s, u) = q∗(s, u), a long a each transit be sampl infinit mani 
time (bertsekas, 2015). the updat equat have two parameters: (i) α be a q-learn rate which 
determin the weight give to new inform over old information, and (ii) γ be the discount fac- 
tor which determin the weight give to short-term reward over futur rewards. the q-learn 
algorithm be model-free, in that the learn agent can solv the task without ever explicitli con- 
struct an estim of environment dynamics. In addition, q-learn be off policy, mean it 
can learn about optim polici while explor via a non-optim behavior distribution, i.e. the 
distribut by which the agent explor it environment. 

We choos the behavior distribut use an �-greedi strategi (mnih et al., 2015). with thi strat- 
egy, a random action be take with probabl � and the greedi action, maxu∈u(si)qt(si, u), be 
chosen with probabl 1− �. We anneal � from 1→ 0 such that the agent begin in an explor 
phase and slowli start move toward the exploit phase. In addition, when the explor 
cost be larg (which be true for our problem setting), it be benefici to use the experi replay 
techniqu for faster converg (lin, 1992). In experi replay, the learn agent be provid 
with a memori of it past explor path and rewards. At a give interval, the agent sampl from 
the memori and updat it q-valu via equat 3. 

4 design neural network architectur with q-learn 

We consid the task of train a learn agent to sequenti choos neural network layers. 
figur 2 show feasibl state and action space (a) and a potenti trajectori the agent may take along 
with the cnn architectur defin by thi trajectori (b). We model the layer select process a a 
markov decis process with the assumpt that a well-perform layer in one network should 
also perform well in anoth network. We make thi assumpt base on the hierarch natur of 
the featur represent learn by neural network with mani hidden layer (lecun et al., 2015). 
the agent sequenti select layer via the �-greedi strategi until it reach a termin state. 
the cnn architectur defin by the agent’ path be train on the chosen learn problem, and the 
agent be give a reward equal to the valid accuracy. the valid accuraci and architectur 

3 



under review a a confer paper at iclr 2017 

layer 1 layer 2 

w 
11 

(1) 

w 
12 

(1) 

w 
13 

(1) 

w 
21 

(1) 

w 
22 

(1) 

w 
23 

(1) 

w 
31 

(1) 

w 
32 

(1) 

w 
33 

(1) 

input 

convolut 
64 filter 
3x3 recept field 

1x1 stride 

max pool 

softmax 

input 

c(64,3,1) 

p(2,2) 

c(64,3,1) 

G 

G G 

G 

p(2,2) 

state 

action 

input 

c(64,3,1) 

p(2,2) 

c(64,3,1) 

G 

G G 

G 

layer 1 layer 2 

c(64,3,1) c(64,3,1) 

G 

G G 

G 

layer n-1 layer N 

p(2,2) p(2,2) p(2,2) 

(a) (b) 

figur 2: markov decis process for cnn architectur generation: figur 2(a) show the 
full state and action space. In thi illustration, action be show to be determinist for clarity, but 
they be stochast in experiments. c(n, f, l) denot a convolut layer with n filters, recept 
field size f , and stride l. P (f, l) denot a pool layer with recept field size f and stride l. G 
denot a termin state (softmax/glob averag pooling). figur 2(b) show a path the agent 
may choose, highlight in green, and the correspond cnn topology. 

descript be store in a replay memory. experi be sampl period from the replay 
memori to updat q-valu via equat 3. the agent follow an � schedul which determin it 
shift from explor to exploitation. 

our method requir three main design choices: (i) reduc cnn layer definit to simpl state 
tuples, (ii) defin a set of action the agent may take, i.e., the set of layer the agent may pick next 
give it current state, and (iii) balanc the size of the state-act space—and correspondingly, the 
model capacity—with the amount of explor need by the agent to converge. We now describ 
the design choic and the learn process in detail. 

4.1 the state space 

each state be defin a a tupl of all relev layer parameters. We allow five differ type of lay- 
ers: convolut (c), pool (p), fulli connect (fc), global averag pool (gap), and softmax 
(sm), though the gener method be not limit to thi set. tabl 1 show the relev paramet for 
each layer type and also the discret we chose for each parameter. each layer have a paramet 
layer depth (shown a layer 1, 2, ... in figur 2). ad layer depth to the state space allow u 
to constrict the action space such that the state-act graph be direct and acycl (dag) and also 
allow u to specifi a maximum number of layer the agent may select befor terminating. 

each layer type also have a paramet call represent size (r-size). convolut net pro- 
gressiv compress the represent of the origin signal through pool and convolution. the 
presenc of these layer in our state space may lead the agent on a trajectori where the intermedi 
signal represent get reduc to a size that be too small for further processing. for example, five 
2× 2 pool layer each with stride 2 will reduc an imag of initi size 32× 32 to size 1× 1. At 
thi stage, further pooling, or convolut with recept field size great than 1, would be mean- 
ingless and degenerate. To avoid such scenarios, we add the r-size paramet to the state tupl s, 
which allow u to restrict action from state with r-size n to those that have a recept field size 
less than or equal to n. To further constrict the state space, we chose to bin the represent size 
into three discret buckets. however, bin add uncertainti to the state transitions: depend on 
the true underli represent size, a pool layer may or may not chang the r-size bin. As a 
result, the action of pool can lead to two differ states, which we model a stochast in state 
transitions. pleas see figur A1 in appendix for an illustr example. 

4.2 the action space 

We restrict the agent from take certain action to both limit the state-act space and make learn- 
ing tractable. first, we allow the agent to termin a path at ani point, i.e. it may choos a termi- 
nation state from ani non-termin state. In addition, we onli allow transit for a state with 
layer depth i to a state with layer depth i + 1, which ensur that there be no loop in the graph. 
thi constraint ensur that the state-act graph be alway a dag. ani state at the maximum layer 
depth, a prescrib in tabl 1, may onli transit to a termin layer. 

4 



under review a a confer paper at iclr 2017 

layer type layer paramet paramet valu 

convolut (c) 

i ∼ layer depth 
f ∼ recept field size 
` ∼ stride 
d ∼ # recept field 
n ∼ represent size 

< 12 
square. ∈ {1, 3, 5} 
square. alway equal to 1 
∈ {64, 128, 256, 512} 
∈ {(∞, 8], (8, 4], (4, 1]} 

pool (p) 
i ∼ layer depth 
(f, `) ∼ (recept field size, strides) 
n ∼ represent size 

< 12 
square. ∈ 

{ 
(5, 3), (3, 2), (2, 2) 

} 
∈ {(∞, 8], (8, 4] and (4, 1]} 

fulli connect (fc) 
i ∼ layer depth 
n ∼ # consecut FC layer 
d ∼ # neuron 

< 12 
< 3 
∈ {512, 256, 128} 

termin state s ∼ previou state 
t ∼ type global avg. pooling/softmax 

tabl 1: experiment state space. for each layer type, we list the relev paramet and the 
valu each paramet be allow to take. 

next, we limit the number of fulli connect (fc) layer to be at maximum two, becaus a larg 
number of FC layer can lead to too may learnabl parameters. the agent at a state with type FC 
may transit to anoth state with type FC if and onli if the number of consecut FC state be 
less than the maximum allowed. furthermore, a state s of type FC with number of neuron d may 
onli transit to either a termin state or a state s′ of type FC with number of neuron d′ ≤ d. 
An agent at a state of type convolut (c) may transit to a state with ani other layer type. An 
agent at a state with layer type pool (p) may transit to a state with ani other layer type other 
than anoth P state becaus consecut pool layer be equival to a single, larg pool 
layer which could lie outsid of our chosen state space. furthermore, onli state with represent 
size in bin (8, 4] and (4, 1] may transit to an FC layer, which ensur that the number of weight 
do not becom unreason huge. note that a major of these constraint be in place to enabl 
faster converg on our limit hardwar (see section 5) and not a limit of the method in 
itself. 

4.3 q-learn train procedur 

for the iterativeq-learn updat (equat 3), we set theq-learn rate (α) to 0.01. In addition, 
we set the discount factor (γ) to 1 to not over-priorit short-term rewards. We decreas � from 1.0 
to 0.1 in steps, where the step-siz be defin by the number of uniqu model train (tabl 2). 
At � = 1.0, the agent sampl cnn architectur with a random walk along a uniformli weight 
markov chain. everi topolog sampl by the agent be train use the procedur describ in 
section 5, and the predict perform of thi network topolog on the valid set be recorded. 
We train a larg number of model at � = 1.0 a compar to other valu of � to ensur that the 
agent have adequ time to explor befor it begin to exploit. We stop the agent at � = 0.1 (and not 
at � = 0) to obtain a stochast final policy, which gener perturb of the global minimum.1 
ideally, we want to identifi sever well-perform model topologies, which can then be ensembl 
to improv predict performance. 

dure the entir train process (start at � = 1.0), we maintain a replay dictionari which store 
(i) the network topolog and (ii) predict perform on a valid set, for all of the sampl 
models. If a model that have alreadi be train be re-sampled, it be not re-trained, but instead the 
previous found valid accuraci be present to the agent. after each model be sampl and 
trained, the agent randomli sampl 100 model from the replay dictionari and appli the q-valu 
updat defin in equat 3 for all transit in each sampl sequence. the q-valu updat be 
appli to the transit in tempor revers order, which have be show to speed up q-valu 
converg (lin, 1993). 

1� = 0 indic a complet determinist policy. becaus we would like to gener sever good model 
for ensembl and analysis, we stop at � = 0.1, which repres a stochast final policy. 

5 



under review a a confer paper at iclr 2017 

� 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 
# model train 1500 100 100 100 150 150 150 150 150 150 

tabl 2: � schedule. the learn agent train the specifi number of uniqu model at each �. 

5 experi detail 

dure the model explor phase, we train each network topolog with a quick and aggress 
train scheme. for each experiment, we creat a valid set by randomli take 5,000 sampl 
from the train set such that the result class distribut be unchanged. for everi network, 
a dropout layer be add after everi two layers. the ith dropout layer, out of a total n dropout 
layers, have a dropout probabl of i2n . each model be train for a total of 20 epoch with the 
adam optim (kingma & ba, 2014) with β1 = 0.9, β2 = 0.999, ε = 10−8. the batch size be 
set to 128, and the initi learn rate be set to 0.001. If the model fail to perform good than a 
random predictor after the first epoch, we reduc the learn rate by a factor of 0.4 and restart 
training, for a maximum of 5 restarts. for model that start learn (i.e., perform good than a 
random predictor), we reduc the learn rate by a factor of 0.2 everi 5 epochs. all weight be 
initi with xavier initi (glorot & bengio, 2010). our experi use caff (jia 
et al., 2014) take 8-10 day to complet for each dataset with a hardwar setup consist of 10 
nvidia gpus. 

after the agent complet the � schedul (tabl 2), we select the top ten model that be found 
over the cours of exploration. these model be then finetun use a much longer train 
schedule, and onli the top five be use for ensembling. We now provid detail of the dataset 
and the finetun process. 

the street view hous number (svhn) dataset have 10 class with a total of 73,257 sampl 
in the origin train set, 26,032 sampl in the test set, and 531,131 addit sampl in the 
extend train set. dure the explor phase, we onli train with the origin train set, 
use 5,000 random sampl a validation. We finetun the top ten model with the origin plu 
extend train set, by creat preprocess train and valid set a describ by lee et al. 
(2016). our final learn rate schedul after tune on valid set be 0.025 for 5 epochs, 0.0125 
for 5 epochs, 0.0001 for 20 epochs, and 0.00001 for 10 epochs. 

cifar-10, the 10 class tini imag dataset, have 50,000 train sampl and 10,000 test samples. 
dure the explor phase, we take 5,000 random sampl from the train set for validation. 
the maximum layer depth be increas to 18. after the experi completed, we use the same 
valid set to tune hyperparameters, result in a final train scheme which we ran on the 
entir train set. In the final train scheme, we set a learn rate of 0.025 for 40 epochs, 
0.0125 for 40 epochs, 0.0001 for 160 epochs, and 0.00001 for 60 epochs, with all other paramet 
unchanged. dure thi phase, we preprocess use global contrast normal and use moder 
data augmentation, which consist of random mirror and random translat by up to 5 pixels. 

mnist, the 10 class handwritten digit dataset, have 60,000 train sampl and 10,000 test 
samples. We preprocess each imag with global mean subtraction. In the final train scheme, 
we train each model for 40 epoch and decreas learn rate everi 5 epoch by a factor of 0.2. 
for further tune detail pleas see appendix C. 

6 result 

model select analysis: from q-learn principles, we expect the learn agent to improv 
in it abil to pick network topolog a � reduc and the agent enter the exploit phase. In 
figur 3, we plot the roll mean of predict accuraci over 100 model and the mean accuraci 
of model sampl at differ � values, for the cifar-10 and svhn experiments. the plot show 
that, while the predict accuraci remain flat dure the explor phase (� = 1) a expected, 
the agent consist improv in it abil to pick better-perform model a � reduc from 1 to 
0.1. for example, the mean accuraci of model in the svhn experi increas from 52.25% at 
� = 1 to 88.02% at � = 0.1. further q-learn analysi can be found in section d.1 of appendix. 

6 



under review a a confer paper at iclr 2017 

0 500 1000 1500 2000 2500 3000 
iter 

0.00 

0.10 

0.20 

0.30 

0.40 

0.50 

0.60 

0.70 

0.80 

0.90 

1.00 

A 
cc 

u 
ra 

cy 

epsilon = 1.0 .9 .8 .7 .6 .5 .4 .3 .2 .1 

svhn q-learn perform 

averag accuraci per epsilon 

roll mean model accuraci 

0 500 1000 1500 2000 2500 3000 3500 
iter 

0.00 

0.10 

0.20 

0.30 

0.40 

0.50 

0.60 

0.70 

0.80 

0.90 

1.00 

A 
cc 

u 
ra 

cy 

epsilon = 1.0 .9 .8.7 .6 .5 .4 .3 .2 .1 

cifar10 q-learn perform 

averag accuraci per epsilon 

roll mean model accuraci 

figur 3: q-learn performance. In the plots, the blue line show a roll mean of model 
accuraci versu iteration, where in each iter of the algorithm the agent be sampl a model. 
each bar (in light blue) mark the averag accuraci over all model that be sampl dure the 
explor phase with the label �. As � decreases, the averag accuraci go up, demonstr 
that the agent learn to select better-perform cnn architectures. 

method cifar-10 svhn mnist cifar-100 
maxout (goodfellow et al., 2013) 9.38 2.47 0.45 38.57 
nin (lin et al., 2013) 8.81 2.35 0.47 35.68 
fitnet (romero et al., 2014) 8.39 2.42 0.51 35.04 
highway (srivastava et al., 2015) 7.72 - - - 
vggnet (simonyan & zisserman, 2014) 7.25 - - - 
all-cnn (springenberg et al., 2014) 7.25 - - 33.71 
metaqnn (ensemble) 7.32 2.06 0.32 - 
metaqnn (top model) 6.92 2.28 0.44 27.14∗ 

tabl 3: error rate comparison with cnn that use convolution, pooling, and fulli connect 
layer alone. We report result for cifar-10 and cifar-100 with moder data augment and 
result for mnist and svhn without ani data augmentation. 

the top model select by the q-learn agent vari in the number of paramet but all demon- 
strate high perform (see appendix tabl 1-3). for example, the number of paramet for the 
top five cifar-10 model rang from 11.26 million to 1.10 million, with onli a 2.32% decreas 
in test error. We find design motif common to the top hand-craft network architectur a well. 
for example, the agent often choos a layer of type c(n, 1, 1) a the first layer in the network. 
these layer gener N learnabl linear transform of the input data, which be similar in spirit 
to preprocess of input data from rgb to a differ color space such a yuv, a found in prior 
work (sermanet et al., 2012; 2013). 

predict performance: We compar the predict perform of the metaqnn network dis- 
cover by theq-learn agent with state-of-the-art method on three datasets. We report the accu- 
raci of our best model, along with an ensembl of top five models. first, we compar metaqnn with 
six exist architectur that be design with standard convolution, pooling, and fully-connect 
layer alone, similar to our designs. As see in tabl 3, our top model alone, a well a the com- 
mitte ensembl of five models, outperform all similar models. next, we compar our result with 
six top network overall, which contain complex layer type and design ideas, includ gener 
pool functions, residu connections, and recurr modules. our result be competit with 
these method a well (tabl 4). finally, our method outperform exist autom network de- 
sign methods. metaqnn obtain an error of 6.92% a compar to 21.2% report by bergstra et al. 
(2011) on cifar-10; and it obtain an error of 0.32% a compar to 7.9% report by verbancs 
& harguess (2013) on mnist. 

the differ in valid error between the top 10 model for mnist be veri small, so we also 
creat an ensembl with all 10 models. thi ensembl achiev a test error of 0.28%—which beat 
the current state-of-the-art on mnist without data augmentation. 

the best cifar-10 model perform 1-2% good than the four next best models, which be whi the 
ensembl accuraci be low than the best model’ accuracy. We posit that the cifar-10 metaqnn 

7 



under review a a confer paper at iclr 2017 

method cifar-10 svhn mnist cifar-100 
dropconnect (wan et al., 2013) 9.32 1.94 0.57 - 
dsn (lee et al., 2015) 8.22 1.92 0.39 34.57 
r-cnn (liang & hu, 2015) 7.72 1.77 0.31 31.75 
metaqnn (ensemble) 7.32 2.06 0.32 - 
metaqnn (top model) 6.92 2.28 0.44 27.14∗ 
resnet(110) (he et al., 2015) 6.61 - - - 
elu (clevert et al., 2015) 6.55 - - 24.28 
tree+max-avg (lee et al., 2016) 6.05 1.69 0.31 32.37 

tabl 4: error rate comparison with state-of-the-art method with complex layer types. We re- 
port result for cifar-10 and cifar-100 with moder data augment and result for mnist 
and svhn without ani data augmentation. 

dataset cifar-100 svhn mnist 
train from scratch 27.14 2.48 0.80 
finetun 34.93 4.00 0.81 
state-of-the-art 24.28 (clevert et al., 2015) 1.69 (lee et al., 2016) 0.31 (lee et al., 2016) 

tabl 5: predict error for the top metaqnn (cifar-10) model train for other tasks. fine- 
tune refer to initi train with the weight found for the optim cifar-10 model. 

do not have adequ explor time give the larg state space compar to that of the svhn 
experiment, caus it to not find more model with perform similar to the best model. fur- 
thermore, the coars train scheme could have be not a well suit for cifar-10 a it be for 
svhn, caus some model to under perform. 

transfer learn ability: network design such a vggnet (simonyan & zisserman, 2014) can 
be adopt to solv a varieti of comput vision problems. To check if the metaqnn network 
provid similar transfer learn ability, we use the best metaqnn model on the cifar-10 dataset 
for train other comput vision tasks. the model perform well (tabl 5) both when train 
from random initializations, and finetun from exist weights. 

7 conclud remark 

neural network be be use in an increasingli wide varieti of domains, which call for scalabl 
solut to produc problem-specif model architectures. We take a step toward thi goal and 
show that a meta-model approach use reinforc learn be abl to gener tailor cnn 
design for differ imag classif tasks. our metaqnn network outperform previou meta- 
model method a well a hand-craft network which use the same type of layers. 

while we report result for imag classif problems, our method could be appli to differ- 
ent problem settings, includ supervis (e.g., classification, regression) and unsupervis (e.g., 
autoencoders). the metaqnn method could also aid constraint-bas network design, by optimiz- 
ing paramet such a size, speed, and accuracy. for instance, one could add a threshold in the 
state-act space bar the agent from creat model larg than the desir limit. In addition, 
one could modifi the reward function to penal larg model for constrain memori or penal 
slow forward pass to incent quick inference. 

there be sever futur avenu for research in reinforc learning-driven network design a 
well. In our current implementation, we use the same set of hyperparamet to train all network 
topolog dure the q-learn phase and further finetun the hyperparamet for top model 
select by the metaqnn agent. however, our approach could be combin with hyperparamet 
optim method to further autom the network design process. moreover, we constrict the 
state-act space use coarse, discret bin to acceler convergence. It would be possibl to 
move to larg state-act space use method for q-function approxim (bertsekas, 2015; 
mnih et al., 2015). 

∗result in thi column obtain with the top metaqnn architectur for cifar-10, train from random 
initi with cifar-100 data. 

8 



under review a a confer paper at iclr 2017 

acknowledg 

We thank peter down for creat the project websit and contribut to illustrations. We ac- 
knowledg center for bit and atom at mit for their help with comput resources. finally, we 
thank member of camera cultur group at mit media lab for their help and support. 

refer 
sander adam, lucian busoniu, and robert babuska. experi replay for real-tim reinforc 

learn control. ieee transact on systems, man, and cybernetics, part C (applic and 
reviews), 42(2):201–212, 2012. 

jame bergstra, daniel yamins, and david D cox. make a scienc of model search: hyperpa- 
ramet optim in hundr of dimens for vision architectures. icml (1), 28:115–123, 
2013. 

jame S bergstra, rémi bardenet, yoshua bengio, and baláz kégl. algorithm for hyper-paramet 
optimization. nips, pp. 2546–2554, 2011. 

dimitri P bertsekas. convex optim algorithms. athena scientif belmont, 2015. 

djork-arné clevert, thoma unterthiner, and sepp hochreiter. fast and accur deep network 
learn by exponenti linear unit (elus). arxiv preprint arxiv:1511.07289, 2015. 

tobia domhan, jost tobia springenberg, and frank hutter. speed up automat hyperparame- 
ter optim of deep neural network by extrapol of learn curves. ijcai, 2015. 

xavier glorot and yoshua bengio. understand the difficulti of train deep feedforward neural 
networks. aistats, 9:249–256, 2010. 

ian J goodfellow, david warde-farley, mehdi mirza, aaron C courville, and yoshua bengio. max- 
out networks. icml (3), 28:1319–1327, 2013. 

kaim he, xiangyu zhang, shaoq ren, and jian sun. deep residu learn for imag recog- 
nition. arxiv preprint arxiv:1512.03385, 2015. 

yangq jia, evan shelhamer, jeff donahue, sergey karayev, jonathan long, ross girshick, ser- 
gio guadarrama, and trevor darrell. caffe: convolut architectur for fast featur embed- 
ding. arxiv preprint arxiv:1408.5093, 2014. 

lesli pack kaelbling, michael L littman, and andrew W moore. reinforc learning: A 
survey. journal of artifici intellig research, 4:237–285, 1996. 

diederik kingma and jimmi ba. adam: A method for stochast optimization. arxiv preprint 
arxiv:1412.6980, 2014. 

yann lecun, yoshua bengio, and geoffrey hinton. deep learning. nature, 521(7553):436–444, 
2015. 

chen-yu lee, sain xie, patrick gallagher, zhengyou zhang, and zhuowen tu. deeply- 
supervis nets. aistats, 2(3):6, 2015. 

chen-yu lee, patrick W gallagher, and zhuowen tu. gener pool function in convolu- 
tional neural networks: mixed, gated, and tree. intern confer on artifici intellig 
and statistics, 2016. 

sergey levine, chelsea finn, trevor darrell, and pieter abbeel. end-to-end train of deep visuo- 
motor policies. jmlr, 17(39):1–40, 2016. 

ming liang and xiaolin hu. recurr convolut neural network for object recognition. cvpr, 
pp. 3367–3375, 2015. 

timothi P lillicrap, jonathan J hunt, alexand pritzel, nicola heess, tom erez, yuval tassa, 
david silver, and daan wierstra. continu control with deep reinforc learning. arxiv 
preprint arxiv:1509.02971, 2015. 

9 



under review a a confer paper at iclr 2017 

long-ji lin. self-improv reactiv agent base on reinforc learning, plan and teaching. 
machin learning, 8(3-4):293–321, 1992. 

long-ji lin. reinforc learn for robot use neural networks. technic report, dtic 
document, 1993. 

min lin, qiang chen, and shuicheng yan. network in network. arxiv preprint arxiv:1312.4400, 
2013. 

volodymyr mnih, koray kavukcuoglu, david silver, andrei A rusu, joel veness, marc G belle- 
mare, alex graves, martin riedmiller, andrea K fidjeland, georg ostrovski, et al. human-level 
control through deep reinforc learning. nature, 518(7540):529–533, 2015. 

nicola pinto, david doukhan, jame J dicarlo, and david D cox. A high-throughput screen 
approach to discov good form of biolog inspir visual representation. plo compu- 
tation biology, 5(11):e1000579, 2009. 

adriana romero, nicola ballas, samira ebrahimi kahou, antoin chassang, carlo gatta, and 
yoshua bengio. fitnets: hint for thin deep nets. arxiv preprint arxiv:1412.6550, 2014. 

J david schaffer, darrel whitley, and larri J eshelman. combin of genet algorithm and 
neural networks: A survey of the state of the art. intern workshop on combin of 
genet algorithm and neural networks, pp. 1–37, 1992. 

pierr sermanet, soumith chintala, and yann lecun. convolut neural network appli to 
hous number digit classification. icpr, pp. 3288–3291, 2012. 

pierr sermanet, koray kavukcuoglu, soumith chintala, and yann lecun. pedestrian detect 
with unsupervis multi-stag featur learning. cvpr, pp. 3626–3633, 2013. 

bobak shahriari, kevin swersky, ziyu wang, ryan P adams, and nando de freitas. take the 
human out of the loop: A review of bayesian optimization. proceed of the ieee, 104(1): 
148–175, 2016. 

david silver, aja huang, chri J maddison, arthur guez, laurent sifre, georg van den driessche, 
julian schrittwieser, ioanni antonoglou, veda panneershelvam, marc lanctot, et al. master 
the game of go with deep neural network and tree search. nature, 529(7587):484–489, 2016. 

karen simonyan and andrew zisserman. veri deep convolut network for large-scal imag 
recognition. arxiv preprint arxiv:1409.1556, 2014. 

jasper snoek, hugo larochelle, and ryan P adams. practic bayesian optim of machin 
learn algorithms. nips, pp. 2951–2959, 2012. 

jost tobia springenberg, alexey dosovitskiy, thoma brox, and martin riedmiller. strive for 
simplicity: the all convolut net. arxiv preprint arxiv:1412.6806, 2014. 

rupesh kumar srivastava, klau greff, and jürgen schmidhuber. highway networks. arxiv preprint 
arxiv:1505.00387, 2015. 

kenneth O stanley and risto miikkulainen. evolv neural network through augment topolo- 
gies. evolutionari computation, 10(2):99–127, 2002. 

kevin swersky, jasper snoek, and ryan P adams. multi-task bayesian optimization. nips, pp. 
2004–2012, 2013. 

phillip verbancs and josh harguess. gener neuroevolut for deep learning. arxiv preprint 
arxiv:1312.5355, 2013. 

joann vermorel and mehryar mohri. multi-arm bandit algorithm and empir evaluation. 
european confer on machin learning, pp. 437–448, 2005. 

Li wan, matthew zeiler, sixin zhang, yann L cun, and rob fergus. regular of neural 
network use dropconnect. icml, pp. 1058–1066, 2013. 

christoph john cornish hellabi watkins. learn from delay rewards. phd thesis, univers 
of cambridge, england, 1989. 

10 



under review a a confer paper at iclr 2017 

appendix 

A algorithm 

We first describ the main compon of the metaqnn algorithm. algorithm 1 show the main 
loop, where the paramet M would determin how mani model to run for a give � and the 
paramet K would determin how mani time to sampl the replay databas to updat q-valu on 
each iteration. the function train refer to train the specifi network and return a valid 
accuracy. algorithm 2 detail the method for sampl a new network use the �-greedi strategy, 
where we assum we have a function transit that return the next state give a state and 
action. finally, algorithm 3 implement theq-valu updat detail in equat 3, with discount 
factor set to 1, for an entir state sequenc in tempor revers order. 

algorithm 1 q-learn for cnn topolog 
initialize: 

replay memory← [ ] 
Q← {(s, u) ∀s ∈ S, u ∈ u(s) : 0.5} 

for episod = 1 to M do 
S, U ← sampl new network(�, Q) 
accuracy← train(s) 
replay memory.append((s, U, accuracy)) 
for memori = 1 to K do 

ssampl , usampl , accuracysampl ← uniform{replay memory} 
Q← updat Q values(q, ssampl , usampl , accuracysample) 

end for 
end for 

algorithm 2 sampl new network(�, Q) 
initialize: 

state sequenc S = [sstart] 
action sequenc U = [ ] 

while U [−1] 6= termin do 
α ∼ uniform[0, 1) 
if α > � then 

u = argmaxu∈u(s[−1])q[(s[−1], u)] 
s′ = transition(s[−1], u) 

els 
u ∼ uniform{u(s[−1])} 
s′ = transition(s[−1], u) 

end if 
u.append(u) 
if u != termin then 

s.append(s′) 
end if 

end while 
return S, U 

algorithm 3 updat Q values(q, S, U , accuracy) 
q[s[−1], U [−1]] = (1− α)q[s[−1], U [−1]] + α · accuraci 
for i = length(s)− 2 to 0 do 

q[s[i], U [i]] = (1− α)q[s[i], U [i]] + αmaxu∈u(s[i+1])q[s[i+ 1], u] 
end for 
return Q 

11 



under review a a confer paper at iclr 2017 

B represent size bin 

As mention in section 4.1 of the main text, we introduc a paramet call represent size 
to prohibit the agent from take action that can reduc the intermedi signal represent to 
a size that be too small for further processing. however, thi process lead to uncertainti in state 
transitions, a illustr in figur a1, which be handl by the standard q-learn formulation. 

p(2,2) 

r-size: 18 
r-size bin: 1 

r-size: 9 
r-size bin: 1 

(a) 

p(2,2) 

r-size: 9 
r-size bin: 1 

r-size: 14 
r-size bin: 1 

(b) 

state 
action 

p 
1 2 

p 

r-size bin: 1 

r-size bin: 1 r-size bin: 2 

p(2,2) 

(c) 

figur a1: represent size binning: In thi figure, we show three exampl state transitions. 
the true represent size (r-size) paramet be includ in the figur to show the true underli 
state. assum there be two r-size bins, r-size bin1: [8,∞) and r-size bin2: (0, 7], figur a1a 
show the case where the initi state be in r-size bin1 and true represent size be 18. after the 
agent choos to pool with a 2×2 filter with stride 2, the true represent size reduc to 9 but the 
r-size bin do not change. In figur a1b, the same 2 × 2 pool layer with stride 2 reduc the 
actual represent size of 14 to 7, but the bin chang to r-size bin2. therefore, in figur a1a 
and a1b, the agent end up in differ final states, despit origin in the same initi state and 
choos the same action. figur a1c show that in our state-act space, when the agent take an 
action that reduc the represent size, it will have uncertainti in which state it will transit to. 

C mnist experi 

We notic that the final mnist model be prone to overfitting, so we increas dropout and 
do a small grid search for the weight regular parameter. for both tune and final training, 
we warm the model with the learn weight from after the first epoch of initi training. the 
final model and solver can be found on our project websit https://bowenbaker.github.io/metaqnn/ . 
figur A2 show the q-learn perform for the mnist experiment. 

D further analysi OF q-learn 

figur 3 of the main text and figur A2 show that a the agent begin to exploit, it improv in 
architectur selection. It be also inform to look at the distribut of model chosen at each �. 
figur A3 give further insight into the perform achiev at each � for both experiments. 

d.1 q-valu analysi 

We now analyz the actualq-valu gener by the agent dure the train process. the learn 
agent iter updat the q-valu of each path dure the �-greedi exploration. each q-valu 
be initi at 0.5. after the �-schedul be complete, we can analyz the final q-valu associ 
with each path to gain insight into the layer select process. In left column of figur a4, we 
plot the averag q-valu for each layer type at differ layer depth (for both svhn and cifar- 
10) datasets. roughli speaking, a high q-valu associ with a layer type indic a high 
probabl that the agent will pick that layer type. In figur a4, we observ that, while the averag 
q-valu be high for convolut and pool layer at low layer depths, the q-valu for fully- 
connect and termin layer (softmax and global averag pooling) increas a we go deeper 
into the network. thi observ match with tradit network designs. 

12 

https://bowenbaker.github.io/metaqnn/ 


under review a a confer paper at iclr 2017 

0 500 1000 1500 2000 2500 3000 3500 
iter 

0.00 

0.10 

0.20 

0.30 

0.40 

0.50 

0.60 

0.70 

0.80 

0.90 

1.00 

A 
cc 

u 
ra 

cy 

epsilon = 1.0 .9.8.7 .6 .5 .4 .3 .2 .1 

mnist q-learn perform 

averag accuraci per epsilon 

roll mean model accuraci 

figur a2: mnist q-learn performance. the blue line show a roll mean of model 
accuraci versu iteration, where in each iter of the algorithm the agent be sampl a model. 
each bar (in light blue) mark the averag accuraci over all model that be sampl dure the 
explor phase with the label �. As � decreases, the averag accuraci go up, demonstr 
that the agent learn to select better-perform cnn architectures. 

We can also plot the averageq-valu associ with differ layer paramet for further analysis. 
In the right column of figur a4, we plot the averageq-valu for convolut layer with recept 
field size 1, 3, and 5 at differ layer depths. the plot show that layer with recept field size 
of 5 have a high q-valu a compar to size 1 and 3 a we go deeper into the networks. thi 
indic that it might be benefici to use larg recept field size in deeper networks. 

In summary, the q-learn method enabl u to perform analysi on rel benefit of differ 
design paramet of our state space, and possibl gain insight for new cnn designs. 

E top topolog select BY algorithm 

In tabl A1 through a3, we present the top five model architectur select with q-learn 
for each dataset, along with their predict error report on the test set, and their to- 
tal number of parameters. To download the caff solver and prototext files, pleas visit 
https://bowenbaker.github.io/metaqnn/ . 

model architectur test error (%) # param (106) 
[c(512,5,1), c(256,3,1), c(256,5,1), c(256,3,1), p(5,3), c(512,3,1), 
c(512,5,1), p(2,2), sm(10)] 

6.92 11.18 

[c(128,1,1), c(512,3,1), c(64,1,1), c(128,3,1), p(2,2), c(256,3,1), 
p(2,2), c(512,3,1), p(3,2), sm(10)] 

8.78 2.17 

[c(128,3,1), c(128,1,1), c(512,5,1), p(2,2), c(128,3,1), p(2,2), 
c(64,3,1), c(64,5,1), sm(10)] 

8.88 2.42 

[c(256,3,1), c(256,3,1), p(5,3), c(256,1,1), c(128,3,1), p(2,2), 
c(128,3,1), sm(10)] 

9.24 1.10 

[c(128,5,1), c(512,3,1), p(2,2), c(128,1,1), c(128,5,1), p(3,2), 
c(512,3,1), sm(10)] 

11.63 1.66 

tabl a1: top 5 model architectures: cifar-10. 

13 

https://bowenbaker.github.io/metaqnn/ 


under review a a confer paper at iclr 2017 

model architectur test error (%) # param (106) 
[c(128,3,1), p(2,2), c(64,5,1), c(512,5,1), c(256,3,1), c(512,3,1), 
p(2,2), c(512,3,1), c(256,5,1), c(256,3,1), c(128,5,1), c(64,3,1), 
sm(10)] 

2.24 9.81 

[c(128,1,1), c(256,5,1), c(128,5,1), p(2,2), c(256,5,1), c(256,1,1), 
c(256,3,1), c(256,3,1), c(256,5,1), c(512,5,1), c(256,3,1), 
c(128,3,1), sm(10)] 

2.28 10.38 

[c(128,5,1), c(128,3,1), c(64,5,1), p(5,3), c(128,3,1), c(512,5,1), 
c(256,5,1), c(128,5,1), c(128,5,1), c(128,3,1), sm(10)] 

2.32 6.83 

[c(128,1,1), c(256,5,1), c(128,5,1), c(256,3,1), c(256,5,1), p(2,2), 
c(128,1,1), c(512,3,1), c(256,5,1), p(2,2), c(64,5,1), c(64,1,1), 
sm(10)] 

2.35 6.99 

[c(128,1,1), c(256,5,1), c(128,5,1), c(256,5,1), c(256,5,1), 
c(256,1,1), p(3,2), c(128,1,1), c(256,5,1), c(512,5,1), c(256,3,1), 
c(128,3,1), sm(10)] 

2.36 10.05 

tabl a2: top 5 model architectures: svhn. note that we do not report the best accuraci on test 
set from the abov model in tabl 3 and 4 from the main text. thi be becaus the model that 
achiev 2.28% on the test set perform the best on the valid set. 

model architectur test error (%) # param (106) 
[c(64,1,1), c(256,3,1), p(2,2), c(512,3,1), c(256,1,1), p(5,3), 
c(256,3,1), c(512,3,1), fc(512), sm(10)] 

0.35 5.59 

[c(128,3,1), c(64,1,1), c(64,3,1), c(64,5,1), p(2,2), c(128,3,1), p(3,2), 
c(512,3,1), fc(512), fc(128), sm(10)] 

0.38 7.43 

[c(512,1,1), c(128,3,1), c(128,5,1), c(64,1,1), c(256,5,1), c(64,1,1), 
p(5,3), c(512,1,1), c(512,3,1), c(256,3,1), c(256,5,1), c(256,5,1), 
sm(10)] 

0.40 8.28 

[c(64,3,1), c(128,3,1), c(512,1,1), c(256,1,1), c(256,5,1), c(128,3,1), 
p(5,3), c(512,1,1), c(512,3,1), c(128,5,1), sm(10)] 

0.41 6.27 

[c(64,3,1), c(128,1,1), p(2,2), c(256,3,1), c(128,5,1), c(64,1,1), 
c(512,5,1), c(128,5,1), c(64,1,1), c(512,5,1), c(256,5,1), c(64,5,1), 
sm(10)] 

0.43 8.10 

[c(64,1,1), c(256,5,1), c(256,5,1), c(512,1,1), c(64,3,1), p(5,3), 
c(256,5,1), c(256,5,1), c(512,5,1), c(64,1,1), c(128,5,1), c(512,5,1), 
sm(10)] 

0.44 9.67 

[c(128,3,1), c(512,3,1), p(2,2), c(256,3,1), c(128,5,1), c(64,1,1), 
c(64,5,1), c(512,5,1), gap(10), sm(10)] 

0.44 3.52 

[c(256,3,1), c(256,5,1), c(512,3,1), c(256,5,1), c(512,1,1), p(5,3), 
c(256,3,1), c(64,3,1), c(256,5,1), c(512,3,1), c(128,5,1), c(512,5,1), 
sm(10)] 

0.46 12.42 

[c(512,5,1), c(128,5,1), c(128,5,1), c(128,3,1), c(256,3,1), 
c(512,5,1), c(256,3,1), c(128,3,1), sm(10)] 

0.55 7.25 

[c(64,5,1), c(512,5,1), p(3,2), c(256,5,1), c(256,3,1), c(256,3,1), 
c(128,1,1), c(256,3,1), c(256,5,1), c(64,1,1), c(256,3,1), c(64,3,1), 
sm(10)] 

0.56 7.55 

tabl a3: top 10 model architectures: mnist. We report the top 10 model for mnist becaus 
we includ all 10 in our final ensemble. note that we do not report the best accuraci on test set 
from the abov model in tabl 3 and 4 from the main text. thi be becaus the model that achiev 
0.44% on the test set perform the best on the valid set. 

14 



under review a a confer paper at iclr 2017 

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 

valid accuraci 

0 

10 

20 

30 

40 

50 

60 

% 
M 

o 
d 
e 
l 

model accuraci distribut 
(svhn) 

epsilon 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1.0 

(a) 

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 

valid accuraci 

0 

10 

20 

30 

40 

50 

60 

% 
M 

o 
d 
e 
l 

model accuraci distribut 
(svhn) 

epsilon 

0.1 1.0 

(b) 

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

valid accuraci 

0 

5 

10 

15 

20 

% 
M 

o 
d 
e 
l 

model accuraci distribut 
(cifar-10) 

epsilon 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1.0 

(c) 

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

valid accuraci 

0 

5 

10 

15 

20 
% 

M 
o 
d 
e 
l 

model accuraci distribut 
(cifar-10) 

epsilon 

0.1 1.0 

(d) 

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
valid accuraci 

0 

20 

40 

60 

80 

100 

% 
M 

od 
el 

s 

model accuraci distribut 
(mnist) 

epsilon 
0.1 
0.2 
0.3 
0.4 
0.5 

0.6 
0.7 
0.8 
0.9 
1.0 

(e) 

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
valid accuraci 

0 

20 

40 

60 

80 

100 

% 
M 

od 
el 

s 

model accuraci distribut 
(mnist) 

epsilon 
0.1 1.0 

(f) 

figur a3: accuraci distribut versu �: figur a3a, a3c, and a3e show the accuraci dis- 
tribut for each � for the svhn, cifar-10, and mnist experiments, respectively. figur a3b, 
a3d, and a3f show the accuraci distribut for the initi � = 1 and the final � = 0.1. one can 
see that the accuraci distribut becom much more peak in the high accuraci rang at small � 
for each experiment. 

15 



under review a a confer paper at iclr 2017 

0 2 4 6 8 10 12 14 
layer depth 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

A 
ve 

ra 
ge 

Q 
-V 

al 
ue 

averag q-valu vs. layer depth 
(svhn) 

convolut 
fulli connect 
pool 
global averag pool 
softmax 

(a) 

0 2 4 6 8 10 12 
layer depth 

0.5 

0.6 

0.7 

0.8 

0.9 

1.0 

A 
ve 

ra 
ge 

Q 
-V 

al 
ue 

averag q-valu vs. layer depth 
for convolut layer (svhn) 

recept field size 1 
recept field size 3 
recept field size 5 

(b) 

0 5 10 15 20 
layer depth 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

A 
ve 

ra 
ge 

Q 
-V 

al 
ue 

averag q-valu vs. layer depth 
(cifar10) 

convolut 
fulli connect 
pool 
global averag pool 
softmax 

(c) 

0 2 4 6 8 10 12 14 16 18 
layer depth 

0.5 

0.6 

0.7 

0.8 

0.9 

1.0 
A 

ve 
ra 

ge 
Q 

-V 
al 

ue 

averag q-valu vs. layer depth 
for convolut layer (cifar10) 

recept field size 1 
recept field size 3 
recept field size 5 

(d) 

0 2 4 6 8 10 12 14 
layer depth 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

A 
ve 

ra 
ge 

Q 
-V 

al 
ue 

averag q-valu vs. layer depth 
(mnist) 

convolut 
fulli connect 
pool 
global averag pool 
softmax 

(e) 

0 2 4 6 8 10 12 
layer depth 

0.5 

0.6 

0.7 

0.8 

0.9 

1.0 

A 
ve 

ra 
ge 

Q 
-V 

al 
ue 

averag q-valu vs. layer depth 
for convolut layer (mnist) 

recept field size 1 
recept field size 3 
recept field size 5 

(f) 

figur a4: averag q-valu versu layer depth for differ layer type be show in the left 
column. averag q-valu versu layer depth for differ recept field size of the convolut 
layer be show in the right column. 

16 


1 introduct 
2 relat work 
3 background 
4 design neural network architectur with q-learn 
4.1 the state space 
4.2 the action space 
4.3 q-learn train procedur 

5 experi detail 
6 result 
7 conclud remark 
A algorithm 
B represent size bin 
C mnist experi 
D further analysi of q-learn 
d.1 q-valu analysi 

E top topolog select by algorithm 

