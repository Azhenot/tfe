


















































ES Is more than just a tradit finite-differ 
approxim 

joel lehman, jay chen, jeff clune, and kenneth O. stanley 
uber AI lab 

san francisco, CA 94103 
{joel.lehman,jayc,jeffclune,kstanley}@uber.com 

abstract 

An evolut strategi (es) variant recent attract signific attent due to 
it surprisingli good perform at optim neural network in challeng 
deep reinforc learn domains. It search directli in the paramet space 
of neural network by gener perturb to the current set of parameters, 
check their performance, and move in the direct of high reward. the 
resembl of thi algorithm to a tradit finite-differ approxim of the 
reward gradient in paramet space natur lead to the assumpt that it be just 
that. however, thi assumpt be incorrect. the aim of thi paper be to definit 
demonstr thi point empirically. ES be a gradient approximator, but optim for 
a differ gradient than just reward (especi when the magnitud of candid 
perturb be high). instead, it optim for the averag reward of the entir 
population, often also promot paramet that be robust to perturbation. thi 
differ can channel ES into significantli differ area of the search space 
than gradient descent in paramet space, and also consequ to network with 
significantli differ properties. thi uniqu robustness-seek property, and it 
consequ for optimization, be demonstr in sever domains. they includ 
humanoid locomotion, where network from polici gradient-bas reinforc 
learn be far less robust to paramet perturb than es-bas polici that 
solv the same task. while the implic of such robust and robustness- 
seek remain open to further study, the main contribut of thi work be to 
highlight that such differ inde exist and deserv attention. 

1 introduct 

saliman et al. (2017) recent demonstr that an evolutionary-inspir approach can compet on 
modern reinforc learn (rl) benchmark that requir large-scal deep learn architectures. 
while the field of evolut strategi (es; schwefel 1993) have a rich histori encompass a broad 
varieti of search algorithm (see beyer and schwefel 2002), saliman et al. (2017) have drawn 
attent to the particular form of ES appli in that paper (which do not reflect the field a a 
whole), in effect a simplifi version of natur ES (nes; wierstra et al. 2014). becaus thi form 
of ES be the focu of thi paper, herein it be refer to simpli a es. one way to view ES be a a 
polici gradient algorithm appli to the paramet space instead of to the state space a be more 
typic (williams, 1992), and the distribut of paramet (rather than actions) be optim to 
maxim the expect of performance. central to thi interpret be how ES estim (and 
follows) the gradient of increas perform with respect to the current distribut of parameters. 
In particular, in ES mani independ paramet vector be drawn from the current distribution, 
their perform be evaluated, and thi inform be then aggreg to estim a gradient of 
distribut improvement. 

ar 
X 

iv 
:1 

71 
2. 

06 
56 

8v 
1 

[ 
c 

.N 
E 

] 
1 

8 
D 

ec 
2 

01 
7 



the implement of thi approach be similar in it realiz to a finite-differ gradient 
estim (richardson, 1911; spall, 1992), wherein evalu tini perturb of a paramet 
vector contribut to a direct calcul of the gradient of performance. As a result, it may be 
attract to interpret the result of saliman et al. (2017) sole through the len of finit differ 
(e.g. a in ebrahimi et al. 2017), conclud that the method be interest or effect onli becaus 
it be approxim the gradient of perform with respect to the parameters. however, such 
a hypothesi ignor that es’ object function be interestingli differ from tradit finit 
differences, which thi paper argu grant it addit properti of interest. In particular, ES 
optim the perform of ani draw from the learn distribut of paramet (call the 
search distribution), while finit differ optim the perform of one particular set of 
the domain parameters. the main contribut of thi paper be to support the hypothesi that thi 
subtl distinct may in fact be import to understand the behavior of ES (and futur nes-lik 
approaches), by conduct experi that highlight how ES be driven to more robust area of the 
search space than either finit differ or a more tradit evolutionari approach. the push 
toward robust carri potenti implic for RL and other applic of deep learn that 
could be miss without highlight it specifically. 

note that the core interest of thi paper be to clarifi a subtl but interest possibl misconception, 
and not mere to debat the semant of what exactli do or do not qualifi a a finite-differ 
approximator. the frame here be that a tradit finite-differ gradient approxim make 
tini perturb of domain paramet to estim the gradient of improv for the current 
point in the search space. while it be true that ES also stochast follow a gradient (i.e. the 
search gradient of how to improv expect perform across the search distribut repres a 
cloud in the paramet space), it do not do so by standard applic of common finite-differ 
methods. In ani case, the most import distinct be that ES optim the expect valu of a 
distribut of paramet with fix variance, while tradit finit differ optim a singular 
optim paramet vector. 

To explor the empir effect of thi distinction, thi paper first us simpl two-dimension fit 
landscap to highlight systemat differ between the behavior of ES and other optim 
approaches. the result from these theoret experi be then valid in the humanoid 
locomot RL benchmark domain, show that es’ drive toward robust manifest also in 
complex domains. interestingly, paramet vector result from ES be much more robust than 
those of similar perform discov by a genet algorithm (ga) or by a non-evolutionari polici 
gradient approach (trpo). 

these result have implic for research in evolutionari comput (ec; De jong 2002) who 
have long be interest in properti like robust (wilk et al., 2001; wagner, 2008; lenski 
et al., 2006) and evolv (wagner and altenberg, 1996; kirschner and gerhart, 1998; lehman 
and stanley, 2013), and also for deep learn research seek to more fulli understand ES and 
how it relat to gradient-bas methods. 

2 background 

the method of finit differ be review first, follow by the relat approach of follow 
search gradient appli by es. finally, method that (like es) encourag robust in EC be 
discussed. 

2.1 finit differ 

one popular numer approach to estim the gradient of a function be the finite-differ 
method. the idea be that a tini (but finite) perturb be appli to the paramet of a scalar-valu 
system (i.e. a system output a scalar), and that evalu the effect of such perturb enabl 
approxim the deriv with respect to the parameters. such a method be use for optim 
when the system a a whole be not differentiable, e.g. in a RL context, when reward come from 
a partially-observ or analytically-intract environment. indeed, becaus of it conceptu 
simplic there be mani polici gradient method motiv by the finite-differ approach (spall, 
1992; glynn, 1987). 

2 



one common finite-differ estim of the deriv of function f with respect to the scalar x be 
give by: 

f ′(x) ≈ f(x) + f(x+ δ) 
δ 

, 

give some small constant δ. thi estim gener natur to the set of paramet vectors, 
where the partial deriv with respect to each element of such a vector can be similarli calculated; 
however, naiv finit differ scale poorli with the size of the problem, a it perturb each param- 
eter individually, make it applic infeas for larg problem (like optim deep neural 
networks). however, finite-difference-bas method such a simultan perturb stochast 
approxim (spsa; spall 1992) can aggreg inform from independ perturb of 
all paramet simultan to estim the gradient more efficiently. indeed, spsa be similar in 
implement to es. 

however, the motiv for tradit finite-differ methods, importantly, be to use tini pertur- 
bations; the larg such perturb become, the less meaning it approxim the underli 
gradient, which be formal defin a the slope of the function with respect to it paramet at a 
particular point. In other words, a perturb becom larger, finit differ becom qualita- 
tive disconnect from the principl motiv it construction; it estim becom increasingli 
influenc by the curvatur of the reward function, and it interpret becom unclear. thi 
consider be import becaus ES be not motiv by tini perturb nor by approxim 
the gradient of perform for ani singular set of parameters, a the next section describ in 
more detail. 

2.2 search gradient 

instead of search directli for a singl high-perform set of a paramet vector, a be typic 
in gradient descent and finite-differ methods, a distinct (but related) approach be to optim the 
search distribut of domain paramet to achiev high averag reward when a particular paramet 
vector be sampl from the distribut (berny, 2000; wierstra et al., 2014; sehnk et al., 2010). 
do so requir follow search gradient (berny, 2000; wierstra et al., 2014), i.e. gradient of 
increas expect fit with respect to distribut paramet (e.g. the mean and varianc of a 
gaussian distribution). 

while the procedur for follow such search gradient us mechan similar to a finite-differ 
gradient approxim (i.e. it involv aggreg fit inform from sampl of domain 
paramet in a local neighborhood), importantli the underli object function from which it 
deriv be different: 

j(θ) = eθf(z) = 

∫ 
f(z)π(z|θ)dz, (1) 

where f(z) be the fit function, and z be a sampl from the search distribut π(z|θ) specifi by 
paramet θ. equat 1 formal the idea that es’ object (like other search-gradi methods) 
be to optim the distribut paramet such that the expect fit of domain paramet drawn 
from that search distribut be maximized. In contrast, the object function for more tradit 
gradient descent approach be to find the optim domain paramet directly: j(θ) = f(θ). 

while ne allow for adjust both the mean and varianc of a search distribution, in the ES of 
saliman et al. (2017), the evolv distribut paramet control onli the mean of a gaussian 
distribut and not it variance. As a result, ES cannot reduc varianc of potentially-sensit 
parameters; importantly, the implic be that ES will be driven toward robust area of the search 
space. for example, imagin two path through the search space of similarli increas reward, where 
one path requir precis set of domain paramet (i.e. onli a low-vari search distribut 
could captur such precision) while the other do not. In thi scenario, ES with a sufficiently-high 
varianc set will onli be abl to follow the latter path, in which perform be gener robust 
to paramet perturbations. the experi in thi paper illumin circumst in which thi 
robust properti of ES impact search. note that the relationship of low-vari ES (which bear 
strong similar to finit differences) to sgd be explor in more depth in zhang et al. (2017). 

3 



2.3 robust in evolutionari comput 

research in EC have long be concern with robust in the face of mutat (wilk et al., 
2001; wagner, 2008; lenski et al., 2006), i.e. the idea that randomli mutat a genotyp will not 
devast it functionality. In particular, evolv genotyp in EC often lack the appar robust 
of natur organ (lehman and stanley, 2011b), which can hinder progress in an evolutionari 
algorithm (ea). In other words, robust be import for it link to evolv (kirschner and 
gerhart, 1998; wagner and altenberg, 1996), or the abil of evolut to gener product 
herit variation. 

As a result, EC research have introduc mechan use to encourag robustness, such a 
self-adapt (meyer-nieberg and beyer, 2007), wherein evolut can modifi or control aspect of 
gener variation. notably, however, such mechan can emphas robust over evolv 
depend on select pressur (clune et al., 2008; lehman and stanley, 2011b), i.e. robust 
can be trivial maxim when a genotyp encod that it should be subject onli to trivial 
perturbations. ES avoid thi potenti patholog becaus the varianc of it distribut be fixed, 
although in a full implement of ne varianc be subject to optim and the robustness- 
evolv trade-off would like re-emerge. 

while the experi in thi paper show that ES be drawn to robust area of the search space a a 
direct consequ of it object (i.e. to maxim expect fit across it search distribution), 
in more tradit ea healthi robust be often a second-ord effect (lehman and stanley, 
2011b; kounio et al., 2016; wilk et al., 2001). for example, if an EA lack elit and mutat 
rate be high, evolut favor more robust optimum although it be not a direct object of search 
(wilk et al., 2001); similarly, when select pressur reward phenotyp or behavior divergence, 
self-adapt can serv to balanc robust and evolv (lehman and stanley, 2011b). 

It be import to note that the relationship between es’ robust drive and evolv be nuanc 
and like domain-dependent. for example, some domain may inde requir certain NN weight 
to be precis specified, and evolv may be hinder by prohibit such specificity. thu an 
interest open question be whether es’ mechan for gener robust can be enhanc to 
good seek evolv in a domain-independ way, and additionally, whether it mechan can 
be abstract such that it direct search for robust can also benefit more tradit eas. 

3 experi 

the approach of thi paper be to demonstr empir the way in which the ES of saliman et al. 
(2017) systemat differ from a more tradit gradient-follow approach. first, in a seri of 
toy landscapes, a finite-differ approxim of domain paramet improv be contrast with 
es’ approxim of distribut paramet improvement, to highlight similar and differences. 
additionally, in the limit of decreas variance, the converg of es’ distribut gradient to 
the domain paramet gradient be also empir validated. 

then, to explor whether these differ also manifest in real world domains, the robust 
properti of ES be investig in a popular RL benchmark, i.e. the humanoid locomot task 
(brockman et al., 2016). polici from ES be compar with those from a genet algorithm (ga) 
and a repres polici gradient RL algorithm call trust region polici optim (trpo; 
schulman et al. 2015), to explor whether ES be drawn to qualit differ area of the paramet 
space. 

3.1 fit landscap 

thi section introduc a seri of illustr fit landscap (shown in figur 1), in which the 
behavior of ES and finit differ can easili be contrasted. In each landscape, perform be 
a determinist function of two variables. for es, the distribut over variabl be an isotrop 
gaussian with fix varianc a in saliman et al. (2017), i.e. each dimens vari independ 
from the other. that is, ES optim two distribut paramet that encod the locat of the 
distribution’ mean. In contrast, while the finite-differ gradient-follow also optim two 
parameters, these repres a singl instanti of domain parameters, and consequ it function 
thu depend onli on f(θ) at that singular position. 

4 



X 

(a) donut landscap 

X 

(b) narrow path landscap 

X 

(c) fleet peak landscap 

X 

(d) gradient gap landscap 

X 

(e) gradient cliff landscap 

figur 1: illustr fit landscapes. A seri of five fit landscap highlight diverg 
between the behavior of ES and finit differences. In all landscapes, darker color indic low 
fit and the red X indic the start point of search. In the (a) donut landscape, there be a 
singl gaussian fit peak, but the small neighborhood immedi around and includ the peak 
have zero reward. In the (b) narrow path landscape, fit increas to the right, but the peak’ 
spread increasingli narrows, test an optimizer’ abil to follow a narrow path. In the (c) fleet 
peak landscape, fit increas to the right, but optim to the true peak be complic by 
a seri of small local optima. the (d) gradient gap landscap be complic by a gradient-fre 
zero-reward gap in an otherwis smooth landscape, highlight es’ abil to cross fit plateau 
(i.e. escap area of the landscap where there be no local gradient). A control for the gradient gap 
landscap be the (e) gradient cliff landscape, wherein there be no promis area beyond the gap. 

In the donut landscap (figur 1a), when the varianc of es’ gaussian be high enough (i.e. σ of 
the search distribut be set to 0.16, show in figur 2a), ES maxim distribut reward by 
center the mean of it domain paramet distribut at the middl of the donut where fit be 
lowest; figur 3a further illumin thi divergence. when es’ varianc be small (σ = 0.04), ES 
instead posit itself such that the tail of it distribut avoid the donut hole (figur 2b). finally, 
when es’ varianc becom tini (σ = 0.002), the distribut becom tightli distribut along 
the edg of the donut-hol (figur 2c). thi final ES behavior be qualit similar to follow a 
finite-differ approxim of the domain paramet perform gradient (figur 2d). 

In the narrow path landscap (figur 1b), when ES be appli with high varianc (σ = 0.12) it be 
unabl to progress far along the narrow path to high fit (figur 4a), becaus expect valu 
be high when a signific portion of the distribut remain on the path. As varianc declin 
(figur 4b and 4c), ES proce further along the path. finite-differ gradient descent be abl to 
easili travers the entir path (figur 4d). 

In the fleet peak landscap (figur 1c), when ES be appli with high varianc (σ = 0.16) the 
search distribut have suffici spread to ignor the local optimum and proce to the maximal-fit 
area (figur 5a). with medium varianc (σ = 0.048; figur 5b), ES gravit to each local optimum 
befor leap to the next one, and ultim becom stuck on the last local optimum (see also 
figur 3b). with low varianc (σ = 0.002; figur 5c), ES latch onto the first local optimum and 
remain stuck there indefinitely. finite-differ gradient descent becom stuck on the same local 
optimum (figur 5d). 

finally, in the gradient gap landscap (figur 1d), ES with high varianc (σ = 0.18) can travers 
a zero-fit non-diffenti gap in the landscap (figur 6a), demonstr es’ abil to “look 

5 



(a) ES with σ = 0.16 (b) ES with σ = 0.04 

(c) ES with σ = 0.002 (d) finit differ with � = 1e− 7 

figur 2: search trajectori comparison in the donut landscape. the plot compar repres 
trajectori of ES with decreas varianc in it search distribut to finite-differ gradient 
descent. with (a) high variance, ES maxim expect fit by move the distribution’ mean into 
a low-fit area. with (b,c) decreas variance, ES be drawn closer to the edg of the low-fit 
area, qualit converg to the behavior of (d) finite-differ gradient descent. 

ahead” in paramet space to cross fit valley between local optimum (see also figur 3c). lower 
varianc ES (not shown) and finit differ cannot cross the gap (figur 6b). highlight that ES 
be inform by sampl at the tail of the search distribut and be not blindli push forward, ES 
with high varianc in the gradient cliff landscap (figur 6c) do not leap into the cliff, and low 
varianc ES (not shown) and finit differ (figur 6d) behav no differ then they do in the 
gradient gap landscape. 

overall, these landscapes, while simple, help to demonstr that there be inde systemat differ- 
enc between ES and tradit gradient descent. they also show that no particular treatment be 
ideal in all cases, so the util of the optim over a fixed-vari search distribution, at least for 
find the global optimum, be (a would be expected) domain-dependent. the next section describ 
result in the humanoid locomot domain that provid a proof-of-concept that these differ 
also manifest themselv when appli ES to modern deep RL benchmarks. 

6 



0 25 50 75 100 125 150 175 200 
iter 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

Fi 
tn 

e 
s 

valu at distribut mean 
expect valu across distribut 

(a) reward of ES on the donut landscap 

0 100 200 300 400 500 600 
iter 

0.5 

1.0 

1.5 

2.0 

2.5 

3.0 

Fi 
tn 

e 
s 

valu at distribut mean 
expect valu across distribut 

(b) reward of ES on the fleet peak landscap 

0 50 100 150 200 250 
iter 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

Fi 
tn 

e 
s 

valu at distribut mean 
expect valu across distribut 

(c) reward of ES on the gradient gap landscap 

figur 3: ES maxim expect valu over the search distribution. these plot show how the 
expect valu of fit and the fit valu evalu at the distribution’ mean can diverg in 
repres run of es. thi diverg be show on (a) the donut landscap with high varianc 
(σ = 0.16), (b) the fleet peak landscap with medium varianc (σ = 0.048), and (c) the gradient 
gap landscap with high varianc (σ = 0.18). 

4 humanoid locomot 

In the humanoid locomot domain, a simul humanoid robot be control by an NN control 
with the object of produc a fast energy-effici gait (tassa et al., 2012; brockman et al., 
2016), implement in the mujoco physic simul (todorov et al., 2012). mani RL method be 
abl to produc compet gaits, which thi paper consid a achiev a fit score of 6,000 
averag across mani independ evaluations, follow the threshold score in saliman et al. 
(2017); averag be necessari becaus the domain be stochastic. the purpos of thi experi be 
not to compar perform across method a be typic in rl, but instead to examin the robust 
of solutions, a defin by the distribut of perform in the neighborhood of solutions. 

three method be compar in thi experiment: es, ga, and trpo. both ES and GA be method 
that directli search through the paramet space for solutions, while trpo us gradient descent 
to modifi polici directedli to more often take action result in high reward. all method 
optim the same underli NN architecture, which be a feedforward NN with two hidden layer of 
256 tanh units, compris approxim 167,000 weight paramet (recal that ES optim the 
same number of parameters, but that they repres the mean of a search distribut over domain 
parameters). thi NN architectur be take from the configur file releas with the sourc code 
from saliman et al. (2017). the architectur describ in their paper be similar, but smaller, have 
64 neuron per layer (saliman et al., 2017). 

the hypothesi be that ES polici will be more robust to polici perturb than polici of similar 
perform gener by either GA or trpo. the GA of petroski such et al. (2017) provid 
a natur control, becaus it mutat oper be the same that gener variat within es, 

7 



(a) ES with σ = 0.12 (b) ES with σ = 0.04 

(c) ES with σ = 0.0005 (d) finit differ with � = 1e− 7 

figur 4: search trajectori comparison in the narrow path landscape. with (a) high variance, 
ES maxim expect fit by stay on the wider part of the path, mean it do not travers 
the entir path. importantly, if ES be be use to ultim discov a singl high-valu policy, a be 
often the case (saliman et al., 2017), thi method will not discov the superior solut further 
down the path. with (b,c) decreas variance, ES be abl to travers further along the narrow path. 
(d) finite-differ gradient descent travers the entir path. 

but it object function do not directli reward robustness. note that ES be train with polici 
perturb from a gaussian distribut with σ = 0.02 while the GA requir a much narrow 
distribut (σ = 0.00224) for success train (petroski such et al., 2017); train the GA 
with a larg mutat distribut destabil evolution, a mutat too rare would preserv 
or improv perform to support adaptation. interestingly, thi destabil itself support the 
idea that robust to high varianc perturb be not pervas throughout thi search space. 
trpo provid anoth use control, becaus it follow the gradient of increas perform 
without gener ani random paramet perturbations; thu if the robust of ES solut be 
high than that of those from trpo, it also provid evid that es’ behavior be distinct, i.e. it 
be not best understood a simpli follow the gradient of improv perform with respect to 
domain paramet (a trpo does); note that thi argument do not impli that trpo be defici if 
it polici be less robust to random paramet perturb than es, a such random perturb 
be not part of it search process. 

8 



(a) ES with σ = 0.16 (b) ES with σ = 0.048 

(c) ES with σ = 0.002 (d) finit differ with � = 1e− 7 

figur 5: search trajectori comparison in the fleet peak landscape. with (a) high variance, 
ES can bypass the local optimum becaus it contribut to expect fit across the distribut 
be small. with (b) medium variance, ES hop between local optima, and with (c) low variance, ES 
converg to a local optimum, similarli to (d) finite-differ gradient descent. 

the experiment methodolog be to take solut from differ method and examin the distribu- 
tion of result perform when polici be perturb with the perturb size of ES and of ga. 
In particular, polici be take from gener 1,000 of the ga, from iter 100 of es, and from 
iter 350 of trpo, where method have approxim evolv a solut of ≈6,000 fitness. the 
ES be run with hyperparamet accord to saliman et al. (2017), the GA be take from petroski 
such et al. (2017), and trpo be base on openai’ baselin packag (dhariw et al., 2017). exact 
hyperparamet be list in the appendix. 

4.1 result 

figur 7 show a repres exampl of the stark differ between the robust of ES solut 
and those from the GA or trpo, even when the GA be subject onli to the lower-vari perturb 
that be appli dure evolution. qualit we observ that thi result appear consist across 
independ train models. A video compar perturb polici of ES and trpo can be view 
at the follow url (along with other video show select fit landscap animations): 
https://goo.gl/yz1mem. 

9 

https://goo.gl/yz1mem 


(a) ES on gradient gap with σ = 0.18 (b) finit differ on gradient gap with � = 1e− 7 

(c) ES on gradient cliff with σ = 0.18 (d) finit differ on gradient cliff with � = 1e−7 

figur 6: search trajectori comparison in the gradient gap and gradient cliff landscapes. 
with (a) high variance, ES can bypass the gradient-fre gap becaus it distribut can span the 
gap; with lower-vari ES or (b) finit differences, search cannot cross the gap. In the control 
gradient cliff landscape, (c) ES with high varianc remain root in the high-fit area, and the 
perform of (d) finit differ be unchang from the gradient gap landscape. 

To further explor thi robust difference, a quantit measur of robust be also applied. 
In particular, for each model, the origin paramet vector’ reward be calcul by averag it 
perform over 1,000 trial in the environment. then, 1,000 perturb be gener for each 
model, and each perturbation’ perform be averag over 100 trial in the environment. finally, 
a robust score be calcul for each model a the ratio of the perturbations’ median perform 
to the unperturb policy’ performance, i.e. a robust score of 0.5 indic that the median 
perturb perform half a well a the unperturb model. the result (shown in figur 8) indic 
that inde by thi measur ES be significantli more robust than the GA or trpo (mann-whitney 
u-test; p < 0.01). the conclus be that the robustness-seek properti of ES demonstr in 
the simpl landscap also manifest itself in thi more challeng and high-dimension domain. 
interestingly, trpo be significantli more robust than both GA treatment (mann-whitney u-test; 
p < 0.01) even though it be not driven by random perturbations; futur work could probe the 
relationship between the sgd updat of polici gradient method and the random perturb 
appli by ES and the ga. 

10 



0 1000 2000 3000 4000 5000 6000 7000 
reward 

0 

20 

40 

60 

80 

Fr 
eq 

ue 
nc 

y 

ES 
GA 

(a) ES (σ = 0.02) v GA (σ = 0.002) 

0 1000 2000 3000 4000 5000 6000 7000 
reward 

0 

50 

100 

150 

200 

Fr 
eq 

ue 
nc 

y 

ES 
GA 

(b) ES (σ = 0.02) v GA (σ = 0.02) 

0 1000 2000 3000 4000 5000 6000 7000 
reward 

0 

20 

40 

60 

80 

Fr 
eq 

ue 
nc 

y 

ES 
trpo 

(c) ES (σ = 0.02) v trpo (σ = 0.02) 

figur 7: ES be more robust to paramet perturb in the humanoid locomot task. the 
distribut of reward be show from perturb model train by es, ga, and trpo. model be 
train to a fit valu of 6,000 reward, and robust be evalu by gener perturb 
with gaussian nois (with the specifi variance) and evalu perturb polici in the domain. 
high-vari perturb of ES produc a healthier distribut of reward than do perturb of 
GA or trpo. 

11 



ES trpo GA (train ) GA (high ) 
method 

0.2 

0.4 

0.6 

0.8 

Ro 
bu 

st 
ne 

s 

figur 8: quantit measur of robust across independ run of es, ga, and trpo. 
the distribut of reward be show from perturb 10 independ model for each of es, ga, 
and trpo under the high-vari perturb use to train ES (σ = 0.02). result from GA 
be show also for perturb drawn from the lower-vari distribut it experienc dure 
train (σ = 0.00224). the conclus be that high-vari perturb of ES retain significantli 
high perform than do perturb of GA or trpo (student’ t-test; p < 0.01). 

5 discuss and conclus 

An import contribut of thi paper be to ensur that awar of the robustness-seek properti 
of es, especi with high σ, be not lose – which be a risk when ES be describ a simpli perform 
stochast finit differences. In effect, when σ be abov some threshold, it be not accur to interpret 
ES a mere an approxim of sgd, nor a a tradit finite-differences-bas approximator. 
rather, it becom a gradient approxim coupl with a compass that seek area of the search 
space robust to paramet perturbations. thi latter properti be not easili avail to point-bas 
gradient methods, a highlight dramat in the humanoid locomot experi in thi paper. 
On the other hand, if one want ES to good mimic finit differ and sgd, that option be still 
feasibl simpli by reduc σ. 

the extent to which seek robust to paramet perturb be import remain open to further 
research. As show in the landscap experiments, when it come to find optima, it clearli depend 
on the domain. If the search space be reminisc of fleet peaks, then ES be like an attract 
option for reach the global optimum. however, if it be more like the narrow path landscape, 
especi if the ultim goal be a singl solut (and there be no concern about it robustness), then 
high-sigma ES be less attract (and the lower-sigma ES explor in zhang et al. (2017) would be 
more appropriate). It would be interest to good understand whether and under what condit 
domain more often resembl fleet peak a oppos to the narrow path. 

An intrigu question that remain open be when and whi such robust might be desir 
even for reason outsid of global optimality. for example, it be possibl that polici encod by 
network in robust region of the search space (i.e. where perturb paramet lead to network of 
similar performance) be also robust to other kind of noise, such a domain noise. It be interest 
to specul on thi possibility, but at present it remain a topic for futur investigation. perhap 
paramet robust also correl to robust to new oppon in coevolut or self-play 
(popovici et al., 2012), but that again cannot yet be answered. anoth open question be how 
robust interact with diverg search techniqu like novelti search (lehman and stanley, 
2011a) or qualiti divers method (pugh et al., 2016); follow-up experi to conti et al. (2017), 
which combin ES with novelti search, could explor thi issue. Of course, the degre to which the 
implic of robust matter like vari by domain a well. for example, in the humanoid 
locomot task the level of domain nois mean that there be littl choic but to choos a high σ 
dure evolut (becaus otherwis the effect of perturb could be drown out by noise), but 
in a domain like mnist there be no obviou need for anyth but an sgd-like process (zhang et al., 
2017). 

12 



anoth possibl benefit of robust be that it could be an indic of compressibility: If small 
mutat tend not to impact function (a be the case for robust nns), then less numer 
precis be requir to specifi an effect set of network weight (i.e. few bit be requir to 
encod them). thi issu too be present unexplored. 

thi studi focu on es, but it rais new question about other relat algorithms. for instance, 
non-evolutionari method may be modifi to includ a drive toward robust or may alreadi 
share abstract connect with es. for example, stochast gradient langevin dynam (well 
and teh, 2011), a bayesian approach to sgd, approxim a distribut of solut over iter 
of train by add gaussian nois to sgd updates, in effect also produc a solut cloud. 
additionally, it be possibl that method combin parameter-spac explor with polici gradient 
(such a plappert et al. 2017) could be modifi to includ robust pressure. 

A relat question is, do all evolutionari algorithms, which be gener population-based, poss 
at least the potenti for the same tendenc toward robustness? perhap some such algorithm 
have a differ mean of turn the knob between gradient follow and robust seeking, but 
nevertheless in effect leav room for the same dual tendencies. one particularli interest rel of 
ES be the ne (wierstra et al., 2014), which adjust σ dynam over the run. given that σ seem 
instrument in the extent to which robust becom paramount, character the tendenc of 
ne in thi respect be also import futur work. 

We hope ultim that the brief demonstr in thi work can serv a a remind that the analog 
between ES and finit differ onli go so far, and there be therefor other intrigu properti 
of the algorithm that remain to be investigated. 

acknowledg 

We thank all of the member of uber AI labs, in particular thoma miconi, martin jankowiak, 
rui wang, xingwen zhang, and zoubin ghahramani for help discussions; felip such for hi 
GA implement and edoardo conti for hi ES implementation, both of which be use in 
thi paper’ experiments. We also thank justin pinkul, mike deats, codi yancey, joel snow, leon 
rosenshein and the entir opusstack team insid uber for provid our comput platform and 
for technic support. 

refer 
berny, A. (2000). statist machin learn and combinatori optimization. In theoret aspect 

of evolutionari computing, page 287–306. springer. 

beyer, h.-g. and schwefel, h.-p. (2002). evolut strategies: A comprehens introduction. 
natur computing, 1:3–52. 

brockman, g., cheung, v., pettersson, l., schneider, j., schulman, j., tang, j., and zaremba, W. 
(2016). openai gym. 

clune, j., misevic, d., ofria, c., lenski, R. e., elena, S. f., and sanjuán, R. (2008). natur select 
fail to optim mutat rate for long-term adapt on rug fit landscapes. plo 
comput biology, 4(9):e1000187. 

conti, e., madhavan, v., petroski such, f., lehman, j., stanley, K. o., and clune, J. (2017). 
improv explor in evolut strategi for deep reinforc learn via a popul of 
novelty-seek agents. arxiv preprint to appear. 

De jong, K. A. (2002). evolutionari computation: A unifi perspective. mit press, cambridge, 
ma. 

dhariwal, p., hesse, c., plappert, m., radford, a., schulman, j., sidor, s., and wu, Y. (2017). openai 
baselines. https://github.com/openai/baselines. 

ebrahimi, s., rohrbach, a., and darrell, T. (2017). gradient-fre polici architectur search and 
adaptation. arxiv e-prints, 1710.05958. 

13 

https://github.com/openai/baselin 


glynn, P. W. (1987). likelilood ratio gradient estimation: an overview. In proceed of the 19th 
confer on winter simulation, page 366–375. acm. 

kirschner, M. and gerhart, J. (1998). evolvability. proceed of the nation academi of sciences, 
95(15):8420–8427. 

kounios, l., clune, j., kouvaris, k., wagner, G. p., pavlicev, m., weinreich, D. m., and watson, 
R. A. (2016). resolv the paradox of evolv with learn theory: how evolut learn to 
improv evolv on rug fit landscapes. arxiv preprint arxiv:1612.05955. 

lehman, J. and stanley, K. O. (2011a). abandon objectives: evolut through the search for 
novelti alone. evolutionari computation, 19(2):189–223. 

lehman, J. and stanley, K. O. (2011b). improv evolv through novelti search and self- 
adaptation. In evolutionari comput (cec), 2011 ieee congress on, page 2693–2700. 
ieee. 

lehman, J. and stanley, K. O. (2013). evolv be inevitable: increas evolv without the 
pressur to adapt. plo one, 8(4):e62186. 

lenski, R. e., barrick, J. e., and ofria, C. (2006). balanc robust and evolvability. plo 
biology, 4(12):e428. 

meyer-nieberg, S. and beyer, h.-g. (2007). self-adapt in evolutionari algorithms. paramet 
set in evolutionari algorithms, page 47–75. 

petroski such, f., madhavan, v., conti, e., lehman, j., stanley, K. o., and clune, J. (2017). deep 
neuroevolution: genet algorithm be a competit altern for train deep neural network 
for reinforc learning. arxiv preprint to appear. 

plappert, m., houthooft, r., dhariwal, p., sidor, s., chen, R. y., chen, x., asfour, t., abbeel, p., and 
andrychowicz, M. (2017). paramet space nois for exploration. arxiv preprint arxiv:1706.01905. 

popovici, e., bucci, a., wiegand, R. p., and De jong, E. D. (2012). coevolutionari principles, page 
987–1033. springer berlin heidelberg, berlin, heidelberg. 

pugh, J. k., soros, L. b., and stanley, K. O. (2016). qualiti diversity: A new frontier for evolutionari 
computation. 3(40). 

richardson, L. F. (1911). the approxim arithmet solut by finit differ of physic 
problem involv differenti equations, with an applic to the stress in a masonri dam. 
philosoph transact of the royal societi of london. seri A, contain paper of a 
mathemat or physic character, 210:307–357. 

salimans, t., ho, j., chen, x., and sutskever, I. (2017). evolut strategi a a scalabl altern 
to reinforc learning. arxiv preprint arxiv:1703.03864. 

schulman, j., levine, s., abbeel, p., jordan, m., and moritz, P. (2015). trust region polici optimiza- 
tion. In proceed of the 32nd intern confer on machin learn (icml-15), page 
1889–1897. 

schwefel, h.-p. P. (1993). evolut and optimum seeking: the sixth generation. john wiley & sons, 
inc. 

sehnke, f., osendorfer, c., rückstieß, t., graves, a., peters, j., and schmidhuber, J. (2010). 
parameter-explor polici gradients. neural networks, 23(4):551–559. 

spall, J. C. (1992). multivari stochast approxim use a simultan perturb gradient 
approximation. ieee transact on automat control, 37(3):332–341. 

tassa, y., erez, t., and todorov, E. (2012). synthesi and stabil of complex behavior 
through onlin trajectori optimization. In intellig robot and system (iros), 2012 ieee/rsj 
intern confer on, page 4906–4913. ieee. 

14 



todorov, e., erez, t., and tassa, Y. (2012). mujoco: A physic engin for model-bas control. 
In intellig robot and system (iros), 2012 ieee/rsj intern confer on, page 
5026–5033. ieee. 

wagner, A. (2008). robust and evolvability: a paradox resolved. proceed of the royal 
societi of london B: biolog sciences, 275(1630):91–100. 

wagner, G. P. and altenberg, L. (1996). perspective: complex adapt and the evolut of 
evolvability. evolution, 50(3):967–976. 

welling, M. and teh, Y. W. (2011). bayesian learn via stochast gradient langevin dynamics. 
In proceed of the 28th intern confer on machin learn (icml-11), page 
681–688. 

wierstra, d., schaul, t., glasmachers, t., sun, y., peters, j., and schmidhuber, J. (2014). natur 
evolut strategies. journal of machin learn research, 15(1):949–980. 

wilke, C. o., nad charl ofria, J. L. w., lenski, R. e., and adami, C. (2001). evolut of digit 
organ at high mutat rate lead to surviv of the flattest. nature, 412:331–333. 

williams, R. J. (1992). simpl statist gradient-follow algorithm for connectionist reinforce- 
ment learning. machin learning, 8(3-4):229–256. 

zhang, x., clune, j., and stanley, K. O. (2017). On the relationship between the openai evolut 
strategi and stochast gradient descent. arxiv preprint to appear. 

appendix A hyperparamet 

thi section describ the relev hyperparamet for the search method (es, ga, and trpo) 
appli in the humanoid walker experiments. 

a.1 ES 

the ES algorithm be base on saliman et al. (2017) and us the same hyperparamet a in their 
humanoid walker experiment. In particular, 10,000 roll-out be use per iter of the algorithm, 
with a fix σ of the paramet distribut set to 0.02. the adam optim be appli with a 
step-siz of 0.01. 

a.2 GA 

the GA be base on petroski such et al. (2017). the popul size be set to 12,501, and σ of the 
normal distribut use to gener mutat perturb be set to 0.00224. truncat select 
be performed, and onli the highest-perform 5% of the popul survived. 

a.3 trpo 

the trpo (schulman et al., 2015) implement be take from the openai baselin packag 
(dhariw et al., 2017). the maximum KL diverg be set to 0.1 and 10 iter of conjug 
gradient be conduct per batch of train data. discount rate (γ) be set to 0.99. 

15 


1 introduct 
2 background 
2.1 finit differ 
2.2 search gradient 
2.3 robust in evolutionari comput 

3 experi 
3.1 fit landscap 

4 humanoid locomot 
4.1 result 

5 discuss and conclus 
A hyperparamet 
a.1 ES 
a.2 GA 
a.3 trpo 


