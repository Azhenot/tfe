






















































counterfactu fair 


counterfactu fair 

matt J. kusner * 1 2 joshua R. loftu * 1 3 chri russel * 1 4 ricardo silva 1 5 

abstract 
machin learn have matur to the point to 
where it be now be consid to autom 
decis in loan lending, employe hiring, and 
predict policing. In mani of these scenario 
however, previou decis have be make that 
be unfairli bia against certain subpopul 
(e.g., those of a particular race, gender, or sex- 
ual orientation). becaus thi past data be of- 
ten biased, machin learn predictor must ac- 
count for thi to avoid perpetu discrimina- 
tori practic (or incident make new ones). 
In thi paper, we develop a framework for mod- 
ele fair in ani dataset use tool from 
counterfactu inference. We propos a defini- 
tion call counterfactu fair that captur 
the intuit that a decis be fair toward an 
individu if it give the same predict in (a) 
the observ world and (b) a world where the in- 
dividu have alway belong to a differ de- 
mograph group, other background caus of 
the outcom be equal. We demonstr our 
framework on two real-world problems: fair pre- 
diction of law school success, and fair model 
of an individual’ crimin in polic data. 

1. introduct 
machin learn have spread to field a divers a credit 
score (khandani et al., 2010), crime predict (bren- 
nan et al., 2009), and loan assess (mahoney & mo- 
hen, 2007). As machin learn enter these new area it 
be necessari for the model to think beyond the simpl ob- 
jectiv of maxim predict accuracy, and to consid 
the societ impact of their work. 

for mani of these applications, it be crucial to ask if the pre- 
diction of a model be fair. for instance, imagin a bank 
wish to predict if an individu should be give a loan 

*equal contribution, author order decid randomli 1alan 
ture institut 2univers of warwick 3univers of cam- 
bridg 4univers of edinburgh 5univers colleg lon- 
don. correspond to: <mkusner@turing.ac.uk>, 
<jloftus@turing.ac.uk>, <crussell@turing.ac.uk>, <ri- 
cardo.silva@ucl.ac.uk>. 

to buy a house. the bank wish to use histor repay- 
ment data, alongsid individu data. If they simpli learn 
a model that predict whether the loan will be paid back, it 
may unjustli favor applic of particular subgroups, due 
to past and present prejudices. the obama administr 
releas a report describ thi which urg data scientist 
to analyz “how technolog can deliber or inadver- 
tentli perpetuate, exacerbate, or mask discrimination”.1 

As a result, there have be immens interest in design al- 
gorithm that make fair predict (bolukbasi et al., 2016; 
calder & verwer, 2010; dwork et al., 2012; grgic-hlaca 
et al., 2016; hardt et al., 2016; joseph et al., 2016; kami- 
ran & calders, 2009; 2012; kamishima et al., 2011; klein- 
berg et al., 2016; louizo et al., 2015; zafar et al., 2015; 
2016; zemel et al., 2013; zliobaite, 2015). In larg part, 
the initi work on fair in machin learn have fo- 
cuse on formal fair into quantit definit 
and use them to solv a discrimin problem in a cer- 
tain dataset. unfortunately, for a practitioner, law-maker, 
judge, or anyon els who be interest in implement al- 
gorithm that control for discrimination, it can be difficult 
to decid which definit of fair to choos for the task 
at hand. indeed, we demonstr that depend on the re- 
lationship between a sensit attribut and the data, certain 
definit of fair can actual increas discrimination. 

We describ how techniqu from causal infer can be 
effect tool for design fair algorithm and argue, a 
in (dedeo, 2014), that it be essenti to properli address 
causality. specifically, we leverag the causal framework 
of pearl et al. (2009) to model the relationship between sen- 
sitiv attribut and data. our contribut be a follows: 

1. We model question of fair within a causal frame- 
work. thi allow u to directli model how unfair 
affect the data at hand. 

2. We introduc counterfactu fairness, which enforc 
that a distribut over possibl predict for an in- 
dividu should remain unchanged, in a world where 
an individual’ sensit attribut have be differ 
from birth. 

3. We analyz how enforc exist definit of fair- 

1https://obamawhitehouse.archives.gov/blog/2016/05/04/big- 
risks-big-opportunities-intersection-big-data-and-civil-right 

ar 
X 

iv 
:1 

70 
3. 

06 
85 

6v 
1 

[ 
st 

at 
.M 

L 
] 

2 
0 

M 
ar 

2 
01 

7 



counterfactu fair 

ness for differ data may correspond or be in conflict 
with counterfactu fairness. In particular, we show 
that depend on the underli state of the world 
some definit of fair may be inappropriate. 

4. We devis techniqu for learn predictor that be 
counterfactu fair and demonstr their use in sev- 
eral examples. 

2. fair 
our goal in thi paper be to design autom algorithm 
that make fair predict across variou demograph 
groups. thi unfair can aris in sever ways: 

histor bia distributions: individu with dif- 
ferent protect attribut A may have mani differ 
attribut due to current and histor bia (e.g., racial 
inequ caus by thing like colonialism, slavery, 
a histori of discrimin in hire and hous etc.). 

select unfairness: the train data could contain se- 
lection bias. for instance, if we be use a dataset 
describ who paid loan back in full in order to train 
a loan predict algorithm, it may be that loan be 
unfairli distributed. sinc we can’t see whether peo- 
ple will pay back a loan if they didn’t receiv one, our 
algorithm may be bia by thi sampling. 

predict unfairness: the learn classifi could use 
either protect attribut such a race or correl 
attribut a features, and learn a bia predictor. 

there have be a wealth of recent work toward fair al- 
gorithms. these includ fair through unawar 
(grgic-hlaca et al., 2016), demograph parity/dispar 
impact (zafar et al., 2015), individu fair (dwork 
et al., 2012; joseph et al., 2016; louizo et al., 2015; zemel 
et al., 2013), and equal of opportun (hardt et al., 2016; 
zafar et al., 2016). 

definit 1 (fair through unawar (ftu)). An 
algorithm be fair so long a ani sensit attribut A be 
not explicitli use in the decision-mak process. ani 
map Ŷ : X → Y that exclud A (or other unfair 
attributes, see grgic-hlaca et al. (2016)) satisfi this. 

initi propos a a baselin method, the approach have 
found favor recent with more gener approach such a 
grgic-hlaca et al. (2016). the approach have a compel 
simplicity, and construct a predictor Ŷ base on a featur 
vector X that exclud A, and in the case of grgic-hlaca 
et al. (2016) other attribut label a unfair. 

definit 2 (individu fair (if)). An algorithm be 
fair if it give similar predict to similar individuals. 

formally, if individu i and j be similar apart from their 
protect attribut ai, Aj then 

Ŷ (x(i), a(i)) ≈ Ŷ (x(j), a(j)). 

thi approach can be understood loos a a continu 
analog of ftu. As describ in (dwork et al., 2012), the 
notion of similar must be care chosen and thi no- 
tion of fair will not correct for the histor bia de- 
scribe above. 

definit 3 (demograph pariti (dp)). An algorithm be 
fair if it predict be independ of the sensit at- 
tribut A across the population. A predict Ŷ satisfi 
thi definit if, 

P (ŷ |A = 0) = P (ŷ |A = 1). 

definit 4 (equal opportun (eo)). An algorithm be 
fair if it be equal accur for each valu of the sensit 
attribut A. A predict Ŷ satisfi thi if, 

P (ŷ = 1|a = 0, Y = 1) = P (ŷ = 1|a = 1, Y = 1). 

while these definit address the notion of algorithm 
fairness, they guarante that histor bia in the data be 
preserved. As show by kleinberg et al. (2016), EO and 
DP be mutual exclus notion of fairness. 

3. causal model and counterfactu 
We follow the framework of pearl (2000), and defin a 
causal model a a tripl (u, V, F ) of set such that 

• U be a set of latent background variables2, which be 
gener by factor outsid of our control, and in gen- 
eral do not depend on ani protect attribut A (un- 
less thi be explicitli specified); 

• V be a set of endogen variables, where each mem- 
ber be determin by other variabl in U ∪ V ; 

• F be a set of function {f1, . . . , fn}, one for each 
Vi ∈ V , such that Vi = fi(pai, upai), pai ⊆ V \{vi} 
and upai ⊆ U . such equat be also know a 
structur equat (bollen, 1989). 

the notat “pai” refer to the “parents” of Vi and be moti- 
vate by the assumpt that the model factor accord 
to a direct acycl graph (dag). that is, we can defin 
a direct graph G = (U ∪ V, E) where each node be an 
element of U ∪ V , and each edg from some Z ⊆ U ∪ V 

2these be sometim call exogen variables, but the fact 
that member of U might depend on each other be not relev to 
what follows. 



counterfactu fair 

to Vi indic that Z ∈ pai ∪ upai . By construction, G be 
acyclic. 

the model be causal in that, give a distribut p(u) over 
the background variabl U , you can deriv the distribu- 
tion of a subset Z ⊆ V follow an intervent on the 
complementari subset V \z. here, an intervent on the 
variabl Vi of valu v refer to the substitut of equat 
Vi = fi(pai, upai) with the equat Vi = v. thi captur 
the idea of an agent, extern to the system, modifi it by 
forc assign valu v to vi. thi occur in a random- 
ize control trial where the valu of Vi be overridden by 
a treatment set it to v, a valu chosen at random, and 
thu independ of ani other causes. 

In contrast with the independ constraint give by a 
dag, the full specif of F requir much strong 
assumpt but also lead to much strong claims. In 
particular, it allow for the calcul of counterfactu 
quantities. In brief, consid the follow counterfactu 
statement, “the valu of Y if Z have take valu z”, for two 
endogen variabl Z and Y in a causal model. By as- 
sumption, the state of ani endogen variabl be fulli de- 
termin by the background variabl and structur equa- 
tions. the counterfactu be model a the solut for Y 
for a give U = u where the equat for Z be replac 
with Z = z. We denot it by yz←z(u) (pearl, 2000), and 
sometim a Yz if the context of the notat be clear. 

counterfactu inference, a specifi by a causal model 
(u, V, F ) give evid W , be the comput of proba- 
biliti P (yz←z(u) |W =w), wherew , Z and Y be sub- 
set of V . infer proce in three steps, a explain 
in more detail in chapter 4 of pearl et al. (2016): 

1. abduction: for a give prior on U , comput the pos- 
terior distribut of U give the evid W = w; 

2. action: substitut the equat for Z with the in- 
tervent valu z, result in the modifi set of 
equat fz; 

3. prediction: comput the impli distribut on the 
remain element of V use Fz and the posterior 
P (U |W = w). 

4. counterfactu fair 
given a causal model (u, V, F ), let A ⊆ V be a set of pro- 
tect attributes, Ŷ ⊆ V a variabl which we will be the 
basi for ani decis making, and W the set of comple- 
mentari measur such that W = V \ (A ∪ {ŷ }). 
definit 5 (counterfactu fairness). We say Ŷ be coun- 
terfactu fair if under ani context uniqu defin by 

A Y uyua employeda Y 

uyua 

prejud qualif 

a employeda Ya 

employ ya0 a0 a0 
employeda Y uyua 

prejud qualif 

(a) 

(b) (c) 

figur 1. (a) the graph correspond to a causal model with A 
be the protect attribut and Y some outcom of interest, 
with background variabl assum to be independent. (b) ex- 
pand the model to includ an intermedi variabl indic 
whether the individu be employ with two (latent) background 
variabl prejud (if the person offer the job be prejudiced) 
and qualif (a measur of the individual’ qualifications). 
(c) A twin network represent of thi system (pearl, 2000) un- 
der two differ counterfactu level for A. thi be creat by 
copi node descend from A, which inherit unaffect par- 
ent from the factual world. 

evid W = w and sensit A = a, 

P (ŷa←a (u) = y |W = w,a = a) = 
P (ŷa←a′(u) = y |W = w,a = a), (1) 

for all y and for ani valu a′ attain by A. 

thi captur the idea that ani decis base on the condi- 
tional distribut of Ŷ would be the same despit A be 
different, give the full implic of A have alway 
be different. We can also see Ŷ a satisfi “counter- 
factual exchangeability” under thi model. 

An associ concept of causal fair appear a exam- 
ple 4.4.4 in pearl et al. (2016). there, the author con- 
dition instead on W , A, and the observ realiz of 
Ŷ , and calcul the probabl of the counterfactu real- 
izat differ from the factual3. thi exampl conflat 
the record decis Ŷ with the inform Y on which 
we should ideal base our decis making, a differ 
which we maintain. our frame make the connect to 
other exist machin learn method more explicit, a 
we discu in section 5. evid use to determin the 
state of background variabl U should come from A and 
W alone, a in mani setup we wish to predict some Y a 
Ŷ , when Y be unavail at ani point in our inference. 

We also emphas that counterfactu fair be an 
individual-level definition. thi be substanti differ 
from compar differ unit that happen to share the 
same “treatment” and coincid on valu ofx , a discuss 
in section 4.3.1 of (pearl et al., 2016). here, differ in 
the valu of X must be caus by variat on A only. 

3the result be an express call the “the probabl of suffi- 
ciency” for A, captur the notion that switch A to a differ 
valu would be suffici to chang Ŷ with some probability. 



counterfactu fair 

4.1. implic 

As discuss by halpern (2016), it be unproduct to de- 
bate if a particular counterfactu definit be the “cor- 
rect” one to satisfi social construct concept such a 
blame and responsibility. the same appli to fairness. 
instead, we discu the implic of definit (5) and 
some choic that aris in it application. 

first, we wish to make explicit the differ between Ŷ , 
the predictor we use for fair decisions, and Y , the relat 
state gener by an unfair world. for instance, Y could 
be an indic of whether a client default on a loan, while 
Ŷ be the actual decis of give the loan. consid the 
dag A → Y for a causal model where V = {a, Y }, 
and in figur 4(a) the dag with explicit inclus of set 
U of independ background variables. assum Y be an 
object ideal measur use in decis making, such 
a a binari indic that the individu default on a loan. 
In thi setup, the mechan fY (a,u) be causal unfair, 
with the arrow A → Y be the result of a world that 
punish individu in a way that be out of their control. 
figur 4(b) show a more fine-grain model, where the 
path be mediat by a measur of whether the person be 
employed, which be itself caus by two background fac- 
tors: one repres whether the person hire be prej- 
udiced, and the other the employee’ qualifications. In 
thi world, A be a caus of defaulting, even if mediat by 
other variables. the counterfactu fair principl how- 
ever forbid u from use Y : use the twin network of 
pearl (2000), we see in figur 4(c) that Ya and ya′ need 
not be ident distribut give the background vari- 
ables. for example, if the function determin employ- 
ment fe(a,p,q) = i(q>0,p=0 ora 6=a) then an individ- 
ual with suffici qualif and prejudic potenti 
employ may have a differ counterfactu employ 
valu for A = a compar to A = a′, and a differ 
chanc of default. 

In contrast, ani function of variabl not descend of A 
can be use a basi for fair decis making. thi means, 
that ani variabl Ŷ defin by Ŷ = g(u) will be counter- 
factual fair for ani function g(·). hence, give a causal 
model, the function defin by the function g(·) minimiz- 
ing some predict error for Y will satisfi the criterion. If 
Ŷ must be randomized, it suffic that the stochast com- 
ponent of it be independ of ani descend of A. 

there be a subtleti to address here: by abduction, U will 
typic depend ona, and henc so will Ŷ when marginal- 
ize over U . thi seem to disagre with the intuit that 
our fair variabl should be not be caus by A. however, 
thi be a comparison across individuals, not within an indi- 
vidual, a discuss by section 4.3.1 of (pearl et al., 2016). 
more intuitively, consid the simpl case where U be fulli 
determin by A and X (which occur in some import 

special cases). In thi scenario, we proceed just a if we 
have measur U from the begin rather than perform- 
ing abduction. We then gener Ŷ from g(u), so U be the 
caus of Ŷ and not A. 

note that we can build counterfactu fair predict 
model for some Ŷ even if the structur equat that 
gener Y be unfair. the idea be that we be learn 
a project of Y into an altern world where it would be 
fair, which we may think of a a “closest world” defin by 
our class of model and the causal structur of the world4. 

A 

X Y 

U A 

X Y 

U A 

X 

U 

Y 

figur 2. three causal model for differ real-world fair predic- 
tion scenarios. see section 4 for discussion. 

4.2. exampl 

To give an intuit for counterfactu fair we will con- 
sider three fair predict scenarios: insur pricing; 
crime prediction; colleg admissions. each of these cor- 
respond to one of the three causal graph in figur 2. 

scenario 1: the red car. imagin a car insur com- 
pani wish to price insur for car owner by predict- 
ing their accid rate Y . they assum there be an unob- 
serv factor correspond to aggress drive U , that 
(a) caus driver to be more like have an accident, and 
(b) caus individu to prefer red car (the observ vari- 
abl x). moreover, individu belong to a certain race 
A be more like to drive red cars. however, these indi- 
vidual be no more like to be aggress or to get in ac- 
cident than ani one else. We show thi in figur 2 (left). 

thus, use the red car featur X to predict accid like- 
lihood Y would seem to be an unfair predict becaus 
it may charg individu of a certain race more than oth- 
ers, even though no race be more like to have an accident. 
counterfactu fair agre with thi notion. 

lemma 1. consid the structur in figur 2 (left). there 
exist model class and loss function where fit a pre- 
dictor tox onli be not counterfactu fair, while the same 
algorithm will give a fair predictor use both A and X . 

proof. As in the definition, we will consid the popula- 

4the notion of “closest world” be pervas in the literatur 
of counterfactu infer under differ mean (halpern, 
2016; pearl, 2000). here, the cost function use to map fair vari- 
abl to unfair outcom also play a role, but thi concern a 
problem depend util function that would be present anyway 
in the unfair predict problem, and be orthogon to the causal 
assumptions. 



counterfactu fair 

tion case, where the joint distribut be known. consid 
the case where the equat describ by the model in fig- 
ure 2 (left) be determinist and linear: 

X = αa+ βu, Y = γU 

and the varianc of U be vU , the varianc ofa be va, and we 
assum all coeffici be non-zero. the predictor Ŷ (x) 
defin by least-squar regress of Y on onlyx be give 
by Ŷ (x) ≡ λX , where λ = cov(x,i )/v ar(x) = 
βγvu/(α 

2va + β 
2vu ) 6= 0. 

We can test whether a predictor Ŷ be counterfactu fair 
use the procedur describ in section 3: (i) comput U 
give observ of x,y,a; (ii) substitut the equat 
involv A with an intervent valu a′; (iii) comput 
the variabl x,i with the intervent valu a′. It be 
clear here that ŷa(u)=λ(αa + βu) 6= ŷa′(u). thi pre- 
dictor be not counterfactu fair. thus, in thi case fair 
through unawar actual perpetu unfairness. 

consid instead do least-squar regress of Y on X 
and A. note that Ŷ (x,a) ≡ λxx + λaa where λX , λA 
can be deriv a follows: 
( 
λX 
λA 

) 
= 

( 
V ar(x) cov(a,x) 
cov(x,a) V ar(a) 

)−1( 
cov(x,i ) 
cov(a, Y ) 

) 

= 
1 

β2vuva 

( 
vA −αva 
−αva α2va + β2vu 

)( 
βγvu 
0 

) 

= 

( 
γ 
β 
−αγ 
β 

) 
(2) 

now imagin we have observ A = a. thi impli 
that X = αa + βU and our predictor be Ŷ (x, a) = 
γ 
β (αa+ βu) + 

−αγ 
β a = γU . thus, if we substitut a with 

a counterfactu a′ (the action step describ in section 3) 
the predictor Ŷ (x,a) be unchanged! thi be becaus our 
predictor be construct in such a way that ani chang inx 
caus by a chang in A be cancel out by the λa. thu 
thi predictor be counterfactu fair. 

note that if figur 2 (left) be the true model for the real 
world then Ŷ (x,a) will also satisfi demograph pariti 
and equal of opportun a Y will be unaffect by A. 

the abov lemma hold in a more gener case for the 
structur give in figur 2 (left): ani non-const esti- 
mator that depend onli on X be not counterfactu fair 
a chang A alway alter X . We also point out that 
the method use in the proof be a special case of a gen- 
eral method to build a predictor base on inform 
deduc about U that will be describ in the next section. 
We note that, outsid of thi particular causal model in fig- 
ure 2 (left), the predictor Ŷ (x,a) be not counterfactu 
fair, a describ in the follow scenarios. 

scenario 2: high crime regions. A local polic 
precinct want to know how like a give hous be to 
be broken into, Y . thi likelihood depend on mani un- 
observ factor (U ) but also upon the neighborhood the 
hous lie in (x). however, differ ethnic group be 
more like to live in particular neighborhoods, and so 
neighborhood and break-in rate be often correl with 
the race A of the hous occupier. thi can be see in fig- 
ure 2 (center). unlik the previou case, a predictor Ŷ 
train use X and A be not counterfactu fair. the 
onli chang from scenario 1 be that now Y depend on X 
a follows: Y = γU + θX . now if we solv for λX , λA it 
can be show that Ŷ (x, a)=(γ− α2θvaβvu )U +αθa. As thi 
predictor depend on the valu of A, Ŷ (x, a) 6= Ŷ (x, a′) 
and thu Ŷ (x,a) be not counterfactu fair. 

scenario 3: univers success. A univers want to 
know if student will be success post-gradu Y . 
they have inform such as: grade point averag (gpa), 
advanc placement (ap) exam results, and other aca- 
demic featur X . the univers believ however, that 
an individual’ gender A may influenc these featur and 
their post-gradu success Y due to social discrimina- 
tion. they also believ that independently, an individual’ 
latent talent U casu X and Y . We show thi in figur 2 
(right). We can again ask, be the predictor Ŷ (x,a) coun- 
terfactu fair? In thi case, the differ between thi and 
scenario 1 be that Y be a function of U and A a follows: 
Y = γU + ηa. We can again solv for λX , λA and show 
that Ŷ (x, a) = (γ − αηvaβvu )U + ηa. again Ŷ (x,a) be a 
function of A so it cannot be counterfactu fair. 

5. method and assess 
given that the unawar and full inform model be 
not counterfactu fair, how can we design predictor that 
are? In gener give a causal model, a counterfactu 
fair classifi Ŷ be one that be a function of ani U and ani 
variabl X which be not descend of A. As defined, 
these variabl be independ ofa and thu ani chang in 
A cannot chang Ŷ . In thi section we describ techniqu 
for construct latent variabl U and a predictor Ŷ . 

befor delv into details, we point out two impor- 
tant observations. first, if a strict subset of U be used, 
the causal model need not be fulli specified: equat 
Vi = fi(pai, upai) can be substitut by a condit 
probabl p(vi | pai, U ′pai), where U ′pai ⊂ upai and 
p(vi | pai, U ′pai) = 

∫ 
fi(pai, upai)du 

′′ 
pai , where U 

′′ 
pai ≡ 

upai\u ′pai . thi margin have implic in mod- 
ele discuss in the next section. 

second, ani random variabl gener independ be 
trivial counterfactu fair. however, we desir that 



counterfactu fair 

Ŷ be a good predictor, not simpli a coin toss. that is, 
Ŷ be typic a parameter function gθ(u,x) where 
θ be learn by minim the empir expect loss 
e[l(y, gθ(u,x)) |x,a]. for instance, l(y, gθ(u,x)) = 
(Y − gθ(u,x))2, or the log-loss for bernoulli classifica- 
tion. In practice, the distribut ofa∪x∪{i } can be the 
empir distribut a give by some train data, while 
p(u | x,a) come from the estim causal model fit to 
the same train data. ani predictor can be use to learn 
gθ(u,x) includ random forest and neural networks. 

5.1. limit and a guid to model build 

causal model requir untest assumptions. exper- 
iment data can sometim be use to infer causal con- 
nections, but counterfactu model requir function 
decomposit between background and endogen vari- 
ables. such decomposit be not uniqu identifi 
with experiment data. As in sever matter of law and 
regulation, fair at an individu level be a counterfactu 
quantiti and some level of assumpt be unavoidable. 
As a guid for build fair predict models, we catego- 
rize assumpt by three level of increas strength. 

level 1 given a causal dag, build Ŷ use a covari onli 
the observ variabl not descend of the pro- 
tect attribut A. thi requir inform about 
the dag, but no assumpt about structur equa- 
tion or prior over background variables. 

level 2 level 1 ignor much information, particularli if the 
protect attribut be typic attribut such a race 
or sex, which be parent of mani other variables. To 
includ inform from descend ofa, we postu- 
late background latent variabl that act a caus of 
observ variables, base on explicit domain knowl- 
edg and learn algorithms5. inform from X 
will propag to the latent variabl by conditioning. 

level 3 In level 2, the model factor a a gener dag, and 
each node follow a non-degener distribut give 
observ and latent variables. In thi level, we remov 
all random from the condit distribut ob- 
tain a full decomposit (u, V, F ) of the model. 
for instance, the distribut p(vi | v1, . . . , vi−1) 
can be treat a an addit error model, Vi = 
fi(v1, . . . , vi−1)+ei (peter et al., 2014). the error 
term ei then becom an input to Ŷ after condit 
on the observ variables. thi maxim the infor- 
mation extract by the fair predictor Ŷ . 

5in some domains, it be actual common to build a model en- 
tire around latent construct with few or no observ parent 
nor connect among observ variabl (bollen, 1989). 

5.2. special case 

consid the graph A → X → Y . In general, if Ŷ be 
a function of X only, then Ŷ need not obey demograph 
parity, i.e. 

P (ŷ | A = a) 6= P (ŷ | A = a′). 
If we postul a structur equat X = αA + eX , then 
give A and X we can deduc eX . If Ŷ be a function of 
eX onli and, by assumption, eX be independ of A, then 
the assumpt impli that Ŷ will satisfi demograph par- 
ity, and that can be falsified. By way of contrast, if eX be 
not uniqu identifi from the structur equat and 
(a,x), then the distribut of Ŷ depend on the valu of 
A a we margin eX , and demograph pariti will not 
follow. thi lead to the following: 
lemma 2. If all background variabl U ′ ⊆ U in the defi- 
nition of Ŷ be determin froma and evidencew , and all 
observ variabl in the definit of Ŷ be independ 
of A give U ′, then Ŷ satisfi demograph parity. 

thus, counterfactu fair can be thought of a a coun- 
terfactu analog of demograph parity. We advoc that 
counterfactu assumpt should underli all approach 
that separ the sourc of variat of the data into “fair” 
and “unfair” components. As an example, louizo et al. 
(2015) explain the variabl in X from A and an inde- 
pendent sourc U follow the dag A → X ← U . As 
U and A be not independ give X in thi representa- 
tion, a type of “posterior regularization” (ganchev et al., 
2010) be enforc such that a posterior pfair(u |a,x) be 
close to the model posterior p(u | a,x) while satisfi 
pfair(u |A = a,x) ≈ pfair(u |A = a′, x). but thi be 
neither necessari nor suffici for counterfactu fair 
if the model forx givena andu be not justifi by a causal 
mechanism. If it is, p(u | a,x) be justifi a distribut 
which we can use to margin U in p(ŷ (u) | a,x), 
without requir regularization. method which estim 
the relationship between A, U and X base on penaliz- 
ing depend measur between an estim U and A 
be relev in estim a causal model (e.g. mooij et al. 
(2009)), but these be motiv by U be be determinis- 
tical infer from A and X by construction. It be unclear 
in louizo et al. (2015) how the ideal label Y be causal 
connect to U and A, and the semant of the “unfair” 
compon of Y be not detailed. 

6. experi 
We test our approach on two practic problem that re- 
quir fairness, the first be predict of success in law school 
and the second be separ actual and perceiv criminal- 
iti in polic stops. for each problem we construct causal 
models, and make explicit how unfair may affect ob- 
serv and unobserv variabl in the world. given these 



counterfactu fair 

model we deriv counterfactu fair predictors, and pre- 
dict latent variabl such a a person’ ‘criminality’ (which 
may be use for predict crime) a well a their ‘per- 
ceiv criminality’ (which may be due to prejudic base 
on race and sex). We analyz empir how counterfac- 
tualli fair the unawar and full predictor are, assum 
knowledg of the correct causal model, and compar the 
predict accuraci of all models. final we judg how 
well our counterfactu fair ‘criminality’ score satisfi 
demograph parity. 

6.1. law school success 

the law school admiss council conduct a survey 
across 163 law school in the unit state (wightman, 
1998). It contain inform on 21,790 law student such 
a their entranc exam score (lsat), their grade-point av- 
erag (gpa) collect prior to law school, and their first 
year averag grade (fya). 

given thi data, a school may wish to predict if an applic 
will have a high fya. the school would also like to make 
sure these predict be not bia by an individual’ race 
and sex. however, the lsat, gpa, and fya scores, may 
be bia due to social factors. We compar our framework 
with two unfair baselines: 1. full: the standard techniqu 
of use all features, includ sensit featur such a 
race and sex to make predictions; 2. unaware: fair 
through unawareness, where we do not use race and sex a 
features. for comparison, we gener predictor Ŷ for all 
model use logist regression. 

fair prediction. As describ in section 5.1, there be 
three way in which we can model a counterfactu fair 
predictor of fya. level 1 us ani featur which be not 
descend of race and sex for prediction. level 2 model 
latent ‘fair’ variabl which be parent of observ vari- 
ables. these variabl be independ of both race and 
sex. level 3 model the data use an addit error model, 
and us the independ error term to make predictions. 
these model make increasingli strong assumpt cor- 
respond to increas predict power. We split the 
dataset 80/20 into a train/test set, preserv label balance, 
to evalu the models. 

As we believ lsat, gpa, and fya be all bia by race 
and sex, we cannot use ani observ featur to construct 
a counterfactu fair predictor a describ in level 1. 

In level 2, we postul that a latent variable: a student’ 
knowledg (k), affect gpa, lsat, and fya scores. the 
causal graph correspond to thi model be show in fig- 

tabl 1. predict result use logist regression. note that we 
must sacrific a small amount of accuraci to ensur counterfac- 
tualli fair predict (fair K, fair add), versu the model that 
use unfair features: gpa, lsat, race, sex (full, unaware). 

full unawar fair K fair add 
rmse 0.873 0.894 0.929 0.918 

ure 3, (level 2). thi be a short-hand for the distributions: 

gpa ∼ N (bg + wkgk + wrgr+ wsgs, σg) 
lsat ∼ poisson(exp(bl + wklk + wrlr+ wsls)) 

fya ∼ N (wkf K + wrfr+ wsfs, 1) 
K ∼ N (0, 1) 

We perform infer on thi model use an observ 
train set to estim the posterior distribut of K. We 
use the probabilist program languag stan (stan de- 
velop team, 2016) to learn K. We call the predictor 
construct use K, fair K. 

know 

gpa 

lsat 

fya 

race 

sex 

gpa 

lsat 

fya 

race 

sex 

level 2 level 3 

✏G 

✏L 

✏F 

figur 3. A causal model for the problem of predict law school 
success fairly. 

0 

1 

2 

3 

−1.0 −0.5 0.0 0.5 
pred_zfya 

de 
n 
iti 

type 
origin 

swap 

0 

1 

2 

3 

−1.0 −0.5 0.0 0.5 
pred_zfya 

de 
n 
iti 

type 
origin 

swap 

0 

1 

2 

3 

−1.0 −0.5 0.0 0.5 
pred_zfya 

de 
n 
iti 

type 
origin 

swap 

0 

1 

2 

3 

−1.0 −0.5 0.0 0.5 
pred_zfya 

de 
n 
iti 

type 
origin 

swap 

0.0 

0.5 

1.0 

1.5 

2.0 

−0.5 0.0 0.5 
pred_zfya 

de 
n 
iti 

type 
origin 

swap 

0.0 

0.5 

1.0 

1.5 

2.0 

−0.4 0.0 0.4 0.8 
pred_zfya 

de 
n 
iti 

type 
origin 

swap 

0.0 

0.5 

1.0 

1.5 

2.0 

−0.4 0.0 0.4 0.8 
pred_zfya 

de 
n 
iti 

type 
origin 

swap 

0.0 

0.5 

1.0 

1.5 

2.0 

−0.4 0.0 0.4 0.8 
pred_zfya 

de 
n 
iti 

type 
origin 

swap 

fya 
V 

fya 
V 

fya 
V 

fya 
V 

fya 
V 

fya 
V 

fya 
V 

de 
n 

iti 

de 
n 

iti 

de 
n 

iti 

de 
n 

iti 
de 

n 
iti 

de 
n 

iti 

de 
n 

iti 

de 
n 

iti 

female$ maleblack$ white asian$ white mexican$ white 

Fu 
ll 

U 
na 

w 
ar 

e 

origin 
data 

counter- 
factual 

figur 4. densiti plot of predict fyaa and fyaa′ . 

In level 3, we model gpa, lsat, and fya a continu 
variabl with addit error term independ of race and 
sex (that may in turn be correl with one-another). thi 
model be show in figur 3, (level 3), and be express by: 

gpa = bG + wrgr+ w 
S 
GS + �g, �G ∼ p(�g) 

lsat = bL + wrlr+ w 
S 
LS + �l, �L ∼ p(�l) 

fya = bF + wrfr+ w 
S 
FS + �F , �F ∼ p(�f ) 

We estim the error term �g, �L by first fit two mod- 
el that each use race and sex to individu predict gpa 



counterfactu fair 

figur 5. understand criminality. the abov map show the decomposit of stop and search data in new york into factor base 
on perceiv crimin (a race depend variable) and latent crimin (a race neutral measure). see section 6.2. 

and lsat. We then comput the residu of each model 
(e.g., �g=gpa−ŷgpa(r,s)). We use these residu esti- 
mate of �g, �L to predict fya. We call thi fair add. 

accuracy. We compar the rmse achiev by logist 
regress for each of the model on the test set in tabl 1. 
the full model achiev the low rmse a it us race 
and sex to more accur reconstruct fya. note that in 
thi case, thi model be not fair even if the data be gen- 
erat by one of the model show in figur 3 a it cor- 
respond to scenario 3. the (also unfair) unawar model 
still us the unfair variabl gpa and lsat, but becaus 
it do not use race and sex it cannot match the rmse of 
the full model. As our model satisfi counterfactu fair- 
ness, they trade off some accuracy. our first model fair 
K us weaker assumpt and thu the rmse be highest. 
use the level 3 assumptions, a in fair add we produc 
a counterfactu fair model that trade low rmse for 
slightli weaker assumptions. 

crimin 

appear 

race 

percept 

arrest 

frisk 

search 

weapon 

forc 

summon 

figur 6. A causal model for the stop and frisk dataset. 

counterfactu fairness. We would like to empir 
test whether the baselin method be counterfactu fair. 
To do so we will assum the true model of the world be 
give by figur 3, (level 2). We can fit the paramet of 
thi model use the observ data and evalu counter- 
factual fair by sampl from it. specifically, we will 
gener sampl from the model give either the observ 
race and sex, or counterfactu race and sex variables. We 

will fit model to both the origin and counterfactu sam- 
plead data and plot how the distribut of predict fya 
chang for both baselin models. figur 6.1 show this, 
where each row correspond to a baselin predictor and 
each column correspond to the couterfactu change. In 
each plot, the blue distribut be densiti of predict fya 
for the origin data and the red distribut be thi densiti 
for the counterfactu data. If a model be counterfactu 
fair we would expect these distribut to lie exactli on 
top of each other. instead, we note that the full model ex- 
hibit counterfactu unfair for all counterfactu ex- 
cept sex. We see a similar trend for the unawar model, 
although it be closer to be counterfactu fair. To see 
whi these model seem to be fair w.r.t. to sex we can look 
at weight of the dag which gener the counterfactu 
data. specif the dag weight from (male,female) to 
gpa be (0.93,1.06) and from (male,female) to lsat be 
(1.1,1.1). thus, these model be fair w.r.t. to sex sim- 
pli becaus of a veri weak causal link between sex and 
gpa/lsat. 

6.2. true vs. perceiv crimin 

sinc 2002, the new york polic depart (nypd) have 
record inform about everi time a polic offic have 
stop someone. the offic record inform such a 
if the person be search or frisked, their appearance, etc. 
We consid the data collect on male stop dure 
2014 which constitut 38,609 records. 

model. We model thi stop-and-frisk data use the 
graph in figur 6. specifically, we posit main caus for 
the observations: arrest (if an individu be arrested), 
summon (an individu be call to a court-summons), 
weapon (an individu be found to be carri a weapon), 
forc (some sort of forc be use dure the stop), 
frisked, and searched. the first caus of these observa- 
tion be some measur of an individual’ latent criminal- 
ity, which we do not observe. We believ there be an addi- 
tional cause, an individual’ perceiv criminality, percep- 
tion, also unobserved. thi second factor be introduc a 
we believ that these observ may be bia base on 



counterfactu fair 

an officer’ percept of whether an individu be like a 
crimin or not. thi percept be affect by an individ- 
ual’ appear and their race. In thi sens crimin 
be counterfactu fair, while percept model how race 
affect each of the other observ variables. 

crimin and percept distributions. after fit 
thi model to the data we can look at the distribut of 
crimin and percept across differ races, show 
a box plot in figur 6. We see that the median criminal- 
iti for each race be nearli identical, while the distribut 
be somewhat different, demonstr that crimin ap- 
proach demograph parity. the differ that due ex- 
ist may be due to unobserv confound variabl that 
be affect by race or unmodel nois in the data. On 
the right percept vari consider by race with white 
individu have the low perceiv crimin while 
black and black hispan individu have the highest. 

● 

● 

● 

● 

●● 

● 
● 

● 

● 

●● 

● 

● 

● 

●● 

● 

● 

●● 

● 

● 

● 

●● 

● 

●● 

● 

● 

● 

●●● 

●● 

●●●●● 

●●● 

● 
● 

● 

● 

● 

● 

●● 

●● 

● 

● 

● 
● 

● 

●● 

●●●●● 

●●● 

● 

● 
●● 

● 

● 

●● 

● 

● 

● 
● 

● 

●● 

● 

● 

● 

●● 

● 

●● 

●●●● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●●● 
● 
● 

●● 

● 

●●● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 
● 

● 

● 

●●● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

●● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
●● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

●● 

● 

● 

● 

● 

●●● 

● 

● 

●● 

● 
● 
● 

● 

● 

● 

●●●● 

●● 

● 

● 

● 
●● 

● 

●●● 

●● 
● 
● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 
●●●●● 

●●●●● 

● 

● 

●●● 

● 

●● 
● 

● 

● 

● 

● 

● 

● 
● 
● 

● 

● 

● 

●● 

● 

●●● 

● 

●● 

● 
● 

● 

● 

● 
● 

● 

● 
●● 

●●● 

● 

● 

● 

●●● 

● 

● 

● 

● 

● 

●●●●●●●●● 

● 
● 

● 
●● 

● 

● 

●● 

● 

● 

● 

● 

●● 

● 
●● 
● 
● 

●●●● 

● 

●● 

●● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

●● 

● 

●● 

●● 

●●●● 

●●●●●● 

● 

●●● 

● 

● 

● 

● 

●● 

● 

●●● 

● 

● 

●● 

● 

● 

● 

●● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

●●●●● 

● 

● 

● 

●● 

●● 

● 

● 

●● 

● 

●● 

●● 

● 

● 

● 

● 

● 
● 

●●● 

●●●● 

●●●● 

● 

● 

● 

●● 
● 
● 

● 

●● 

● 

● 

● 

●●● 

●● 
● 

●● 

● 

● 

● 

● 

● 

● 

●●● 

● 

● 

● 

● 
● 

● 

●● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

●● 

●●● 

● 

● 

● 

●● 

● 

● 

● 

●●● 

● 

● 

● 
● 

● 

● 

●● 

● 
●●● 

● 

● 

● 

● 

●● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

●● 

●● 

●● 

● 

● 

● 

● 
●● 

● 

● 
● 

● 

● 

●● 

●●● 

● 

● 

● 

● 
● 

● 

●● 

● 

●● 

●●● 

● 

● 

● 

● 

● 

● 

●●●● 

● 

●● 

●●● 

● 

● 

●● 

● 

●● 

●● 

● 

● 

● 

● 

●●●● 

● 

● 

● 

● 

●●● 

● 

● 

● 

● 
● 

●●●● 

● 

●● 

● 

● 

● 

● 

● 

●●●● 

● 

● 

● 

●● 
● 

● 

●●●●●●●●● 

● 

● 

● 

●●● 

● 

●● 
●●●● 

● 

●● 

● 

● 
●●● 

● 

● 

● 

● 

●● 

●● 

● 

● 
● 

● 

● 

●● 

●●● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

●● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

●● 

●● 

● 

● 

● 

●● 

● 

●● 

● 

● 

● 

● 

●● 

● 

● 

●● 

● 

●● 

● 

● 

● 

●●● 

● 

● 

● 

● 

● 

●● 
● 
●● 

● 

● 

●● 

●● 

●●●● 

● 

●●●●●● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 
● 

● 

●●● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

●●● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

●● 

● 

● 

● 

● 

●● 

● 

● 
● 

● 

● 

● 

●● 

● 

● 

● 

●● 

● 

● 

●● 

●● 

● 

● 

●● 

● 

● 

●● 

● 

●● 

● 
● 
● 

● 

● 

● 

●●●●● 

●● 

● 

●● 

●●● 

●● 

● 

●● 

● 

● 

●● 

● 

●●●●● 

● 
● 

● 

● 

● 

●● 

● 
● 

●●●● 

● 

●●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

●● 

● 
● 

● 

●● 

● 

● 

● 
● 

● 

● 

● 

● 

●●●● 

●● 

● 

● 
● 
● 

●● 
● 

● 

● 

●● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 
● 
●●●●●●● 
● 

● 

● 

●●● 

● 

● 

● 
●● 
● 

● 

● 

● 

● 
●● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

●●● 

● 

● 

●● 

●● 

● 

● 

● 

●●● 
● 

● 

●● 

● 

● 
● 

●● 
● 

● 

●●● 

● 

● 

● 
● 

●●● 

● 

● 

● 

●● 
● 

●● 

● 

● 

●●● 

● 

●● 

● 

● 

● 

● 

● 

● 
●● 

● 

●●● 

●● 

● 

●●●● 

● 

● 

● 

● 

●● 

●● 

● 

● 

● 

●● 

● 

● 

● 

●● 

● 

● 

● 

●●● 

● 

●● 
● 
● 

●●● 

●● 

●● 
●● 

●●●●●● 

● 

● 
● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 
●●● 

●● 

● 

●●●●● 
● 

●●●● 

●●●●● 

● 

● 

● 

● 

● 

● 

●●● 

● 

● 

● 

● 
●● 
● 

● 

●●● 

● 

●●● 
● 

● 

● 

●● 

● 

●●● 

●●● 

● 

● 

● 

● 

● 

● 

●●●●●● 

● 

●● 

● 

●●●● 

● 

●● 

● 

●●● 

●●● 

●●●● 

● 

●● 

● 

● 

●● 

● 

● 

● 

●● 

● 

● 

●● 

● 

● 

● 

● 

● 

●●● 

● 

● 

● 

● 

●●●● 

● 

●● 

● 

● 

●● 

●● 

● 

● 

● 

● 

● 

● 

● 

●●●●● 
●● 

●● 

● 

● 

●● 
● 

●●● 

● 

● 

● 

● 

● 

●● 

● 

● 

●● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 
● 
● 

● 

● 

● 

● 

●● 

●● 

● 

●●● 

● 

●● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 
●● 

●● 

● 
● 

● 

●● 

● 

● 

● 

●● 
● 

● 

●● 

● 

● 

●● 

● 

●● 

● 

● 

● 

● 
● 

● 

● 

● 

●● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

●● 

●●● 

● 
●● 

● 

● 

●● 

●● 
● 

● 

●●● 

● 

●● 

● 
● 

● 

● 

●● 

●●●● 

● 
● 

● 

● 
● 

● 
●● 

●●●●●●● 

● 

●● 

●● 

● 

●● 

● 

● 

● 

●● 

●● 

●●●● 

● 

● 

● 

● 

● 
●●●● 

● 

● 

● 

● 

● 

● 

●●●● 

● 

● 

● 

●● 

●●●● 

● 

● 

●● 

●● 

● 

● 

●●● 

● 

● 

● 

● 

● 

● 

●● 

●● 

● 

●● 

●● 

● 

● 

●●● 

●● 

● 

● 

● 
● 

●●●●● 

● 
● 

●●●● 

●●●●●●● 
● 

● 

● 

●● 
●●●● 

●● 

● 

● 

●● 

● 

● 

● 

● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

●●●● 

● 

● 

● 

●●● 

● 

●●● 

● 

● 

● 

● 

● 

● 

● 

●●●●● 

● 

●●● 

● 
● 

●● 

●● 

●● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 
● 

● 

●● 

● 

●●●●●● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

●●● 

● 

●●● 

●●● 
● 

● 

● 

● 

●● 

● 

● 

●●● 

●● 

● 

● 

●●●● 

● 

● 
● 

● 

● 

● 

● 

●●● 

● 

● 

● 

● 

●● 

●●●● 

● 
● 
●●● 
● 

● 

● 

● 
● 

● 

● 

●● 

● 

● 
● 

● 

● 

● 

● 

●●●●●●●● 

● 

●● 

● 

●● 

● 

●● 

● 

● 

●● 

● 

● 

●● 

● 

● 

● 

● 

●● 

● 

● 

●●●●●● 

● 

● 

● 

● 
● 

● 

● 

● 

●●● 

●● 

●●● 

● 

● 
●● 

● 

●●● 

● 

● 

●●● 

● 

●●●● 

● 

● 

●● 
● 

● 

● 

● 

●●● 

●● 

● 

●●●● 

● 

●● 

● 

● 

●●●●● 

● 

● 

●●●●●●●● 

● 

● 

● 

●●●● 

●● 

●●● 

●● 

●● 

● 

●●● 

● 

●●● 

●●● 

● 
●● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
●● 

● 

●●●● 

● 

● 

● 

● 

● 

● 

● 

●●● 

● 

● 

● 

● 

●●● 

●● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●●● 

●●●● 

● 

●● 

● 
● 

●●●● 

● 

● 

● 
●● 

●● 

●● 

●● 

● 

● 

● 
● 

● 

● 

●●● 

● 

● 

●●● 

● 

● 

● 

●●●● 

● 

● 

●● 

●● 
●●● 

● 

● 
● 

● 

● 

●●● 

● 

● 

●● 

●● 

● 

● 

●●●● 

● 

●●●●● 

● 

● 

● 

●●●● 

● 

● 

● 

● 

● 

●● 

● 

● 

●●●● 

● 

● 

● 
●● 

●● 

●●●● 

● 

● 

●●● 

● 

●● 
● 

● 

● 

● 

●● 

● 

●●●●● 

● 

● 

●● 

●● 

● 

● 

● 

●●● 

● 

● 
● 

●●● 

● 

● 

● 

● 

● 
● 

●● 

●● 

● 

● 

●●● 

● 

●●●●●●●● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

●● 

● 

● 

● 

● 

● 

● 

● 

●● 
● 

● 

●● 

● 

●●●●●● 

● 

● 

● 

● 

● 

●● 

● 

●● 

●● 

●● 

● 
● 

●● 

● 

●● 

● 

●●●● 

● 

● 

●● 

●●●●●● 

●● 
●● 

● 

● 

● 

●●●● 

● 

● 

● 

●●●●●● 

● 

● 

●● 

● 

● 

● 

●●●● 

● 

●● 

● 

● 

●● 

● 

● 
● 
●● 

● 

● 

● 

● 

●●●● 

● 

● 

●●●●● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

●●● 

● 

● 

●● 

● 

●● 

● 

● 

● 
● 
● 

●●● 

● 

●● 

● 

● 
● 

● 

● 

●●●●● 

● 

● 

●● 

●● 

● 

●●● 

● 

●●●●● 

● 

●● 

●● 

● 

●● 

● 

● 

●●●●●● 

●● 

● 
● 
●●● 

● 

● 

● 

●●●●●●●●●●● 

● 

● 

● 

● 

●●● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●●● 

● 

●●● 

● 
● 
● 

●●● 

● 

● 

● 
● 

● 

●● 

●● 

● 

● 

● 

●● 

● 

● 

● 

● 

●● 

● 

● 

●● 

●● 

●●● 

● 

● 

● 

●● 

●● 

● 

●●● 

● 
● 

●●●●●●●●● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

●● 

● 

● 
● 

● 

● 

● 

●● 

● 

● 

●●● 

● 
●●● 

● 

●●● 

● 
●● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

●●● 

● 

●● 

● 

●● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

●● 

● 

● 

●●● 

●● 

●● 

● 

●● 

● 

● 
●● 

● 

● 

● 

●●● 

● 
● 

● 
●● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

●●● 

● 

● 

● 
● 

●●●● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

●● 
● 

●● 

● 

● 

● 

● 

●● 

● 
●●● 

● 

● 

● 

● 

● 

●●●●●●●●●●● 
●●●● 

● 
● 
● 

●●● 

● 

● 

● 

● 

●●● 

● 

●● 

●● 

● 

● 

● 

● 

● 

● 

● 

●●●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

●● 
● 

● 

● 
● 

● 

● 

● 

● 

●●●● 

● 

● 

● 
● 

● 

● 

●●● 

●● 

●● 

●●●●● 

● 
● 

● 

● 

● 

●●●●●● 

● 

● 

●● 
● 

● 

●● 

●●● 

●● 

● 

● 

● 

● 

●● 

● 

● 

●● 

● 

● 

● 

● 

●●● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 
● 
● 
● 

● 

● 

● 

● 
● 

●● 

● 

● 
●● 

● 

●● 

● 

●●● 
● 

●●● 

● 

● 

● 

● 
● 

● 

● 

●● 

●● 

● 

● 
●●● 
● 
● 

●●● 
● 
● 

●● 

● 

●●● 

● 

●● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

●●● 

●● 

● 

● 

● 

● 
● 

●● 

● 

● 

● 
● 

●● 

● 

● 

●● 

● 

● 

● 

●● 

● 

●● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●●● 

● 

●●●●● 

● 

●● 

● 

●● 

● 

●●●●●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●●● 

● 

● 

● 
● 

●● 

●●●●● 

● 

● 

● 

● 
●●● 
●● 

●●● 

● 

●●●● 

● 

●● 

● 

●● 

●●●● 

● 

● 

●●● 

● 

● 

● 

● 

● 

● 

● 

●●● 

●● 

● 

● 

● 

● 

●● 

●●●●● 

● 

● 

● 

● 

● 

● 
●● 

●● 

● 

● 

● 

●●● 
● 

●●● 

● 

●● 

●● 

● 

●● 

●● 

● 

● 

● 

●● 

● 

● 

● 

●● 

●● 

●●●● 

●● 

● 

●●●●● 

● 

● 

● 

● 

● 

● 

●● 

● 

●● 

● 
● 

● 

● 

● 
● 

●●●●●● 

●●●●●● 

● 
● 

●● 

● 

●●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

●● 

● 

● 

● 

● 

● 

● 

●● 

● 

●● 

●● 

● 

● 

● 

●● 

● 

● 

● 
●● 

● 

● 

●●● 

● 

● 

● 

● 
●● 

● 

● 
● 

● 

● 

●● 

● 
●● 
● 

● 

● 

● 

● 

●● 

● 

● 

●● 

●● 
● 

● 

● 
● 

● 

● 

●●●●●● 

●●● 

● 
● 

● 

●●● 

● 

●● 

● 

●● 
● 

● 

● 

● 

● 

● 

● 

●●● 

● 

● 

● 

●● 

● 

● 

● 
●● 
● 

● 

●●● 

●●● 
● 

●●● 

● 

● 

● 

● 

●● 

● 

● 

●● 

● 

● 

● 

●●●● 

● 

● 

● 

● 

● 
● 

● 

● 

●●●●●●●●●●● 

● 

●● 

● 

●● 

● 

● 

●●●●● 

● 

● 

●●●●● 

● 

● 
● 

● 

● 

● 
● 
● 

● 

● 

●● 

● 

● 

● 

● 

●● 

● 

● 

● 

●● 

●● 

●●● 

● 
● 

● 

●●● 

● 

●●●●●● 

● 

●● 

● 

● 

●● 

● 

● 

● 

● 

● 

●● 

● 
● 

● 

● 

● 
●● 

● 

● 
● 

● 

● 

● 

● 

● 

●●●● 

● 

● 

● 

● 

●●● 

● 

●● 

●● 

●● 
● 

●●●● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

●● 

●●● 

●● 

● 

● 

● 
● 

● 

● 

●● 

●●●● 

● 

● 

●● 

●●● 

● 
●● 

● 

● 

●●●●● 

●●●● 

● 

● 

● 

●●● 

● 
● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 
●● 

● 

● 
● 

●●●●● 

● 

● 

●●● 

●● 

● 

● 

● 

● 

● 

● 

●●●●● 

● 

● 

●●● 

● 

● 
●●● 

● 

● 

● 

● 

● 

●●● 

●● 

● 

● 

● 

● 

● 

●● 

●●●●●● 
●● 

● 

●● 

●● 

● 

● 

● 

● 

● 

● 

● 

●●● 

● 

● 

●●● 

● 

●● 

● 

● 

●●●● 

● 

● 

●● 

● 

●● 

● 

● 

●● 

● 

● 

● 

●● 

● 

●●● 

● 

● 

●●● 

● 

● 

●● 

● 

● 

● 

● 

● 

●●● 

● 
● 

● 

● 

●●●● 

●● 

●●●●●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

●● 

●● 

● 

● 

● 

●●● 

● 

● 

●● 

● 

● 
● 

● 

●● 

● 

●● 

● 

● 

● 
●●●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●●● 

● 

● 

● 

●●● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

●●● 

●● 

● 

●● 

● 

●●● 

● 

● 

● 

● 

● 
●● 

● 

●● 

● 

●●●●● 

● 

● 

●●● 

●● 

● 

● 

● 

● 
●● 

●● 

● 

● 

● 
● 

●●● 

● 

●● 

● 
● 
● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

●● 

● 

●● 

● 

● 

● 

●● 

● 

● 
●● 

●●●● 

● 

● 

●● 

●● 
● 

● 

●●●● 

● 

● 

● 
● 
● 

● 

● 

●● 

● 

● 

● 

● 

●●●● 

● 

●●●● 

● 

● 

● 

● 
● 
● 

● 

● 

● 

● 

● 

● 

●●● 

● 

● 

● 

● 

● 

●●● 

● 

● 

● 
● 

● 

●●●●● 

● 

● 

●● 

●●● 

● 

● 

● 

● 

● 

● 

● 

● 
● 
● 

● 

● 
● 
● 
●● 

● 

● 

●● 

● 

●●● 

●● 

●●●●●●● 

● 

● 

●● 

●● 

● 

● 

●● 

● 
●● 

● 

● 

●● 

● 

● 

●●● 

● 

●●●● 

● 

● 

● 
● 

●● 

● 

● 

●● 

●● 

● 

● 
● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 
●● 

●●●●●●●●●●●●●●●●●●●●●●●● 
●●●●●●●●●● 
● 
●●●●● 

● 

●●●●●●●●●●●●●●●●●●●●●●●●●●●●● 
●●●●●●●●●●●● 
●●● 
● 
●●●●●●●●●●●●●● 
●● ●●●●●●●●●●●●●●● 

●●●●●●●●●●●●● 
● 
● 
●●●●●●●● 
●●●●●● ● 

●●●●●●●● 
●●● ● 

● 

crimin percept 

black blackhisp hispan white aspi naam black blackhisp hispan white aspi naam 

−2 

0 

2 

race 

E 
st 

im 
at 

ed 
la 

te 
nt 

fa 
ct 

or 

result for stop and frisk exampl 

figur 7. distribut of estim latent percept and crimi- 
naliti score for the stop and frisk dataset. 

visual on a map of new york city. each of the 
stop can be map to longitud and latitud point for 
where the stop occurred6. thu we can visual crimi- 
naliti and percept alongsid race and the combin 
of arrest and summons, show in figur 5. criminal- 
iti seem to be a continu approxim of arrest and 
summon a both plot show red in similar areas. how- 
ever, the plot show that certain areas, while have a lot 
of arrest have low crimin score such a south bronx 
and west queen (circl in orange). We can also com- 
pare the perceiv crimin with a plot of race, where 
we have divid the race into group A: black, black his- 
panic, hispanic, and nativ american (shown in purple); 
and group B: white and asian/pacif island (shown in 
green). group A be all race that have posit weight on 
the connect from race to percept in the fit model, 
while group B all have neg weights. thu be in 
group A lead one to have a high perceiv criminal- 
iti than be in group B. thi can be see in the right- 

6https://github.com/stablemarkets/stopandfrisk 

most plot of figur 5. certain area of town such a central 
brooklyn, central bronx, and southern queen have veri 
high crimin and almost all stop be by member of 
group A (circl in yellow). 

7. conclus 
We have present a new model of fair we refer to a 
counterfactu fairness. It allow u to propos fair algo- 
rithm that, rather than simpli ignor protect attributes, 
be abl to take into account the differ social bia that 
may aris toward individu of a particular race, gender, 
or sexual and compens for these bia effectively. 
We experiment contrast our approach with previou 
unfair approach and show that our explicit causal mod- 
el captur these social bia and make clear the implicit 
trade-off between predict accuraci and fair in an 
unfair world. We propos that fair should be regul 
by explicitli model the causal structur of the world. 
criteria base pure on probabilist independ cannot 
satisfi thi and be unabl to address how unfair be oc- 
cur in the task at hand. By provid such causal tool 
for address fair question we hope we can provid 
practition with custom techniqu for solv a wide 
array of fair model problems. 

refer 
bollen, K. structur equat with latent variables. 

john wiley & sons, 1989. 

bolukbasi, tolga, chang, kai-wei, zou, jame Y, 
saligrama, venkatesh, and kalai, adam T. man be to 
comput programm a woman be to homemaker? de- 
bias word embeddings. In advanc in neural infor- 
mation process systems, pp. 4349–4357, 2016. 

brennan, tim, dieterich, william, and ehret, beate. evalu- 
ate the predict valid of the compa risk and need 
assess system. crimin justic and behavior, 36 
(1):21–40, 2009. 

calders, toon and verwer, sicco. three naiv bay ap- 
proach for discrimination-fre classification. data 
mine and knowledg discovery, 21(2):277–292, 
2010. 

dedeo, simon. wrong side of the tracks: big data and 
protect categories. arxiv preprint arxiv:1412.4643, 
2014. 

dwork, cynthia, hardt, moritz, pitassi, toniann, reingold, 
omer, and zemel, richard. fair through aware- 
ness. In proceed of the 3rd innov in theoret- 
ical comput scienc conference, pp. 214–226. acm, 
2012. 



counterfactu fair 

ganchev, k., graca, j., gillenwater, j., and taskar, B. pos- 
terior regular for structur latent variabl mod- 
els. journal of machin learn research, 11:2001– 
2049, 2010. 

grgic-hlaca, nina, zafar, muhammad bilal, gummadi, 
krishna P, and weller, adrian. the case for process 
fair in learning: featur select for fair decis 
making. nip symposium on machin learn and the 
law, 2016. 

halpern, J. actual causality. mit press, 2016. 

hardt, moritz, price, eric, srebro, nati, et al. equal 
of opportun in supervis learning. In advanc in 
neural inform process systems, pp. 3315–3323, 
2016. 

joseph, matthew, kearns, michael, morgenstern, jamie, 
neel, seth, and roth, aaron. rawlsian fair for ma- 
chine learning. arxiv preprint arxiv:1610.09559, 2016. 

kamiran, faisal and calders, toon. classifi without 
discriminating. In computer, control and communica- 
tion, 2009. ic4 2009. 2nd intern confer on, 
pp. 1–6. ieee, 2009. 

kamiran, faisal and calders, toon. data preprocess- 
ing techniqu for classif without discrimination. 
knowledg and inform systems, 33(1):1–33, 2012. 

kamishima, toshihiro, akaho, shotaro, and sakuma, 
jun. fairness-awar learn through regular ap- 
proach. In data mine workshop (icdmw), 2011 
ieee 11th intern confer on, pp. 643–650. 
ieee, 2011. 

khandani, amir E, kim, adlar J, and lo, andrew W. 
consum credit-risk model via machine-learn algo- 
rithms. journal of bank & finance, 34(11):2767– 
2787, 2010. 

kleinberg, jon, mullainathan, sendhil, and raghavan, 
manish. inher trade-off in the fair determin of 
risk scores. arxiv preprint arxiv:1609.05807, 2016. 

louizos, christos, swersky, kevin, li, yujia, welling, 
max, and zemel, richard. the variat fair autoen- 
coder. arxiv preprint arxiv:1511.00830, 2015. 

mahoney, john F and mohen, jame M. method and sys- 
tem for loan origin and underwriting, octob 23 
2007. US patent 7,287,008. 

mooij, j., janzing, d., peters, j., and scholkopf, B. regres- 
sion by depend minim and it applic to 
causal infer in addit nois models. In proceed- 
ing of the 26th annual intern confer on ma- 
chine learning, pp. 745–752, 2009. 

pearl, J. causality: models, reason and inference. 
cambridg univers press, 2000. 

pearl, j., glymour, m., and jewell, N. causal infer in 
statistics: a primer. wiley, 2016. 

pearl, judea et al. causal infer in statistics: An 
overview. statist surveys, 3:96–146, 2009. 

peters, j., mooij, J. m., janzing, d., and schölkopf, B. 
causal discoveri with continu addit nois mod- 
els. journal of machin learn research, 15:2009– 
2053, 2014. url http://jmlr.org/papers/ 
v15/peters14a.html. 

stan develop team. rstan: the r interfac to stan, 
2016. R packag version 2.14.1. 

wightman, linda F. lsac nation longitudin bar passag 
study. lsac research report series. 1998. 

zafar, muhammad bilal, valera, isabel, rodriguez, 
manuel gomez, and gummadi, krishna P. learn fair 
classifiers. arxiv preprint arxiv:1507.05259, 2015. 

zafar, muhammad bilal, valera, isabel, rodriguez, 
manuel gomez, and gummadi, krishna P. fair 
beyond dispar treatment & dispar impact: learn- 
ing classif without dispar mistreatment. arxiv 
preprint arxiv:1610.08452, 2016. 

zemel, richard S, wu, yu, swersky, kevin, pitassi, toni- 
ann, and dwork, cynthia. learn fair representations. 
icml (3), 28:325–333, 2013. 

zliobaite, indre. A survey on measur indirect dis- 
crimin in machin learning. arxiv preprint 
arxiv:1511.00148, 2015. 

http://jmlr.org/papers/v15/peters14a.html 
http://jmlr.org/papers/v15/peters14a.html 

