






















































model-agnost interpret of machin learn 


model-agnost interpret of machin learn 

marco tulio ribeiro marcotcr@cs.uw.edu 
sameer singh sameer@cs.uw.edu 
carlo guestrin guestrin@cs.uw.edu 
univers of washington seattle, WA 98195 usa 

abstract 
understand whi machin learn model 
behav the way they do empow both system 
design and end-us in mani ways: in model 
selection, featur engineering, in order to trust 
and act upon the predictions, and in more intuit 
user interfaces. thus, interpret have becom 
a vital concern in machin learning, and work 
in the area of interpret model have found re- 
new interest. In some applications, such model 
be a accur a non-interpret ones, and thu 
be prefer for their transparency. even when 
they be not accurate, they may still be prefer 
when interpret be of paramount importance. 
however, restrict machin learn to inter- 
pretabl model be often a sever limitation. In thi 
paper we argu for explain machin learn 
predict use model-agnost approaches. By 
treat the machin learn model a black- 
box functions, these approach provid crucial 
flexibl in the choic of models, explanations, 
and representations, improv debugging, com- 
parison, and interfac for a varieti of user and 
models. We also outlin the main challeng for 
such methods, and review a recently-introduc 
model-agnost explan approach (lime) that 
address these challenges. 

1. introduct 
As machin learn becom a crucial compon of an 
ever-grow number of user-fac applications, inter- 
pretabl machin learn have becom an increasingli 
import area of research for a number of reasons. first, 
a human be the one who train, deploy, and often use the 
predict of machin learn model in the real world, it 
be of utmost import for them to be abl to trust the model. 

2016 icml workshop on human interpret in machin 
learn (whi 2016), new york, ny, usa. copyright by the 
author(s). 

apart from indic such a accuraci on sampl instances, 
a user’ trust be directli impact by how much they can 
understand and predict the model’ behavior, a oppos 
to treat it a a black box. second, a system design 
who understand whi their model be make predict be 
certainli good equip to improv it by mean of featur 
engineering, paramet tuning, or even by replac the 
model with a differ one. lastly, even in low stake 
domain such a movi or book recommendations, get a 
rational such a “you will probabl like thi book becaus 
of your interest in russian literature” make the model 
much more use to the users, and more like to be trusted. 
thu there be a crucial need to be abl to explain machin 
learn predictions, i.e. provid user a rational for whi a 
predict be make use textual and visual compon 
of the data, and/or produc counter-factu knowledg of 
what would happen be the compon different. 

the prevail solut to thi explan problem be to 
use so call “interpretable” models, such a decis trees, 
rule (letham et al., 2015; wang & rudin, 2015), addit 
model (caruana et al., 2015), attention-bas network (xu 
et al., 2015), or spars linear model (ustun & rudin, 2015). 
instead of support model that be function black- 
boxes, such a an arbitrari neural network or random forest 
with thousand of trees, these approach use model in 
which there be the possibl of meaning inspect 
model compon directli — e.g. a path in a decis tree, 
a singl rule, or the weight of a specif featur in a linear 
model. As long a the model be accur for the task, and 
us a reason restrict number of intern compon 
(i.e. paths, rules, or features), such approach provid 
extrem use insights. 

An altern approach to interpret in machin 
learn be to be model-agnostic, i.e. to extract post-hoc 
explan by treat the origin model a a black 
box. thi involv learn an interpret model on the 
predict of the black box model (craven & shavlik, 1996; 
baehren et al., 2010), perturb input and see how 
the black box model react (strumbelj & kononenko, 2010; 
kraus et al., 2016), or both (ribeiro et al., 2016). 

91 

ar 
X 

iv 
:1 

60 
6. 

05 
38 

6v 
1 

[ 
st 

at 
.M 

L 
] 

1 
6 

Ju 
n 

20 
16 



model-agnost interpret of machin learn 

In thi posit paper, we argu for separ explan 
from the model (i.e. be model agnostic). the summari 
of our posit be that restrict the space of model to be 
interpret be a constraint that result in less flexibility, 
accuracy, and usability. We develop thi posit with 
examples, while also describ the inher challeng 
in model agnosticism. finally, we review the recently- 
introduc lime approach (ribeiro et al., 2016), and 
discu how it provid mani of the desir characterist 
for model-agnost explanations. 

2. A case for model agnostic 
In thi section, we make a case for model-agnost inter- 
pretability, a oppos to just use interpret models. 

2.1. model flexibl 

for most real-world applications, it be necessari to train 
model that be accur for the task, irrespect of how 
complex or uninterpret the underli mechan may 
be. We can observ thi ideolog manifest with the 
increas commonplac deploy of uninterpret 
deep neural architectur for a wide varieti of tasks. 

interpret model for such task remain unsatisfying; 
such model be inher crippl by the need to be 
understandable, be suscept to the limit “percept 
budget” (miller, 1956) of the users. thi trade-off between 
model flexibl and interpret (freitas, 2014) impli 
one cannot use a model whose behavior be veri complex, yet 
expect human to fulli comprehend it globally. for example, 
for a task such a predict the sentiment of a sentence, 
produc an accur model that be understand seem 
like an unfeas task. the size of the vocabulari alon 
make it imposs for a short set of rules, a decis tree, or 
an addit model to be suffici accurate, not to mention 
more complex word interact such a negation. task 
that involv sensori data, such a audio and images, also 
suffer from the same problem: for a model to be useful, it 
must be suffici flexibl to handl the data complexity. 

In model-agnost interpretability, the model be treat a a 
black box. the separ of interpret from the model 
thu free up the model to be a flexibl a necessari for the 
task, enabl the use of ani machin learn approach - 
including, for example, arbitrari deep neural networks. It 
also allow for the control of the complexity-interpret 
trade-off (see next section), or “fail gracefully” if an 
interpret explan be not possible. 

2.2. explan flexibl 

differ kind of explan meet differ inform 
needs. In some cases, user may onli care about posit 
evid toward a certain predict (e.g. which part of an 

imag be most respons for the prediction), while in other 
instanc know the neg evid may be use 
(e.g. in debug a classifier). yet in other cases, the 
inform need may be of counter-factuals, e.g. how the 
model would behav if certain featur have differ values. 
differ user may also be abl to handl differ kind 
of explanations; a user train in statist may be abl to 
understand a bayesian network, while a linear model be 
more intuit to the layman. even if the explan type 
be kept fixed, user may toler differ granular in 
differ situations. for example, freita (2014) note a case 
where 41 rule be consid overwhelming, and contrast 
it to anoth user who patient analyz 29,050 rules. 

most interpret model are, however, restrict in 
what explan be possible, be it a prototyp (kim 
et al., 2014), a set of rule (letham et al., 2015) or line 
graph (caruana et al., 2015). further, other constraint 
on interpretability, such a granularity, also have to be set 
a priori (e.g. max number of rules). On the other hand, 
by keep the model separ from the explanations, one 
be abl to tailor the explan to the inform need, 
while keep the model fixed. If it be possibl to measur 
how faith the explan be to the origin model, one 
can effect control the trade-off between fidel and 
interpretability, a favor by freita (2014). such ap- 
proach may also be abl to provid multipl explan 
of differ type to the user, perhap automat pick 
the one with the high faithfulness. thus, by be 
model-agnostic, the same model can be explain with 
differ type of explanations, and differ degre of 
interpret for each type of explanation. 

2.3. represent flexibl 

In domain such a images, audio and text, mani of 
the featur use to repres instanc in state-of-the-art 
solut be themselv not interpretable. unsupervis 
featur learn produc represent such a word 
embed (mikolov et al., 2013), or the so-cal deep 
featur (zhou et al., 2014). while an interpret 
model train on such featur be still uninterpretable, 
model-agnost approach can gener explan use 
differ featur than the one use by the underli model. 
thus, even if the model be use word embeddings, the 
explan can be in term of words, for example. 

2.4. lower cost to switch 

switch model be not an uncommon oper in machin 
learn pipelines. If one commit to use an interpret 
model, one be “locked-in” to a particular model and a 
particular kind of explan - even if newer, more 
accur model be developed. even when the switch be 
from one interpret model to another, user may have to 

92 



model-agnost interpret of machin learn 

be re-train in understand the new explanations, and the 
model’ util may decreas due to cognit overhead. In 
contrast, if one us model-agnost explanations, switch 
the underli model for a new one be trivial, while the way 
in which the explan be present be maintained. 

2.5. compar two model 

when deploy machin learn in the real world, a 
system design often have to decid between one or more 
contenders, and an incumb model. thi comparison be 
hard to do if ani of the system be use interpret 
models, while other be not. further, even if all of the 
model be interpretable, it may still be difficult to compar 
the insight gain from each if the underli explan 
be differ in their represent - for exampl compar 
a rule-bas model with a tree-bas model. It be also not 
clear what to do if one of the contend be less accur 
but more interpretable, or vice versa. with model-agnost 
explanations, the model be compar can be explain 
use the same techniqu and representations. 

3. challeng for model-agnost 
explan 

while we have make a case for model agnosticism, thi 
approach be not without it challenges. for example, 
get a global understand of the model may be hard 
if the model be veri complex, due to the trade-off between 
flexibl and interpretability. To make matter worse, local 
explan may be inconsist with one another, sinc a 
flexibl model may use a certain featur in differ way 
depend on the other features. In ribeiro et al. (2016) 
we explain text model by select a small number 
of repres and non-redund individu predict 
explan obtain via submodular optimization, similar 
in spirit to show prototyp (kim et al., 2014). however, 
it be unclear on how to extend thi approach to domain such 
a imag or tabular data, where the data itself be not sparse. 

In some domains, exact explan may be requir (e.g. 
for legal or ethic reasons), and use a black-box may 
be unaccept (or even illegal). interpret model 
may also be more desir when interpret be much 
more import than accuracy, or when interpret model 
train on a small number of care engin featur 
be a accur a black-box models. 

anoth challeng for model-agnost explan be to be 
actionable. use a white box make it easi to incorpor 
user feedback in system like ibcm (kim et al., 2015), or 
inject logic into matrix factor (rocktaschel et al., 
2015). featur label (druck et al., 2008) or annot 
rational (zaidan & eisner, 2008) be other form of 
feedback that should be support for explanations. A basic 

form of featur engin (remov bad features) via 
explan have be show to be effect (ribeiro et al., 
2016), but incorpor more power form of feedback 
from the user be still a challeng research direction, in 
particular while remain model-agnostic. 

4. local interpret model-agnost 
explan (lime) 

We now briefli review lime (ribeiro et al., 2016), and dis- 
cuss how it maintain model-agnosticism, while address 
some of the challeng that be describ in the previou 
section. We denot x ∈ Rd a the origin represent 
of an instanc be explained, and we use x′ ∈ rd′ to 
denot a vector for it interpret representation. As 
exemplifi before, x may be a featur vector contain 
word embeddings, with x′ be the bag of words. 

lime’ goal be to identifi an interpret model over 
the interpret represent that be local faith to 
the classifier. even though an interpret model may 
not be abl to approxim the black box model globally, 
approxim it in the vicin of an individu instanc may 
be feasible. formally, the explan model be g : rd′ → 
R, g ∈ G, where G be a class of potenti interpret 
models, such a linear models, decis trees, or rule lists, 
i.e. give a model g ∈ G, we can present it to the user 
a an explan with visual or textual artifacts. As note 
before, not everi g ∈ G be simpl enough to be interpret 
- thu we let ω(g) be a measur of complex (a oppos to 
interpretability) of g, which may be either a soft constraint 
(e.g. the depth of a tree, or the number of non-zero in a 
linear model) or a hard constraint (e.g. ∞ if the depth or the 
number of non-zero be abov a certain threshold). 

let the model be explain be f : Rd → R, e.g. in 
classif f(x) be the probabl that x belong to a 
certain class. We further use πx(z) a a proxim measur 
between an instanc z to x, so a to defin local around x. 
finally, let l(f, g,πx) be a measur of how unfaith g be 
in approxim f in the local defin by πx. In order 
to ensur both interpret and local fidelity, we must 
minim l(f, g,πx) while have ω(g) be low enough to 
be interpret by humans. the explan ξ(x) produc 
by lime be obtain by solving: 

ξ(x) = argmin 
g∈g 

l(f, g,πx) + ω(g) (1) 

thi formul can be use with differ explan 
famili G, fidel function L, and complex measur 
Ω. We estim L by gener perturb sampl around 
x, make predict with the black box model f and 
weight them accord to πx. the intuit for thi 
be present in figur 1, where a global complex model be 
explain use a locally-faith linear explanation. 

93 



model-agnost interpret of machin learn 

figur 1. toy exampl to present intuit for lime. the black- 
box model’ complex decis function f (unknown to lime) 
be repres by the blue/pink background. the bright bold red 
cross be the instanc be explained. lime sampl instances, 
get predict use f , and weigh them by the proxim to the 
instanc be explain (repres here by size). the dash 
line be the explan that be local (but not globally) faithful. 

discuss 

some approach be model agnost by approxim the 
black box model by an interpret one global (craven 
& shavlik, 1996; baehren et al., 2010; sanchez et al., 
2015). global explanation, however, be often either not 
interpretable, or too simplist to repres the origin 
model. lime’ focu on explain individu predict 
allow more accur explan while retain model 
flexibility. for example, it be easi to explain whi sentenc 
such a “thi be not bad.” have a posit sentiment, even if 
we be not abl to explain the complet sentiment model. 

for explan flexibility, the practition have complet 
control over G and ω(g); in ribeiro et al. (2016), for exam- 
ple, we use veri spars linear models. thi represent 
be simpl enough for non-expert mechan turker to 
perform model select and featur engin effect 
for complex, uninterpret models. furthermore, sinc 
lime estim the local fidel through L, we can directli 
control the interpret of the explan (e.g. use a 
mani word a need to maintain faithfulness) or whether 
to onli display interpret explan when they be 
accur to the black box model. lime also support 
explor multipl explan famili G simultaneously, 
and pick the one with high faithfulness. 

represent flexibl be built into lime, with the dis- 
tinction between origin x and interpret represent 
x′. In ribeiro et al. (2016), we explain model train 
on on word embed by use word a interpret 
representation, and a neural network train on raw pixel 
by use contigu super-pixel a x′. 

We demonstr the small switch cost of lime by 
explain a wide varieti of model (random forests, svms, 
neural networks, linear models, and near neighbors) 
use the same type of explanations. We also demonstr 
lime’ util for model comparison by enabl non-expert 

(a) logist regress train on unigram 

(b) lstm train on sentenc embeddings. 

figur 2. explain sentiment predict for the sentenc “thi 
be not bad.”, use differ model and represent 

mechan turk user to select which of two compet 
model would gener good use the explanations. 

As a final illustration, we explain the predict two 
sentiment analysi classifi on the sentenc “thi be not 
bad.”, use the class of linear model a G. the classifi 
vari wildli in complex and underli represent 
- one be a logist regress train on unigrams, while 
the other an lstm neural network train on sentenc 
embed (wiet et al., 2015). explanations, give in 
term of word (and their associ weight in a bar chart) 
in figur 2, demonstr that complet differ classifi 
can be describ in a unified, interpret manner. In figur 
2(b), the explan assign posit weight to both “not” 
and “bad”, a onli the conjunct be respons for the 
lstm’ posit predict (even though interact be 
not model explicitly). 

5. conclus 
although interpret model provid crucial insight into 
whi predict be made, they impos restrict on the 
model, represent (features), and the expertis of the 
users. We argu that model-agnost explan system 
provid a gener framework for interpret that allow 
for flexibl in the choic of models, representations, and 
the user expertise. We outlin a number of challeng 
that need to be address for model-agnost approaches; 
some of which be address by the recent introduc 
lime (ribeiro et al., 2016), while other be left a futur 
work. We thu conclud that model-agnost interpret 
be a key compon in make machin learn more 
trustworthi - and ultimately, more useful. 

94 



model-agnost interpret of machin learn 

acknowledg 
thi work be support in part by onr award #w911nf- 
13-1-0246 and #n00014-13-1-0023, and in part by ter- 
raswarm, one of six center of starnet, a semiconductor 
research corpor program sponsor by marco and 
darpa. 

refer 
baehrens, david, schroeter, timon, harmeling, stefan, 

kawanabe, motoaki, hansen, katja, and müller, klaus- 
robert. how to explain individu classif decisions. 
journal of machin learn research, 11, 2010. 

caruana, rich, lou, yin, gehrke, johannes, koch, paul, 
sturm, marc, and elhadad, noemie. intellig model 
for healthcare: predict pneumonia risk and hospit 
30-day readmission. In knowledg discoveri and data 
mine (kdd), 2015. 

craven, mark W and shavlik, jude W. extract tree- 
structur represent of train networks. advanc 
in neural inform process systems, pp. 24–30, 
1996. 

druck, gregory, mann, gideon, and mccallum, andrew. 
learn from label featur use gener 
expect criteria. In acm sigir confer on 
research and develop in inform retrieval, pp. 
595–602. acm, 2008. 

freitas, alex A. comprehens classif models: 
A posit paper. sigkdd explor. newsl., 15(1):1–10, 
march 2014. issn 1931-0145. 

kim, been, rudin, cynthia, and shah, juli A. the bayesian 
case model: A gener approach for case-bas 
reason and prototyp classification. In ghahramani, z., 
welling, m., cortes, c., lawrence, n.d., and weinberger, 
k.q. (eds.), advanc in neural inform process 
system 27, pp. 1952–1960. curran associates, inc., 
2014. 

kim, been, glassman, elena, johnson, brittney, and shah, 
julie. ibcm: interact bayesian case model empow 
human via intuit interaction. 2015. 

krause, josua, perer, adam, and ng, kenney. interact 
with predictions: visual inspect of black-box machin 
learn models. 2016. 

letham, benjamin, rudin, cynthia, mccormick, tyler h., 
and madigan, david. interpret classifi use rule 
and bayesian analysis: build a good stroke predict 
model. annal of appli statistics, 2015. 

mikolov, tomas, sutskever, ilya, chen, kai, corrado, 
greg S, and dean, jeff. distribut represent of 
word and phrase and their compositionality. In neural 
inform process system (nips). 2013. 

miller, george. the magic number seven, plu or 
minu two: some limit on our capac for process 
information, 1956. 

ribeiro, marco tulio, singh, sameer, and guestrin, carlos. 
“whi should I trust you?”: explain the predict of 
ani classifier. In knowledg discoveri and data mine 
(kdd), 2016. 

rocktaschel, tim, singh, sameer, and riedel, sebastian. 
inject logic background knowledg into embed 
for relat extraction. In annual confer of the north 
american chapter of the associ for comput 
linguist (naacl), 2015. 

sanchez, ivan, rocktaschel, tim, riedel, sebastian, 
and singh, sameer. toward extract faith and 
descript represent of latent variabl models. In 
aaai spring syposium on knowledg represent 
and reason (krr): integr symbol and neural 
approaches, 2015. 

strumbelj, erik and kononenko, igor. An effici 
explan of individu classif use game 
theory. journal of machin learn research, 11, 2010. 

ustun, berk and rudin, cynthia. superspars linear integ 
model for optim medic score systems. machin 
learning, 2015. 

wang, fulton and rudin, cynthia. fall rule lists. In 
artifici intellig and statist (aistats), 2015. 

wieting, john, bansal, mohit, gimpel, kevin, and 
livescu, karen. toward univers paraphrast sentenc 
embeddings. corr, abs/1511.08198, 2015. 

xu, kelvin, ba, jimmy, kiros, ryan, cho, kyunghyun, 
courville, aaron, salakhutdinov, ruslan, zemel, richard, 
and bengio, yoshua. show, attend and tell: neural imag 
caption gener with visual attention. In intern 
confer on machin learn (icml), 2015. 

zaidan, omar F. and eisner, jason. model annotators: A 
gener approach to learn from annot rationales. 
In emnlp 2008, pp. 31–40, octob 2008. 

zhou, bolei, lapedriza, agata, xiao, jianxiong, torralba, 
antonio, and oliva, aude. learn deep featur for 
scene recognit use place database. In ghahramani, 
z., welling, m., cortes, c., lawrence, N. d., and 
weinberger, K. Q. (eds.), advanc in neural inform 
process system 27, pp. 487–495. curran associates, 
inc., 2014. 

95 


