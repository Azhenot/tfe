






















































deep video portrait 


deep video portrait 

hyeongwoo kim,max planck institut for informatics, germani 
pablo garrido, technicolor, franc 
ayush tewari and weipeng xu,max planck institut for informatics, germani 
justu thi and matthia niessner, technic univers of munich, germani 
patrick pérez, technicolor, franc 
christian richardt, univers of bath, unit kingdom 
michael zollhöfer, stanford university, unit state of america 
christian theobalt,max planck institut for informatics, germani 

input output input output 

fig. 1. unlik current face reenact approach that onli modifi the express of a target actor in a video, our novel deep video portrait approach enabl 
full control over the target by transfer the rigid head pose, facial express and eye motion with a high level of photorealism. 

We present a novel approach that enabl photo-realist re-anim of 

portrait video use onli an input video. In contrast to exist approach 

that be restrict to manipul of facial express only, we be the irst 

to transfer the full 3D head position, head rotation, face expression, eye gaze, 

and eye blink from a sourc actor to a portrait video of a target actor. the 

core of our approach be a gener neural network with a novel space-tim 

architecture. the network take a input synthet render of a parametr 

face model, base on which it predict photo-realist video frame for a 

give target actor. the realism in thi rendering-to-video transfer be achiev 

by care adversari training, and a a result, we can creat modii target 

video that mimic the behavior of the synthetically-cr input. In order 

to enabl source-to-target video re-animation, we render a synthet target 

video with the reconstruct head anim paramet from a sourc 

video, and feed it into the train network ś thu take full control of the 

authors’ addresses: hyeongwoo kim, max planck institut for informatics, campu 
e1.4, saarbrücken, 66123, germany, hyeongwoo.kim@mpi-inf.mpg.de; pablo gar- 
rido, technicolor, 975 avenu de champ blancs, cesson-sévigné, 35576, france, 
pablo.garrido.adrian@gmail.com; ayush tewari, atewari@mpi-inf.mpg.de; weipeng 
xu, wxu@mpi-inf.mpg.de, max planck institut for informatics, campu e1.4, saar- 
brücken, 66123, germany; justu thies, justus.thies@tum.de; matthia nießner, 
niessner@tum.de, technic univers of munich, boltzmannstraß 3, garching, 85748, 
germany; patrick pérez, technicolor, 975 avenu de champ blancs, cesson-sévigné, 
35576, france, patrick.perez@technicolor.com; christian richardt, univers of bath, 
claverton down, bath, ba2 7ay, unit kingdom, christian@richardt.name; michael 
zollhöfer, stanford university, 353 serra mall, stanford, ca, 94305, unit state of 
america, zollhoefer@cs.stanford.edu; christian theobalt, max planck institut for 
informatics, campu e1.4, saarbrücken, 66123, germany, theobalt@mpi-inf.mpg.de. 

© 2018 associ for comput machinery. 
thi be the author’ version of the work. It be post here for your person use. not for 
redistribution. the deinit version of record be publish in acm transact on 
graphics, https://doi.org/10.1145/3197517.3201283. 

target. with the abil to freeli recombin sourc and target parameters, 

we be abl to demonstr a larg varieti of video rewrit applic 

without explicitli model hair, bodi or background. for instance, we can 

reenact the full head use interact user-control editing, and realiz 

high-idel visual dubbing. To demonstr the high qualiti of our output, 

we conduct an extens seri of experi and evaluations, where for 

instanc a user studi show that our video edit be hard to detect. 

cc concepts: · comput methodolog → comput graphics; 

neural networks; appear and textur representations; animation; ren- 

dering; 

addit key word and phrases: facial reenactment, video portraits, 

dubbing, deep learning, condit gan, rendering-to-video translat 

acm refer format: 

hyeongwoo kim, pablo garrido, ayush tewari, weipeng xu, justu thies, 

matthia nießner, patrick pérez, christian richardt, michael zollhöfer, 

and christian theobalt. 2018. deep video portraits. acm trans. graph. 37, 4, 

articl 163 (august 2018), 14 pages. https://doi.org/10.1145/3197517.3201283 

1 introduct 

synthes and edit video portraits, i.e., video frame to show 

a person’ head and upper body, be an import problem in com- 

puter graphics, with applic in video edit and movi post- 

production, visual efects, visual dubbing, virtual reality, and telep- 

resence, among others. In thi paper, we address the problem of 

synthes a photo-realist video portrait of a target actor that 

mimic the action of a sourc actor, where sourc and target can be 

difer subjects. more speciically, our approach enabl a sourc 

actor to take full control of the rigid head pose, face express and 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 

ar 
X 

iv 
:1 

80 
5. 

11 
71 

4v 
1 

[ 
c 

.C 
V 

] 
2 

9 
M 

ay 
2 

01 
8 



163:2 • H. kim, P. garrido, A. tewari, W. xu, J. thies, M. nießner, P. pérez, C. richardt, M. zollhöfer, and C. theobalt 

eye motion of the target actor; even face ident can be modii to 

some extent. all of these dimens can be manipul togeth or 

independently. full target frames, includ the entir head and hair, 

but also a realist upper bodi and scene background compli 

with the modii head, be automat synthesized. 

recently, mani method have be propos for face-interior 

reenact [liu et al. 2001; olszewski et al. 2017; suwajanakorn 

et al. 2017; thi et al. 2015, 2016; vlasic et al. 2005]. here, onli 

the face express can be modii realistically, but not the full 

3D head pose, includ a consist upper bodi and a consist 

chang background. mani of these method it a parametr 3D 

face model to rgb(-d) video [thi et al. 2015, 2016; vlasic et al. 

2005], and re-rend the modii model a a blend overlay over 

the target video for reenactment, even in real time [thi et al. 

2015, 2016]. synthes a complet portrait video under full 3D 

head control be much more challenging. averbuch-elor et al. [2017] 

enabl mild head pose chang driven by a sourc actor base on 

imag warping. they gener reactiv dynam proil pictur 

from a static target portrait photo, but not fulli reenact videos. 

also, larg chang in head pose caus artifact (see section 7.3), 

the target gaze cannot be controlled, and the ident of the target 

person be not fulli preserv (mouth appear be copi from the 

sourc actor). 

performance-driven 3D head anim method [cao et al. 2015, 

2014a, 2016; Hu et al. 2017; ichim et al. 2015; Li et al. 2015; olszewski 

et al. 2016; weis et al. 2011] be relat to our work, but have 

orthogon methodolog and applic goals. they typic drive 

the full head pose of styliz 3D CG avatar base on visual sourc 

actor input, e.g., for game or styliz VR environments. recently, 

cao et al. [2016] propos image-bas 3D avatar with dynam 

textur base on a real-tim face tracker. however, their goal be 

full 3D anim head control and rendering, often intent in 

a styliz rather than a photo-realist fashion. 

We take a difer approach that directli gener entir photo- 

realist video portrait in front of gener static background under 

full control of a target’ head pose, facial expression, and eye mo- 

tion. We formul video portrait synthesi and reenact a 

a rendering-to-video translat task. input to our algorithm be 

synthet render of onli the coars and fully-control 3D 

face interior model of a target actor and separ render eye 

gaze images, which can be robustli and eicient obtain via 

a state-of-the-art model-bas reconstruct technique. the in- 

put be automat translat into full-fram photo-realist video 

output show the entir upper bodi and background. sinc we 

onli track the face, we cannot activ control the motion of the 

torso or hair, or control the background, but our rendering-to-video 

translat network be abl to implicitli synthes a plausibl bodi 

and background (includ some shadow and relections) for a 

give head pose. thi translat problem be tackl use a novel 

space-tim encoderśdecod deep neural network, which be train 

in an adversari manner. 

At the core of our approach be a condit gener adversari 

network (cgan) [isola et al. 2017], which be speciic tailor 

to video portrait synthesis. for tempor stability, we use a novel 

space-tim network architectur that take a input short sequenc 

of condit input frame of head and eye gaze in a slide 

window manner to synthes each target video frame. our target 

and scene-speci network onli requir a few minut of portrait 

video footag of a person for training. To the best of our knowledge, 

our approach be the irst to synthes full photo-realist video 

portrait of a target person’ upper body, includ realist cloth 

and hair, and consist scene background, under full 3D control of 

the target’ head. To summarize, we make the follow technic 

contributions: 

• A rendering-to-video translat network that transform 

coars face model render into full photo-realist portrait 

video output. 

• A novel space-tim encod a condit input for tempo- 

ralli coher video synthesi that repres face geometry, 

relectance, and motion a well a eye gaze and eye blinks. 

• A comprehens evalu on sever applic to demon- 

strate the lexibl and efect of our approach. 

We demonstr the potenti and high qualiti of our method in 

mani intrigu applications, rang from face reenact and 

visual dub for foreign languag movi to user-guid interac- 

tive edit of portrait video for movi postproduction. A compre- 

hensiv comparison to state-of-the-art method and a user studi 

conirm the high idel of our results. 

2 relat work 

We discu relat optim and learning-bas method that 

aim at reconstructing, anim and re-writ face in imag 

and videos, and review relev image-to-imag translat work. 

for a comprehens overview of current method we refer to a 

recent state-of-the-art report on monocular 3D face reconstruction, 

track and applic [zollhöfer et al. 2018]. 

monocular face reconstruction. face reconstruct method aim 

to reconstruct 3D face model of shape and appear from visual 

data. optimization-bas method it a 3D templat model, mainli 

the inner face region, to singl imag [blanz et al. 2004; blanz 

and vetter 1999], unstructur imag collect [kemelmacher- 

shlizerman 2013; kemelmacher-shlizerman et al. 2011; roth et al. 

2017] or video [cao et al. 2014b; fyfe et al. 2014; garrido et al. 2016; 

ichim et al. 2015; shi et al. 2014; suwajanakorn et al. 2014; thi et al. 

2016; Wu et al. 2016]. recently, booth et al. [2018] propos a large- 

scale parametr face model construct from almost ten thousand 

3D scans. learning-bas approach leverag a larg corpu of 

imag or imag patch to learn a regressor for predict either 

3D face shape and appear [richardson et al. 2016; tewari et al. 

2017; tran et al. 2017], ine-scal skin detail [cao et al. 2015], or 

both [richardson et al. 2017; sela et al. 2017]. deep neural network 

have be show to be quit robust for infer the coars 3D 

facial shape and appear of the inner face region, even when 

train on synthet data [richardson et al. 2016]. tewari et al. 

[2017] show that encoderśdecod architectur can be train 

fulli unsupervis on in-the-wild imag by integr physic 

imag format into the network. richardson et al. [2017] train 

an end-to-end regressor to recov facial geometri at a coars and 

ine-scal level. sela et al. [2017] use an encoderśdecod network 

to infer a detail depth imag and a dens correspond map, 

which serv a a basi for non-rigidli deform a templat mesh. 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 



deep video portrait • 163:3 

fig. 2. deep video portrait enabl a sourc actor to fulli control a target video portrait. first, a low-dimension parametr represent (let) of both 
video be obtain use monocular face reconstruction. the head pose, express and eye gaze can now be transfer in paramet space (middle). We do not 
focu on the modif of the ident and scene illumin (hatch background), sinc we be interest in reenactment. finally, we render condit 
input imag that be convert to a photo-realist video portrait of the target actor (right). obama video courtesi of the white hous (public domain). 

still, none of these method creat a fulli gener model for the 

entir head, hair, mouth interior, and eye gaze, like we do. 

video-bas facial reenactment. facial reenact method re- 

write the face content of a target actor in a video or imag by trans- 

fer facial express from a sourc actor. facial express 

be commonli transfer via dens motion ield [averbuch-elor 

et al. 2017; liu et al. 2001; suwajanakorn et al. 2015], paramet 

[thi et al. 2016, 2018; vlasic et al. 2005], or by warp candid 

frame that be select base on the facial motion [dale et al. 2011], 

appear metric [kemelmacher-shlizerman et al. 2010] or both 

[garrido et al. 2014; Li et al. 2014]. the method describ abov 

irst reconstruct and track the sourc and target faces, which be 

repres a a set of spars 2D landmark or dens 3D models. 

most approach onli modifi the inner region of the face and thu 

be mainli intend for alter facial expressions, but they do not 

take full control of a video portrait in term of rigid head pose, facial 

expression, and eye gaze. recently, wood et al. [2018] propos an 

approach for eye gaze redirect base on a it parametr eye 

model. their approach onli provid control over the eye region. 

one notabl except to pure facial reenact be averbuch- 

elor et al.’ approach [2017], which enabl the reenact of a 

portrait imag and allow for slight chang in head pose via imag 

warp [fri et al. 2016]. sinc thi approach be base on a singl 

target image, it copi the mouth interior from the sourc to the 

target, thu preserv the target’ ident onli partially. We take 

advantag of learn from a target video to allow for larg chang 

in head pose, facial reenactment, and joint control of the eye gaze. 

visual dubbing. visual dub be a particular instanc of face 

reenact that aim to alter the mouth motion of the target actor 

to match a new audio track, commonli spoken in a foreign languag 

by a dub actor. here, we can ind speech-driven [bregler et al. 

1997; chang and ezzat 2005; ezzat et al. 2002; liu and ostermann 

2011; suwajanakorn et al. 2017] or performance-driven [garrido 

et al. 2015; thi et al. 2016] techniques. speech-driven dub tech- 

niqu learn a person-speci phoneme-to-visem map from a 

train sequenc of the actor. these method produc accur lip 

sync with visual impercept artifacts, a recent demonstr 

by suwajanakorn et al. [2017]. however, they cannot directli con- 

trol the target’ facial expressions. performance-driven techniqu 

overcom thi limit by transfer semantically-meaning 

motion paramet and re-rend the target model with photo- 

realist relect [thi et al. 2016], and ine-scal detail [garrido 

et al. 2015, 2016]. these approach gener better, but do not 

edit the head pose and still struggl to synthes photo-realist 

mouth deformations. In contrast, our approach learn to synthes 

photo-realist facial motion and action from coars renderings, 

thu enabl the synthesi of express and joint modiic of 

the head pose, with consist bodi and background. 

image-to-imag translation. approach use condit gan 

[mirza and osindero 2014], such a isola et al.’ łpix2pixž [2017], 

have show impress result on image-to-imag translat task 

which convert between imag of two difer domains, such a 

map and satellit photos. these combin encoderśdecod architec- 

ture [hinton and salakhutdinov 2006], often with skip-connect 

[ronneberg et al. 2015], with adversari loss function [goodfel- 

low et al. 2014; radford et al. 2016]. chen and koltun [2017] be 

the irst to demonstr high-resolut result with 2megapixel 

resolution, use cascad reinement network without adversari 

training. the late trend show that it be even possibl to train high- 

resolut gan [karra et al. 2018] and condit gan [wang 

et al. 2018] at similar resolutions. however, the main challeng be 

the requir for pair train data, a correspond imag 

pair be often not available. thi problem be tackl by cyclegan 

[zhu et al. 2017], dualgan [yi et al. 2017], and unit [liu et al. 

2017] ś multipl concurr unsupervis image-to-imag trans- 

lation techniqu that onli requir two set of unpair train 

samples. these techniqu have captur the imagin of mani 

peopl by translat between photograph and paintings, hors 

and zebras, face photo and depth a well a correspond map 

[sela et al. 2017], and translat from face photo to cartoon draw- 

ing [taigman et al. 2017]. ganin et al. [2016] learn photo-realist 

gaze manipul in images. olszewski et al. [2017] synthes a 

realist inner face texture, but cannot gener a fulli control 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 



163:4 • H. kim, P. garrido, A. tewari, W. xu, J. thies, M. nießner, P. pérez, C. richardt, M. zollhöfer, and C. theobalt 

output video, includ person-speci hair. lassner et al. [2017] 

propos a gener model to synthes peopl in clothing, and 

Ma et al. [2017] gener new imag of person in arbitrari pose 

use image-to-imag translation. In contrast, our approach enabl 

the synthesi of temporally-coher video portrait that follow the 

anim of a sourc actor in term of head pose, facial express 

and eye gaze. 

3 overview 

our deep video portrait approach provid full control of the head 

of a target actor by transfer the rigid head pose, facial expres- 

sion, and eye motion of a sourc actor, while preserv the target’ 

ident and appearance. full target video frame be synthesized, 

includ consist upper bodi posture, hair and background. first, 

we track the sourc and target actor use a state-of-the-art monoc- 

ular face reconstruct approach that us a parametr face and 

illumin model (see section 4). the result sequenc of low- 

dimension paramet vector repres the actor’ identity, head 

pose, expression, eye gaze, and the scene light for everi video 

frame (figur 2, left). thi allow u to transfer the head pose, ex- 

pression, and/or eye gaze paramet from the sourc to the target, 

a desired. In the next step (figur 2, middle), we gener new 

synthet render of the target actor base on the modii pa- 

ramet (see section 5). In addit to a normal color rendering, we 

also render correspond map and eye gaze images. these ren- 

dere serv a condit input to our novel rendering-to-video 

translat network (see section 6), which be train to convert the 

synthet input into photo-realist output (see figur 2, right). for 

tempor coher results, our network work on space-tim vol- 

ume of condit inputs. To process a complet video, we input 

the condit space-tim volum in a slide window fashion, 

and assembl the inal video from the output frames. We evalu 

our approach (see section 7) and show it potenti on sever video 

rewrit applications, such a full-head reenactment, gaze redirection, 

video dubbing, and interact parameter-bas video control. 

4 monocular face reconstruct 

We employ a state-of-the-art dens face reconstruct approach 

that it a parametr model of face and illumin to each video 

frame. It obtain a meaning parametr face represent for 

the sourc Vs = {i 
f 
| f = 1, . . . ,n } and target V 

t 
= {it 

f 
| f = 

1, . . . ,nt } video sequence, where Ns and Nt denot the total num- 

ber of sourc and target frames, respectively. let P• = {p• 
f 
| f = 

1, . . . ,n•} be the correspond paramet sequenc that fulli de- 

scribe the sourc or target facial performance. the set of recon- 

struct paramet encod the rigid head pose (rotat R• ∈so(3) 

and translat t• ∈r3), facial ident coeicient α • ∈rnα (ge- 

ometry, Nα = 80) and β 
• ∈rnβ (relectance, Nβ = 80), express 

coeicient δ• ∈rnδ (nδ =64), gaze direct for both eye e 
• ∈r4, 

and spheric harmon illumin coeicientsγ• ∈ r27. overall, 

our monocular face tracker reconstruct Np =261 paramet per 

video frame. In the following, we provid more detail on the face 

track algorithm a well a the parametr face representation. 

parametr face representation. We repres the space of facial 

ident base on a parametr head model [blanz and vetter 1999], 

and the space of facial express via an ain model. mathemati- 

cally, we model geometri variat through an ain model v∈r3n 

that stack per-vertex deform of the underli templat mesh 

with N vertices, a follows: 

v(α ,δ) = ageo + 

Nα 
∑ 

k=1 

αkb 
geo 

k 
+ 

Nδ 
∑ 

k=1 

δkb 
exp 
k 

. (1) 

difus skin relect be model similarli by a second ain 

model r∈r3n that stack the difus per-vertex albedo: 

r(β) = aref + 

Nβ 
∑ 

k=1 

βkb 
ref 
k 

. (2) 

the vector ageo ∈ R 
3N and aref ∈ R 

3N store the averag facial 

geometri and correspond skin relectance, respectively. the 

geometri basi {b 
geo 

k 
} 
Nα 
k=1 

have be comput by appli princip 

compon analysi (pca) to 200 high-qual face scan [blanz 

and vetter 1999]. the relect basi {bref 
k 

} 
Nβ 
k=1 

have be obtain 

in the same manner. for dimension reduction, the express 

basi {b 
exp 
k 

} 
Nδ 
k=1 

have be comput use pca, start from the 

blendshap of alexand et al. [2010] and cao et al. [2014b]. their 

blendshap have be transfer to the topolog of blanz and 

vetter [1999] use deform transfer [sumner and popović 2004]. 

imag format model. To render synthet head images, we 

assum a full perspect camera that map model-spac 3D point 

v via camera space v̂∈r3 to 2D point p=π(v̂) ∈r2 on the imag 

plane. the perspect map Π contain the multipl with 

the camera intrins and the perspect division. We assum a 

ixe and ident camera for all scenes, i.e., world and camera space 

be the same, and the face model account for all the scene motion. 

base on a distant illumin assumption, we use the spheric 

harmon (sh) basi function Yb : R 
3 → R to approxim the 

incom radianc B from the environment: 

b(ri ,ni ,γ ) = ri · 

B2 
∑ 

b=1 

γbyb (ni ). (3) 

here, B be the number of spheric harmon bands,γb ∈R 
3 be the 

SH coeicients, and ri and ni be the relect and unit normal 

vector of the i-th vertex, respectively. for difus materials, an av- 

erag approxim error below 1 percent be achiev with onli 

B = 3 bands, independ of the illumin [ramamoorthi and 

hanrahan 2001], sinc the incid radianc be in gener a smooth 

function. thi result in b2=9 paramet per color channel. 

dens face reconstruction. We employ a dens data-parallel face 

reconstruct approach to eicient comput the paramet P• 

for both sourc and target videos. face reconstruct be base on an 

analysis-by-synthesi approach that maxim photo-consist 

between a synthet render of the model and the input. the 

reconstruct energi combin term for dens photo-consistency, 

landmark align and statist regularization: 

e(x) = wphotoephoto(x) +wlandeland(x) +wregereg(x), (4) 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 



deep video portrait • 163:5 

withx= {r•, t•,α •, β•,δ•,γ•}. thi enabl the robust reconstruc- 

tion of ident (geometri and skin relectance), facial expression, 

and scene illumination. We use 66 automat detect facial 

landmark of the true vision solut tracker1, which be a commer- 

cial implement of saragih et al. [2011], to dein the spars 

align term eland. similar to thi et al. [2016], we use a robust 

ℓ1-norm for dens photometr align ephoto. the regular 

ereg enforc statist plausibl paramet valu base on the 

assumpt of normal distribut data. the eye gaze estim e• 

be directli obtain from the landmark tracker. the ident be onli 

estim in the irst frame and be kept constant afterwards. all 

other paramet be estim everi frame. for more detail on 

the energi formulation, we refer to garrido et al. [2016] and thi 

et al. [2016]. We use a data-parallel implement of iter 

re-weight least squar (irls), similar to thi et al. [2016], to 

ind the optim set of parameters. one difer to their work be 

that we comput and explicitli store the jacobian J and the residu 

vector F to global memori base on a data-parallel strategi that 

launch one thread per matrix/vector element. afterwards, a data- 

parallel matrixśmatrix/matrixśvector multipl comput the 

right- and left-hand side of the normal equat that have to be 

solv in each irl step. the result small linear system (97×97 

in track mode, 6 dof rigid pose, 64 express paramet and 27 

SH coeicients) be solv on the cpu use choleski factor 

in each irl step. the reconstruct of a singl frame take 670m 

(all parameters) and 250m (without identity, track mode). thi 

allow the eicient gener of the train corpu that be requir 

by our space-tim rendering-to-video translat network (see sec- 

tion 6). contrari to garrido et al. [2016] and thi et al. [2016], our 

model featur dimens to model eyelid closure, so eyelid motion 

be captur well. 

5 synthet condit input 

use the method from section 4, we reconstruct the face in each 

frame of the sourc and unmodii target video. next, we obtain the 

modii paramet vector for everi frame of the target sequence, 

e.g., for full-head reenactment, we modifi the rigid head pose, ex- 

pression and eye gaze of the target actor. all paramet be copi 

in a rel manner from the sourc to the target, i.e., with respect 

to a neutral refer frame. then we render synthet condit 

imag of the target actor’ face model under the modii parame- 

ter use hardwar rasterization. for high tempor coherence, 

our rendering-to-video translat network take a space-tim vol- 

ume of condit imag {cf −o |o=0, . . . , 10} a input, with f 

be the index of the current frame. We use a tempor window of 

size Nw =11, with the current frame be at it end. thi provid 

the network a histori of the earli motions. 

for each frame Cf −o of the window, we gener three difer 

condit inputs: a color rendering, a correspond image, and 

an eye gaze imag (see figur 3). the color render show the 

modii target actor model under the estim target illumination, 

while keep the target ident (geometri and skin relectance) 

ixed. thi imag provid a good start point for the follow 

rendering-to-video translation, sinc in the face region onli the 

1http://truevisionsolutions.net 

diffus render correspond eye and gaze map 

fig. 3. the synthet input use for condit our rendering-to-video 
translat network: (1) color face render under target illumination, 
(2) correspond image, and (3) the eye gaze image. 

delta to a real imag have to be learned. In addit to thi color input, 

we also provid a correspond imag encod the index of the 

parametr face model’ vertex that project into each pixel. To thi 

end, we textur the head model with a constant uniqu gradient 

texturemap, and render it. finally, we also provid an eye gaze imag 

that sole contain the white region of both eye and the locat 

of the pupil a blue circles. thi imag provid inform about 

the eye gaze direct and blink to the network. 

We stack all Nw condit input of a time window in a 3D 

tensor X of sizew ×H × 9nw (3 images, with 3 channel each), to 

obtain the input to our rendering-to-video translat network. To 

process the complet video, we feed the condit space-tim 

volum in a slide window fashion. the inal gener photo- 

realist video output be assembl directli from the output frames. 

6 rendering-to-video translat 

the gener condit space-tim video tensor be the input to 

our rendering-to-video translat network. the network learn to 

convert the synthet input into full frame of a photo-realist target 

video, in which the target actor now mimic the head motion, facial 

express and eye gaze of the synthet input. the network learn to 

synthes the entir actor in the foreground, i.e., the face for which 

condit input exists, but also all other part of the actor, such a 

hair and body, so that they compli with the target head pose. It also 

synthes the appropri modii and illed-in background, 

includ even some consist light efect between foreground 

and background. the network be train for a speciic target actor 

and a speciic static, but otherwis gener scene background. our 

rendering-to-video translat network follow an encoderśdecod 

architectur and be train in an adversari manner base on a 

discrimin that be jointli trained. In the following, we explain 

the network architectures, the use loss function and the train 

procedur in detail. 

network architecture. We show the architectur of our rendering- 

to-video translat network in figur 4. our condit gener 

adversari network consist of a space-tim transform network 

T and a discrimin D. the transform network T take the 

W × H × 9nw space-tim tensor X a input and output a photo- 

real imag t(x) of the target actor. the tempor input enabl the 

network to take the histori of motion into account by inspect 

previou condit images. the tempor axi of the input tensor 

be align along the network channels, i.e., the convolut in the 

irst layer have 9nw channels. note, we store all imag data in 

normal [−1,+1]-space, i.e, black be map to [−1,−1,−1]⊤ and 

white be map to [+1,+1,+1]⊤. 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 



163:6 • H. kim, P. garrido, A. tewari, W. xu, J. thies, M. nießner, P. pérez, C. richardt, M. zollhöfer, and C. theobalt 

Y 

bilinear downsampl 

t(x) 

channels64 128 256 128 649�# 3… 3 

d256 

X 

… 

T 

64 
128 

256 

128 

64 

64 

32 

32 
64 

128 

tanh 

256 

256 

128 

32 64 128 

= 

BN lrelu Up 

= 

deconv BN relu refin 

= 

conv BN reluup refin 

= 

conv drop drop 

in 

out 

in 

out 

3× 
3 

4× 
4 

refin 

p 
ro 

b 
. 

p 
ro 

b 
. 

st 
ri 

d 
e 

2 

st 
ri 

d 
e 

1 

st 
ri 

d 
e 

2 
4× 

4 

fig. 4. architectur of our rendering-to-video translat network for an 
input resolut of 256×256: the encod have 8 downsampl modul 
with (64, 128, 256, 512, 512, 512, 512, 512) output channels. the decod 
have 8 upsampl modul with (512, 512, 512, 512, 256, 128, 64, 3) output 
channels. the upsampl modul use the follow dropout probabl 
(0.5, 0.5, 0.5, 0, 0, 0, 0, 0). the first downsampl and the last upsampl 
modul do not employ batch normal (bn). the final non-linear 
(tanh) bring the output to the employ normal [−1, +1]-space. 

our network consist of two main parts, an encod for com- 

put a low-dimension latent representation, and a decod for 

synthes the output image. We employ skip connect [ron- 

neberg et al. 2015] to enabl the network to transfer ine-scal 

structure. To gener video frame with suicient resolution, our 

network also employ a cascad reinement strategi [chen and 

koltun 2017]. In each downsampl step, we use a convolut 

(4 × 4, stride 2) follow by batch normal and a leaki relu 

non-linearity. the upsampl modul be speciic design to 

produc high-qual output, and have the follow structure: irst, 

the resolut be increas by a factor of two base on deconvolu- 

tion (4 × 4, upsampl factor of 2), batch normalization, dropout 

and relu. afterwards, two reinement step base on convolut 

(3 × 3, stride 1, stay on the same resolution) and relu be applied. 

the inal hyperbol tangent non-linear (tanh) bring the output 

tensor to the normal [−1,+1]-space use for store the imag 

data. for more details, pleas refer to figur 4. 

the input to our discrimin D be the condit input tensor 

X (sizew ×H × 9nw ), and either the predict output imag t(x) 

or the ground-truth image, both of sizew × H × 3. the employ 

discrimin be inspir by the patchgan classiier, propos by 

isola et al. [2017]. We extend it to take volum of condit 

imag a input. 

object function. We train in an adversari manner to ind the 

best rendering-to-video translat network: 

T∗ = argmin 
T 

max 
D 

ecgan(t,d) + λeℓ1 (t). (5) 

thi object function compris an adversari loss ecgan(t,d) 

and an ℓ1-norm reproduct loss eℓ1 (t). the constant weight of 

λ=100 balanc the contribut of these two terms. the adversari 

loss have the follow form: 

egan(t,d) = ex,i 
[ 

logd(x,y) 
] 

+ EX 

[ 

log 
( 

1 − d(x,t(x)) 
) ] 

. (6) 

We do not inject a nois vector while train our network to pro- 

duce determinist outputs. dure adversari training, the discrim- 

inat D tri to get good at classifi give imag a real or 

synthetic, while the transform network T tri to improv in 

fool the discriminator. the ℓ1-norm loss penal the distanc 

between the synthes imag t(x) and the ground-truth imag Y, 

which encourag the sharp of the synthes output: 

eℓ1 (t) = ex,i 
[ 

∥Y − t(x)∥1 
] 

. (7) 

training. We construct the train corpu T= {(xi ,yi )}i base 

on the track video frame of the target video sequence. typically, 

two thousand video frames, i.e., about one minut of video footage, 

be suicient to train our network (see section 7). our train 

corpu consist of Nt −(nw −1) render condit space-tim 

volum Xi and the correspond ground-truth imag Yi (use a 

window size of Nw =11). We train our network use the tensor- 

flow [abadi et al. 2015] deep learn framework. the gradient 

for back-propag be obtain use adam [kingma and Ba 

2015]. We train for 31,000 iter with a batch size of 16 (approx. 

250 epoch for a train corpu of 2000 frames) use a base learn- 

ing rate of 0.0002 and irst momentum of 0.5; all other paramet 

have their default value. We train our network from scratch, and 

initi the weight base on a normal distribut n(0, 0.2). 

7 result 

our approach enabl full-fram target video portrait synthesi un- 

der full 3D head pose control. We measur the runtim for train 

and test on an intel xeon e5-2637 with 3.5 ghz (16gb ram) and 

an nvidia geforc gtx titan Xp (12gb ram). train our net- 

work take 10 hour for a target video resolut of 256×256 pixels, 

and 42 hour for 512×512 pixels. track the sourc actor take 

250m per frame (without identity), and the rendering-to-video 

convers (inference) take 65m per frame for 256×256 pixels, or 

196m for 512×512 pixels. 

In the following, we evalu the design choic of our deep video 

portrait algorithm, compar to current state-of-the-art reenact 

approaches, and show the result of a large-scal web-bas user 

study. We further demonstr the potenti of our approach on sev- 

eral video rewrit applications, such a reenact under full head 

and facial express control, facial express reenact only, 

video dubbing, and live video portrait edit under user control. 

In total, we appli our approach to 14 difer target sequenc 

of 13 difer subject and use 5 difer sourc sequences; see 

appendix A for details. A comparison to a simpl nearest-neighbor 

retriev approach can be found in figur 6 and in the supplement 

video. our approach requir onli a few minut of target video 

footag for training. 

7.1 applic 

our approach enabl u to take full control of the rigid head pose, 

facial expression, and eye motion of a target actor in a video por- 

trait, thu open up a wide rang of video rewrit applications. 

all paramet dimens can be estim and transfer from a 

sourc video sequenc or edit manual through an interact 

user interface. 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 



deep video portrait • 163:7 

S 
o 
u 
rc 
e 

T 
a 
rg 
e 
t 

S 
o 
u 
rc 
e 

T 
a 
rg 
e 
t 

fig. 5. ualit result of full-head reenactment: our approach enabl full-fram target video portrait synthesi under full 3D head pose control. the 
output video portrait be photo-realist and hard to distinguish from real videos. note that even the shadow in the background of the second row move 
consist with the modifi foreground head motion. In the sequenc at the top, we onli transfer the translat in the camera plane, while we transfer the 
full 3D translat for the sequenc at the botom. for full sequences, pleas refer to our video. obama video courtesi of the white hous (public domain). 

input oursnearest neighbor 

fig. 6. comparison to a nearest-neighbor approach in paramet space (pose 
and expression). our result have high qualiti and be tempor more 
coher (see supplement video). for the nearest-neighbor approach, it be 
dificult to find the right trade-of between pose and expression. thi lead 
to mani result with one of the two dimens not be well-matched. 
the result be also tempor unstable, sinc the near neighbor abruptli 
changes, especi for small train sets. 

reenact under full head control. our approach be the irst that 

can photo-realist transfer the full 3D head pose (spatial posit 

and rotation), facial expression, a well a eye gaze and eye blink 

of a captur sourc actor to a target actor video. figur 5 show 

some exampl of full-head reenact between difer sourc 

and target actors. here, we use the full target video for train 

and the sourc video a the drive sequence. As can be seen, the 

output of our approach achiev a high level of realism and faith 

mimic the drive sequence, while still retain the manner 

of the origin target actor. note that the shadow in the background 

move consist with the posit of the actor in the scene, a 

show in figur 5 (second row). We also demonstr the high 

qualiti of our result and evalu our approach quantit in a 

self-reenact scenario, see figur 7. for the quantit analysis, 

we use two third of the target video for train and one third for 

testing. We captur the face in the train and drive video with 

our model-bas tracker, and then render the condit images, 

which serv a input to our network for synthes the output. for 

further details, pleas refer to section 7.2. note that the synthes 

result be nearli indistinguish from the ground truth. 

facial reenact and video dubbing. besid full-head reen- 

actment, our approach also enabl facial reenactment. In thi ex- 

periment, we replac the express coeicient of the target actor 

with those of the sourc actor befor synthes the condit 

input to our rendering-to-video translat network. here, the head 

pose and position, and eye gaze remain unchanged. figur 8 show 

facial reenact results. observ that the face express in the 

synthes target video nice match the express of the sourc 

actor in the drive sequence. pleas refer to the supplement video 

for the complet video sequences. 

our approach can also be appli to visual dubbing. In mani 

countries, foreign-languag movi be dubbed, i.e., the origin 

voic of an actor be replac with that of a dub actor speak 

in anoth language. dub often caus visual discomfort due 

to the discrep between the actor’ mouth motion and the new 

audio track. even profession dub studio achiev onli approx- 

imat audio align at best. visual dub aim at alter the 

mouth motion of the target actor to match the new foreign-languag 

audio track spoken by the dubber. figur 9 show result where 

we modifi the facial motion of actor speak origin in ger- 

man to adher to an english translat spoken by a profession 

dub actor, who be ilm in a dub studio [garrido et al. 

2015]. more precisely, we transfer the captur facial express 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 



163:8 • H. kim, P. garrido, A. tewari, W. xu, J. thies, M. nießner, P. pérez, C. richardt, M. zollhöfer, and C. theobalt 

fig. 7. uantit evalu of the photometr re-rend error. We evalu our approach quantit in a self-reenact seting, where the 
ground-truth video portrait be known. We train our rendering-to-video translat network on two third of the video sequence, and test on the remain third. 
the error map show per-pixel euclidean distanc in rgb (color channel in [0, 255]); the mean photometr error of the test set be show in the top-right. the 
error be consist low in region with condit input, with high error in region without conditioning, such a the upper body. obama video courtesi 
of the white hous (public domain). putin video courtesi of the kremlin (cc by). may video courtesi of the UK govern (open govern licence). 

fig. 8. facial reenact result of our approach. We transfer the express from the sourc to the target actor, while retain the head pose (rotat and 
translation) a well a the eye gaze of the target actor. for the full sequences, pleas refer to the supplement video. obama video courtesi of the white hous 
(public domain). putin video courtesi of the kremlin (cc by). reagan video courtesi of the nation archiv and record administr (public domain). 

fig. 9. dub comparison on two sequenc of garrido et al. [2015]. for 
visual dubbing, we transfer the facial express of the dub actor 
(‘input’) to the target actor. We compar our result to garrido et al.’s. our 
approach obtain high qualiti result in term of the synthes mouth 
shape and mouth interior. note that our approach also enabl full-head 
reenact in addit to express transfer. for the full comparison, we 
refer to the supplement video. 

of the dub actor to the target actor, while leav the origin 

target gaze and eye blink intact, i.e., we use the origin eye gaze 

imag of the track target sequenc a conditioning. As can be 

seen, our approach achiev dub result of high quality. In fact, 

we produc imag with more realist mouth interior and more 

emot content in the mouth region. pleas see the supplement 

video for full video results. 

interact edit of video portraits. We built an interact editor 

that enabl user to reanim video portrait with live feedback by 

modifi the paramet of the coars face model render into the 

condit imag (see our live demo in the supplement video). 

figur 10 show a few static snapshot that be take while the 

user be play with our editor. our approach enabl chang 

of all paramet dimensions, either independ or all together, 

a show in figur 10. more speciically, we show independ 

chang of the expression, head rotation, head translation, and eye 

gaze (includ eye blinks). pleas note the realist and consist 

gener of the torso, head and background. even shadow or 

relect appear veri consist in the background. In addition, 

we show user edit that modifi all paramet simultaneously. our 

interact editor run at approxim 9 fps. while not the focu of 

thi paper, our approach also enabl modiic of the geometr 

facial identity, see figur 11. these combin modiic show a 

a proof of concept that our network gener beyond the train 

corpus. 

7.2 uantit evalu 

We perform a quantit evalu of the re-rend quality. 

first, we evalu our approach in a self-reenact setting, where 

the ground-truth video portrait be known. We train our rendering-to- 

video translat network on the irst two third of a video sequenc 

and test it on the remain last third of the video, see figur 7. the 

photometr error map show the per-pixel euclidean distanc in 

rgb color space, with each channel be in [0, 255]. We perform 

thi test for three difer video and the mean photometr er- 

ror be 2.88 (vladimir putin), 4.76 (theresa may), and 4.46 (barack 

obama). our approach obtain consist low error in region 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 



deep video portrait • 163:9 

refer express gaze 

rotat translat combin 

refer express gaze 

rotat translat combin 

refer express gaze 

rotat translat combin 

fig. 10. interact editing. our approach provid full parametr control over video portrait (bi control head model paramet in condit images). 
thi enabl modif of the rigid head pose (rotat and translation), facial express and eye motion. all of these dimens can be manipul 
togeth or independently. We also show these modif live in the supplement video. obama video courtesi of the white hous (public domain). 

refer ident chang 

fig. 11. ident modification. while not the main focu of our approach, 
it also enabl modif of the facial shape via the geometri shape 
parameters. thi show that our network pick up the correspond 
between the model and the video portrait. note that the produc output 
be also consist in region that be not constrain by the condit 
input, such a the hair and background. 

O 
u 
r 

A 
v 
e 
rb 

u 
ch 

-E 
lo 
r1 
7 

In 
p 
u 
t 

fig. 12. comparison to the imag reenact approach of averbuch-elor 
et al. [2017] in the full-head reenact scenario. sinc their method be 
base on a singl target image, they copi the mouth interior from the 
sourc to the target, thu not preserv the target’ identity. our learning- 
base approach enabl larg modif of the rigid head pose without 
appar artifacts, while their warping-bas approach distort the head 
and background. In addition, our enabl joint control of the eye gaze 
and eye blinks. the difer be most evid in the supplement video. 
obama video courtesi of the white hous (public domain). 

fig. 13. comparison to suwajanakorn et al. [2017]. their approach produc 
accur lip sync with visual impercept artifacts, but provid no direct 
control over facial expressions. thus, the express in the output do not 
alway perfectli match the input (box, mouth), especi for express 
chang without audio cue. our visual dub approach accur trans- 
fer the express from the sourc to the target. In addition, our approach 
provid more control over the target video by also transfer the eye gaze 
and eye blink (box, eyes), and the rigid head pose (arrows). sinc the sourc 
sequenc show more head-pos variat than the target sequence, we 
scale the transfer rotat and translat by 0.5 in thi experiment. for 
the full video sequence, we refer to the supplement video. obama video 
courtesi of the white hous (public domain). 

with condit input (face) and high error be found in region 

that be unexplain by the condit input. pleas note that 

while the synthes video portrait slightli difer from the ground 

truth outsid the face region, the synthes hair and upper bodi 

be still plausible, consist with the face region, and free of visual 

artifacts. for a complet analysi of these sequences, we refer to the 

supplement video. 

We evalu our space-tim condit strategi in figur 16. 

without space-tim conditioning, the photometr error be signii- 

cantli higher. the averag error over the complet sequenc be 

4.9 without vs. 4.5 with tempor condit (barack obama) and 

5.3 without vs. 4.8 with tempor condit (theresa may). In 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 



163:10 • H. kim, P. garrido, A. tewari, W. xu, J. thies, M. nießner, P. pérez, C. richardt, M. zollhöfer, and C. theobalt 

fig. 14. comparison to the state-of-the-art facial reenact approach 
of thi et al. [2016]. our approach achiev express transfer of similar 
quality, while also enabl full-head reenactment, i.e., it also transfer the 
rigid head pose, gaze direction, and eye blinks. for the video result, we 
refer to the supplement video. obama video courtesi of the white hous 
(public domain). 

addit to a low photometr error, space-tim condit also 

lead to tempor signiicantli more stabl video outputs. thi 

can be see best in the supplement video. 

We also evalu the import of the train set size. In thi 

experiment, we train our rendering-to-video translat network 

with 500, 1000, 2000 and 4000 frame of the target sequence, see 

figur 15. As can be expected, larg train set produc good 

results, and the best result be obtain with the full train set. 

We also evalu difer imag resolut by train our 

rendering-to-video translat network for resolut of 256×256, 

512×512 and 1024×1024 pixels. We evalu the qualiti in the self- 

reenact setting, a show in figur 17. gener network 

of high resolut be harder to train and requir signiicantli 

longer train times: 10 hour for 256×256, 42 hour for 512×512, 

and 110 hour for 1024×1024 (on a titan xp). therefore, we use a 

resolut of 256×256 pixel for most results. 

7.3 comparison to the state of the art 

We compar our deep video portrait approach to current state-of- 

the-art video and imag reenact techniques. 

comparison to thi et al. [2016]. We compar our approach to 

the state-of-the-art face2fac facial reenact method of thi 

et al. [2016]. In comparison to face2face, our approach achiev 

express transfer of similar quality. what distinguish our ap- 

proach be the capabl for full-head reenactment, i.e., the abil to 

also transfer the rigid head pose, gaze direction, and eye blink in 

addit to the facial expressions, a show in figur 14. As can be 

seen, in our result, the head pose and eye motion nice match the 

sourc sequence, while the output gener by face2fac follow 

the head and eye motion of the origin target sequence. pleas see 

the supplement video for the video result. 

comparison to suwajanakorn et al. [2017]. We also compar to 

the audio-bas dub approach of suwajanakorn et al. [2017], 

see figur 13. their audiotoobama approach produc accur lip 

sync with visual impercept artifacts, but provid no direct 

control over facial expressions. thus, the express in the output 

do not alway perfectli match the input (box, mouth), especi 

for express chang without an audio cue. our visual dub 

approach accur transfer the express from the sourc to 

the target. In addition, our approach provid more control over 

the target video by also transfer the eye gaze and eye blink 

(box, eyes) and the gener rigid head pose (arrows). while their 

approach be train on a huge amount of train data (17 hours), 

our approach onli us a small train dataset (1.3minutes). the 

difer be best visibl in the supplement video. 

comparison to averbuch-elor et al. [2017]. We compar our ap- 

proach in the full-head reenact scenario to the imag reenact- 

ment approach of averbuch-elor et al. [2017], see figur 12. their 

approach do not preserv the ident of the target actor, sinc 

they copi the teeth and mouth interior from the sourc to the target 

sequence. our learning-bas approach enabl larg modiic 

of the head pose without appar artifacts, while their warping- 

base approach signiicantli distort the head and background. In 

addition, we enabl the joint modiic of the gaze direct and 

eye blinks; see supplement video. 

7.4 user studi 

We conduct two extens web-bas user studi to quantita- 

tive evalu the realism of our results. We prepar short 5- 

second video clip that we extract from both real and synthes 

video (see figur 18), to evalu three applic of our approach: 

self-reenactment, same-person-reenact and visual dubbing. We 

opt for self-reenactment, same-person-reenact (two speech 

of barack obama) and visual dub to guarante that the motion 

type in the evalu real and synthes video pair be match- 

ing. thi elimin the motion type a a confound factor from 

the statist analysis, e.g., have unrealist motion for a public 

speech in the synthes video would neg bia the out- 

come of the study. our evalu be focu on the visual qualiti 

of the synthes results. most video clip have a resolut of 

256×256 pixels, but some be 512×512 pixels. In our user study, we 

present one video clip at a time, and ask particip to re- 

spond to the statement łthi video clip look real to mež on a 5-point 

likert scale (1śstrongli disagree, 2śdisagree, 3śdon’t know, 4śagree, 

5śstrongli agree). video clip be show in a random order, and 

each video clip be show exactli onc to ass participants’ irst 

impression. We recruit 135 and 69 anonym particip for 

our two studies, larg from north america and europe. 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 



deep video portrait • 163:11 

fig. 15. uantit evalu of the train set size. We train our rendering-to-video translat network with train corpu of difer sizes. the error 
map show the per-pixel distanc in rgb color space with each channel be in [0, 255]; the mean photometr error be show in the top-right. smaller 
train set have larg photometr errors, especi for region outsid of the face. for the full comparison, we refer to the supplement video. obama 
video courtesi of the white hous (public domain). may video courtesi of the UK govern (open govern licence). 

fig. 16. uantit evalu of the influenc of the propos space-tim 
condit input. the error map show the per-pixel distanc in rgb color 
space with each channel be in [0, 255]; the mean photometr error be 
show in the top-right. without space-tim conditioning, the photometr 
error be higher. tempor condit add signific tempor stability. 
thi be best see in the supplement video. obama video courtesi of the 
white hous (public domain).may video courtesi of the UK govern 
(open govern licence). 

fig. 17. uantit comparison of difer resolutions. We train three 
rendering-to-video translat network for resolut of 256×256, 512×512 
and 1024×1024 pixels. the error map show the per-pixel distanc in rgb 
color space with each channel be in [0, 255]; the mean photometr error 
be show in the top-right. for the full comparison, see our video. may video 
courtesi of the UK govern (open govern licence). 

fig. 18. We perform a user studi to evalu the qualiti of our result and 
see if user can distinguish between real (top) and synthes video clip 
(botom). the video clip includ self-reenactment, same-person-reenact- 
ment, and video dubbing. putin video courtesi of the kremlin (cc by). 
obama video courtesi of the white hous (public domain). elizabeth II 
video courtesi of the governor gener of canada (public domain). 

the result in tabl 1 show that onli 80% of particip rat real 

256×256 video a real, i.e. (strongly) agre to the video look 

real; it seem that in anticip of synthet video clips, partici- 

pant becom overli critical. At the same time, 50% of particip 

consid our 256×256 result to be real, which increas slightli to 

52% for 512×512. our best result be the self-reenact of vladimir 

putin at 256×256 resolution, which 65% of particip consid 

to be real, compar to 78% for the real video. We also evalu 

partial and full reenact by transfer a speech by barack 

obama to anoth video clip of himself. tabl 2 indic that we 

achiev good realism rate with full reenact compris fa- 

cial express and pose (50%) compar to transfer onli facial 

express (38%). thi might be becaus full-head reenact 

keep express and head motion synchronized. suwajanakorn 

et al.’ speech-driven reenact approach [2017] achiev a re- 

alism rate of 64% compar to the real sourc and target video 

clips, which achiev 70ś86%. our full-head reenact result be 

consid to be at least a real a suwajanakorn et al.’ by 60% 

of participants. We inal compar our dub result to vdub 

[garrido et al. 2015] in tabl 3. overall, 57% of particip give 

our result a high realism rate (and 32% give the same rating). 

our result be again consid to be real by 51% of participants, 

compar to onli 21% for vdub. 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 



163:12 • H. kim, P. garrido, A. tewari, W. xu, J. thies, M. nießner, P. pérez, C. richardt, M. zollhöfer, and C. theobalt 

tabl 1. user studi result for self-reenact video (n=135). column 1ś5 
show the percentag of rate give about the statement łthi video clip 
look real to mež, from 1 (strongli disagree) to 5 (strongli agree). 4+5=‘real’. 

real video our result 

re 1 2 3 4 5 ‘real’ 1 2 3 4 5 ‘real’ 

obama 256 2 8 10 62 19 81% 13 33 11 37 6 43% 

putin 256 2 11 10 58 20 78% 3 17 15 54 11 65% 

eliabeth II 256 2 6 12 59 21 80% 6 32 20 33 9 42% 

obama 512 0 7 3 49 42 91% 9 35 13 36 8 44% 

putin 512 4 13 10 47 25 72% 2 20 15 44 19 63% 

eliabeth II 512 1 7 4 55 34 89% 7 33 10 38 13 51% 

mean 256 2 8 10 60 20 80% 7 27 15 41 9 50% 

mean 512 2 9 6 50 34 84% 6 29 12 39 13 52% 

tabl 2. user studi result for express and full head transfer between two 
video of barack obama compar to the input video and suwajanakorn 
et al.’ approach (n=69, mean of 4 clips). 

rate 

1 2 3 4 5 ‘real’ 

sourc video (real) 0 8 6 43 42 86% 

target video (real) 1 14 14 47 23 70% 

suwajanakorn et al. [2017] 2 20 14 47 17 64% 

express transfer (ours) 9 37 17 29 9 38% 

full head transfer (ours) 3 31 16 37 13 50% 

tabl 3. user studi result for dub comparison to vdub (n=135). 

garrido et al. [2015] our result 

1 2 3 4 5 ‘real’ 1 2 3 4 5 ‘real’ 

ingmar (3 clips) 21 36 21 20 2 22% 4 21 25 42 8 50% 

thoma (3 clips) 33 36 11 16 4 20% 7 25 17 42 9 51% 

mean (6 clips) 27 36 16 18 3 21% 6 23 21 42 9 51% 

On average, across all scenario and both studies, our result be 

consid to be real by 47% of the particip (1,767 ratings), com- 

par to onli 80% for real video clip (1,362 ratings). thi suggest 

that our result alreadi fool about 60% of the particip ś a good 

result give the critic particip pool. however, there be some 

variat across our results: low realism rate be give for 

well-known person like barack obama, while high rate 

be give for instanc to the unknown dub actors. 

8 discuss 

while we have demonstr highli realist reenact result 

in a larg varieti of applic and scenarios, our approach be 

also subject to a few limitations. similar to all other learning-bas 

approaches, our work veri well insid the span of the train 

corpus. extrem target head poses, such a larg rotations, or ex- 

pression far outsid thi span can lead to a degrad of the 

visual qualiti of the gener video portrait, see figur 19 and the 

supplement video. sinc we onli track the face with a parametr 

model, we cannot activ control the motion of the torso or hair, or 

control the background. the network learn to extrapol and ind 

a plausibl and consist upper bodi and background (includ 

some shadow and relections) for a give head pose. thi limit 

fig. 19. our approach work well within the span of the train corpus. 
extrem chang in head pose far outsid the train set or strong chang 
to the facial express might lead to artifact in the synthes video. thi 
be a common limit of all learning-bas approaches. In these cases, 
artifact be most promin outsid the face region, a these region have 
no condit input. may video courtesi of the UK govern (open 
govern licence). malou video courtesi of louisa malou (cc by). 

could be overcom by also track the bodi and use the underly- 

ing bodi model to gener an extend set of condit images. 

currently, we be onli abl to produc medium-resolut output 

due to memori and train time limitations. the limit output 

resolut make it especi diicult to reproduc ine-scal de- 

tail, such a individu teeth, in a tempor coher manner. yet, 

recent progress on high-resolut discrimin adversari net- 

work [karra et al. 2018; wang et al. 2017] be promis and could 

be leverag to further increas the resolut of the gener out- 

put. On a broader scale, and not be a limitation, democrat 

of advanc high-qual video edit possibilities, ofer by our 

and other methods, call for addit care in ensur veriiabl 

video authenticity, e.g., through invis watermarking. 

9 conclus 

We present a new approach to synthes entir photo-r video 

portrait of a target actor in front of gener static backgrounds. 

It be the irst to transfer head pose and orientation, face expression, 

and eye gaze from a sourc actor to a target actor. the propos 

method be base on a novel rendering-to-video translat network 

that convert a sequenc of simpl comput graphic render 

into photo-realist and temporally-coher video. thi map be 

learn base on a novel space-tim condit volum formula- 

tion. We have show through experi and a user studi that our 

method outperform prior work in qualiti and expand over their 

possibilities. It thu open up a new level of capabl in mani 

applications, like video reenact for virtual realiti and telep- 

resence, interact video editing, and visual dubbing. We see our 

approach a a step toward highli realist synthesi of full-fram 

video content under control of meaning parameters. We hope 

that it will inspir futur research in thi veri challeng ield. 

acknowledg 

We be grate to all our actors. We thank true-visionsolut 

pti ltd for kindli provid the 2D face tracker and adob for a 

premier pro CC license. We also thank supasorn suwajanakorn 

and hadar averbuch-elor for the comparisons. thi work be sup- 

port by erc start grant capreal (335545), a tum-ia rudolf 

mößbauer fellowship, a googl faculti award, rcuk grant cam- 

era (ep/m023281/1), an nvidia corpor gpu grant, and the 

max planck center for visual comput and commun 

(mpc-vcc). 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 



deep video portrait • 163:13 

A appendix 

thi appendix describ all the use datasets, see tabl 4 (target 

actors) and tabl 5 (sourc actors). 

tabl 4. target videos: name and length of sequenc (in frames).mal 
video courtesi of louisa malou (cc by). may video courtesi of the UK 
govern (open govern licence). obama video courtesi of the 
white hous (public domain). putin video courtesi of the kremlin (cc by). 
reagan video courtesi of the nation archiv and record administr 
(public domain). elizabeth II video courtesi of the governor gener of 
canada (public domain). reagan video courtesi of the nation archiv 
and record administr (public domain).wolf video courtesi of tom 
wolf (cc by). 

ingmar malou may obama1 obama2 

3,000 15,000 5,000 2,000 3,613 

putin elizabeth II reagan thoma wolf 

4,000 1,500 6,984 2,239 15,000 

db1 db2 db3 db4 

8,000 18,138 6,500 30,024 

tabl 5. sourc videos: name and length of sequenc (in frames). obama 
video courtesi of the white hous (public domain). 

obama3 david1 david2 db5 db6 

1,945 4,611 3,323 3,824 2,380 

refer 
martín abadi, ashish agarwal, paul barham, eugen brevdo, zhifeng chen, craig citro, 

greg S. corrado, andi davis, jefrey dean, matthieu devin, sanjay ghemawat, 
ian goodfellow, andrew harp, geofrey irving, michael isard, yangq jia, rafal 
jozefowicz, lukasz kaiser, manjunath kudlur, josh levenberg, dan mané, rajat 
monga, sherri moore, derek murray, chri olah, mike schuster, jonathon shlens, 
benoit steiner, ilya sutskever, kunal talwar, paul tucker, vincent vanhoucke, vijay 
vasudevan, fernanda viégas, oriol vinyals, petewarden,martinwattenberg,martin 
wicke, yuan yu, and xiaoqiang zheng. 2015. tensorflow: large-scal machin 
learn on heterogen systems. https://www.tensorlow.org/ softwar 
avail from tensorlow.org. 

oleg alexander, mike rogers, william lambeth, jen-yuan chiang, wan-chun ma, 
chuan-chang wang, and paul debevec. 2010. the digit emili project: achiev 
a photorealist digit actor. ieee comput graphic and applic 30, 4 
(july/august 2010), 20ś31. https://doi.org/10.1109/mcg.2010.65 

hadar averbuch-elor, daniel cohen-or, johann kopf, and michael F. cohen. 2017. 
bring portrait to life. acm transact on graphic (siggraph asia) 36, 6 
(novemb 2017), 196:1ś13. https://doi.org/10.1145/3130800.3130818 

volker blanz, kristina scherbaum, thoma vetter, and hans-pet seidel. 2004. ex- 
chang face in images. comput graphic forum (eurographics) 23, 3 (septemb 
2004), 669ś676. https://doi.org/10.1111/j.1467-8659.2004.00799.x 

volker blanz and thoma vetter. 1999. amorph model for the synthesi of 3D faces. 
In annual confer on comput graphic and interact techniqu (siggraph). 
187ś194. https://doi.org/10.1145/311535.311556 

jame booth, anastasio roussos, allan ponniah, david dunaway, and stefano 
zafeiriou. 2018. larg scale 3dmorphabl models. intern journal of comput 
vision 126, 2 (april 2018), 233ś254. https://doi.org/10.1007/s11263-017-1009-7 

christoph bregler, michel covell, and malcolm slaney. 1997. video rewrite: drive 
visual speechwithaudio. inannu confer on comput graphic and interact 
techniqu (siggraph). 353ś360. https://doi.org/10.1145/258734.258880 

chen cao, derek bradley, kun zhou, and thabo beeler. 2015. real-tim high-idel 
facial perform capture. acm transact on graphic (siggraph) 34, 4 (juli 
2015), 46:1ś9. https://doi.org/10.1145/2766943 

chen cao, qime hou, and kun zhou. 2014a. displac dynam express regres- 
sion for real-tim facial track and animation. acm transact on graphic 
(siggraph) 33, 4 (juli 2014), 43:1ś10. https://doi.org/10.1145/2601097.2601204 

chen cao, yanlinweng, shun zhou, yiy tong, and kun zhou. 2014b. facewarehouse: 
A 3D facial express databas for visual computing. ieee transact on 
visual and comput graphic 20, 3 (march 2014), 413ś425. https://doi.org/ 
10.1109/tvcg.2013.249 

chen cao, hongzhi wu, yanlin weng, tianjia shao, and kun zhou. 2016. real-tim 
facial anim with image-bas dynam avatars. acmtransact on graphic 
(siggraph) 35, 4 (juli 2016), 126:1ś12. https://doi.org/10.1145/2897824.2925873 

yao-jen chang and toni ezzat. 2005. transfer videorealist speech animation. 
In symposium on comput anim (sca). 143ś151. https://doi.org/10.1145/ 
1073368.1073388 

qifeng chen and vladlen koltun. 2017. photograph imag synthesi with cascad 
reinement networks. In intern confer on comput vision (iccv). 1520ś 
1529. https://doi.org/10.1109/iccv.2017.168 

kevin dale, kalyan sunkavalli, micah K. johnson, daniel vlasic, wojciech matusik, 
and hanspet pister. 2011. video face replacement. acm transact on graphic 
(siggraph asia) 30, 6 (decemb 2011), 130:1ś10. https://doi.org/10.1145/2070781. 
2024164 

toni ezzat, gadi geiger, and tomaso poggio. 2002. trainabl videorealist speech 
animation. acm transact on graphic (siggraph) 21, 3 (juli 2002), 388ś398. 
https://doi.org/10.1145/566654.566594 

ohad fried, eli shechtman, dan B. goldman, and adam finkelstein. 2016. perspective- 
awar manipul of portrait photos. acm transact on graphic (siggraph) 
35, 4 (juli 2016), 128:1ś10. https://doi.org/10.1145/2897824.2925933 

graham fyfe, andrew jones, oleg alexander, ryosuk ichikari, and paul debevec. 
2014. drive high-resolut facial scan with video perform capture. acm 
transact on graphic 34, 1 (decemb 2014), 8:1ś14. https://doi.org/10.1145/ 
2638549 

yaroslav ganin, daniil kononenko, diana sungatullina, and victor lempitsky. 2016. 
deepwarp: photorealist imag resynthesi for gaze manipulation. In euro- 
pean confer on comput vision (eccv). 311ś326. https://doi.org/10.1007/ 
978-3-319-46475-6_20 

pablo garrido, levi valgaerts, ole rehmsen, thorsten thormaehlen, patrick pérez, and 
christian theobalt. 2014. automat face reenactment. In confer on comput 
vision and pattern recognit (cvpr). 4217ś4224. https://doi.org/10.1109/cvpr. 
2014.537 

pablo garrido, levi valgaerts, hamid sarmadi, ingmar steiner, kiran varanasi, patrick 
pérez, and christian theobalt. 2015. vdub: modifi face video of actor for 
plausibl visual align to a dub audio track. comput graphic forum 
(eurographics) 34, 2 (may 2015), 193ś204. https://doi.org/10.1111/cgf.12552 

pablo garrido, michael zollhöfer, dan casas, levi valgaerts, kiran varanasi, patrick 
pérez, and christian theobalt. 2016. reconstruct of person 3D face rig 
from monocular video. acm transact on graphic 35, 3 (june 2016), 28:1ś15. 
https://doi.org/10.1145/2890493 

ian J. goodfellow, jean pouget-abadie, mehdi mirza, bing xu, david warde-farley, 
sherjil ozair, aaron courville, and yoshua bengio. 2014. gener adversari 
nets. In advanc in neural inform process systems. 

geofrey E. hinton and ruslan salakhutdinov. 2006. reduc the dimension 
of data with neural networks. scienc 313, 5786 (juli 2006), 504ś507. https: 
//doi.org/10.1126/science.1127647 

liwen hu, shunsuk saito, lingyu wei, koki nagano, jaewoo seo, jen fursund, iman 
sadeghi, carri sun, yen-chun chen, and hao li. 2017. avatar digit from a 
singl imag for real-tim rendering. acm transact on graphic (siggraph 
asia) 36, 6 (novemb 2017), 195:1ś14. https://doi.org/10.1145/3130800.31310887 

alexandru eugen ichim, soien bouaziz, and mark pauly. 2015. dynam 3D avatar 
creation from hand-held video input. acm transact on graphic (siggraph) 
34, 4 (juli 2015), 45:1ś14. https://doi.org/10.1145/2766974 

phillip isola, jun-yan zhu, tinghui zhou, and alexei A. efros. 2017. image-to-imag 
translat with condit adversari networks. In confer on comput 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 



163:14 • H. kim, P. garrido, A. tewari, W. xu, J. thies, M. nießner, P. pérez, C. richardt, M. zollhöfer, and C. theobalt 

vision and pattern recognit (cvpr). 5967ś5976. https://doi.org/10.1109/cvpr. 
2017.632 

tero karras, timo aila, samuli laine, and jaakko lehtinen. 2018. progress grow 
of gan for improv quality, stability, and variation. In intern confer 
on learn represent (iclr). 

ira kemelmacher-shlizerman. 2013. internet-bas morphabl model. In intern 
confer on comput vision (iccv). 3256ś3263. https://doi.org/10.1109/iccv. 
2013.404 

ira kemelmacher-shlizerman, aditya sankar, eli shechtman, and steven M. seitz. 2010. 
be john malkovich. In european confer on comput vision (eccv). 341ś353. 
https://doi.org/10.1007/978-3-642-15549-9_25 

ira kemelmacher-shlizerman, eli shechtman, rahul garg, and steven M. seitz. 2011. 
explor photobios. acm transact on graphic (siggraph) 30, 4 (august 
2011), 61:1ś10. https://doi.org/10.1145/2010324.1964956 

diederik P. kingma and jimmi ba. 2015. adam: A method for stochast optimization. 
In intern confer on learn represent (iclr). 

christoph lassner, gerard pons-moll, and peter V. gehler. 2017. A gener model of 
peopl in clothing. In intern confer on comput vision (iccv). 853ś862. 
https://doi.org/10.1109/iccv.2017.98 

hao li, laura trutoiu, kyle olszewski, lingyu wei, tristan trutna, pei-lun hsieh, 
aaron nicholls, and chongyang ma. 2015. facial perform sens head- 
mount display. acm transact on graphic (siggraph) 34, 4 (juli 2015), 
47:1ś9. https://doi.org/10.1145/2766939 

kai li, qionghai dai, ruip wang, yebin liu, feng xu, and jue wang. 2014. A data- 
driven approach for facial express retarget in video. ieee transact 
on multimedia 16, 2 (februari 2014), 299ś310. https://doi.org/10.1109/tmm.2013. 
2293064 

kang liu and joern ostermann. 2011. realist facial express synthesi for an image- 
base talk head. In intern confer on multimedia and expo (icme). 
https://doi.org/10.1109/icme.2011.6011835 

ming-yu liu, thoma breuel, and jan kautz. 2017. unsupervis image-to-imag 
translat networks. In advanc in neural inform process systems. 

zicheng liu, ying shan, and zhengyou zhang. 2001. express express map 
with ratio images. In annual confer on comput graphic and interact 
techniqu (siggraph). 271ś276. https://doi.org/10.1145/383259.383289 

liqian ma, qianru sun, Xu jia, bernt schiele, tinn tuytelaars, and luc van gool. 2017. 
pose guid person imag generation. In advanc in neural inform process 
systems. 

mehdi mirza and simon osindero. 2014. condit gener adversari nets. 
(2014). https://arxiv.org/abs/1411.1784 arxiv:1411.1784. 

kyle olszewski, zimo li, chao yang, Yi zhou, ronald yu, zeng huang, sitao xiang, 
shunsuk saito, pushmeet kohli, and hao li. 2017. realist dynam facial textur 
from a singl imag use gans. In intern confer on comput vision 
(iccv). 5439ś5448. https://doi.org/10.1109/iccv.2017.580 

kyle olszewski, joseph J. lim, shunsuk saito, and hao li. 2016. high-idel facial 
and speech anim for VR hmds. acm transact on graphic (siggraph 
asia) 35, 6 (novemb 2016), 221:1ś14. https://doi.org/10.1145/2980179.2980252 

alec radford, luke metz, and soumith chintala. 2016. unsupervis represent 
learn with deep convolut gener adversari networks. In interna- 
tional confer on learn represent (iclr). 

ravi ramamoorthi and pat hanrahan. 2001. An eicient represent for irradi 
environ maps. In annual confer on comput graphic and interact 
techniqu (siggraph). 497ś500. https://doi.org/10.1145/383259.383317 

elad richardson, matan sela, and ron kimmel. 2016. 3D face reconstruct by 
learn from synthet data. In intern confer on 3D vision (3dv). 460ś 
469. https://doi.org/10.1109/3dv.2016.56 

elad richardson, matan sela, roy or-el, and ron kimmel. 2017. learn detail 
face reconstruct from a singl image. In confer on comput vision and 
pattern recognit (cvpr). 5553ś5562. https://doi.org/10.1109/cvpr.2017.589 

olaf ronneberger, philipp fischer, and thoma brox. 2015. u-net: convolut 
network for biomed imag segmentation. In intern confer onmed 
imag comput and computer-assist intervent (miccai). 234ś241. https: 
//doi.org/10.1007/978-3-319-24574-4_28 

joseph roth, yiy tong tong, and xiaom liu. 2017. adapt 3D face recon- 
struction from unconstrain photo collections. ieee transact on pattern 
analysi and machin intellig 39, 11 (novemb 2017), 2127ś2141. https: 
//doi.org/10.1109/tpami.2016.2636829 

jason M. saragih, simon lucey, and jefrey F. cohn. 2011. real-tim avatar anim 
from a singl image. In intern confer on automat face and gestur 
recognit (fg). 117ś124. https://doi.org/10.1109/fg.2011.5771383 

matan sela, elad richardson, and ron kimmel. 2017. unrestrict facial geometri 
reconstruct use image-to-imag translation. In intern confer on 
comput vision (iccv). 1585ś1594. https://doi.org/10.1109/iccv.2017.175 

fuhao shi, hsiang-tao wu, xin tong, and jinxiang chai. 2014. automat acquisit 
of high-idel facial perform use monocular videos. acm transact 
on graphic (siggraph asia) 33, 6 (novemb 2014), 222:1ś13. https://doi.org/10. 

1145/2661229.2661290 
robert W. sumner and jovan popović. 2004. deform transfer for triangl meshes. 

acm transact on graphic (siggraph) 23, 3 (august 2004), 399ś405. https: 
//doi.org/10.1145/1015706.1015736 

supasorn suwajanakorn, ira kemelmacher-shlizerman, and steven M. seitz. 2014. total 
move face reconstruction. In european confer on comput vision (eccv) 
(lectur note in comput science), vol. 8692. 796ś812. https://doi.org/10.1007/ 
978-3-319-10593-2_52 

supasorn suwajanakorn, steven M. seitz, and ira kemelmacher-shlizerman. 2015. what 
make tom hank look like tom hanks. In intern confer on comput 
vision (iccv). 3952ś3960. https://doi.org/10.1109/iccv.2015.450 

supasorn suwajanakorn, steven M. seitz, and ira kemelmacher-shlizerman. 2017. 
synthes obama: learn lip sync from audio. acm transact on graphic 
(siggraph) 36, 4 (juli 2017), 95:1ś13. https://doi.org/10.1145/3072959.3073640 

yaniv taigman, adam polyak, and lior wolf. 2017. unsupervis cross-domain imag 
generation. In intern confer on learn represent (iclr). 

ayush tewari, michael zollhöfer, hyeongwoo kim, pablo garrido, florian bernard, 
patrick pérez, and christian theobalt. 2017. mofa:model-bas deep convolut 
face autoencod for unsupervis monocular reconstruction. In intern 
confer on comput vision (iccv). 3735ś3744. https://doi.org/10.1109/iccv. 
2017.401 

justu thies, michael zollhöfer, matthia nießner, levi valgaerts, marc stamminger, 
and christian theobalt. 2015. real-tim express transfer for facial reenactment. 
acm transact on graphic (siggraph asia) 34, 6 (novemb 2015), 183:1ś14. 
https://doi.org/10.1145/2816795.2818056 

justu thies, michael zollhöfer, marc stamminger, christian theobalt, and matthia 
nießner. 2016. face2face: real-tim face captur and reenact of rgb videos. 
In confer on comput vision and pattern recognit (cvpr). 2387ś2395. https: 
//doi.org/10.1109/cvpr.2016.262 

justu thies, michael zollhöfer, marc stamminger, christian theobalt, and matthia 
nießner. 2018. facevr: real-tim facial reenact and eye gaze control in 
virtual reality. acm transact on graphic (2018). 

anh tuan tran, tal hassner, iacopo masi, and gerard medioni. 2017. regress robust 
and discrimin 3D morphabl model with a veri deep neural network. In 
confer on comput vision and pattern recognit (cvpr). 1493ś1502. https: 
//doi.org/10.1109/cvpr.2017.163 

daniel vlasic, matthew brand, hanspet pister, and jovan popović. 2005. face transfer 
with multilinear models. acm transact on graphic (siggraph) 24, 3 (juli 
2005), 426ś433. https://doi.org/10.1145/1073204.1073209 

chao wang, haiyong zheng, zhibin yu, ziqiang zheng, zhaorui gu, and bing zheng. 
2017. discrimin region propos adversari network for high-qual image- 
to-imag translation. (2017). https://arxiv.org/abs/1711.09554 arxiv:1711.09554. 

ting-chun wang, ming-yu liu, jun-yan zhu, andrew tao, jan kautz, and bryan 
catanzaro. 2018. high-resolut imag synthesi and semant manipul 
with condit gans. In confer on comput vision and pattern recognit 
(cvpr). 

thibaut weise, soien bouaziz, hao li, and mark pauly. 2011. realtim performance- 
base facial animation. acm transact on graphic (siggraph) 30, 4 (juli 
2011), 77:1ś10. https://doi.org/10.1145/2010324.1964972 

errol wood, tada baltrušaitis, louis-philipp morency, peter robinson, and andrea 
bulling. 2018. gazedirector: fulli articul eye gaze redirect in video. comput 
graphic forum (eurographics) 37, 2 (2018). https://doi.org/10.1111/cgf.13355 

chenglei wu, derek bradley, marku gross, and thabo beeler. 2016. An anatomically- 
constrain local deform model for monocular face capture. acm transac- 
tion on graphic (siggraph) 35, 4 (juli 2016), 115:1ś12. https://doi.org/10.1145/ 
2897824.2925882 

zili yi, hao zhang, ping tan, and minglun gong. 2017. dualgan: unsupervis dual 
learn for image-to-imag translation. In intern confer on comput 
vision (iccv). 2868ś2876. https://doi.org/10.1109/iccv.2017.310 

jun-yan zhu, taesung park, phillip isola, and alexei A. efros. 2017. unpair image-to- 
imag translat use cycle-consist adversari networks. In intern 
confer on comput vision (iccv). 2242ś2251. https://doi.org/10.1109/iccv. 
2017.244 

michael zollhöfer, justu thies, pablo garrido, derek bradley, thabo beeler, patrick 
pérez, marc stamminger, matthia nießner, and christian theobalt. 2018. state of 
the art onmonocular 3D face reconstruction, tracking, and applications. comput 
graphic forum 37, 2 (2018). https://doi.org/10.1111/cgf.13382 

receiv januari 2018; revis april 2018; inal version may 2018; accept 

may 2018 

acm trans. graph., vol. 37, no. 4, articl 163. public date: august 2018. 


