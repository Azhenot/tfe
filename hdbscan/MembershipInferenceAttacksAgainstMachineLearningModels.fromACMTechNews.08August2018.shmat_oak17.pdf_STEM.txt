


















































membership infer attack against 
machin learn model 

reza shokri 
cornel tech 

shokri@cornell.edu 

marco stronati⇤ 
inria 

marco@stronati.org 

congzheng song 
cornel 

cs2296@cornell.edu 

vitali shmatikov 
cornel tech 

shmat@cs.cornell.edu 

abstract—w quantit investig how machin learn 
model leak inform about the individu data record on 
which they be trained. We focu on the basic membership 
infer attack: give a data record and black-box access to 
a model, determin if the record be in the model’ train 
dataset. To perform membership infer against a target model, 
we make adversari use of machin learn and train our own 
infer model to recogn differ in the target model’ 
predict on the input that it train on versu the input 
that it do not train on. 

We empir evalu our infer techniqu on classi- 
ficat model train by commerci “machin learn a a 
service” provid such a googl and amazon. use realist 
dataset and classif tasks, includ a hospit discharg 
dataset whose membership be sensit from the privaci perspec- 
tive, we show that these model can be vulner to membership 
infer attacks. We then investig the factor that influenc 
thi leakag and evalu mitig strategies. 

I. introduct 

machin learn be the foundat of popular internet 
servic such a imag and speech recognit and natur lan- 
guag translation. mani compani also use machin learn 
internally, to improv market and advertising, recommend 
product and servic to users, or good understand the data 
gener by their operations. In all of these scenarios, ac- 
tiviti of individu users—their purchas and preferences, 
health data, onlin and offlin transactions, photo they take, 
command they speak into their mobil phones, locat they 
travel to—ar use a the train data. 

internet giant such a googl and amazon be alreadi 
offer “machin learn a a service.” ani custom in 
possess of a dataset and a data classif task can upload 
thi dataset to the servic and pay it to construct a model. 
the servic then make the model avail to the customer, 
typic a a black-box api. for example, a mobile-app maker 
can use such a servic to analyz users’ activ and queri 
the result model insid the app to promot in-app purchas 
to user when they be most like to respond. some machine- 
learn servic also let data owner expos their model to 
extern user for queri or even sell them. 
our contributions. We focu on the fundament question 
know a membership inference: give a machin learn 
model and a record, determin whether thi record be use a 

⇤thi research be perform while the author be at cornel tech. 

part of the model’ train dataset or not. We investig thi 
question in the most difficult setting, where the adversary’ 
access to the model be limit to black-box queri that 
return the model’ output on a give input. In summary, 
we quantifi membership inform leakag through the 
predict output of machin learn models. 

To answer the membership infer question, we turn 
machin learn against itself and train an attack model 
whose purpos be to distinguish the target model’ behavior 
on the train input from it behavior on the input that it 
do not encount dure training. In other words, we turn the 
membership infer problem into a classif problem. 

attack black-box model such a those built by com- 
mercial “machin learn a a service” provid requir 
more sophist than attack white-box model whose 
structur and paramet be know to the adversary. To 
construct our attack models, we invent a shadow train 
technique. first, we creat multipl “shadow models” that 
imit the behavior of the target model, but for which we 
know the train dataset and thu the ground truth about 
membership in these datasets. We then train the attack model 
on the label input and output of the shadow models. 

We develop sever effect method to gener train 
data for the shadow models. the first method us black-box 
access to the target model to synthes thi data. the second 
method us statist about the popul from which the 
target’ train dataset be drawn. the third method assum 
that the adversari have access to a potenti noisi version 
of the target’ train dataset. the first method do not 
assum ani prior knowledg about the distribut of the target 
model’ train data, while the second and third method 
allow the attack to queri the target model onli onc befor 
infer whether a give record be in it train dataset. 

our infer techniqu be gener and not base on ani 
particular dataset or model type. We evalu them against 
neural networks, a well a black-box model train use 
amazon ML and googl predict api. all of our experi- 
ment on amazon’ and google’ platform be do without 
know the learn algorithm use by these services, nor 
the architectur of the result models, sinc amazon and 
googl don’t reveal thi inform to the customers. for our 
evaluation, we use realist classif task and standard 
model-train procedur on concret dataset of images, 
retail purchases, locat traces, and hospit inpati stays. In 



addit to demonstr that membership infer attack 
be successful, we quantifi how their success relat to the 
classif task and the standard metric of overfitting. 

infer inform about the model’ train dataset 
should not be confus with techniqu such a model in- 
version that use a model’ output on a hidden input to infer 
someth about thi input [17] or to extract featur that 
character one of the model’ class [16]. As explain 
in [27] and section ix, model invers do not produc an 
actual member of the model’ train dataset, nor, give a 
record, do it infer whether thi record be in the train 
dataset. By contrast, the membership infer problem we 
studi in thi paper be essenti the same a the well-known 
problem of identifi the presenc of an individual’ data in a 
mix pool give some statist about the pool [3], [15], [21], 
[29]. In our case, however, the goal be to infer membership 
give a black-box api to a model of unknown structure, a 
oppos to explicit statistics. 

our experiment result show that model creat use 
machine-learning-as-a-servic platform can leak a lot of in- 
format about their train datasets. for multi-class clas- 
sific model train on 10,000-record retail transact 
dataset use google’ and amazon’ servic in default 
configurations, our membership infer achiev median 
accuraci of 94% and 74%, respectively. even if we make 
no prior assumpt about the distribut of the target 
model’ train data and use fulli synthet data for our 
shadow models, the accuraci of membership infer against 
google-train model be 90%. our result for the texa 
hospit discharg dataset (over 70% accuracy) indic that 
membership infer can present a risk to health-car dataset 
if these dataset be use to train machin learn model 
and access to the result model be open to the public. 
membership in such dataset be highli sensitive. 

We discu the root caus that make these attack possi- 
ble and quantit compar mitig strategi such a 
limit the model’ predict to top k classes, decreas 
the precis of the predict vector, increas it entropy, 
or use regular while train the model. 

In summary, thi paper demonstr and quantifi the 
problem of machin learn model leak inform 
about their train datasets. To creat our attack models, we 
develop a new shadow learn techniqu that work with 
minim knowledg about the target model and it train 
dataset. finally, we quantifi how the leakag of membership 
inform be relat to model overfitting. 

ii. machin learn background 
machin learn algorithm help u good understand and 

analyz complex data. when the model be creat use 
unsupervis training, the object be to extract use featur 
from the unlabel data and build a model that explain it 
hidden structure. when the model be creat use supervis 
training, which be the focu of thi paper, the train record 
(a input of the model) be assign label or score (a 
output of the model). the goal be to learn the relationship 

between the data and the label and construct a model that can 
gener to data record beyond the train set [19]. model- 
train algorithm aim to minim the model’ predict er- 
ror on the train dataset and thu may overfit to thi dataset, 
produc model that perform good on the train input 
than on the input drawn from the same popul but not 
use dure the training. mani regular techniqu have 
be propos to prevent model from becom overfit 
to their train dataset while minim their predict 
error [19]. 

supervis train be often use for classif and other 
predict tasks. for example, a retail may train a model 
that predict a customer’ shop style in order to offer her 
suitabl incentives, while a medic research may train a 
model to predict which treatment be most like to succeed 
give a patient’ clinic symptom or genet makeup. 
machin learn a a service. major internet compani 
now offer machin learn a a servic on their cloud 
platforms. exampl includ googl predict api,1 amazon 
machin learn (amazon ml),2 microsoft azur machin 
learn (azur ml),3 and bigml.4 

these platform provid simpl api for upload the data 
and for train and queri models, thu make machin 
learn technolog avail to ani customer. for example, 
a develop may creat an app that gather data from users, 
upload it into the cloud platform to train a model (or updat 
an exist model with new data), and then us the model’ 
predict insid the app to improv it featur or good 
interact with the users. some platform even envis data 
holder train a model and then share it with other 
through the platform’ api for profit.5 

the detail of the model and the train algorithm be 
hidden from the data owners. the type of the model may be 
chosen by the servic adaptively, depend on the data and 
perhap accuraci on valid subsets. servic provid do 
not warn custom about the consequ of overfit and 
provid littl or no control over regularization. for example, 
googl predict api hide all details, while amazon ML 
provid onli a veri limit set of pre-defin option (l1- or 
l2-norm regularization). the model cannot be download 
and be access onli through the service’ api. servic 
provid deriv revenu mainli by charg custom for 
queri through thi api. therefore, we treat “machin learn- 
ing a a service” a a black box. all infer attack we 
demonstr in thi paper be perform entir through the 
services’ standard apis. 

iii. privaci IN machin learn 

befor deal with infer attacks, we need to defin 
what privaci mean in the context of machin learn or, 

1https://cloud.google.com/predict 
2https://aws.amazon.com/machine-learn 
3https://studio.azureml.net 
4https://bigml.com 
5https://cloud.google.com/prediction/docs/galleri 

2 



alternatively, what it mean for a machin learn model to 
breach privacy. 

A. infer about member of the popul 
A plausibl notion of privacy, know in statist disclosur 

control a the “daleniu desideratum,” state that the model 
should reveal no more about the input to which it be appli 
than would have be know about thi input without appli 
the model. thi cannot be achiev by ani use model [14]. 

A relat notion of privaci appear in prior work on model 
invers [17]: a privaci breach occur if an adversari can 
use the model’ output to infer the valu of unintend 
(sensitive) attribut use a input to the model. As observ 
in [27], it may not be possibl to prevent thi “breach” if 
the model be base on statist fact about the population. 
for example, suppos that train the model have uncov 
a high correl between a person’ extern observ 
phenotyp featur and their genet predisposit to a certain 
disease. thi correl be now a publicli know scientif 
fact that allow anyon to infer inform about the person’ 
genom after observ that person. 

critically, thi correl appli to all member of a give 
population. therefore, the model breach “privacy” not just of 
the peopl whose data be use to creat the model, but also of 
other peopl from the same population, even those whose data 
be not use and whose ident may not even be know to 
the model’ creator (i.e., thi be “spooki action at a distance”). 
valid model generalize, i.e., they make accur predict 
on input that be not part of their train datasets. thi 
mean that the creator of a generaliz model cannot do 
anyth to protect “privacy” a defin abov becaus the 
correl on which the model be based—and the infer 
that these correl enable—hold for the entir population, 
regardless of how the train sampl be chosen or how the 
model be creat from thi sample. 

B. infer about member of the train dataset 
To bypass the difficulti inher in defin and protect 

privaci of the entir population, we focu on protect privaci 
of the individu whose data be use to train the model. thi 
motiv be close relat to the origin goal of differenti 
privaci [13]. 

Of course, member of the train dataset be member 
of the population, too. We investig what the model reveal 
about them beyond what it reveal about an arbitrari member 
of the population. our ultim goal be to measur the mem- 
bership risk that a person incur if they allow their data to be 
use to train a model. 

the basic attack in thi set be membership inference, 
i.e., determin whether a give data record be part of the 
model’ train dataset or not. when a record be fulli know 
to the adversary, learn that it be use to train a particular 
model be an indic of inform leakag through the 
model. In some cases, it can directli lead to a privaci breach. 
for example, know that a certain patient’ clinic record 
be use to train a model associ with a diseas (e.g, to 

determin the appropri medicin dosag or to discov the 
genet basi of the disease) can reveal that the patient have thi 
disease. 

We investig the membership infer problem in the 
black-box scenario where the adversari can onli suppli input 
to the model and receiv the model’ output(s). In some 
situations, the model be avail to the adversari indirectly. 
for example, an app develop may use a machine-learn 
servic to construct a model from the data collect by the app 
and have the app make api call to the result model. In thi 
case, the adversari would suppli input to the app (rather than 
directli to the model) and receiv the app’ output (which be 
base on the model’ outputs). the detail of intern model 
usag vari significantli from app to app. for simplic and 
generality, we will assum that the adversari directli suppli 
input to and receiv output from the black-box model. 

iv. problem statement 
consid a set of label data record sampl from some 

popul and partit into classes. We assum that a 
machin learn algorithm be use to train a classif 
model that captur the relationship between the content of 
the data record and their labels. 

for ani input data record, the model output the predict 
vector of probabilities, one per class, that the record belong 
to a certain class. We will also refer to these probabl 
a confid values. the class with the high confid 
valu be select a the predict label for the data record. 
the accuraci of the model be evalu by measur how it 
gener beyond it train set and predict the label of 
other data record from the same population. 

We assum that the attack have queri access to the model 
and can obtain the model’ predict vector on ani data 
record. the attack know the format of the input and 
output of the model, includ their number and the rang of 
valu they can take. We also assum that the attack either 
(1) know the type and architectur of the machin learn 
model, a well a the train algorithm, or (2) have black-box 
access to a machin learn oracl (e.g., a “machin learn 
a a service” platform) that be use to train the model. In 
the latter case, the attack do not know a priori the model’ 
structur or meta-parameters. 

the attack may have some background knowledg about 
the popul from which the target model’ train dataset 
be drawn. for example, he may have independ drawn 
sampl from the population, disjoint from the target model’ 
train dataset. alternatively, the attack may know some 
gener statist about the population, for example, the 
margin distribut of featur values. 

the set for our infer attack be a follows. the 
attack be give a data record and black-box queri access 
to the target model. the attack succeed if the attack can 
correctli determin whether thi data record be part of the 
model’ train dataset or not. the standard metric for attack 
accuraci be precis (what fraction of record infer a 
member be inde member of the train dataset) and 

3 



(data record, class label) target model 

attack model 

data 2 train set ? 

predict(data) 

label 

predict 

fig. 1: membership infer attack in the black-box setting. the 
attack queri the target model with a data record and obtain 
the model’ predict on that record. the predict be a vector of 
probabilities, one per class, that the record belong to a certain class. 
thi predict vector, along with the label of the target record, be 
pass to the attack model, which infer whether the record be in 
or out of the target model’ train dataset. 

ML api 

privat train set target model 

shadow train set 1 

shadow model 1 

shadow train set 2 

shadow model 2 

. 
. 
. 

. 
. 
. 

shadow train set k 
shadow model k 

train() 

train() 

train() 

train() 

fig. 2: train shadow model use the same machin learn 
platform a be use to train the target model. the train dataset 
of the target and shadow model have the same format but be disjoint. 
the train dataset of the shadow model may overlap. all models’ 
intern paramet be train independently. 

recal (what fraction of the train dataset’ member be 
correctli infer a member by the attacker). 

V. membership infer 

A. overview of the attack 

our membership infer attack exploit the observ 
that machin learn model often behav differ on the 
data that they be train on versu the data that they “see” 
for the first time. overfit be a common reason but not the 
onli one (see section vii). the object of the attack be to 
construct an attack model that can recogn such differ 
in the target model’ behavior and use them to distinguish 
member from non-memb of the target model’ train 
dataset base sole on the target model’ output. 

our attack model be a collect of models, one for each 
output class of the target model. thi increas accuraci of the 

attack becaus the target model produc differ distribut 
over it output class depend on the input’ true class. 

To train our attack model, we build multipl “shadow” 
model intend to behav similarli to the target model. In 
contrast to the target model, we know the ground truth for each 
shadow model, i.e., whether a give record be in it train 
dataset or not. therefore, we can use supervis train on 
the input and the correspond output (each label “in” or 
“out”) of the shadow model to teach the attack model how to 
distinguish the shadow models’ output on member of their 
train dataset from their output on non-members. 

formally, let f 
target 

() be the target model, and let dtrain 
target 

be it privat train dataset which contain label data 
record (x{i}, y{i}) 

target 

. A data record x{i} 
target 

be the input to 
the model, and y{i} 

target 

be the true label that can take valu 
from a set of class of size c 

target 

. the output of the target 
model be a probabl vector of size c 

target 

. the element of 
thi vector be in [0, 1] and sum up to 1. 

let f 
attack 

() be the attack model. it input x 
attack 

be com- 
pose of a correctli label record and a predict vector 
of size c 

target 

. sinc the goal of the attack be decision 
membership inference, the attack model be a binari classifi 
with two output classes, “in” and “out.” 

figur 1 illustr our end-to-end attack process. for a 
label record (x, y), we use the target model to comput 
the predict vector y = f 

target 

(x). the distribut of y 
(classif confid values) depend heavili on the true 
class of x. thi be whi we pa the true label y of x in 
addit to the model’ predict vector y to the attack 
model. given how the probabl in y be distribut around 
y, the attack model comput the membership probabl 
pr{(x, y) 2 dtrain 

target 

}, i.e., the probabl that ((x, y),y) 
belong to the “in” class or, equivalently, that x be in the 
train dataset of f 

target 

(). 
the main challeng be how to train the attack model to 

distinguish member from non-memb of the target model’ 
train dataset when the attack have no inform about the 
intern paramet of the target model and onli limit queri 
access to it through the public api. To solv thi conundrum, 
we develop a shadow train techniqu that let u train 
the attack model on proxi target for which we do know the 
train dataset and can thu perform supervis training. 

B. shadow model 
the attack creat k shadow model f i 

shadow 

(). each 
shadow model i be train on a dataset dtrain 

shadow 

i of the same 
format a and distribut similarli to the target model’ train- 
ing dataset. these shadow train dataset can be gener 
use one of method describ in section v-c. We assum 
that the dataset use for train the shadow model be 
disjoint from the privat dataset use to train the target model 
(8i,dtrain 

shadow 

i \ dtrain 
target 

= ;). thi be the bad case for the 
attacker; the attack will perform even good if the train 
dataset happen to overlap. 

the shadow model must be train in a similar way to 
the target model. thi be easi if the target’ train algorithm 

4 



algorithm 1 data synthesi use the target model 
1: procedur synthesize(class : c) 
2: x randrecord(.) . initi a record randomli 
3: y⇤ 

c 

0 
4: j 0 
5: k k 

max 

6: for iter = 1 · · · iter 
max 

do 
7: y f 

target 

(x) . queri the target model 
8: if y 

c 

� y⇤ 
c 

then . accept the record 
9: if y 

c 

> conf 

min 

and c = argmax(y) then 
10: if rand() < y 

c 

then . sampl 
11: return x . synthet data 
12: end if 
13: end if 
14: x⇤ x 
15: y⇤ 

c 

y 
c 

16: j 0 
17: els 
18: j j + 1 
19: if j > rej 

max 

then . mani consecut reject 
20: k max(k 

min 

, dk/2e) 
21: j 0 
22: end if 
23: end if 
24: x randrecord(x⇤, k) . random k featur 
25: end for 
26: return ? . fail to synthes 
27: end procedur 

(e.g., neural networks, svm, logist regression) and model 
structur (e.g., the wire of a neural network) be known. 
machin learn a a servic be more challenging. here the 
type and structur of the target model be not known, but 
the attack can use exactli the same servic (e.g., googl 
predict api) to train the shadow model a be use to 
train the target model—se figur 2. 

the more shadow models, the more accur the attack 
model will be. As describ in section v-d, the attack model 
be train to recogn differ in shadow models’ behavior 
when these model oper on input from their own train 
dataset versu input they do not encount dure training. 
therefore, more shadow model provid more train fodder 
for the attack model. 

C. gener train data for shadow model 

To train shadow models, the attack need train data 
that be distribut similarli to the target model’ train data. 
We develop sever method for gener such data. 

model-bas synthesis. If the attack do not have real 
train data nor ani statist about it distribution, he can 
gener synthet train data for the shadow model use 
the target model itself. the intuit be that record that be 
classifi by the target model with high confid should 

be statist similar to the target’ train dataset and thu 
provid good fodder for shadow models. 

the synthesi process run in two phases: (1) search, use 
a hill-climb algorithm, the space of possibl data record 
to find input that be classifi by the target model with high 
confidence; (2) sampl synthet data from these records. after 
thi process synthes a record, the attack can repeat it until 
the train dataset for shadow model be full. 

see algorithm 1 for the pseudocod of our synthesi 
procedure. first, fix class c for which the attack want to 
gener synthet data. the first phase be an iter process. 
start by randomli initi a data record x. assum that 
the attack know onli the syntact format of data records, 
sampl the valu for each featur uniformli at random from 
among all possibl valu of that feature. In each iteration, 
propos a new record. A propos record be accept onli 
if it increas the hill-climb objective: the probabl of 
be classifi by the target model a class c. 

each iter involv propos a new candid record by 
chang k randomli select featur of the late accept 
record x⇤. thi be do by flip binari featur or resam- 
pling new valu for featur of other types. We initi k to 
k 

max 

and divid it by 2 when rej 
max 

subsequ propos 
be rejected. thi control the diamet of search around the 
accept record in order to propos a new record. We set the 
minimum valu of k to k 

min 

. thi control the speed of the 
search for new record with a potenti high classif 
probabl y 

c 

. 
the second, sampl phase start when the target model’ 

probabl y 
c 

that the propos data record be classifi a 
belong to class c be larg than the probabl for all 
other class and also larg than a threshold conf 

min 

. thi 
ensur that the predict label for the record be c, and that the 
target model be suffici confid in it label prediction. We 
select such record for the synthet dataset with probabl y⇤ 

c 

and, if select fails, repeat until a record be selected. 
thi synthesi procedur work onli if the adversari can 

effici explor the space of possibl input and discov 
input that be classifi by the target model with high confi- 
dence. for example, it may not work if the input be high- 
resolut imag and the target model perform a complex 
imag classif task. 

statistics-bas synthesis. the attack may have some statis- 
tical inform about the popul from which the target 
model’ train data be drawn. for example, the attack 
may have prior knowledg of the margin distribut of 
differ features. In our experiments, we gener synthet 
train record for the shadow model by independ 
sampl the valu of each featur from it own margin 
distribution. the result attack model be veri effective. 

noisi real data. the attack may have access to some data 
that be similar to the target model’ train data and can be 
consid a a “noisy” version thereof. In our experi 
with locat datasets, we simul thi by flip the (bi- 
nary) valu of 10% or 20% randomli select features, then 

5 



(data record, class label) predict(data) (prediction, class label, “in” / “out”) 

shadow train set 1 

shadow test set 1 

shadow model 1 “in” predict set 1 

“out” predict set 1 

·· 
· 

·· 
· 

·· 
· 

shadow train set k 

shadow test set k 

shadow model k “in” predict set k 

“out” predict set k 

attack train set 

attack model 

train() 

fig. 3: train the attack model on the input and output of the shadow models. for all record in the train dataset of a shadow model, 
we queri the model and obtain the output. these output vector be label “in” and add to the attack model’ train dataset. We also 
queri the shadow model with a test dataset disjoint from it train dataset. the output on thi set be label “out” and also add to the 
attack model’ train dataset. have construct a dataset that reflect the black-box behavior of the shadow model on their train and 
test datasets, we train a collect of c 

target 

attack models, one per each output class of the target model. 

train our shadow model on the result noisi dataset. 
thi scenario model the case where the train data for the 
target and shadow model be not sampl from exactli the 
same population, or els sampl in a non-uniform way. 

D. train the attack model 

the main idea behind our shadow train techniqu be that 
similar model train on rel similar data record use 
the same servic behav in a similar way. thi observ be 
empir born out by our experi in the rest of thi 
paper. our result show that learn how to infer membership 
in shadow models’ train dataset (for which we know the 
ground truth and can easili comput the cost function dure 
supervis training) produc an attack model that success 
infer membership in the target model’ train dataset, too. 

We queri each shadow model with it own train dataset 
and with a disjoint test set of the same size. the output on 
the train dataset be label “in,” the rest be label “out.” 
now, the attack have a dataset of records, the correspond 
output of the shadow models, and the in/out labels. the 
object of the attack model be to infer the label from the 
record and correspond outputs. 

figur 3 show how to train the attack model. for all 
(x, y) 2 dtrain 

shadow 

i , comput the predict vector y = 
f 

i 

shadow 

(x) and add the record (y,y, in) to the attack train 
set dtrain 

attack 

. let dtest 
shadow 

i be a set of record disjoint from the 
train set of the ith shadow model. then, 8(x, y) 2 dtest 

shadow 

i 

comput the predict vector y = f i 
shadow 

(x) and add the 
record (y,y, out) to the attack train set dtrain 

attack 

. finally, 
split dtrain 

attack 

into c 
target 

partitions, each associ with a 
differ class label. for each label y, train a separ model 
that, give y, predict the in or out membership statu for x. 

If we use model-bas synthesi from section v-c, all of 
the raw train data for the attack model be drawn from 
the record that be classifi by the target model with high 
confidence. thi be true, however, both for the record use in 
the shadow models’ train dataset and for the test record 
left out of these datasets. therefore, it be not the case that 
the attack model simpli learn to recogn input that be 
classifi with high confidence. instead, it learn to perform 
a much subtler task: how to distinguish between the train 
input classifi with high confid and other, non-train 
input that be also classifi with high confidence. 

In effect, we convert the problem of recogn the com- 
plex relationship between member of the train dataset and 
the model’ output into a binari classif problem. binari 
classif be a standard machin learn task, thu we can 
use ani state-of-the-art machin learn framework or servic 
to build the attack model. our approach be independ of the 
specif method use for attack model training. for example, 
in section VI we construct the attack model use neural 
network and also use the same black-box googl predict 
api that we be attacking, in which case we have no control 
over the model structure, model parameters, or train meta- 
parameters—but still obtain a work attack model. 

vi. evalu 

We first describ the dataset that we use for evaluation, 
follow by the descript of the target model and our exper- 
iment setup. We then present the result of our membership 
infer attack in sever set and studi in detail how and 
whi the attack work against differ dataset and machin 
learn platforms. 

6 



A. data 

cifar. cifar-10 and cifar-100 be benchmark dataset 
use to evalu imag recognit algorithm [24]. cifar-10 
be compos of 32⇥32 color imag in 10 classes, with 6, 000 
imag per class. In total, there be 50, 000 train imag 
and 10, 000 test images. cifar-100 have the same format a 
cifar-10, but it have 100 class contain 600 imag each. 
there be 500 train imag and 100 test imag per 
class. We use differ fraction of thi dataset in our attack 
experi to show the effect of the train dataset size on 
the accuraci of the attack. 
purchases. our purchas dataset be base on kaggle’ “ac- 
quir valu shoppers” challeng dataset that contain shop- 
ping histori for sever thousand individuals.6 the purpos 
of the challeng be to design accur coupon promot 
strategies. each user record contain hi or her transact 
over a year. the transact includ mani field such a 
product name, store chain, quantity, and date of purchase. 

for our experiments, we deriv a simplifi purchas 
dataset (with 197, 324 records), where each record consist 
of 600 binari features. each featur correspond to a product 
and repres whether the user have purchas it or not. To 
design our classif tasks, we first cluster the record 
into multipl classes, each repres a differ purchas 
style. In our experiments, we use 5 differ classif 
task with a differ number of class {2, 10, 20, 50, 100}. 
the classif task be to predict the purchas style of a 
user give the 600-featur vector. We use 10, 000 randomli 
select record from the purchas dataset to train the target 
model. the rest of the dataset contribut to the test set and 
(if necessary) the train set of the shadow models. 
locations. We creat a locat dataset from the publicli 
avail set of mobil users’ locat “check-ins” in the 
foursquar social network, restrict to the bangkok area 
and collect from april 2012 to septemb 2013 [36].7 the 
check-in dataset contain 11, 592 user and 119, 744 locations, 
for a total of 1, 136, 481 check-ins. We filter out user with 
few than 25 check-in and venu with few than 100 visits, 
which left u with 5, 010 user profiles. for each locat venue, 
we have the geograph posit a well a it locat type 
(e.g., indian restaurant, fast food, etc.). the total number of 
locat type be 128. We partit the bangkok map into area 
of size 0.5km ⇥ 0.5km, yield 318 region for which we 
have at least one user check-in. 

each record in the result dataset have 446 binari features, 
repres whether the user visit a certain region or 
locat type, i.e., the user’ semant and geograph profile. 
the classif task be similar to the purchas dataset. We 
cluster the locat dataset into 30 classes, each repres 
a differ geosoci type. the classif task be to predict 
the user’ geosoci type give hi or her record. We use 1, 600 
randomli select record to train the target model. the rest 

6https://kaggle.com/c/acquire-valued-shoppers-challenge/data 
7https://sites.google.com/site/yangdingqi/home/foursquare-dataset 

of the dataset contribut to the test set and (if necessary) the 
train set of the shadow models. 
texa hospit stays. thi dataset be base on the hospit 
discharg data public use file with inform about inpa- 
tient stay in sever health facilities,8 releas by the texa 
depart of state health servic from 2006 to 2009. each 
record contain four main group of attributes: the extern 
caus of injuri (e.g., suicide, drug misuse), the diagnosi 
(e.g., schizophrenia, illeg abortion), the procedur the pa- 
tient underw (e.g., surgery) and some gener inform 
such a the gender, age, race, hospit id, and length of stay. 

our classif task be to predict the patient’ main proce- 
dure base on the attribut other than secondari procedures. 
We focu on the 100 most frequent procedures. the result 
dataset have 67, 330 record and 6, 170 binari features. We use 
10, 000 randomli select record to train the target model. 

note that our experi do not involv re-identif 
of know individu and fulli compli with the data use 
agreement for the origin public use data file. 
mnist. thi be a dataset of 70, 000 handwritten digit 
format a 32 ⇥ 32 imag and normal so that the 
digit be locat at the center of the image.9 We use 10, 000 
randomli select imag to train the target model. 
uci adult (censu income). thi dataset includ 48, 842 
record with 14 attribut such a age, gender, education, 
marit status, occupation, work hours, and nativ country. 
the (binary) classif task be to predict if a person make 
over $50k a year base on the censu attributes.10 We use 
10, 000 randomli select record to train the target model. 

B. target model 

We evalu our infer attack on three type of target 
models: two construct by cloud-bas “machin learn a 
a service” platform and one we implement locally. In all 
cases, our attack treat the model a black boxes. for the 
cloud services, we do not know the type or structur of the 
model they create, nor the valu of the hyper-paramet 
use dure the train process. 
machin learn a a service. the first cloud-bas machin 
learn servic in our studi be googl predict api. with 
thi service, the user upload a dataset and obtain an api 
for queri the result model. there be no configur 
paramet that can be chang by the user. 

the other cloud servic be amazon ml. the user cannot 
choos the type of the model but can control a few meta- 
parameters. In our experiments, we vari the maximum num- 
ber of pass over the train data and L2 regular 
amount. the former determin the number of train epoch 
and control the converg of model training; it default 
valu be 10. the latter tune how much regular be per- 
form on the model paramet in order to avoid overfitting. 

8https://www.dshs.texas.gov/thcic/hospitals/download.shtm 
9http://yann.lecun.com/exdb/mnist 
10http://archive.ics.uci.edu/ml/datasets/adult 

7 



0.5 

0.6 

0.7 

0.8 

0.9 

1 

1 2 3 4 5 6 7 8 9 10 

Pr 
ec 

be 
io 

n 

class 

cifar-10, cnn, membership infer attack 

2500 
5000 

10000 
15000 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 2000 4000 6000 8000 10000 12000 14000 

Pr 
ec 

be 
io 

n 

train set size 

cifar-10, cnn, membership infer attack 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 5000 10000 15000 20000 25000 30000 

Pr 
ec 

be 
io 

n 

train set size 

cifar-100, cnn, membership infer attack 

fig. 4: precis of the membership infer attack against neural network train on cifar datasets. the graph show precis for 
differ class while vari the size of the train datasets. the median valu be connect across differ train set sizes. the 
median precis (from the small dataset size to largest) be 0.78, 0.74, 0.72, 0.71 for cifar-10 and 1, 1, 0.98, 0.97 for cifar-100. recal 
be almost 1 for both datasets. the figur on the left show the per-class precis (for cifar-10). random guess accuraci be 0.5. 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

accuraci 

purchas dataset, amazon (10,1e-6), membership infer attack 

precis 
recal 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

accuraci 

purchas dataset, amazon (100,1e-4), membership infer attack 

precis 
recal 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

accuraci 

purchas dataset, google, membership infer attack 

precis 
recal 

fig. 5: empir cdf of the precis and recal of the membership infer attack against differ class of the model train use 
amazon ML (in two differ configurations) and googl predict api on 10, 000 purchas records. 50, 75, 90-percentil of precis be 
0.74, 0.79, 0.84 on amazon (10, 1e � 6), 0.84, 0.88, 0.91 on amazon (100, 1e � 4), and 0.94, 0.97, 1 on google, respectively. recal be 
close to 1. 

We use the platform in two configurations: the default set 
(10, 1e� 6) and (100, 1e� 4). 

neural networks. neural network have becom a veri 
popular approach to large-scal machin learning. We use 
torch7 and it nn packages,11 a deep-learn librari that have 
be use and extend by major internet compani such a 
facebook.12 

On cifar datasets, we train a standard convolut neural 
network (cnn) with two convolut and max pool layer 
plu a fulli connect layer of size 128 and a softmax layer. 
We use tanh a the activ function. We set the learn 
rate to 0.001, the learn rate decay to 1e � 07, and the 
maximum epoch of train to 100. 

On the purchas dataset (see section vi-a), we train a fulli 
connect neural network with one hidden layer of size 128 
and a softmax layer. We use tanh a the activ function. 
We set the learn rate to 0.001, the learn rate decay to 
1e� 07, and the maximum epoch of train to 200. 

11https://github.com/torch/nn 
12https://github.com/facebook/fblualib 

C. experiment setup 

the train set and the test set of each target and shadow 
model be randomli select from the respect datasets, have 
the same size, and be disjoint. there be no overlap between the 
dataset of the target model and those of the shadow models, 
but the dataset use for differ shadow model can overlap 
with each other. 

We set the train set size to 10, 000 for the purchas 
dataset a well a the texa hospital-stay dataset, adult dataset 
and the mnist dataset. We set it to 1, 200 for the locat 
dataset. We vari the size of the train set for the cifar 
datasets, to measur the differ in the attack accuracy. 
for the cifar-10 dataset, we choos 2, 500; 5, 000; 10, 000; 
and 15, 000. for the cifar-100 dataset, we choos 4, 600; 
10, 520; 19, 920; and 29, 540. 

the experi on the cifar dataset be run lo- 
cally, against our own models, so we can vari the model’ 
configur and measur the impact on the attack accu- 
racy. the experi on the other dataset (purchas with 
{2, 10, 20, 50, 100} classes, texa hospit stays, locations, 
adult, and mnist) be run against model train use 
either googl or amazon services, where we have no visibl 

8 



0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

accuraci 

texa dataset, google, membership infer attack 

precis 
recal 

fig. 6: precis and recal of the membership infer attack against 
the classif model train use googl predict api on the 
texa hospital-stay dataset. 

into their choic of the model type and structur and littl 
control over the train process (see section vi-b). 

for the purchas dataset, we built target model on all plat- 
form (google, amazon, local neural networks) employ the 
same train dataset, thu enabl u to compar the leakag 
from differ models. We use similar train architectur 
for the attack model across differ platforms: either a fulli 
connect neural network with one hidden layer of size 64 
with relu (rectifi linear units) activ function and a 
softmax layer, or a google-train black-box model. 

We set the number of shadow model to 100 for the cifar 
datasets, 20 for the purchas dataset, 10 for the texa hospital- 
stay dataset, 60 for the locat dataset, 50 for the mnist 
dataset, and 20 for the adult dataset. increas the number 
of shadow model would increas the accuraci of the attack 
but also it cost. 

D. accuraci of the attack 

the attacker’ goal be to determin whether a give record 
be part of the target model’ train dataset. We evalu 
thi attack by execut it on randomli reshuffl record from 
the target’ train and test datasets. In our attack evaluation, 
we use set of the same size (i.e, equal number of member 
and non-members) in order to maxim the uncertainti of 
inference, thu the baselin accuraci be 0.5. 

We evalu the attack use the standard precis and 
recal metrics. precis be the fraction of the record infer 
a member of the train dataset that be inde members. 
recal measur coverag of the attack, i.e., the fraction of 
the train record that the attack can correctli infer a 
members. most measur be report per class becaus 
the accuraci of the attack can vari consider for differ 
classes. thi be due to the differ in size and composit 
of the train data belong to each class and highli depend 
on the dataset. 

the test accuraci of our target neural-network model with 
the larg train dataset (15, 000 and 29, 540 records, 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

accuraci 

purchas dataset, membership infer attack 

googl 
amazon (100,1e-4) 
amazon (10,1e-6) 

neural network 

fig. 7: precis of the membership infer attack against model 
train on the same dataset but use differ platforms. the attack 
model be a neural network. 

respectively) be 0.6 and 0.2 for cifar-10 and cifar-100, 
respectively. the accuraci be low, indic that the model 
be heavili overfit on their train sets. figur 4 show 
the result of the membership infer attack against the 
cifar models. for both cifar-10 and cifar-100, the 
attack perform much good than the baseline, with cifar- 
100 especi vulnerable. 

tabl I show the train and test accuraci of the model 
construct use differ machin learn platform for the 
purchas dataset with 100 classes. larg gap between train 
and test accuraci indic overfitting. larger test accuraci 
indic good generaliz and high predict power. 

figur 5 show the result of the membership infer 
attack against the black-box model train by google’ and 
amazon’ machin learn platforms. figur 7 compar 
precis of the attack against these model with the attack 
against a neural-network model train on the same data. mod- 
el train use googl predict api exhibit the big 
leakage. 

for the texa hospital-stay dataset, we evalu our attack 
against a google-train model. the train accuraci of the 
target model be 0.66 and it test accuraci be 0.51. figur 6 
show the accuraci of membership inference. precis be 
mostli abov 0.6, and for half of the classes, it be abov 0.7. 
precis be abov 0.85 for more than 20 classes. 

for the locat dataset, we evalu our attack against 
a google-train model. the train accuraci of the target 
model be 1 and it test accuraci be 0.66. figur 8 show the 
accuraci of membership inference. precis be between 0.6 
and 0.8, with an almost constant recal of 1. 

E. effect of the shadow train data 

figur 8 report precis of the attack train on the 
shadow model whose train dataset be noisi version of 
the real data (disjoint from the target model’ train dataset 
but sampl from the same population). precis drop a the 
amount of nois increases, but the attack still outperform the 

9 



ML platform train test 
googl 0.999 0.656 
amazon (10,1e-6) 0.941 0.468 
amazon (100,1e-4) 1.00 0.504 
neural network 0.830 0.670 

tabl I: train and test accuraci of the model construct use 
differ ml-as-a-servic platform on the purchas dataset (with 100 
classes). 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

precis 

locat dataset, google, membership infer attack 

real data 
noisi data 10% 
noisi data 20% 

fig. 8: empir cdf of the precis of the membership infer 
attack against the google-train model for the locat dataset. 
result be show for the shadow model train on real data and for 
the shadow model train on noisi data with 10% and 20% nois 
(i.e., x% of featur be replac with random values). precis of 
the attack over all class be 0.678 (real data), 0.666 (data with 10% 
noise), and 0.613 (data with 20% noise). the correspond recal 
of the attack be 0.98, 0.99, and 1.00, respectively. 

baselin and, even with 10% of the featur in the shadows’ 
train data replac by random values, match the origin 
attack. thi demonstr that our attack be robust even 
if the attacker’ assumpt about the distribut of the 
target model’ train data be not veri accurate. 

figur 9 report precis of the attack when the attack 
have no real data (not even noisy) for train hi shadow mod- 
els. instead, we use the margin distribut of individu 
featur to gener 187, 300 synthet purchas records, then 
train 20 shadow model on these records. 

We also gener 30, 000 synthet record use the 
model-bas approach present in algorithm 1. In our ex- 
periment with the purchas dataset where record have 600 
binari features, we initi k to k 

max 

= 128 and divid it 
by 2 when rej 

max 

= 10 subsequ propos be rejected. 
We set it minimum valu k 

min 

= 4. In the sampl phase, 
we set the minimum confid threshold conf 

min 

to 0.2. 
for our final set of sampl records, the target model’ 

confid in classifi the record be 0.24 on averag (just 
a bit over our threshold conf 

min 

= 0.2). On average, each 
synthet record need 156 queri (of propos records) 
dure our hill-climb two-phas process (see section v-c). 
We train 8 shadow model on thi data. 

figur 9 compar precis of the attack when shadow 
model be train on real data versu shadow model train 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

precis 

purchas dataset, google, membership infer attack 

real data 
marginal-bas synthet 

model-bas synthet 

fig. 9: empir cdf of the precis of the membership infer 
attack against the google-train model for the purchas dataset. 
result be show for differ way of gener train data for 
the shadow model (real, synthet gener from the target model, 
synthet gener from margin statistics). precis of the attack 
over all class be 0.935 (real data), 0.795 (marginal-bas synthet 
data), and 0.896 (model-bas synthet data). the correspond 
recal of the attack be 0.994, 0.991, and 0.526, respectively. 

on synthet data. the overal precis be 0.935 on real data 
compar to 0.795 for marginal-bas synthet and 0.895 
for model-bas synthetics. the accuraci of the attack use 
marginal-bas synthet data be notic reduc versu real 
data, but be nevertheless veri high for most classes. the attack 
use model-bas synthet data exhibit dual behavior. for 
most class it precis be high and close to the attack 
that use real data for shadow training, but for a few class 
precis be veri low (less than 0.1). 

the reason for the attack’ low precis on some class 
be that the target classifi cannot confid model the dis- 
tribut of data record belong to these classes—becaus 
it have not see enough examples. these class be under- 
repres in the target model’ train dataset. for example, 
each of the class where the attack have less than 0.1 precis 
contribut under 0.6% of the target model’ train dataset. 
some of these class have few than 30 train record (out 
of 10, 000). thi make it veri difficult for our algorithm to 
synthes repres of these class when search the 
high-dimension space of possibl records. 

for the major of the target model’ classes, our attack 
achiev high precision. thi demonstr that a membership 
infer attack can be train with onli black-box access 
to the target model, without ani prior knowledg about 
the distribut of the target model’ train data if the 
attack can effici gener input that be classifi by 
the target model with high confidence. 

F. effect of the number of class and train data per class 

the number of output class of the target model contribut 
to how much the model leaks. the more classes, the more 
signal about the intern state of the model be avail to 
the attacker. thi be one of the reason whi the result in fig. 4 

10 



0.5 

0.6 

0.7 

0.8 

0.9 

1 

2 10 20 50 100 

At 
ta 

ck 
P 

re 
ci 

si 
on 

number of class 

purchas dataset, google, membership infer attack 

fig. 10: precis of the membership infer attack against differ- 
ent purchas classif model train on the googl platform. 
the boxplot show the distribut of precis over differ classi- 
ficat task (with a differ number of classes). 

be good for cifar-100 than for cifar-10. the cifar- 
100 model be also more overfit to it train dataset. for 
the same number of train record per class, the attack 
perform good against cifar-100 than against cifar-10. 
for example, compar cifar-10 when the size of the train 
dataset be 2, 000 with cifar-100 when the size of the train 
dataset be 20, 000. the averag number of data record per 
class be 200 in both cases, but the attack accuraci be much 
good (close to 1) for cifar-100. 

To quantifi the effect that the number of class have 
on the accuraci of the attack, we train target model 
use googl predict api on the purchas dataset with 
{2, 10, 20, 50, 100} classes. figur 10 show the distribut 
of attack precis for each model. model with few class 
leak less inform about their train inputs. As the 
number of class increases, the model need to extract more 
distinct featur from the data to be abl to classifi input 
with high accuracy. informally, model with more output 
class need to rememb more about their train data, thu 
they leak more information. 

figur 11 show the relationship between the amount of 
train data per class and the accuraci of membership infer- 
ence. thi relationship be more complex, but, in general, the 
more data in the train dataset be associ with a give 
class, the low the attack precis for that class. 

tabl II show the precis of membership infer 
against google-train models. for the mnist dataset, the 
train accuraci of the target model be 0.984 and it test 
accuraci be 0.928. the overal precis of the membership 
infer attack be 0.517, which be just slightli abov random 
guessing. the lack of random in the train data for each 
class and the small number of class contribut to the failur 
of the attack. 

for the adult dataset, the train accuraci of the target 
model be 0.848 and it test accuraci be 0.842. the overal 
precis of the attack be 0.503, which be equival to random 

dataset train test attack 
accuraci accuraci precis 

adult 0.848 0.842 0.503 
mnist 0.984 0.928 0.517 
locat 1.000 0.673 0.678 
purchas (2) 0.999 0.984 0.505 
purchas (10) 0.999 0.866 0.550 
purchas (20) 1.000 0.781 0.590 
purchas (50) 1.000 0.693 0.860 
purchas (100) 0.999 0.659 0.935 
TX hospit stay 0.668 0.517 0.657 

tabl ii: accuraci of the google-train model and the corre- 
spond attack precision. 

guessing. there could be two reason for whi membership 
infer fail against thi model. first, the model be not 
overfit (it test and train accuraci be almost the same). 
second, the model be a binari classifier, which mean that the 
attack have to distinguish member from non-memb by 
observ the behavior of the model on essenti 1 signal, 
sinc the two output be complement of each other. thi 
be not enough for our attack to extract use membership 
inform from the model. 

G. effect of overfit 
the more overfit a model, the more it leaks—but onli 

for model of the same type. for example, the amazon- 
train (100, 1e�4) model that, accord to tabl I, be more 
overfit leak more than the amazon-train (10, 1e � 6) 
model. however, they both leak less than the google-train 
model, even though the googl model be less overfit than 
one of the amazon model and have a much good predict 
power (and thu generalizability) than both amazon models. 
therefore, overfit be not the onli factor that caus 
a model to be vulner to membership inference. the 
structur and type of the model also contribut to the problem. 

In figur 11, we look deeper into the factor that contribut 
to attack accuraci per class, includ how overfit the 
model be and what fraction of the train data belong to each 
class. the (train-test) accuraci gap be the differ between 
the accuraci of the target model on it train and test data. 
similar metric be use in the literatur to measur how 
overfit a model be [18]. We comput thi metric for each 
class. bigger gap indic that the model be overfit on it 
train data for that class. the plot show that, a expected, 
big (train-test) accuraci gap be associ with high 
precis of membership inference. 

vii. whi our attack work 
tabl II show the relationship between the accuraci of 

our membership infer attack and the (train-test) gap of 
the target models. figur 12 also illustr how the target 
models’ output distinguish member of their train dataset 
from the non-members. thi be the inform that our attack 
exploits. 

specifically, we look at how accur the model predict 
the correct label a well a it predict uncertainty. the ac- 
curaci for class i be the probabl that the model classifi an 

11 



0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 

At 
ta 

ck 
P 

re 
ci 

si 
on 

fraction of the train set for a class 

purchas dataset, 10-100 classes, google, membership infer attack 

10 class 
20 class 
50 class 

100 class 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 

At 
ta 

ck 
P 

re 
ci 

si 
on 

target model (train-test) accuraci gap 

purchas dataset, 10-100 classes, google, membership infer attack 

10 class 
20 class 
50 class 

100 class 
0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 

Ta 
rg 

et 
M 

od 
el 

(T 
ra 

in 
-T 

e 
t) 

Ac 
cu 

ra 
cy 

G 
ap 

fraction of the train set for a class 

purchas dataset, 10-100 classes, google, membership infer attack 

10 class 
20 class 
50 class 

100 class 

fig. 11: relationship between the precis of the membership infer attack on a class and the (train-test) accuraci gap of the target 
model, a well a the fraction of the train dataset that belong to thi class. each point repres the valu for one class. the (train-test) 
accuraci gap be a metric for gener error [18] and an indic of how overfit the target model is. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
predict uncertainti 

purchas dataset, 10 classes, google, membership infer attack 

member 
non-memb 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
predict uncertainti 

purchas dataset, 20 classes, google, membership infer attack 

member 
non-memb 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
predict uncertainti 

purchas dataset, 100 classes, google, membership infer attack 

member 
non-memb 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
predict accuraci 

purchas dataset, 10 classes, google, membership infer attack 

member 
non-memb 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
predict accuraci 

purchas dataset, 20 classes, google, membership infer attack 

member 
non-memb 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
predict accuraci 

purchas dataset, 100 classes, google, membership infer attack 

member 
non-memb 

fig. 12: classif uncertainti (top row) and predict accuraci (bottom row) of the target model for the member of it train dataset 
vs. non-members, visual for sever sampl classes. the differ between the member and non-memb output distribut be among 
the factor that our attack exploit to infer membership. the accuraci of our attack be high for the model where the two distribut be 
more distinguish (see tabl ii). 

input with label i a i. predict uncertainti be the normal 
entropi of the model’ predict vector: �1 

log(n) 

P 
i 

p 

i 

log(p 

i 

), 
where p 

i 

be the probabl that the input belong to class i, 
and n be the number of classes. the plot show that there 
be an observ differ between the output (both accuraci 
and uncertainty) of the model on the member input versu the 
non-memb input in the case where our attack be successful. 

success of membership infer be directli relat to the 
(1) generaliz of the target model and (2) divers of it 
train data. If the model overfit and do not gener well 
to input beyond it train data, or if the train data be not 
representative, the model leak inform about it train 
inputs. We quantifi thi relationship in fig. 11. from the 
machin learn perspective, overfit be harm becaus 
it produc model that lack predict power. In thi paper, 
we show anoth harm of overfitting: the leakag of sensit 
inform about the train data. 

As we explain in section vi, overfit be not the onli 
reason whi our infer attack work. differ machin 
learn models, due to their differ structures, “remember” 
differ amount of inform about their train datasets. 
thi lead to differ amount of inform leakag even if 
the model be overfit to the same degre (see tabl i). 

viii. mitig 

As explain in section vii, overfit be an import 
(but not the only) reason whi machin learn model leak 
inform about their train datasets. Of course, overfit 
be a canon problem in machin learn becaus it limit 
the predict power and generaliz of models. thi 
mean that instead of the usual tradeoff between util and 
privacy, machin learn research and privaci research have 
similar object in thi case. regular techniqu such 
a dropout [31] can help defeat overfit and also strengthen 

12 



privaci guarante in neural network [23]. regular be 
also use for object perturb in differenti privat 
machin learn [9]. 

(ideal) well-regular model should not leak much infor- 
mation about their train data, and our attack can serv a 
a metric to quantifi this. also, model with a trivial structur 
(e.g., xor of some input features) gener to the entir 
univers and do not leak information. 

If the train process be differenti privat [12], the 
probabl of produc a give model from a train dataset 
that includ a particular record be close to the probabl of 
produc the same model when thi record be not included. 
differenti privat model are, by construction, secur 
against membership infer attack of the kind develop in 
thi paper becaus our attack oper sole on the output of 
the model, without ani auxiliari information. one obstacl be 
that differenti privat model may significantli reduc the 
model’ predict accuraci for small ✏ values. In section ix, 
we survey some of the relat work in thi area. 

In the case of machin learn a a service, platform 
oper such a googl and amazon have signific re- 
sponsibl to the user of their services. In their current 
form, these servic simpli accept the data, produc a model 
of unknown type and structure, and return an opaqu api to 
thi model that data owner use a they see fit, without ani 
understand that by do so, they may be leak out their 
data. machin learn servic do not inform their custom 
about the risk of overfit or the harm that may result 
from model train on inadequ dataset (for example, with 
unrepres record or too few repres for certain 
classes). 

instead, when adapt choos a model for a customer- 
suppli dataset, servic such a googl predict api and 
amazon ML should take into account not onli the accuraci of 
the model but also the risk that it will leak inform about 
it train data. furthermore, they need to explicitli warn 
custom about thi risk and provid more visibl into the 
model and the method that can be use to reduc thi leakage. 
our infer attack can be use a metric to quantifi 
leakag from a specif model, and also to measur the 
effect of futur privaci protect techniqu deploy 
by machine-learn services. 

A. mitig strategi 
We quantit evalu sever defens against mem- 

bership inference. 
restrict the predict vector to top k classes. when the 
number of class be large, mani class may have veri small 
probabl in the model’ predict vector. the model will 
still be use if it onli output the probabl of the most 
like k classes. To implement this, we add a filter to the last 
layer of the model. the small k is, the less inform the 
model leaks. In the extrem case, the model return onli the 
label of the most like class without report it probability. 
coarsen precis of the predict vector. To implement 
this, we round the classif probabl in the predict 

purchas dataset test attack attack attack 
accuraci total accuraci precis recal 

No mitig 0.66 0.92 0.87 1.00 
top k = 3 0.66 0.92 0.87 0.99 
top k = 1 0.66 0.89 0.83 1.00 
top k = 1 label 0.66 0.66 0.60 0.99 
round d = 3 0.66 0.92 0.87 0.99 
round d = 1 0.66 0.89 0.83 1.00 
temperatur t = 5 0.66 0.88 0.86 0.93 
temperatur t = 20 0.66 0.84 0.83 0.86 
L2 � = 1e� 4 0.68 0.87 0.81 0.96 
L2 � = 1e� 3 0.72 0.77 0.73 0.86 
L2 � = 1e� 2 0.63 0.53 0.54 0.52 

hospit dataset test attack attack attack 
accuraci total accuraci precis recal 

No mitig 0.55 0.83 0.77 0.95 
top k = 3 0.55 0.83 0.77 0.95 
top k = 1 0.55 0.82 0.76 0.95 
top k = 1 label 0.55 0.73 0.67 0.93 
round d = 3 0.55 0.83 0.77 0.95 
round d = 1 0.55 0.81 0.75 0.96 
temperatur t = 5 0.55 0.79 0.77 0.83 
temperatur t = 20 0.55 0.76 0.76 0.76 
L2 � = 1e� 4 0.56 0.80 0.74 0.92 
L2 � = 5e� 4 0.57 0.73 0.69 0.86 
L2 � = 1e� 3 0.56 0.66 0.64 0.73 
L2 � = 5e� 3 0.35 0.52 0.52 0.53 

tabl iii: the accuraci of the target model with differ mitiga- 
tion techniqu on the purchas and texa hospital-stay dataset (both 
with 100 classes), a well a total accuracy, precision, and recal of 
the membership infer attack. the rel reduct in the metric 
for the attack show the effect of the mitig strategy. 

vector down to d float point digits. the small d is, the 
less inform the model leaks. 
increas entropi of the predict vector. one of the signal 
that membership infer exploit be the differ between 
the predict entropi of the target model on it train input 
versu other inputs. As a mitig techniqu for neural- 
network models, we can modifi (or add) the softmax layer and 
increas it normal temperatur t > 0. the softmax layer 
convert the logit comput for each class into probabilities. 
for the logit vector z, the ith output of the softmax function 
with temperatur t be e 

zi/t 

P 
j e 

zj/t 
. thi technique, also use 

in knowledg distil and inform transfer between 
model [20], would increas the entropi of the predict 
vector. note that for a veri larg temperature, the output 
becom almost uniform and independ of it input, thu 
leak no information. 
use regularization. regular techniqu be use to 
overcom overfit in machin learning. We use L 

2 

-norm 
standard regular that penal larg paramet by 
add � 

P 
i 

✓ 

2 

i 

to the model’ loss function, where ✓ 
i 

s be 
model’ parameters. We implement thi techniqu with variou 
valu for the regular factor �. the larg � is, the 
strong the effect of regular dure the training. 

B. evalu of mitig strategi 

To evalu the effect of differ mitig strate- 
gies, we implement all of them in local train mod- 

13 



el over which we have full control. the infer attack, 
however, still assum onli black-box access to the result 
models. the baselin model for these experi be a neural 
network with one hidden layer with 256 unit (for the purchas 
dataset) and 1,000 unit (for the texa hospital-stay dataset). 
We use tanh a the activ function. 

tabl iii show the result of our evaluation. It compar 
differ mitig strategi base on how they degrad the 
accuraci of our attack rel to the attack on a model 
that do not use ani mitigation. the mitig strategi 
that we implement do not impos ani cost on the target 
model’ predict accuracy, and in the case of regularization, 
the target model’ predict accuraci increas a expected. 
note that more regular (bi increas � even further) 
would potenti result in a signific reduct of the target 
model’ test accuracy, even if it foil membership inference. 
thi be show in the tabl for � = 1e � 2 on the purchas 
dataset, and for � = 5e�3 on the texa hospit stay dataset. 

overall, our attack be robust against these mitig strate- 
gies. filter out low-prob class from the predic- 
tion vector and limit the vector to the top 1 or 3 most 
like class do not foil the attack. even restrict the 
predict vector to a singl label (most like class), 
which be the absolut minimum a model must output to 
remain useful, be not enough to fulli prevent membership 
inference. our attack can still exploit the mislabel behavior 
of the target model becaus member and non-memb of 
the train dataset be mislabel differ (assign to 
differ wrong classes). If the predict vector contain 
probabl in addit to the labels, the model leak even 
more inform that can be use for membership inference. 

some of the mitig method be not suitabl for 
machine-learning-as-servic api use by gener applic 
and services. regularization, however, appear to be neces- 
sari and useful. As mention above, it (1) gener the 
model and improv it predict power and (2) decreas 
the model’ inform leakag about it train dataset. 
however, regular need to be deploy care to 
avoid damag the model’ perform on the test datasets. 

ix. relat work 

attack on statist and machin learn models. In [2], 
knowledg of the paramet of svm and hmm model be 
use to infer gener statist inform about the train 
dataset, for example, whether record of a particular race be 
use dure training. By contrast, our infer attack work 
in a black-box setting, without ani knowledg of the model’ 
parameters, and infer inform about specif record in the 
train dataset, a oppos to gener statistics. 

homer et al. [21] develop a technique, which be further 
studi in [3], [15], for infer the presenc of a particular 
genom in a dataset, base on compar the publish statis- 
tic about thi dataset (in particular, minor allel frequencies) 
to the distribut of these statist in the gener population. 
By contrast, our infer attack target train machin 
learn models, not explicit statistics. 

fig. 13: imag produc by model invers on a train cifar-10 
model. top: airplane, automobile, bird, cat, deer. bottom: dog, frog, 
horse, ship, truck. the imag do not correspond to ani specif 
imag from the train dataset, be not human-recognizable, and at 
best (e.g., the truck class image) be vagu similar to the averag 
imag of all object in a give class. 

other attack on machin learn includ [7], where the 
adversari exploit chang in the output of a collabor 
recommend system to infer input that caus these changes. 
these attack exploit tempor behavior specif to the recom- 
mender system base on collabor filtering. 

model inversion. model invers [16], [17] us the output 
of a model appli to a hidden input to infer certain featur 
of thi input. see [27] for a detail analysi of thi attack and 
an explan of whi it do not necessarili entail a privaci 
breach. for example, in the specif case of pharmacogenet 
analyz in [17], the model captur the correl between 
the patient’ genotyp and the dosag of a certain medicine. 
thi correl be a valid scientif fact that hold for all 
patients, regardless of whether they be includ in the 
model’ train dataset or not. It be not possibl to prevent 
disclosur due to popul statist [14]. 

In general, model invers cannot tell whether a particular 
record be use a part of the model’ train dataset. given 
a record and a model, model invers work exactli the same 
way when the record be use to train the model and when 
it be not used. In the case of pharmacogenet [17], model 
invers produc almost ident result for member and 
non-members. due to the overfit of the model, the result 
be a littl (4%) more accur for the members, but thi 
accuraci can onli be measur in retrospect, if the adversari 
alreadi know the ground truth (i.e., which record be inde 
member of the model’ train dataset). By contrast, our goal 
be to construct a decis procedur that distinguish member 
from non-members. 

model invers have also be appli to face recognit 
model [16]. In thi scenario, the model’ output be set to 1 
for class i and 0 for the rest, and model invers be use to 
construct an input that produc these outputs. thi input be 
not an actual member of the train dataset but simpli an 
averag of the featur that “characterize” the class. 

In the face recognit scenario—and onli in thi specif 
scenario—each output class of the model be associ with a 
singl person. all train imag for thi class be differ 
photo of that person, thu model invers construct an 
artifici imag that be an averag of these photos. becaus 
they all depict the same person, thi averag be recogniz 

14 



(bi a human) a that person. critically, model invers do 
not produc ani specif imag from the train dataset, which 
be the definit of membership inference. 

If the imag in a class be divers (e.g., if the class contain 
multipl individu or mani differ objects), the result of 
model invers a use in [16] be semant meaningless 
and not recogniz a ani specif imag from the train 
dataset. To illustr this, we ran model invers against 
a convolut neural network13 train on the cifar-10 
dataset, which be a standard benchmark for object recognit 
models. each class includ differ imag of a singl type 
of object (e.g., an airplane). figur 13 show the imag 
“reconstructed” by model inversion. As expected, they do not 
depict ani recogniz object, let alon an imag from the 
train dataset. We expect similar result for other models, 
too. for the pharmacogenet model mention above, thi 
form of model invers produc an averag of differ 
patients’ genomes. for the model that classifi locat trace 
into geosoci profil (see section vi-a), it produc an 
averag of the locat trace of differ people. In both 
cases, the result of model invers be not associ with 
ani specif individu or specif train input. 

In summary, model invers produc the averag of the 
featur that at best can character an entir output class. 
It do not (1) construct a specif member of the train 
dataset, nor (2) give an input and a model, determin if thi 
specif input be use to train the model. 
model extraction. model extract attack [32] aim to 
extract the paramet of a model train on privat data. 
the attacker’ goal be to construct a model whose predict 
perform on valid data be similar to the target model. 

model extract can be a step stone for infer 
inform about the model’ train dataset. In [32], thi be 
illustr for a specif type of model call kernel logist 
regress (klr) [38]. In klr models, the kernel function 
includ a tini fraction of the train data (so call “import 
points”) directli into the model. sinc import point be 
paramet of the model, extract them result in the leakag 
of that particular part of the data. thi result be veri specif 
to klr and do not extend to other type of model sinc 
they do not explicitli store train data in their parameters. 

even for klr models, leakag be not quantifi other than 
via visual similar of a few chosen import point and “the 
closest (in L1 norm) extract representers” on the mnist 
dataset of handwritten digits. In mnist, all member of a 
class be veri similar (e.g., all member of the first class be 
differ way of write digit “1”). thus, ani extract digit 
must be similar to all imag in it class, whether thi digit 
be in the train set or not. 
privacy-preserv machin learning. exist literatur on 
privaci protect in machin learn focu mostli on how 
to learn without direct access to the train data. secur 
multiparti comput (smc) have be use for learn 
decis tree [26], linear regress function [11], naiv 

13https://github.com/lasagne/recipes/blob/master/modelzoo/cifar10 nin.pi 

bay classifi [33], and k-mean cluster [22]. the goal 
be to limit inform leakag dure training. the train 
algorithm be the same a in the non-privacy-preserv case, 
thu the result model be a vulner to infer attack 
a ani convent train model. thi also hold for the 
model train by comput on encrypt data [4], [6], [35]. 

differenti privaci [12] have be appli to linear and 
logist regress [8], [37], support vector machin [28], risk 
minim [5], [9], [34], deep learn [1], [30], learn 
an unknown probabl distribut over a discret popul 
from random sampl [10], and releas hyper-paramet 
and classifi accuraci [25]. By definition, differenti pri- 
vate model limit the success probabl of membership 
infer attack base sole on the model, which includ 
the attack describ in thi paper. 

X. conclus 

We have designed, implemented, and evalu the first 
membership infer attack against machin learn models, 
notabl black-box model train in the cloud use googl 
predict api and amazon ml. our attack be a general, 
quantit approach to understand how machin learn 
model leak inform about their train datasets. when 
choos the type of the model to train or a machin learn 
servic to use, our attack can be use a one of the select 
metrics. 

our key technic innov be the shadow train tech- 
niqu that train an attack model to distinguish the target 
model’ output on member versu non-memb of it train- 
ing dataset. We demonstr that shadow model use in thi 
attack can be effect creat use synthet or noisi data. 
In the case of synthet data gener from the target model 
itself, the attack do not requir ani prior knowledg about 
the distribut of the target model’ train data. 

membership in hospital-stay and other health-car dataset 
be sensit from the privaci perspective. therefore, our result 
have substanti practic privaci implications. 
acknowledgments. thank to adam smith for explain 
differenti privaci and the state of the art in membership 
infer attack base on explicit statistics. 

thi work be support by the nsf grant 1409442 and a 
googl research award. 

refer 

[1] M. abadi, A. chu, I. goodfellow, H. B. mcmahan, I. mironov, K. tal- 
war, and L. zhang, “deep learn with differenti privacy,” in ccs, 
2016. 

[2] G. ateniese, L. V. mancini, A. spognardi, A. villani, D. vitali, and 
G. felici, “hack smart machin with smarter ones: how to extract 
meaning data from machin learn classifiers,” intern jour- 
nal of secur and networks, vol. 10, no. 3, pp. 137–150, 2015. 

[3] M. backes, P. berrang, M. humbert, and P. manoharan, “membership 
privaci in microrna-bas studies,” in ccs, 2016. 

[4] M. barni, P. failla, R. lazzeretti, A. sadeghi, and T. schneider, “privacy- 
preserv ecg classif with branch program and neural 
networks,” trans. info. forens and security, vol. 6, no. 2, pp. 452– 
468, 2011. 

[5] R. bassily, A. smith, and A. thakurta, “privat empir risk minimiza- 
tion: effici algorithm and tight error bounds,” in focs, 2014. 

15 



[6] J. bos, K. lauter, and M. naehrig, “privat predict analysi on 
encrypt medic data,” J. biomed. informatics, vol. 50, pp. 234–243, 
2014. 

[7] J. calandrino, A. kilzer, A. narayanan, E. felten, and V. shmatikov, 
““you might also like:” privaci risk of collabor filtering,” in s&p, 
2011. 

[8] K. chaudhuri and C. monteleoni, “privacy-preserv logist regres- 
sion,” in nips, 2009. 

[9] K. chaudhuri, C. monteleoni, and A. sarwate, “differenti privat 
empir risk minimization,” jmlr, vol. 12, pp. 1069–1109, 2011. 

[10] I. diakonikolas, M. hardt, and L. schmidt, “differenti privat 
learn of structur discret distributions,” in nips, 2015. 

[11] W. du, Y. han, and S. chen, “privacy-preserv multivari statist 
analysis: linear regress and classification.” in sdm, 2004. 

[12] C. dwork, “differenti privacy,” in encyclopedia of cryptographi and 
security. springer, 2011, pp. 338–340. 

[13] C. dwork, F. mcsherry, K. nissim, and A. smith, “calibr nois to 
sensit in privat data analysis,” in tcc, 2006. 

[14] C. dwork and M. naor, “on the difficulti of disclosur prevent in 
statist databas or the case for differenti privacy,” J. privaci and 
confidentiality, vol. 2, no. 1, pp. 93–107, 2010. 

[15] C. dwork, A. smith, T. steinke, J. ullman, and S. vadhan, “robust 
traceabl from trace amounts,” in focs, 2015. 

[16] M. fredrikson, S. jha, and T. ristenpart, “model invers attack that 
exploit confid inform and basic countermeasures,” in ccs, 
2015. 

[17] M. fredrikson, E. lantz, S. jha, S. lin, D. page, and T. ristenpart, 
“privaci in pharmacogenetics: An end-to-end case studi of person 
warfarin dosing,” in usenix security, 2014. 

[18] M. hardt, B. recht, and Y. singer, “train faster, gener better: 
stabil of stochast gradient descent,” in icml, 2016. 

[19] T. hastie, R. tibshirani, J. friedman, and J. franklin, “the element 
of statist learning: data mining, infer and prediction,” the 
mathemat intelligencer, vol. 27, no. 2, pp. 83–85, 2005. 

[20] G. hinton, O. vinyals, and J. dean, “distil the knowledg in a neural 
network,” arxiv:1503.02531, 2015. 

[21] N. homer, S. szelinger, M. redman, D. duggan, W. tembe, 
J. muehling, J. V. pearson, D. A. stephan, S. F. nelson, and D. W. craig, 
“resolv individu contribut trace amount of dna to highli 
complex mixtur use high-dens snp genotyp microarrays,” 
plo genetics, vol. 4, no. 8, 2008. 

[22] G. jagannathan and R. wright, “privacy-preserv distribut k-mean 
cluster over arbitrarili partit data,” in kdd, 2005. 

[23] P. jain, V. kulkarni, A. thakurta, and O. williams, “to drop or not 
to drop: robustness, consist and differenti privaci properti of 
dropout,” arxiv:1503.02031, 2015. 

[24] A. krizhevsky, “learn multipl layer of featur from tini images,” 
master’ thesis, univers of toronto, 2009. 

[25] M. J. kusner, J. R. gardner, R. garnett, and K. Q. weinberger, 
“differenti privat bayesian optimization,” in icml, 2015. 

[26] Y. lindel and B. pinkas, “privaci preserv data mining,” in crypto, 
2000. 

[27] F. mcsherry, “statist infer consid harmful,” 
https://github.com/frankmcsherry/blog/blob/master/posts/2016-06- 
14.md, 2016. 

[28] B. rubinstein, P. bartlett, L. huang, and N. taft, “learn in a larg 
function space: privacy-preserv mechan for svm learning,” J. 
privaci and confidentiality, vol. 4, no. 1, p. 4, 2012. 

[29] S. sankararaman, G. obozinski, M. I. jordan, and E. halperin, “ge- 
nomic privaci and limit of individu detect in a pool,” natur 
genetics, vol. 41, no. 9, pp. 965–967, 2009. 

[30] R. shokri and V. shmatikov, “privacy-preserv deep learning,” in ccs, 
2015. 

[31] N. srivastava, G. hinton, A. krizhevsky, I. sutskever, and R. salakhut- 
dinov, “dropout: A simpl way to prevent neural network from over- 
fitting,” jmlr, vol. 15, no. 1, pp. 1929–1958, 2014. 

[32] F. tramèr, F. zhang, A. juels, M. K. reiter, and T. ristenpart, “steal 
machin learn model via predict apis,” in usenix security, 
2016. 

[33] J. vaidya, M. kantarcıoğlu, and C. clifton, “privacy-preserv naiv 
bay classification,” vldb, vol. 17, no. 4, pp. 879–898, 2008. 

[34] M. wainwright, M. jordan, and J. duchi, “privaci awar learning,” in 
nips, 2012. 

[35] P. xie, M. bilenko, T. finley, R. gilad-bachrach, K. lauter, and 
M. naehrig, “crypto-nets: neural network over encrypt data,” 
arxiv:1412.6181, 2014. 

[36] D. yang, D. zhang, and B. qu, “participatori cultur map base on 
collect behavior data in location-bas social networks,” acm tist, 
vol. 7, no. 3, p. 30, 2016. 

[37] J. zhang, Z. zhang, X. xiao, Y. yang, and M. winslett, “function 
mechanism: regress analysi under differenti privacy,” vldb, 
vol. 5, no. 11, pp. 1364–1375, 2012. 

[38] J. zhu and T. hastie, “kernel logist regress and the import vector 
machine,” in nips, 2001. 

16 


