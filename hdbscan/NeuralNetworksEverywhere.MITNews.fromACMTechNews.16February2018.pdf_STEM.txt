






































neural network everywher | mit new 


neural network everywher | mit 
new 

most recent advanc in artificial-intellig system such a speech- 

or face-recognit program have come courtesi of neural networks, 

dens interconnect mesh of simpl inform processor that 

learn to perform task by analyz huge set of train data. 

but neural net be large, and their comput be energi intensive, 

so they’r not veri practic for handheld devices. most smartphon 

app that reli on neural net simpli upload data to internet servers, 

which process it and send the result back to the phone. 

now, mit research have develop a special-purpos chip that 

increas the speed of neural-network comput by three to seven 

time over it predecessors, while reduc power consumpt 94 to 95 

percent. that could make it practic to run neural network local on 

smartphon or even to emb them in household appliances. 

“the gener processor model be that there be a memori in some part of 

the chip, and there be a processor in anoth part of the chip, and you 

move the data back and forth between them when you do these 

computations,” say avishek biswas, an mit graduat student in 

electr engin and comput science, who lead the new chip’ 

development. 

“sinc these machine-learn algorithm need so mani computations, 

thi transfer back and forth of data be the domin portion of the 

energi consumption. but the comput these algorithm do can be 

simplifi to one specif operation, call the dot product. our 

approach was, can we implement thi dot-product function insid 

the memori so that you don’t need to transfer thi data back and forth?” 

biswa and hi thesi advisor, anantha chandrakasan, dean of mit’ 

school of engin and the vannevar bush professor of electr 

engin and comput science, describ the new chip in a paper 

that biswa be present thi week at the intern solid state 

circuit conference. 

neural network everywher | mit new http://news.mit.edu/2018/chip-neural-networks-battery-powered-devices... 

1 sur 3 16-02-18 à 19:21 



back to analog 

neural network be typic arrang into layers. A singl process 

node in one layer of the network will gener receiv data from sever 

node in the layer below and pa data to sever node in the layer 

above. each connect between node have it own “weight,” which 

indic how larg a role the output of one node will play in the 

comput perform by the next. train the network be a matter of 

set those weights. 

A node receiv data from multipl node in the layer below will 

multipli each input by the weight of the correspond connect and 

sum the results. that oper — the summat of multipl — 

be the definit of a dot product. If the dot product exce some 

threshold value, the node will transmit it to node in the next layer, over 

connect with their own weights. 

A neural net be an abstraction: the “nodes” be just weight store in a 

computer’ memory. calcul a dot product usual involv fetch 

a weight from memory, fetch the associ data item, multipli 

the two, store the result somewhere, and then repeat the oper 

for everi input to a node. given that a neural net will have thousand or 

even million of nodes, that’ a lot of data to move around. 

but that sequenc of oper be just a digit approxim of what 

happen in the brain, where signal travel along multipl neuron 

meet at a “synapse,” or a gap between bundl of neurons. the neurons’ 

fire rate and the electrochem signal that cross the synaps 

correspond to the data valu and weights. the mit researchers’ new 

chip improv effici by replic the brain more faithfully. 

In the chip, a node’ input valu be convert into electr voltag 

and then multipli by the appropri weights. sum the product 

be simpli a matter of combin the voltages. onli the combin 

voltag be convert back into a digit represent and store for 

further processing. 

the chip can thu calcul dot product for multipl node — 16 at a 

time, in the prototyp — in a singl step, instead of shuttl between a 

processor and memori for everi computation. 

neural network everywher | mit new http://news.mit.edu/2018/chip-neural-networks-battery-powered-devices... 

2 sur 3 16-02-18 à 19:21 



all or noth 

one of the key to the system be that all the weight be either 1 or -1. 

that mean that they can be implement within the memori itself a 

simpl switch that either close a circuit or leav it open. recent 

theoret work suggest that neural net train with onli two weight 

should lose littl accuraci — somewher between 1 and 2 percent. 

biswa and chandrakasan’ research bear that predict out. In 

experiments, they ran the full implement of a neural network on a 

convent comput and the binary-weight equival on their chip. 

their chip’ result be gener within 2 to 3 percent of the 

convent network’s. 

"thi be a promis real-world demonstr of sram-bas in- 

memori analog comput for deep-learn applications,” say dario 

gil, vice presid of artifici intellig at ibm. "the result show 

impress specif for the energy-effici implement of 

convolut oper with memori arrays. It certainli will open the 

possibl to employ more complex convolut neural network for 

imag and video classif in iot [the internet of things] in the 

future." 

neural network everywher | mit new http://news.mit.edu/2018/chip-neural-networks-battery-powered-devices... 

3 sur 3 16-02-18 à 19:21 


