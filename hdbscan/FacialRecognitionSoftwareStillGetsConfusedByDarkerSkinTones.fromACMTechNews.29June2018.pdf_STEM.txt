






































businessweek - bloomberg 


businessweek - bloomberg 

@lizette_chapman more stori by lizett chapman 

technolog 

facial recognit softwar still get confus by darker skin tones. 

By and 

26 juin 2018 à 11:00 utc+2 correct 26 juin 2018 à 17:20 utc+2 

businessweek - bloomberg https://www.bloomberg.com/businessweek 

1 sur 5 29-06-18 à 19:59 



businessweek - bloomberg https://www.bloomberg.com/businessweek 

2 sur 5 29-06-18 à 19:59 



brian brackeen, founder of kairo AR inc., in philadelphia. 

photographer: yael malka for bloomberg businessweek 

A coupl of year ago, a brian brackeen be prepar to pitch hi 
facial recognit softwar to a potenti custom a a convenient, 
secur altern to passwords, the softwar stop working. 
panicked, he tri adjust the room’ lighting, then the wi-fi 
connection, befor he realiz the problem be hi face. brackeen be 
black, but like most facial recognit developers, he’d train hi 
algorithm with a set of mostli white faces. He get a white, blond 
colleagu to pose for the demo, and they close the deal. It be a 
pyrrhic victory, he says: “it be like have your own child not 
recogn you.” 

At kairo AR inc., hi 40-person facial recognit compani in miami, 
brackeen say he’ improv the softwar by add more black and 
brown face to hi imag sets, but the result be still imperfect. the 
same problem bedevil compani includ microsoft, ibm, and 
amazon and their grow rang of custom for similar services. 
facial recognit be be use to help india’ govern find 
miss children, and british news outlet spot celebr at royal 
weddings. more controversially, it’ be use in a grow number of 
context by law enforc agencies, which be often less than 
forthcom about what they’r use it for and whether they’r do 
enough about potenti pitfalls. brackeen believ the problem of racial 
bia be seriou enough that law enforc shouldn’t use facial 
recognit at all. 

microsoft, ibm, and china’ face++ misidentifi darker-skin 
woman a often a 35 percent of the time and darker-skin men 12 
percent of the time, accord to a report publish by mit research 
earli thi year. the gender differ owe to a small set of women’ 
faces. such softwar can see onli what it’ be taught to see. 

In recent months, major vendor say they’v diversifi their train 
data set to includ darker-color face and have make stride in 
reduc bias. microsoft corp. say it plan to announc on june 26 
that it will releas a version of it softwar tool face api that now 
misidentifi darker-skin women, the group for which it’ most 
error-prone, onli 1.9 percent of the time. (the compani say it error 
rate for other group be zero percent.) intern busi machin 
corp. say it watson visual recognition, which be similarli at it 
weak in identifi darker-skin women, get it wrong 3.5 percent 
of the time. both ibm and microsoft acknowledg their result haven’t 
be independ verifi and that real-world error rate could be 
differ from those for their collect of stock images. the maker of 
face++ didn’t respond to request for comment. 

“an inaccur system will implic peopl for crime they didn’t 

businessweek - bloomberg https://www.bloomberg.com/businessweek 

3 sur 5 29-06-18 à 19:59 



commit” 

it’ amazon.com inc. that may have to worri most about real-world 
results. On june 15 a group of amazon sharehold sent the compani 
a letter ask it to stop market it rekognit system to polic 
depart and other govern agenc until guidelin be 
develop to ensur the softwar isn’t lead to civil right violations. 
In anoth letter the follow week, amazon worker ask chief 
execut offic jeff bezo to stop sell rekognit to law 
enforc agenc give “the u.s.’ increasingli inhuman 
treatment of refuge and immigrants.” amazon declin to comment 
for thi story. 

govern agenc have no broadli agreed-upon standard for 
evalu facial recognit systems. A 2016 studi by georgetown 
univers found that almost none of the law enforc agenc that 
use facial recognit requir supplier to meet a minimum threshold 
for overal accuracy, let alon racial disparities. “an inaccur system 
will implic peopl for crime they didn’t commit and shift the 
burden to innoc defend to show they be not who the system 
say they are,” say jennif lynch, senior staff attorney for the 
electron frontier foundation, an advoc for civil liberti online. 

and the problem isn’t just in the u.s. thi spring, a report from big 
brother watch, a u.k. civil right group that examin public-record 
request make to sever law enforc agenc use facial 
recognition, conclud that the system be terrible. for example, the 
south wale police, which use facial recognit to screen peopl at 
public events, report more than 90 percent of the match be 
erroneous. the depart say in a statement on it websit that the 
use of facial recognit have be a “resound success.” It didn’t 
respond to an interview request. 

maker of facial recognit technology, includ microsoft and ibm, 
have say the softwar continu to be a work in progress, with 
engin focu on improv accuraci and transpar around 
how the improv be be made. they say the technolog have 
help bust sex traffick and apprehend would-b terrorists, though 
they’v provid few details. 

andrew ferguson, a law professor at the univers of the district of 
columbia and the author of the rise of big data policing, say use 
the power technolog while it’ still under develop with scant 
regul be dangerous. law enforc agenc have consist 
botch their adopt of novel tech. “polic be beta-test new 
technolog or pilot new idea in polic without a vet process 
to think through bia or how it might affect citizens’ civil rights,” he 
says. 

engin be improv how they train algorithm a more agenc 

businessweek - bloomberg https://www.bloomberg.com/businessweek 

4 sur 5 29-06-18 à 19:59 



purchas the software, but they may not be abl to head off grow 
call for regulation. the author of the georgetown report call for state 
and feder law govern how polic depart use facial 
recognit and call on the polic to test regularli for algorithm bias. 
In april a group of civil right organ say it be “categor 
unethical” to deploy real-tim facial recognit analysi of footag 
captur by polic bodi cameras. 

some, includ the eff’ lynch, argu that their concern will onli 
increas a the technolog improves. An accur imag merg with 
person inform about an individu such a location, famili ties, 
vote records, and the like can be pull togeth by author use 
product such a those from palantir technolog inc. to creat a 
digit dossier on peopl without their consent or knowledge. “even if 
we have a 100 percent accur system, I don’t want that system,” 
lynch says. “that mean we can no longer walk around and interact 
with peopl without the govern know who we are, where we 
are, and who we’r talk to.” 

(correct microsoft error rate in fourth paragraph) 

bottom line - microsoft say it’ cut it facial recognit error rate 
to zero percent for everyon except darker-skin women, but a with 
rivals, those number be like to rise in the real world. 

befor it' here, it' on the bloomberg terminal. learn more 

juli 2, 2018 

businessweek - bloomberg https://www.bloomberg.com/businessweek 

5 sur 5 29-06-18 à 19:59 


