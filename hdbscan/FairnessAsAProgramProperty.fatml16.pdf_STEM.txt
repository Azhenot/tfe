









































fair a a program properti 

aw albarghouthi, lori d’antoni, 
samuel drew 

univers of wisconsin–madison 

aditya nori 

microsoft research 

abstract We explor the follow question: Is a 
decision-mak program fair, for some use defi- 
nition of fairness? first, we describ how sever al- 
gorithm fair question can be phrase a pro- 
gram verif problems. second, we discu an au- 
tomat verif techniqu for prove or disprov- 
ing fair of decision-mak program with respect 
to a model of the population. 

1. introduct 
algorithm have becom power arbitr of a 
rang of signific decis with far-reach soci- 
etal impact—hir [21, 22], welfar alloc [15], 
prison sentenc [2], polic [5, 25], amongst mani 
others. with the rang and sensit of algorithm de- 
cision expand by the day, the question of whether 
an algorithm be fair be a press one. indeed, the notion 
of algorithm fair have captur the attent of a 
broad spectrum of experts: machin learn and the- 
ori research [6, 13, 16, 29]; privaci research and 
investig journalist [2, 10, 26, 28]; law scholar 
and social scientist [1, 3, 27]; government agenc 
and ngo [24]. 

ultimately, algorithm fair be a question about 
program and their properties: Is a give program P 
fair, under some definit of fairness? or, how fair 
be P? In thi paper, we describ a line of work that 
approach the question of algorithm fair from 
a program-analyt perspective, in which our goal be 
to analyz a give decision-mak program and con- 
struct a proof of it fair or unfairness—just a a 
tradit static program verifi would prove correct- 
ness of a program with respect to, for example, lack of 
divis by zero, integ overflows, null-point dere- 
frences, etc. 

We start by analyz what be the challeng and 
research question in check algorithm fair for 
decis make program (section 2). We then present 

decision-mak program P 
(e.g., a learn classifier) 

popul model M 
(a probabilist program) 

fair verifi 
a probabilist 

program verifi 

program P be fair wrt M 
(proof of fairness) 

program P be unfair wrt M 
(proof of unfairness) 

fair 
properti 

figur 1. overview 

a simpl case studi and show how techniqu for ver- 
ifi probabilist program can be use to automat- 
ical prove or disprov global fair for a class of 
program that subsum a rang of machin learn 
classifi (section 3). finally, we lay a list of mani 
challeng and interest question that the algo- 
rithm and program languag commun need 
to answer to achiev the ultim goal of build a 
fulli autom system for verifi and guarante 
algorithm fair in real-world applic (sec- 
tion 4). 

2. prove program fair 
In thi section, we describ the compon of the fair- 
ness verif problem. intuitively, our goal be to 
prove whether a certain program be fair with respect to 
the set of possibl input over which it operates. tack- 
ling the fairness-verif problem requir answer- 
ing a number of challeng questions: 
– what class of decision-mak program should our 

program model capture? 

1 



– how can we defin the set of possibl input to the 
program and captur complex probabl distribu- 
tion that be use and amen to verification? 

– how can we describ what it mean for the program 
to be fair? 

– how can we fulli autom the verif process? 
figur 1 provid a high-level pictur of our pro- 

pose framework. As shown, the fair verifi take 
a (white-box) decision-mak program P and a pop- 
ulat model M . the verifi then proce to prove 
or disprov that P be fair for the give popul de- 
fin by the model M . here, the model M defin a 
joint probabl distribut on the input of P . exist- 
ing definit of fair defin program a fair or un- 
fair with respect to a give concret dataset. while us- 
ing a concret dataset simplifi the verif prob- 
lem, it also rais question of whether the dataset be 
repres for the popul for which we be try- 
ing to prove fairness. our techniqu move away from 
concret dataset and replac them with a probabilist 
popul model. We envis a futur in which fair- 
ness verif be regulated.1 for instance, a govern- 
mental agenc can publish a probabilist popul 
model (e.g., gener from censu data). ani orga- 
nizat employ a decision-mak algorithm with 
potenti signific consequ (e.g., hiring) must 
quantifi fair of their algorithm process against 
the current pictur of the population, a specifi by 
the popul model. 

decision-mak program In the context of algorith- 
mic fairness, a program P take a input a vector of 
argument v repres a set of input attribut (fea- 
tures), where one (or more) of the argument v in the 
vector v be sensitive—e.g., gender or race. evalu 
p(v) may return a boolean valu indicating—e.g., hire 
or not hire—if the program be a binari or a numer 
value—e.g., a mortgag rate. the set of combinators, 
operations, and type use by the program can vastli 
affect the complex of the verif procedures. for 
example, loop be the hardest type of program 
construct to reason about, but most machin learn 
classifi do not contain loops. similarly, sinc classi- 
fier typic oper over real values, we can limit the 
set of possibl type allow in our program to onli 
be real or other type that can be desugar into 

1 the european union (eu), for instance, have alreadi begin regu- 
late algorithm decision-mak [17]. 

reals. all these decis be crucial in the design of a 
verif procedure. 
popul model To be abl to reason about the out- 
come of the program we need to specifi what kind of 
input the program will oper on. for example, al- 
though a program that alloc mortgag might be 
“fair” with respect a certain set of applicants, it may be- 
come unfair when consid a differ pool of peo- 
ple. In program verification, the “kind of inputs” over 
which the program oper be call the precondit 
and be typic state a a formal logic properti with 
the program input a free variables. An exampl of 
program precondit be 

vgender = f → vjob 6= priest 

which indic that none of the program input be both 
a woman and a priest. Of course, there be mani pos- 
sibl choic for what languag we can use to describ 
the program’ precondition. In particular, if we want to 
captur a certain probabl distribut over the input 
of the program, our languag will be a logic that can 
describ probabl and random variables. for exam- 
ple, we might want to be abl to specifi that half of 
the input be female, pr[vgender = f ] = 0.5, or that 
the age of the process input have a particular distri- 
bution, vage ∼ gauss(18, 5). again, the choic of the 
languag allow in the precondit be crucial in the 
design of a verif procedure. from now on, we 
refer to the program precondition, dpop, a the popula- 
tion model. 
fair properti the next step be to defin a prop- 
erti state that the program’ outcom be fair with re- 
spect to the program’ precondition. In program verifi- 
cation, thi be call the postcondit of the program. 
As observ in the fair literature, there be mani 
way to defin when and whi a program be fair or un- 
fair. 

for example, if we want to prove group fairness— 
i.e., that the algorithm be just a like to hire a mi- 
noriti applic (m) a it be for other, non-minor 
applicants—our postcondit will be an express of 
the form 

pr[p(v) = true | v = m] 
pr[p(v) = true | v 6= m] 

> 1− � 

where true be the desir return valu of the program, 
e.g., indic hiring. On the other hand, if we want to 
prove individu fairness—i.e., similar input should 

2 



have similar outcomes—our postcondit will be an 
express of the form 

pr[p(v) 6= p(v′) | v ∼ v′] < � 

notic that the last postcondit relat the outcom 
of the program on differ input values. As the two 
type of properti we describ be radic differ- 
ent, they will also requir differ verif mecha- 
nisms. 
proof of (un)fair the task of prove whether 
a program be fair boil down to static check 
whether, on input satisfi the precondition, the out- 
come of the program satisfi the post-condition. for 
simpl definitions, such a group fairness, the verifica- 
tion problem reduc to comput the probabl of 
a number of event with respect to the program and 
the popul model. for more complex definitions, 
such a individu fairness, prove fair requir 
more complex reason involv multipl run of the 
program (i.e., a hyperproperti [9]), a notori hard 
problem. In the case of a neg result, the verifi 
should provid the user with a proof of unfairness. de- 
pend on the fair definition, produc a human- 
readabl proof might be challeng a the argument 
might involv multipl and potenti infinit inputs. 
for example, in the case of group fair it might be 
challeng to explain whi the program output true on 
40% of the minor input and on 70% of the major 
inputs. 

3. case studi 
We now describ a simplifi case studi demonstrat- 
ing how our fair verif methodolog can be 
use to prove or disprov fair of a give decision- 
make program. 
A program and a popul model consid the fol- 
low program dec, which be a decision-mak pro- 
gram that take a job applicant’ colleg rank and 
year of experi and decid whether they get hire 
or not (the fair target). the program implement 
a decis tree, perhap one gener by a machine- 
learn algorithm. A person be hire if they attend 
a top-5 colleg (colrank <= 5) or have lot of expe- 
rienc compar to their college’ rank (exprank > 
-5). observ that dec do not access ethnicity. 

defin dec(colrank, yexp) 
exprank ← yexp - colrank 
if (colrank <= 5) 

hire ← true 
elif (exprank > -5) 
hire ← true 

els 
hire ← fals 

return hire 

now, consid the program popmodel, which be a 
probabilist program describ a simpl model of the 
population. here, a member of the popul have three 
attributes, all of which be real-valued: (i) ethnicity; 
(ii) colrank, the rank of the colleg the person at- 
tend (lower be better); and (iii) yexp, the year of 
work experi a person has. We consid a person be 
a member of a protect group if ethnic > 10; we 
call thi the sensit condition. the popul model 
can be view a a gener model of record of 
individuals—th more like a combin be to occur 
in the population, the more like it will be generated. 
for instance, the year of experi an individu have 
(line 4) follow a gaussian distribut with mean 10 
and standard deviat 5. 

defin popmodel() 
ethnic ~ gauss(0,10) 
colrank ~ gauss(25,10) 
yexp ~ gauss(10,5) 
if (ethnic > 10) 
colrank ← colrank + 5 

return colrank, yexp 

A note on the program model note that our pro- 
gram model, while admit arbitrari programs, be 
rich enough to captur program (classifiers) gener 
by standard machin learn algorithms. for exam- 
ple, linear support vector machines, decis trees, and 
neural networks, can be repres in our languag 
simpli use assign with arithmet express 
and conditionals. similarly, the popul model be a 
probabilist program, where assign can be make 
by draw valu from predefin distributions. like 
other probabilist program languages, our pro- 
gram model be rich enough to subsum graphic 
model like bayesian network [19]. 

group fair suppos that our goal be to prove 
group fairness, follow the definit of feldman et 
al. [16]: 

pr[hire | min] 
pr[hire | ¬min] 

> 1− � 

where min be shorthand for the sensit condit eth- 
niciti > 10. 

3 



probabilist infer a volum comput To 
prove (un)fair of the decision-mak model with 
respect to the population, we need to comput the prob- 
abil appear in the group fair ratio. for il- 
lustration, suppos we be comput the probabl 
pr[hire ∧ ¬min]. We need to reason about the com- 
posit of the two programs, dec ◦ popmodel. that 
is, we want to comput the probabl that (i) pop- 
model gener a non-minor applicant, and (ii) dec 
hire that applicant. To do so, we observ that everi 
possibl execut of the composit dec ◦ popmodel 
be uniqu character by the set of the three prob- 
abilist choic make by popmodel. In other words, 
everi execut be character by a vector v ∈ r3. 

thus, our goal be to comput the probabl that we 
draw a vector v that result in a minor applic be- 
ing hired. probabilist program languages, e.g., 
church [18], R2 [23], and stan [7], employ approxi- 
mate infer techniques, like mcmc, which converg 
in the limit but offer no guarante on how far we be 
from the exact result. In our work, we consid exact 
inference, which have primarili receiv attent in the 
bayesian network setting, and boil down to solv a 
#sat instanc [8]. In our setting, however, we be deal- 
ing with real-valu variables. 

use standard techniqu from program analysi 
and verification, we can character the set of all 
such vector a a formula ϕ, which be compris of 
boolean combin (conjunctions/disjunctions) of 
linear inequalities—sinc our program onli have lin- 
ear expressions. geometrically, the formula ϕ be a set 
of convex polyhedron in rn. therefore, the probabil- 
iti pr[hire ∧ ¬min] be the same a the probabl of 
draw a vector v that lie insid of ϕ. In other words, 
we be interest in the volum of ϕ, weight by the 
probabilist choices. formally: 

pr[hire ∧ ¬min] = 
∫ 
ϕ 
pepypc dedydc 

where, e.g., pe be the probabl densiti function of the 
distribut gauss(0,10)—th distribut from which 
the valu of ethnic be drawn in line 2 of popmodel. 

the volum comput problem be a well-studi 
and hard problem [14, 20]. indeed, even for a convex 
polytope, comput it volum be #p-hard. leverag- 
ing the great develop in satisfiabiltiy modulo the- 
ori (smt) solver [4], we develop a procedur that 
reduc the volum compuat problem to a seri of 

colrank 

ethnic yexp 

underapproxim of ' 
a a union of hyperrectangl 

formula ' in R3 
(blue face be unbounded) 

figur 2. underapproxim of ϕ a hyperrectangl 

call to the smt solver, view complet a an or- 
acle. specifically, our procedur us the smt solver 
to sampl subregion of ϕ that be hyperrectangular. 
intuitively, for hyperrectangular region in rn, evalu- 
ate the abov integr be a matter of evalu the 
cdf of the variou distributions. thus, by systemat- 
ical sampl more and more non-overlap hy- 
perrectangl in ϕ, we maintain a low bound on the 
probabl of interest. figur 2 pictori illustr 
ϕ and an under-approxim with 4 hyperrectangles. 
similarly, to comput an upper bound on the probabil- 
ity, we can simpli invok our procedur on ¬ϕ. 
fair certif the fair verif tool ter- 
minat when it have comput lower/upp bound that 
prove or disprov the desir fair criteria. the hy- 
perrectangl sampl in the process of comput vol- 
ume can serv a proof certificates. that is, an extern 
entiti can take the hyperrectangles, comput their vol- 
umes, and ensur that they inde lie in the expect 
region in rn. 

4. experi and futur outlook 
experi We have built a fairness-verif tool, 
call fairsquare, that take a decision-mak pro- 
gram, a popul model, and verifi fair of the 
program with respect to the model. So far, we have fo- 
cuse on group fairness. the tool us the popular Z3 
smt solver [12] for manipul first-ord formula 
over arithmet theories. 

We have use fairsquar to prove or disprov fair- 
ness of a suit of popul model and program rep- 
resent machine-learn classifi that be auto- 
matic gener from real-world dataset use in 
other work on algorithm fair [11, 16, 29]. specif- 
ically, we have consid linear svms, simpl neural 
network with rectifi linear units, and decis trees. 
futur outlook look forward, we see a wide rang 
of avenu for improv and exploration. for in- 

4 



stance, we be current work on the problem of 
make an unfair program fair. that is, give a pro- 
gram P that be consid unfair, what be the small 
tweak that would make it fair. our goal be to repair the 
program, make it fair, while ensur that it be seman- 
tical close to the origin program. 

refer 
[1] ifeoma ajunwa, sorel friedler, carlo E scheidegger, 

and suresh venkatasubramanian. hire by algorithm: 
predict and prevent dispar impact. avail 
at ssrn 2746078, 2016. 

[2] julia angwin, jeff larson, surya mattu, and 
lauren kirchner. machin bias: there’ soft- 
ware use across the countri to predict fu- 
ture criminals. and it’ bia against blacks. 
https://www.propublica.org/article/machine- 
bias-risk-assessments-in-criminal-sentencing, 
may 2016. (access on 06/18/2016). 

[3] solon baroca and andrew D selbst. big data’ dis- 
parat impact. avail at ssrn 2477899, 2014. 

[4] clark W. barrett, roberto sebastiani, sanjit A. seshia, 
and cesar tinelli. satisfi modulo theories. In 
handbook of satisfiability, page 825–885. 2009. 

[5] nate berg. predict crime, lapd-style. 
https://www.theguardian.com/cities/2014/jun/ 
25/predicting-crime-lapd-los-angeles-police- 
data-analysis-algorithm-minority-report, june 
2014. (access on 06/18/2016). 

[6] toon calder and sicco verwer. three naiv bay ap- 
proach for discrimination-fre classification. data 
mine and knowledg discovery, 21(2):277–292, 
2010. 

[7] bob carpenter, andrew gelman, matt hoffman, daniel 
lee, ben goodrich, michael betancourt, marcu A 
brubaker, jiqiang guo, peter li, and allen riddell. 
stan: a probabilist program language. journal 
of statist software, 2015. 

[8] mark chavira and adnan darwiche. On probabilist 
infer by weight model counting. artifici intel- 
ligence, 172(6):772–799, 2008. 

[9] michael R clarkson and fred B schneider. hyper- 
properties. journal of comput security, 18(6):1157– 
1210, 2010. 

[10] amit datta, michael carl tschantz, and anupam 
datta. autom experi on ad privaci set- 
tings. proceed on privaci enhanc technolo- 
gies, 2015(1):92–112, 2015. 

[11] anupam datta, shayak sen, and yair zick. algorith- 
mic transpar via quantit input influence. In 
proceed of 37th ieee symposium on secur and 
privacy, 2016. 

[12] leonardo De moura and nikolaj bjørner. z3: An 
effici smt solver. In intern confer on 
tool and algorithm for the construct and analysi 
of systems, page 337–340. springer, 2008. 

[13] cynthia dwork, moritz hardt, toniann pitassi, omer 
reingold, and richard S. zemel. fair through 
awareness. In innov in theoret comput 
scienc 2012, cambridge, ma, usa, januari 8-10, 
2012, page 214–226, 2012. 

[14] martin E. dyer and alan M. frieze. On the complex- 
iti of comput the volum of a polyhedron. siam 
journal on computing, 17(5):967–974, 1988. 

[15] virginia eubanks. the danger of let algorithm 
enforc policy. http://www.slate.com/articles/ 
technology/future_tense/2015/04/the_dangers_ 
of_letting_algorithms_enforce_policy.html, 
april 2015. (access on 06/18/2016). 

[16] michael feldman, sorel A. friedler, john moeller, 
carlo scheidegger, and suresh venkatasubramanian. 
certifi and remov dispar impact. In proceed- 
ing of the 21th acm sigkdd intern confer- 
enc on knowledg discoveri and data mining, syd- 
ney, nsw, australia, august 10-13, 2015, page 259– 
268, 2015. 

[17] B. goodman and S. flaxman. EU regul on algo- 
rithmic decision-mak and a “right to explanation”. 
arxiv e-prints, june 2016. 

[18] noah goodman, vikash mansinghka, daniel M roy, 
keith bonawitz, and joshua B tenenbaum. church: 
a languag for gener models. arxiv preprint 
arxiv:1206.3255, 2012. 

[19] andrew D gordon, thoma A henzinger, aditya V 
nori, and sriram K rajamani. probabilist program- 
ming. In proceed of the on futur of softwar en- 
gineering, page 167–181. acm, 2014. 

[20] leonid khachiyan. complex of polytop volum 
computation. springer, 1993. 

[21] nicol kobie. who do you blame when an algorithm 
get you fired? http://www.wired.co.uk/article/ 
make-algorithms-accountable, januari 2016. (ac- 
cess on 06/18/2016). 

[22] clair cain miller. can an algorithm hire good than 
a human? http://www.nytimes.com/2015/06/26/ 
upshot/can-an-algorithm-hire-better-than-a- 
human.html, june 2015. (access on 06/18/2016). 

[23] aditya V nori, chung-kil hur, sriram K rajamani, 
and selva samuel. r2: An effici mcmc sampler 
for probabilist programs. In aaai, page 2476–2482, 
2014. 

[24] execut offic of the president. big data: seiz- 
ing opportunities, preserv values. https: 
//www.whitehouse.gov/sites/default/files/ 

5 



docs/big_data_privacy_report_may_1_2014.pdf, 
may 2014. (access on 06/18/2016). 

[25] walt L perry. predict policing: the role of crime 
forecast in law enforc operations. rand cor- 
poration, 2013. 

[26] latanya sweeney. discrimin in onlin ad delivery. 
queue, 11(3):10, 2013. 

[27] andrew tutt. An fda for algorithms. avail at ssrn 
2747994, 2016. 

[28] jennif valentino-devries, jeremi singer-vine, and 
ashkan soltani. websit vari prices, deal base on 
users’ information. http://www.wsj.com/articles/ 
sb10001424127887323777204578189391813881534, 
decemb 2012. (access on 06/18/2016). 

[29] richard S. zemel, Yu wu, kevin swersky, toniann 
pitassi, and cynthia dwork. learn fair representa- 
tions. In proceed of the 30th intern confer- 
enc on machin learning, icml 2013, atlanta, ga, 
usa, 16-21 june 2013, page 325–333, 2013. 

6 


