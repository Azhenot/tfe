









































microsoft word - bigdatadatascienceandcivilright final.docx 


1 


big data, data science, and civil right 


solon barocas, elizabeth bradley, vasant honavar, and foster provost 


abstract 
advanc in data analyt bring with them civil right implications. data-driven and 
algorithm decis make increasingli determin how busi target 
advertis to consumers, how polic depart monitor individu or groups, how 
bank decid who get a loan and who do not, how employ hire, how colleg and 
univers make admiss and financi aid decisions, and much more. As data-driven 
decis increasingli affect everi corner of our lives, there be an urgent need to ensur 
they do not becom instrument of discrimination, barrier to equality, threat to social 
justice, and sourc of unfairness. In thi paper, we argu for a concret research agenda 
aim at address these concerns, compris five area of emphasis: (i) determin if 
model and model procedur exhibit objection bias; (ii) build awar of 
fair into machin learn methods; (iii) improv the transpar and control of 
data- and model-driven decis making; (iv) look beyond the algorithm(s) for 
sourc of bia and unfairness—in the myriad human decis make dure the problem 
formul and model process; and (v) support the cross-disciplinari scholarship 
necessari to do all of that well. 



over the past sever years, government, academia, and the privat sector have increasingli recogn 
that the use of big data and data scienc in more and more decis have import implic for civil 
rights, from racial discrimin to incom equal to social justice. We have see mani fruit 
meet and discussions, some of which be summar briefli in an appendix below and have 
inform thi report. however, a coher research agenda for address these topic be onli begin 
to emerge. 

the need for such an agenda be critic and timely. big data and data scienc have begin to profoundli 
affect decis make becaus the modern world be more broadli instrument to gather data—from 
financi transactions, mobil phone calls, web and app interactions, emails, chats, facebook posts, 
tweets, cars, fitbits, and on and on. increasingli sophist algorithm can extract pattern from that 
data, enabl import advanc in science, medicine, and commerce. As describ in a recent 60 
minut segment, for instance, ibm' watson have help doctor identifi treatment strategi for cancer.1 
xerox now cede hire decis for it 48,700 call-cent job to software, cut attrit by a fifth.2 
and if you use the web, you have receiv advertis target base on fine-grain detail of your 
onlin behavior. 

along with improv scienc and commerc come import civil right implications. for example, data 
analyt tool can captur and instanti decision-mak pattern that be implicitli discriminatory— 
and can do so unintentionally, simpli from distil the data. implicit discrimin by algorithm 
requir our attent becaus such data-driven method be deploy in mani of our most crucial social 
institutions. risk assess tools, for instance, be increasingli common in the crimin justic system, 
inform critic decis like pre-trial detention, bond amounts, sentenc lengths, and parole. last year, 
propublica complet a studi of a risk assess tool employ in a number of courtroom across that 


1“artifici intelligence,” 60 minutes, octob 9, 2016, http://www.cbsnews.com/videos/artificial-intelligence/. 
2 walker, joseph. “meet the new boss: big data: compani trade in hunch-bas hire for comput modeling,” the wall 
street journal, septemb 20, 2012, http://online.wsj.com/articles/sb10000872396390443890304578006252019616768. 



2 

nation that be equal accur in predict whether black and white defend will recidivate, but be far 
more like to assign a high risk score to black defend who do not go on to reoffend. white 
defend who do go on to commit a crime when releas were, in turn, more like to be mislabel a 
have low risk. the studi establish that even when a model be equal accur in make predict 
about member of differ racial groups, the fals posit and fals neg rate might differ between 
groups. In thi case, the cost of fals posit (unwarr incarceration) disproportion fell on one 
group.3 

what’ more, practition seldom provid explan of the reason for decis make by such 
systems, give no view of whi you get turn down for a job or flag a a terrorist. As cathi o'neil 
write in weapon of math destruction, “the model be use today be opaque, unregulated, and 
uncontestable, even when they’r wrong.”4 there be some momentum in the research commun to fix 
this,5 but the issu be complex. there be the reason whi a particular model make a decision—for 
example, you be deni credit becaus you’v onli be at your current job for two months, and you 
transact with merchant that default frequent. and there be deeper reasons: whi be these particular 
“attributes” deem by the system to be import evid of default? Do we want to use thi sort of 
evid for thi sort of decis making? are the model codifying, and therebi reinforcing, the effect 
of prior unfairness? are the model simpli incorrect, due to unrecogn bia in the data? for 
example, a kate crawford and ryan calo note, “a 2015 studi show that a machine-learn techniqu 
use to predict which hospit patient would develop pneumonia complic work well in most 
situations. but it make one seriou error: it instruct doctor to send patient with asthma home even 
though such peopl be in a high-risk category.6 becaus the hospit automat sent patient with 
asthma to intens care, these peopl be rare on the ‘requir further care’ record on which the 
system be trained.”7 

year by year, the sophist of these data-driven algorithm increases, and we alreadi cannot escap 
their effects. over the next decade, the data that feed these algorithm will becom more pervas and 
more personal. progress toward address issu of civil right and fair will be make onli if 
incent be put in place to bypass the consider roadblock to success: (1) most comput scientist 
do not have a deep understand of issu of fair and civil rights, and they thu be not tradit or 
natur issu for comput scientist to address; (2) tradit civil-right scholar gener lack the 
sophist understand of big data and data scienc need to make substant progress; (3) the key 
question for which answer be need be not broadli accept a be import research problems. 

determin if model learn from data exhibit objection bia 
establish whether a model discrimin on the basi of race, gender, age, or other legal protect or 
otherwis sensit characterist might seem like a straightforward task: do the model includ ani of 
these features? If not, one might quickli conclud that it decis cannot exhibit ani bias. 
unfortunately, there be a number of problem that might nevertheless result in a bia model, even if 
the model do not consid these featur explicitly. exist scholarli work have address the notion 
that other featur can act a surrog for explicitli sensit characteristics, most famous locat of 


3 angwin, julia, jeff larson, surya mattu, and lauren kirchner, “machin bias,” propublica, may 23, 2016, 
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing. 
4 o’neil, cathy, weapon of math destruction: how big data increas inequ and threaten democracy, new york: 
crown, 2016. 
5 fairness, accountability, and transpar in machin learning, http://www.fatml.org. 
6 caruana, rich, yin lou, johann gehrke, paul koch, marc sturm, and noemi elhadad, "intellig model for healthcare: 
predict pneumonia risk and hospit 30-day readmission," proceed of the 21th acm sigkdd intern confer 
on knowledg discoveri and data mining, pp. 1721-1730. acm, 2015. 
7 crawford, kate, and ryan calo, "there be a blind spot in AI research," natur 538, no. 7625, 2016. 



3 

resid act a a surrog for race. however, there be even more complex reason one might end up 
with a bia model, despit consciou effort to the contrary. 

the select of the data use to build the models—th train data—i an import sourc of potenti 
bias. non-repres sampl of the popul will often lead to model that exhibit systemat 
errors. such sampl bia be easi to overlook and sometim imposs to fulli recognize; worse, 
standard valid method that depend on hold-out data drawn from the same sampl will fail to reveal 
them. even repres samples—or dataset that captur the entir popul of interest—can fail to 
ensur that model perform equal well for differ part of the population. when minor group do 
not follow the same pattern of behavior a the major group, machin learn may struggl to model 
the behavior of the minor a effect a the major becaus there will be proport few 
exampl of the minor behavior from which to learn. under these conditions, the domin group in 
societi may well enjoy rel high accuraci rates. train data may also encod prior prejudici 
or bia assessments. A model train on histor hire data could easili lead to futur hire 
decis that simpli replic the discrimin at work in previou human decision-mak upon which 
such model hop to improve. taint train exampl might wrongli instruct the machin to see 
featur that actual predict success on the job a indic of poor performance. 

ani bia exhibit by such model would be unintentional, but no less pernici than decis that 
explicitli consid legal protect characteristics. indeed, such model could be more pernici 
precis becaus the bia stem from problem with the train data that be easi to overlook. while 
data scientist often learn and care deepli about the challeng pose by sampl bias, the difficulti or 
imposs of establish ground truth, and the mani way to measur model performance, there be an 
urgent need to support research to develop more rigor method for establish whether a model 
exhibit objection bias. 

law and polici often look to dispar impact analysi a a way to establish whether a decis procedur 
might be discriminatory. such analysi ask whether the decision-mak in question result in a dispar 
in outcom along line of race, gender, age or other protect characteristics. if, for example, white job 
applic receiv offer of employ at a rate 20% high than black applicants, thi might suggest 
that the process of assess job applic suffer from some kind of bias. It might, however, also 
indic that the legitim qualiti sought by the employ happen to be held at uneven rate by 
member of differ racial groups. and it can be exceedingli difficult to establish whether a dispar 
impact stem from the former or the latter—especi when a research do not have direct access to 
the model or train data, or do not fulli comprehend the data-gener process. 

much of the research to date that aim to measur bia in system that reli on machin learn have be 
perform under adversari conditions. outsid have observ how system respond to differ 
inputs, and have attempt to uncov ethic salient differ in outputs. such techniqu be 
commonli know a “algorithm auditing” and they be what mani comment have in mind when 
they call for “algorithm accountability.” research oper under these condit face consider 
challeng in make well-justifi claim about the sourc of bias. By necessity, most of these audit 
have so far focu on system that be consumer-facing, to which research can input some data and 
observ the output; the result be a seri of import case domin by instanc of machin learn 
appli to web services. research be need not onli to explor way to overcom these challenges, but 
also to understand whether differ method would be necessari or more effect when organ 
attempt to audit their own model or grant outsid access. 

support the emerg field of fairness-awar machin learn 
comput scientist have begin to investig how concern with fair and reduc or elimin 
unwant discrimin might becom part of the model-build process. In particular, research have 



4 

develop a number of differ formal definit of fair that model can be forc to satisfy. one 
such notion be group parity, which requir that model gener equal outcom for member of, for 
example, differ racial groups. thi requirement, however, might well be in tension with a notion of 
individu fairness, where assess be expect to be maxim accur for each individual. other 
have propos that fair would be best serv by requir that a machin learn algorithm classifi 
differ popul with the same true posit rate; for instance, when such an algorithm be use to 
decid who get a loan, the algorithm should have an equal probabl of classifi a loan-worthi 
individu a loan-worthy, irrespect of which subpopul that individu be from. still other have 
worri about case where accuraci rate be comparable, but where the type of error differ between 
group and where these error have differ costs, a in propublica’ stori on recidiv prediction. one 
group might be subject to a high rate of fals posit (potenti veri costli for the affect 
individual) while the other experi a high rate of fals neg (potenti desir from the 
individual’ perspective). 

these approach have tend to trace the sourc of unfair back to differ weak in machin 
learning. some assum that the main problem resid with train data, which may suffer from all sort 
of biases. the task, in such cases, be to compens for flaw in the data from which the machin will 
learn. other have identifi case where machin learn fail to perform a well for minor group 
even when the train data be pristine, often becaus minor group do not conform to the same pattern 
a the major group. thi work assum that the task be to develop method that can gener model 
with more even perform across a divers population, but to do so without have to drag down the 
model’ perform for the major group. 

the field have begin to grappl with the tension between differ notion of fairness, but there be 
signific disagr about the appropri direct for futur research. some see hard trade-off 
between compet idea of fairness; other wonder if data scientist just need incent to collect more 
data (both train exampl and a larg set of features) to close the gap in performance; still other 
wonder if test of valid be even a legitim measur of fairness, give how deepli bia may suffus 
both the train data and the data held out for testing. shield machin learn from thi taint may 
requir a much more aggress strategi that assum that the actual distribut of qualiti and 
capac across the popul be far more equal than compromis data might suggest. recent 
scholarship have further complic thi debat by establish the imposs of achiev pariti 
across a set of intuit measur of fair when group differ in their underli rate of import 
behaviors.8 thu hard choic will be necessary, and deep understand required. 

these discuss need to involv a more divers rang of stakeholders, both to improv the qualiti of the 
research and to gain legitimaci and buy-in. the valu underli the differ notion of fair need to 
be debat more openli and explicitly. comput scientist can contribut in mani way here—e.g., by 
offer technic solutions, such a taxonomi for fair and algorithm for achiev fairness. 
invest should provid the resources, both materi and intellectual, to support and foster these 
discussions, and to push the field to develop tool that make clear the full rang of possibl for 
defin and achiev fairness. futur research will also need to consid how organ would 
deploy these method in practice. paradoxically, most of the method propos so far can onli achiev 
fair by take class membership (e.g., gender) into account explicitly. In other words, organ 
would have to collect inform that could easili serv a the basi for intent discrimin in 

8 kleinberg, jon, sendhil mullainathan, and manish raghavan, "inher trade-off in the fair determin of risk scores," 
arxiv:1609.05807, 2016. 
chouldechova, alexandra, "fair predict with dispar impact: A studi of bia in recidiv predict instruments," arxiv: 
1703.00056, 2017. 
corbett-davies, sam, emma pierson, avi feller, sharad goel, and aziz huq, "algorithm decis make and the cost of 
fairness," arxiv:1701.08230, 2017. 



5 

order to prevent unintent discrimination. organ might also balk at demand to collect more 
information, even in the interest of improv how well machin learn perform for minor groups, if 
do so would appear to intrud on people’ privacy, involv signific expense, or creat a perceiv 
acknowledg of wrongdo or even actual liability. 

provid transpar into and control over the data-driven infer make about citizen 
discuss and analysi of fair and bia in algorithm decision-mak be stymi by the difficulti 
for citizens, users, regulators, and even research to understand the reason that data-driven infer 
be made. research into build so-cal “interpretable” or “comprehensible” machine-learn model 
have receiv some attent for decades, but mani research and practition still believ that such 
models—beyond the simplest—ar essenti black boxes. recent work suggest that we can build 
model that be possibl for human to meaning inspect without sacrific accuracy,9 but thi area 
need substanti more attention, both in clarifi what count a “interpretable” or “comprehensible” 
and when such properti be desir or necessary.10 

however, build such model be onli one strand of research into transpar of data-driven inferences. 
often the crucial interest be not in understand the model, but in understand the precis reason for a 
particular inference. As discuss above: I be deni credit. why? research into explain the 
decis make by data-driven system be even sparser than research into comprehens models—but 
possibl more practic useful. encouragingly, explain an individu decis may actual be easi 
than explain a complex model behind the decision.11 for example, one may take a counterfactu 
approach:12 consid the algorithm input a a collect of evidence, what be the minim set of 
evid the remov of which would inhibit the inference? If we be interest in provid an 
explan that might inform futur behavior, we might instead ask: which featur (characteristics, 
aspect of behavior, etc.) would be the least costli for an individu to change, so a to produc the 
desir chang in the inference?13 We need much more research on explain inferences, and do so 
effici and effectively, befor we can be confid that we be inde give suffici transparency. 

As a society, we also may want to give citizen control over the data that be use to make infer 
about them. someon may not want their visit histori to gay right websit to be use in decisions— 
automat or otherwise—that be make about him or her. decision-specif transpar seem to be a 
prerequisit for give such control.14 

more deeply, in order truli to understand the civil right implic of data-driven systems, we don’t 
onli need to understand the model and the decis that they make, but we also need to understand whi 
the model be a they are! thi tie model comprehens to all the other research stream describ 
in thi document, a model be a they be becaus of the select of machin learn algorithm, the 
select of train data (and evalu data), and more insidiously, mani other decis make in the 
process of formul the problem and the evolut of the system. 

look beyond the algorithm for the sourc of unfairness, discrimination, etc. 

9 zeng, jiaming, berk ustun, and cynthia rudin, "interpret classif model for recidiv prediction," 
arxiv:1503.07810, 2015. 
10 lipton, zachari C, "the mytho of model interpretability," arxiv:1606.03490, 2016. 
11 martens, david, and foster provost, "explain data-driven document classifications," mi quarterli 38.1 , pp 73-99, 
2014. 
12 ibid. and chen, et al. “enhanc transpar and control when draw data-driven infer about individuals,” 2016 
icml workshop on human interpret in machin learning, https://arxiv.org/abs/1606.08063. 
13 nb: thi be a much more difficult problem than the former, a it requir model causal relations, costs/benefits, and 
statist depend involv the individual, rather than just causal relat between the input and output of the model. 
14 ibid. and chen, et al. “enhanc transpar and control when draw data-driven infer about individuals,” 2016 
icml workshop on human interpret in machin learning, https://arxiv.org/abs/1606.08063. 



6 

one veri crucial aspect of data-driven decision-mak be onli just begin to be take into account in 
discuss of and research into ethics, data science, and civil rights, although it do not surpris savvi 
practitioners: the technic formul of the problem make all the difference. It have long be accept 
within data scienc circl that build data-driven system be a process15 that involv care 
understand the problem to be addressed, understand the data avail (sometim at a cost), and 
formul the problem to which machin learning/statist infer algorithm will be applied. take 
a an exampl supervis predict modeling. formul the problem involves: decid on the 
instanc to be modeled, craft a definit of the target variable, obtain “labels” (ground truth or 
proxi for it) for the train data, select an appropri sampl of the data from which to train, and 
then engin a set of featur that will be predict (or use algorithm that build the featur 
autonomously). In practice, each of these choic often incorpor approximations, proxies, surrogates, 
and biases. In the ideal case, these be chosen with full consider of the like (side) effects, but in 
practice, not onli be the consequ of the choic not apparent; sometim the bia and 
approxim be not even well understood by the research and practitioners. for example, an unseen 
racial bia in prior decision-mak may be recapitul in model learn from those data. An 
unrecogn select bia in data sampl may miss a critic subpopulation—for example, consid 
sampl data from user of smartphones. 

A robust understand of the ethic use of data-driven system need substanti focu on the possibl 
threat to civil right that may result from the formul of the problem. such threat be insidious, 
becaus problem formul be iterative. mani decis be make earli and quickly, befor there be ani 
notion that the effort will lead to a success system, and onli rare be prior problem-formul 
decis revisit with a critic eye. In addition, system whose underli knowledg evolv over 
time, be it via continu machin learn or manual intervention, may themselv be make implicit 
decis on (for example) the select of the population. what be the implic for fair treatment, if 
fair treatment be not consid in the design of the select mechanisms? A system that start with a 
small bia may unwittingli magnifi it. 

creat cross-disciplinari scholar 
the recognit that applic of big data and data scienc can implic civil right have spur call 
for great involv of social scientists, lawyers, and polici expert in the development, deployment, 
and review of data-driven systems. while laudable, call for cross-disciplinari collaboration, and 
especi collabor research, on these issu rare consid the challeng that member from these 
differ commun will face when attempt to engag with one another. those who work with data 
science, includ comput scientists, be rare train in law and policy; expert in civil right rare 
have a background in machin learn or comput statistics. neither be well prepar to identifi the 
mani way that a particular applic of data scienc may implic civil rights—or what to do in such 
cases. 

the difficulti of cross-disciplinari collabor have alreadi lead to a rather troubl pattern in work on 
civil right and data science. critic write often struggl to recogn how machine-learn system 
differ from other type of formal decision-mak and how these differ introduc novel danger 
for civil rights, onli some of which can be effect address with standard polici instruments. 
likewise, data scienc research concern with civil right have tend to tackl issu of fair a if 
such weighti topic have not alreadi receiv consider attent in law, social science, and 
philosophy. 

work integr civil right and data scienc cannot be easili divid between collaborators, where the 
more expert team member would handl each task. meaning legal analysi will requir technic 

15 F. provost & T. fawcett, data scienc for business, o’reilli media, 2013. 



7 

expertise; rigor technic review will depend on a nuanc understand of legal concepts. valuabl 
breakthrough be most like to come from research who combin expertis in both domains. futur 
invest in research should foster collabor that do more than put differ commun in 
contact; invest should support the train necessari to cultiv a futur gener of research 
who be simultan expert in both fields. 

while facilit of collabor between research be important, a workforc that understand the 
relationship between ethic and data be also key. the recent groundswel of interest in data science, and 
the emerg of new interdisciplinari graduat and undergradu program in thi area, provid a 
natur opportun here. the challeng be that these curriculum be alreadi overli crowd and often 
patch togeth from exist cours in differ departments. ad a singl cours about data, ethics, 
algorithm bias, fairness, account and the law to such a program would be a good start; a good 
idea would be to thread the associ idea through all of the cours in a coher manner. As data 
scienc method and tool becom integr and essenti element of research across a broad rang of 
disciplines, it would be worthwhil to broaden the exist research ethic train requir to 
includ these aspects. A national-level convers about those import ideas—what they be and how 
to teach them—could help individu institut with those initiatives. 

needless to say, it will be import to engag the educ research commun in that conversation. 
data scienc be also make inroad into the high-school curriculum, which provid even earli 
opportun to weav togeth knowledg about ethics, law, and data. there the issu be somewhat 
different; not onli crowd curricula, but also the forc of standard testing, a well a teacher 
training. develop of easy-to-deploy materi that convey the import concept and issues, while 
also align with exist learn objectives, will be essenti to success here. this, too, should involv 
the educ research community. initi like the nation scienc foundation’ “stem + c”16 
and the nation academi of engineering’ seri of workshop in thi area17 can also use inform 
these conversations. 




















16 “stem + comput partnership (stem+c),” nation scienc foundation, 2017, 
https://www.nsf.gov/funding/pgm_summ.jsp?pims_id=505006. 
17 “roundtabl on data scienc post-secondari educ meet #5 integr societ and ethic issu into data science,” 
the nation academi of scienc engin medicine, 2017, 
http://sites.nationalacademies.org/deps/bmsa/deps_178021. 



8 

appendix 
sever recent meet and report have brought to light issu and concern regard big data, data 
science, algorithm decision-making, and civil rights: 


● the white hous issu big data report in 2014, notabl rais concern about discrimin 
and the inscrut of algorithm and call for more research to establish how data scienc 
might implic civil right and how to mitig against these dangers. 

● the feder trade commiss issu it own report at the start of 2016, lay out how exist 
law appli to commerci use of data scienc that rais concern with discrimin and fairness, 
but also note gap in polici where commerci actor should exercis care judgment, despit 
the lack of clear technic or ethic guidance. 

● the white hous follow with a more detail and narrowli focu report in 2016 on “big 
data: A report on algorithm systems, opportunity, and civil rights”, which consid how 
data scienc could be a boon—but also a threat—to civil right in the area of consum credit, 
employment, education, and crimin justice. 

● alongsid thi report, the white hous also releas a document outlin a nation privaci 
research strategy, which, notably, call for new work on the danger pose by “analyt 
algorithms” a issu distinct from tradit privaci concerns. 

● the white hous recent issu anoth report on artifici intellig that laid out in great 
technic detail concern with "fairness, safety, and governance”—and the need for further 
invest in and research on these topics. 




for citat use: barocas, bradley, honavar, & provost. (2017). big data, data science, and civil rights: 
A white paper prepar for the comput commun consortium committe of the comput 
research association. http://cra.org/ccc/resources/ccc-led-whitepapers/ 


thi materi be base upon work support by the nation scienc foundat under grant no. 
1136993. ani opinions, findings, and conclus or recommend express in thi materi be 
those of the author and do not necessarili reflect the view of the nation scienc foundation. 


