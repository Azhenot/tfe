


















































under review a a confer paper at iclr 2017 

gradient OF counterfactu 

mukund sundararajan, ankur tali & qiqi yan 
googl inc. 
mountain view, CA 94043, usa 
{mukunds,ataly,qiqiyan}@google.com 

abstract 

gradient have be use to quantifi featur import in machin learn mod- 
els. unfortunately, in nonlinear deep networks, not onli individu neuron but 
also the whole network can saturate, and a a result an import input featur can 
have a tini gradient. We studi variou networks, and observ that thi phenomenon 
be inde widespread, across mani inputs. 

We propos to examin interior gradients, which be gradient of counterfactu 
input construct by scale down the origin input. We appli our method to the 
googlenet architectur for object recognit in images, a well a a ligand-bas 
virtual screen network with categor featur and an lstm base languag 
model for the penn treebank dataset. We visual how interior gradient good 
captur featur importance. furthermore, interior gradient be applic to a 
wide varieti of deep networks, and have the attribut properti that the featur 
import score sum to the the predict score. 

best of all, interior gradient can be comput just a easili a gradients. In 
contrast, previou method be complex to implement, which hinder practic 
adoption. 

1 introduct 

practition of machin learn regularli inspect the coeffici of linear model a a measur of 
featur importance. thi process allow them to understand and debug these models. the natur 
analog of these coeffici for deep model be the gradient of the predict score with respect 
to the input. for linear models, the gradient of an input featur be equal to it coefficient. for deep 
nonlinear models, the gradient can be thought of a a local linear approxim (simonyan et al. 
(2013)). unfortunately, (see the next section), the network can satur and a a result an import 
input featur can have a tini gradient. 

while there have be other work (see section 2.8) to address thi problem, these techniqu involv 
instrument the network. thi instrument current involv signific develop effort be- 
caus they be not primit oper in standard machin learn libraries. besides, these tech- 
niqu be not simpl to understand—they invert the oper of the network in differ ways, and 
have their own peculiarities—for instance, the featur import be not invari over network 
that comput the exact same function (see figur 14). 

In contrast, the method we propos build on the veri familiar, primit concept of the gradient—al 
it involv be inspect the gradient of a few care chosen counterfactu input that be scale 
version of the initi input. thi allow anyon who know how to extract gradients—presum 
even novic practition that be not veri familiar with the network’ implementation—to debug 
the network. ultimately, thi seem essenti to ensur that deep network perform predict 
when deployed. 

1 

ar 
X 

iv 
:1 

61 
1. 

02 
63 

9v 
2 

[ 
c 

.L 
G 

] 
1 

5 
N 

ov 
2 

01 
6 



under review a a confer paper at iclr 2017 

(a) origin image. 

(b) ablat image. 

figur 1: pixel import use gradient at the image. 

2 our techniqu 

2.1 gradient DO not reflect featur import 

let u start by investig the perform of gradient a a measur of featur importance. We 
use an object recognit network built use the googlenet architectur (szegedi et al. (2014)) a 
a run example; we refer to thi network by it codenam inception. (we present applic 
of our techniqu to other network in section 3.) the network have be train on the imagenet 
object recognit dataset (russakovski et al. (2015)). It be be 22 layer deep with a softmax layer on 
top for classifi imag into one of the 1000 imagenet object classes. the input to the network be 
a 224× 224 size rgb image. 
befor evalu the use of gradient for featur importance, we introduc some basic notat that 
be use throughout the paper. 

We repres a 224× 224 size rgb imag a a vector in r224×224×3. let incpl : r224×224×3 → 
[0, 1] be the function repres by the incept network that comput the softmax score for the 
object class label L. let 5incpl(img) be the gradient of incpl at the input imag img. thus, 
the vector 5incpl(img) be the same size a the imag and lie in r224×224×3. As a shorthand, we 
write5incpli,j,c(img) for the gradient of a specif pixel (i, j) and color channel c ∈ {r,g,b}. 

We comput the gradient of incpl (with respect to the image) for the highest-scor object class, 
and then aggreg the gradients5incpl(img) along the color dimens to obtain pixel import 
scores.1 

∀i, j : pli,j(img) ::= σc∈{r,g,b}| 5 incp 
L 
i,j,c(img)| (1) 

next, we visual pixel import score by scale the intens of the pixel in the origin 
imag in proport to their respect scores; thus, high the score brighter would be the pixel. 
figur 1a show a visual for an imag for which the high score object class be “reflex 
camera” with a softmax score of 0.9938. 

1 these pixel import score be similar to the gradient-bas salienc map defin by simonyan et al. 
(2013) with the differ be in how the gradient be aggreg along the color channel. 

2 



under review a a confer paper at iclr 2017 

intuitively, one would expect the the high gradient pixel for thi classif to be one fall 
on the camera or those provid use context for the classif (e.g., the len cap). however, 
most of the highlight pixel seem to be on the left or abov the camera, which to a human seem 
not essenti to the prediction. thi could either mean that (1) the highlight pixel be somehow 
import for the intern comput perform by the incept network, or (2) gradient of the 
imag fail to appropri quantifi pixel importance. 

let u consid hypothesi (1). In order to test it we ablat part of the imag on the left and abov 
the camera (bi zero out the pixel intensities) and run the ablat imag through the incept 
network. see figur 1b. the top predict categori still remain “reflex camera” with a softmax 
score of 0.9966 — slightli high than before. thi indic that the ablat portion be inde 
irrelev to the classification. On comput gradient of the ablat image, we still find that most 
of the high gradient pixel lie outsid of the camera. thi suggest that for thi image, it be in fact 
hypothesi (2) that hold true. upon studi more imag (see figur 4), we find that the gradient 
often fail to highlight the relev pixel for the predict object label. 

2.2 satur 

In theory, it be easi to see that the gradient may not reflect featur import if the predict 
function flatten in the vicin of the input, or equivalently, the gradient of the predict function 
with respect to the input be tini in the vicin of the input vector. thi be what we call saturation, 
which have also be report in previou work (shrikumar et al. (2016), glorot & bengio (2010)). 

We analyz how widespread satur be in the incept network by inspect the behavior of 
the network on counterfactu imag obtain by uniformli scale pixel intens from zero 
to their valu in an actual image. formally, give an input imag img ∈ r224×224×3, the set of 
counterfactu imag be 

{α img | 0 ≤ α ≤ 1} (2) 

figur 2a show the trend in the softmax output of the high score class, for thirti randomli 
chosen imag form the imagenet dataset. more specifically, for each imag img, it show the trend 
in incpl(α img) a α vari from zero to one with L be the label of high score object class 
for img. It be easi to see that the trend flatten (saturates) for all imag α increases. notic that 
satur be present even for imag whose final score be significantli below 1.0. moreover, for a 
major of images, satur happen quit soon when α = 0.2. 

one may argu that sinc the output of the incept network be the result of appli the softmax 
function to a vector of activ values, the satur be expect due to the squash properti of 
the softmax function. however, a show in figur 2b, we find that even the pre-softmax activ 
score for the high score class saturate. 

In fact, to our surprise, we found that the satur be inher present in the incept network and 
the output of the intermedi layer also saturate. We plot the distanc between the intermedi 
layer neuron activ for a scale down input imag and the actual input imag with respect to 
the scale parameter, and find that the trend flattens. due to lack of space, we provid these plot 
in figur 12 in the appendix. 

It be quit clear from these plot that satur be widespread across imag in the incept network, 
and there be a lot more activ in the network for counterfactu imag at rel low valu of 
the scale paramet α. thi observ form the basi of our techniqu for quantifi featur 
importance. 

note that it be well know that the satur of gradient prevent the model from converg to 
a good qualiti minimum (glorot & bengio (2010)). So one may expect good qualiti model to 
not have satur and henc for the (final) gradient to convey featur importance. clearly, our 
observ on the incept model show that thi be not the case. It have good predict accuracy, 
but also exhibit satur (see figur 2). our hypothesi be that the gradient of import featur 
be not satur earli in the train process. the gradient onli satur after the featur have 
be learn adequately, i.e., the input be far away from the decis boundary. 

3 



under review a a confer paper at iclr 2017 

2.3 interior gradient 

We studi the import of input featur in a predict make for an input by examin the gra- 
dient of the counterfactu obtain by scale the input; we call thi set of gradient interior 
gradients. 

while the method of examin gradient of counterfactu input be broadli applic to a wide 
rang of networks, we first explain it in the context of inception. here, the counterfactu imag 
input we consid be obtain by uniformli scale pixel intens from zero to their valu in 
the actual imag (thi be the same set of counterfactu that be use to studi saturation). the 
interior gradient be the gradient of these images. 

interiorgrads(img) ::= {5incp(α img) | 0 ≤ α ≤ 1} (3) 

these interior gradient explor the behavior of the network along the entir scale curv depict 
in figur 2a, rather than at a specif point. We can aggreg the interior gradient along the color 
dimens to obtain interior pixel import score use equat 1. 

interiorpixelimportance(img) ::= {p(α img) | 0 ≤ α ≤ 1} (4) 

We individu visual the pixel import score for each scale paramet α by scale the 
intens of the pixel in the actual imag in proport to their scores. the visual show 
how the import of each pixel evolv a we scale the image, with the last visual be 
ident to one gener by gradient at the actual image. In thi regard, the interior gradient offer 
strictli more insight into pixel import than just the gradient at the actual image. 

figur 3 show the visual for the “reflex camera” imag from figur 1a for variou valu of 
the scale paramet α. the plot in the top right corner show the trend in the absolut magnitud 
of the averag pixel import score. the magnitud be significantli larg at low valu of α and 
nearli zero at high valu — the latter be a consequ of saturation. note that each visual 
be onli indic of the rel distribut of the import score across pixel and not the 
absolut magnitud of the scores, i.e., the late snapshot be respons for tini increas in the 
score a the chart in the top right depicts. 

the visual show that at low valu of α, the pixel that lie on the camera be most impor- 
tant, and a α increases, the region abov the camera gain importance. given the high magnitud 
of gradient at low valu of α, we consid those gradient to be the primari driver of the final 
predict score. they be more indic of featur import in the predict compar to the 
gradient at the actual imag (i.e., when α = 1). 

the visual of the interior pixel gradient can also be view togeth a a singl anim 
that chain the visual in sequenc of the scale parameter. thi anim offer a concis 
yet complet summari of how pixel import move around the imag a the scale paramet 
increas from zero to one. 

rationale. while measur satur via counterfactu seem natural, use them for quanti- 
fy featur import deserv some discussion. the first thing one may tri to identifi featur 
import be to examin the deep network like one would with human author code. thi seem 
hard; just a deep network employ distribut represent (such a embeddings), they perform 
convolut (pun intended) distribut reasoning. So instead, we choos to probe the network with 
sever counterfactu input (relat to the input at hand), hop to trigger all the intern work- 
ing of the network. thi process would help summar the effect of the network on the protagonist 
input; the assumpt be that the input be human understandable. naturally, it help to work with 
gradient in thi process a via back propagation, they induc an aggreg view over the function 
comput by the neurons. 

interior gradient use counterfactu input to artifactu induc a procedur on how the network 
attent move across the imag a it comput the final predict score. from the animation, 
we gather that the network focu on strong and distinct pattern in the imag at low valu 
of the scale parameter, and subtl and weak pattern in the imag at high values. thus, we 
specul that the network’ comput can be loos abstract by a procedur that first recogn 
distinct featur of the imag to make an initi prediction, and then fine tune (these be small 
score jump a the chart in figur 3 shows) the predict use weaker pattern in the image. 

4 



under review a a confer paper at iclr 2017 

(a) softmax score for top label (b) pre-softmax score for top label 

figur 2: satur in incept 

input imag and trend of the pixel import score obtain from interior gradients. 

α = 0.02 α = 0.04 α = 0.06 α = 0.08 α = 0.1 

α = 0.2 α = 0.4 α = 0.6 α = 0.8 α = 1.0 

figur 3: visual of interior gradients. notic that the visual at low valu of the 
scale paramet (α) be sharper and much good at surfac import featur of the input image. 

2.4 cumul interior gradient 

A differ summar of the interior gradient can be obtain by cumul them. while there 
be a few way of cumul counterfactu gradients, the approach we take have the nice attribut 
properti (proposit 1) that the featur import score approxim add up to the predict 
score. the featur import score be thu also refer to a attributions. 

notic that the set of counterfactu imag {α img | 0 ≤ α ≤ 1} fall on a straight line path in 
r224×224×3. interior gradient — which be the gradient of these counterfactu imag — can 
be cumul by integr them along thi line. We call the result gradient a integr 
gradients. In what follows, we formal integr gradient for an arbitrari function F : Rn → 
[0, 1] (repres a deep network), and an arbitrari set of counterfactu input fall on a path in 
rn. 

5 



under review a a confer paper at iclr 2017 

let x ∈ Rn be the input at hand, and γ = (γ1, . . . , γn) : [0, 1]→ Rn be a smooth function specifi 
the set of counterfactuals; here, γ(0) be the baselin input (for inception, a black image), and γ(1) 
be the actual input (for inception, the imag be studied). specifically, {γ(α) | 0 ≤ α ≤ 1} be the 
set of counterfactu (for inception, a seri of imag that interpol between the black imag and 
the actual input). 

the integr gradient along the ith dimens for an input x ∈ Rn be defin a follows. 

integratedgradsi(x) ::= 

∫ 1 
α=0 

∂F (γ(α)) 
∂γi(α) 

∂γi(α) 
∂α dα (5) 

where ∂F (x)∂xi be the gradient of F along the i 
th dimens at x. 

A nice technic properti of the integr gradient be that they add up to the differ between the 
output of F at the final counterfactu γ(1) and the baselin counterfactu γ(0). thi be formal 
by the proposit below, which be an instanti of the fundament theorem of calculu for path 
integrals. 

proposit 1 If F : Rn → R be differenti almost everywher 2, and γ : [0, 1] → Rn be smooth 
then 

σni=1integratedgradsi(x) = F (γ(1))− F (γ(0)) 

for most deep networks, it be possibl to choos counterfactu such that the predict at the base- 
line counterfactu be near zero (F (γ(0)) ≈ 0). for instance, for the incept network, the coun- 
terfactu defin by the scale path satisfi thi properti a incp(0224×224×3) ≈ 0. In such cases, 
it follow from the proposit that the integr gradient form an attribut of the predict 
output F (x), i.e., they almost exactli distribut the output to the individu input features. 

the addit properti provid a form of saniti check for the integr gradient and ensur 
that we do not under or over attribut to features. thi be a common pitfal for attribut scheme 
base on featur ablations, wherein, an ablat may lead to small or a larg chang in the predict 
score depend on whether the ablat featur interact disjunct or conjunct to the rest of 
the features. thi addit be even more desir when the network score be numer critical, 
i.e., the score be not use pure in an ordin sense. In thi case, the attribut (togeth with 
additivity) guarante that the attribut be in the unit of the score, and account for all of the 
score. 

We note that these path integr of gradient have be use to perform attribut in the context 
of small non-linear polynomi (sun & sundararajan (2011)), and also within the cost-shar 
literatur in econom where function at hand be a cost function that model the cost of a project 
a a function of the demand of variou participants, and the attribut correspond to cost-shares. 
the specif path we use correspond to a cost-shar method call aumann-shapley (aumann & 
shapley (1974)). 

comput integr gradients. the integr gradient can be effici approxim by rie- 
mann sum, wherein, we simpli sum the gradient at point occur at suffici small interv 
along the path of counterfactuals. 

integratedgradsapproxi (x) ::= Σ 
m 
k=1 

∂F (γ(k/m)) 
∂γi(α) 

(γ( km )− γ( 
k−1 
m )) (6) 

here m be the number of step in the riemman approxim of the integral. notic that the 
approxim simpli involv comput the gradient in a for loop; comput the gradient be 
central to deep learn and be a pretti effici operation. the implement should therefor 
be straightforward in most deep learn frameworks. for instance, in tensorflow (ten), it es- 
sential amount to call tf.gradient in a loop over the set of counterfactu input (i.e., 
γ( km ) for k = 1, . . . ,m), which could also be batched. go forward, we abus the term “inte- 
grate gradients” to refer to the approxim describ above. 

2formally, thi mean that the partial deriv of F along each input dimens satisfi lebesgue’ inte- 
grabil condition, i.e., the set of discontinu point have measur zero. deep network built out of sigmoids, 
relus, and pool oper should satisfi thi condition. 

6 



under review a a confer paper at iclr 2017 

integr gradient for inception. We comput the integr gradient for the incept network 
use the counterfactu obtain by scale the input image; γ(α) = α img where img be the input 
image. similar to the interior gradients, the integr gradient can also be aggreg along the 
color channel to obtain pixel import score which can then be visual a discuss earlier. 
figur 4 show these visual for a bunch of images. for comparison, it also present the 
correspond visual obtain from the gradient at the actual image. from the visualizations, 
it seem quit evid that the integr gradient be good at captur import features. 

attribut be independ of network implementation. two network may be function 
equivalent3 despit have veri differ intern structures. see figur 14 in the appendix for an 
example. ideally, featur attribut should onli be determin by the function of the network 
and not it implementation. attribut gener by integr gradient satisfi thi properti by 
definit sinc they be base onli on the gradient of the function repres by the network. 

In contrast, thi simpl properti do not hold for all featur attribut method know to us, 
including, deeplift (shrikumar et al. (2016)), layer-wis relev propag (lrp) (binder 
et al. (2016)), deconvolut network (deconvnets) (zeiler & fergu (2014)), and guid back- 
propag (springenberg et al. (2014)) (a counter-exampl be give in figur 14). We discu these 
method in more detail in section 2.8 

2.5 evalu our approach 

We discu an evalu of integr gradient a a measur of featur importance, by compar 
them against (final) gradients. 

pixel ablations. the first evalu be base on a method by samek et al. (2015). here we ablate4 
the top 5000 pixel (10% of the image) by import score, and comput the score drop for the 
high score object class. the ablat be perform 100 pixel at a time, in a sequenc of 50 
steps. At each perturb step k we measur the averag drop in score up to step k. thi quantiti 
be refer to a area over the perturb curv (aopc) by samek et al. (2015). 

figur 5 show the aopc curv with respect to the number of perturb step for integr 
gradient and gradient at the image. aopc valu at each step repres the averag over a dataset 
of 150 randomli chosen images. It be clear that ablat the top pixel identifi by integr 
gradient lead to a larg score drop that those identifi by gradient at the image. 

have say that, we note an import issu with the technique. the imag result from pixel 
perturb be often unnatural, and it could be that the score drop simpli becaus the network have 
never see anyth like it in training. 

localization. the second evalu be to consid imag with human-drawn bound box 
around objects, and comput the percentag of pixel attribut insid the bound box. We use the 
2012 imagenet object local challeng dataset to get a set of human-drawn bound boxes. 
We run our evalu on 100 randomli chosen imag satisfi the follow properti — (1) 
the total size of the bound box(es) be less than two third of the imag size, and (2) ablat the 
bound box significantli drop the predict score for the object class. (1) be for ensur that 
the box be not so larg that the bulk of the attribut fall insid them by definition, and (2) be 
for ensur that the box part of the imag be inde respons for the predict score for the 
image. We find that on 82 imag the integr gradient techniqu lead to a high fraction of 
the pixel attribut insid the box than gradient at the actual image. the averag differ in the 
percentag pixel attribut insid the box for the two techniqu be 8.4%. 

while these result be promising, we note the follow caveat. integr gradient be meant to 
captur pixel import with respect to the predict task. while for most objects, one would 
expect the pixel locat on the object to be most import for the prediction, in some case the 

3formally, two network F and G be function equival if and onli if ∀ x : F (x) = g(x). 
4ablat in our set amount to zero out (or black out) the intens for the R, G, B channels. We 

view thi a a natur mechan for remov the inform carri by the pixel (than, say, random the 
pixel’ intens a propos by samek et al. (2015), especi sinc the black imag be a natur baselin for 
vision tasks. 

7 



under review a a confer paper at iclr 2017 

context in which the object occur may also contribut to the prediction. the cabbag butterfli 
imag from figur 4 be a good exampl of thi where the pixel on the leaf be also surfac by the 
integr gradients. 

eyeballing. ultimately, it be hard to come up with a perfect evalu technique. So we do spend 
a larg amount of time appli and eyebal the result of our techniqu to variou networks— 
the one present in thi paper, a well a some network use within products. for the incept 
network, we welcom you to eyebal more visual in figur 11 in the appendix and also at: 
https://github.com/ankurtaly/attributions. while we found our method to beat 
gradient at the imag for the most part, thi be clearli a subject process prone to interpret 
and cherry-picking, but be also ultim the measur of the util of the approach—debug 
inher involv the human. 

finally, also note that we do not compar against other whitebox attribut techniqu (e.g., 
deeplift (shrikumar et al. (2016))), becaus our focu be on black-box techniqu that be easi to 
implement, so compar against gradient seem like a fair comparison. 

2.6 debug network 

despit the widespread applic of deep neural network to problem in scienc and technology, 
their intern work larg remain a black box. As a result, human have a limit abil to 
understand the predict make by these networks. thi be view a hindranc in scenario where 
the bar for precis be high, e.g., medic diagnosis, obstacl detect for robots, etc. (dar (2016)). 
quantifi featur import for individu predict be a first step toward understand the 
behavior of the network; at the veri least, it help debug misclassifi inputs, and saniti check the 
intern workings. We present evid to support thi below. 

We use featur import to debug misclassif make by the incept network. In particular, 
we consid imag from the imagenet dataset where the groundtruth label for the imag not in 
the top five label predict by the incept network. We use interior gradient to comput pixel 
import score for both the incept label and the groundtruth label, and visual them to gain 
insight into the caus for misclassification. 

figur 6 show the visual for two misclassifi images. the top imag genuin have two 
objects, one correspond to the groundtruth label and other correspond to the incept label. 
We find that the interior gradient for each label be abl to emphas the correspond objects. 
therefore, we suspect that the misclassif be in the rank logic for the label rather than 
the recognit logic for each label. for the bottom image, we observ that the interior gradient 
be larg similar. moreover, the cricket get emphas by the interior gradient for the manti 
(incept label). thus, we suspect thi to be a more seriou misclassification, stem from the 
recognit logic for the mantis. 

2.7 discuss 

faithfullness. A natur question be to ask whi gradient of counterfactu obtain by scale 
the input captur featur import for the origin image. first, from studi the visual 
in figur 4, the result look reason in that the highlight pixel captur featur repres 
of the predict class a a human would perceiv them. second, we confirm that the network too 
seem to find these featur repres by perform ablations. It be somewhat natur to expect 
that the incept network be robust to to chang in input intensity; presum there be some low 
bright imag in the train set. 

however, these counterfactu seem reason even for network where such scale do not cor- 
respond to a natur concept like intensity, and when the counterfactu fall outsid the train set; 
for instanc in the case of the ligand-bas virtual screen network (see section 3.1). We specul 
that the reason whi these counterfactu make sens be becaus the network be built by compos 
relus. As one scale the input start from a suitabl baseline, variou neuron activate, and the 
scale process that do a somewhat thorough job of explor all these event that contribut to 
the predict for the input. there be an analog argument for other oper such a max pool, 
averag pool, and softmax—her the trigger event arent discret but the argument be analogous. 

8 

https://github.com/ankurtaly/attribut 


under review a a confer paper at iclr 2017 

limit of approach. We discu some limit of our technique; in a sens these be 
limit of the problem statement and appli equal to other techniqu that attribut to base 
input features. 

• inabl to captur featur interactions: the model could perform logic that effec- 
tive combin featur via a conjunct or an implication-lik operations; for instance, 
it could be that a molecul bind to a site if it have a certain structur that be essenti a 
conjunct of certain atom and certain bond between them. attribut or import 
score have no way to repres these interactions. 

• featur correlations: featur correl be a bane to the understand of all ma- 
chine learn models. If there be two featur that frequent co-occur, the model be free 
to assign weight to either or both features. the attribut would then respect thi weight 
assignment. but, it could be that the specif weight assign chosen by the model be 
not human-intelligible. though there have be approach to featur select that reduc 
featur correl (yu & liu (2003)), it be unclear how they appli to deep model on 
dens input. 

2.8 relat work 

over the last few years, there have be a vast amount work on demystifi the inner work 
of deep networks. most of thi work have be on network train on comput vision tasks, and 
deal with understand what a specif neuron comput (erhan et al. (2009); Le (2013)) and 
interpret the represent captur by neuron dure a predict (mahendran & vedaldi 
(2015); dosovitskiy & brox (2015); yosinski et al. (2015)). 

our work instead focu on understand the network’ behavior on a specif input in term of the 
base level input features. our techniqu quantifi the import of each featur in the prediction. 
known approach for accomplish thi can be divid into three categories. 

gradient base methods. the first approach be to use gradient of the input featur to quantifi 
featur import (baehren et al. (2010); simonyan et al. (2013)). thi approach be the easi to 
implement. however, a discuss earlier, naiv use the gradient at the actual input do not 
accur quantifi featur import a gradient suffer from saturation. 

score back-propag base methods. the second set of approach involv back-propag 
the final predict score through each layer of the network down to the individu features. 
these includ deeplift (shrikumar et al. (2016)), layer-wis relev propag (lrp) (binder 
et al. (2016)), deconvolut network (deconvnets) (zeiler & fergu (2014)), and guid back- 
propag (springenberg et al. (2014)). these method larg differ in the backpropag logic 
for variou non-linear activ functions. while deconvnets, guid back-propag and lrp 
reli on the local gradient at each non-linear activ function, deeplift reli on the deviat in 
the neuron’ activ from a certain baselin input. 

similar to integr gradients, the deeplift and lrp also result in an exact distribut of the 
predict score to the input features. however, a show by figur 14, the attribut be not 
invari across function equival networks. besides, the primari advantag of our method 
over all these method be it eas of implementation. the aforesaid method requir knowledg of 
the network architectur and the intern neuron activ for the input, and involv implement 
a somewhat complic back-propag logic. On the other hand, our method be agnost to the 
network architectur and reli onli on comput gradient which can do effici in most 
deep learn frameworks. 

model approxim base methods. the third approach, propos first by ribeiro et al. 
(2016a;b), be to local approxim the behavior of the network in the vicin of the input be- 
ing explain with a simpler, more interpret model. An appeal aspect of thi approach be that 
it be complet agnost to the structur of the network and onli deal with it input-output behav- 
ior. the approxim be learn by sampl the network’ output in the vicin of the input at 
hand. In thi sense, it be similar to our approach of use counterfactuals. sinc the counterfactu 
be chosen somewhat arbitrarily, and the approxim be base pure on the network’ output at 

9 



under review a a confer paper at iclr 2017 

figur 4: compar integr gradient with gradient at the image. left-to-right: origin 
input image, label and softmax score for the high score class, visual of integr gradi- 
ents, visual of gradient at the image. notic that the visual obtain from integr 
gradient be good at reflect distinct featur of the image. 

the counterfactuals, the faithful question be far more crucial in thi setting. the method be also 
expens to implement a it requir train a new model local around the input be explained. 

3 applic TO other network 

the techniqu of quantifi featur import by inspect gradient of counterfactu input be 
gener applic across deep networks. while for network perform vision tasks, the coun- 
terfactu input be obtain by scale pixel intensities, for other network they may be obtain 
by scale an emb represent of the input. 

10 



under review a a confer paper at iclr 2017 

figur 5: aopc (samek et al. (2015)) for integr gradient and gradient at image. 

figur 6: interior gradient of misclassifi images. left-to-right: origin image, softmax score 
for the top label assign by the incept network and the groundtruth label provid by imagenet, 
visual of integr gradient w.r.t. incept label, visual of integr gradient w.r.t. 
groundtruth label. 

As a proof of concept, we appli the techniqu to the molecular graph convolut network 
of kearn et al. (2016) for ligand-bas virtual screen and an lstm model (zaremba et al. 
(2014)) for the languag model of the penn treebank dataset (marcu et al. (1993)). 

3.1 ligand-bas virtual screen 

the ligand-bas virtual screen problem be to predict whether an input molecul be activ 
against a certain target (e.g., protein or enzyme). the process be meant to aid the discoveri of 
new drug molecules. deep network built use molecular graph convolut have recent be 
propos by kearn et al. (2016) for solv thi problem. 

onc a molecul have be identifi a activ against a target, the next step for medicin chemist 
be to identifi the molecular features—formally, pharmacophores5—that be respons for the ac- 
tivity. thi be akin to quantifi featur importance, and can be achiev use the method of 
integr gradients. the attribut obtain from the method help with identifi the domin 
molecular features, and also help saniti check the behavior of the network by shed light on 
it inner workings. with regard to the latter, we discu an anecdot late in thi section on how 
attribut surfac an anomali in w1n2 network architectur propos by kearn et al. (2016). 

defin the counterfactu inputs. the first step in comput integr gradient be to defin 
the set of counterfactu inputs. the network requir an input molecul to be encod by hand a 
a set of atom and atom-pair featur describ the molecul a an undirect graph. atom be 

5A pharmacophor be the ensembl of steric and electron featur that be necessari to ensur the a molecul 
be activ against a specif biolog target to trigger (or to block) it biolog response. 

11 



under review a a confer paper at iclr 2017 

featur use a one-hot encod specifi the atom type (e.g., C, O, S, etc.), and atom-pair be 
featur by specifi either the type of bond (e.g., single, double, triple, etc.) between the atoms, 
or the graph distanc between them 6 

the counterfactu input be obtain by scale down the molecul featur down to zero vectors, 
i.e., the set {αfeatures(mol) | 0 ≤ α ≤ 1} where features(mol) be an encod of the molecul 
into atom and atom-pair features. 

the care reader might notic that these counterfactu input be not valid featur of 
molecules. however, we argu that they be still valid input for the network. first, all opera- 
tor in the network (e.g., relus, linear filters, etc.) treat their input a continu real number 
rather than discret zero and ones. second, all field of the counterfactu input be bound be- 
tween zero and one, therefore, we don’t expect them to appear spuriou to the network. We discu 
thi further in section 2.7 

In what follows, we discu the behavior of a network base on the w2n2-simpl architectur 
propos by kearn et al. (2016). On inspect the behavior of the network over counterfactu 
inputs, we observ satur here a well. figur 13a show the trend in the softmax score for the 
task pcba-588342 for twenti five activ molecul a we vari the scale paramet α from zero 
to one. while the overal satur region be small, satur do exist near vicin of the input 
(0.9 ≤ α ≤ 1). figur 13b in the appendix show that the total featur gradient vari significantli 
along the scale path; thus, just the gradient at the molecul be fulli indic of the behavior of 
the network. 

visual integr gradients. We cumul the gradient of these counterfactu input to 
obtain an attribut of the predict score to each atom and atom-pair feature. unlik imag 
inputs, which have dens features, the set of input featur for molecul be sparse. consequently, 
the attribut be spars and can be inspect directly. figur 7 show heatmap for the atom and 
atom-pair attribut for a specif molecule. 

use the attributions, one can easili identifi the atom and atom-pair that that have a strongli pos- 
itiv or strongli neg contribution. sinc the attribut add up to the final predict score (see 
proposit 1), the attribut magnitud can be use for account the contribut of each fea- 
ture. for instance, the atom-pair that have a bond between them contribut cumul contribut 
46% of the predict score, while all other atom pair cumul contribut −3%. 
We present the attribut for 100 molecul activ against a specif task to a few chemists. 
the chemist be abl to immedi spot domin function group (e.g., aromat rings) be 
surfac by the attributions. A next step could be cluster the aggreg the attribut across a larg 
set of molecul activ against a specif task to identifi a common denomin of featur share 
by all activ molecules. 

identifi dead features. We now discu how attribut help u spot an anomali in the 
w1n2 architecture. On appli the integr gradient method to the w1n2 network, we found 
that sever atom in the same molecul receiv the exact same attribution. for instance, for the 
molecul in figur 7, we found that sever carbon atom at posit 2, 3, 14, 15, and 16 receiv 
the same attribut of 0.0043 despit be bond to differ atoms, for e.g., carbon at posit 3 
be bond to an oxygen wherea carbon at posit 2 be not. thi be surpris a one would expect 
two atom with differ neighborhood to be treat differ by the network. 

On investig the problem further we found that sinc the w1n2 network have onli one convo- 
lution layer, the atom and atom-pair featur be not fulli convolved. thi caus all atom that 
have the same atom type, and same number of bond of each type to contribut ident to the 
network. thi be not the case for network that have two or more convolut layers. 

despit the aforement problem, the w1n2 network have good predict accuracy. one hy- 
pothesi for thi be that the atom type and their neighborhood be tightli correlated; for instanc 
an outgo doubl bond from a carbon be alway to anoth carbon or oxygen atom. As a result, 
give the atom type, an explicit encod of the neighborhood be not need by the network. thi 

6thi featur be refer to a “simple” input featur in kearn et al. (2016). 

12 



under review a a confer paper at iclr 2017 

figur 7: attribut for a molecul under the w2n2 network (kearn et al. (2016)). the 
molecul be activ on task pcba-58432. 

also suggest that equival predict accuraci can be achiev use a simpler “bag of atoms” 
type model. 

3.2 languag model 

To appli our techniqu for languag modeling, we studi word-level languag model of the 
penn treebank dataset (marcu et al. (1993)), and appli an lstm-base sequenc model base 
on zaremba et al. (2014). for such a network, give a sequenc of input words, and the softmax 
predict for the next word, we want to identifi the import of the preced word for the 
score. 

As in the case of the incept model, we observ satur in thi lstm network. To describ 
the setup, we choos 20 randomli chosen section of the test data, and for each of them inspect the 
predict score of the next word use the first 10 words. then we give each of the 10 input word 
a weight of α ∈ [0, 1], which be appli to scale their emb vectors. In figur 8, we plot the 
predict score a a function of α. for all except one curves, the curv start near zero at α = 0, 
move around in the middle, stabilizes, and turn flat around α = 1. for the interest special case 
where softmax score be non-zero at α = 0, it turn out that that the word be predict repres 
out of vocabulari words. [!h] 

In tabl 9 and tabl 10 we show two comparison of gradient to integr gradients. due to 
saturation, the magnitud of gradient be so small compar to the predict score that it be 
difficult to make sens of them. In comparison, (approximate) integr gradient have a total 
amount close to the prediction, and seem to make sense. for example, in the first example, the 
integr gradient attribut the predict score of “than“ to the preced word “more”. thi 
make sens a “than” often follow right after “more“ in english. On the other hand, standard 
gradient give a slightli neg attribut that betray our intuition. In the second example, in 
predict the second “ual”, integr gradient be clearli the high for the first occurr of 
“ual”, which be the onli word that be highli predict of the second “ual”. On the other hand, 
standard gradient be not onli tiny, but also similar in magnitud for multipl words. 

4 conclus 

We present interior gradients, a method for quantifi featur importance. the method can be 
appli to a varieti of deep network without instrument the network, in fact, the amount of 
code requir be fairli tiny. We demonstr that it be possibl to have some understand of the 

13 



under review a a confer paper at iclr 2017 

figur 8: softmax score of the next word in the lstm languag model (section 3.2) 

sentenc the sharehold claim more than $ N million in loss 
integr gradient -0.1814 -0.1363 0.1890 0.6609 
gradient 0.0007 -0.0021 0.0054 -0.0009 

figur 9: predict for than: 0.5307, total integr gradient: 0.5322 

sentenc and N minut after the ual trade 
integr gradient (*1e-3) 0.0707 0.1286 0.3619 1.9796 -0.0063 4.1565 0.2213 
gradient (*1e-3) 0.0066 0.0009 0.0075 0.0678 0.0033 0.0474 0.0184 
sentenc (cont.) halt come news that the ual group 
integr gradient (*1e-3) -0.8501 -0.4271 0.4401 -0.0919 0.3042 
gradient (*1e-3) -0.0590 -0.0059 0.0511 0.0041 0.0349 

figur 10: predict for ual: 0.0062, total integr gradient: 0.0063 

perform of the network without a detail understand of it implementation, open up the 
possibl of easi and wide application, and lower the bar on the effort need to debug deep 
networks. 

We also wonder if interior gradient be use within train a a measur against saturation, or 
inde in other place that gradient be used. 

acknowledg 

We would like to thank patrick riley and christian szegedi for their help feedback on the tech- 
niqu and on draft of thi paper. 

refer 

tensorflow. https://www.tensorflow.org/. 

explain artifici intelligence. http://www.darpa.mil/attachments/ 
darpa-baa-16-53.pdf, 2016. 

14 

https://www.tensorflow.org/ 
http://www.darpa.mil/attachments/darpa-baa-16-53.pdf 
http://www.darpa.mil/attachments/darpa-baa-16-53.pdf 


under review a a confer paper at iclr 2017 

R. J. aumann and L. S. shapley. valu of non-atom games. princeton univers press, prince- 
ton, nj, 1974. 

david baehrens, timon schroeter, stefan harmeling, motoaki kawanabe, katja hansen, and klaus- 
robert müller. how to explain individu classif decisions. journal of machin learn 
research, pp. 1803–1831, 2010. 

alexand binder, grégoir montavon, sebastian bach, klaus-robert müller, and wojciech samek. 
layer-wis relev propag for neural network with local renorm layers. corr, 
2016. url http://arxiv.org/abs/1604.00825. 

alexey dosovitskiy and thoma brox. invert visual represent with convolut networks, 
2015. url http://arxiv.org/abs/1506.02753. 

dumitru erhan, yoshua bengio, aaron courville, and pascal vincent. visual higher-lay 
featur of a deep network. technic report 1341, univers of montreal, 2009. 

xavier glorot and yoshua bengio. understand the difficulti of train deep feedforward neural 
networks. In artifici intellig and statist (aistats10), 2010. 

steven kearnes, kevin mccloskey, marc berndl, vijay pande, and patrick riley. molecular graph 
convolutions: move beyond fingerprints. journal of computer-aid molecular design, pp. 
595–608, 2016. 

quoc V. le. build high-level featur use larg scale unsupervis learning. In intern 
confer on acoustics, speech, and signal process (icassp), pp. 8595–8598, 2013. 

aravindh mahendran and andrea vedaldi. understand deep imag represent by invert 
them. In confer on comput vision and pattern recognit (cvpr), pp. 5188–5196, 2015. 

mitchel P. marcus, beatric santorini, and mari ann marcinkiewicz. build a larg annot 
corpu of english: the penn treebank. comput linguistics, pp. 313–330, 1993. 

marco túlio ribeiro, sameer singh 0001, and carlo guestrin. ”whi should I trust you?”: ex- 
plain the predict of ani classifier. In 22nd acm intern confer on knowledg 
discoveri and data mining, pp. 1135–1144. acm, 2016a. 

marco túlio ribeiro, sameer singh 0001, and carlo guestrin. model-agnost interpret of 
machin learning. corr, 2016b. url http://arxiv.org/abs/1606.05386. 

olga russakovsky, jia deng, hao su, jonathan krause, sanjeev satheesh, sean ma, zhiheng 
huang, andrej karpathy, aditya khosla, michael bernstein, alexand C. berg, and Li fei-fei. 
imagenet larg scale visual recognit challenge. intern journal of comput vision 
(ijcv), pp. 211–252, 2015. 

wojciech samek, alexand binder, grégoir montavon, sebastian bach, and klaus-robert müller. 
evalu the visual of what a deep neural network have learned. corr, 2015. url 
http://arxiv.org/abs/1509.06321. 

avanti shrikumar, peyton greenside, anna shcherbina, and anshul kundaje. not just a black 
box: learn import featur through propag activ differences. corr, 2016. url 
http://arxiv.org/abs/1605.01713. 

karen simonyan, andrea vedaldi, and andrew zisserman. deep insid convolut networks: vi- 
sualis imag classif model and salienc maps. corr, 2013. url http://arxiv. 
org/abs/1312.6034. 

jost tobia springenberg, alexey dosovitskiy, thoma brox, and martin A. riedmiller. strive for 
simplicity: the all convolut net. corr, 2014. url http://arxiv.org/abs/1412. 
6806. 

Yi sun and mukund sundararajan. axiomat attribut for multilinear functions. In 12th acm 
confer on electron commerc (ec), pp. 177–178, 2011. 

15 

http://arxiv.org/abs/1604.00825 
http://arxiv.org/abs/1506.02753 
http://arxiv.org/abs/1606.05386 
http://arxiv.org/abs/1509.06321 
http://arxiv.org/abs/1605.01713 
http://arxiv.org/abs/1312.6034 
http://arxiv.org/abs/1312.6034 
http://arxiv.org/abs/1412.6806 
http://arxiv.org/abs/1412.6806 


under review a a confer paper at iclr 2017 

christian szegedy, wei liu, yangq jia, pierr sermanet, scott E. reed, dragomir anguelov, 
dumitru erhan, vincent vanhoucke, and andrew rabinovich. go deeper with convolutions. 
corr, abs/1409.4842, 2014. url http://arxiv.org/abs/1409.4842. 

jason yosinski, jeff clune, anh mai nguyen, thoma fuchs, and hod lipson. understand 
neural network through deep visualization. corr, 2015. url http://arxiv.org/abs/ 
1506.06579. 

lei Yu and huan liu. featur select for high-dimension data: A fast correlation-bas filter 
solution. In 20th intern confer on machin learn (icml), pp. 856–863, 2003. 
url http://www.aaai.org/papers/icml/2003/icml03-111.pdf. 

wojciech zaremba, ilya sutskever, and oriol vinyals. recurr neural network regularization. 
corr, 2014. url http://arxiv.org/abs/1409.2329. 

matthew D. zeiler and rob fergus. visual and understand convolut networks. In 13th 
european confer on comput vision (eccv), pp. 818–833, 2014. 

16 

http://arxiv.org/abs/1409.4842 
http://arxiv.org/abs/1506.06579 
http://arxiv.org/abs/1506.06579 
http://www.aaai.org/papers/icml/2003/icml03-111.pdf 
http://arxiv.org/abs/1409.2329 


under review a a confer paper at iclr 2017 

A appendix 

figur 11: more visual compar integr gradient with gradient at the image. 
left-to-right: origin input image, label and softmax score for the high score class, visualiza- 
tion of integr gradients, visual of gradient at the image. 

17 



under review a a confer paper at iclr 2017 

layer mixed5b layer mixed4d 

layer mixed4b layer mixed3b 

figur 12: satur in intermedi layer of inception. for each layer we plot the L2 and 
cosin distanc between the activ vector for a scale down imag and the actual input image, 
with respect to the scale parameter. each plot show the trend for 30 randomli chosen imag 
from the imagenet dataset. notic that trend in all plot flatten a the scale paramet increases. 
for the deepest incept layer mixed5b, the cosin distanc to the activ vector at the imag 
be less than 0.01 when α > 0.6, which be realli tini give that thi layer have 50176 neurons. 

(a) softmax score for task (b) sum of the featur gradient 

figur 13: satur in the w2n2 network (kearn et al. (2016)). plot for the softmax score 
for task pcba-58834, and the sum of the featur gradient w.r.t. the same task for twenti molecules. 
all molecul be activ against the task 

18 



under review a a confer paper at iclr 2017 

(a) 

integr gradient x1 = 0.25, x2 = 0.25 
deeplift x1 = 0.53 , x2 = 

1 
3 

layer-wis relev propag x1 = 0.53 , x2 = 
1 
3 

deconvnet x1 = 1.25, x2 = 0.75 
guid backpropag x1 = 1.25, x2 = 0.75 

(b) 

integr gradient x1 = 0.25, x2 = 0.25 
deeplift x1 = 0.25, x2 = 0.25 
layer-wis relev propag x1 = 0.25, x2 = 0.25 
deconvnet x1 = 1.0, x2 = 1.0 
guid backpropag x1 = 1.0, x2 = 1.0 

figur 14: attribut for two function equival network use integr gradients, 
deeplift (shrikumar et al. (2016)), layer-wis relev propag (binder et al. (2016)), de- 
convnet (zeiler & fergu (2014)), and guid backpropag (springenberg et al. (2014)). the 
input be x1 = 1.0, x2 = 1.0. the refer input for the deeplift method be x1 = 0, x2 = 0. all 
method except integr gradient provid differ attribut for the two networks. 

19 


1 introduct 
2 our techniqu 
2.1 gradient Do not reflect featur import 
2.2 satur 
2.3 interior gradient 
2.4 cumul interior gradient 
2.5 evalu our approach 
2.6 debug network 
2.7 discuss 
2.8 relat work 

3 applic to other network 
3.1 ligand-bas virtual screen 
3.2 languag model 

4 conclus 
A appendix 

