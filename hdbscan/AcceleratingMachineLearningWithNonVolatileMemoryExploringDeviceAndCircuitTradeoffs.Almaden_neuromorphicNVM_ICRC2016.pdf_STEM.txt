









































acceler machin learn with non-volatil 
memory: explor devic and circuit tradeoff 

alessandro fumarola∗‡, pritish narayanan∗, luca L. sanches∗, severin sidler∗‡, junwoo jang∗†, 
kibong moon†, robert M. shelby∗, hyunsang hwang†, and geoffrey W. burr∗ 

∗ibm research–almaden, 650 harri road, san jose, CA 95120, tel: (408) 927–1512, email: gwburr@us.ibm.com 
†depart of materi scienc and engineering, pohang univers of scienc and technology, pohang 790-784, korea 

‡epfl, lausanne, ch–1015 switzerland 

abstract—larg array of the same nonvolatil memori 
(nvm) be develop for storage-class memori (scm) – 
such a phase chang memori (pcm) and resist ram 
(reram) – can also be use in non-von neumann neuromor- 
phic comput schemes, with devic conduct serv 
a synapt “weight.” thi allow the all-import multiply- 
accumul oper within these algorithm to be perform 
effici at the weight data. 

In contrast to other group work on spike-tim de- 
pendent plastic (stdp), we have be explor the use of 
nvm and other inherently-analog devic for artifici neural 
network (ann) train with the backpropag algorithm. 
We recent show a large-scal (165,000 two-pcm synapses) 
hardware-softwar demo (iedm 2014, [1], [2]) and analyz the 
potenti speed and power advantag over gpu-bas train 
(iedm 2015, [3]). 

In thi paper, we extend thi work in sever use directions. 
We ass the impact of undesired, time-vari conduct 
change, includ drift in pcm and leakag of analog cmo ca- 
pacitors. We investig the use of non-filamentary, bidirect 
reram devic base on prcamno, with an eye to develop- 
ing materi variant that provid suitabl linear conduct 
change. and finally, we explor tradeoff in design peripher 
circuitry, balanc simplic and area-effici against the 
impact on ann performance. 

I. introduct 

By perform comput at the locat of data, non-von 
neumann (non–vn) comput ought to provid signific 
power and speed benefit (fig. 1) on specif and assum 
import tasks. for one such non–vn approach — on-chip 

cpu memori 

bu 

von neumann 
“bottleneck”(a) (b) 

fig. 1. In the von neumann architectur (a), data (both oper and 
operands) must move to and from the dedic central process unit (cpu) 
along a bus. In contrast, in a non–von neumann architecture, distribut 
comput take place at the locat of the data, reduc the time and 
energi spent move data around [1]. 

train of large-scal ann use nvm-base synapsi [1]– 
[4] — viabil will requir sever things. first, despit the 
inher imperfect of nvm devic such a phase chang 
memori (pcm) [1], [2] or resist ram (rram) [4], such 
nvm-base network must achiev competit perform 
level (e.g., classif accuracies) when compar to ann 
train use cpu or gpus. second, the benefit of perform- 
ing comput at the data (fig. 2) must confer a decid 
advantag in either train power or speed (or preferably, 
both). and finally, ani on-chip acceler should be appli- 
cabl toward network of differ type (fully–connect 
“deep” NN or convolut nn) and/or be reconfigur 
for network of differ shape (wide, with mani neurons, or 
deep, with mani layers). 

We briefli review our work [1]–[4] in assess the accu- 
racy, speed and power potenti of on-chip nvm–base ml. 

A. compar analysi of speed and power 

We have previous assess the potenti advantages, in 
term of speed and power, of on-chip machin learn (ml) 
of large-scal artifici neural network (ann) use non- 
volatil memori (nvm)-base synapses, in comparison to 
convent gpu–bas hardwar [3]. 

under moderately-aggress assumpt for parallel–read 
and –write speed, pcm-base on-chip machin learn can 

selector devicesynapt weight 

nvmn1 

N2 

N1 

M1 

M2 
P 

P1 

O1 
pair 

conduct 

1 

N2 

P2 

O2 

G+ G– 

Nn 

Oo 
+ ‐ + ‐ 

Nn Mm 
mmmm Pp 

M1 
+ 

fig. 2. neuro-inspir non-von neumann comput [1]–[4], in which 
neuron activ each other through dens network of programm synapt 
weights, can be implement use dens crossbar array of nonvolatil 
memori (nvm) and selector device-pair [1]. 

978-1-5090-1370-8/16/$31.00 c© 2016 ieee 



26.5x 

1.9x 
5.9x 8.2x 

20.3x 

0 15x 0 49x 0 89x 1.79x 
3.58xtrainingtim 0.15x 0.49x 0.89x 

1msec 
300usec 

time 

pcm (conservative) 
(per example) 

10usec 
30usec 
100usec gpu pcm (aggressive) 

#1 #2 #3 #4 #5network: #1 #2 #3 #4 #5 
7 layer 7 layer 4 layer 7 layer 4 layer 

7.7e6 synapsi 36e6 synapsi 52e6 synapsi 450e6 synapsi 485e6 synapsi 
51 gb/sec (24%) 84 gb/sec (34%) 99 gb/sec (40%) 250 gb/sec (100%) 250 gb/sec (100%) 

network: 

100w 
10w 
1W pcm ( i ) 

gpu 
768 gflop (14%) 1136 gflop (25%) 1447 gflop (32%) 4,591 gflop (100%) 4,591 gflop (100%) 

34, 200x 13, 600x 5, 700x 890x2,850x 1,130x 620x 

1W 
100mw 
10mw 

train 

pcm (aggressive) 
pcm (conservative) 

2 470x34, 200x 13, 600x 5, 700x 890x, 620x 220x 120x 
g 

power 2, 470x 

fig. 3. predict train time (per ann example) and power for 5 
anns, rang from 0.2gb to nearli 6gb [3]. under moderately-aggress 
assumpt for parallel–read and –write speed, pcm-base on-chip machin 
learn can offer low power and faster train for both larg and small 
network [3]. 

250 125 10528 250 
hidden 
neuron 

125 
hidden 
neuron 

10 
output 
neuron 

“0” 

x1 
crop 
(22x24 

528 
input 

neuron 

A 

A 

x1 
B 

wij 

(22x24 
pixel) 
mnist 
imag 

“1” 

xi 
A 

xj 
B 

ij 

x wA 

xj 
“8” 

B 

xi wij 

xj =f(xi wij) 

xj 
“9” 

x528 

B A 
A 

x250 
B 

fig. 4. In forward evalu of a multilay perceptron, each layer’ neuron 
drive the next layer through weight wij and a nonlinear f(). input neuron 
be driven by input (for instance, pixel from success mnist imag 
(crop to 22×24)); the 10 output neuron classifi which digit be present 
[1]. 

potenti offer low power and faster train (per ann 
example) than gpu-bas train for both larg and small 
network (fig. 3), even with the time and energi requir for 
occasion reset (forc by the larg asymmetri between 
gentl partial-set and abrupt reset in pcm). critic here 
be the design of area-effici read/writ circuitry, so that mani 
copi of thi circuitri oper in parallel (each handl a 
small number of column (rows), cs). 

B. potenti for competit classif accuraci 

use 2 phase-chang memori (pcm) devic per synapse, 
we demonstr a 3–layer perceptron (fully-connect ann) 
with 164,885 synapsi [1], train with backpropag [5] 
on a subset (5000 examples) of the mnist databas of 
handwritten digit [6] (fig. 4), use a modifi weight-upd 
rule compat with nvm+selector crossbar array [1]. We 
prove that thi weight-upd modif do not degrad 
the high “test” (generalization) accuraci such a 3–layer 
network inher deliv on thi problem when train in 
softwar [1]. however, nonlinear and asymmetri in pcm 

conduct respons limit both “training” and “test” accu- 
raci in our original, mix hardware-softwar experi to 
82–83% [1] (fig. 5). 

asymmetri (between the gentl conduct increas of 
pcm partial–set and the abrupt of pcm reset) be 
mitig by an occasion reset strategy, which could be 
both infrequ and inaccur [1]. while in these initi 
experiments, network paramet such a learn rate η have 
to be tune veri carefully, a modifi ‘lg’ algorithm offer 
wider toler to η, high classif accuracies, and 
low train energi [3] (fig. 6). 

toleranc result show that all nvm-base ann can 
be expect to be highli resili to random effect (nvm 
variability, yield, and stochasticity), but highli sensit to 
“gradient” effect that act to steer all synapt weight 
[1]. We show that a bidirect nvm with a symmetric, 
linear conduct respons of finit but larg dynam rang 
(e.g., each conduct step be rel small) can deliv the 
same high classif accuraci on the mnist digit a 

90 

100 

% 
] 500 x 661 pcm = (2 pcm/synaps * 164,885 synapses) + 730 unus pcm 

80 

90 

ac 
y 

[% 

experi 

60 

70 
ac 

cu 
r 

80 
90 

100 match simul 

40 

50 

nt 
al 

a 

50 
60 
70 

20 

30 

ri 
m 

en 

map of 
final 10 

20 
30 
40 

10 

20 

train epochex 
pe 

final 
pcm 0 5 10 15 200 

10 

conduct (5000 imageseach) 
0 1 2 

0 
g p each) 

fig. 5. train accuraci for a 3–layer perceptron of 164,885 hardware- 
synapsi [1], with all weight oper take place on a 500 × 661 array of 
mushroom-cel pcm devices. also show be a match comput simul 
of thi nn, use paramet extract from the experi [1]. 

100 

% 
] 

90 

new 
techniqu 

80 

ac 
y 

[ 

70 
on test set 

on train set 

60 

cc 
ur 

a 70 

iedm 2014 

t t t 
40ed 

a 
c 

50 

on train set 

iedm 2014 
condit 

on test set 

20 

m 
ul 

at 
e 

30 

0 

Si 
m 10 iedm 

2014 experi 
0.1 1 

0 

learn rate 
10 

p 

fig. 6. A larg number of synapsi tend to “dither,” with frequent updat 
whose aggreg effect ought to be zero (but which be non-zero due to the 
nonlinear and asymmetri of nvm–base synapses). By suppress updat 
of such synapses, NN perform can be improv and train energi 
reduced, while reduc the need to tune the learn rate precisely. 



a conventional, software-bas implement (fig. 7). one 
key observ be the import of avoid constraint on 
weight magnitud that aris when the two conduct be 
either both small or both larg — e.g., synapsi should remain 
in the center stripe of the “g-diamond” [2]. 

In thi paper, we extend upon these observ and address 
sever differ yet use topics. We ass the impact of 
undesired, time-vari conduct change, includ drift in 
phase chang memori (pcm) and leakag of analog cmo 
capacitors. We investig the use of non-filamentary, bidi- 
rection reram devic base on prcamno (pcmo), with 
an eye to develop materi variant that provid suitabl 
linear conduct change. and finally, we explor tradeoff in 
design peripher circuitry, balanc simplic and area- 
effici against the impact on ann performance. 

C. jump-tabl concept 

A highli use concept in model the behavior of real 
nvm devic for neuromorph applic be the concept 
of a “jump-table.” for backpropag training, where one 
or more copi of the same program puls be appli 
to the nvm for adjust the weight [1], we simpli need 
one jump-tabl for potenti (set) and one for depress 
(reset). 

with a pair of such jump-tables, we can captur the nonlin- 
eariti of conduct respons a a function of conduct 

100 
train set targets: 

y 
[% 

] train set 97% 
94% 

(60,000)train 
with 

5 000 

ur 
ac 

y 

90 linear, unbounded, 
symmetrictest set 

train with 
60,000 imag 

(5,000)5,000 
imag 

d 
ac 

cu symmetr 
fulli bidirect 

linear, bounded, symmetr 

test set 

80 

at 
ed 

linear, bounded, symmetr 

im 
ul 

dynam rang 
(# f l d d t f G t G )S 70 

5 10 100 200 50020 50 

(# of puls need to move from gmin to gmax) 

fig. 7. when the dynam rang of the linear respons be large, the 
classif accuraci can now reach the peak accuraci support by the 
origin neural network (a test accuraci of 94% when train with 5,000 
images; of 97% when train with all 60,000 images) [2]. 

C 
o 

n 
d 

u 
ct 

an 
ce 

G 
[a 

.u 
.] 

# of puls 

0 

100 

200 

300 

400 

C 
h 

an 
ge 

in 
co 

n 
d 

u 
ct 

an 
ce 

D 
G 

[a 
.u 

.] 

conduct G [a.u.] 

a) b) 

300 

400 

500 

600 

700 

800 

900 

1000 

1100 

1200 

0 2 8 12 16 204 6 10 14 18 

400 600 800 1000 1200 

fig. 8. (a) exampl median (blue) and ±1σ (red) conduct respons 
for potentiation. (b) associ jump-tabl that fulli captur thi (artifici 
construct in thi case) conduct response, with cumul probabl 
plot in color (from 0 to 100%) of ani conduct chang ∆G at ani 
give initi conduct G. 

(e.g., the same puls might creat a larg “jump” at low 
conductance, but a much small jump at high conductance), 
the asymmetri between posit (set) and neg (reset) 
conduct changes, and the inher stochast natur of 
each jump. fig. 8(a) plot median conduct chang for 
potenti (blue) togeth with the ±1σ stochast variat 
about thi median chang (red). fig. 8(b) show the jump- 
tabl that fulli captur thi conduct response, plot 
the cumul probabl (in color, from 0 to 100%) of ani 
conduct chang ∆G at ani give initi conduct G. 
thi tabl be ideal for comput simul becaus a random 
number r (uniform deviate, between 0.0 and 1.0) can be 
convert to a result ∆G produc by a singl puls by 
scan along the row associ with the conduct G 
(of the devic befor the puls be applied) to find the point at 
which the tabl entri just exce r. 

We have previous use a measur jump-tabl to simul 
the set respons of pcm devic [1]. We have recent 
publish a studi of variou artificially-construct jump- 
tables, in order to help develop an intuit understand of 
the impact that variou featur of such jump-tabl have on 
the classif perform in the ann applic [7]. 

ii. time-depend conduct respons 

one aspect of phase chang memori that we do not 
address in our origin toleranc paper [1] be the role 
of resist drift [8], also know a amorph relaxation. 
As show in fig. 9, after a reset operation, amorph 
relax caus conduct to decrease, rapidli at first 
but then more and more slowly. here we model thi in our 
neural network simul for the network of fig. 4, for 
an otherwis near-perfect pcm device, in which partial-set 
conduct increas be gentl and linear (each ∼0.5% of 
the conduct extent) and occasional-reset be perform 
fairli frequent (everi 100 examples) with high precision. 
the time respons for drift start upon reset operations, with 
partial-set oper assum onli to shift the conduct 
state without affect the underli time-respons of the 
amorph relaxation. 

As expected, a drift coeffici increas dramat (to 
the valu of ν ∼ 0.1 observ for fulli amorph (strong 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 20 40 60 80 1001 
10 

100 
103 

104 
105 

106 
107 

108 

time timer 
e 

la 
ti 

ve 
co 

n 
d 

u 
ct 

an 
ce 

G / t-º 

º ~ 0.1 
º ~ 0.1 

º ~ 0.01 
º ~ 0.01 

fig. 9. after a reset operation, amorph relax caus conduct 
to decrease, rapidli at first but then more and more slowly. plot show the 
same evolut of linear conduct a a function of time on log- (left) and 
linear- (right) scales, for two differ valu of drift coeffcient ν. 



reset) states), then accuraci be eventu affect (fig. 10). 
however, for the much low ν valu (ν ∼ 0.005 – 0.01) 
associ with set and the near-set state relev to pcm- 
base implement of neural networks, accuraci be onli 
minim affected. 

We perform a similar studi for the case of fulli volatil 
analog memori elements, such a cmo capacitors, in which 
ani conduct state immedi begin to decay toward 
zero after a program operation. thi studi be perform 
with perfectli linear bidirect conduct with ∼0.5% 
conduct chang per pulse, and thu without drift, be 
ident to the right-hand side of fig 7, where accuraci 
becom extrem high for high synapt dynam range. 

In thi study, we quantifi the effect decay constant (the 
“rc time constant”) not in absolut units, but rel to 
the time requir for train of a singl data exampl (e.g., 
forward evaluation, revers propagation, and weight update). 
As show in fig. 11, accuraci be strongli affect a soon 
a the ratio between the RC time-const and the time-per- 
exampl fall below 10,000. however, these initi result 
reveal an extrem interest depend on the choic 

0.0001 0.001 0.01 0.1 

drift coeffici º 
60 

65 

70 

75 

80 

85 

90 

95 

100 

NO 
drift 

test 

train 

C 
la 

s 
s 
if 
ic 

a 
ti 
o 

n 
a 

c 
c 
u 

ra 
c 
y 

fig. 10. At the larg drift coeffici associ with fully-amorph 
reset phase chang memori devic (ν ∼ 0.1), neural network accuraci 
be significantli degraded. At the valu of ν ∼ 0.005–0.01 relev to the 
set and the near-set state that domin the pcm-base implement 
of neural networks, accuraci be onli slightli degraded. (result show for 10 
epoch of simul train on 5000 exampl from the mnist dataset.) 

100 1000 10k 100k 1M 
10 

20 

30 

40 

50 

60 

70 

80 

90 

100 

RC time-const / time-per-training-exampl 

/ 

C 
la 

s 
s 
if 
ic 

a 
tio 

n 
a 

c 
c 
u 

ra 
c 
y 

fig. 11. neural network accuraci be strongli affect a soon a the RC 
time-const becom less than 10,000× larg than the time need for 
each train example. (result show for 10 epoch of simul train on 
5000 exampl from the mnist dataset, all at the same global learn rate, 
η ∼ 1). 

of learn rate, impli that some further optim may 
be possible. fig. 12 show that the same global learn rate 
which be optim for a truli non-volatil conduct (infinit 
RC time-constant) be decidedli sub-optim when the RC time- 
constant becom lower. thi impli that it be good to either 
updat so mani weight that one can counteract the loss of 
conduct by retrain those weights, or so few that the 
number of weight be touch (and thu place into a mode 
where they will decay rapidly) be much lower. 

iii. impact OF measur pcmo conduct 
respons 

We have previous studi the impact of the conduct 
respons of pcmo materi by fit a set of function to 
the averag conduct respons [4]. however, thi approach 
be limit by the discrep between the real conduct 
respons and the function chosen, and it do not includ ani 
stochast aspect of the conduct response, for scenario 
where the conduct respons can vari significantli from 
the averag conduct response. 

here, we studi the use of measur jump-tabl for the non- 
filamentari rram materi prxca1−xmno3, also know a 
pcmo. 

A. analog bidirect switch 

resist switch in pcmo-bas devic be caus by 
slow and gradual drift of oxygen ion and vacanc in the 
polycrystallin pcmo layer. inject (removal) of oxygen 
ion take place at the pcmo-oxid (-metal) interfac through 
oxid (reduction) reactions. asymmetri in the devic 
structur and the oxidation-reduct reaction contribut to 
the asymmetri in the switch characteristics, but pcmo- 
base nvm show gradual set and reset characteristics. 
thus, unlik phase chang memori (pcm) materials, there be 
no need to stop train and perform an occasional–reset 
oper [1]. both the averag conduct respons and it 
statist behavior can be describ by a measur jump tabl 
(fig. 13). (note that unlik non-filamentari rram such a 

0.1 1 10 100 

learn rate 
0 

10 

20 

30 

40 

50 

60 

70 

80 

90 

100 

C 
la 

s 
s 
if 
ic 

a 
tio 

n 
a 

c 
c 
u 

ra 
c 
y 

RC time-const / time-per-training-exampl 

=/ 

= 104 

= 3000 

= 1000 

solid: train 
open: test 

fig. 12. for truli non-volatil weight (infinit RC time constant), neural 
network accuraci be optim by use a global learn rate that be larg 
enough to affect a moder number of weights, but not so mani that chao 
ensues. however, a the RC time constant decreases, the volatil of the 
conduct state favor either a larg learn rate (e.g., we adjust for 
the decay weight by retrain mani more of them) or, curiously, low 
learn rate (assum reduc the number of recently-touch weight 
that cannot be trust not to move without be activ programmed). 



pcmo, a filament-bas rram such a hfox, taox, or 
tiox exhibit onli gradual reset characteristics, mean 
that such filamentari rram devic will like still requir 
an “occasional–set” step just like pcm.) 

B. fabric process 

A 10nm pcmo polycrystallin layer be deposit on 
a 50-nm-thick Pt layer, which serv a bottom electrode. 
next, an 100-nm-thick sinx layer be deposit by plasma- 
enhanc chemic vapor deposition, and via-hol (from 0.15 
to 1.0 µm) be form by convent lithographi and 
reactiv ion etching. the Al and Mo layer (20nm and 3nm, 
respectively) and an 50-nm-thick Pt layer (top electrode) be 

G (% of maxg) 

G (% of maxg) 

measur conduct 
changedg (a.u.) 

measur conduct 
changedg (a.u.) 

set 

reset 

100% 

50% 

0% 

fig. 13. jump-tabl of al/mo/pcmo-bas rram devic for posit (set) 
and neg (reset) conduct changes. unlik phase chang memori 
(pcm) devices, these materi provid both gradual reset and gradual 
set, enabl truli bidirect programming. 50000 total set puls (- 
4.0v, 10ms) and reset puls (3.5v, 10ms) follow by -1v read puls 
be use on three identically-s (200nm) devices. 

dwij+=hset *dg 
+ 

set,ij 

xi 

δj 

gij+ gij– 





wij = gij+ - gij- 

dwij+=hreset*dg 
- 
reset,ij 


(or) 

fig. 14. schemat show crossbar-compat [1] weight-upd rule for 
analog bidirect nvms. weight increas (decreases) can be implement 
either a a set oper on G+ (g−) or a reset oper on G− (g+) 
devices. asymmetri in the partial set and reset oper be compens 
by appli a differ learn rate paramet (ηset , ηreset ) that 
modul the number of puls fire from the neuron into the array. 

deposit and pattern by convent lithography. electr 
characterist of the al/mo/pcmo-bas resist memori 
devic be measur use an agil b1500a. 

C. simul perform 

A three-lay perceptron with two pcmo-bas devic 
per synaps be simul perform a classif task 
on the mnist databas (same network show in fig. 4). 
fig. 13 plot the model conduct respons of the resist 
switch elements. for averag valu of conduct G (e.g., 
the central region of the plot), the respons be mostli linear, 
although somewhat asymmetric, with differ averag jump 
valu for set and reset. In constrast, for extrem valu 
of the conduct (left and right edg of each jump-table), 
a high degre of non-linear be observed. however, we have 
previous observ that when the extent of the non-linear 
region be suffici small, high classif accuraci can 
still be achiev [7]. 

the network paramet be tune to achiev a good 
performance, with particular focu give to the ratio of 
ηset/ηreset, use to compens the asymmetri of the jump- 
table. fig. 14 show a schemat version of the crossbar- 
compaibil weight updat rule for backpropagation, in which 
upstream neuron fire a set of puls (shown in red) along the 
horizont word-lines, base sole on their knowledg of xi 
and the global learn rate (η = ηset) [1]. simultaneously, the 
downstream neuron first puls (shown in magenta) along the 
vertic bit-lin connect to a larg number of G+ and G− 

conductances. these puls be base onli on the downstream 
neuron’ knowledg of δj and the global learn rate. 

becaus these puls affect all the devic along the share 
word-lin and bit-lines, their amplitud and durat cannot 
be tune to optim the program of ani one particular 
conduct value. thi lead to signific problem when 
conduct respons be nonlinear, sinc the same puls can 
caus small conduct to increas much more significantli 
than conduct that be alreadi large. 

fig. 15. simul train and test accuraci for a three-lay perceptron 
use pcmo-bas devic a synapt weights. the asymmetri between 
posit and neg jump can be compens by tune individu 
the learn rate for set and reset (see fig. 14). the classif 
accuraci of the network improv a the ratio of set to reset learn 
rate (ηset/ηreset) increases. 



however, the downstream neuron can easili fire differ 
pulse-train on the separ G+ and G− bit-lines, and knowl- 
edg of δj can be suffici to identifi whether set or reset 
will occur (xi need onli be constrain to be non-negative). 
thu it be straightforward to appli a differ global learn 
rate for reset and for set, thu lead to more or few 
pulses, and provid a way to compens for jump-tabl 
asymmetry. fig. 15 show that classif accuraci can be 
improv for the al/mo/pcmo jump-tabl show in fig. 13, 
with an optim ratio of ηset / ηreset of approxim 3–4. 

D. switch energi 

the switch energi of the devic be measur by 
integr the product between the voltag and the current for 
the durat of a program puls (10ms). the conduct 
be measur with read puls of −1v . pcmo-bas memori 
devic (like other non-filamentari switch elements) show 
a depend of the program energi on the activ area. 
switch energi rang from sub-nj to ten of µJ be 
measur on devic with hole size from 0.15nmto 1µm 
(fig. 16(a). the switch energi be then normal with 
respect to the activ devic area (fig. 16(b)) to show a good 
linear depend between switch current and devic hole- 
size. follow the trend from 150nm down to 25nm, one 
can anticip an improv in switch energi by roughli 

fig. 16. switch (a) energi a a function of conduct and (b) energi 
densiti a a function of conduct density, measur for al/mo/pcmo- 
base devic with -1v read voltage. 

35×. If the switch time could potenti be reduc from 
10m down to 10ns, then one would be abl to achiev femto- 
joul switch energy. such aggress scale of both devic 
area and switch time would be necessari in order to enabl 
highly-parallel weight updat operations. 

iv. circuit need 

A crossbar-array-bas neural network implement the 
multiply-accumul oper at the heart of most neural 
network algorithm extrem efficiently, through ohm’ law 
follow by current summat (kirchoff’ current law). 
however, an import consider be the design of highli 
area-effici neuron circuit that resid at the edg of these 
array enabl read and write of mani synapt row or 
column in parallel. such high parallel be essenti if we 
wish to achiev order of magnitud perform and power 
benefit over convent cpu/gpu approach [3]. given 
thi need for a larg number of distinct copi of neural 
circuit that can be execut in parallel, it be critic to embrac 
approxim function (for e.g. non-linear squash func- 
tions, calcul and multipli deriv etc.) rather than 
rigorously-precis yet highli area-ineffici functionality. 

In thi section, we present exampl of design choic that 
simplifi the underli hardwar by leverag the inher 
toler of ann algorithm to error. We discu circuit 
need for the forward- and reverse-evalu operations, in- 
clude precision/rang of the comput neuron activ 
and backpropag errors, use piecewis linear (pwl) 
approxim of non-linear squash functions, and sim- 
plifi the deriv includ dure revers propag to 
avoid complex floating-point arithmet operations. We then 
demonstr that these approxim do not significantli 
degrad classif accuraci a compar to neuron im- 
plement with rigorously-precis functionality. 

A. circuit-ne for forward and revers propag 

forward propag (fig. 17) in a fulli connect neural 
network involv the calcul of the neuron activ of 
a hidden/output layer, base on the neuron activ of the 
previou layer and the interven synapt weights. thi be 
a two-stag process, with the multiply- accumul oper 
occur in the crossbar array, and the non-linear squash 
function appli at the periphery. one commonli use function 
in softwar implement be tanh() (the hyperbolic-tang 
function), which be difficult to implement exactli unless a 
larg number of transistor be included. however, a piece- 
wise linear implement of thi squash function would 
be fairli straightforward to implement (fig. 17). 

A second design choic be the rang of distinct neuron 
activ valu that need to be support by the hardware. In 
a digit implement thi translat into the number of bits, 
which would have area implic depend on the amount 
of local storag required, a well a the resolut of ani ana- 
log to digit convers circuit use to convert signal from 
the crossbar array into those bits. In an analog implementation, 



forward propag 

xa1 

xa2 

xan 

xb1 

xb2 

xbn 

xbj = f(swij xai) 

f( . ): non-linear 
activ 

input/ hidden 
neuron 

hidden/output 
neuron 

tanh 
piece- 
linear 

w11 

accumul sum 

R 
a 

n 
g 

e 
o 

f 
N 

eu 
ro 

n 
S 

ta 
te 

s 

) 

linear 

-wise 
linear 

fig. 17. forward propag oper in a deep neural network. the 
multiply-accumul oper occur on the crossbar array. neuron circuitri 
must handl the non-linear squash function. 

thi would directli translat into the resolut between analog 
voltag level and/or time-steps. 

revers propag (fig. 18) be similar to forward propaga- 
tion, but from output/hidden neuron to preced hidden neu- 
rons. the quantiti δ, know a the correct or error, togeth 
with the forward-propag neuron activations, control the 
weight updat for neural network train (see fig. 14). An 
import distinct from forward propag be that the non- 
linear squash function be not applied. instead, the multiply- 
accumul sum (integr on the crossbar array, but in a 
direct orthogon to the integr perform dure the 
forward-propag step) need to be scale by the deriv 
of the activ function, a evalu at the neuron activ 
value. again, an exact tanh() deriv be not effici to 
comput and multiply. 

instead, a step-funct deriv with two distinct state 
can be used. multipl by deriv valu of zero and 
one be fairli straightforward to implement in hardware. thi 
correspond to simpli enabl or disabl the transmiss 
of an accumul sum-of-delta from ani neuron stage to 
the preced stage. however, multipl by arbitrari 
scale factor may be difficult to achiev sinc floating-point 
multipli be not readili available. the impact of such 
approxim on neural network train be studi in the 
next subsection. 

B. results: circuit approxim 

We explor the impact of the aforement circuit 
approxim on the train and test perform of the 
mnist dataset of handwritten digit through simulations. A 
subset of onli 5000 train imag from the origin dataset 
of 60000 imag be used. imag be crop to 24× 22 
pixels. the same 3-layer neural network (528-250-125-10) be 
use (fig. 4. A crossbar-compat weight updat rule [1] 
be use to emul how weight updat would be do on a 
real crossbar array. the baselin train and test accuraci 
assum 20 epoch of training, 256 neuron activ states, 
a tanh() activ function and exact deriv be found 
to be 99.7% and 93.6% respect (blue curv and star, 
fig. 19). note that, a per fig. 7, both train and test 

3 

revers propag 

xa1 

xa2 

xan 

xb1 

xb2 

xbn 

δai = f’(x 
a)●(swij δbj) 

f’( ): deriv of 
activ function 

input/ hidden 
neuron 

hidden/output 
neuron 

tanh 

piec 
-wise 
linear 

w11 

fig. 18. revers propag oper in a deep neural network. multiply- 
accumul oper on δ occur on the crossbar array. neuron circuitri 
must handl gener and multipl of the deriv of the squash 
function. 

train epoch 
2 106 14 184 128 16 20 

% 
C 
or 
re 
ct 

tanh 
piece‐wis linear 

fig. 19. train and test accuraci obtain on mnist with tanh() and 
piece-wis linear activ functions. pwl achiev test accuraci compar 
to tanh(). 

accuraci increas (to ∼100% and ∼97-98%) when all 60,000 
exampl be use for training. 

fig. 19 also show the train and test accuraci use 
a piece-wis linear (pwl) activ function. On mnist, 
one observ that the test accuraci obtain (92.7%) be al- 
readi compar to the full tanh() implementation. further 
improv in test accuraci can be obtain by optim 
the low valu of the derivative. thi be akin to the intent 
implement of ‘leaky’ deriv in some convent 
machin learn techniques, especi in the case of rectifi 
linear unit (relu). A leaki deriv ensur that some 
contribut from the downstream neuron get pass on to 
earli stages, therebi particip in the program of 
those weights. 

fig. 20 show that the test accuraci can be further improv 
to 93.2% when the deriv of the piecewise-linear squash 
function at extrem valu be make non-zero. however, the 
multipl oper be non-trivial. In a digit implemen- 
tation, one might be abl to do bit-shift oper (restrict 
deriv valu to power of 2). An analog implement 
can offer more freedom, sinc we need onli enabl one of two 
non-zero scale factor when transmit accumul analog 
voltag to preced stages. 

In addit to the squash function and it derivative, 
the impact of the number of distinct neuron activ and 
error state on the test accuraci be analyzed. valu from 8 



% 
T 
e 
t A 
cc 
ur 
ac 
y 

91.5 

92 

92.5 

93 

deriv low valu 
0 0.10.05 0.15 

non‐zero 
low deriv 

fig. 20. optim the low deriv valu enabl further improv 
in test accuracy, yet requir some circuit complex to implement an 
approxim multipl function. 

% 
T 
e 
t A 
cc 
ur 
ac 
y 

85 

89 

91 

93 

87 

number of neuron state 
8 3216 64 128 256 

tanh 

piece‐wis linear 
with non‐zero deriv. 

fig. 21. If the number of distinct neuron activ and error state be low 
than 32, then test accuraci degrades. however, reduc the total number 
of neuron state can help enabl significantli more area-effici peripher 
circuitry. 

to 256 be consid (fig. 21). high test accuraci be 
maintain down to 32 distinct neuron state for both the 
tanh() and piece-wis linear implementations. reduc the 
total number of neuron state can be extrem benefici in 
area-effici circuit design. In a digit implementation, thi 
allow a reduct in the total number of latch or flip-flops. 
In an analog implementation, it permit a wider separ of 
analog voltag levels, relax nois constraint and enabl 
simpler circuits. 

V. conclus 

We have studi sever aspect of system design when non- 
volatil memori (nvm) devic be employ a the synapt 
weight element for on-chip acceler of the backpropaga- 
tion train of large-scal artifici neural network (ann). 

We have assess the impact of undesired, time-vari 
conduct change, includ drift in phase chang memori 
(pcm) devic and leakag of analog cmo capacitors. We 
have investig the use of non-filamentary, bidirect 
reram devic base on prcamno, which can be consid 
a promis materi variant that could potenti provid both 
gradual conduct increas and conduct decrease. and 
finally, we have explor some of the tradeoff in design 
peripher circuitry, balanc simplic and area-effici 
against the impact on ann perform for the nonlinear 
squash function, the evalu of it derivation, and the 

number of resolv level when integr both x (forward- 
propagate) and δ (reverse-propagate) values. 

We briefli review our previou work toward achiev 
competit perform (classif accuracies) for such 
ann with both phase-chang memori [1], [2] and non- 
filamentari reram base on prcamno (pcmo) [4], and 
toward assess the potenti advantag for ML train 
over gpu–bas hardwar in term of speed (up to 25× faster) 
and power (from 120–2850× low power) [3]. We discuss 
the “jump-table” concept, previous introduc to model 
real-world nvm such a pcm [1] or pcmo, to describ 
the full cumul distribut function (cdf) of result 
conductance-chang at each possibl conduct value, for 
both potenti (set) and depress (reset). 

while the ‘lg’ algorithm, togeth with other approaches, 
should help a nonlinear, asymmetr nvm (such a pcm) act 
more like an ideal linear, bidirect nvm, the identif 
of nvm devic and/or pulse-schem that can offer a con- 
ductanc respons that be at least partli linear, use circuitri 
that can be highli area-effici (and thu massively-parallel), 
will help significantli in achiev equally-high classif 
accuraci while offer faster and lower-pow train than 
convent gpu and cpus. 

refer 
[1] G. W. burr, R. M. shelby, C. di nolfo, J. W. jang, R. S. shenoy, 

P. narayanan, K. virwani, E. U. giacometti, B. kurdi, and H. hwang, 
“experiment demonstr and toleranc of a large-scal neural 
network (165,000 synapses), use phase-chang memori a the synapt 
weight element,” in iedm, 2014, p. 29.5. 

[2] G. W. burr, R. M. shelby, S. sidler, C. di nolfo, J. jang, I. boybat, 
R. S. shenoy, P. narayanan, K. virwani, E. U. giacometti, B. kurdi, and 
H. hwang, “experiment demonstr and toleranc of a large–scal 
neural network (165,000 synapses), use phase–chang memori a the 
synapt weight element,” ieee trans. electr. dev., vol. 62, no. 11, pp. 
3498–3507, 2015. 

[3] G. W. burr, p.narayanan, R. M. shelby, S. sidler, I. boybat, C. di 
nolfo, and Y. leblebici, “large–scal neural network implement 
with nonvolatil memori a the synapt weight element: compar 
perform analysi (accuracy, speed, and power),” in iedm technic 
digest, 2015, p. 4.4. 

[4] j.-w. jang, S. park, G. W. burr, H. hwang, and y.-h. jeong, “optimiza- 
tion of conduct chang in pr1−xcaxmno3–bas synapt devic 
for neuromorph systems,” ieee electron devic letters, vol. 36, no. 5, 
pp. 457–459, 2015. 

[5] D. rumelhart, G. E. hinton, and J. L. mcclelland, “A gener framework 
for parallel distribut processing,” in parallel distribut processing. 
mit press, 1986. 

[6] Y. lecun, L. bottou, Y. bengio, and P. haffner, “gradient-bas learn 
appli to document recognition,” proceed of the ieee, vol. 86, 
no. 11, p. 2278, 1998. 

[7] S. sidler, I. boybat, R. M. shelby, P. narayanan, J. jang, A. fumarola, 
K. moon, Y. leblebici, H. hwang, and G. W. burr, “large-scal neural 
network implement with non-volatil memori a the synapt weight 
element: impact of conduct response,” in essderc 2016, 2016, p. 
to appear. 

[8] A. pirovano, A. L. lacaita, F. pellizzer, S. A. kostylev, A. benvenuti, 
and R. bez, “low–field amorph state resist and threshold voltag 
drift in chalcogenid materials,” ieee trans. electr. dev., vol. 51, no. 5, 
pp. 714–719, 2004. 


