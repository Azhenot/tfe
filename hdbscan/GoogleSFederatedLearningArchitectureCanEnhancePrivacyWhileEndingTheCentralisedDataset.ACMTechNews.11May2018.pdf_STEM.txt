






































googleâ•ž feder learn architectur can enhanc privaci while end the centralis dataset 


google’ feder learn architectur can enhanc privaci 
while end the centralis dataset 

traditionally, dataset be a huge part of the machin learn pipeline. with the advent of deep learn techniques, the amount 
of high quality, well-label data be paramount to the success of the machin learn project. the standard way of execut 
machin learn also need the data to be on the datacentr or the machin where the model be be trained. but now, 
engin at googl have come up with a new secur and robust cloud infrastructur architectur for process data call 
feder learning. It have be creat for model which be train from user interact on mobil devices. 

A signific amount of research have be do on enabl the effici distribut train of neural networks. We can take 
sever approach to distribut the train of deep learn networks. 

model parallelism: here, differ machin in the distribut system be respons for the comput in differ 
part of a singl network 
data parallelism: here differ machin have a complet copi of the model; each machin simpli get a differ portion 
of the data, and result from each be somehow combined. 

feder learn : the architectur 

feder learn take advantag of mobil phone to collabor and learn a share predict model. thi be do while all 
the train data stay on the devic and be not sent over to the cloud. thi decoupl help u to execut machin learn on 
the devic without the the need to store the data in the cloud. thi be differ from the use case such a the mobil vision api 
and on-devic smart repli where predict be do on device; in feder learn the model train happen on the devic 
a well. 

advertis 

google’ feder learn architectur can enhanc privaci while e... https://analyticsindiamag.com/googles-federated-learning-architecture-c... 

1 sur 2 18-05-18 à 19:26 



the phone personalis the model locally, base on your usag (a). mani users’ updat be aggreg (b) to form a 
consensu chang (c) to the share model, after which the procedur be repeated. 

here how it works: the mobil devic download the current model, improv it by learn from data on your phone, and then 
make a summari of the knowledg it have learn from the data a a small focu update. onli thi updat to the model be sent 
to the cloud, use encrypt communication, where it be immedi averag with other user updat to improv the share 
model. all the train data remain on your device, and no individu updat be store in the cloud. 

thi kind of architectur result in smarter models, low latency, and less power consumption; all thi while ensur complet 
privaci of user data. the architectur also send an updat to the share model, but the improv model on the phone can now 
be use immediately, and thi result in a veri power user experience. 

make feder learn possibl 

the system of feder learn be alreadi be test in gboard on android, the veri popular googl keyboard. whenev 
gboard show a suggest query, the mobil devic local store inform about the current context and whether you use 
the suggestion. feder learn process that histori on-devic to suggest improv to the next iter of gboard’ 
queri suggest model. 

the research team at googl have to overcom mani algorithm and research challeng to make feder learn possible. 
optimis algorithm like stochast gradient descent (sgd) which be typic use in mani machin learn system run 
on a larg dataset. most of the time these dataset be partit homogen across server in the cloud. mani highli 
iter algorithm requir low-latency, high-throughput connect to the train data. but in thi particular setting, the data 
be distribut across million of mobil and cellular devic in a highli heterogen fashion. In addition, these devic have 
significantli higher-latency, lower-throughput connect and be onli intermitt avail for training. 

To put such a system in deploy to million heterogen phone run gboard requir a fairli advanc technolog 
stack. on-devic train us a minimis version of tensorflow. upload speed be typic much slow than download 
speeds, the research also develop a novel way to reduc upload commun cost up to anoth 100x by compress 
updat use random rotat and quantisation. 

the futur Of feder learn 

the research still feel that the work be onli the beginning. they say in their offici blog, “our work have onli scratch the 
surfac of what be possible. feder learn can’t solv all machin learn problem (for example, learn to recognis 
differ dog breed by train on care label examples), and for mani other model the necessari train data be alreadi 
store in the cloud (like train spam filter for gmail).“ 

the applic of feder learn requir that machin learn practition to use new tool and a new way of look at 
the problem. model development, training, and evalu with no direct access to or label of raw data, with commun 
cost a a limit factor. the research believ that the user benefit of feder learn make tackl the technic 
challeng worthwhile. 

provid your comment below 

comment 

google’ feder learn architectur can enhanc privaci while e... https://analyticsindiamag.com/googles-federated-learning-architecture-c... 

2 sur 2 18-05-18 à 19:26 


