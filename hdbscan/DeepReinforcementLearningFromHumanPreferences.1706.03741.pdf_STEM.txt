


















































deep reinforc learn 
from human prefer 

paul F christiano 
openai 

paul@openai.com 

jan leik 
deepmind 

leike@google.com 

tom B brown 
nottombrown@gmail.com 

miljan martic 
deepmind 

miljanm@google.com 

shane legg 
deepmind 

legg@google.com 

dario amodei 
openai 

damodei@openai.com 

abstract 

for sophist reinforc learn (rl) system to interact use with 
real-world environments, we need to commun complex goal to these systems. 
In thi work, we explor goal defin in term of (non-expert) human prefer 
between pair of trajectori segments. We show that thi approach can effect 
solv complex RL task without access to the reward function, includ atari 
game and simul robot locomotion, while provid feedback on less than 
1% of our agent’ interact with the environment. thi reduc the cost of 
human oversight far enough that it can be practic appli to state-of-the-art 
RL systems. To demonstr the flexibl of our approach, we show that we can 
success train complex novel behavior with about an hour of human time. 
these behavior and environ be consider more complex than ani which 
have be previous learn from human feedback. 

1 introduct 

recent success in scale reinforc learn (rl) to larg problem have be driven in domain 
that have a well-specifi reward function (mnih et al., 2015, 2016; silver et al., 2016). unfortunately, 
mani task involv goal that be complex, poorly-defined, or hard to specify. overcom thi 
limit would greatli expand the possibl impact of deep RL and could increas the reach of 
machin learn more broadly. 

for example, suppos that we want to use reinforc learn to train a robot to clean a tabl or 
scrambl an egg. it’ not clear how to construct a suitabl reward function, which will need to be a 
function of the robot’ sensors. We could tri to design a simpl reward function that approxim 
captur the intend behavior, but thi will often result in behavior that optim our reward 
function without actual satisfi our preferences. thi difficulti underli recent concern about 
misalign between our valu and the object of our RL system (bostrom, 2014; russell, 
2016; amodei et al., 2016). If we could success commun our actual object to our agents, 
it would be a signific step toward address these concerns. 

If we have demonstr of the desir task, we can extract a reward function use invers 
reinforc learn (ng and russell, 2000). thi reward function can then be use to train 
an agent with reinforc learning. more directly, we can use imit learn to clone the 
demonstr behavior. however, these approach be not directli applic to behavior that be 
difficult for human to demonstr (such a control a robot with mani degre of freedom but 
veri non-human morphology). 

ar 
X 

iv 
:1 

70 
6. 

03 
74 

1v 
2 

[ 
st 

at 
.M 

L 
] 

2 
J 

ul 
2 

01 
7 



An altern approach be to allow a human to provid feedback on our system’ current behavior 
and to use thi feedback to defin the task. In principl thi fit within the paradigm of reinforc 
learning, but use human feedback directli a a reward function be prohibit expens for RL 
system that requir hundr or thousand of hour of experience. In order to practic train deep 
RL system with human feedback, we need to decreas the amount of feedback requir by sever 
order of magnitude. 

our approach be to learn a reward function from human feedback and then to optim that reward 
function. thi basic approach have be consid previously, but we confront the challeng involv 
in scale it up to modern deep RL and demonstr by far the most complex behavior yet learn 
from human feedback. 

In summary, we desir a solut to sequenti decis problem without a well-specifi reward 
function that 

1. enabl u to solv task for which we can onli recogn the desir behavior, but not 
necessarili demonstr it, 

2. allow agent to be taught by non-expert users, 

3. scale to larg problems, and 

4. be econom with user feedback. 

RL algorithm environ 

observ 

action 

human 
feedback 

reward predictorpredict 
reward 

figur 1: schemat illustr of our approach: 
the reward predictor be train asynchron 
from comparison of trajectori segments, and the 
agent maxim predict reward. 

our algorithm fit a reward function to the hu- 
man’ prefer while simultan train 
a polici to optim the current predict reward 
function (see figur 1). We ask the human to 
compar short video clip of the agent’ behav- 
ior, rather than to suppli an absolut numer 
score. We found comparison to be easi for hu- 
man to provid in some domains, while be 
equal use for learn human preferences. 
compar short video clip be nearli a fast a 
compar individu states, but we show that 
the result comparison be significantli more 
helpful. moreover, we show that collect feed- 
back onlin improv the system’ perform 
and prevent it from exploit weak of 
the learn reward function. 

our experi take place in two domains: atari game in the arcad learn environ (belle- 
mare et al., 2013), and robot task in the physic simul mujoco (todorov et al., 2012). We 
show that a small amount of feedback from a non-expert human, rang from fifteen minut to five 
hours, suffic to learn most of the origin RL task even when the reward function be not observable. 
We then consid some novel behavior in each domain, such a perform a backflip or drive 
with the flow of traffic. We show that our algorithm can learn these behavior from about an hour of 
feedback—even though it be unclear how to hand-engin a reward function that would incentiv 
them. 

1.1 relat work 

A long line of work studi reinforc learn from human rate or rankings, includ akrour 
et al. (2011), pilarski et al. (2011), akrour et al. (2012), wilson et al. (2012), sugiyama et al. (2012), 
wirth and fürnkranz (2013), daniel et al. (2015), El asri et al. (2016), wang et al. (2016), and wirth 
et al. (2016). other line of research consid the gener problem of reinforc learn from 
prefer rather than absolut reward valu (fürnkranz et al., 2012; akrour et al., 2014), and 
of optim use human prefer in set other than reinforc learn (machw and 
parmee, 2006; secretan et al., 2008; brochu et al., 2010; sørensen et al., 2016). 

our algorithm follow the same basic approach a akrour et al. (2012) and akrour et al. (2014). they 
consid continu domain with four degre of freedom and small discret domains, where they 
can assum that the reward be linear in the expect of hand-cod features. We instead consid 

2 



physic task with dozen of degre of freedom and atari task with no hand-engin features; 
the complex of our environ forc u to use differ RL algorithm and reward models, and 
to cope with differ algorithm tradeoffs. one notabl differ be that akrour et al. (2012) and 
akrour et al. (2014) elicit prefer over whole trajectori rather than short clips. So although we 
gather about two order of magnitud more comparisons, our experi requir less than one order 
of magnitud more human time. other differ focu on chang our train procedur to cope 
with the nonlinear reward model and modern deep rl, for exampl use asynchron train and 
ensembling. 

our approach to feedback elicit close follow wilson et al. (2012). however, wilson et al. 
(2012) assum that the reward function be the distanc to some unknown “target” polici (which be 
itself a linear function of hand-cod features). they fit thi reward function use bayesian inference, 
and rather than perform RL they produc trajectori use the map estim of the target policy. 
their experi involv “synthetic” human feedback which be drawn from their bayesian model, 
while we perform experi with feedback gather from non-expert users. It be not clear if the 
method in wilson et al. (2012) can be extend to complex task or if they can work with real human 
feedback. 

macglashan et al. (2017), pilarski et al. (2011), knox and stone (2009), and knox (2012) perform 
experi involv reinforc learn from actual human feedback, although their algorithm 
approach be less similar. In macglashan et al. (2017) and pilarski et al. (2011), learn onli occur 
dure episod where the human trainer provid feedback. thi appear to be infeas in domain 
like atari game where thousand of hour of experi be requir to learn a high-qual policy, 
and would be prohibit expens even for the simplest task we consider. tamer (knox, 2012) 
follow our approach of learn a reward function and then use that function even when human 
feedback be not available. however, tamer us the reward function myopically, which also appear 
to be unsuit for set where the desir polici be veri complex and requir mani hour of 
experi to learn. 

compar to all prior work, our key contribut be to scale human feedback up to deep reinforc 
learn and to learn much more complex behaviors. thi fit into a recent trend of scale reward 
learn method to larg deep learn systems, for exampl invers RL (finn et al., 2016), imit 
learn (ho and ermon, 2016; stadi et al., 2017), semi-supervis skill gener (finn et al., 
2017), and bootstrap RL from demonstr (silver et al., 2016; hester et al., 2017). 

2 preliminari and method 

2.1 set and goal 

We consid an agent interact with an environ over a sequenc of steps; at each time t the 
agent receiv an observ ot ∈ O from the environ and then send an action at ∈ A to the 
environment. 

In tradit reinforc learning, the environ would also suppli a reward rt ∈ R and the 
agent’ goal would be to maxim the discount sum of rewards. instead of assum that the 
environ produc a reward signal, we assum that there be a human overs who can express 
prefer between trajectori segments. A trajectori segment be a sequenc of observ and 
actions, σ = ((o0, a0), (o1, a1), . . . , (ok−1, ak−1)) ∈ (O ×a)k. write σ1 � σ2 to indic that the 
human prefer trajectori segment σ1 to trajectori segment σ2. informally, the goal of the agent be 
to produc trajectori which be prefer by the human, while make a few queri a possibl to 
the human. 

more precisely, we will evalu our algorithms’ behavior in two ways: 

quantitative: We say that prefer � be gener by a reward function1 r : O ×A → R if(( 
o10, a 

1 
0 

) 
, . . . , 

( 
o1k−1, a 

1 
k−1 
)) 
� 
(( 
o20, a 

2 
0 

) 
, . . . , 

( 
o2k−1, a 

2 
k−1 
)) 

1here we assum here that the reward be a function of the observ and action. In our experi in 
atari environments, we instead assum the reward be a function of the preced 4 observations. In a gener 
partial observ environment, we could instead consid reward function that depend on the whole sequenc 
of observations, and model thi reward function with a recurr neural network. 

3 



whenev 

r 
( 
o10, a 

1 
0 

) 
+ · · ·+ r 

( 
o1k−1, a 

1 
k−1 
) 
> r 
( 
o20, a 

2 
0 

) 
+ · · ·+ r 

( 
o2k−1, a 

2 
k−1 
) 
. 

If the human’ prefer be gener by a reward function r, then our agent ought to 
receiv a high total reward accord to r. So if we know the reward function r, we can 
evalu the agent quantitatively. ideal the agent will achiev reward nearli a high a if it 
have be use RL to optim r. 

qualitative: sometim we have no reward function by which we can quantit evalu 
behavior (thi be the situat where our approach would be practic useful). In these 
cases, all we can do be qualit evalu how well the agent satisfi to the human’ 
preferences. In thi paper, we will start from a goal express in natur language, ask a 
human to evalu the agent’ behavior base on how well it fulfil that goal, and then 
present video of agent attempt to fulfil that goal. 

our model base on trajectori segment comparison be veri similar to the trajectori prefer 
queri use in wilson et al. (2012), except that we don’t assum that we can reset the system to 
an arbitrari state2 and so our segment gener begin from differ states. thi complic the 
interpret of human comparisons, but we show that our algorithm overcom thi difficulti even 
when the human rater have no understand of our algorithm. 

2.2 our method 

At each point in time our method maintain a polici π : O → A and a reward function estim 
r̂ : O ×A → R, each parametr by deep neural networks. 
these network be updat by three processes: 

1. the polici π interact with the environ to produc a set of trajectori {τ1, . . . , τ i}. 
the paramet of π be updat by a tradit reinforc learn algorithm, in order 
to maxim the sum of the predict reward rt = r̂(ot, at). 

2. We select pair of segment 
( 
σ1, σ2 

) 
from the trajectori {τ1, . . . , τ i} produc in step 1, 

and send them to a human for comparison. 
3. the paramet of the map r̂ be optim via supervis learn to fit the comparison 

collect from the human so far. 

these process run asynchronously, with trajectori flow from process (1) to process (2), human 
comparison flow from process (2) to process (3), and paramet for r̂ flow from process (3) 
to process (1). the follow subsect provid detail on each of these processes. 

2.2.1 optim the polici 

after use r̂ to comput rewards, we be left with a tradit reinforc learn problem. We 
can solv thi problem use ani RL algorithm that be appropri for the domain. one subtleti be 
that the reward function r̂ may be non-stationary, which lead u to prefer method which be robust 
to chang in the reward function. thi lead u to focu on polici gradient methods, which have be 
appli success for such problem (ho and ermon, 2016). 

In thi paper, we use advantag actor-crit (a2c; mnih et al., 2016) to play atari games, and trust 
region polici optim (trpo; schulman et al., 2015) to perform simul robot tasks. In 
each case, we use paramet set which have be found to work well for tradit RL tasks. 
the onli hyperparamet which we adjust be the entropi bonu for trpo. thi be becaus trpo 
reli on the trust region to ensur adequ exploration, which can lead to inadequ explor if 
the reward function be changing. 

We normal the reward produc by r̂ to have zero mean and constant standard deviation. thi be 
a typic preprocess step which be particularli appropri here sinc the posit of the reward be 
underdetermin by our learn problem. 

2wilson et al. (2012) also assum the abil to sampl reason initi states. but we work with high 
dimension state space for which random state will not be reachabl and the intend polici inhabit a 
low-dimension manifold. 

4 



2.2.2 prefer elicit 

the human overs be give a visual of two trajectori segments, in the form of short movi 
clips. In all of our experiments, these clip be between 1 and 2 second long. 

the human then indic which segment they prefer, that the two segment be equal good, or that 
they be unabl to compar the two segments. 

the human judgment be record in a databas D of tripl 
( 
σ1, σ2, µ 

) 
, where σ1 and σ2 be the 

two segment and µ be a distribut over {1, 2} indic which segment the user preferred. If the 
human select one segment a preferable, then µ put all of it mass on that choice. If the human 
mark the segment a equal preferable, then µ be uniform. finally, if the human mark the segment 
a incomparable, then the comparison be not includ in the database. 

2.2.3 fit the reward function 

We can interpret a reward function estim r̂ a a preference-predictor if we view r̂ a a latent factor 
explain the human’ judgment and assum that the human’ probabl of prefer a segment 
σi depend exponenti on the valu of the latent reward sum over the length of the clip:3 

P̂ 
[ 
σ1 � σ2 

] 
= 

exp 
∑ 
r̂ 
( 
o1t , a 

1 
t 

) 
exp 

∑ 
r̂(o1t , a 

1 
t ) + exp 

∑ 
r̂(o2t , a 

2 
t ) 
. (1) 

We choos r̂ to minim the cross-entropi loss between these predict and the actual human 
labels: 

loss(r̂) = − 
∑ 

(σ1,σ2,µ)∈d 

µ(1) log P̂ 
[ 
σ1 � σ2 

] 
+ µ(2) log P̂ 

[ 
σ2 � σ1 

] 
. 

thi be simpli the special of the luce-shephard choic rule (luce, 2005; shepard, 1957) to 
prefer over trajectori segments, and be a standard model assumpt in prefer learning. 
It can be understood a equat reward with a prefer rank scale analog to the famou 
elo rank system develop for chess (elo, 1978). just a the differ in elo point of two chess 
player estim the probabl of one player defeat the other in a game of chess, the differ 
in predict reward of two trajectori segment estim the probabl that one be chosen over the 
other by the human. 

our actual algorithm incorpor a number of modif to thi basic approach, which earli 
experi discov to be help and which be analyz in section 3.3: 

• We fit an ensembl of predictors, each train on |d| tripl sampl from D with replace- 
ment. the estim r̂ be defin by independ normal each of these predictor and 
then averag the results. 

• A fraction of 1/e of the data be held out to be use a a valid set for each predictor. 
We use `2 regular and adjust the regular coeffici to keep the valid loss 
between 1.1 and 1.5 time the train loss. In some domain we also appli dropout for 
regularization. 

• rather than appli a softmax directli a describ in equat 1, we assum there be a 
10% chanc that the human respond uniformli at random. conceptu thi adjust be 
need becaus human rater have a constant probabl of make an error, which doesn’t 
decay to 0 a the differ in reward differ becom extreme. 

2.2.4 select queri 

We decid how to queri prefer base on an approxim to the uncertainti in the reward 
function estimator, similar to daniel et al. (2014): we sampl a larg number of pair of trajectori 
segment of length k, use each reward predictor in our ensembl to predict which segment will be 
prefer from each pair, and then select those trajectori for which the predict have the high 
varianc across ensembl members. thi be a crude approxim and the ablat experi in 

3equat 1 do not use discounting, which could be interpret a model the human to be indiffer 
about when thing happen in the trajectori segment. use explicit discount or infer the human’ discount 
function would also be reason choices. 

5 



section 3 show that in some task it actual impair performance. ideally, we would want to queri 
base on the expect valu of inform of the queri (akrour et al., 2012; krueger et al., 2016), 
but we leav it to futur work to explor thi direct further. 

3 experiment result 

We implement our algorithm in tensorflow (abadi et al., 2016). We interfac with mu- 
joco (todorov et al., 2012) and the arcad learn environ (bellemar et al., 2013) through 
the openai gym (brockman et al., 2016). 

3.1 reinforc learn task with unobserv reward 

In our first set of experiments, we attempt to solv a rang of benchmark task for deep RL without 
observ the true reward. instead, the agent learn about the goal of the task onli by ask a human 
which of two trajectori segment be better. our goal be to solv the task in a reason amount of 
time use a few queri a possible. 

In our experiments, feedback be provid by contractor who be give a 1-2 sentenc descript 
of each task befor be ask to compar sever hundr to sever thousand pair of trajectori 
segment for that task (see appendix B for the exact instruct give to contractors). each trajectori 
segment be between 1 and 2 second long. contractor respond to the averag queri in 3-5 seconds, 
and so the experi involv real human feedback requir between 30 minut and 5 hour of 
human time. 

for comparison, we also run experi use a synthet oracl whose prefer over trajectori 
exactli reflect reward in the underli task. that is, when the agent queri for a comparison, instead 
of send the queri to a human, we immedi repli by indic a prefer for whichev 
trajectori segment actual receiv a high reward in the underli task4. We also compar to 
the baselin of RL train use the real reward. our aim here be not to outperform but rather to 
do nearli a well a RL without access to reward inform and instead reli on much scarcer 
feedback. nevertheless, note that feedback from real human do have the potenti to outperform 
RL (and a show below it actual do so on some tasks), becaus the human feedback might 
provid a better-shap reward. 

We describ the detail of our experi in appendix A, includ model architectures, modifica- 
tion to the environment, and the RL algorithm use to optim the policy. 

3.1.1 simul robot 

the first task we consid be eight simul robot tasks, implement in mujoco (todorov 
et al., 2012), and includ in openai gym (brockman et al., 2016). We make small modif 
to these task in order to avoid encod inform about the task in the environ itself (the 
modif be describ in detail in appendix a). the reward function in these task be linear 
function of distances, posit and velocities, and all be a quadrat function of the features. We 
includ a simpl cartpol task (“pendulum”) for comparison, sinc thi be repres of the 
complex of task studi in prior work. 

figur 2 show the result of train our agent with 700 queri to a human rater, compar to 
learn from 350, 700, or 1400 synthet queries, a well a to RL learn from the real reward. 
with 700 label we be abl to nearli match reinforc learn on all of these tasks. train 
with learn reward function tend to be less stabl and high variance, while have a compar 
mean performance. 

surprisingly, by 1400 label our algorithm perform slightli good than if it have simpli be give 
the true reward, perhap becaus the learn reward function be slightli good shaped—th reward 
learn procedur assign posit reward to all behavior that be typic follow by high 
reward. 

4in the case of atari game with spars rewards, it be rel common for two clip to both have zero 
reward in which case the oracl output indifference. becaus we consid clip rather than individu states, 
such tie never make up a larg major of our data. moreover, tie still provid signific inform to the 
reward predictor a long a they be not too common. 

6 



figur 2: result on mujoco simul robot a measur on the tasks’ true reward. We compar 
our method use real human feedback (purple), our method use synthet feedback provid by 
an oracl (shade of blue), and reinforc learn use the true reward function (orange). all 
curv be the averag of 5 runs, except for the real human feedback, which be a singl run, and 
each point be the averag reward over five consecut batches. for reacher and cheetah feedback 
be provid by an author due to time constraints. for all other tasks, feedback be provid by 
contractor unfamiliar with the environ and with our algorithm. the irregular progress on 
hopper be due to one contractor deviat from the typic label schedule. 

real human feedback be typic onli slightli less effect than the synthet feedback; depend 
on the task human feedback rang from be half a effici a ground truth feedback to be 
equal efficient. On the ant task the human feedback significantli outperform the synthet 
feedback, appar becaus we ask human to prefer trajectori where the robot be “stand 
upright,” which prove to be use reward shaping. (there be a similar bonu in the RL reward 
function to encourag the robot to remain upright, but the simpl hand-craft bonu be not a 
useful.) 

3.1.2 atari 

the second set of task we consid be a set of seven atari game in the arcad learn environ- 
ment (bellemar et al., 2013), the same game present in mnih et al., 2013. 

figur 3 show the result of train our agent with 5,500 queri to a human rater, compar to 
learn from 350, 700, or 1400 synthet queries, a well a to RL learn from the real reward. 
our method have more difficulti match RL in these challeng environments, but nevertheless it 
display substanti learn on most of them and match or even exce RL on some. specifically, 
on beamrid and pong, synthet label match or come close to RL even with onli 3,300 such 
labels. On seaquest and qbert synthet feedback eventu perform near the level of RL but learn 
more slowly. On spaceinvad and breakout synthet feedback never match rl, but nevertheless 
the agent improv substantially, often pass the first level in spaceinvad and reach a score of 
20 on breakout, or 50 with enough labels. 

On most of the game real human feedback perform similar to or slightli bad than synthet 
feedback with the same number of labels, and often compar to synthet feedback that have 40% 
few labels. thi may be due to human error in labeling, inconsist between differ contractor 
label the same run, or the uneven rate of label by contractors, which can caus label to be 
overli concentr in narrow part of state space. the latter problem could potenti be address 
by futur improv to the pipelin for outsourc labels. On qbert, our method fail to learn 
to beat the first level with real human feedback; thi may be becaus short clip in qbert can be 
confus and difficult to evaluate. finally, enduro be difficult for a3c to learn due to the difficulti 

7 



figur 3: result on atari game a measur on the tasks’ true reward. We compar our method use 
real human feedback (purple), our method use synthet feedback provid by an oracl (shade of 
blue), and reinforc learn use the true reward function (orange). all curv be the averag 
of 3 runs, except for the real human feedback which be a singl run, and each point be the averag 
reward over about 150,000 consecut frames. 

figur 4: four frame from a singl backflip. the agent be train to perform a sequenc of backflips, 
land upright each time. the video be avail at https://goo.gl/mhgviu. 

of success pass other car through random exploration, and be correspondingli difficult to 
learn with synthet labels, but human label tend to reward ani progress toward pass cars, 
essenti shape the reward and thu outperform a3c in thi game (the result be compar 
to those achiev with dqn). 

3.2 novel behavior 

experi with tradit RL task help u understand whether our method be effective, but the 
ultim purpos of human interact be to solv task for which no reward function be available. 

use the same paramet a in the previou experiments, we show that our algorithm can learn 
novel complex behaviors. We demonstrate: 

1. the hopper robot perform a sequenc of backflip (see figur 4). thi behavior be 
train use 900 queri in less than an hour. the agent learn to consist perform a 
backflip, land upright, and repeat. 

2. the half-cheetah robot move forward while stand on one leg. thi behavior be 
train use 800 queri in under an hour. 

3. keep alongsid other car in enduro. thi be train with roughli 1,300 queri 
and 4 million frame of interact with the environment; the agent learn to stay almost 
exactli even with other move car for a substanti fraction of the episode, although it get 
confus by chang in background. 

8 

https://goo.gl/mhgviu 


figur 5: perform of our algorithm on mujoco task after remov variou components, a 
describ in section section 3.3. all graph be averag over 5 runs, use 700 synthet label 
each. 

video of these behavior can be found at https://goo.gl/mhgviu. these behavior be train 
use feedback from the authors. 

3.3 ablat studi 

In order to good understand the perform of our algorithm, we consid a rang of modifications: 

1. We pick queri uniformli at random rather than priorit queri for which there be 
disagr (random queries). 

2. We train onli one predictor rather than an ensembl (no ensemble). In thi setting, we also 
choos queri at random, sinc there be no longer an ensembl that we could use to estim 
disagreement. 

3. We train on queri onli gather at the begin of training, rather than gather through- 
out train (no onlin queries). 

4. We remov the `2 regular and use onli dropout (no regularization). 
5. On the robot task only, we use trajectori segment of length 1 (no segments). 
6. rather than fit r̂ use comparisons, we consid an oracl which provid the true 

total reward over a trajectori segment, and fit r̂ to these total reward use mean squar 
error (target). 

the result be present in figur 5 for mujoco and figur 6 for atari. 

Of particular interest be the poor perform of offlin reward predictor training; here we find 
that due to the nonstationar of the occup distribution, the predictor captur onli part of the 
true reward, and maxim thi partial reward can lead to bizarr behavior that be undesir a 
measur by the true reward (amodei et al., 2016). for instance, on pong offlin train sometim 
lead our agent to avoid lose point but not to score points; thi can result in extrem long volley 
that repeat the same sequenc of event ad infinitum (video at https://goo.gl/l5eabk). thi 
type of behavior demonstr that in gener human feedback need to be intertwin with RL 
learn rather than provid statically. 

our main motiv for elicit comparison rather than absolut score be that we found it much 
easi for human to provid consist comparison than consist absolut scores, especi on the 
continu control task and on the qualit task in section 3.2; nevertheless it seem import 
to understand how use comparison affect performance. for continu control task we found 

9 

https://goo.gl/mhgviu 
https://goo.gl/l5eabk 


figur 6: perform of our algorithm on atari task after remov variou components, a 
describ in section 3.3. all curv be an averag of 3 run use 5,500 synthet label (see minor 
except in section a.2). 

that predict comparison work much good than predict scores. thi be like becaus the 
scale of reward vari substanti and thi complic the regress problem, which be smooth 
significantli when we onli need to predict comparisons. In the atari task we clip reward and 
effect onli predict the sign, avoid these difficulti (thi be not a suitabl solut for the 
continu control task becaus the rel magnitud of the reward be import to learning). In 
these task comparison and target have significantli differ performance, but neither consist 
outperform the other. 

We also observ larg perform differ when use singl frame rather than clips5. In order 
to obtain the same result use singl frame we would need to have collect significantli more 
comparisons. In gener we discov that ask human to compar longer clip be significantli 
more help per clip, and significantli less help per frame. We found that for short clip it take 
human rater a while just to understand the situation, while for longer clip the evalu time be 
a roughli linear function of the clip length. We tri to choos the shortest clip length for which 
the evalu time be linear. In the atari environ we also found that it be often easi to 
compar longer clip becaus they provid more context than singl frames. 

4 discuss and conclus 

agent-environ interact be often radic cheaper than human interaction. We show that by 
learn a separ reward model use supervis learning, it be possibl to reduc the interact 
complex by roughli 3 order of magnitude. not onli do thi show that we can meaning 
train deep RL agent from human preferences, but also that we be alreadi hit diminish return 
on further sample-complex improv becaus the cost of comput be alreadi compar to 
the cost of non-expert feedback.6 

although there be a larg literatur on prefer elicit and reinforc learn from unknown 
reward functions, we provid the first evid that these techniqu can be econom scale up to 
state-of-the-art reinforc learn systems. thi repres a step toward practic applic 
of deep RL to complex real-world tasks. 

5we onli ran these test on continu control task becaus our atari reward model depend on a sequenc 
of consecut frame rather than a singl frame, a describ in section a.2 

6for the atari experi we be use a virtual machin with 16 cpu and one nvidia k80 gpu which 
cost ~$700/month on gce. train take about a day, so the comput cost be ~$25. train with 5k label 
correspond roughli to 5 hour of human labour, at US minimum wage thi total ~$36. 

10 



futur work may be abl to improv the effici of learn from human preferences, and expand 
the rang of task to which it can be applied. 

In the long run it would be desir to make learn a task from human prefer no more difficult 
than learn it from a programmat reward signal, ensur that power RL system can be appli 
in the servic of complex human valu rather than low-complex goals. 

acknowledg 

We thank olivi pietquin, bilal piot, laurent orseau, pedro ortega, victoria krakovna, owain 
evans, andrej karpathy, igor mordatch, and jack clark for read draft of the paper. We thank 
tyler adkisson, mandi beri, jessica richards, heather tran, and other contractor for provid the 
data use to train our agents. finally, we thank openai and deepmind for provid a support 
research environ and for support and encourag thi collaboration. 

refer 
martin abadi et al. tensorflow: large-scal machin learn on heterogen distribut systems. 

arxiv preprint arxiv:1603.04467, 2016. 

riad akrour, marc schoenauer, and michel sebag. preference-bas polici learning. machin 
learn and knowledg discoveri in databases, page 12–27, 2011. 

riad akrour, marc schoenauer, and michèl sebag. april: activ prefer learning-bas 
reinforc learning. In joint european confer on machin learn and knowledg 
discoveri in databases, page 116–131, 2012. 

riad akrour, marc schoenauer, michèl sebag, and jean-christoph souplet. program by 
feedback. In intern confer on machin learning, page 1503–1511, 2014. 

dario amodei, chri olah, jacob steinhardt, paul christiano, john schulman, and dan mané. 
concret problem in AI safety. arxiv preprint arxiv:1606.06565, 2016. 

marc G bellemare, yavar naddaf, joel veness, and michael bowling. the arcad learn 
environment: An evalu platform for gener agents. journal of artifici intellig research, 
47:253–279, 2013. 

nick bostrom. superintelligence: paths, dangers, strategies. oxford univers press, 2014. 

eric brochu, tyson brochu, and nando de freitas. A bayesian interact optim approach 
to procedur anim design. In proceed of the 2010 acm siggraph/eurograph 
symposium on comput animation, page 103–112. eurograph association, 2010. 

greg brockman, vicki cheung, ludwig pettersson, jona schneider, john schulman, jie tang, and 
wojciech zaremba. openai gym. arxiv preprint arxiv:1606.01540, 2016. 

christian daniel, malt viering, jan metz, oliv kroemer, and jan peters. activ reward learning. 
In robotics: scienc and systems, 2014. 

christian daniel, oliv kroemer, malt viering, jan metz, and jan peter 0001. activ reward 
learn with a novel acquisit function. auton. robots, 39(3):389–405, 2015. 

layla El asri, bilal piot, matthieu geist, romain laroche, and olivi pietquin. score-bas 
invers reinforc learning. In intern confer on autonom agent and multiag 
systems, page 457–465, 2016. 

arpad elo. the rate of chessplayers, past and present. arco pub., 1978. 

chelsea finn, sergey levine, and pieter abbeel. guid cost learning: deep invers optim control 
via polici optimization. In intern confer on machin learning, volum 48, 2016. 

chelsea finn, tianh yu, justin fu, pieter abbeel, and sergey levine. gener skill with 
semi-supervis reinforc learning. In intern confer on learn representations, 
2017. 

11 



johann fürnkranz, eyk hüllermeier, weiwei cheng, and sang-hyeun park. preference-bas 
reinforc learning: A formal framework and a polici iter algorithm. machin learning, 
89(1-2):123–156, 2012. 

todd hester, matej vecerik, olivi pietquin, marc lanctot, tom schaul, bilal piot, andrew 
sendonaris, gabriel dulac-arnold, ian osband, john agapiou, joel Z leibo, and audruna 
gruslys. learn from demonstr for real world reinforc learning. arxiv preprint 
arxiv:1704.03732, 2017. 

jonathan Ho and stefano ermon. gener adversari imit learning. In advanc in neural 
inform process systems, page 4565–4573, 2016. 

W bradley knox and peter stone. interact shape agent via human reinforcement: the 
tamer framework. In intern confer on knowledg capture, page 9–16, 2009. 

william bradley knox. learn from human-gener reward. 2012. 

david krueger, jan leike, owain evans, and john salvatier. activ reinforc learning: observ- 
ing reward at a cost. In futur of interact learn machines, nip workshop, 2016. 

R duncan luce. individu choic behavior: A theoret analysis. courier corporation, 2005. 

jame macglashan, mark K ho, robert loftin, bei peng, david roberts, matthew E taylor, and 
michael L littman. interact learn from policy-depend human feedback. arxiv preprint 
arxiv:1701.06049, 2017. 

AT machw and IC parmee. introduc machin learn within an interact evolutionari design 
environment. In DS 36: proceed design 2006, the 9th intern design conference, 
dubrovnik, croatia, 2006. 

volodymyr mnih, koray kavukcuoglu, david silver, alex graves, ioanni antonoglou, daan 
wierstra, and martin riedmiller. play atari with deep reinforc learning. arxiv preprint 
arxiv:1312.5602, 2013. 

volodymyr mnih, koray kavukcuoglu, david silver, andrei A rusu, joel veness, marc G bellemare, 
alex graves, martin riedmiller, andrea K fidjeland, georg ostrovski, stig petersen, charl 
beattie, amir sadik, ioanni antonoglou, helen king, dharshan kumaran, daan wierstra, shane 
legg, and demi hassabis. human-level control through deep reinforc learning. nature, 
518(7540):529–533, 2015. 

volodymyr mnih, adria puigdomenech badia, mehdi mirza, alex graves, timothi lillicrap, tim 
harley, david silver, and koray kavukcuoglu. asynchron method for deep reinforc 
learning. In intern confer on machin learning, page 1928–1937, 2016. 

andrew Y Ng and stuart russell. algorithm for invers reinforc learning. In intern 
confer on machin learning, page 663–670, 2000. 

patrick M pilarski, michael R dawson, thoma degris, farbod fahimi, jason P carey, and richard 
sutton. onlin human train of a myoelectr prosthesi control via actor-crit reinforc 
learning. In intern confer on rehabilit robotics, page 1–7, 2011. 

stuart russell. should we fear supersmart robots? scientif american, 314(6):58, 2016. 

john schulman, sergey levine, pieter abbeel, michael I jordan, and philipp moritz. trust region 
polici optimization. In intern confer on machin learning, page 1889–1897, 2015. 

jimmi secretan, nichola beato, david B D ambrosio, adelein rodriguez, adam campbell, and 
kenneth O stanley. picbreeder: evolv pictur collabor online. In confer on human 
factor in comput systems, page 1759–1768, 2008. 

roger N shepard. stimulu and respons generalization: A stochast model relat gener 
to distanc in psycholog space. psychometrika, 22(4):325–345, 1957. 

12 



david silver, aja huang, chri J maddison, arthur guez, laurent sifre, georg van den driessche, 
julian schrittwieser, ioanni antonoglou, veda panneershelvam, marc lanctot, sander dieleman, 
dominik grewe, john nham, nal kalchbrenner, ilya sutskever, timothi lillicrap, madelein 
leach, koray kavukcuoglu, thore graepel, and demi hassabis. master the game of Go with 
deep neural network and tree search. nature, 529(7587):484–489, 2016. 

patrikk D sørensen, jeppeh M olsen, and sebastian risi. breed a divers of super mario 
behavior through interact evolution. In comput intellig and game (cig), 2016 
ieee confer on, page 1–7. ieee, 2016. 

bradli C stadie, pieter abbeel, and ilya sutskever. third-person imit learning. In intern 
confer on learn representations, 2017. 

hiroaki sugiyama, toyomi meguro, and yasuhiro minami. preference-learn base invers 
reinforc learn for dialog control. In interspeech, page 222–225, 2012. 

emanuel todorov, tom erez, and yuval tassa. mujoco: A physic engin for model-bas control. 
In intern confer on intellig robot and systems, page 5026–5033, 2012. 

sida I wang, perci liang, and christoph D manning. learn languag game through interaction. 
arxiv preprint arxiv:1606.02447, 2016. 

aaron wilson, alan fern, and prasad tadepalli. A bayesian approach for polici learn from 
trajectori prefer queries. In advanc in neural inform process systems, page 
1133–1141, 2012. 

christian wirth and johann fürnkranz. preference-bas reinforc learning: A preliminari 
survey. In ecml/pkdd workshop on reinforc learn from gener feedback: 
beyond numer rewards, 2013. 

christian wirth, J fürnkranz, gerhard neumann, et al. model-fre preference-bas reinforc 
learning. In aaai, page 2222–2228, 2016. 

13 



A experiment detail 

mani RL environ have termin condit that depend on the behavior of the agent, such 
a end an episod when the agent dy or fall over. We found that such termin condit 
encod inform about the task even when the reward function be not observable. To avoid thi 
subtl sourc of supervision, which could potenti confound our attempt to learn from human 
prefer only, we remov all variable-length episodes: 

• In the gym version of our robot tasks, the episod end when certain paramet go 
outsid of a prescrib rang (for exampl when the robot fall over). We replac these 
termin condit by a penalti which encourag the paramet to remain in the rang 
(and which the agent must learn). 

• In atari games, we do not send life loss or episod end signal to the agent (we do continu 
to actual reset the environment), effect convert the environ into a singl 
continu episode. when provid synthet oracl feedback we replac episod end 
with a penalti in all game except pong; the agent must learn thi penalty. 

remov variabl length episod leaf the agent with onli the inform encod in the 
environ itself; human feedback provid it onli guidanc about what it ought to do. 

At the begin of train we compar a number of trajectori segment drawn from rollout of an 
untrain (randomli initialized) policy. In the atari domain we also pretrain the reward predictor 
for 200 epoch befor begin RL training, to reduc the likelihood of irrevers learn a bad 
polici base on an untrain predictor. for the rest of training, label be fed in at a rate decay 
invers with the number of timesteps; after twice a mani timestep have elapsed, we answer about 
half a mani queri per unit time. the detail of thi schedul be describ in each section. thi 
“label annealing” allow u to balanc the import of have a good predictor from the start with 
the need to adapt the predictor a the RL agent learn and encount new states. when train 
with real human feedback, we attempt to similarli anneal the label rate, although in practic thi be 
approxim becaus contractor give feedback at uneven rates. 

except where otherwis state we use an ensembl of 3 predictors, and draw a factor 10 more clip 
pair candid than we ultim present to the human, with the present clip be select via 
maximum varianc between the differ predictor a describ in section 2.2.4. 

a.1 simul robot task 

the openai gym continu control task penal larg torques. becaus torqu be not di- 
rectli visibl to a human supervisor, these reward function be not good repres of human 
prefer over trajectori and so we remov them. 

for the simul robot tasks, we optim polici use trust region polici optim (trpo, 
schulman et al., 2015) with discount rate γ = 0.995 and λ = 0.97. the reward predictor be a two- 
layer neural network with 64 hidden unit each, use leaki relu (α = 0.01) a nonlinearities.7 We 
compar trajectori segment that last 1.5 seconds, which vari from 15 to 60 timestep depend 
on the task. 

We normal the reward predict to have standard deviat 1. when learn from the reward 
predictor, we add an entropi bonu of 0.01 on all task except swimmer, where we use an entropi 
bonu of 0.001. As note in section 2.2.1, thi entropi bonu help to incentiv the increas 
explor need to deal with a chang reward function. 

We collect 25% of our comparison from a randomli initi polici network at the begin of 
training, and our rate of label after T frame 2 ∗ 106/(t + 2 ∗ 106). 

7all of these reward function be second degre polynomi of the input features, and so if we be 
concern onli with these task we could take a simpler approach to learn the reward function. however, 
use thi more flexibl architectur allow u to immedi gener to task for which the reward function 
be not so simple, a describ in section 3.2. 

14 



a.2 atari 

our atari agent be train use the standard set of environ wrapper use by mnih et al. 
(2015): 0 to 30 no-op in the begin of an episode, max-pool over adjac frames, stack 
of 4 frames, a frameskip of 4, life loss end an episod (but not reset the environment), and 
reward clip to [−1, 1]. 
atari game includ a visual display of the score, which in theori could be use to trivial infer 
the reward. sinc we want to focu instead on infer the reward from the complex dynam 
happen in the game, we replac the score area with a constant black background on all seven 
games. On beamrid we addit blank out the enemi ship count, and on enduro we blank out 
the speedometer. 

for the atari task we optim polici use the a3c algorithm (mnih et al., 2016) in synchron 
form (a2c), with polici architectur a describ in mnih et al. (2015). We use standard set for 
the hyperparameters: an entropi bonu of β = 0.01, learn rate of 0.0007 decay linearli to reach 
zero after 80 million timestep (although run be actual train for onli 50 million timesteps), 
n = 5 step per update, N = 16 parallel workers, discount rate γ = 0.99, and polici gradient use 
adam with α = 0.99 and � = 10−5. 

for the reward predictor, we use 84x84 imag a input (the same a the input to the policy), and 
stack 4 frame for a total 84x84x4 input tensor. thi input be fed through 4 convolut layer 
of size 7x7, 5x5, 3x3, and 3x3 with stride 3, 2, 1, 1, each have 16 filters, with leaki relu 
nonlinear (α = 0.01). thi be follow by a fulli connect layer of size 64 and then a scalar 
output. all convolut layer use batch norm and dropout with α = 0.5 to prevent predictor 
overfitting. In addit we use `2 regular with the adap scheme describ in section 2.2.3. 
sinc the reward predictor be ultim use to compar two sum over timesteps, it scale be arbitrary, 
and we normal it to have a standard deviat of 0.05 (we could equival have adjust our 
learn rate and entropi bonus, but thi choic allow u to use the same paramet a for the real 
reward function). 

We compar trajectori segment of 25 timestep (1.7 second at 15 fp with frame skipping). 

We collect 500 comparison from a randomli initi polici network at the begin of training, 
and our rate of label after T frame of train be decreas everi 5 ∗ 106 frames, to be roughli 
proport to 5 ∗ 106/(t + 5 ∗ 106). 
the predictor be train asynchron from the RL agent, and on our hardwar typic process 
1 label per 10 RL timesteps. We maintain a buffer of onli the last 3,000 label and loop over thi 
buffer continuously; thi be to ensur that the predictor give enough weight to new label (which can 
repres a shift in distribution) when the total number of label becom large. 

In the ablat studi of figur 5b, pretrain have 5,000 label rather than 5,500, and the “target” 
beamrid curv be averag over 2 run rather than 3. 

B instruct provid to contractor 

b.1 mujoco 

give feedback 

sign up for a slot in the spreadsheet. then go to the appropri url’ that we give you, and you’ll 
be repeatedli present with two video clip of an AI control a virtual robot. 

look at the clip and select the one in which good thing happen. onli decid on event you 
actual wit in the clip. 

here’ a guid on what constitut good and bad behavior in each specif domain: 

• hopper: the “center” of the robot be the joint closest to the pointi end. the first prioriti be 
for the center of the robot to move to the right (move to the left be bad than not move 
at all). If the two robot be roughli tie on thi metric, then the tiebreak be how high the 
center is. 

15 



• walker: the “center” of the robot be the joint where the three limb meet. the first prioriti 
be for the center of the robot to move to the right. If the two robot be roughli tie on thi 
metric, then the tiebreak be how high the center is. 

• swimmer: the “center” of the robot be the mark in the middl of it body. the center should 
move to the right a fast a possible. 

• cheetah: the robot should move to the right a fast a possible. 
• ant: the first prioriti be for the robot to be stand upright, and fail that for the center of 

the robot to be a high up a possible. If both robot be upright or neither is, the tie breaker 
be whichev one be move faster to the right. 

• reacher: the green dot on the robot arm should be a close a possibl to the red dot. be 
near for a while and far for a while be bad than be at an intermedi distanc for the 
entir clip. 

• pendulum: the pendulum should be point approxim up. there will be a lot of tie 
where the pendulum have fall and a lot of “can’t tells” where it be off the side of the screen. 
If you can see one pendulum and it hasn’t fall down, that’ good than be unabl to see 
the other pendulum. 

• double-pendulum: both pendulum should be point approxim up (if they fall down, 
the cart should tri to swing them back up) and the cart should be near the center of the track. 
be high for a while and low for a while be bad than be at an intermedi distanc 
the entir time. 

If both clip look about the same to you, then click “tie”. If you don’t understand what’ go on in 
the clip or find it hard to evaluate, then click “can’t tell”. 

you can speed up your feedback by use the arrow key 
left and right select clips, up be a tie, down be “can’t tell”. 

faq 

I get an error say that we’r out of clips. what’ up? occasion the server may run out of 
clip to give you, and you’ll see an error message. thi be normal, just wait a minut and refresh the 
page. If you don’t get clip for more than a coupl minutes, pleas ping @tom on slack. 

Do I need to start right at the time list in the spreadsheet? start 10 minut befor or after 
the list time be fine. 

b.2 atari 

In thi task you’ll be tri to teach an AI to play atari game by give it feedback 
on how well it be playing. 

important. first play the game yourself for 5 minut 

befor provid feedback to the ai, play the game yourself for a five minut to get a sens of how 
it works. it’ often hard to tell what the game be about just by look at short clips, especi if 
you’v never play it before. 

play the game onlin for 5 minutes.8 you’ll need to press f12 or click the game reset button to 
start the game. then set a timer for 5 minut and explor the game to see how it works. 

give feedback 

sign up for a slot in the spreadsheet. then go to the appropri url’ that we give you, and you’ll 
be repeatedli present with two video clip of an AI play the game. 

look at the clip and select the one in which good thing happen. for example, if the left clip 
show the AI shoot an enemi ship while the right clip show it be shot by an enemi ship, then 
good thing happen in the left clip and thu the left clip be better. onli decid on action you actual 
wit in the clip. 

8e.g. http://www.free80sarcade.com/2600_beamrider.php 

16 

http://www.free80sarcade.com/2600_beamrider.php 


here’ a guid on what constitut good and bad play in each specif game: 

• beamrider: shoot enemi ship (good), and don’t get shot (veri bad) 
• breakout: hit the ball with the paddle, break the color blocks, and don’t let the ball fall 

off the bottom of the screen 
• enduro: pa a mani car a you can, and don’t get pass by car 
• pong: knock the ball past the opponent’ orang paddl on the left (good), and don’t let it 

go past your green paddl on the right (bad) 
• qbert: chang the color of a mani block a you can (good), but don’t jump off the side or 

run into enemi (veri bad) 
• spaceinvaders: shoot enemi ship (good), and don’t let your ship (the one at the bottom of 

the screen) get shot (veri bad) 
• seaquest: shoot the fish and enemi submarin (good) and pick up the scuba divers. don’t 

let your submarin run out of air or get hit by a fish or torpedo (veri bad) 
• enduro (even mode): avoid pass car OR get pass by them, you want to stay 

even with other car (not have ani around be OK too) 

don’t worri about how the agent get into the situat it be in (for instance, it doesn’t matter if 
one agent have more lives, or be now on a more advanc level); just focu on what happen in the clip 
itself. 

If both clip look about the same to you, then click “tie”. If you don’t understand what’ go on 
in the clip or find it hard to evaluate, then click “can’t tell”. tri to minim respond “can’t tell” 
unless you truli be confused. 

you can speed up your feedback by use the arrow key 
left and right select clips, up be a tie, down be “can’t tell”. 

faq 

I get an error say that we’r out of clips. what’ up? occasion the server may run out of 
clip to give you, and you’ll see an error message. thi be normal, just wait a minut and refresh the 
page. If you don’t get clip for more than a coupl minutes, pleas ping @tom on slack. 

If the agent be alreadi dead when the clip starts, how should I compar it? If the clip be after 
get kill (but not show the dying), then it perform dure the clip be neither good nor 
bad. you can treat it a pure averag play. If you see it die, or it’ possibl that it contain a frame 
of it dying, then it’ definit bad. 

Do I need to start right at the time list in the spreadsheet? start 30 minut befor or after 
the list time be fine. 

17 


1 introduct 
1.1 relat work 

2 preliminari and method 
2.1 set and goal 
2.2 our method 
2.2.1 optim the polici 
2.2.2 prefer elicit 
2.2.3 fit the reward function 
2.2.4 select queri 


3 experiment result 
3.1 reinforc learn task with unobserv reward 
3.1.1 simul robot 
3.1.2 atari 

3.2 novel behavior 
3.3 ablat studi 

4 discuss and conclus 
A experiment detail 
a.1 simul robot task 
a.2 atari 

B instruct provid to contractor 
b.1 mujoco 
b.2 atari 


