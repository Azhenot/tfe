






































welcom the era of deep neuroevolut 


welcom the era of deep 
neuroevolut 

By kenneth O. stanley & jeff clune 

decemb 18, 2017 

On behalf of an uber AI lab team that also includ joel lehman, jay 

chen, edoardo conti, vashisht madhavan, felip petroski such, & 

xingwen zhang. 

In the field of deep learning, deep neural network (dnns) with mani 

layer and million of connect be now train routin through 

stochast gradient descent (sgd). mani assum that the abil of sgd 

to effici comput gradient be essenti to thi capability. however, 

we be releas a suit of five paper that support the emerg 

realiz that neuroevolution, where neural network be optim 

through evolutionari algorithms, be also an effect method to train 

deep neural network for reinforc learn (rl) problems. uber 

have a multitud of area where machin learn can improv it 

operations, and develop a broad rang of power learn 

approach that includ neuroevolut will help u achiev our 

mission of develop safer and more reliabl transport solutions. 

genet algorithm a a competit altern for train 

deep neural network 

use a new techniqu we invent to effici evolv dnns, we be 

surpris to discov that an extrem simpl genet algorithm (ga) 

can train deep convolut network with over 4 million paramet 

to play atari game from pixels, and on mani game outperform 

modern deep reinforc learn (rl) algorithm (e.g. dqn and 

a3c) or evolut strategi (es), while also be faster due to good 

parallelization. thi result be surpris both becaus gas, which be 

not gradient-based, be not expect to scale well to such larg 

paramet space and also becaus match or outperform the 

state-of-the-art in RL use ga be not thought to be possible. We 

welcom the era of deep neuroevolut https://eng.uber.com/deep-neuroevolution/ 

1 sur 9 12-01-18 à 19:09 



further show that modern GA enhanc that improv the power of 

gas, such a novelti search, also work at dnn scale and can promot 

explor to solv decept problem (those with challeng local 

optima) that stymi reward-maxim algorithm such a q-learn 

(dqn), polici gradient (a3c), es, and the ga. 

thi GA polici score 10,500 on 
frostbite. dqn, ac3, and ES score 
less than 1,000 on thi game. 

the GA play asteroid well. It 
outperform dqn and ES on 
average, but not a3c. 

safe mutat through gradient comput 

In a separ paper, we show how gradient can be combin with 

neuroevolut to improv the abil to evolv recurr and veri deep 

neural networks, enabl the evolut of dnn with over one 

hundr layers, a level far beyond what be previous show possibl 

through neuroevolution. We do so by comput the gradient of 

network output with respect to the weight (i.e. not the gradient of 

error a in convent deep learning), enabl the calibr of 

random mutat to treat the most sensit paramet more 

delic than the least, therebi solv a major problem with random 

mutat in larg networks. 

both anim show a batch of mutat of a singl network that 

could solv the maze (with start at low left and goal at upper left). 

normal mutat mostli lose the abil to reach the end while safe 

mutat larg preserv it while still yield diversity, illustr 

the signific advantag of mutat safely. 

how ES relat to sgd 

our paper complement an already-emerg realization, which be 

first note by a team from openai, that the evolut strategi varieti of 

neuroevolut can optim deep neural network competit on 

deep RL tasks. however, to date, the broader implic of thi result 

have remain a subject of some speculation. set the stage for 

further innov with es, we provid deeper insight into it 

relationship to sgd through a comprehens studi that examin how 

close the ES gradient approxim actual come to the optim 

gradient for each mini-batch comput by sgd on mnist, and also 

welcom the era of deep neuroevolut https://eng.uber.com/deep-neuroevolution/ 

2 sur 9 12-01-18 à 19:09 



how close thi approxim must be to perform well. We show that ES 

can achiev 99 percent accuraci on mnist if enough comput be 

provid to improv it gradient approximation, hint at whi ES will 

increasingli be a seriou contend in deep rl, where no method have 

privileg access to perfect gradient information, a parallel 

comput increases. 

ES be not just tradit finit differ 

ad further understanding, a companion studi confirm empir 

that ES (with a larg enough perturb size parameter) act 

differ than sgd would, becaus it optim for the expect 

reward of a popul of polici describ by a probabl 

distribut (a cloud in the search space), wherea sgd optim 

reward for a singl polici (a point in the search space). thi chang 

caus ES to visit differ area of the search space, for good or for 

bad (both case be illustrated). An addit consequ of 

optim over a popul of paramet perturb be that ES 

acquir robust properti not attain through sgd. highlight 

that ES optim over a popul of paramet also emphas an 

intrigu link between ES and bayesian methods. 

random perturb of the weight of a walker learn by trpo 

lead to significantli less stabl gait than random perturb of a 

walker of equival qualiti evolv by es. the origin learn 

walker be at the center of each nine-fram composite. 

tradit finit differ (gradient descent) cannot cross a narrow 

gap of low fit while ES easili cross it to find high fit on the 

other side. 

ES stall a a path of high fit narrows, while tradit finit 

differ (gradient descent) travers the same path with no 

problem, illustr along with the previou video the distinct and 

trade-off between the two differ approaches. 

improv explor in ES 

An excit consequ of deep neuroevolut be that the collect of 

tool previous develop for neuroevolut now becom candid 

welcom the era of deep neuroevolut https://eng.uber.com/deep-neuroevolution/ 

3 sur 9 12-01-18 à 19:09 



for enhanc the train of deep neural networks. We explor thi 

opportun by introduc new algorithm that combin the 

optim power and scalabl of ES with method uniqu to 

neuroevolut that promot explor in RL domain via a 

popul of agent incentiv to act differ from one 

another. such population-bas explor be differ from the single- 

agent tradit in rl, includ recent work on explor in deep rl. 

our experi reveal that add thi new style of explor 

improv the perform of ES in mani domain that requir 

explor to avoid decept local optima, includ some atari game 

and a humanoid locomot task in the mujoco simulator. 

ES ES w/ explor reward dure train 

with our hyperparameters, ES converg quickli to a local optimum 
of not come up for oxygen becaus do so temporarili forego 
earn reward. with exploration, however, the agent learn to come 
up for oxygen and thu accru much high reward in the future. 
note that saliman et al. 2017 do not report es, with their 
hyperparameters, encount thi particular local optima, but the 
point that ES without explor can get stuck indefinit on some 
local optimum (and that explor help it get unstuck) be a gener 
one, a our paper shows. 

ES ES w/ explor reward dure train 

the agent be task with run a far forward a it can. ES never 
learn to escap the decept trap. with a pressur to explore, 

welcom the era of deep neuroevolut https://eng.uber.com/deep-neuroevolution/ 

4 sur 9 12-01-18 à 19:09 



however, one of the agent learn to navig around the trap. 

conclus 

for neuroevolut research interest in move toward deep 

network there be sever import considerations: first, these kind 

of experi requir more comput than in the past; for the 

experi in these new papers, we often use hundr or even 

thousand of simultan cpu per run. however, the hunger for 

more cpu or gpu should not be view a a liability; in the long run, 

the simplic of scale evolut to massiv parallel comput 

center mean that neuroevolut be perhap best pois to take 

advantag of the world that be coming. 

the new result be so differ from what be previous observ in 

lower-dimension neuroevolut that they effect overturn year 

of intuitions, in particular on the implic of search in high 

dimensions. As have be discov in deep learning, abov some 

threshold of complexity, it appear that search actual becom easi 

in high dimens in the sens that it be less suscept to local 

optima. while the field of deep learn be familiar with thi way of 

thinking, it implic be onli begin to be digest in 

neuroevolution. 

the reemerg of neuroevolut be yet anoth exampl that old 

algorithm combin with modern amount of comput can work 

surprisingli well. the viabil of neuroevolut be interest becaus 

the mani techniqu that have be develop in the neuroevolut 

commun immedi becom avail at dnn-scale, each offer 

differ tool to solv challeng problems. moreover, a our paper 

show, neuroevolut search differ than sgd, and thu offer an 

interest altern approach to have in the machin learn 

toolbox. We wonder whether deep neuroevolut will experi a 

renaiss just a deep learn has. If so, 2017 may mark the 

begin of the era, and we be excit to see what will unfold in the 

year to come! 

here be the five paper we be releas today, along with a summari 

of their key findings: 

welcom the era of deep neuroevolut https://eng.uber.com/deep-neuroevolution/ 

5 sur 9 12-01-18 à 19:09 



deep neuroevolution: genet algorithm be a competit altern 

for train deep neural network for reinforc learn 

evolv dnn with a simple, traditional, population-bas genet 

algorithm that perform well on hard deep RL problems. On atari, the 

GA perform a well a evolut strategi and deep reinforc 

learn algorithm base on q-learn (dqn) and polici gradient 

(a3c). 

the “deep ga” success evolv network with over four million free 

parameters, the larg neural network ever evolv with a tradit 

evolutionari algorithm. 

suggest intriguingli that in some case follow the gradient be not 

the best choic for optim performance. 

combin dnn with novelti search, an explor algorithm 

design for task with decept or spars reward functions, to solv a 

deceptive, high-dimension problem on which reward-maxim 

algorithm (e.g. GA and es) fail. 

show the deep GA parallel good than, and be thu faster than, es, 

a3c, and dqn, and enabl a state-of-the-art compact encod 

techniqu that can repres million-paramet dnn in thousand of 

bytes. 

includ result for random search on atari. surprisingly, on some 

game random search substanti outperform dqn, a3c, and es, 

although it never outperform the ga. 

00:26 -00:09 00:35 

copi and past thi html code into your webpag to embed. 

spaceplay / paus 

qunload | stop 

welcom the era of deep neuroevolut https://eng.uber.com/deep-neuroevolution/ 

6 sur 9 12-01-18 à 19:09 



ffullscreen 

shift + ←→slower / faster 

←→seek 

. seek to previou 

surprisingly, a dnn found through random search play frostbit 

well, outperform dqn, a3c, and es, but not the ga. 

safe mutat for deep and recurr neural network through 

output gradient 

safe mutat through gradient (sm-g) greatli improv the efficaci 

of mutat in large, deep, and recurr network by measur the 

sensit of the network to chang in particular connect weights. 

comput gradient of output with respect to weight instead of 

gradient of error or loss a in convent deep learning, allow 

random, but safe, exploratori steps. 

both type of safe mutat requir no addit trial or rollout in the 

domain. 

the result: deep network (over 100 layers) and larg recurr 

network now evolv effect onli through variant of sm-g. 

On the relationship between the openai evolut strategi and 

stochast gradient descent 

explor the relationship between ES and sgd by compar the 

approxim gradient comput by ES with the exact gradient 

comput by sgd in mnist under differ conditions. 

develop fast proxi that predict ES expect perform with 

differ popul sizes. 

introduc and demonstr differ way to speed up and improv 

the perform of es. 

limit perturb ES significantli speed up execut on parallel 

infrastructure. 

no-mini-batch ES improv the gradient estim by replac the 

mini-batch convent design for sgd with a differ approach 

custom for es: A random subset of the whole train batch be 

welcom the era of deep neuroevolut https://eng.uber.com/deep-neuroevolution/ 

7 sur 9 12-01-18 à 19:09 



assign to each member of the ES popul within each iter of 

the algorithm. thi es-specif approach provid good accuraci for 

ES with equival computation, and the learn curv be much 

smoother than even for sgd. 

no-mini-batch ES reach 99 percent accuraci in a test run, the best 

report perform of an evolutionari approach in thi supervis 

learn task. 

overal help to show whi ES would be abl to compet in rl, where 

gradient inform obtain through trial in the domain be less 

inform with respect to the perform object than in 

supervis learning. 

ES Is more than just a tradit finit differ approxim 

highlight an import distinct between ES and tradit finit 

differ methods, which be that ES optim for an optim 

distribut of solut (a oppos to one optim solution). 

one interest consequence: solut found by ES tend to be robust 

to paramet perturbation. for example, we show that humanoid 

walk solut from ES be significantli more robust to paramet 

perturb than similar solut found by ga and by trpo. 

anoth import consequence: ES be expect to solv some 

problem where convent method would becom trapped, and vice 

versa. simpl exampl illustr these differ dynam between ES 

and convent gradient-following. 

improv explor in evolut strategi for deep reinforc 

learn via a popul of novelty-seek agent 

add the abil to encourag deep explor in es. 

show that algorithm that have be invent to promot explor 

in small-scal evolv neural network via popul of explor 

agent — specif novelti search (ns) and qualiti divers (qd) 

algorithm — can be hybrid with ES to improv it perform on 

spars or decept deep RL tasks, while retain scalability. 

confirm that the result new algorithms, ns-e and a version of 

qd-e call nsr-es, avoid local optimum encount by ES to achiev 

high perform on task rang from simul robot learn to 

welcom the era of deep neuroevolut https://eng.uber.com/deep-neuroevolution/ 

8 sur 9 12-01-18 à 19:09 



walk around a decept trap to the high-dimension pixel-to-act 

task of play atari games. 

add thi new famili of population-bas explor algorithm to the 

deep RL toolbox. 

To be notifi of futur uber AI lab blog posts, pleas sign up for our 

mail list, or you can subscrib to the uber AI lab youtub 

channel. If you be interest in join uber AI labs, pleas appli at 

uber.ai. 

header imag credit: eric frank 

categories: uber data / 

tags: ai, AI labs, already-emerg realization, artifici intelligence, 

atari, deep neuroevolution, dnns, es, evolution, finit difference, 

jeff clune, kenneth stanley, machin learning, ml, mnist, mujoco, 

neuroevolution, novelti search, openai, peter dayan, polici 

gradients, q-learning, reinforc learning, sgd, sm-r, 

stochast gradient descent, trpo, uber, uber AI labs, uber 

engin 

welcom the era of deep neuroevolut https://eng.uber.com/deep-neuroevolution/ 

9 sur 9 12-01-18 à 19:09 


