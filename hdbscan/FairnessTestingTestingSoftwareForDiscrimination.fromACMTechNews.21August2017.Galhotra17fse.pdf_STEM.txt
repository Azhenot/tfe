




















































fair testing: test softwar for discrimin 


fair testing: 
test softwar for discrimin 
sainyam galhotra yuriy brun alexandra meliou 

univers of massachusetts, amherst 
amherst, massachusett 01003-9264, usa 
{sainyam, brun, ameli}@cs.umass.edu 

abstract 
thi paper defin softwar fair and discrimin and devel- 
op a testing-basedmethod formeasur if and howmuch softwar 
discriminates, focu on causal in discriminatori behavior. 
evid of softwar discrimin have be found in modern 
softwar system that recommend crimin sentences, grant access 
to financi products, and determin who be allow to particip 
in promotions. our approach, themis, gener effici test suit 
to measur discrimination. given a schema describ valid system 
inputs, themi gener discrimin test automat and 
do not requir an oracle. We evalu themi on 20 softwar 
systems, 12 of which come from prior work with explicit focu 
on avoid discrimination. We find that (1) themi be effect at 
discov softwar discrimination, (2) state-of-the-art techniqu 
for remov discrimin from algorithm fail in mani situa- 
tions, at time discrimin against a much a 98% of an input 
subdomain, (3) themi optim be effect at produc 
effici test suit for measur discrimination, and (4) themi be 
more effici on system that exhibit more discrimination. We thu 
demonstr that fair test be a critic aspect of the softwar 
develop cycl in domain with possibl discrimin and 
provid initi tool for measur softwar discrimination. 

cc concept 
• softwar and it engin → softwar test and de- 
bug 

keyword 
discrimin testing, fair testing, softwar bias, test 

acm refer format: 
sainyam galhotra, yuriy brun, and alexandra meliou. 2017. fair 
testing: test softwar for discrimination. In proceed of 2017 11th 
joint meet of the european softwar engin confer and the acm 
sigsoft symposium on the foundat of softwar engineering, paderborn, 
germany, septemb 4–8, 2017 (esec/fse’17), 13 pages. 
https://doi.org/10.1145/3106237.3106277 

permiss to make digit or hard copi of all or part of thi work for person or 
classroom use be grant without fee provid that copi be not make or distribut 
for profit or commerci advantag and that copi bear thi notic and the full citat 
on the first page. copyright for compon of thi work own by other than the 
author(s) must be honored. abstract with credit be permitted. To copi otherwise, or 
republish, to post on server or to redistribut to lists, requir prior specif permiss 
and/or a fee. request permiss from permissions@acm.org. 
esec/fse’17, septemb 4–8, 2017, paderborn, germani 
© 2017 copyright held by the owner/author(s). public right licens to associa- 
tion for comput machinery. 
acm isbn 978-1-4503-5105-8/17/09. . . $15.00 
https://doi.org/10.1145/3106237.3106277 

1 introduct 
softwar have becom ubiquit in our societi and the import 
of it qualiti have increased. today, automation, advanc in ma- 
chine learning, and the avail of vast amount of data be 
lead to a shift in how softwar be used, enabl the softwar to 
make more autonom decisions. already, softwar make deci- 
sion in what product we be lead to buy [53], who get a loan [62], 
self-driv car action that may lead to properti damag or human 
injuri [32], medic diagnosi and treatment [74], and everi stage 
of the crimin justic system includ arraign and sentenc 
that determin who go to jail and who be set free [5, 28]. the im- 
portanc of these decis make fair and nondiscrimin 
in softwar a import a softwar quality. 

unfortunately, softwar fair be undervalu and littl at- 
tention be paid to it dure the develop lifecycle. countless 
exampl of unfair softwar have emerged. In 2016, amazon.com, 
inc. use softwar to determin the part of the unit state to 
which it would offer free same-day delivery. the softwar make 
decis that prevent minor neighborhood from participat- 
ing in the program, often when everi surround neighborhood 
be allow to particip [36, 52]. similarly, softwar be be 
use to comput risk-assess score for suspect criminals. 
these scores— an estim probabl that the person arrest for 
a crime be like to commit anoth crime— be use to inform deci- 
sion about who can be set free at everi stage of the crimin justic 
system process, from assign bond amounts, to decid guilt, 
to sentencing. today, the u.s. justic department’ nation insti- 
tute of correct encourag the use of such assess scores. 
In arizona, colorado, delaware, kentucky, louisiana, oklahoma, 
virginia, washington, and wisconsin, these score be give to 
judg dure crimin sentencing. the wisconsin suprem court 
recent rule unanim that the compa comput program, 
which us attribut includ gender, can assist in sentenc de- 
fendant [28]. despit the import of these scores, the softwar 
be know to make mistakes. In forecast who would reoffend, the 
softwar be “particularli like to fals flag black defend a fu- 
ture criminals, wrongli label them thi way at almost twice the 
rate a white defendants; white defend be mislabel a low 
risk more often than black defendants” [5]. prior crimin histori 
do not explain thi difference: control for crimin history, 
recidivism, age, and gender show that the softwar predict black 
defend to be 77% more like to be peg a at high risk 
of commit a futur violent crime than white defend [5]. 
go forward, the import of ensur fair in softwar 
will onli increase. for example, “it’ likely, and some say inevitable, 
that futur ai-pow weapon will eventu be abl to oper 
with complet autonomy, lead to a watersh moment in the 

498 

mailto:sainyam@cs.umass.edu,brun@cs.umass.edu,ameli@cs.umass.edu 
https://doi.org/10.1145/3106237.3106277 
https://doi.org/10.1145/3106237.3106277 


esec/fse’17, septemb 4–8, 2017, paderborn, germani sainyam galhotra, yuriy brun, and alexandra meliou 

histori of warfare: for the first time, a collect of microchip and 
softwar will decid whether a human be life or dies” [34]. In 
fact, in 2016, the u.s. execut offic of the presid identifi 
bia in softwar a a major concern for civil right [27]. and one of 
the ten principl and goal satya nadella, the ceo of microsoft 
co., have laid out for artifici intellig be “ai must guard against 
bias, ensur proper, and repres research so that the wrong 
heurist cannot be use to discriminate” [61]. 

thi paper defin causal softwar discrimin and propos 
themis, a softwar test method for evalu the fair of 
software. our definit captur causal relationship between in- 
put and outputs, and can, for example, detect when sentenc 
softwar behav such that “chang onli the applicant’ race af- 
fect the software’ sentenc recommend for 13% of possibl 
applicants.” prior work on detect discrimin have focu on 
measur correl or mutual inform between input and 
output [79], discrep in the fraction of input that produc a 
give output [18, 19, 39–42, 87–89, 91], or discrep in output 
probabl distribut [51]. these approach do not captur 
causal and can miss discrimin that our causal approach de- 
tects, e.g., when the softwar discrimin neg with respect 
to a group in one settings, but posit in another. restrict the 
input space to real-world input [1] may similarli hide softwar 
discrimin that causal test can reveal. unlik prior work 
that requir manual write test [79], themi automat 
gener test suit that measur discrimination. To the best of 
our knowledge, thi work be the first to automat gener test 
suit to measur causal discrimin in software. 

themi would be use for compani and govern agen- 
cie reli on softwar decisions. for example, amazon.com, inc. 
receiv strong neg public after it same-day deliveri al- 
gorithm make racial bia decisions. politician and citizen 
in massachusetts, new york, and illinoi demand that the com- 
pani offer same-day deliveri servic to minor neighborhood in 
boston, new york city, and chicago, and the compani be forc 
to revers cours within mere day [72, 73]. surely, the compani 
would have prefer to test it softwar for racial bia and to de- 
velop a strategi (e.g., fix the software, manual review and 
modifi racist decisions, or not use the software) prior to de- 
ploy it. themi could have analyz the softwar and detect 
the discrimin prior to deployment. similarly, a govern 
may need to set nondiscrimin requir on software, and 
be abl to evalu if softwar satisfi those requir befor 
mandat it to be use in the justic system. In 2014, the u.s. 
attorney gener eric holder warn that step need to be take 
to prevent the risk-assess score inject bia into the courts: 
“although these measur be craft with the best of intentions, 
I be concern that they inadvert undermin our effort to 
ensur individu and equal justic [and] they may exacerb 
unwarr and unjust dispar that be alreadi far too com- 
mon in our crimin justic system and in our society.” [5]. As with 
softwar quality, test be like to be the best way to evalu 
softwar fair properties. 

unlik prior work, thi paper defin discrimin a a causal 
relationship between an input and an output. As defin here, dis- 
crimin be not necessarili bad. for example, a softwar system 

design to identifi if a pictur be of a cat should discrimin be- 
tween cat and dogs. It be not our goal to elimin all discrimin 
in software. instead, it be our goal to empow the develop and 
stakehold to identifi and reason about discrimin in soft- 
ware. As describ above, there be plenti of real-world exampl 
in which compani would prefer to have discov discrimina- 
tion earlier, prior to release. specifically, our technique’ job be to 
identifi if softwar discrimin with respect to a specif set of 
characteristics. If the stakehold expect cat vs. dog discrimination, 
she would exclud it from the list of input characterist to test. 
however, learn that the softwar frequent misclassifi black 
cat can help the stakehold improv the software. know if 
there be discrimin can lead to better-inform decis making. 

there be two main challeng to measur discrimin via 
testing. first, gener a practic set of test input suffici for 
measur discrimination, and second, process those test inputs’ 
execut to comput discrimination. thi paper tackl both chal- 
lenges, but the main contribut be comput discrimin from 
a set of executions. We be awar of no prior test technique, 
neither autom nor manual, that produc a measur of a soft- 
ware system’ causal discrimination. the paper also contribut 
within the space of effici test input gener for the specif 
purpos of discrimin test (see section 4), but some prior 
work, specif in combinatori testing, e.g., [6, 44, 47, 80], may 
further help the effici of test generation, though these tech- 
niqu have not be previous appli to discrimin testing. 
We leav a detail examin of how combinatori test and 
other autom test gener can help further improv themi 
to futur work. 

thi paper’ main contribut are: 

(1) formal definit of softwar fair and discrimination, in- 
clude a causality-bas improv on the state-of-the-art 
definit of algorithm fairness. 

(2) themis, a techniqu and open-sourc implementation— 
https://github.com/laser-umass/themis— formeasur dis- 
crimin in software. 

(3) A formal analysi of the theoret foundat of themis, in- 
clude proof of monoton of discrimin that lead to 
provabl sound two-to-thre order of magnitud improve- 
ment in test suit size, a proof of the relationship between 
fair definitions, and a proof that themi be more effici 
on system that exhibit more discrimination. 

(4) An evalu of the fair of 20 real-world softwar instanc 
(base on 8 softwar systems), 12 of which be design with 
fair in mind, demonstr that (i) even when fair be a 
design goal, develop can easili introduc discrimin in 
software, and (ii) themi be an effect fair test tool. 

themi requir a schema for gener inputs, but do not 
requir an oracle. our causal fair definit be design specif- 
ical to be testable, unlik definit that requir probabilist 
estim or knowledg of the futur [38]. softwar test offer 
a uniqu opportun to conduct causal experi to determin 
statist causal [67]: one can run the softwar on an input (e.g., 
a defendant’ crimin record), modifi a specif input character- 
istic (e.g., the defendant’ race), and observ if that modif 
caus a chang in the output. We defin softwar to be causal 

499 

https://github.com/laser-umass/themi 


fair testing: test softwar for discrimin esec/fse’17, septemb 4–8, 2017, paderborn, germani 

fair with respect to input characterist χ if for all inputs, vari 
the valu of χ do not alter the output. for example, a sentence- 
recommend system be fair with respect to race if there be 
no two individu who differ onli in race but for whom the sys- 
tem’ sentenc recommend differs. In addit to captur 
causality, thi definit requir no oracle— the equival of the 
output for the two input be itself the oracle—which help fulli 
autom test generation. 

the rest of thi paper be structur a follows. section 2 provid 
an intuit to fair measur and section 3 formal defin 
softwar fairness. section 4 describ themis, our approach to 
fair testing. section 5 evalu themis. finally, section 6 
place our work in the context of relat research and section 7 
summar our contributions. 

2 softwar fair measur 
suppos a bank employ loan softwar to decid if loan applic 
should be give loans. loan input be each applicant’ name, 
age, race, income, savings, employ status, and request loan 
amount, and the output be a binari “give loan” or “do not give loan”. 
for simplicity, suppos age and race be binary, with age either <40 
or >40, and race either green or purple. 

some prior work on measur and remov discrimin 
from algorithm [18, 19, 39–42, 88, 89, 91] have focu on what we 
call group discrimination, which say that to be fair with respect to 
an input characteristic, the distribut of output for each group 
should be similar. for example, the loan softwar be fair with re- 
spect to age if it give loan to the same fraction of applic 
<40 and >40. To be fair with respect to multipl characteristics, 
for example, age and race, all group with respect to those char- 
acterist — purpl <40, purpl >40, green <40, and green >40— 
should have the same outcom fractions. the calders-verw (cv) 
score [19] measur the strength of group discrimin a the 
differ between the larg and the small outcom fractions; 
if 30% of peopl <40 get the loan, and 40% of peopl >40 get the 
loan, then loan be 40% − 30% = 10% group discriminating. 

while group discrimin be easi to reason about and measure, 
it have two inher limitations. first, group discrimin may fail 
to observ some discrimination. for example, suppos that loan 
produc differ output for two loan applic that differ in 
race, but be otherwis identical. while loan clearli discrimin 
with respect to race, the group discrimin score will be 0 if loan 
discrimin in the opposit way for anoth pair of applications. 
second, softwar may circumv discrimin detection. for 
example, suppos loan recommend loan for a random 30% of the 
purpl applicants, and the 30% of the green applic who have 
the most savings. then the group discrimin score with respect 
to race will deem loan perfectli fair, despit a clear discrep in 
how the applic be process base on race. 

To address these issues, we defin a new measur of discrimi- 
nation. softwar test enabl a uniqu opportun to conduct 
causal experi to determin statist causat [67] between 
input and outputs. for example, it be possibl to execut loan on 
two individu ident in everi way except race, and verifi if 
chang the race caus a chang in the output. causal discrimina- 
tion say that to be fair with respect to a set of characteristics, the 

softwar must produc the same output for everi two individu 
who differ onli in those characteristics. for example, the loan 
softwar be fair with respect to age and race if for all pair of indi- 
vidual with ident name, income, savings, employ status, 
and request loan amount but differ race or age characteristics, 
loan either give all of them or none of them the loan. the fraction 
of input for which softwar causal discrimin be a measur of 
causal discrimination. 

thu far, we have discuss softwar oper on the full input 
domain, e.g., everi possibl loan application. however, appli 
softwar to partial input domain may mask or effect discrimi- 
nation. for example, while softwar may discrimin on some 
loan applications, a bank may care about whether that softwar 
discrimin onli with respect to applic repres of 
their customers, a oppos to all possibl human beings. In thi 
case, a partial input domain may mask discrimination. If a partial 
input domain exhibit correl between input characteristics, it 
can effect discrimination. for example, suppos old individu 
have, on average, high incom and larg savings. If loan onli 
consid incom and save in make it decision, even though 
it do not consid age, for thi population, loan give loan to 
a high fraction of old individu than young ones. We call 
the measur of group or causal discrimin on a partial 
input domain appar discrimination. appar discrimin de- 
pend on the oper profil of the system system’ use [7, 58]. 
appar discrimin be import to measure. for example, 
amazon.com, inc. softwar that determin where to offer free 
same-day deliveri do not explicitli consid race but make race- 
correl decis becaus of correl between race and other 
input characterist [36, 52]. despit the algorithm not look 
at race explicitly, amazon.com, inc. would have prefer to have 
test for thi kind of discrimination. 

3 formal fair definit 
We make two simplifi assumptions. first, we defin softwar 
a a black box that map input characterist to an output char- 
acteristic. while softwar is, in general, more complex, for the 
purpos of fair testing, without loss of generality, thi defini- 
tion be sufficient: all user action and environment variabl be 
model a input characteristics, and each softwar effect be mod- 
ele a an output characteristic. when softwar have multipl output 
characteristics, we defin fair with respect to each output char- 
acterist separately. the definit can be extend to includ 
multipl output characterist without signific conceptu re- 
formulation. second, we assum that the input characterist and 
the output characterist be categor variables, each have a set 
of possibl valu (e.g., race, gender, eye color, age ranges, incom 
ranges). thi assumpt simplifi our measur of causality. while 
our definit do not appli directli to non-categor input and 
output characterist (such a continu variables, e.g., int and 
double), they, and our techniques, can be appli to softwar with 
non-categor input and output characterist by use bin 
(e.g., age<40 and age>40). the output domain distanc function 
(definit 3.3) illustr one way our definit can be extend 
to continu variables. futur work will extend our discrimin 
measur directli to a broader class of data types. 

500 



esec/fse’17, septemb 4–8, 2017, paderborn, germani sainyam galhotra, yuriy brun, and alexandra meliou 

A characterist be a categor variable. An input type be a set 
of characteristics, an input be a valuat of an input type (assign- 
ment of a valu to each characteristic), and an output be a singl 
characteristic. 

definit 3.1 (characteristic). let L be a set of valu labels. A 
characterist χ over L be a variabl that can take on the valu in L. 

definit 3.2 (input type and input). for all n ∈ N, let l1,l2, . . . , 
Ln be set of valu labels. then an input type X over those valu 
label be a sequenc of characterist X = ⟨χ1, χ2, . . . , χn⟩, where 
for all i ≤ n, χi be a characterist over Li . 

An input of type X be k = ⟨l1 ∈ l1, l2 ∈ l2, . . . , ln ∈ ln⟩, a 
valuat of an input type. 

We say the size of input k and of input type X be n. 

discrimin can bemeasur in softwar thatmak decisions. 
when the output characterist be binari (e.g., “give loan” vs. “do not 
give loan”) the signific of the two differ output valu be clear. 
when output be not binary, identifi potenti discrimin 
requir understand the signific of differ in the output. 
for example, if the softwar output an order of hotel list 
(that may be influenc by the comput you be using, a be the 
case when softwar use by orbitz lead appl user to higher-pr 
hotel [53]), domain expertis be need to compar two output 
and decid the degre to which their differ be significant. the 
output domain distanc function encod thi expertise, map 
pair of output valu to a distanc measure. 

definit 3.3 (output domain distanc function). let Lo be a set of 
valu labels. then for all lo1, lo2 ∈ Lo , the output distanc function 
be δ : Lo × Lo → [0..1] such that lo1 = lo2 =⇒ δ (lo1, lo2) = 0. 

the output domain distanc function gener our work be- 
yond binari outputs. for simplic of exposition, for the remaind 
of thi paper, we assum softwar output binari decisions— a nat- 
ural domain for fair testing. while true or fals output 
(correspond to decis such a “give loan” vs. “do not give 
loan”) be easi to understand, the output domain distanc func- 
tion enabl compar non-binari output in two ways. first, a 
threshold output domain distanc function can determin when two 
output be dissimilar enough to warrant potenti discrimination. 
second, a relat output domain distanc function can describ 
how differ two input be and how much they contribut to 
potenti discrimination. definit 3.5, 3.6, 3.8, and 3.7, could be 
extend to handl non-binari output by chang their exact 
output comparison to fraction similar comparison use an 
output domain distanc function, similar to the way input have 
be handl in prior work [24]. 

definit 3.4 (decis software). let n ∈ N be an input size, let 
l1,l2, . . . ,ln be set of valu labels, let X = ⟨χ1, χ2, . . . , χn⟩ be 
an input type, and let K be the set of all possibl input of type 
X . decis softwar be a function S : K → {true, false}. that is, 
when softwar S be appli to an input ⟨l1 ∈ l1, l2 ∈ l2, . . . , ln ∈ 
ln⟩, it produc true or false. 

the group discrimin score vari from 0 to 1 and measur 
the differ between fraction of input group that lead to the 
same output (e.g., the differ between the fraction of green and 

purpl individu who be give a loan). thi definit be base 
on the CV score [19], which be limit to a binari input type or a 
binari partit of the input space. our definit extend to 
the more broad categor input types, reflect the rel com- 
plexiti of arbitrari decis software. the group discrimin 
score with respect to a set of input characterist be the maximum 
frequenc with which the softwar output true minu the mini- 
mum such frequenc for the group that onli differ in those input 
characteristics. becaus the CV score be limit to a singl binari 
partitioning, that differ repres all the encod discrimi- 
nation inform in that setting. In our more gener set 
with multipl non-binari characteristics, the score focu on the 
range—differ between the maximum and minimum—a op- 
pose to the distribution. one could consid measuring, say, the 
standard deviat of the distribut of frequenc instead, which 
would good measur deviat from a complet fair algorithm, 
a oppos to the maxim deviat for two extrem groups. 

definit 3.5 (univari group discrimin score d̃). let K be 
the set of all possibl input of size n ∈ N of type X = ⟨χ1, χ2, . . . , 
χn⟩ over label valu l1,l2, . . . ,ln . let softwar S : K → {true, 
false}. 

for all i ≤ n, fix one characterist χi . that is, letm = |li | and 
for all m̂ ≤ m, let km̂ be the set of all input with χi = lm̂ . (km̂ be 
the set of all input with the χi th characterist fix to be lm̂ .) let 
pm̂ be the fraction of input k ∈ km̂ such that s(k ) = true. and 
let P = ⟨p1,p2, . . . ,pm⟩. 

then the univari group discrimin score with respect to 
χi , denot d̃χi (s), be max(p ) −min(p ). 

for example, consid loan softwar that decid to give loan 
to 23% of green individuals, and to 65% of purpl individuals. 
when comput loan’ group discrimin score with respect 
to race, d̃race (loan) = 0.65 − 0.23 = 0.42. 

the multivari group discrimin score gener the uni- 
variat version to multipl input characteristics. 

definit 3.6 (multivari group discrimin score d̃). for all 
α , β , . . . ,γ ≤ n, fix the characterist χα , χβ , . . . , χγ . that is, let 
mα = |lα |, mβ = |lβ |, . . ., mγ = |lγ |, let m̂α ≤ mα , m̂β ≤ mβ , 
. . . , m̂γ ≤ mγ , andm = mα ×mβ × · · · ×mγ , let km̂α ,m̂β , ...,m̂γ 
be the set of all input with χα = lm̂α , χβ = lm̂β , . . ., χγ = lm̂γ . 
(km̂α ,m̂β , ...,m̂γ be the set of all input with the χα characterist 
fix to be lm̂α , χβ characterist fix to be lm̂β , and so on.) let 
pm̂α ,m̂β , ...,m̂γ be the fraction of input k ∈ km̂α ,m̂β , ...,m̂γ such 
that s(k ) = true. and let P be an unord sequenc of all 
pm̂α ,m̂β , ...,m̂γ . 

then the multivari group discrimin score with respect 
to χα , χβ , . . . , χγ , denot d̃χα , χβ , ..., χγ (s) be max(p ) −min(p ). 

our causal discrimin score be a strong measur of discrimi- 
nation, a it seek out causal in software, measur the fraction 
of input for which chang specif input characterist caus 
the output to chang [67]. the causal discrimin score identi- 
fie chang which characterist directli affect the output. As a 
result, for example, while the group and appar discrimin 
score penal softwar that give loan to differ fraction of 

501 



fair testing: test softwar for discrimin esec/fse’17, septemb 4–8, 2017, paderborn, germani 

individu of differ races, the causal discrimin score penal- 
ize softwar that give loan to individu of one race but not to 
otherwis ident individu of anoth race. 

definit 3.7 (multivari causal discrimin score d⃗). let 
K be the set of all possibl input of size n ∈ N of type X = 
⟨χ1, χ2, . . . , χn⟩ over label valu l1,l2, . . . ,ln . let softwar S : 
K → {true, false}. for all α , β, . . . ,γ ≤ n, let χα , χβ , . . . , χγ be 
input characteristics. 

then the causal discrimin score with respect to χα , χβ , . . . , 
χγ , denot d⃗χα , χβ , ..., χγ (s) be the fraction of input k ∈ K such 
that there exist an input k ′ ∈ K such that k and k ′ differ onli in 
the input characterist χα , χβ , . . . , χγ , and s(k ) , s(k ′). that 
is, the causal discrimin score with respect to χα , χβ , . . . , χγ 
be the fraction of input for which chang at least one of those 
characterist caus the output to change. 

thu far, we have measur discrimin of the full input do- 
main, consid everi possibl input with everi valu of everi 
characteristic. In practice, input domain may be partial. A com- 
pani may, for example, care about whether softwar discrimin 
onli with respect to their custom (recal section 2). appar 
discrimin captur thi notion, appli group or causal dis- 
crimin score measur to a subset of the input domain, 
which can be describ by an oper profil [7, 58]. 

definit 3.8 (multivari appar discrimin score). let 
K̈ ⊆ K be a subset of the input domain to S. then the appar 
group discrimin score be the group discrimin score appli 
to K̈ , and the appar causal discrimin score be the causal dis- 
crimin score appli to K̈ (a oppos to appli to the full K ). 

have defin discrimination, we now defin the problem of 
check softwar for discrimination. 

definit 3.9 (discrimin check problem). given an input 
type X , decis softwar S with input type X , and a threshold 
0 ≤ θ ≤ 1, comput all X ′ ⊆ X such that d̃x ′ (s) ≥ θ or d⃗x ′ (s) ≥ θ . 

4 the themi solut 
thi section describ themis, our approach to effici fair 
testing. To use themis, the user provid a softwar executable, a 
desir confid level, an accept error bound, and an input 
schema describ the format of valid inputs. themi can then be 
use in three ways: 
(1) themi gener a test suit to comput the softwar group or 

causal discrimin score for a particular set of characteristics. 
for example, one can use themi to check if, and how much, a 
softwar system discrimin against race and age. 

(2) given a discrimin threshold, themi gener a test suit 
to comput all set of characterist against which a softwar 
group or causal discrimin more than that threshold. 

(3) given a manually-written or automatically-gener test suite, 
or an oper profil describ an input distribut [7, 58], 
themi comput the appar group or causal discrimin 
score for a particular set of characteristics. for example, one 
can use themi to check if a system discrimin against race 
on a specif popul of input repres of the way the 

algorithm 1: comput group discrimination. given a soft- 
ware S and a subset of it input characterist X ′, groupdis- 
crimin return d̃x ′ (s), the group discrimin score 
with respect to X ′, with confid conf and error margin ϵ . 
groupdiscrimination(s, X ′, conf , ϵ ) 

1 mingroup ← ∞, maxgroup ← 0, testsuit ← ∅ ▷initi 
2 foreach A, where A be a valu assign for X ′ do 
3 r ← 0 ▷initi number of sampl 
4 count ← 0 ▷initi number of posit output 
5 while r < max_sampl do 
6 r ← r + 1 
7 k ← newrandominput (X ′ ← A) ▷new input k with 

k .X ′ = A 
8 testsuit ← testsuit ∪ {k } ▷add input to the test suit 
9 if notcach (k ) then ▷no cach execut of k exist 

10 compute(s(k )) ▷evalu softwar on input k 
11 cacheresult (k, s(k )) ▷cach the result 

12 els ▷retriev cach result 
13 s(k ) ← retrievecach (k ) 
14 if s(k ) then 
15 count ← count + 1 
16 if r > sampling_threshold then 

▷after suffici samples, check error margin 
17 p ← countr ▷current proport of posit output 

18 if conf .zvalu 
√ 
p (1−p ) 

r < ϵ then 
19 break ▷achiev error < ϵ , with confid conf 

20 maxgroup ← max(maxgroup, p ) 
21 mingroup ← min(mingroup, p ) 
22 return testsuite, d̃x ′ (s) ← maxgroup −mingroup 

system will be used. thi method do not comput the score’ 
confid a it be onli a strong a the developers’ confid 
that test suit or oper profil be repres of real- 
world executions. 

measur group and causal discrimin exactli requir ex- 
haustiv testing, which be infeas for nontrivi software. solv 
the discrimin check problem (definit 3.9) further re- 
quir measur discrimin over all possibl subset of charac- 
terist to find those that exceed a certain discrimin threshold. 

themi address these challeng by employ three optimiza- 
tions: (1) test caching, (2) adaptive, confidence-driven sampling, and 
(3) sound pruning. all three techniqu reduc the number of test 
case need to comput both group and causal discrimination. sec- 
tion 4.1 describ how themi employ cach and sampling, and 
section 4.2 describ how themi prune the test suit search space. 

4.1 cach and approxim 
groupdiscrimin (algorithm 1) and causaldiscrimin 
(algorithm 2) present the themi comput of multivari 
group and causal discrimin score with respect to a set of 
characteristics. these algorithm implement definit 3.6 and 3.7, 
respectively, and reli on two optimizations. We first describ these 
optim and then the algorithms. 

502 



esec/fse’17, septemb 4–8, 2017, paderborn, germani sainyam galhotra, yuriy brun, and alexandra meliou 

test caching. precis comput the group and causal discrimi- 
nation score requir execut a larg set of tests. however, a lot 
of thi comput be repetitive: test relev to group discrimina- 
tion be also relev to causal discrimination, and test relev to 
one set of characterist can also be relev to anoth set. thi 
redund in fair test allow themi to exploit cach to 
reus test result without re-execut tests. test cach have low 
storag overhead and offer signific runtim gains. 
adaptive, confidence-driven sampling. sinc exhaust test- 
ing be infeasible, themi comput approxim group and causal 
discrimin score through sampling. sampl in themi be 
adaptive, use the ongo score comput to determin if a 
specifi margin of error ϵ with a desir confid level conf have 
be reached. themi gener input uniformli at random use 
an input schema, and maintain the proport of sampl (p) for 
which the softwar output true (in groupdiscrimination) or for 
which the softwar chang it output (in causaldiscrimination). 
the margin of error for p be then comput as: 

error = z∗ 
√ 

p (1 − p) 
r 

where r be the number of sampl so far and z∗ be the normal distri- 
bution z∗ score for the desir confid level. themi return if 
error < ϵ , or gener anoth test otherwise. 
groupdiscrimin (algorithm 1) measur the group discrim- 
inat score with respect to a subset of it input characterist X ′. 
As per definit 3.6, groupdiscrimin fix X ′ to particular 
valu (line 2) to comput what portion of all test with X ′ valu 
fix produc a true output. the while loop (line 5) gener 
random input assign for the remain input characterist 
(line 7), store them in the test suite, and measur the count of 
posit outputs. the algorithm execut the test, if that execut 
be not alreadi cach (line 9); otherwise, the algorithm retriev 
the softwar output from the cach (line 12). after pass the 
minimum sampl threshold (line 16), it check if ϵ error margin 
be achiev with the desir confid (line 18). If it is, groupdis- 
crimin termin the comput for the current group 
and updat the max and min valu (line 20–21). 
causaldiscrimin (algorithm 2) similarli appli test cach 
and adapt sampling. It take a random test k0 (line 4) and test 
if chang ani of it X ′ characterist chang the output. If k0 
result be not cach (line 6), the algorithm execut it and cach the 
result. It then iter through test k that differ from k0 in one or 
more characterist in X ′ (line 11). all gener input be store 
in the test suite. the algorithm typic onli need to examin a 
small number of test befor discov causal discrimin for 
the particular input (line 18). In the end, causaldiscrimin 
return the proport of test for which the algorithm found causal 
discrimin (line 25). 

4.2 sound prune 
measur softwar discrimin (definit 3.9) involv execut- 
ing groupdiscrimin and causaldiscrimin over each 
subset of the input characteristics. the number of these execut 
grow exponenti with the number of characteristics. themi re- 
lie on a power prune optim to dramat reduc the 

algorithm 2: comput causal discrimination. given a soft- 
ware S and a subset of it input characterist X ′, causaldis- 
crimin return d⃗x ′ (s), the causal discrimin score 
with respect to X ′, with confid conf and error margin ϵ . 
causaldiscrimination(s, X ′, conf , ϵ ) 

1 count ← 0; r ← 0, testsuit ← ∅ ▷initi 
2 while r < max_sampl do 
3 r ← r + 1 
4 k0 ← newrandominput ▷new input without valu restrict 
5 testsuit ← testsuit ∪ {k0 } ▷add input to the test suit 
6 if notcach (k0) then ▷no cach execut of k0 exist 
7 compute(s(k0)) ▷evalu softwar on input k0 
8 cacheresult (k0, s(k0)) ▷cach the result 

9 els ▷retriev cach result 
10 s(k0) ← retrievecach (k0) 
11 foreach k ∈ {k | k , k0; ∀χ < X ′, k .χ = k0 .χ } do 

▷all input that match k0 in everi characterist χ < X ′ 

testsuit ← testsuit ∪ {k } ▷add input to the test suit 
1313 if notcach (k ) then ▷no cach execut of k exist 
14 compute(s(k )) ▷evalu softwar on input k 
15 cacheresult (k, s(k )) ▷cach the result 

16 els ▷retriev cach result 
17 s(k ) ← retrievecach (k ) 
18 if s(k ) , s(k0) then ▷causal discrimin 
19 count = count + 1 
20 break 

21 if r > sampling_threshold then 
▷onc we have suffici samples, check error margin 

22 p ← countr ▷current proport of posit output 

23 if conf .zvalu 
√ 
p (1−p ) 

r < ϵ then 
24 break ▷achiev error < ϵ , with confid conf 

25 return testsuite, d⃗x ′ (s) ← p 

number of evalu characterist subsets. prune be base on a 
fundament monoton properti of group and causal discrimina- 
tion: if a softwar S discrimin over threshold θ with respect to 
a set of characterist X ′, then S also discrimin over threshold 
θ with respect to all superset of X ′. onc themi discov that S 
discrimin against X ′, it can prune test all superset of X ′. 

next, we formal prove group (theorem 4.1) and causal (the- 
orem 4.2) discrimin monotonicity. these result guarante 
that themi prune strategi be sound. discriminationsearch 
(algorithm 3) us prune in solv the discrimin check 
problem. As section 5.4 will evalu empirically, prune lead 
to, on average, a two-to-thre order of magnitud reduct in test 
suit size. 

theorem 4.1 (group discrimin monotonicity). letx be 
an input type and let S be a decis softwar with input typex . then 
for all set of characterist X ′,x ′′ ⊆ X , X ′′ ⊇ X ′ =⇒ d̃x ′′ (s) ≥ 
d̃x ′ (s). 

proof. let d̃x ′ (s) = θ ′. recal (definit 3.6) that to comput 
d̃x ′ (s), we partit the space of all input into equival class 

503 



fair testing: test softwar for discrimin esec/fse’17, septemb 4–8, 2017, paderborn, germani 

algorithm 3: discrimin search. given a softwar S with 
input type X and a discrimin threshold θ , discrimina- 
tionsearch identifi all minim subset of characterist 
X ′ ⊆ X such that the (group or causal) discrimin score 
of S with respect to X ′ be great than θ with confid conf 
and error margin ϵ . 
discriminationsearch(s, θ , conf , ϵ ) 

1 D← ∅ ▷initi discrimin subset of characterist 
2 for i = 1 . . . |X | do 
3 foreach X ′ ⊆ X , |X ′ | = i do ▷check subset of size i 
4 discrimin ← fals 
5 for X ′′ ∈ D do 
6 if X ′′ ⊆ X ′ then 

▷superset of a discrimin set discrimin 
(theorem 4.1 and 4.2) 

7 discrimin ← true 
8 break 

9 if discrimin then 
10 continu ▷do not store X ′; D alreadi contain a subset 

11 {testsuite, d } = discrimination(s, X ′, conf , ϵ ) 
▷test S for (group or causal) discrimin with respect to X ′ 

12 if d > θ then ▷X ′ be a minim discrimin set 
13 d.append (X ′) 

such that all element in each equival class have ident 
valu label assign to each characterist in X ′, then, comput 
the frequenc with which input in each equival class lead 
S to a true output, and final comput the differ between 
the minimum and maximum of these frequencies. let p̂′ and p̌′ be 
those maximum and minimum frequencies, and K̂ ′ and Ǩ ′ be the 
correspond equival class of inputs. 

now consid the comput of θ ′′ = d̃x ′′ (s). note that the 
equival class of input for thi comput will be strict 
subset of the equival class in the θ ′ computation. In par- 
ticular, the equival subset K̂ ′ will be split into sever equiv- 
alenc classes, which we call K̂ ′′1 , K̂ 

′′ 
2 , . . . there be two possibil- 

ities: (1) either the frequenc with which the input in each of 
these subclass lead S to a true output equal the frequenc of 
K̂ ′, or (2) some subclass have low frequenc and some have 
high than K̂ ′ (sinc when combined, they must equal that of 
K̂ ′). either way, the maximum frequenc of the K̂ ′′1 , K̂ 

′′ 
2 , . . . , K̂ 

′′ 
j 

subclass be ≥ K̂ ′. and therefore, the maximum overal frequenc 
p̂′′ for all the equival class in the comput of θ ′′ be 
≥ p̂′. By the same argument, the minimum overal frequenc 
p̌′′ for all the equival class in the comput of θ ′′ be 
≤ p̌′. therefore, θ ′′ = (p̂′′ − p̌′′) ≥ (p̂′ − p̌′) ≤= θ ′, and there- 
fore, X ′′ ⊇ X ′ =⇒ d̃x ′′ (s) ≥ d̃x ′ (s). □ 

theorem 4.2 (causal discrimin monotonicity). let X 
be an input type and let S be a decis softwar with input type 
X . then for all set of characterist X ′,x ′′ ⊆ X , X ′′ ⊇ X ′ =⇒ 
d⃗x ′′ (s) ≥ d⃗x ′ (s). 

proof. recal (definit 3.7) that the causal discrimin 
score with respect tox ′ be the fraction of input for which chang 

the valu of at least one characterist in X ′ chang the output. 
consid K ′, the entir set of such input for X ′, and similarli K ′′, 
the entir set of such input for X ′′. sinc X ′′ ⊇ X ′, everi input in 
K ′must also be ink ′′ becaus if chang at least one characterist 
in X ′ chang the output and those characterist be also in X ′′. 
therefore, the fraction of such input must be no small for X ′′ 

than for X ′, and therefore, X ′′ ⊇ X ′ =⇒ d⃗x ′′ (s) ≥ d⃗x ′ (s). □ 

A further opportun for prune come from the relationship 
between group and causal discrimination. As theorem 4.3 shows, 
if softwar group discrimin against a set of characteristics, it 
must causal discrimin against that set at least a much. 

theorem 4.3. let X be an input type and let S be a decis 
softwar with input typex . then for all set of characteristicsx ′ ⊆ X , 
d̃x ′ (s) ≤ d⃗x ′ (s). 

proof. let d̃x ′ (s) = θ . recal (definit 3.6) that to comput 
d̃x ′ (s), we partit the space of all input into equival class 
such that all element in each equival class have ident valu 
label assign to each characterist in X ′. It be evid that same 
equival class input have same valu for characterist in X ′ 
and the one in differ equival class differ in at least one 
of the characterist in X ′. 

now, d̃x ′ (s) = θ mean that for θ fraction of inputs, the output 
be true, and after chang just some valu of X ′ (produc an 
input in anoth equival class), the output be false. thi be 
becaus if there be θ ′ < θ fraction of input with a differ 
output when chang the equival classes, then d̃x ′ (s) would 
have be θ ′. henc d⃗x ′ (s) > θ . □ 

5 evalu 
In evalu themis, we focu on two research questions: 
rq1: doe research on discrimination-awar algorithm design 

(e.g., [18, 40, 88, 91]) produc fair algorithms? 
rq2: how effect do the optim from section 4 make 

themi at identifi discrimin in software? 
To answer these research questions, we carri out three exper- 

iment on twenti instanc of eight softwar system that make 
financi decisions.1 seven of the eight system (seventeen out of 
the twenti instances) be write by origin system developers; we 
reimplement one of the system (three instances) becaus origi- 
nal sourc code be not available. eight of these softwar instanc 
use standard machin learn algorithm to infer model from 
dataset of financi and demograph data. these system make 
no attempt to avoid discrimination. the other twelv instanc 
be take from relat work on devis discrimination-awar al- 
gorithm [18, 40, 88, 91]. these softwar instanc use the same 
dataset and attempt to infer discrimination-fre solutions. four of 
them focu on not discrimin against race, and eight against 
gender. section 5.1 describ our subject system and the two 
dataset they use. 

1we use the term system instanc to mean instanti of a softwar system with 
a configuration, use a specif dataset. two instanc of the same system use 
differ configur and differ data be like to differ significantli in their 
behavior and in their discrimin profile. 

504 



esec/fse’17, septemb 4–8, 2017, paderborn, germani sainyam galhotra, yuriy brun, and alexandra meliou 

5.1 subject softwar system 
our twenti subject instanc use two financi datasets. the adult 
dataset (also know a the censu incom dataset)2 contain fi- 
nancial and demograph data for 45k individuals; each individ- 
ual be describ by 14 attributes, such a occupation, number of 
work hours, capit gain and losses, educ level, gender, race, 
marit status, age, countri of birth, income, etc. thi dataset be 
well vetted: it have be use by other to devis discrimination- 
free algorithm [18, 88, 89], a well a for non-discrimin pur- 
pose [3, 43, 86]. the statlog german credit dataset3 contain 
credit data for 1,000 individuals, classifi each individu a hav- 
ing “good” or “bad” credit, and includ 20 other piec of data 
for each individual, such a gender, hous arrangement, credit 
history, year employed, credit amount, etc. thi dataset be also well 
vetted: it have be use by other to devis discrimination-fre algo- 
rithm [18, 89], a well a for non-discrimin purpos [25, 31]. 

We use a three-paramet name scheme to refer to our softwar 
instances. An exampl of an instanc name be A censusrac . the “a” 
refer to the system use to gener the instanc (describ next). 
the “census” refer to the dataset use for the system instance. thi 
valu can be “census” for the adult censu incom dataset or “credit” 
for the statlog german credit dataset. finally, the “race” refer to a 
characterist the softwar instanc attempt to not discrimin 
against. In our evaluation, thi valu can be “race” or “gender” for 
the censu dataset and can onli be “gender” for the credit dataset.4 
some of the system make no attempt to avoid discrimin and 
their name leav thi part of the label blank. 

prior research have attempt to build discrimination-fre sys- 
tem use these two dataset [18, 40, 88, 91]. We contact the 
author and obtain the sourc code for three of these four sys- 
tems, A [88], C [18], and D [91], and reimplement the other, 
B [40], ourselves. We verifi that our reimplement pro- 
duce result consist with the the evalu of the origin 
system [40]. We addit use standard machin learn li- 
brari a four more discrimination-unawar softwar system E , 
F , G , and H , on these datasets. We use scikit-learn [68] for E 
(naiv bayes), G (logist regression), and H (support vector ma- 
chines), and a publicli avail decis tree implement [90] 
for F and for our reimplement of B . 

the four discrimination-fre softwar system use differentmeth- 
od to attempt to reduc or elimin discrimination: 

A be a modifi logist regress approach that constrain the 
regression’ loss function with the covari of the characteristics’ 
distribut that the algorithm be ask to be fair with respect 
to [88]. 

B be a modifi decis tree approach that constrain the split- 
ting criterion by the output characterist (a the standard decis 
tree approach does) and also the characterist that the algorithm 
be ask to be fair with respect to [40]. 

C manipul the train dataset for a naiv bay classi- 
fier. the approach balanc the dataset to equat the number of 

2https://archive.ics.uci.edu/ml/datasets/adult 
3https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data) 
4the credit dataset combin marit statu and gender into a singl characteristic; we 
refer to it simpli a gender in thi paper for exposition. the credit dataset do not 
includ race. 

input that lead to each output value, and then tweak the dataset 
by introduc nois of flip output for some inputs, and by 
introduc weight for each datapoint [18]. 

D be amodifi decis tree approach that balanc the train 
dataset to equat the number of input that lead to each output 
value, remov and repeat some inputs, and flip output valu of 
input close to the decis tree’ decis boundaries, introduc 
nois around the critic boundari [91]. 
configur the subject systems. the incom characterist of 
the censu dataset be binary, repres the incom be abov 
or below $50,000. most prior research develop system that use 
the other characterist to predict the income; we do the same. 
for the credit dataset, the system predict if the individual’ credit 
be “good” or “bad”. We train each system, separ on the cen- 
su and credit datasets. thus, for example, the G credit instanc be 
the logistic-regression-bas system train on the credit dataset. 
for the censu dataset, we randomli sampl 15.5k individu to 
balanc the number who make more than and less than $50,000, 
and train each system on the sampl subset use 13 character- 
istic to classifi each individu a either have abov or below 
$50,000 income. for the credit dataset, we similarli randomli sam- 
plead 600 individu to balanc the number with “good” and “bad” 
credit, and train each system on the sampl subset use the 20 
characterist to classifi each individual’ credit a “good” or “bad”. 

each discrimination-awar system can be train to avoid dis- 
crimin against sensit characteristics. In accord with the 
prior work on build these system [18, 40, 88, 91], we chose gen- 
der and race a sensit characteristics. use all configur 
exactli a describ in the prior work, we creat 3 instanc of 
each discrimination-awar system. for example, for system A , we 
have A censusgend and A 

censu 
race , two instanc train on the censu 

data to avoid discrimin on gender and race, respectively, and 
A creditgender, an instanc train on the credit data to avoid discrimina- 
tion on gender. the left column of figur 1 list the twenti system 
instanc we use a subjects. 

5.2 race and gender discrimin 
We use themi to measur the group and causal discrimin 
score for our twenti softwar instanc with respect to race and, 
separately, with respect to gender. figur 1 present the results. We 
make the follow observations: 
• themi be effective. themi be abl to (1) verifi that mani 

of the softwar instanc do not discrimin against race and 
gender, and (2) identifi the softwar that does. 

• discrimin be present even in instanc design to 
avoid discrimination. for example, a discrimination-awar 
decis tree approach train not to discrimin against gen- 
der, B censusgender, have a causal discrimin score over 11%: more 
than 11% of the individu have the output flip just by alter- 
ing the individual’ gender. 

• the causal discrimin score detect critic evid 
of discrimin miss by the group score. often, the 
group and causal discrimin score convey the same in- 
formation, but there be case in which themi detect causal 
discrimin even though the group discrimin score be 

505 

https://archive.ics.uci.edu/ml/datasets/adult 
https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data) 


fair testing: test softwar for discrimin esec/fse’17, septemb 4–8, 2017, paderborn, germani 

system race gender 
instanc group causal group causal 

A creditgend — 3.78% 3.98% 
A censusgend 2.10% 2.25% 3.80% 3.80% 
A censusrac 2.10% 1.13% 8.90% 7.20% 
B creditgend — 0.80% 2.30% 
B censusgend 36.55% 38.40% 0.52% 11.27% 
B censusrac 2.28% 1.78% 5.84% 5.80% 
C creditgend — 0.35% 0.18% 
C censusgend 2.43% 2.91% < 0.01% < 0.01% 
C censusrac 0.08% 0.08% 35.20% 34.50% 
D creditgend — 0.23% 0.29% 
D censusgend 0.21% 0.26% < 0.01% < 0.01% 
D censusrac 0.12% 0.13% 4.64% 4.94% 
E credit — 0.32% 0.37% 
E censu 0.74% 0.85% 0.26% 0.32% 
F credit — 0.05% 0.06% 
F censu 0.11% 0.05% < 0.01% < 0.01% 
G credit — 3.94% 2.41% 
G censu 0.02% 2.80% < 0.01% < 0.01% 
H credit — < 0.01% < 0.01% 
H censu < 0.01% 0.01% < 0.01% < 0.01% 

figur 1: the group and causal discrimin score with respect 
to race and gender. some number be miss becaus the credit 
dataset do not contain inform on race. 

low. for example, for B censusgender, the causal score be more than 
21× high than the group score (11.27% vs. 0.52%). 

• today’ discrimination-awar approach be insufficient. 
the B approach be design to avoid a variant of group dis- 
crimin (a be other discrimination-awar approaches), but 
thi design is, at least in some conditions, insuffici to prevent 
causal discrimination. further, focu on avoid discrim- 
inat against one characterist may creat discrimin 
against another, e.g., B censusgend limit discrimin against gen- 
der but discrimin against race with a causal score of 38.40%. 

• there be no clear evid that discrimination-awaremeth- 
od outperform discrimination-unawar ones. In fact, the 
discrimination-unawar approach typic discrimin less 
than their discrimination-awar counterparts, with the excep- 
tion of logist regression. 

5.3 comput discriminated-against 
characterist 

To evalu how effect themi be at comput the discrimin 
check problem (definit 3.9), we use themi to comput the 
set of characterist each of the twenti softwar instanc discrim- 
inat against causally. for each instance, we first use a threshold 
of 75% to find all subset of characterist against which the in- 
stanc discriminated. We next examin the discrimin with 

A censusrac d⃗ {д,r } = 13.7% C censusgend d⃗ {m } = 35.2% 

B censusgend d⃗ {д,m,r } = 77.2% D 
censu 
gender d⃗ {m } = 12.9% 

d⃗ {д,r } = 52.5% d⃗ {c } = 7.6% 
d⃗ {д } = 11.2% D censusrac d⃗ {m } = 16.2% 
d⃗ {m } = 36.1% d⃗ {m,r } = 52.3% 
d⃗ {r } = 36.6% E censu d⃗ {m } = 7.9% 

B censusrac d⃗ {д } = 5.8% F censu d⃗ {c,r } = 98.1% 
C creditgend d⃗ {a } = 7.6% d⃗ {r,e } = 76.3% 

C censusrac d⃗ {a } = 25.9% G censu d⃗ {e } = 14.8% 
d⃗ {д } = 35.2% 

d⃗ {д,r } = 41.5% 

figur 2: set of sensit characterist that the subject instanc 
discrimin against causal at least 5% and that contribut to sub- 
set of characterist that be discrimin against at least 75%. 
We abbrevi sensit characterist as: (a)ge, (c)ountry, (g)ender, 
(m)arit status, (r)ace, and r(e)lation. 

respect to each of the characterist in those set individually. finally, 
we check the causal discrimin score for pair of those char- 
acterist that be sensitive, a defin by prior work [18, 40, 88, 91] 
(e.g., race, age, marit status, etc.). for example, if themi found 
that an instanc discrimin causal against {capit gains, race, 
marit status}, we check the causal discrimin score for 
{capit gains}, {race}, {marit status}, and then {race, marit sta- 
tus}. figur 2 report which sensit characterist each instanc 
discrimin against by at least 5%. 

themi be abl to discov signific discrimination. for ex- 
ample, B censusgend discrimin against gender, marit status, and 
race with a causal score of 77.2%. that mean for 77.2% of the indi- 
viduals, chang onli the gender, marit status, or race caus the 
output of the algorithm to flip. even worse, F censu discrimin 
against countri and race with a causal score of 98.1%. 

It be possibl to build an algorithm that appear to be fair with 
respect to a characterist in general, but discrimin heavili 
against that characterist when the input space be partit by 
anoth characteristic. for example, an algorithm may give the 
same fraction of white and black individu loans, but discrimin 
against black canadian individu a compar to white canadian 
individuals. thi be the case with B censusgender, for example, a it 
causal discrimin score against gender be 11.2%, but against 
gender, marit status, and race be 77.2%. prior work on fair have 
not consid thi phenomenon, and these find suggest that 
the softwar design to produc fair result sometim achiev 
fair at the global scale by creat sever discrimin for 
certain group of inputs. 

thi experi demonstr that themi effect discov- 
er discrimin and can test softwar for unexpect softwar 
discrimin effect across a wide varieti of input partitions. 

5.4 themi effici and the prune effect 
themi us prune to minim test suit (section 4.2). We eval- 
uat the effici improv due to prune by compar 
the number of test case need to achiev the same confid 

506 



esec/fse’17, septemb 4–8, 2017, paderborn, germani sainyam galhotra, yuriy brun, and alexandra meliou 

group causal 

A creditgend 
934,230 
167,420 = 5.6× 

29,623 
15,764 = 1.9× 

A censusrac 
7,033,150 

672 = 10, 466 × 
215,000 

457 = 470 × 
B creditgend 

934,230 
3,413 = 274 × 

29,623 
1,636 = 18 × 

B censusrac 
6,230,000 

80,100 = 78 × 
20,500 
6,300 = 33 × 

C creditgend 
934,230 

113 = 8, 268 × 
29,623 

72 = 411 × 
C censusrac 

7,730,120 
75,140 = 103 × 

235,625 
6,600 = 36 × 

D creditgend 
934,230 

720 = 1, 298 × 
29,623 

472 = 63 × 
D censusrac 

7,730,120 
6,600,462 = 1.2× 

235,625 
200,528 = 1.2× 

E credit 934,230145 = 6, 443 × 
29,623 

82 = 361 × 
E censu 7,730,12084,040 = 92 × 

235,625 
7,900 = 30 × 

F credit 934,2303,410 = 274 × 
29,623 
2,647 = 11 × 

F censu 6,123,000461 = 13, 282 × 
205,000 

279 = 735 × 
G credit 934,230187 = 4, 996 × 

29,623 
152 = 195 × 

G censu 7,730,1205,160,125 = 1.5× 
235,625 
190,725 = 1.2× 

H credit 934,230412,020 = 2.3× 
29,623 
10,140 = 2.9× 

H censu 1,530,0001,213,500 = 1.3× 
510,000 
324,582 = 1.6× 

arithmet mean 2, 849 × 148 × 
geometr mean 151 × 26.4× 

figur 3: prune greatli reduc the number of test need to 
comput both group and causal discrimination. We present here 
the comput that be need for the experi of figur 2: find- 
ing all subset of characterist for which the softwar instanc 
discrimin with a score of at least 75%, for a 99% confid and 
error margin 0.05. for each technique, we show the number of test 
neededwithout prune divid by the number of test neededwith 
pruning, and the result factor reduct in the number of tests. 
for example, reduc the number of test need to comput the 
group discrimin score from 7,033,150 to 672 (2nd row) be an im- 
provement of a factor of 10,466. 

and error bound with and without pruning. figur 3 show the 
number of test case need for each of the twenti softwar in- 
stanc to achiev a confid level of 99% and 0.05 error bound, 
with and without pruning. prune reduc the number of test 
case by, on average, a factor of 2, 849 for group and 148 for causal 
discrimination. 

the more a system discriminates, the more effect prune 
is, make themi more effici becaus prune happen when 
small set of characterist discrimin abov the chosen threshold. 
such set enabl prune away larg superset of characteristics. 
theorem 5.1 formal thi statement. 

theorem 5.1 (prune monotonicity). let X be an input type 
and S and S′ be decis softwar with input typesx . If for allx ′ ⊆ X , 
d⃗x ′ (s) ≥ d⃗x ′ (s′) (respectively, d̃x ′ (s) ≥ d̃x ′ (s′)), then for allx ′′ ⊆ 
X , if themi can prunex ′′when computingdiscriminationsearch(s′, 
θ , conf , ϵ), then it can also prune X ′′ when comput discrimina- 
tionsearch(s,θ , conf , ϵ). 

proof. for themi to prune X ′′ when comput discrimina- 
tionsearch(s′, θ , conf , ϵ), there must exist a set X̂ ′′ ⊊ X ′′ such 
that d⃗x̂ ′′ (S 

′) ≥ θ . sinc d⃗x̂ ′′ (s) ≥ d⃗x̂ ′′ (S 
′) ≥ θ , when comput 

discriminationsearch(s, θ , conf ), themi can also prune X ′′. 
the same argument hold for group discrimin d̃ . □ 

We measur thi effect by measur prune while decreas 
the discrimin threshold θ ; decreas θ effect simul in- 
creas system discrimination. We verifi that prune increas 
when θ decreas (or equivalently, when discrimin increased). 
for example, themi need 3,413 test to find set of characteris- 
tic that B creditgend discrimin with a score of more than θ = 0.7, 
but onli 10 test when we reduc θ to 0.6. similarly, the number 
of test for F credit drop from 920 to 10 when lower θ from 
0.6 to 0.5. thi confirm that themi be more effici when the ben- 
efit of fair test increas becaus the softwar discrimin 
more. 

5.5 discuss 
In answer our two research questions, we found that (1) state- 
of-the-art approach for design fair system often miss dis- 
crimin and themi can detect such discrimin via fair 
testing. (2) themi be effect at find both group and causal 
discrimination. while we do not evalu thi directly, themi 
can also measur appar discrimin (definit 3.8) via a 
developer-provid test suit or oper profile. (3) themi 
employ provabl sound prune to reduc test suit size and be- 
come more effect for system that discrimin more. overall, 
prune reduc test suit sizes, on average, two to three order of 
magnitude. 

6 relatedwork 
softwar discrimin be a grow concern. discrimin 
show up in mani softwar applications, e.g., advertis [75], 
hotel book [53], and imag search [45]. yet softwar be enter 
domain in which discrimin could result in seriou neg 
consequences, includ crimin justic [5, 28], financ [62], and 
hire [71]. softwar discrimin may occur unintentionally, 
e.g., a a result of implement bugs, a an unintend properti 
of self-organ system [11, 13, 15, 16], a an emerg properti 
of compon interact [12, 14, 17, 49], or a an automat 
learn properti from bia data [18, 19, 39–42, 88, 89, 91]. 

some prior work on measur fair in machin learn 
classifi have focu on the calders-verw (cv) score [19] to 
measur discrimin [18, 19, 39–42, 87–89, 91]. our group dis- 
crimin score gener the CV score to the softwar domain 
with more complex inputs. our causal discrimin score go 
beyond prior work by measur causal [67]. An altern defi- 
nition of discrimin be that a “better” input be never depriv of 
the “better” output [24]. that definit requir a domain expert 
to creat a distanc function for compar inputs; by contrast, our 
definit be simpler, more gener applicable, and amen 
to optim techniques, such a pruning. reduc discrimi- 
nation (cv score) in classifi [18, 40, 88, 91], a our evalu 
have shown, often fail to remov causal discrimin and discrim- 
inat against certain groups. By contrast, our work do not 
attempt to remov discrimin but offer develop a tool to 
identifi and measur discrimination, a critic first step in remov 
it. problem-specif discrimin measures, e.g., the contextu 

507 



fair testing: test softwar for discrimin esec/fse’17, septemb 4–8, 2017, paderborn, germani 

bandit problem, have demonstr that fair may result in 
otherwis suboptim behavior [38]. By contrast, our work be gen- 
eral and we believ that strive for fair may be a princip 
requir in a system’ design. 

counterfactu fair requir output probabl distribut 
to match for input popul that differ onli in the label valu of a 
sensit input characterist [51]. counterfactu fair be relat 
to causal fair but can miss some instanc of discrimination, 
e.g., if loan show preferenti treatment for some purpl inputs, 
but at the same time against some other similar purpl inputs. 

fairtest, an implement of the unwarr associ 
framework [79] us manual write test to measur four kind 
of discrimin scores: the CV score and a relat ratio, mutual 
information, pearson correlation, and a regress between the 
output and sensit inputs. By contrast, our approach gener 
test automat and measur causal discrimination. 

causal test comput pair of similar input whose output 
differ. however, input characterist may correlate, e.g., educ 
correl with age, so perturb some characterist without 
perturb other may creat input not repres of the real 
world. fairml [1] us orthogon project to co-perturb charac- 
teristics, which can mask some discrimination, but find discrimi- 
nation that be more like to be observ in real-world scenarios, 
somewhat analog to our appar discrimin measure. 

combinatori test minim the number of test need 
to explor certain combin of input characteristics. for ex- 
ample, all-pair test gener test that evalu everi pos- 
sibl valu combin for everi pair of input characteristics, 
which can be particularli help when test softwar product 
line [6, 44, 46, 47]. the number of test need to evalu everi 
possibl valu pair can be significantli small than the exhaust 
test altern sinc each test can simultan contribut to 
multipl valu pair [22, 44, 80]. such combinatori test opti- 
mizat be complementari to our work on discrimin testing. 
our main goal be to develop a method to process test execut to 
measur softwar discrimination, wherea that be not a goal of com- 
binatori testing. advanc in combinatori testing, e.g., use 
static or dynam analysi for vacuiti test [8, 33] or to identifi 
configur option that cannot affect a test’ output [48], can 
directli improv effici of discrimin test by identifi 
that chang a particular input characterist cannot affect a par- 
ticular test’ output, and thu no causal discrimin be possibl 
with respect to that particular input. We leav such optim 
to futur work. 

It be possibl to test for discrimin softwar without explicit 
access to it. for example, adfish [23] collect inform on how 
chang in googl ad set and prior visit webpag affect the 
ad googl serves. adfish comput a variant of group discrimi- 
nation, but it could be integr with themi and it algorithm 
to measur causal discrimination. 

themi measur appar discrimin by either execut 
a provid test suite, or by gener a test suit follow a 
provid oper profile. oper profil [58] describ 
the input distribut like to be observ in the field. becaus 
developer-written test suit be often not a repres of field 
execut a develop would like [81], oper profil can 

significantli improv the effect of test by more accu- 
rate repres real-world system use [7, 50] and the use of 
oper profil have be show to more accur measur 
system properties, such a reliabl [35]. the work on oper 
profil be complementari to ours: themi us oper pro- 
file and work on more effici test gener from oper 
profil can directli benefit discrimin testing. meanwhil no 
prior work on oper profil test have measur softwar 
discrimination. 

causal relationship in data manag system [54, 55] can 
help explain queri result [57] and debug error [82–84] by track 
and use data proven [56]. for softwar system that use data 
management, such provenance-bas reason may aid test 
for causal relationship between input characterist and outputs. 
our prior work on test softwar that reli on data manag 
system have focu on data error [59, 60], wherea thi work 
focu on test fairness. 

autom test research have produc tool to gener tests, 
includ random testing, such a randoop [63, 64], nighthawk [4], 
jcrasher [20], carfast [65], and T3 [69]; search-bas testing, 
such a evosuit [29], test [9], and etoc [78]; dynam sym- 
bolic execut tools, such a dsc [37], symbol pathfind [66], 
jcute [70], seeker [76], symstra [85], and pex [77], among others; 
and commerci tools, such a agitar [2]. the goal of the gener 
test be typic find bug [29] or gener specif [21]. 
these tool deal with more complex input space than themis, but 
none of them focu on test fair and they requir oracl 
wherea themi do not need oracl a it measur discrimin 
by compar tests’ outputs. futur work could extend these tool 
to gener fair tests, modifi test gener to produc 
pair of input that differ onli in the input characterist be 
tested. while prior work have tackl the oracl problem [10, 26, 30] 
typic use infer pre- and post-condit or documenta- 
tion, our oracl be more precis and easi to compute, but be onli 
applic to fair testing. 

7 contribut 
We have formal defin softwar fair test and introduc 
a causality-bas measur of discrimination. We have further de- 
scribe themis, an approach and it open-sourc implement 
for measur discrimin in softwar and for gener effi- 
cient test suit to perform these measurements. our evalu 
demonstr that discrimin in softwar be common, even 
when fair be an explicit design goal, and that fair test 
be critic to measur discrimination. further, we formal prove 
sound of our approach and show that themi effect mea- 
sure discrimin and produc effici test suit to do so. 
with the current use of softwar in society-crit ways, fair 
test research be becom paramount, and our work present an 
import first step in merg test techniqu with softwar 
fair requirements. 

acknowledg 
thi work be support by the nation scienc foundat under 
grant no. ccf-1453474, iis-1453543, and cns-1744471. 

508 



esec/fse’17, septemb 4–8, 2017, paderborn, germani sainyam galhotra, yuriy brun, and alexandra meliou 

refer 
[1] juliu adebayo and lalana kagal. iter orthogon featur project for 

diagnos bia in black-box models. corr, abs/1611.04967, 2016. 
[2] agitar technologies. agitarone. http://www.agitar.com/solutions/products/ 

automated_junit_generation.html, 2016. 
[3] rakesh agrawal, ramakrishnan srikant, and dili thomas. privacy-preserv 

olap. In acm sigmod intern confer on manag of data (sig- 
mod), page 251–262, baltimore, md, usa, 2005. 

[4] jame H. andrews, felix C. H. li, and tim menzies. nighthawk: A two-level 
genetic-random unit test data generator. In intern confer on autom 
softwar engin (ase), page 144–153, atlanta, ga, usa, 2007. 

[5] julia angwin, jeff larson, surya mattu, and lauren kirchner. ma- 
chine bias. propublica, may 23, 2016. https://www.propublica.org/article/ 
machine-bias-risk-assessments-in-criminal-sentencing. 

[6] sven apel, alexand von rhein, philipp wendler, armin größlinger, and dirk 
beyer. strategi for product-lin verification: case studi and experiments. 
In intern confer on softwar engin (icse), page 482–491, san 
francisco, ca, usa, 2013. 

[7] andrea arcuri and lionel briand. A practic guid for use statist test to 
ass random algorithm in softwar engineering. inacm/iee intern 
confer on softwar engin (icse), page 1–10, honolulu, hi, usa, 2011. 

[8] thoma ball and orna kupferman. vacuiti in testing. In intern confer 
on test and proof (tap), page 4–17, prato, italy, april 2008. 

[9] luciano baresi, pier luca lanzi, and matteo miraz. testful: An evolutionari test 
approach for java. In intern confer on softwar testing, verification, 
and valid (icst), page 185–194, paris, france, april 2010. 

[10] earl T. barr, mark harman, phil mcminn, muzammil shahbaz, and shin yoo. 
the oracl problem in softwar testing: A survey. ieee transact on softwar 
engin (tse), 41(5):507–525, may 2015. 

[11] yuriy brun, ron desmarais, kurt geihs, marin litoiu, antonia lopes, mari 
shaw, and mike smit. A design space for adapt systems. In rogério de lemos, 
holger giese, hausi A. müller, and mari shaw, editors, softwar engin for 
self-adapt system ii, volum 7475, page 33–50. springer-verlag, 2013. 

[12] yuriy brun, georg edwards, jae young bang, and nenad medvidovic. smart re- 
dundanc for distribut computation. In intern confer on distribut 
comput system (icdcs), page 665–676, minneapolis, mn, usa, june 2011. 

[13] yuriy brun and nenad medvidovic. An architectur style for solv compu- 
tation intens problem on larg networks. In softwar engin for 
adapt and self-manag system (seams), minneapolis, mn, usa, may 2007. 

[14] yuriy brun and nenad medvidovic. fault and adversari toler a an emerg 
properti of distribut systems’ softwar architectures. In internationalworkshop 
on engin fault toler system (efts), page 38–43, dubrovnik, croatia, 
septemb 2007. 

[15] yuriy brun and nenad medvidovic. keep data privat while comput in the 
cloud. In intern confer on cloud comput (cloud), page 285–294, 
honolulu, hi, usa, june 2012. 

[16] yuriy brun and nenad medvidovic. entrust privat comput and data 
to untrust networks. ieee transact on depend and secur comput 
(tdsc), 10(4):225–238, july/august 2013. 

[17] yuriy brun, jae young bang, georg edwards, and nenad medvidovic. self- 
adapt reliabl in distribut softwar systems. ieee transact on softwar 
engin (tse), 41(8):764–780, august 2015. 

[18] toon calders, faisal kamiran, and mykola pechenizkiy. build classifi with 
independ constraints. In proceed of the 2009 ieee intern confer 
on data mine (icdm) workshops, page 13–18, miami, fl, usa, decemb 2009. 

[19] toon calder and sicco verwer. three naiv bay approach for discrimination- 
free classification. data mine and knowledg discovery, 21(2):277–292, 2010. 

[20] christoph csallner and yanni smaragdakis. jcrasher: An automat robust 
tester for java. softwar practic and experience, 34(11):1025–1050, septemb 
2004. 

[21] valentin dallmeier, nikolai knopp, christoph mallon, gordon fraser, sebastian 
hack, and andrea zeller. automat gener test case for specif 
mining. ieee transact on softwar engin (tse), 38(2):243–257, march 
2012. 

[22] marcelo d’amorim, steven lauterburg, and darko marinov. delta execut for 
effici state-spac explor of object-ori programs. In intern 
symposium on softwar test and analysi (issta), page 50–60, london, uk, 
2007. 

[23] amit datta, michael carl tschantz, and anupam datta. autom experi 
on ad privaci settings. proceed on privaci enhanc technolog (popets), 
1:92–112, 2015. 

[24] cynthia dwork, moritz hardt, toniann pitassi, omer reingold, and richard 
zemel. fair through awareness. In innov in theoret comput 
scienc confer (itcs), page 214–226, cambridge, ma, usa, august 2012. 

[25] jeroen eggermont, joost N. kok, and walter A. kosters. genet program 
for data classification: partit the search space. In acm symposium on 
appli comput (sac), page 1001–1005, nicosia, cyprus, 2004. 

[26] michael D. ernst, alberto goffi, alessandra gorla, and mauro pezzè. automat 
gener of oracl for except behaviors. In intern symposium on 
softwar test and analysi (issta), page 213–224, saarbrücken, genmany, 
juli 2016. 

[27] execut offic of the president. big data: A report on algorithm systems, 
opportunity, and civil rights. https://www.whitehouse.gov/sites/default/files/ 
microsites/ostp/2016_0504_data_discrimination.pdf, may 2016. 

[28] katelyn ferral. wisconsin suprem court allow state to continu us- 
ing comput program to assist in sentencing. the capit times, 
juli 13, 2016. http://host.madison.com/ct/news/local/govt-and-politics/ 
wisconsin-supreme-court-allows-state-to-continue-using-computer-program/ 
article_7eb67874-bf40-59e3-b62a-923d1626fa0f.html. 

[29] gordon fraser and andrea arcuri. whole test suit generation. ieee transact 
on softwar engin (tse), 39(2):276–291, februari 2013. 

[30] gordon fraser and andrea zeller. gener parameter unit tests. In 
intern symposium on softwar test and analysi (issta), page 364–374, 
toronto, on, canada, 2011. 

[31] jesu A. gonzalez, lawrenc B. holder, and dian J. cook. graph-bas relat 
concept learning. In intern confer on machin learn (icml), page 
219–226, sydney, australia, 2002. 

[32] noah J. goodall. can you program ethic into a self-driv car? ieee spectrum, 
53(6):28–58, june 2016. 

[33] luigi Di guglielmo, franco fummi, and graziano pravadelli. vacuiti analysi 
for properti qualif by mutat of checkers. In design, autom test in 
europ confer exhibit (date), page 478–483, march 2010. 

[34] erico guizzo and evan ackerman. when robot decid to kill. ieee spectrum, 
53(6):38–43, june 2016. 

[35] herman hartmann. A statist analysi of oper profil driven testing. In 
intern confer on softwar quality, reliability, and secur companion 
(qrs-c), page 109–116, viena, austria, august 2016. 

[36] david ingold and spencer soper. amazon doesn’t consid the race of it cus- 
tomers. should it? bloomberg, april 21, 2016. http://www.bloomberg.com/ 
graphics/2016-amazon-same-day. 

[37] mainul islam and christoph csallner. dsc+mock: A test case + mock class 
gener in support of cod against interfaces. In intern workshop on 
dynam analysi (woda), page 26–31, trento, italy, 2010. 

[38] matthew joseph, michael kearns, jami morgenstern, and aaron roth. fair 
in learning: classic and contextu bandits. corr, abs/1207.0016, 2012. https: 
//arxiv.org/abs/1605.07139. 

[39] faisal kamiran and toon calders. classifi without discriminating. In inter- 
nation confer on computer, control, and commun (ic4), page 1–6, 
karachi, pakistan, februari 2009. 

[40] faisal kamiran, toon calders, and mykola pechenizkiy. discrimin awar 
decis tree learning. In intern confer on data mine (icdm), page 
869–874, sydney, australia, decemb 2010. 

[41] faisal kamiran, asim karim, and xiangliang zhang. decis theori for 
discrimination-awar classification. In intern confer on data mine 
(icdm), page 924–929, brussels, belgium, decemb 2012. 

[42] toshihiro kamishima, shotaro akaho, hideki asoh, and jun sakuma. fairness- 
awar classifi with prejudic remov regularizer. In joint european confer 
on machin learn and knowledg discoveri in databas (ecml pkdd), page 
35–50, bristol, england, uk, septemb 2012. 

[43] wei-chun kao, kai-min chung, chia-liang sun, and chih-jen lin. decomposi- 
tion method for linear support vector machines. neural computation, 16(8):1689– 
1704, august 2004. 

[44] christian kästner, alexand von rhein, sebastian erdweg, jona pusch, sven 
apel, tillmann rendel, and klau ostermann. toward variability-awar testing. 
In intern workshop on feature-ori softwar develop (fosd), 
page 1–8, dresden, germany, 2012. 

[45] matthew kay, cynthia matuszek, and sean A. munson. unequ represent 
and gender stereotyp in imag search result for occupations. In confer on 
human factor in comput system (chi), page 3819–3828, seoul, republ of 
korea, 2015. 

[46] chang hwan peter kim, don S. batory, and sarfraz khurshid. reduc combi- 
nator in test product lines. In intern confer on aspect-ori 
softwar develop (aost), page 57–68, porto de galinhas, brazil, 2011. 

[47] chang hwan peter kim, sarfraz khurshid, and don batory. share execut 
for effici test product lines. In intern symposium on softwar 
reliabl engin (issre), page 221–230, novemb 2012. 

[48] chang hwan peter kim, darko marinov, sarfraz khurshid, don batory, sabrina 
souto, paulo barros, and marcelo d’amorim. splat: lightweight dynam 
analysi for reduc combinator in test configur systems. In european 
softwar engin confer and acm sigsoft intern symposium on 
foundat of softwar engin (esec/fse), page 257–267, saint petersburg, 
russia, 2013. 

[49] ivo krka, yuriy brun, georg edwards, and nenad medvidovic. synthes 
partial component-level behavior model from system specifications. In european 
softwar engin confer and acm sigsoft intern symposium on 
foundat of softwar engin (esec/fse), page 305–314, amsterdam, the 

509 

http://www.agitar.com/solutions/products/automated_junit_generation.html 
http://www.agitar.com/solutions/products/automated_junit_generation.html 
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentenc 
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentenc 
https://www.whitehouse.gov/sites/default/files/microsites/ostp/2016_0504_data_discrimination.pdf 
https://www.whitehouse.gov/sites/default/files/microsites/ostp/2016_0504_data_discrimination.pdf 
http://host.madison.com/ct/news/local/govt-and-politics/wisconsin-supreme-court-allows-state-to-continue-using-computer-program/article_7eb67874-bf40-59e3-b62a-923d1626fa0f.html 
http://host.madison.com/ct/news/local/govt-and-politics/wisconsin-supreme-court-allows-state-to-continue-using-computer-program/article_7eb67874-bf40-59e3-b62a-923d1626fa0f.html 
http://host.madison.com/ct/news/local/govt-and-politics/wisconsin-supreme-court-allows-state-to-continue-using-computer-program/article_7eb67874-bf40-59e3-b62a-923d1626fa0f.html 
http://www.bloomberg.com/graphics/2016-amazon-same-day 
http://www.bloomberg.com/graphics/2016-amazon-same-day 
https://arxiv.org/abs/1605.07139 
https://arxiv.org/abs/1605.07139 


fair testing: test softwar for discrimin esec/fse’17, septemb 4–8, 2017, paderborn, germani 

netherlands, august 2009. 
[50] K. saravana kumar and ravindra babu misra. softwar oper profil base 

test case alloc use fuzzi logic. intern journal of autom and 
computing, 4(4):388–395, 2007. 

[51] matt J. kusner, joshua R. loftus, chri russell, and ricardo silva. counterfactu 
fairness. corr, abs/1703.06856, 2017. 

[52] rafi letzter. amazon just show u that ‘unbiased’ algorithm can be in- 
advert racist. tech insider, april 21, 2016. http://www.techinsider.io/ 
how-algorithms-can-be-racist-2016-4. 

[53] dana mattioli. On orbitz, mac user steer to pricier hotels. the 
wall street journal, august 23, 2012. http://www.wsj.com/articles/ 
sb10001424052702304458604577488822667325882. 

[54] alexandra meliou, wolfgang gatterbauer, joseph Y. halpern, christoph koch, 
katherin F. moore, and dan suciu. causal in databases. ieee data engin 
bulletin, 33(3):59–67, 2010. 

[55] alexandra meliou, wolfgang gatterbauer, katherin F. moore, and dan su- 
ciu. the complex of causal and respons for queri answer and non- 
answers. proceed of the vldb endow (pvldb), 4(1):34–45, 2010. 

[56] alexandra meliou, wolfgang gatterbauer, and dan suciu. bring proven 
to it full potenti use causal reasoning. In 3rd usenixworkshop on the theori 
and practic of proven (tapp), heraklion, greece, june 2011. 

[57] alexandra meliou, sudeepa roy, and dan suciu. causal and explan in 
databases. proceed of the vldb endow (pvldb), 7(13):1715–1716, 2014. 
(tutori presentation). 

[58] john D. musa. oper profil in software-reli engineering. ieee 
software, 10(2):14–32, march 1993. 

[59] kıvanç muşlu, yuriy brun, and alexandra meliou. data debug with continu- 
ou testing. In proceed of the new idea track at the 9th joint meet of the 
european softwar engin confer and acm sigsoft symposium on the 
foundat of softwar engin (esec/fse), page 631–634, saint petersburg, 
russia, august 2013. 

[60] kıvanç muşlu, yuriy brun, and alexandra meliou. prevent data error with 
continu testing. In proceed of the acm sigsoft intern symposium 
on softwar test and analysi (issta), page 373–384, baltimore, md, usa, 
juli 2015. 

[61] satya nadella. the partnership of the future. slate, june 28, 2016. http: 
//www.slate.com/articles/technology/future_tense/2016/06/microsoft_ceo_ 
satya_nadella_humans_and_a_i_can_work_together_to_solve_society.html. 

[62] parmi olson. the algorithm that beat your bank manager. cnn 
money, march 15, 2011. http://www.forbes.com/sites/parmyolson/2011/03/15/ 
the-algorithm-that-beats-your-bank-manager/#cd84e4f77ca8. 

[63] carlo pacheco and michael D. ernst. randoop: feedback-direct random 
test for java. In confer on object-ori program system and 
applic (oopsla), page 815–816, montreal, qc, canada, 2007. 

[64] carlo pacheco, shuvendu K. lahiri, michael D. ernst, and thoma ball. feedback- 
direct random test generation. In acm/iee intern confer on soft- 
ware engin (icse), page 75–84, minneapolis, mn, usa, may 2007. 

[65] sangmin park, B. M. mainul hossain, ishtiaqu hussain, christoph csallner, 
mark grechanik, kunal taneja, chen fu, and qing xie. carfast: achiev 
high statement coverag faster. In symposium on the foundat of softwar 
engin (fse), page 35:1–35:11, cary, nc, usa, 2012. 

[66] corina S. păsăreanu and neha rungta. symbol pathfinder: symbol execut 
of java bytecode. In intern confer on autom softwar engin 
(ase), page 179–180, antwerp, belgium, 2010. 

[67] judea pearl. causal infer in statistics: An overview. statist surveys, 
3:96–146, 2009. 

[68] fabian pedregosa, gaël varoquaux, alexandr gramfort, vincent michel, 
bertrand thirion, olivi grisel, mathieu blondel, peter prettenhofer, ron 
weiss, vincent dubourg, jake vanderplas, alexandr passos, david courna- 
peau, matthieu brucher, matthieu perrot, and édouard duchesnay. scikit-learn: 
machin learn in python. journal of machin learn research, 12:2825–2830, 
2011. 

[69] I. S. wishnu B. prasetya. budget-awar random test with t3: benchmark 
at the sbst2016 test tool contest. In intern workshop on search-bas 
softwar test (sbst), page 29–32, austin, tx, usa, 2016. 

[70] koushik sen and gul agha. cute and jcute: concol unit test and ex- 
plicit path model-check tools. In intern confer on comput aid 
verif (cav), page 419–423, seattle, wa, usa, 2006. 

[71] aarti shahani. now algorithm be decid whom to hire, base on voice. npr 
all thing considered, march 2015. 

[72] spencer soper. amazon to bring same-day deliveri to bronx, chicago after outcry. 
bloomberg, may 1, 2016. http://www.bloomberg.com/news/articles/2016-05-01/ 
amazon-pledges-to-bring-same-day-delivery-to-bronx-after-outcry. 

[73] spencer soper. amazon to bring same-day deliveri to roxburi after outcry. 
bloomberg, april 26, 2016. http://www.bloomberg.com/news/articles/2016-04-26/ 
amazon-to-bring-same-day-delivery-to-roxbury-after-outcry. 

[74] eliza strickland. doc bot prep for the o.r. ieee spectrum, 53(6):32–60, june 
2016. 

[75] latanya sweeney. discrimin in onlin ad delivery. commun of the 
acm (cacm), 56(5):44–54, may 2013. 

[76] suresh thummalapenta, tao xie, nikolai tillmann, jonathan de halleux, and 
zhendong su. synthes method sequenc for high-coverag testing. In 
intern confer on object orient program system languag and 
applic (oopsla), page 189–206, portland, or, usa, 2011. 

[77] nikolai tillmann and jonathan De halleux. pex: white box test gener for 
.net. In intern confer on test and proof (tap), page 134–153, prato, 
italy, 2008. 

[78] paolo tonella. evolutionari test of classes. In intern symposium on 
softwar test and analysi (issta), page 119–128, boston, ma, usa, 2004. 

[79] florian tramer, vagg atlidakis, roxana geambasu, daniel hsu, jean-pierr 
hubaux, mathia humbert, ari juels, , and huang lin. fairtest: discov un- 
warrant associ in data-driven applications. In ieee european symposium 
on secur and privaci (euros&p), paris, france, april 2017. 

[80] alexand von rhein, sven apel, and franco raimondi. introduc binari 
decis diagram in the explicit-st verif of java code. In the java 
pathfind workshop (jpf), lawrence, ks, usa, novemb 2011. 

[81] qianqian wang, yuriy brun, and alessandro orso. behavior execut compar- 
ison: are test repres of field behavior? In intern confer on 
softwar testing, verification, and valid (icst), tokyo, japan, march 2017. 

[82] xiaolan wang, xin luna dong, and alexandra meliou. data x-ray: A diagnost 
tool for data errors. In intern confer on manag of data (sigmod), 
2015. 

[83] xiaolan wang, alexandra meliou, and eugen wu. qfix: demonstr error 
diagnosi in queri histories. In intern confer on manag of data 
(sigmod), page 2177–2180, 2016. (demonstr paper). 

[84] xiaolan wang, alexandra meliou, and eugen wu. qfix: diagnos error 
through queri histories. In intern confer on manag of data 
(sigmod), 2017. 

[85] tao xie, darko marinov, wolfram schulte, and david notkin. symstra: A 
framework for gener object-ori unit test use symbol execution. In 
intern confer on tool and algorithm for the construct and analysi 
of system (tacas), page 365–381, edinburgh, scotland, uk, 2005. 

[86] bianca zadrozny. learn and evalu classifi under sampl select bias. 
In intern confer on machin learn (icml), page 114–121, banff, 
ab, canada, 2004. 

[87] muhammad bilal zafar, isabel valera, manuel gomez rodriguez, and krishna P. 
gummadi. fair constraints: A mechan for fair classification. In fairness, 
accountability, and transpar in machin learn (fat ml), lille, france, 
juli 2015. 

[88] muhammad bilal zafar, isabel valera, manuel gomez rodriguez, and krishna P 
gummadi. learn fair classifiers. corr, abs/1507.05259, 2015. 

[89] richard zemel, Yu (ledell) wu, kevin swersky, toniann pitassi, and cynthia 
dwork. learn fair representations. In intern confer on machin 
learn (icml), publish in jmlr w&cp: 28(3):325–333), atlanta, ga, usa, 
june 2013. 

[90] farhang zia. github repositori for the decis tree classifi project. https: 
//github.com/novaintel/csci4125/tree/master/decision-tree, 2014. 

[91] indr žliobaite, faisal kamiran, and toon calders. handl condit dis- 
crimination. In intern confer on data mine (icdm), page 992–1001, 
vancouver, bc, canada, decemb 2011. 

510 

http://www.techinsider.io/how-algorithms-can-be-racist-2016-4 
http://www.techinsider.io/how-algorithms-can-be-racist-2016-4 
http://www.wsj.com/articles/sb10001424052702304458604577488822667325882 
http://www.wsj.com/articles/sb10001424052702304458604577488822667325882 
http://www.slate.com/articles/technology/future_tense/2016/06/microsoft_ceo_satya_nadella_humans_and_a_i_can_work_together_to_solve_society.html 
http://www.slate.com/articles/technology/future_tense/2016/06/microsoft_ceo_satya_nadella_humans_and_a_i_can_work_together_to_solve_society.html 
http://www.slate.com/articles/technology/future_tense/2016/06/microsoft_ceo_satya_nadella_humans_and_a_i_can_work_together_to_solve_society.html 
http://www.forbes.com/sites/parmyolson/2011/03/15/the-algorithm-that-beats-your-bank-manager/#cd84e4f77ca8 
http://www.forbes.com/sites/parmyolson/2011/03/15/the-algorithm-that-beats-your-bank-manager/#cd84e4f77ca8 
http://www.bloomberg.com/news/articles/2016-05-01/amazon-pledges-to-bring-same-day-delivery-to-bronx-after-outcri 
http://www.bloomberg.com/news/articles/2016-05-01/amazon-pledges-to-bring-same-day-delivery-to-bronx-after-outcri 
http://www.bloomberg.com/news/articles/2016-04-26/amazon-to-bring-same-day-delivery-to-roxbury-after-outcri 
http://www.bloomberg.com/news/articles/2016-04-26/amazon-to-bring-same-day-delivery-to-roxbury-after-outcri 
https://github.com/novaintel/csci4125/tree/master/decision-tre 
https://github.com/novaintel/csci4125/tree/master/decision-tre 

