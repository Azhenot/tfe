




































composite4.dvi 


integr topic and syntax 

thoma L. griffith mark steyver 
gruffydd@mit.edu msteyver@uci.edu 

massachusett institut of technolog univers of california, irvin 
cambridge, MA 02139 irvine, CA 92614 

david M. blei joshua B. tenenbaum 
blei@cs.berkeley.edu jbt@mit.edu 

univers of california, berkeley massachusett institut of technolog 
berkeley, CA 94720 cambridge, MA 02139 

abstract 

statist approach to languag learn typic focu on either 
short-rang syntact depend or long-rang semant depend 
between words. We present a gener model that us both kind of 
dependencies, and be capabl of simultan find syntact class 
and semant topic despit have no knowledg of syntax or seman- 
tic beyond statist dependency. thi model be competit on task 
like part-of-speech tag and document classif with model that 
exclus use short- and long-rang depend respectively. 

1 introduct 

A word can appear in a sentenc for two reasons: becaus it serf a syntact function, or 
becaus it provid semant content. word that play differ role be treat differ 
in human languag processing: function and content word produc differ pattern of 
brain activ [1], and have differ development trend [2]. so, how might a languag 
learner discov the syntact and semant class of words? cognit scientist have 
show that unsupervis statist method can be use to identifi syntact class [3] 
and to extract a represent of semant content [4], but none of these method captur 
the interact between function and content words, or even recogn that these role 
be distinct. here we explor how statist learning, with no prior knowledg of either 
syntax or semantics, can discov the differ between function and content word and 
simultan organ word into syntact class and semant topics. 

our approach reli on the differ kind of depend between word produc by 
syntact and semant constraints. syntact constraint result in rel short-rang de- 
pendencies, span sever word but not go beyond the limit of a sentence. seman- 
tic constraint result in long-rang dependencies: differ sentenc within a document 
be like to have similar content, and use similar words. We present an algorithm that cap- 
ture the interact between short- and long-rang dependencies, base upon a gener 
model for text in which a hidden markov model (hmm) determin when to emit a word 
from a topic model. the differ capac of the two compon of the model result in a 
factor of a sentenc into function words, handl by the hmm, and content words, 
handl by the topic model. each compon divid word into finer group accord 
to a differ criterion: the function word be divid into syntact classes, and the con- 

in: advanc in neural inform process systems, 17 



tent word be divid into semant topics. In addit to produc clean syntact and 
semant class and identifi function and content words, our composit model be com- 
petit in quantit tasks, such a part-of-speech tag and document classification, 
with model special to detect onli one kind of dependency. 

the plan of the paper be a follows. first, we introduc the approach, consid the 
gener question of how syntact and semant gener model might be combined, 
and argu that a composit model be necessari to captur the differ role that word 
can play in a document. We then defin a gener model of thi form, and describ 
a markov chain mont carlo algorithm for infer in thi model. finally, we present 
result illustr the qualiti of the recov syntact class and semant topics. 

2 combin syntact and semant gener model 

A probabilist gener model specifi a simpl stochast procedur by which data 
might be generated, usual make refer to unobserv random variabl that express 
latent structure. onc defined, thi procedur can be invert use statist inference, 
comput distribut over latent variabl condit on a dataset. such an approach be 
appropri for model language, where word be gener from the latent structur of 
the speaker’ intentions, and be wide use in statist natur languag process (e.g., 
[5]). 

probabilist model of languag be typic driven exclus by either short-rang or 
long-rang depend between words. hmm and probabilist context-fre grammar 
(e.g., [5]) gener document pure base on syntact relat among unobserv word 
classes, while “bag-of-words” model like naiv bay or topic model (e.g., [6]) gener 
document base on semant correl between words, independ of word order. By 
consid onli one of the factor influenc the word that appear in documents, these 
approach be forc to ass all word on a singl criterion: an hmm will group noun 
together, a they play the same syntact role even though they vari across contexts, and a 
topic model will assign determin to topics, even though they bear littl semant content. 

A major advantag of gener model be modularity. A gener model for text spec- 
ifi a probabl distribut over word in term of other probabl distribut over 
words, and differ model be thu easili combined. We can produc a model that ex- 
press both the short- and long-rang depend of word by combin two model 
that be each sensit to one kind of dependency. however, the form of combin must 
be chosen carefully. In a mixtur of syntact and semant models, each word would ex- 
hibit either short-rang or long-rang dependencies, while in a product of model (e.g. [7]), 
each word would exhibit both short-rang and long-rang dependencies. consider of 
the structur of languag reveal that neither of these model be appropriate. In fact, onli 
a subset of word – the content word – exhibit long-rang semant dependencies, while 
all word obey short-rang syntact dependencies. thi asymmetri can be captur in a 
composit model, where we replac one of the probabl distribut over word use in 
the syntact model with the semant model. thi allow the syntact model to choos 
when to emit a content word, and the semant model to choos which word to emit. 

2.1 A composit model 

We will explor a simpl composit model, in which the syntact compon be an hmm 
and the semant compon be a topic model. the graphic model for thi composit be 
show in figur 1(a). the model be defin in term of three set of variables: a sequenc 
of word w = {w1, . . . , wn}, with each wi be one of W words, a sequenc of topic 
assign z = {z1, . . . zn}, with each zi be one of T topics, and a sequenc of 
class c = {c1, . . . , cn}, with each ci be one of C classes. one class, say ci = 1, be 
design the “semantic” class. the zth topic be associ with a distribut over word 



network 
neural 

network 
output 

... 

imag 
imag 
object 
object 

... 

support 
vector 
svm 
... 

kernel 

in 
with 
for 
on 
... 

use 
train 

obtain 
describ 

... 

0.5 0.4 0.1 

0.7 

0.9 

0.8 

0.2 

neural network train with svm imag 

imag 

output 

network forus imag 

kernelwithobtain 

describ with object 

s s s s1 42 3 

w w w w1 42 3 

z z z z 

θ 

1 42 3 

(a) (b) 

figur 1: the composit model. (a) graphic model. (b) gener phrases. 

φ(z), each class c 6= 1 be associ with a distribut over word φ(c), each document 
d have a distribut over topic θ(d), and transit between class ci−1 and ci follow a 
distribut π(si−1). A document be gener via the follow procedure: 

1. sampl θ(d) from a dirichlet(α) prior 
2. for each word wi in document d 

(a) draw zi from θ(d) 

(b) draw ci from π(ci−1) 

(c) If ci = 1, then draw wi from φ(zi), els draw wi from φ(ci) 

figur 1(b) provid an intuit represent of how phrase be gener by the com- 
posit model. the figur show a three class hmm. two class be simpl multinomi 
distribut over words. the third be a topic model, contain three topics. transit 
between class be show with arrows, annot with transit probabilities. the top- 
ic in the semant class also have probabilities, use to choos a topic when the hmm 
transit to the semant class. phrase be gener by follow a path through the 
model, choos a word from the distribut associ with each syntact class, and a 
topic follow by a word from the distribut associ with that topic for the seman- 
tic class. sentenc with the same syntax but differ content would be gener if the 
topic distribut be different. the gener model thu act like it be play a game 
of “madlibs”: the semant compon provid a list of topic word (shown in black) 
which be slot into templat gener by the syntact compon (shown in gray). 

2.2 infer 

the EM algorithm can be appli to the graphic model show in figur 1, treat the 
document distribut θ, the topic and class φ, and the transit probabl π a 
parameters. however, EM produc poor result with topic models, which have mani pa- 
ramet and mani local maxima. consequently, recent work have focu on approxim 
infer algorithm [6, 8]. We will use markov chain mont carlo (mcmc; see [9]) to 
perform full bayesian infer in thi model, sampl from a posterior distribut over 
assign of word to class and topics. 

We assum that the document-specif distribut over topics, θ, be drawn from a 
dirichlet(α) distribution, the topic distribut φ(z) be drawn from a dirichlet(β) dis- 
tribution, the row of the transit matrix for the hmm be drawn from a dirichlet(γ) 
distribution, the class distribut φ(c) be drawn from a dirichlet(δ) distribution, and all 
dirichlet distribut be symmetric. We use gibb sampl to draw iter a topic 
assign zi and class assign ci for each word wi in the corpu (see [8, 9]). 

given the word w, the class assign c, the other topic assign z 
−i, and the 

hyperparameters, each zi be drawn from: 

P (zi|z−i, c,w) ∝ P (zi|z−i) P (wi|z, c,w−i) 

∝ 

{ 

n 
(di) 
zi + α 

(n 
(di) 
zi + α) 

n 
(zi) 
wi 

+β 

n(zi)+wβ 

ci 6= 1 
ci = 1 



where n(di)zi be the number of word in document di assign to topic zi, n 
(zi) 
wi be the number 

of word assign to topic zi that be the same a wi, and all count includ onli word for 
which ci = 1 and exclud case i. We have obtain these condit distribut by use 
the conjugaci of the dirichlet and multinomi distribut to integr out the paramet 
θ, φ. similarli condit on the other variables, each ci be drawn from: 
P (ci|c−i, z,w) ∝ P (wi|c, z,w−i) P (ci|c−i) 

∝ 

 

 

 

 

 

n 
(ci) 
wi 

+δ 

n 
(ci) 
· 

+wδ 

(n 
(ci−1) 
ci 

+γ)(n 
(ci) 
ci+1 

+i(ci−1=ci)·i(ci=ci+1)+γ) 

n 
(ci) 
· 

+i(ci−1=ci)+cγ 
ci 6= 1 

n 
(zi) 
wi 

+β 

n 
(zi) 
· 

+wβ 

(n 
(ci−1) 
ci 

+γ)(n 
(ci) 
ci+1 

+i(ci−1=ci)·i(ci=ci+1)+γ) 

n 
(ci) 
· 

+i(ci−1=ci)+cγ 
ci = 1 

where n(zi)wi be a before, n 
(ci) 
wi be the number of word assign to class ci that be the 

same a wi, exclud case i, and n 
(ci−1) 
ci be the number of transit from class ci−1 

to class ci, and all count of transit exclud transit both to and from ci. i(·) be an 
indic function, take the valu 1 when it argument be true, and 0 otherwise. increas 
the order of the hmm introduc addit term into P (ci|c−i), but do not otherwis 
affect sampling. 

3 result 

We test the model on the brown corpu and a concaten of the brown and tasa 
corpora. the brown corpu [10] consist of D = 500 document and n = 1, 137, 466 word 
tokens, with part-of-speech tag for each token. the tasa corpu be an untag collect 
of educ materi consist of D = 37, 651 document and n = 12, 190, 931 word 
tokens. word appear in few than 5 document be replac with an asterisk, but 
punctuat be included. the combin vocabulari be of size W = 37, 202. 

We dedic one hmm class to sentenc start/end marker {.,?,!}. In addit to run 
the composit model with T = 200 and C = 20, we examin two special cases: T = 200, 
C = 2, be a model where the onli hmm class be the start/end and semant classes, 
and thu equival to latent dirichlet alloc (lda; [6]); and T = 1, C = 20, be 
an hmm in which the semant class distribut do not vari across documents, and 
simpli have a differ hyperparamet from the other classes. On the brown corpus, we 
ran sampler for lda and 1st, 2nd, and 3rd order hmm and composit models, with three 
chain of 4000 iter each, take sampl at a lag of 100 iter after a burn-in of 
2000 iterations. On brown+tasa, we ran a singl chain for 4000 iter for lda and 
the 3rd order hmm and composit models. We use a gaussian metropoli propos to 
sampl the hyperparameters, take 5 draw of each hyperparamet for each gibb sweep. 

3.1 syntact class and semant topic 

the two compon of the model be sensit to differ kind of depend among 
words. the hmm be sensit to short-rang depend that be constant across docu- 
ments, and the topic model be sensit to long-rang depend that vari across docu- 
ments. As a consequence, the hmm alloc word that vari across context to the se- 
mantic class, where they be differenti into topics. the result of the algorithm, take 
from the 20th sampl on brown+tasa, be show in figur 2. the model cleanli sep- 
arat word that play syntact and semant roles, in sharp contrast to the result of the 
lda model, also show in the figure, where all word be forc into topics. the syntact 
categori includ prepositions, pronouns, past-tens verbs, and punctuation. while one 
state of the hmm, show in the eighth column of the figure, emit common nouns, the 
major of noun be assign to the semant class. 

the design of word a syntact or semant depend upon the corpus. for compar- 
ison, we appli a 3rd order composit model with 100 topic and 50 class to a set of 



the the the the the a the the the 
blood , , of a the , , , 

, and and , of of of a a 
of of of to , , a of in 

bodi a in in in in and and game 
heart in land and to water in drink ball 
and tree to class pictur be stori alcohol and 
in tree farmer govern film and be to team 
to with for a imag matter to bottl to 
be on farm state len be a in play 

blood forest farmer govern light water stori drug ball 
heart tree land state eye matter stori drug game 

pressur forest crop feder len molecul poem alcohol team 
bodi land farm public imag liquid charact peopl * 
lung soil food local mirror particl poetri drink basebal 

oxygen area peopl act eye ga charact person player 
vessel park farm state glass solid author effect footbal 
arteri wildlif wheat nation object substanc poem marijuana player 

* area farm law object temperatur life bodi field 
breath rain corn depart len chang poet use basketbal 

the in he * be say can time , 
a for it new have make would way ; 

hi to you other see use will year ( 
thi on they first make come could day : 
their with i same do go may part ) 
these at she great know found have number 
your by we good get call must kind 
her from there small go do place 
my a thi littl take have 

some into who old find do 

figur 2: upper: topic extract by the lda model. lower: topic and class from the 
composit model. each column repres a singl topic/class, and word appear in order 
of probabl in that topic/class. sinc some class give almost all probabl to onli a 
few words, a list be termin when the word account for 90% of the probabl mass. 

D = 1713 nip paper from volum 0-12. We use the full text, from the abstract to 
the acknowledg or refer section, exclud section headers. thi result in 
n = 4, 312, 614 word tokens. We replac all word appear in few than 3 paper 
with an asterisk, lead to W = 17, 268 types. We use the same sampl scheme a 
brown+tasa. A select of topic and class from the 20th sampl be show in figur 
3. word that might convey semant inform in anoth setting, such a “model”, “al- 
gorithm”, or “network”, form part of the syntax of nips: the consist use of these word 
across document lead them to be incorpor into the syntact component. 

3.2 identifi function and content word 

identifi function and content word requir use inform about both syntact 
class and semant context. In a machin learn paper, the word “control” might be an 
innocu verb, or an import part of the content of a paper. likewise, “graph” could 
refer to a figure, or indic content relat to graph theory. tag class might indic 
that “control” appear a a verb rather than a noun, but decid that “graph” refer to a 
figur requir use inform about the content of the rest of the document. 

the factor of word between the hmm and the lda compon provid a simpl 
mean of assess the role that a give word play in a document: evalu the posterior 
probabl of assign to the lda component. the result of use thi procedur to 
identifi content word in sentenc excerpt from nip paper be show in figur 4. 
probabl be evalu by averag over assign from all 20 samples, and take 
into account the semant context of the whole document. As a result of combin short- 
and long-rang dependencies, the model be abl to pick out the word in each sentenc that 
concern the content of the document. select the word that have high probabl of 



imag data state membran chip expert kernel network 
imag gaussian polici synapt analog expert support neural 
object mixtur valu cell neuron gate vector network 
object likelihood function * digit hme svm output 
featur posterior action current synaps architectur kernel input 

recognit prior reinforc dendrit neural mixtur # train 
view distribut learn potenti hardwar learn space input 

# em class neuron weight mixtur function weight 
pixel bayesian optim conduct # function machin # 
visual paramet * channel vlsi gate set output 

in be see use model network howev # 
with be show train algorithm valu also * 
for have note obtain system result then i 
on becom consid describ case model thu x 

from denot assum give problem paramet therefor t 
at be present found network unit first n 

use remain need present method data here - 
into repres propos defin approach function now c 
over exist describ gener paper problem henc r 

within seem suggest show process algorithm final p 

figur 3: topic and class from the composit model on the nip corpus. 

1. 

In contrast to thi approach, we studi here how the overal network activ can control singl cell 
paramet such a input resistance, a well a time and space constants, paramet that be crucial for 
excit and spariotempor (sic) integration. 

the integr architectur in thi paper combin feed forward control and error feedback adapt 

control use neural networks. 

2. 

In other words, for our proof of convergence, we requir the softassign algorithm to return a doubli 
stochast matrix a *sinkhorn theorem guarante that it will instead of a matrix which be mere close 
to be doubli stochast base on some reason metric. 

the aim be to construct a portfolio with a maxim expect return for a give risk level and time 
horizon while simultan obey *institut or *legal requir constraints. 

3. 

the left graph be the standard experi the right from a train with # samples. 

the graph G be call the *guest graph, and H be call the host graph. 

figur 4: function and content word in the nip corpus. graylevel indic posterior 
probabl of assign to lda component, with black be highest. the box word 
appear a a function word and a content word in one element of each pair of sentences. 
asterisk word have low frequency, and be treat a a singl word type by the model. 

be assign to syntact hmm class produc templat for write nip papers, into 
which content word can be inserted. for example, replac the content word that the 
model identifi in the second sentenc with content word appropri to the topic of the 
present paper, we could write: the integr architectur in thi paper combin simpl 
probabilist syntax and topic-bas semant use gener models. 

3.3 margin probabl 

We assess the margin probabl of the data under each model, P (w), use the har- 
monic mean of the likelihood over the last 2000 iter of sampling, a standard method 
for evalu bay factor via mcmc [11]. thi probabl take into account the com- 
plexiti of the models, a more complex model be penal by integr over a latent 
space with larg region of low probability. the result be show in figur 5. lda out- 
perform the hmm on the brown corpus, but the hmm out-perform lda on the larg 
brown+tasa corpus. the composit model provid the best account of both corpora, 



1st 2nd 3rd 1st 2nd 3rd 
−6e+06 

−5.5e+06 

−5e+06 

−4.5e+06 

−4e+06 

lda 

hmm 

composit 

brown 

M 
ar 

gi 
na 

l l 
ik 

el 
ih 

oo 
d 

1st 2nd 3rd 1st 2nd 3rd 
−8e+07 

−7e+07 

−6e+07 

−5e+07 

−4e+07 

lda 

hmm 

composit 

brown+tasa 

M 
ar 

gi 
na 

l l 
ik 

el 
ih 

oo 
d 

figur 5: log margin probabl of each corpu under differ models. label on 
horizont axi indic the order of the hmm. 

1st 2nd 3rd 1st 2nd 3rd 1st 2nd 3rd 1st 2nd 3rd 
0 

0.2 

0.4 

0.6 

A 
dj 

u 
te 

d 
R 

an 
d 

In 
de 

x 

brown brown+tasa brown brown+tasa 

DC hmm composit 
0 

0.2 

0.4 

0.6 

0.8 
1000 most frequent word 

A 
dj 

u 
te 

d 
R 

an 
d 

In 
de 

x 

figur 6: part-of-speech tag for hmm, composite, and distribut cluster (dc). 

be abl to use whichev kind of depend inform be most predictive. use 
a higher-ord transit matrix for either the hmm or the composit model produc lit- 
tle improv in margin likelihood for the brown corpus, but the third-ord model 
perform best on brown+tasa. 

3.4 part-of-speech tag 

part-of-speech tag – identifi the syntact class of a word – be a standard task in 
comput linguistics. most unsupervis tag method use a lexicon that identifi 
the possibl class for differ words. thi simplifi the problem, a most word belong 
to a singl class. however, genuin unsupervis recoveri of parts-of-speech have be 
use to ass statist model of languag learning, such a distribut cluster [3]. 

We assess tag perform on the brown corpus, use two tagsets. one set con- 
sist of all brown tags, exclud those for sentenc markers, leav a total of 297 tags. 
the other set collaps these tag into ten high-level designations: adjective, adverb, con- 
junction, determiner, foreign, noun, preposition, pronoun, punctuation, and verb. We eval- 
uat tag perform by use the adjust rand index [12], to measur the con- 
cordanc between the tag and the class assign of the hmm and composit model 
in the 20th sample. the adjust rand index rang from −1 to 1, with an expect 
of 0. result be show in figur 6. both model produc class assign that be 
strongli concord with part-of-speech, although the hmm give a slightli good match 
to the full tagset, and the composit model give a closer match to the top-level tags. thi be 
partli becaus all word that vari strongli in frequenc across context get assign to the 
semant class in the composit model, so it miss some of the fine-grain distinct 
express in the full tagset. both the hmm and the composit model perform good 
than the distribut cluster method describ in [3], which be use to form the 1000 
most frequent word in brown into 19 clusters. figur 6 compar thi cluster with the 
class for those word from the hmm and composit model train on brown. 

3.5 document classif 

the 500 document in the brown corpu be classifi into 15 groups, from editori jour- 
nalism to romanc fiction. We assess the qualiti of the topic recov by the lda and 



composit model by train a naiv bay classifi on the topic vector produc by the 
two models. We comput classif accuraci use 10-fold cross valid for the 
20th sampl from a singl chain. the two model perform similarly. baselin accuracy, 
choos class accord to the prior, be 0.09. train on brown, the lda model give 
an accuraci of 0.51, while 1st, 2nd, and 3rd order composit model give 0.45, 0.41, 0.42 
respectively. train on brown+tasa, the lda model give 0.54, while the 1st. 2nd, and 
3rd order composit model give 0.48, 0.48, 0.46 respectively. the slightli low accuraci 
of the composit model may result from have few data in which to find correlations: it 
onli see the word alloc to the semant component, which account for approxim 
20% of the word in the corpus. 

4 conclus 

the composit model we have describ captur the interact between short- and long- 
rang depend between words. As a consequence, it be abl to simultan learn 
syntact class and semant topic and identifi the role that word play in documents, 
and be competit in part-of-speech tag and classif with model that special 
in onli one form of dependency. clearly, such a model do not do justic to the depth 
of syntact or semant structure, or their interaction. however, it illustr how a sensi- 
tiviti to differ kind of statist depend might be suffici for the first stage of 
languag acquisition, discov the syntact and semant build block that form the 
basi for learn more sophist representations. 

acknowledgements. the tasa corpu appear courtesi of tom landauer and touchston appli 
scienc associates, and the nip corpu be provid by sam roweis. thi work be support by 
the darpa calo program and ntt commun scienc laboratories. 

refer 
[1] H. J. neville, D. L. mills, and D. S. lawson. fraction language: differ neural sub- 

sytem with differ sensit periods. cerebr cortex, 2:244–258, 1992. 

[2] R. brown. A first language. harvard univers press, cambridge, ma, 1973. 

[3] M. redington, N. chater, and S. finch. distribut information: A power cue for acquir- 
ing syntact categories. cognit science, 22:425–469, 1998. 

[4] T. K. landauer and S. T. dumais. A solut to plato’ problem: the latent semant anal- 
ysi theori of acquisition, induction, and represent of knowledge. psycholog review, 
104:211–240, 1997. 

[5] C. man and H. schütze. foundat of statist natur languag processing. mit press, 
cambridge, ma, 1999. 

[6] D. M. blei, A. Y. ng, and M. I. jordan. latent dirichlet allocation. journal of machin 
learn research, 3:993–1022, 2003. 

[7] N. coccaro and D. jurafsky. toward good integr of semant predictor in statist 
languag modeling. In proceed of icslp-98, volum 6, page 2403–2406. 1998. 

[8] T. L. griffith and M. steyvers. find scientif topics. proceed of the nation academi 
of science, 101:5228–5235, 2004. 

[9] w.r. gilks, S. richardson, and D. J. spiegelhalter, editors. markov chain mont carlo in 
practice. chapman and hall, suffolk, 1996. 

[10] H. kucera and W. N. francis. comput analysi of present-day american english. brown 
univers press, providence, ri, 1967. 

[11] R. E. kass and A. E. rafferty. bay factors. journal of the american statist association, 
90:773–795, 1995. 

[12] L. hubert and P. arabie. compar partitions. journal of classification, 2:193–218, 1985. 


