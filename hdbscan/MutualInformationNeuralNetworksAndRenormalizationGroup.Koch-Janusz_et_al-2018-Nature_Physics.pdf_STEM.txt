


































































mutual information, neural network and the renorm group 


articl 
https://doi.org/10.1038/s41567-018-0081-4 

1institut for theoret physics, eth zurich, zurich, switzerland. 2racah institut of physics, hebrew univers of jerusalem, jerusalem, israel. 
*e-mail: maciejk@ethz.ch 

machin learn have be captiv public attent late due to groundbreak advanc in autom translation, imag and speech recognition1, game-playing2 and achiev- 
ing super-human perform in task in which human excel 
while more tradit algorithm approach struggled3. the 
applic of those techniqu in physic be veri recent, initi 
leverag the trademark prowess of machin learn in classifica- 
tion and pattern recognit and appli them to classifi phase 
of matter4–8, studi amorph materials9,10, or exploit the neural 
networks’ potenti a effici nonlinear approxim of arbitrari 
functions11,12 to introduc a new numer simul method for 
quantum systems13,14. however, the excit possibl of employ 
machin learn not a a numer simulator, or a hypothesi tester, 
but a an integr part of the physic reason process be still larg 
unexplor and, give the stagger pace of progress in the field of 
artifici intelligence, of fundament import and promise. 

the renorm group (rg) approach have be one of the 
conceptu most profound tool of theoret physic sinc it 
inception. It underli the semin work on critic phenomena15, 
and the discoveri of asymptot freedom in quantum chromody- 
namics16, and of the kosterlitz–thouless phase transition17,18. the 
RG be not a monolith, but rather a conceptu framework compris- 
ing differ techniques: real-spac rg19, function rg20 and den- 
siti matrix rg21, among others. while all of those scheme differ 
quit substanti in their details, style and applicability, there be 
an underli physic intuit that encompass all of them—th 
essenc of RG lie in identifi the ‘relevant’ degre of freedom 
and integr out the ‘irrelevant’ one iteratively, therebi arriv 
at a universal, low-energi effect theory. howev potent the RG 
idea, those relev degre of freedom need to be identifi first22,23. 
thi be often a challeng conceptu step, particularli for strongli 
interact systems, and may involv a sequenc of mathemat 
map to models, whose behaviour be good understood24,25. 

here we introduc an artifici neural network algorithm itera- 
tive identifi the physic relev degre of freedom in 
a spatial region and perform an RG coarse-grain step. the 
input data be sampl of the system configur drawn from 

a boltzmann distribution; no further knowledg about the micro- 
scopic detail of the system be provided. the intern paramet 
of the network, which ultim encod the degre of freedom 
of interest at each step, be optim (‘learned’, in neural network 
parlance) by a train algorithm base on evalu real-spac 
mutual inform (rsmi) between spatial separ regions. 
We valid our approach by studi the ise and dimer model 
of classic statist physic in two dimensions. We obtain the RG 
flow and extract the ise critic exponent. the robust of the 
rsmi algorithm to physic irrelev nois be demonstrated. 

the identif of the import degre of freedom, and the 
abil to execut a real-spac RG procedure19, have not onli quanti- 
tativ but also conceptu significance: it allow one to gain insight 
into the correct way of think about the problem at hand, rais 
the prospect that machine-learn techniqu may augment the 
scientif inquiri in a fundament fashion. 

the rsmi algorithm 
befor go into more detail, let u provid a bird’ eye view of our 
method and results. We begin by phrase the problem in probabi- 
listic/information-theoret terms, a languag also use in ref 26–30. 
To thi end, we consid a small ‘visible’ spatial area V , which 
togeth with it environ E form the system X , and we defin 
a particular condit probabl distribut ∣Λ H VP ( ), which 
describ how the relev degre of freedom H (‘dub hiddens’) 
in V depend on both V and E . We then show that the sought-aft 
condit probabl distribut be found by an algorithm maxi- 
mize an information-theoret quantity, the mutual information, 
and that thi algorithm lend itself to a natur implement 
use artifici neural networks. We describ how RG be practic 
perform by coarse-grain with respect to ∣Λ H VP ( ) and iter 
the procedure. finally, we provid a verif of our claim by 
consid two paradigmat model of statist physics: the ise 
model—for which the RG procedur yield the famou kadanoff 
block spins—and the dimer model, whose relev degre of free- 
dom be much less trivial. We reconstruct the RG flow of the ise 
model and extract the critic exponent. 

mutual information, neural network and the 
renorm group 
maciej koch-janusz 1* and zohar ringel2 

physic system differ in their microscop detail often display strikingli similar behaviour when probe at macroscop 
scales. those univers properties, larg determin their physic characteristics, be reveal by the power renormal- 
izat group (rg) procedure, which systemat retain ‘slow’ degre of freedom and integr out the rest. however, 
the import degre of freedom may be difficult to identify. here we demonstr a machine-learn algorithm capabl of 
identifi the relev degre of freedom and execut RG step iter without ani prior knowledg about the system. 
We introduc an artifici neural network base on a model-independent, information-theoret character of a real-spac 
RG procedure, which perform thi task. We appli the algorithm to classic statist physic problem in one and two dimen- 
sions. We demonstr RG flow and extract the ise critic exponent. our result demonstr that machine-learn tech- 
niqu can extract abstract physic concept and consequ becom an integr part of theory- and model-building. 

© 2018 macmillan publish limited, part of springer nature. all right reserved. 

natur physic | www.nature.com/naturephys 

mailto:maciejk@ethz.ch 
http://orcid.org/0000-0002-2903-5202 
http://www.nature.com/naturephys 


articl natur physic 

consid then a classic system of local degre of freedom 
= … ≡X x x x{ , , } { }N i1 , defin by a hamiltonian energi function 

h({xi}) and associ statist probabl ∝ β−xp( ) e xh({ })i , 
where β be the invers temperature. altern (and suffici 
for our purposes), the system be give by mont carlo sampl of the 
equilibrium distribut xp( ). We denot a small spatial region of 
interest by ≡V v{ }i and the remaind of the system by ≡E e{ }i , so 
that =X V E( , ). We adopt a probabilist point of view, and treat X E, 
and so on a random variables. our goal be to extract the relev 
degre of freedom H from V . 

‘relevance’ be understood here in the follow way: the degre 
of freedom that RG captur govern the long-dist behaviour 
of the theory, and therefor the experiment measur physi- 
cal properties; they carri the most inform about the system 
at large, a oppos to local fluctuations. We thu formal defin 
the random variabl H a a composit function of degre of free- 
dom in V maxim the ‘mutual information’ between H and the 
environ E . thi definition, a we discu in the supplementari 
information, be relat to the requir that the effect coarse- 
grain hamiltonian be compact and short-ranged, which be a con- 
dition ani success standard RG scheme should satisfy. As we also 
show, it be support by numer results. 

mutual information, denot by iλ, measur the total amount of 
inform about one random variabl contain in the other9,10,31 
(thus, it be more gener than correl coefficients). It be give in 
our set by: 

∑=λ Λ Λ 
Λ 

 

 
 

 

 
 

H E E H 
E H 

H E 
H E 

I P 
P 

P P 
( : ) ( , )log 

( , ) 
( ) ( ) (1) 

, 

the unknown distribut Λ E HP ( , ) and it margin 
Λ HP ( ), depend on a set of paramet Λ (which we keep gener 

at thi point), be function of V ep( , ) and of ∣Λ H VP ( ), which be the 
central object of interest. 

find ∣Λ H VP ( ) that maxim IΛ under certain constraint be 
a well-pos mathemat question and have a formal solution32. 

however, sinc the space of probabl distribut grow expo- 
nential with the number of local degre of freedom, it is, in 
practice, imposs to use without further assumpt for ani 
but the small physic systems. our approach be to exploit the 
remark dimension reduct properti of artifici neural 
networks11. We use restrict boltzmann machin (rbms), a class 
of probabilist network well adapt to approxim arbitrari 
data probabl distributions. An rbm be compos of two layer 
of nodes, the ‘visible’ layer, correspond to local degre of free- 
dom in our setting, and a ‘hidden’ layer. the interact between 
the layer be defin by an energi function ≡ θΘ V HE E ( , )a b, , = 
− ∑ b hj j j − ∑ a vi i i − θ∑ v hij i ij j, such that the joint probabl distri- 
bution for a particular configur of visibl and hidden degre 
of freedom be give by a boltzmann weight: 

=Θ 
− θV H 

Z 
V HP ( , ) 1 e (2)e ( , )a b, , 

where Z be the normalization. the goal of the network train be 
to find paramet θij (‘weights’ or ‘filters’) and ai,bi optim a 
chosen object function. 

three distinct rbm be used. two be train a effici 
approxim of the probabl distribut V ep( , ) and vp( ), 
use the celebr contrast diverg (cd) algorithm33. their 
train paramet be use by the third network (see fig. 1b), 
which have a differ objective: to find ∣Λ H VP ( ) maxim iλ. To 
the end we introduc the real-spac mutual inform (rsmi) 
network, whose architectur be show in fig. 1a. the hidden unit 
of rsmi correspond to coarse-grain variabl H. 

the paramet λΛ = a b( , , )i j i 
j of the rsmi network be train 

by an iter procedure. At each iteration, a mont carlo estim 
of function Λ H EI ( : ) and it gradient be perform for the current 
valu of paramet Λ. the gradient be then use to improv 
the valu of weight in the next step, use a stochast gradient 
descent procedure. 

the train weight Λ defin the probabl ∣Λ H VP ( ) of a 
boltzmann form, which be use to gener MC sampl of the coarse- 
grain system. those, in turn, becom input to the next iter of 
the rsmi algorithm. the estim of mutual information, weight of 
the train rbm and set of gener MC sampl at everi RG step 
can be use to extract quantit inform about the system in 
the form of correl functions, critic expon and so on, a we 
show below and in the supplementari information. We also empha- 
size that the paramet Λ identifi relev degre of freedom be 
re-comput at everi RG step. thi potenti allow rsmi to captur 
the evolut of the degre of freedom along the RG flow34. 

valid 
To valid our approach, we consid two import classic mod- 
el of statist physics: the ise model, whose coarse-grain 
degre of freedom resembl the origin ones, and the fulli pack 
dimer model, where they be entir different. 

Ha 

b 

B 

P( ) 

CD CD rsmi 

pλ(h∣ ) 
λ ji 

θ 

θ( ) 
), 

( ), 

P( 

fig. 1 | the rsmi algorithm. a, the rsmi neural network architecture. the 
hidden layer H be directli coupl to the visibl layer V via the weight λi 

j 
(red arrows). however, the train algorithm for the weight estim 
mutual inform between H and the environ E . the buffer B be 
introduc to filter out local correl within V (see supplementari 
information). b, the workflow of the algorithm. the cd-algorithm-train 
rbm learn to approxim probabl distribut V ep( , ) and vp( ). their 
final parameters, denot collect by V eθ( , ) and vθ( ), be input for the 
main rsmi network learn to extract H v∣λp ( ) by maxim iλ. the final 
weight λi 

j of the rsmi network identifi the relev degre of freedom. 
they be show in figs. 2 and 4 for ise and dimer problems. 

0 1 

0 

a b 

1 
–2.5 

0 

2.5 

0 1 

0 

1 

0 1 

0 

1 

0 1 

0 

1 

0 1 

0 

1 

fig. 2 | the weight of the rsmi network train on the ise model. 
visual of the weight of the rsmi network train on the ise model 
for a visibil area V of 2 × 2 spins. the ann coupl strongli to area with 
larg absolut valu of the weights. a, the weight for Nh = 1 hidden neuron: 
the ann discov kadanoff blocking. b, the weight for Nh = 4 hidden 
neurons: each neuron track one origin spin. 

© 2018 macmillan publish limited, part of springer nature. all right reserved. 

natur physic | www.nature.com/naturephys 

http://www.nature.com/naturephys 


articlesnatur physic 

the ise hamiltonian on a two-dimension (2d) squar lattic is: 

∑= 
⟨ ⟩ 

H s s (3) 
i j 

i jI 
, 

with si = ± 1 and the summat over near neighbours. real-spac 
RG of the ise model proce by the block-spin construction19, 
wherebi each 2 × 2 block of spin be coarse-grain into a singl 
effect spin, whose orient be decid by a ‘majority’ rule. 

the result of the rsmi algorithm train on ise model sam- 
ple be show in fig. 2. We vari the number of both hidden neu- 
ron Nh and the visibl units, which be arrang in a 2D area V 
of size L × L (see fig. 1a). for a four-spin area, the network inde 
rediscov the famou kadanoff block-spin: fig. 2a show a singl 
hidden unit coupl uniformli to four visibl spin (that is, the ori- 
entat of the hidden unit be decid by the averag magnet 
in the area). figur 2b be a trivial but import saniti check: give 
four hidden unit to extract relev degre of freedom from an 
area of four spins, the network coupl each hidden unit to a dif- 
ferent spin, a expected. In the supplementari inform we also 
compar the weight for area V of differ size, which be general- 
izat of the kadanoff procedur to larg blocks. 

We next studi the dimer model, give by an entropy-onli parti- 
tion function, which count the number of dimer cover of the 
lattic (that is, subset of edg such that everi vertex be the endpoint 
of exactli one edge). figur 3a show sampl dimer configur 
(and addit spin degre of freedom add to gener noise). 
thi decept simpl descript hide non-trivi physics35 and 
correspondingly, the RG procedur for the dimer model be more 
subtle, since—in contrast to the ise case—th correct degre of 
freedom to perform RG on be not dimers, but rather look like effec- 
tive local electr fields. thi be reveal by a mathemat map 
to a ‘height field’ h (see fig. 3a,b and ref. 36), whose gradient behav 
like electr fields. the continuum limit of the dimer model be give 
by the follow action: 

∫ ∫= ∇ ≡S h x h xx E x[ ] d ( ( )) d ( ) (4)dim 2 2 2 2 

and therefor the coarse-grain degre of freedom be low- 
momentum (fourier) compon of the electr field ex,ey in the 
x and y directions. they correspond to ‘staggered’ dimer configura- 
tion show in fig. 3a. 

remarkably, the rsmi algorithm extract the local electr field 
from the dimer model sampl without ani knowledg of those 
mappings. In fig. 4, the weight for Nh = 2 and Nh = 4 hidden neu- 
rons, for an 8 × 8 area (similar to fig. 3a), be shown: the pattern 
of larg neg (blue) weight coupl strongli to a dimer pattern 

0 1 2 3 4 5 6 7 

0 

a 

b 

1 

2 

3 

4 

5 

6 

7 

0 

1 

2 

3 

4 

5 

6 

7 

0 

1 

2 

3 

4 

5 

6 

7 

0 

1 

2 

3 

4 

5 

6 

7 

0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 

0 1 2 3 4 5 6 7 

0 

1 

2 

3 

4 

5 

6 

7 

0 

1 

2 

3 

4 

5 

6 

7 

0 1 2 3 4 5 6 7 
–2 

–1 

0 

1 

2 

fig. 4 | the weight of the rsmi network train on dimer model data. a, Nh = 2 hidden neuron for a visibl area V of 8 × 8 spins. the two filter 
recogn Ey and Ex + Ey electr fields, respect (compar with dimer pattern in fig. 3a). b, the train weight for Nh = 4 hidden neurons. 

0 

a 

b 

1 0 1 

2 233 

4 5 4 5 

77 66 

0 

0 0 

0 

1 

11 

1 

3 32 2 

3 32 2 

fig. 3 | the dimer model. a, two sampl dimer configur (blue links), 
correspond to the Ey and Ex electr fields, respectively. the coupl 
pair of addit spin degre of freedom on vertex and face of the 
lattic (wiggli lines) be decoupl from the dimer and from each other. 
their fluctuat constitut irrelev noise. b, An exampl of map the 
dimer model to local electr fields. the so-cal stagger configur 
on the left map to uniform non-vanish field in the vertic direction: 
⟨ ⟩ ≠E 0y . the ‘columnar’ configur on the right produc both Ex and Ey 
that be zero on averag (see ref. 36 for detail of the mapping). 

© 2018 macmillan publish limited, part of springer nature. all right reserved. 

natur physic | www.nature.com/naturephys 

http://www.nature.com/naturephys 


articl natur physic 
correspond to local uniform Ey field (see left panel of fig. 3a,b). 
the larg posit (yellow) weight select an ident pattern, trans- 
late by one link. the remain neuron extract linear superposi- 
tion Ex + Ey or Ex − Ey of the fields. 

To demonstr the robust of the rsmi, we add physic 
irrelev noise, form nevertheless a pronounc pattern, which 
we model by addit spin degre of freedom, strongli coupl 
(ferromagnetically) in pair (wiggli line in fig. 3a). decoupl 
from the dimers, and from other pairs, they form a trivial system, 
whose fluctuat be short-rang nois on top of the dimer model. 
vanish weight (green in fig. 4a,b) on site where pair of spin 
resid prove that rsmi discard their fluctuat a irrelev for 
long-rang physics, despit their regular pattern. 

notably, the filter obtain use our approach for the dimer 
model, which match the analyt expectation, be orthogon to 
those obtain use kullback–leibl divergence. As expand on 
in the supplementari information, thi show that standard rbm 
minim the kullback–leibl diverg do not gener per- 
form rg, therebi contradict prior claims37. 

finally, we demonstr that by iter the rsmi algorithm the 
qualit insight into the natur of relev degre of freedom 
give rise to quantit results. To thi end, we revisit the 2D ise 
model that (contrari to the dimer model) exhibit a non-trivi crit- 
ical point at the temperatur = + ∕ −T (log(1 2 ) 2)c 

1, separ the 
paramagnet and ferromagnet phases. We gener mont carlo 
sampl of the system of size 128 × 128 at valu T around the criti- 
cal point, and for each one we perform up to four RG steps, by com- 
put the Λ filter use rsmi, coarse-grain the system with 
respect to those filter (effect halv the linear dimensions) 
and reiter the procedure. In addit to the set of mont carlo 
configur for the coarse-grain system, estim of mutual 
inform a well a the filter of the cd-train rbm be gener- 
ate and stored. the effect temperatur T of the system at each 
RG step can be evalu entir intrins either from correla- 
tion or the mutual information, a discuss in the supplementari 
information. use the rbm filters, spin–spin correl (for 
instance, next-nearest neighbour) can be computed. By compar 
these with know analyt results38, an addit cross-check of 
the effect temperatur can be obtained. 

In fig. 5, the effect T be plot against ξ ξ∕log ( )2 128 , where ξ 
and ξ128 be the current and 128 × 128 systems’ correl lengths, 
respect (thi have the mean of an RG step for integ values). 
the RG flow of the 2D ise model be recovered: system start 
with T < Tc flow toward ever-decreas T (that is, an order 
state), while the one with T > Tc flow toward a paramagnet. In fact, 
the posit of the critic point can be estim with 1% accuraci 
just from the diverg flow. furthermore, we evalu the correla- 
tion length expon ν, defin by ξ ∝ τ−ν. use the finite-s data 
collaps (see supplementari fig. 4), it value, equal to the neg 
slope, be estim to be ν ≈ 1.0 ± 0.15, consist with the exact ana- 
lytic result ν = 1. 

futur direct 
artifici neural network base on rsmi optim have 
prove capabl of extract complex inform about physi- 
calli relev degre of freedom and use it to perform a real- 
space RG procedure. the rsmi algorithm we propos allow for 
the studi of the exist and locat of critic points, and RG 
flow in their vicinity, a well a estim of correl func- 
tions, critic expon and so on. thi approach be an exampl 
of a new paradigm in appli machin learn in physics: the 
intern data represent discov by suitabl design algo- 
rithm be not just technic mean to an end, but instead be a 
clear reflect of the underli structur of the physic system 
(see also ref. 39). thus, in spite of their ‘black box’ reputation, the 
innard of such architectur may teach u fundament lessons. 

thi rais the prospect of employ machin learn in scienc 
in a collabor fashion, exploit the machines’ power to distil 
subtl inform from vast data, and human creativ and back- 
ground knowledge40. 

numer further research direct can be pursued. most 
directly, equilibrium system with less understood relev 
degre of freedom—for example, disord and glassi sys- 
tems—can be investigated9,10. the abil of the rsmi algorithm 
to re-comput the relev degre of freedom at everi RG step 
potenti allow one to studi their evolut along the (more 
complicated) RG flow34. furthermore, although we studi clas- 
sical systems, the extens to the quantum domain be possibl 
via the quantum-to-class map of euclidean path integr 
formalism. A more detail analysi of the mutual-informa- 
tion-bas RG procedur may prove fruit from a theoret 
perspective. finally, applic of rsmi beyond physic be 
possible, sinc it offer a neural network implement of a 
variant of the inform bottleneck method32, success in 
compress and cluster analyses41; it can also be use a a 
local-noise-filt pre-train stage for other machine-learn- 
ing algorithms. 

data availability. the data that support the plot within thi paper 
and other find of thi studi be avail from the correspond- 
ing author upon request. 

received: 11 may 2017; accepted: 13 februari 2018; 
published: xx xx xxxx 

refer 
1. lecun, y., bengio, Y. & hinton, G. E. deep learning. natur 521, 

436–444 (2015). 
2. silver, D. et al. master the game of Go with deep neural network and tree 

search. natur 529, 584–589 (2016). 
3. hershey, J. r., rennie, S. j., olsen, P. A. & kristjansson, T. T. super-human 

multi-talk speech recognition: A graphic model approach. comput. 
speech lang. 24, 45–66 (2010). 

4. carrasquilla, J. & melko, R. G. machin learn phase of matter. nat. phys. 
13, 431–434 (2017). 

5. torlai, G. & melko, R. G. learn thermodynam with boltzmann 
machines. phys. rev. B 94, 165134 (2016). 

6. van nieuwenburg, E. P. l., liu, y.-h. & huber, S. D. learn phase 
transit by confusion. nat. phys. 13, 435–439 (2017). 

7. wang, L. discov phase transit with unsupervis learning. 
phys. rev. B 94, 195105 (2016). 

8. ohtsuki, T. & ohtsuki, T. deep learn the quantum phase transit in 
random electron systems: applic to three dimensions. J. phys. soc. jpn 
86, 044708 (2017). 

t/tc 

R 
G 

s 
te 

p 

paramagnetferromagnet 

0.94 0.96 0.98 1.00 1.02 1.04 1.06 1.08 

0.0 

0.5 

1.0 

1.5 

2.0 

2.5 

3.0 

1.03 1.02 1.01 1.007 

1.002 0.99 0.98 

fig. 5 | RG flow for the 2D ise model. the temperatur T (in unit 
of tc) a a function of the RG step for system with initi mont carlo 
temperatur (denot by line of differ colour) below and abov tc. 
see supplementari inform for details. 

© 2018 macmillan publish limited, part of springer nature. all right reserved. 

natur physic | www.nature.com/naturephys 

http://www.nature.com/naturephys 


articlesnatur physic 
9. ronhovde, p.et al detect hidden spatial and spatio-tempor structur in 

glass and complex physic system by multiresolut network clustering. 
eur. phys. J. E 34, 105 (2011). 

10. ronhovde, p.et al detect of hidden structur for arbitrari scale in 
complex physic systems. sci. rep. 2, 329 (2012). 

11. hinton, G. E. & salakhutdinov, R. R. reduc the dimension of data 
with neural networks. scienc 313, 504–507 (2006). 

12. lin, H. W. & tegmark, M. whi do deep and cheap learn work so well? 
J. stat. phys. 168, 1223–1247 (2017). 

13. carleo, G. & troyer, M. solv the quantum many-bodi problem with 
artifici neural networks. scienc 355, 602–606 (2017). 

14. deng, d.-l., li, X. & sarma, S. D. machin learn topolog states. 
phys. rev. B 96, 195145 (2017). 

15. wilson, K. G. the renorm group: critic phenomenon and the kondo 
problem. rev. mod. phys. 47, 773–840 (1975). 

16. politzer, H. D. reliabl perturb result for strong interactions? 
phys. rev. lett. 30, 1346–1349 (1973). 

17. berezinskii, V. L. destruct of long-rang order in one-dimension and 
two-dimension system have a continu symmetri group I. classic 
systems. sov. J. exp. theor. phys. 32, 493 (1971). 

18. kosterlitz, J. M. & thouless, D. ordering, metast and phase transit 
in two-dimension systems. J. phys. C 6, 1181 (1973). 

19. kadanoff, L. P. scale law for ise model near t(c). physic 2, 
263–272 (1966). 

20. wetterich, C. exact evolut equat for the effect potential. phys. lett. B 
301, 90–94 (1993). 

21. white, S. R. densiti matrix formul for quantum renorm groups. 
phys. rev. lett. 69, 2863–2866 (1992). 

22. ma, s.-k, dasgupta, C. & hu, c.-k random antiferromagnet chain. 
phys. rev. lett. 43, 1434–1437 (1979). 

23. corboz, P. & mila, F. tensor network studi of the shastry–sutherland model 
in zero magnet field. phys. rev. B 87, 115144 (2013). 

24. capponi, s., chandra, V. r., auerbach, A. & weinstein, M. p6 chiral 
reson valenc bond in the kagom antiferromagnet. phys. rev. B 87, 
161118 (2013). 

25. auerbach, A. interact electron and quantum magnet 
(springer, new york, ny, 1994). 

26. gaite, J. & o’connor, D. field theori entropy, the h theorem, and the 
renorm group. phys. rev. D 54, 5163–5173 (1996). 

27. preskill, J. quantum inform and physics: some futur directions. 
J. mod. opt. 47, 127–137 (2000). 

28. apenko, S. M. inform theori and renorm group flows. phys. A 
391, 62–77 (2012). 

29. machta, B. b., chachra, r., transtrum, M. K. & sethna, J. P. paramet space 
compress underli emerg theori and predict models. scienc 342, 
604–607 (2013). 

30. beny, C. & osborne, T. J. the renorm group via statist inference. 
new. J. phys. 17, 083005 (2015). 

31. stephan, j.-m., inglis, s., fendley, P. & melko, R. G. geometr mutual 
inform at classic critic points. phys. rev. lett. 112, 127204 (2014). 

32. tishby, n., pereira, F. C. & bialek, W. the inform bottleneck method. In 
proc. 37th allerton conf. on communication, control and comput 
(ed hajek, B. & sreenivas, R. s.) 49, 368–377 (univers of illinois, 2001). 

33. hinton, G. E. train product of expert by minim contrast 
divergence. neural comput. 14, 1771–1800 (2002). 

34. ludwig, A. W. W. & cardy, J. L. perturb evalu of the conform 
anomali at new critic point with applic to random systems. 
nucl. phys. B 285, 687–718 (1987). 

35. fisher, M. E. & stephenson, J. statist mechan of dimer on a plane 
lattice. ii. dimer correl and monomers. phys. rev. 132, 
1411–1431 (1963). 

36. fradkin, E. field theori of condens matter physic (cambridg 
univ. press, cambridge, 2013). 

37. mehta, P. & schwab, D. J. An exact map between the variat 
renorm group and deep learning. preprint at 
abs/1410.3831 (2014). 

38. mccoy, B. M. & wu, T. T. the two-dimension ise model (harvard univ. 
press, cambridge, ma, 1973). 

39. schoenholz, S. s., cubuk, E. d., sussman, D. m., kaxiras, E. & liu, A. J. A 
structur approach to relax in glassi liquids. nat. phys. 12, 
469–471 (2016). 

40. jordan, M. I. & mitchell, T. M. machin learning: trends, perspectives, and 
prospects. scienc 349, 255–260 (2015). 

41. slonim, N. & tishby, N. document cluster use word cluster via the 
inform bottleneck method. In proc. 23rd annual intern acm 
sigir conf. on research and develop in inform retrieval, sigir ’00 
208–215 (acm, 2000). 

acknowledg 
We thank S. huber and P. fendley for discussions. m.k.-j. grate acknowledg 
the support of the swiss nation scienc foundation. z.r. be support by the 
european union' horizon 2020 research and innov programm under the mari 
sklodowska-curi grant agreement no. 657111. 

author contribut 
m.k.-j. and z.r. contribut equal to thi work. 

compet interest 
the author declar no compet interests. 

addit inform 
supplementari inform be avail for thi paper at https://doi.org/10.1038/ 
s41567-018-0081-4. 

reprint and permiss inform be avail at www.nature.com/reprints. 

correspond and request for materi should be address to m.k. 

publisher’ note: springer natur remain neutral with regard to jurisdict claim in 
publish map and institut affiliations. 

© 2018 macmillan publish limited, part of springer nature. all right reserved. 

natur physic | www.nature.com/naturephys 

https://doi.org/10.1038/s41567-018-0081-4 
https://doi.org/10.1038/s41567-018-0081-4 
http://www.nature.com/reprint 
http://www.nature.com/naturephys 

mutual information, neural network and the renorm group 
the rsmi algorithm 
valid 
futur direct 
data availability. 

acknowledg 
fig. 1 the rsmi algorithm. 
fig. 2 the weight of the rsmi network train on the ise model. 
fig. 3 the dimer model. 
fig. 4 the weight of the rsmi network train on dimer model data. 
fig. 5 RG flow for the 2D ise model. 




