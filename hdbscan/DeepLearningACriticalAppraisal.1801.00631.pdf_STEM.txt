


















































deep learn a critic appraisal.formatted.pag 


deep learning: 
A critic apprais 

gari marcu 1 
new york univers 

abstract 

although deep learn have histor root go back decades, neither the term “deep 
learning” nor the approach be popular just over five year ago, when the field be 
reignit by paper such a krizhevsky, sutskev and hinton’ now classic 2012 
(krizhevsky, sutskever, & hinton, 2012)deep net model of imagenet. 

what have the field discov in the five subsequ years? against a background of 
consider progress in area such a speech recognition, imag recognition, and game 
playing, and consider enthusiasm in the popular press, I present ten concern for deep 
learning, and suggest that deep learn must be supplement by other techniqu if we 
be to reach artifici gener intelligence. 

! depart of psycholog and neural science, new york university, gary.marcu at nyu.edu. I thank christina 1 
chen, françoi chollet, erni davis, zack lipton, stefano pacifico, suchi saria, and athena vouloumano for 
sharp-ey comments, all gener suppli on short notic dure the holiday at the close of 2017. 

page ! of !1 27 

http://nyu.edu 


for most problem where deep learn have enabl 
transform good solut (vision, speech), we'v 
enter diminish return territori in 2016-2017. 

françoi chollet, google, author of kera 
neural network librari 
decemb 18, 2017 

‘scienc progress one funer at a time.' the futur 
depend on some graduat student who be deepli suspici 
of everyth I have said. 

geoff hinton, grandfath of deep learn 
septemb 15, 2017 

1. Is deep learn approach a wall? 

although deep learn have histor root go back decades(schmidhuber, 2015), it 
attract rel littl notic until just over five year ago. virtual everyth 
chang in 2012, with the public of a seri of highli influenti paper such a 
krizhevsky, sutskev and hinton’ 2012 imagenet classif with deep 
convolut neural network (krizhevsky, sutskever, & hinton, 2012), which 
achiev state-of-the-art result on the object recognit challeng know a imagenet 
(deng et al., ). other lab be alreadi work on similar work (cireşan, meier, masci, 
& schmidhuber, 2012). befor the year be out, deep learn make the front page of 
the new york time , and it rapidli becom the best know techniqu in artifici 2 
intelligence, by a wide margin. If the gener idea of train neural network with 
multipl layer be not new, it was, in part becaus of increas in comput power 
and data, the first time that deep learn truli becom practical. 

deep learn have sinc yield numer state of the art results, in domain such a 
speech recognition, imag recognit , and languag translat and play a role in a 
wide swath of current AI applications. corpor have invest billion of dollar 
fight for deep learn talent. one promin deep learn advocate, andrew ng, have 
go so far to suggest that “if a typic person can do a mental task with less than one 
second of thought, we can probabl autom it use AI either now or in the near 

http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-2 
intelligence.html 

page ! of !2 27 

http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html 
http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html 
http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html 
https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf 
https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf 


future.” (a, 2016). A recent new york time sunday magazin articl , larg about 3 
deep learning, impli that the techniqu be “pois to reinvent comput itself.” 

yet deep learn may well be approach a wall, much a I anticip earlier, at 
begin of the resurg (marcus, 2012), and a lead figur like hinton (sabour, 
frosst, & hinton, 2017) and chollet (2017) have begin to impli in recent months. 

what exactli be deep learning, and what have it show about the natur of intelligence? 
what can we expect it to do, and where might we expect it to break down? how close or 
far be we from “artifici gener intelligence”, and a point at which machin show a 
human-lik flexibl in solv unfamiliar problems? the purpos of thi paper be both 
to temper some irrat exuber and also to consid what we a a field might need 
to move forward. 

thi paper be write simultan for research in the field, and for a grow set of 
AI consum with less technic background who may wish to understand where the 
field be headed. As such I will begin with a veri brief, nontechn introduct aim at 4 
elucid what deep learn system do well and whi (section 2), befor turn to an 
assess of deep learning’ weak (section 3) and some fear that aris from 
misunderstand about deep learning’ capabl (section 4), and close with 
perspect on go forward (section 5). 

deep learn be not like to disappear, nor should it. but five year into the field’ 
resurg seem like a good moment for a critic reflection, on what deep learn have 
and have not be abl to achieve. 

2. what deep learn is, and what it do well 

deep learning, a it be primarili used, be essenti a statist techniqu for classifi 
patterns, base on sampl data, use neural network with multipl layers. 5 

https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html3 

for more technic introduction, there be mani excel recent tutori on deep learn includ (chollet, 4 
2017) and (goodfellow, bengio, & courville, 2016), a well a insight blog and onlin resourc from zachari 
lipton, chri olah, and mani others. 

other applic of deep learn beyond classif be possible, too, though current less popular, and 5 
outsid of the scope of the current article. these includ use deep learn a an altern to regression, a a 
compon in gener model that creat (e.g.,) synthet images, a a tool for compress images, a a tool for 
learn probabl distributions, and (relatedly) a an import techniqu for approxim know a variat 
inference. 

page ! of !3 27 



neural network in the deep learn literatur typic consist of a set of input unit that 
stand for thing like pixel or words, multipl hidden layer (the more such layers, the 
deeper a network be say to be) contain hidden unit (also know a node or neurons), 
and a set output units, with connect run between those nodes. In a typic 
applic such a network might be train on a larg set of handwritten digit (these 
be the inputs, repres a images) and label (these be the outputs) that identifi the 
categori to which those input belong (thi imag be a 2, that one be a 3, and so forth). 


over time, an algorithm call back-propag allow a process call gradient descent 
to adjust the connect between unit use a process, such that ani give input tend to 
produc the correspond output. 

collectively, one can think of the relat between input and output that a neural 
network learn a a mapping. neural networks, particularli those with multipl hidden 
layer (henc the term deep) be remark good at learn input-output mappings, 

such system be commonli describ a neural network becaus the input nodes, 
hidden nodes, and output node can be thought of a loos analog to biolog 
neurons, albeit greatli simplified, and the connect between node can be thought of 
a in some way reflect connect between neurons. A longstand question, outsid 
the scope of the current paper, concern the degre to which artifici neural network be 
biolog plausible. 

most deep learn network make heavi use of a techniqu call convolut (lecun, 
1989), which constrain the neural connect in the network such that they innat 
captur a properti know a translat invariance. thi be essenti the idea that an 
object can slide around an imag while maintain it identity; a circl in the top left can 
be presumed, even absent direct experience) to be the same a a circl in the bottom right. 

page ! of !4 27 

input layer output layer 
hidden layer 

... 
... ... 

... 



deep learn be also know for it abil to self-gener intermedi representations, 
such a intern unit that may respond to thing like horizont lines, or more complex 
element of pictori structure. 

In principle, give infinit data, deep learn system be power enough to repres 
ani finit determinist “mapping” between ani give set of input and a set of 
correspond outputs, though in practic whether they can learn such a map 
depend on mani factors. one common concern be get caught in local minima, in 
which a system get stuck on a suboptim solution, with no good solut nearbi in the 
space of solut be searched. (expert use a varieti of techniqu to avoid such 
problems, to reason good effect). In practice, result with larg data set be often 
quit good, on a wide rang of potenti mappings. 

In speech recognition, for example, a neural network learn a map between a set of 
speech sounds, and set of label (such a word or phonemes). In object recognition, a 
neural network learn a map between a set of imag and a set of label (such that, 
for example, pictur of car be label a cars). In deepmind’ atari game system 
(mnih et al., 2015), neural network learn map between pixel and joystick 
positions. 

deep learn system be most often use a classif system in the sens that the 
mission of a typic network be to decid which of a set of categori (defin by the 
output unit on the neural network) a give input belong to. with enough imagination, 
the power of classif be immense; output can repres words, place on a Go 
board, or virtual anyth else. 

In a world with infinit data, and infinit comput resources, there might be littl 
need for ani other technique. 

3. limit on the scope of deep learn 

deep learning’ limit begin with the contrapositive: we live in a world in which 
data be never infinite. instead, system that reli on deep learn frequent have to 
gener beyond the specif data that they have seen, whether to a new pronunci 
of a word or to an imag that differ from one that the system have see before, and where 
data be less than infinite, the abil of formal proof to guarante high-qual 
perform be more limited. 

page ! of !5 27 



As discuss late in thi article, gener can be thought of a come in two 
flavors, interpol between know examples, and extrapolation, which requir go 
beyond a space of know train exampl (marcus, 1998a). 

for neural network to gener well, there gener must be a larg amount of data, 
and the test data must be similar to the train data, allow new answer to be 
interpol in between old ones. In krizhevski et al’ paper (krizhevsky, sutskever, & 
hinton, 2012), a nine layer convolut neural network with 60 million paramet and 
650,000 node be train on roughli a million distinct exampl drawn from 
approxim one thousand categories. 6 

thi sort of brute forc approach work well in the veri finit world of imagenet, into 
which all stimulu can be classifi into a compar small set of categories. It also 
work well in stabl domain like speech recognit in which exemplar be map in 
constant way onto a limit set of speech sound categories, but for mani reason deep 
learn cannot be consid (a it sometim be in the popular press) a a gener 
solut to artifici intelligence. 

here be ten challeng face by current deep learn systems: 

3.1. deep learn thu far be data hungri 

human be can learn abstract relationship in a few trials. If I told you that a schmister 
be a sister over the age of 10 but under the age of 21, perhap give you a singl 
example, you could immedi infer whether you have ani schmisters, whether your best 
friend have a schmister, whether your child or parent have ani schmisters, and so forth. 
(odd are, your parent no longer do, if they ever did, and you could rapidli draw that 
inference, too.) 

In learn what a schmister is, in thi case through explicit definition, you reli not on 
hundr or thousand or million of train examples, but on a capac to repres 
abstract relationship between algebra-lik variables. 

human can learn such abstractions, both through explicit definit and more implicit 
mean (marcus, 2001). inde even 7-month old infant can do so, acquir learn 
abstract language-lik rule from a small number of unlabel examples, in just two 

use a common techniqu know a data augmentation, each exampl be actual present along with it label 6 
in a mani differ locations, both in it origin form and in mirror revers form. A second type of data 
augment vari the bright of the images, yield still more exampl for training, in order to train the 
network to recogn imag with differ intensities. part of the art of machin learn involv know what 
form of data augment will and won’t help within a give system. 

page ! of !6 27 



minut (marcus, vijayan, bandi rao, & vishton, 1999). subsequ work by gervain 
and colleagu (2012) suggest that newborn be capabl of similar computations. 

deep learn current lack a mechan for learn abstract through explicit, 
verbal definition, and work best when there be thousands, million or even billion of 
train examples, a in deepmind’ work on board game and atari. As brenden lake 
and hi colleagu have recent emphas in a seri of papers, human be far more 
effici in learn complex rule than deep learn system be (lake, salakhutdinov, 
& tenenbaum, 2015; lake, ullman, tenenbaum, & gershman, 2016). (see also relat 
work by georg et al (2017), and my own work with steven pinker on children’ 
overregular error in comparison to neural network (marcu et al., 1992).) 

geoff hinton have also worri about deep learning’ relianc on larg number of label 
examples, and express thi concern in hi recent work on capsul network with hi 
coauthor (sabour et al., 2017) note that convolut neural network (the most 
common deep learn architecture) may face “exponenti ineffici that may lead to 
their demise. A good candid be the difficulti that convolut net have in 
gener to novel viewpoint [ie perspect on object in visual recognit tasks]. 
the abil to deal with translation[ invariance] be built in, but for the other ... [common 
type of] transform we have to chose between replic featur detector on a grid 
that grow exponenti ... or increas the size of the label train set in a similarli 
exponenti way.” 

In problem where data be limited, deep learn often be not an ideal solution. 

3.2.deep learn thu far be shallow and have limit capac for 
transfer 

although deep learn be capabl of some amaz things, it be import to realiz that 
the word “deep” in deep learn refer to a technical, architectur properti (the larg 
number of hidden layer use in a modern neural networks, where there predecessor 
use onli one) rather than a conceptu one (the represent acquir by such 
network don’t, for example, natur appli to abstract concept like “justice”, 
“democracy” or “meddling”). 

even more down-to-earth concept like “ball” or “opponent” can lie out of reach. 
consid for exampl deepmind’ atari game work (mnih et al., 2015) on deep 
reinforc learning, which combin deep learn with reinforc learn (in 
which a learner tri to maxim reward). ostensibly, the result be fantastic: the system 
meet or beat human expert on a larg sampl of game use a singl set of 
“hyperparameters” that govern properti such a the rate at which a network alter it 
weights, and no advanc knowledg about specif games, or even their rules. but it be 

page ! of !7 27 



easi to wildli overinterpret what the result show. To take one example, accord to a 
widely-circul video of the system learn to play the brick-break atari game 
breakout, “after 240 minut of training, [the system] realiz that dig a tunnel 
thought the wall be the most effect techniqu to beat the game”. 

but the system have learn no such thing; it doesn’t realli understand what a tunnel, or 
what a wall is; it have just learn specif conting for particular scenarios. transfer 
test — in which the deep reinforc learn system be confront with scenario 
that differ in minor way from the one one on which the system be train show that 
deep reinforc learning’ solut be often extrem superficial. for example, a 
team of research at vicari show that a more effici successor technique, 
deepmind’ atari system [asynchron advantag actor-critic; also know a a3c], 
fail on a varieti of minor perturb to breakout (kanski et al., 2017) from the 
train set, such a move the Y coordin (height) of the paddle, or insert a wall 
midscreen. these demonstr make clear that it be mislead to credit deep 
reinforc learn with induc concept like wall or paddle; rather, such remark 
be what compar (animal) psycholog sometim call overattributions. it’ not that 
the atari system genuin learn a concept of wall that be robust but rather the system 
superfici approxim break through wall within a narrow set of highli train 
circumstances. 7 

My own team of research at a startup compani call geometr intellig (later 
acquir by uber) found similar result a well, in the context of a slalom game, In 2017, 
a team of research at berkeley and openai have show that it be not difficult to 
construct compar adversari exampl in a varieti of games, undermin not onli 
dqn (the origin deepmind algorithm) but also a3c and sever other relat 
techniqu (huang, papernot, goodfellow, duan, & abbeel, 2017). 

recent experi by robin jia and perci liang (2017) make a similar point, in a 
differ domain: language. variou neural network be train on a question 
answer task know a squad (deriv from the stanford question answer 
database), in which the goal be to highlight the word in a particular passag that 
correspond to a give question. In one sample, for instance, a train system correctly, 
and impressively, identifi the quarterback on the win of super bowl xxxiii a 
john elway, base on a short paragraph. but jia and liang show the mere insert of 
distractor sentenc (such a a fiction one about the alleg victori of google’ jeff 

In the same paper, vicari propos an altern to deep learn call schema network (kanski et al., 2017) 7 
that can handl a number of variat in the atari game breakout, albeit appar without the multi-gam 
gener of deepmind’ atari system. 

page ! of !8 27 



dean in anoth bowl game ) caus perform to drop precipitously. across sixteen 8 
models, accuraci drop from a mean of 75% to a mean of 36%. 

As be so often the case, the pattern extract by deep learn be more superfici than 
they initi appear. 

3.3.deep learn thu far have no natur way to deal with 
hierarch structur 

To a linguist like noam chomsky, the troubl jia and liang document would be 
unsurprising. fundamentally, most current deep-learn base languag model 
repres sentenc a mere sequenc of words, wherea chomski have long argu that 
languag have a hierarch structure, in which larg structur be recurs 
construct out of small components. (for example, in the sentenc the teenag who 
previous cross the atlant set a record for fli around the world, the main claus be 
the teenag set a record for fli around the world, while the emb claus who 
previous cross the atlant be an emb claus that specifi which teenager.) 

In the 80’ fodor and pylyshyn (1988)express similar concerns, with respect to an 
earli breed of neural networks. likewise, in (marcus, 2001), I conjectur that singl 
recurr neural network (srns; a forerunn to today’ more sophist deep 
learn base recurr neural networks, know a rnns; elman, 1990) would have 
troubl systemat repres and extend recurs structur to variou kind of 
unfamiliar sentenc (see the cite articl for more specif claim about which types). 

earlier thi year, brenden lake and marco baroni (2017) test whether such pessimist 
conjectur continu to hold true. As they put it in their title, contemporari neural net 
be “still not systemat after all these years”. rnn could “gener well when the 
differ between train and test ... be small [but] when gener requir 
systemat composit skills, rnn fail spectacularly”. 

similar issu be like to emerg in other domains, such a plan and motor control, 
in which complex hierarch structur be needed, particular when a system be like to 
encount novel situations. one can see indirect evid for thi in the struggl with 
transfer in atari game mention above, and more gener in the field of robotics, in 
which system gener fail to gener abstract plan well in novel environments. 

here’ the full super bowl passage; jia and liang’ distractor sentenc that confus the model be at the end. 8 
peyton man becom the first quarterback ever to lead two differ team to multipl super bowls. He be also 
the old quarterback ever to play in a super bowl at age 39. the past record be held by john elway, who lead the 
bronco to victori in super bowl xxxiii at age 38 and be current denver’ execut vice presid of footbal 
oper and gener manager. quarterback jeff dean have jersey number 37 in champ bowl xxxiv. 

page ! of !9 27 



the core problem, at least at present, be that deep learn learn correl between 
set of featur that be themselv “flat” or nonhierachical, a if in a simple, unstructur 
list, with everi featur on equal footing. hierarch structur (e.g., syntact tree that 
distinguish between main claus and emb claus in a sentence) be not inher 
or directli repres in such systems, and a a result deep learn system be forc 
to use a varieti of proxi that be ultim inadequate, such a the sequenti posit 
of a word present in a sequences. 

system like word2vec (mikolov, chen, corrado, & dean, 2013) that repres 
individu word a vector have be modestli successful; a number of system that 
have use clever trick 
tri to repres complet sentenc in deep-learn compat vector space (socher, 
huval, manning, & ng, 2012). but, a lake and baroni’ experi make clear. 
recurr network continu limit in their capac to repres and gener rich 
structur in a faith manner. 

3.4.deep learn thu far have struggl with open-end infer 
If you can’t repres nuanc like the differ between “john promis mari to leave” 
and “john promis to leav mary”, you can’t draw infer about who be leav 
whom, or what be like to happen next. current machin read system have achiev 
some degre of success in task like squad, in which the answer to a give 
question be explicitli contain within a text, but far less success in task in which 
infer go beyond what be explicit in a text, either by combin multipl sentenc 
(so call multi-hop inference) or by combin explicit sentenc with background 
knowledg that be not state in a specif text selection. humans, a they read texts, 
frequent deriv wide-rang infer that be both novel and onli implicitli 
licensed, a when they, for example, infer the intent of a charact base onli on 
indirect dialog. 

altough bowman and colleagu (bowman, angeli, potts, & manning, 2015; williams, 
nangia, & bowman, 2017) have take some import step in thi direction, there is, at 
present, no deep learn system that can draw open-end infer base on real- 
world knowledg with anyth like human-level accuracy. 

3.5.deep learn thu far be not suffici transpar 
the rel opac of “black box” neural network have be a major focu of discuss 
in the last few year (samek, wiegand, & müller, 2017; ribeiro, singh, & guestrin, 
2016). In their current incarnation, deep learn system have million or even billion 
of parameters, identifi to their develop not in term of the sort of human 

page ! of !10 27 



interpret label that canon programm use (“last_character_typed”) but onli in 
term of their geographi within a complex network (e.g., the activ valu of the ith node 
in layer j in network modul k). although some stride have be in visual the 
contribut of individu node in complex network (nguyen, clune, bengio, 
dosovitskiy, & yosinski, 2016), most observ would acknowledg that neural network 
a a whole remain someth of a black box. 

how much that matter in the long run remain unclear (lipton, 2016). If system be 
robust and self-contain enough it might not matter; if it be import to use them in the 
context of larg systems, it could be crucial for debuggability. 

the transpar issue, a yet unsolved, be a potenti liabil when use deep learn 
for problem domain like financi trade or medic diagnosis, in which human user 
might like to understand how a give system make a give decision. As catherin 
o’neil (2016) have point out, such opac can also lead to seriou issu of bias. 


3.6.deep learn thu far have not be well integr with prior 
knowledg 

the domin approach in deep learn be hermeneutic, in the sens of be self- 
contain and isol from other, potenti use knowledge. work in deep learn 
typic consist of find a train database, set of input associ with respect 
outputs, and learn all that be requir for the problem by learn the relat between 
those input and outputs, use whatev clever architectur variant one might devise, 
along with techniqu for clean and augment the data set. with just a hand of 
exceptions, such a lecun’ convolut constraint on how neural network be 
wired(lecun, 1989), prior knowledg be often deliber minimized. 

thus, for example, in a system like lerer et al’ (2016) effort to learn about the physic 
of fall towers, there be no prior knowledg of physic (beyond what be impli in 
convolution). newton’ laws, for example, be not explicitli encoded; the system instead 
(to some limit degree) approxim them by learn conting from raw, pixel 
level data. As I note in a forthcom paper in innat (marcus, in prep) research in 
deep learn appear to have a veri strong bia against includ prior knowledg even 
when (a in the case of physics) that prior knowledg be well known. 

It also not straightforward in gener how to integr prior knowledg into a deep 
learn system:, in part becaus the knowledg repres in deep learn system 
pertain mainli to (larg opaque) correl between features, rather than to 
abstract like quantifi statement (e.g. all men be mortal), see discuss of 
universally-quantifi one-to-one-map in marcu (2001), or gener (violabl 

page ! of !11 27 



statement like dog have four leg or mosquito carri west nile viru (gelman, leslie, 
was, & koch, 2015)). 

A relat problem stem from a cultur in machin learn that emphas competit 
on problem that be inher self-contained, without littl need for broad gener 
knowledge. thi tendenc be well exemplifi by the machin learn contest platform 
know a kaggle, in which contest vie for the best result on a give data set. 
everyth they need for a give problem be neatli packaged, with all the relev input 
and output files. great progress have be make in thi way; speech recognit and some 
aspect of imag recognit can be larg solv in the kaggl paradigm. 

the trouble, however, be that life be not a kaggl competition; child don’t get all the 
data they need neatli packag in a singl directory. real-world learn offer data 
much more sporadically, and problem aren’t so neatli encapsulated. deep learn 
work great on problem like speech recognit in which there be lot of label 
examples, but scarc ani even know how to appli it to more open-end problems. 
what’ the best way to fix a bicycl that have a rope caught in it spokes? should I major 
in math or neuroscience? No train set will tell u that. 

problem that have less to do with categor and more to do with commonsens 
reason essenti lie outsid the scope of what deep learn be appropri for, and so 
far a I can tell, deep learn have littl to offer such problems. In a recent review of 
commonsens reasoning, erni davi and I (2015) begin with a set of easily-drawn 
infer that peopl can readili answer without anyth like direct training, such a 
who be taller, princ william or hi babi son princ george? can you make a salad out 
of a polyest shirt? If you stick a pin into a carrot, do it make a hole in the carrot or in the 
pin? 

As far a I know, nobodi have even tri to tackl thi sort of thing with deep learning. 

such appar simpl problem requir human to integr knowledg across vastli dispar 
sources, and a such be a long way from the sweet spot of deep learning-styl perceptu 
classification. instead, they be perhap best thought of a a sign that entir differ 
sort of tool be needed, along with deep learning, if we be to reach human-level 
cognit flexibility. 

3.7.deep learn thu far cannot inher distinguish causat 
from correl 

If it be a truism that causat do not equal correlation, the distinct between the two 
be also a seriou concern for deep learning. roughli speaking, deep learn learn 
complex correl between input and output features, but with no inher 

page ! of !12 27 



represent of causality. A deep learn system can easili learn that height and 
vocabulari are, across the popul a a whole, correlated, but less easili repres the 
way in which that correl deriv from growth and develop (kid get big a 
they learn more words, but that doesn’t mean that grow tall caus them to learn more 
words, nor that learn new word caus them to grow). causal have be central 
strand in some other approach to AI (pearl, 2000) but, perhap becaus deep learn be 
not gear toward such challenges, rel littl work within the deep learn 
tradit have tri to address it. 9 

3.8.deep learn presum a larg stabl world, in way that may 
be problemat 

the logic of deep learn be such that it be like to work best in highli stabl worlds, 
like the board game go, which have unvari rules, and less well in system such a 
polit and econom that be constantli changing. To the extent that deep learn be 
appli in task such a stock prediction, there be a good chanc that it will eventu 
face the fate of googl flu trends, which initi do a great job of predict 
epidemolog data on search trends, onli to complet miss thing like the peak of the 
2013 flu season (lazer, kennedy, king, & vespignani, 2014). 

3.9. deep learn thu far work well a an approximation, but it 
answer often cannot be fulli trust 

In part a a consequ of the other issu rais in thi section, deep learn system 
be quit good at some larg fraction of a give domain, yet easili fooled. 

An ever-grow array of paper have show thi vulnerability, from the linguist 
exampl of jia and liang mention abov to a wide rang of demonstr in the 
domain of vision, where deep learn system have mistaken yellow-and-black pattern 
of stripe for school bu (nguyen, yosinski, & clune, 2014) and sticker-clad park 
sign for well-stock refriger (vinyals, toshev, bengio, & erhan, 2014) in the 
context of a caption system that otherwis seem impressive. 

more recently, there have be real-world stop signs, lightli defaced, that have be 
mistaken for speed limit sign (evtimov et al., 2017) and 3d-print turtl that have be 
mistak for rifl (athalye, engstrom, ilyas, & kwok, 2017). A recent news stori 

one exampl of interest recent work be (lopez-paz, nishihara, chintala, schölkopf, & bottou, 2017), albeit 9 
focu specif on an rather unusu sens of the term causat a it relat to the presenc or absenc of 
object (e.g., “the presenc of car caus the presenc of wheel[s]). thi strike me a quit differ from the sort of 
causat one find in the relat between a diseas and the symptom it causes. 

page ! of !13 27 



recount the troubl a british polic system have have in distinguish nude from sand 
dunes. 10 

the “spoofability” of deep learn system be perhap first note by szegedi et 
al(2013). four year later, despit much activ research, no robust solut have be 
found. 11 

3.10. deep learn thu far be difficult to engin with 
anoth fact that follow from all the issu rais abov be that be simpli hard to do 
robust engin with deep learning. As a team of author at googl put it in 2014, in 
the titl of an important, and a yet unansw essay (sculley, phillips, ebner, 
chaudhary, & young, 2014), machin learn be “the high-interest credit card of 
technic debt”, mean that be compar easi to make system that work in some 
limit set of circumst (short term gain), but quit difficult to guarante that they 
will work in altern circumst with novel data that may not resembl previou 
train data (long term debt, particularli if one system be use a an element in anoth 
larg system). 

In an import talk at icml, leon bottou (2015) compar machin learn to the 
develop of an airplan engine, and note that while the airplan design reli on 
build complex system out of simpler system for which it be possibl to creat 
sound guarante about performance, machin learn lack the capac to produc 
compar guarantees. As google’ peter norvig (2016) have noted, machin learn a 
yet lack the incrementality, transpar and debugg of classic programming, 
trade off a kind of simplic for deep challeng in achiev robustness. 

henderson and colleagu have recent extend these points, with a focu on deep 
reinforc learning, note some seriou issu in the field relat to robust and 
replic (henderson et al., 2017). 

although there have be some progress in autom the process of develop machin 
learn system (zoph, vasudevan, shlens, & le, 2017), there be a long way to go. 

https://gizmodo.com/british-cops-want-to-use-ai-to-spot-porn-but-it-keeps-m-1821384511/amp10 

deep learning’ predecessor be vulner to similar problems, a pinker and princ (1988)point out, in a 11 
discuss of neural network that produc bizarr past tens form for a subset of it inputs. the verb to mail, for 
example, be inflect in the past tens a membled, the verb tour a toureder. children rare if ever make mistak 
like these. 

page ! of !14 27 



3.11. discuss 

Of course, deep learning, be by itself, just mathematics; none of the problem identifi 
abov be becaus the underli mathemat of deep learn be somehow flawed. In 
general, deep learn be a perfectli fine way of optim a complex system for 
repres a map between input and outputs, give a suffici larg data set. 

the real problem lie in misunderstand what deep learn is, and be not, good for. the 
techniqu excel at solv closed-end classif problems, in which a wide rang of 
potenti signal must be map onto a limit number of categories, give that there be 
enough data avail and the test set close resembl the train set. 

but deviat from these assumpt can caus problems; deep learn be just a 
statist technique, and all statist techniqu suffer from deviat from their 
assumptions. 

deep learn system work less well when there be limit amount of train data 
available, or when the test set differ importantli from the train set, or when the space 
of exampl be broad and fill with novelty. and some problem cannot, give real- 
world limitations, be thought of a classif problem at all. open-end natur 
languag understanding, for example, should not be thought of a a classifi map 
between a larg finit set of sentenc and large, finit set of sentences, but rather a 
map between a potenti infinit rang of input sentenc and an equal vast array 
of meanings, mani never previous encountered. In a problem like that, deep learn 
becom a squar peg slam into a round hole, a crude approxim when there 
must be a solut elsewhere. 

one clear way to get an intuit sens of whi someth be amiss to consid a set of 
experi I do long ago, in 1997, when I test some simplifi aspect of languag 
develop on a class of neural network that be then popular in cognit science. 
the 1997-vintag network were, to be sure, simpler than current model — they use no 
more than three layer (input node connect to hidden node connect to output 
node), and lack lecun’ power convolut technique. but they be driven by 
backpropag just a today’ system are, and just a beholden to their train data. 

In language, the name of the game be gener — onc I hear a sentenc like john 
pilk a footbal to mary, I can infer that be also grammat to say john pilk mari the 
football, and eliza pilk the ball to alec; equal if I can infer what the word pilk means, 
I can infer what the latter sentenc would mean, even if I have not hear them before. 

page ! of !15 27 



distil the broad-rang problem of languag down to a simpl exampl that I 
believ still have reson now, I ran a seri of experi in which I train three- 
layer perceptron (fulli connect in today’ technic parlance, with no convolution) on 
the ident function, f(x) = x, e.g, f(12)=12. 

train exampl be repres by a set of input node (and correspond output 
nodes) that repres number in term of binari digits. the number 7 for example, 
would be repres by turn on the input (and output) node repres 4, 2, and 1. 
As a test of generalization, I train the network on variou set of even numbers, and 
test it all possibl inputs, both odd and even. 

everi time I ran the experiment, use a wide varieti of parameters, the result be the 
same: the network would (unless it get stuck in local minimum) correctli appli the 
ident function to the even number that it have see befor (say 2, 4, 8 and 12), and to 
some other even number (say 6 and 14) but fail on all the odd numbers, yielding, for 
exampl f(15) = 14. 

In general, the neural net I test could learn their train examples, and interpol to a 
set of test exampl that be in a cloud of point around those exampl in n- 
dimension space (which I dub the train space), but they could not extrapol 
beyond that train space. 

odd number be outsid the train space, and the network could not gener 
ident outsid that space. ad more hidden unit didn’t help, and nor do add 12 
more hidden layers. simpl multilay perceptron simpli couldn’t gener outsid 
their train space (marcus, 1998a; marcus, 1998b; marcus, 2001). (chollet make quit 
similar point in the close chapter of hi hi (chollet, 2017) text.) 

what we have see in thi paper be that challeng in gener beyond a space of 
train exampl persist in current deep learn networks, nearli two decad later. 
mani of the problem review in thi paper — the data hungriness, the vulner to 
fooling, the problem in deal with open-end infer and transfer — can be see 
a extens of thi fundament problem. contemporari neural network do well on 
challeng that remain close to their core train data, but start to break down on case 
further out in the periphery. 

Of course, the network have never see an odd number before, but pretrain the network on odd number in a 12 
differ context didn’t help. and of cours people, in contrast, readili gener to novel word immedi upon 
hear them. likewise, the experi I do with seven-month-old consist entir of novel words. 

page ! of !16 27 



the widely-adopt addit of convolut guarante that one particular class of 
problem that be akin to my ident problem can be solved: so-cal translat 
invariances, in which an object retain it ident when it be shift to a location. but the 
solut be not general, a for exampl lake’ recent demonstr show. (data 
augment offer anoth way of deal with deep learning’ challeng in 
extrapolation, by tri to broaden the space of train exampl itself, but such 
techniqu be more use in 2d vision than in language). 

As yet there be no gener solut within deep learn to the problem of gener 
outsid the train space. and it be for that reason, more than ani other, that we need to 
look to differ kind of solut if we want to reach artifici gener intelligence. 

4. potenti risk of excess hype 

one of the big risk in the current overhyp of AI be anoth AI winter, such a the 
one that devast the field in the 1970’s, after the lighthil report (lighthill, 1973), 
suggest that AI be too brittle, too narrow and too superfici to be use in practice. 
although there be vastli more practic applic of AI now than there be in the 
1970s, hype be still a major concern. when a high-profil figur like andrew Ng write in 
the harvard busi review promis a degre of immin autom that be out of 
step with reality, there be fresh risk for serious dash expectations. machin cannot in 
fact do mani thing that ordinari human can do in a second, rang from reliabl 
comprehend the world to understand sentences. No healthi human be would 
ever mistak a turtl for a rifl or park sign for a refrigerator. 

execut invest massiv in AI may turn out to be disappointed, especi give 
the poor state of the art in natur languag understanding. already, some major project 
have be larg abandoned, like facebook’ M project, which be launch in august 
2015 with much public a a gener purpos person assistant, and then late 13 
downgrad to a significantli small role, help user with a vastli small rang of 
well-defin task such a calendar entry. 

It be probabl fair to say that chatbot in gener have not live up to the hype they 
receiv a coupl year ago. if, for example, driverless car should also, disappoint, 
rel to their earli hype, by prove unsaf when roll out at scale, or simpli not 
achiev full autonomi after mani promises, the whole field of AI could be in for a 
sharp downturn, both in popular and funding. We alreadi may be see hint of this, 

https://www.wired.com/2015/08/how-facebook-m-works/13 

page ! of !17 27 



a in a just publish wire articl that be entitl “after peak hype, self-driv car 14 
enter the trough of disillusionment.” 

there be other seriou fears, too, and not just of the apocalypt varieti (which for now 
to still seem to be stuff of scienc fiction). My own larg fear be that the field of AI 
could get trap in a local minimum, dwell too heavili in the wrong part of 
intellectu space, focu too much on the detail explor of a particular class of 
access but limit model that be gear around captur low-hang fruit — 
potenti neglect riskier excurs that might ultim lead to a more robust path. 

I be remind of peter thiel’ famou (if now slightli outdated) damn of an often 
too-narrowli focu tech industry: “we want fli cars, instead we get 140 
characters”. I still dream of rosi the robost, a full-servic domest robot that take of 
my home; but for now, six decad into the histori of ai, our bot do littl more than play 
music, sweep floors, and bid on advertisements. 

If didn’t make more progress, it would be a shame. AI come with risk, but also great 
potenti rewards. ai’ great contribut to society, I believe, could and should 
ultim come in domain like autom scientif discovery, lead among other 
thing toward vastli more sophist version of medicin than be current possible. 
but to get there we need to make sure that the field a whole doesn’t first get stuck in a 
local minimum. 

5. what would be better? 

despit all of the problem I have sketched, I don’t think that we need to abandon deep 
learning. 

rather, we need to reconceptu it: not a a univers solvent, but simpli a one tool 
among many, a power screwdriv in a world in which we also need hammers, wrenches, 
and pliers, not to mention chisel and drills, voltmeters, logic probes, and oscilloscopes. 

In perceptu classification, where vast amount of data be available, deep learn be a 
valuabl tool; in other, richer cognit domains, it be often far less satisfactory. 

the question is, where els should we look? here be four possibilities. 

https://www.wired.com/story/self-driving-cars-challenges/14 

page ! of !18 27 

https://www.wired.com/story/self-driving-cars-challenges/ 


5.1.unsupervis learn 

In interviews, deep learn pioneer geoff hinton and yann lecun have both recent 
point to unsupervis learn a one key way in which to go beyond supervised, data- 
hungri version of deep learning. 

To be clear, deep learn and unsupervis learn be not in logic opposition. deep 
learn have mostli be use in a supervis context with label data, but there be 
way of use deep learn in an unsupervis fashion. but there be certainli reason in 
mani domain to move away from the massiv demand on data that supervis deep 
learn typic requires. 

unsupervis learning, a the term be commonli used, tend to refer to sever kind of 
systems. one common type of system “clusters” togeth input that share properties, 
even without have them explicitli labeled. google’ cat detector model (le et al., 2012) 
be perhap the most publicli promin exampl of thi sort of approach. 

anoth approach, advoc research such a yann lecun (luc, neverova, couprie, 
verbeek, & lecun, 2017), and not mutual exclus with the first, be to replac label 
data set with thing like movi that chang over time. the intuit be that system 
train on video can use each pair of success frame a a kind of ersatz teach 
signal, in which the goal be to predict the next frame; frame t becom a predictor for 
frame t1, without the need for ani human labeling. 

My view be that both of these approach be use (and so be some other not discuss 
here), but that neither inher solv the sort of problem outlin in section 3. one be 
still left with data hungri system that lack explicit variables, and I see no advanc there 
toward open-end inference, interpret or debuggability. 

that said, there be a differ notion of unsupervis learning, less discussed, which I find 
deepli interesting: the kind of unsupervis learn that human child do. children 
often y set themselv a novel task, like creat a tower of lego brick or climb 
through a small aperture, a my daughter recent do in climb through a chair, in the 
space between the seat and the chair back . often, thi sort of exploratori problem 
solv involv (or at least appear to involve) a good deal of autonom goal set 
(what should I do?) and high level problem solv (how do I get my arm through the 
chair, now that the rest of my bodi have pass through?), a well the integr of 
abstract knowledg (how bodi work, what sort of apertur and afford variou 
object have, and so forth). If we could build system that could set their own goal and 
do reason and problem-solv at thi more abstract level, major progress might 
quickli follow. 

page ! of !19 27 



5.2.symbol-manipulation, and the need for hybrid model 

anoth place that we should look be toward classic, “symbolic” ai, sometim refer 
to a gofai (good old-fashion ai). symbol AI take it name from the idea, central 
to mathematics, logic, and comput science, that abstract can be repres by 
symbols. equat like f = ma allow u to calcul output for a wide rang of inputs, 
irrespect of whether we have see ani particular valu before; line in comput 
program do the same thing (if the valu of variabl x be great than the valu of variabl 
y, perform action a). 

By themselves, symbol system have often proven to be brittle, but they be larg 
develop in era with vastli less data and comput power than we have now. the 
right move today may be to integr deep learning, which excel at perceptu 
classification, with symbol systems, which excel at infer and abstraction. one 
might think such a potenti merger on analog to the brain; perceptu input systems, 
like primari sensori cortex, seem to do someth like what deep learn does, but there 
be other areas, like broca’ area and prefront cortex, that seem to oper at much 
high level of abstraction. the power and flexibl of the brain come in part from it 
capac to dynam integr mani differ comput in real-time. the process 
of scene perception, for instance, seamlessli integr direct sensori inform with 
complex abstract about object and their properties, light sources, and so forth. 

some tent step toward integr alreadi exist, includ neurosymbol 
model (besold et al., 2017) and recent trend toward system such a differenti 
neural comput (grave et al., 2016), program with differenti interpret 
(bošnjak, rocktäschel, naradowsky, & riedel, 2016), and neural program with 
discret oper (neelakantan, le, abadi, mccallum, & amodei, 2016). while none 
of thi work have yet fulli scale toward anyth like full-servic artifici gener 
intelligence, I have long argu (marcus, 2001) that more on integr microprocessor- 
like oper into neural network could be extrem valuable. 

To the extent that the brain might be see a consist of “a broad array of reusabl 
comput primitives—elementari unit of process akin to set of basic 
instruct in a microprocessor—perhap wire togeth in parallel, a in the 
reconfigur integr circuit type know a the field-programm gate array”, a I 
have argu elsewhere(marcus, marblestone, & dean, 2014), step toward enrich the 
instruct set out of which our comput system be built can onli be a good thing. 

page ! of !20 27 



5.3.more insight from cognit and development psycholog 

anoth potenti valuabl place to look be human cognit (davi & marcus, 2015; 
lake et al., 2016; marcus, 2001; pinker & prince, 1988). there be no need for machin 
to liter replic the human mind, which is, after all, deepli error prone, and far from 
perfect. but there remain mani areas, from natur languag understand to 
commonsens reasoning, in which human still retain a clear advantage; learn the 
mechan underli those human strength could lead to advanc in ai, even the 
goal be not, and should not be, an exact replica of human brain. 

for mani people, learn from human mean neuroscience; in my view, that may be 
premature. We don’t yet know enough about neurosci to liter revers engin the 
brain, per se, and may not for sever decades, possibl until AI itself get better. AI can 
help u to deciph the brain, rather than the other way around. 

either way, in the meantime, it should certainli be possibl to use techniqu and insight 
drawn from cognit and development and psychology, now, in order to build more 
robust and comprehens artifici intelligence, build model that be motiv not 
just by mathemat but also by clue from the strength of human psychology. 

A good start point might be to first to tri understand the innat machineri in human 
minds, a a sourc of hypothesi into mechan that might be valuabl in develop 
artifici intelligences; in companion articl to thi one (marcus, in prep) I summar a 
number of possibilities, some drawn from my own earli work (marcus, 2001) and 
other from elizabeth spelke’ (spelk & kinzler, 2007). those drawn from my own 
work focu on how inform might be repres and manipulated, such a by 
symbol mechan for repres variabl and distinct between kind and 
individu from a class; those drawn from spelk focu on how infant might repres 
notion such a space, time, and object. 

A second focal point might be on common sens knowledge, both in how it develop 
(some might be part of our innat endowment, much of it be learned), how it be 
represented, and how it be integr on line in the process of our interact with the 
real world (davi & marcus, 2015). recent work by lerer et al (2016), watter and 
colleagu (2017), tenenbaum and colleagues(wu, lu, kohli, freeman, & tenenbaum, 
2017) and davi and myself (davis, marcus, & frazier-logue, 2017) suggest some 
compet approach to how to think about this, within the domain of everyday physic 
reasoning. 

page ! of !21 27 



A third focu might be on human understand of narrative, a notion long ago suggest 
by roger schank and abelson (1977) and due for a refresh (marcus, 2014; kočiský et al., 
2017). 

5.4.bolder challeng 
whether deep learn persist in current form, morph into someth new, or get 
replac altogether, one might consid a varieti of challeng problem that push system 
to move beyond what can be learn in supervis learn paradigm with larg 
datasets. draw in part of from a recent special issu of AI magazin devot to 
move beyond the ture test that I edit with francesca rossi, manuelo veloso 
(marcus, rossi, veloso - AI magazine, & 2016, 2016), here be a few suggestions: 

• A comprehens challeng (paritosh & marcus, 2016; kočiský et al., 2017)] which 
would requir a system to watch an arbitrari video (or read a text, or listen to a 
podcast) and answer open-end question about what be contain therein. (who be the 
protagonist? what be their motivation? what will happen if the antagonist succeed in 
her mission?) No specif supervis train set can cover all the possibl 
contingencies; inferer and real-world knowledg integr be necessities. 

• scientif reason and understanding, a in the allen AI institute’ 8th grade scienc 
challeng (schoenick, clark, tafjord, P, & etzioni, 2017; davis, 2016). while the 
answer to mani basic scienc question can simpli be retriev from web searches, 
other requir infer beyond what be explicitli stated, and the integr of gener 
knowledge. 

• gener game play (genesereth, love, & pell, 2005), with transfer between game 
(kanski et al., 2017), such that, for example, learn about one first-person shooter 
enhanc perform on anoth with entir differ images, equip and so 
forth. (A system that can learn mani games, separately, without transfer between them, 
such a deepmind’ atari game system, would not qualify; the point be to acquir 
cumulative, transferr knowledge). 

• A physic embodi test an ai-driven robot that could build thing (ortiz jr, 2016), 
rang from tent to ikea shelves, base on instruct and real-world physic 
interact with the object parts, rather than vast amount trial-and-error. 

No one challeng be like to be sufficient. natur intellig be multi-dimension 
(gardner, 2011), and give the complex of the world, gener artifici intellig 
will necessarili be multi-dimension a well. 

By push beyond perceptu classif and into a broader integr of infer 
and knowledge, artifici intellig will advance, greatly. 

page ! of !22 27 



6. conclus 

As a measur of progress, it be worth consid a somewhat pessimist piec I write 
for the new yorker five year ago , conjectur that “deep learn be onli part of the 15 
larg challeng of build intellig machines” becaus “such techniqu lack way of 
repres causal relationship (such a between diseas and their symptoms), and be 
like to face challeng in acquir abstract idea like “sibling” or “ident to.” they 
have no obviou way of perform logic inferences, and they be also still a long way 
from integr abstract knowledge, such a inform about what object are, what 
they be for, and how they be typic used.” 

As we have seen, mani of these concern remain valid, despit major advanc in 
specif domain like speech recognition, machin translation, and board games, and 
despit equal impress advanc in infrastructur and the amount of data and comput 
available. 

intriguingly, in the last year, a grow array of other scholars, come from an 
impress rang of perspectives, have begin to emphas similar limits. A partial list 
includ brenden lake and marco baroni (2017), françoi chollet (2017), robin jia and 
perci liang (2017), dileep georg and other at vicari (kanski et al., 2017) and 
pieter abbeel and colleagu at berkeley (stoica et al., 2017). 

perhap most notabl of all, geoff hinton have be courag enough to reconsid have 
own beliefs, reveal in an august interview with the news site axio that he be 16 
“deepli suspicious” of back-propagation, a key enabl of deep learn that he help 
pioneer, becaus of hi concern about it depend on label data sets. 

instead, he suggest (in axios’ paraphrase) that “entir new method will probabl 
have to be invented.” 

I share hinton’ excit in see what come next. 

https://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence15 

https://www.axios.com/ai-pioneer-advocates-starting-over-2485537027.html16 

page ! of !23 27 

https://www.axios.com/ai-pioneer-advocates-starting-over-2485537027.html 


refer 
athalye, a., engstrom, l., ilyas, a., & kwok, K. (2017). synthes robust adversari 

examples. arxiv, cs.cv. 
besold, T. r., garcez, A. d., bader, s., bowman, h., domingos, p., hitzler, P. et al. (2017). 

neural-symbol learn and reasoning: A survey and interpretation. arxiv, cs.ai. 
bošnjak, m., rocktäschel, t., naradowsky, j., & riedel, S. (2016). program with a 

differenti forth interpreter. arxiv. 
bottou, L. (2015). two big challeng in machin learning. proceed from 32nd intern 

confer on machin learning. 
bowman, S. r., angeli, g., potts, c., & manning, C. D. (2015). A larg annot corpu for 

learn natur languag inference. arxiv, cs.cl. 
chollet, F. (2017). deep learn with python. man publications. 
cireşan, d., meier, u., masci, j., & schmidhuber, J. (2012). multi-column deep neural network 

for traffic sign classification. neural networks. 
davis, e., & marcus, G. (2015). commonsens reason and commonsens knowledg in 

artifici intelligence. commun of the acm, 58(9)(9), 92-103. 
davis, E. (2016). how to write scienc question that are easi for peopl and hard for 

computers. AI magazine, 37(1)(1), 13-22. 
davis, e., marcus, g., & frazier-logue, N. (2017). commonsens reason about contain 

use radic incomplet information. artifici intelligence, 248, 46-84. 
deng, j., dong, w., socher, r., li, L. j., Li - comput vision and, k., & 2009 imagenet: A 

large-scal hierarch imag database. proceed from comput vision and pattern 
recognition, 2009. cvpr 2009. ieee confer on. 

elman, J. L. (1990). find structur in time. cognit science, 14(2)(2), 179-211. 
evtimov, i., eykholt, k., fernandes, e., kohno, t., li, b., prakash, A. et al. (2017). robust 

physical-world attack on deep learn models. arxiv, cs.cr. 
fodor, J. a., & pylyshyn, Z. W. (1988). connection and cognit architecture: a critic 

analysis. cognition, 28(1-2)(1-2), 3-71. 
gardner, H. (2011). frame of mind: the theori of multipl intelligences. basic books. 
gelman, S. a., leslie, S. j., was, A. m., & koch, C. M. (2015). children’ interpret of 

gener quantifiers, specif quantifiers, and generics. lang cogn neurosci, 30(4)(4), 
448-461. 

genesereth, m., love, n., & pell, B. (2005). gener game playing: overview of the aaai 
competition. AI magazine, 26(2)(2), 62. 

george, d., lehrach, w., kansky, k., lázaro-gredilla, m., laan, c., marthi, B. et al. (2017). A 
gener vision model that train with high data effici and break text-bas 
captchas. science, 358(6368)(6368). 

gervain, j., berent, i., & werker, J. F. (2012). bind at birth: the newborn brain detect ident 
relat and sequenti posit in speech. J cogn neurosci, 24(3)(3), 564-574. 

goodfellow, i., bengio, y., & courville, A. (2016). deep learning. mit press. 

page ! of !24 27 



graves, a., wayne, g., reynolds, m., harley, t., danihelka, i., grabska-barwińska, A. et al. 
(2016). hybrid comput use a neural network with dynam extern memory. nature, 
538(7626)(7626), 471-476. 

henderson, p., islam, r., bachman, p., pineau, j., precup, d., & meger, D. (2017). deep 
reinforc learn that matters. arxiv, cs.lg. 

huang, s., papernot, n., goodfellow, i., duan, y., & abbeel, P. (2017). adversari attack on 
neural network policies. arxiv, cs.lg. 

jia, r., & liang, P. (2017). adversari exampl for evalu read comprehens 
systems. arxiv. 

kahneman, D. (2013). thinking, fast and slow (1st pbk. ed. ed.). new york: farrar, strau and 
giroux. 

kansky, k., silver, t., mély, D. a., eldawy, m., lázaro-gredilla, m., lou, X. et al. (2017). 
schema networks: zero-shot transfer with a gener causal model of intuit 
physics. arxiv, cs.ai. 

kočiský, t., schwarz, j., blunsom, p., dyer, c., hermann, K. m., melis, G. et al. (2017). the 
narrativeqa read comprehens challenge. arxiv, cs.cl. 

krizhevsky, a., sutskever, i., & hinton, G. E. (2012). imagenet classif with deep 
convolut neural networks. In (pp. 1097-1105). 

lake, B. m., salakhutdinov, r., & tenenbaum, J. B. (2015). human-level concept learn 
through probabilist program induction. science, 350(6266)(6266), 1332-1338. 

lake, B. m., ullman, T. d., tenenbaum, J. b., & gershman, S. J. (2016). build machin 
that learn and think like people. behav brain sci, 1-101. 

lake, B. m., & baroni, M. (2017). still not systemat after all these years: On the composit 
skill of sequence-to-sequ recurr networks. arxiv. 

lazer, d., kennedy, r., king, g., & vespignani, A. (2014). big data. the parabl of googl flu: 
trap in big data analysis. science, 343(6176)(6176), 1203-1205. 

le, Q. v., ranzato, m.-a., monga, r., devin, m., chen, k., corrado, G. et al. (2012). build 
high-level featur use larg scale unsupervis learning. proceed from intern 
confer on machin learning. 

lecun, Y. (1989). gener and network design strategies. technic report crg-tr-89-4. 
lerer, a., gross, s., & fergus, R. (2016). learn physic intuit of block tower by 

example. arxiv, cs.ai. 
lighthill, J. (1973). artifici intelligence: A gener survey. artifici intelligence: a paper 

symposium. 
lipton, Z. C. (2016). the mytho of model interpretability. arxiv, cs.lg. 
lopez-paz, d., nishihara, r., chintala, s., schölkopf, b., & bottou, L. (2017). discov 

causal signal in images. proceed from proceed of comput vision and pattern 
recognit (cvpr). 

luc, p., neverova, n., couprie, c., verbeek, j., & lecun, Y. (2017). predict deeper into the 
futur of semant segmentation. intern confer on comput vision (iccv 
2017). 

page ! of !25 27 



marcus, g., rossi, f., veloso - AI magazine, m., & 2016. (2016). beyond the ture test. AI 
magazine, whole issue. 

marcus, g., marblestone, a., & dean, T. (2014). the atom of neural computation. science, 
346(6209)(6209), 551-552. 

marcus, G. (in prep). innateness, alphazero, and artifici intelligence. 
marcus, G. (2014). what come after the ture test? the new yorker. 
marcus, G. (2012). Is “deep learning” a revolut in artifici intelligence? the new yorker. 
marcus, G. F. (2008). kluge : the haphazard construct of the human mind. boston: houghton 

mifflin. 
marcus, G. F. G. F. (2001). the algebra mind: integr connection and cognit 

science. cambridge, mass.: mit press. 
marcus, G. F. (1998a). rethink elimin connectionism. cogn psychol, 37(3)(3), 243-282. 
marcus, G. F. (1998b). can connection save constructivism? cognition, 66(2)(2), 153-182. 
marcus, G. f., pinker, s., ullman, m., hollander, m., rosen, T. j., & xu, F. (1992). 

overregular in languag acquisition. monogr soc re child dev, 57(4)(4), 1-182. 
marcus, G. f., vijayan, s., bandi rao, s., & vishton, P. M. (1999). rule learn by seven- 

month-old infants. science, 283(5398)(5398), 77-80. 
mikolov, t., chen, k., corrado, g., & dean, J. (2013). effici estim of word 

represent in vector space. arxiv. 
mnih, v., kavukcuoglu, k., silver, d., rusu, A. a., veness, j., bellemare, M. G. et al. (2015). 

human-level control through deep reinforc learning. nature, 518(7540)(7540), 
529-533. 

neelakantan, a., le, Q. v., abadi, m., mccallum, a., & amodei, D. (2016). learn a natur 
languag interfac with neural programmer. arxiv. 

ng, A. (2016). what artifici intellig can and can’t Do right now. harvard busi 
review. 

nguyen, a., clune, j., bengio, y., dosovitskiy, a., & yosinski, J. (2016). plug & play 
gener networks: condit iter gener of imag in latent space. arxiv, 
cs.cv. 

nguyen, a., yosinski, j., & clune, J. (2014). deep neural network be easili fooled: high 
confid predict for unrecogniz images. arxiv, cs.cv. 

norvig, P. (2016). state-of-the-art ai: build tomorrow’ intellig systems. proceed 
from emtech digital, san francisco. 

o’neil, C. (2016). weapon of math destruct : how big data increas inequ and threaten 
democracy. 

ortiz jr, C. L. (2016). whi we need a physic embodi ture test and what it might look 
like. AI magazine, 37(1)(1), 55-63. 

paritosh, p., & marcus, G. (2016). toward a comprehens challenge, use crowdsourc a a 
tool. AI magazine, 37(1)(1), 23-31. 

pearl, J. (2000). causal : models, reasoning, and infer /. cambridge, u.k.; new york : 
cambridg univers press. 

page ! of !26 27 



pinker, s., & prince, A. (1988). On languag and connectionism: analysi of a parallel distribut 
process model of languag acquisition. cognition, 28(1-2)(1-2), 73-193. 

ribeiro, M. t., singh, s., & guestrin, C. (2016). “whi should I trust you?”: explain the 
predict of ani classifier. arxiv, cs.lg. 

sabour, s., frosst, n., & hinton, G. E. (2017). dynam rout between capsules. arxiv, 
cs.cv. 

samek, w., wiegand, t., & müller, k.-r. (2017). explain artifici intelligence: 
understanding, visual and interpret deep learn models. arxiv, cs.ai. 

schank, R. c., & abelson, R. P. (1977). scripts, plans, goal and understanding: an inquiri into 
human knowledg structures. hillsdale, nj: L. erlbaum. 

schmidhuber, J. (2015). deep learn in neural networks: An overview. neural networks. 
schoenick, c., clark, p., tafjord, o., P, t., & etzioni, O. (2017). move beyond the ture test 

with the allen AI scienc challenge. commun of the acm, 60 (9)(9), 60-64. 
sculley, d., phillips, t., ebner, d., chaudhary, v., & young, M. (2014). machin learning: the 

high-interest credit card of technic debt. proceed from se4ml: softwar 
engin for machin learn (nip 2014 workshop). 

socher, r., huval, b., manning, C. d., & ng, A. Y. (2012). semant composition through 
recurs matrix-vector spaces. proceed from proceed of the 2012 joint confer 
on empir method in natur languag process and comput natur languag 
learning. 

spelke, E. s., & kinzler, K. D. (2007). core knowledge. dev sci, 10(1)(1), 89-96. 
stoica, i., song, d., popa, R. a., patterson, d., mahoney, M. w., katz, R. et al. (2017). A 

berkeley view of system challeng for ai. arxiv, cs.ai. 
szegedy, c., zaremba, w., sutskever, i., bruna, j., erhan, d., goodfellow, I. et al. (2013). 

intrigu properti of neural networks. arxiv, cs.cv. 
vinyals, o., toshev, a., bengio, s., & erhan, D. (2014). show and tell: A neural imag caption 

generator. arxiv, cs.cv. 
watters, n., tacchetti, a., weber, t., pascanu, r., battaglia, p., & zoran, D. (2017). visual 

interact networks. arxiv. 
williams, a., nangia, n., & bowman, S. R. (2017). A broad-coverag challeng corpu for 

sentenc understand through inference. arxiv, cs.cl. 
wu, j., lu, e., kohli, p., freeman, b., & tenenbaum, J. (2017). learn to see physic via 

visual de-animation. proceed from advanc in neural inform process 
systems. 

zoph, b., vasudevan, v., shlens, j., & le, Q. V. (2017). learn transfer architectur for 
scalabl imag recognition. arxiv, cs.cv. 

page ! of !27 27 


