




































Science Journals — AAAS 


RESEARCH ARTICLE 
◥ 

MACHINE LEARNING 

Neural scene representation 
and render 
S. M. Ali Eslami*†, Danilo Jimenez Rezende†, Frederic Besse, Fabio Viola, 
Ari S. Morcos, Marta Garnelo, Avraham Ruderman, Andrei A. Rusu, Ivo Danihelka, 
Karol Gregor, David P. Reichert, Lars Buesing, Theophane Weber, Oriol Vinyals, 
Dan Rosenbaum, Neil Rabinowitz, Helen King, Chloe Hillier, Matt Botvinick, 
Daan Wierstra, Koray Kavukcuoglu, Demis Hassabis 

Scene representation—the process of convert visual sensory data into concise 
descriptions—is a requirement for intelligent behavior. Recent work have show that neural 
network excel at this task when provide with large, label datasets. However, remove 
the reliance on human label remains an important open problem. To this end, we 
introduce the Generative Query Network (GQN), a framework within which machine learn 
to represent scene use only their own sensors. The GQN take a input image of a 
scene take from different viewpoints, construct an internal representation, and us this 
representation to predict the appearance of that scene from previously unobserved 
viewpoints. The GQN demonstrates representation learn without human label or 
domain knowledge, pave the way toward machine that autonomously learn to 
understand the world around them. 

M 
odern artificial vision system be base 
on deep neural network that consume 
large, label datasets to learn function 
that map image to human-generated 
scene descriptions. They do so by, for ex- 

ample, categorize the dominant object in the 
image (1), classify the scene type (2), detect 
object-bounding box (3), or label individual 
pixel into predetermine category (4, 5). In 
contrast, intelligent agent in the natural world 
appear to require little to no explicit supervision 
for perception (6). Higher mammals, include 
human infants, learn to form representation 
that support motor control, memory, planning, 
imagination, and rapid skill acquisition without 
any social communication, and generative pro- 

ce have be hypothesize to be instrumen- 
tal for this ability (7–10). It be thus desirable to 
create artificial system that learn to represent 
scene by model data [e.g., two-dimensional 
(2D) image and the agent’s position in space] 
that agent can directly obtain while processing 
the scene themselves, without recourse to se- 
mantic label (e.g., object classes, object loca- 
tions, scene types, or part labels) provide by a 
human (11). 
To that end, we present the Generative Query 

Network (GQN). In this framework, a an agent 
navigates a 3D scene i, it collect K image xki 
from 2D viewpoint vki , which we collectively 
refer to a it observationsoi ¼ fðxki ; vki Þgk¼1;…;K. 
The agent pass these observation to a GQN 

compose of two main parts: a representation 
network f and a generationnetwork g (Fig. 1). The 
representationnetwork take a input the agent’s 
observation and produce a neural scene rep- 
resentation r, which encodes information about 
the underlie scene (we omit scene subscript i 
where possible, for clarity). Each additional ob- 
servation accumulates further evidence about 
the content of the scene in the same represen- 
tation. The generation network then predicts 
the scene from an arbitrary query viewpoint vq, 
use stochastic latent variable z to create vari- 
ability in it output where necessary. The two 
network be train jointly, in an end-to-end 
fashion, to maximize the likelihood of generat- 
ing the ground-truth image that would be ob- 
serve from the query viewpoint. More formally, 
(i) r ¼ fqðoiÞ, (ii) the deep generation network 
g defines a probability density gqðxjvq; rÞ ¼ 
∫gqðx; zjvq; rÞdzof an image x be observe at 
query viewpoint vq for a scene representation r 
use latent variable z, and (iii) the learnable 
parameter be denote by q. Although the GQN 
training objective be intractable, owe to the 
presence of latent variables, we can employ var- 
iational approximation and optimize with sto- 
chastic gradient descent. 
The representation network be unaware of the 

viewpoint that the generation network will be 
query to predict. As a result, it will produce 
scene representation that contain all informa- 
tion (e.g., object identities, positions, colors, 
counts, and room layout) necessary for the gen- 
erator to make accurate image predictions. In 
other words, the GQN will learn by itself what 
these factor are, a well a how to extract them 
from pixels. Moreover, the generator internalizes 
any statistical regularity (e.g., typical color 
of the sky, a well a object shape regularity 
and symmetries, patterns, and textures) that 
be common across different scenes. This allows 

RESEARCH 

Eslami et al., Science 360, 1204–1210 (2018) 15 June 2018 1 of 7 

DeepMind, 5 New Street Square, London EC4A 3TW, UK. 
*Corresponding author. Email: aeslami@google.com 
†These author contribute equally to this work. 

Fig. 1. Schematic illustration of 
the Generative Query Network. 
(A) The agent observes training 
scene i from different viewpoint 

(in this example, from v1i , v 
2 
i , and v 

3 
i ). 

(B) The input to the representa- 
tion network f be observation 

make from viewpoint v1i and v 
2 
i , 

and the output be the scene repre- 
sentation r, which be obtain by 
element-wise sum of the 
observations’ representations. The 
generation network, a recurrent 
latent variable model, us the 
representation to predict what the 

scene would look like from a different viewpoint v3i . The generator can succeed only if r contains accurate and complete information about the content of 
the scene (e.g., the identities, positions, colors, and count of the objects, a well a the room’s colors). Training via back-propagation across many 
scenes, randomize the number of observations, lead to learn scene representation that capture this information in a concise manner. Only a handful 
of observation need to be record from any single scene to train the GQN. h1, h2,…hL be the L layer of the generation network. 

on June 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nloaded from 


http://science.sciencemag.org/ 


the GQN to reserve it representation capacity 
for a concise, abstract description of the scene, 
with the generator fill in the detail where 
necessary. For instance, instead of specify 
the precise shape of a robot arm, the represen- 
tation network can succinctly communicate the 
configuration of it joints, and the generator 
know how this high-level representation mani- 
fests itself a a fully render arm with it pre- 
cise shape and colors. In contrast, voxel (12–15) 
or point-cloud (16) method (as typically obtain 
by classical structure-from-motion) employ lit- 
eral representation and therefore typically scale 
poorly with scene complexity and size and be 
also difficult to apply to nonrigid object (e.g., 
animals, vegetation, or cloth). 

Rooms with multiple object 

To evaluate the feasibility of the framework, we 
experimentedwith a collection of environment 
in a simulated 3D environment. In the first set 

of experiments, we consider scene in a square 
roomcontaining a variety of objects.Wall textures— 
aswell a the shapes, positions, and color of the 
object and lights—are randomized, allow 
for an effectively infinite number of total scene 
configurations; however, we use finite datasets 
to train and test the model [see section 4 of (17) 
for details]. After training, the GQN computes 
it scene representation by observe one ormore 
image of a previously unencountered, held-out 
test scene. With this representation, which can 
be a small a 256 dimensions, the generator’s 
prediction at query viewpoint be highly accu- 
rate and mostly indistinguishable from ground 
truth (Fig. 2A). The onlyway inwhich themodel 
can succeed at this task be by perceive and com- 
pactly encode in the scene representation vector 
r the number of object present in each scene, 
their position in the room, the color in which 
they appear, the color of the walls, and the in- 
directly observe position of the light source. 

Unlike in traditional supervise learning, GQNs 
learn to make these inference from image 
without any explicit human label of the con- 
tent of scenes. Moreover, the GQN’s generator 
learns an approximate 3D renderer (in other 
words, a program that can generate an image 
when give a scene representation and camera 
viewpoint) without any prior specification of 
the law of perspective, occlusion, or light 
(Fig. 2B). When the content of the scene be 
not explicitly specify by the observation (e.g., 
because of heavy occlusion), the model’s un- 
certainty be reflect in the variability of the 
generator’s sample (Fig. 2C). These property 
be best observe in real-time, interactive query- 
ing of the generator (movie S1). 
Notably, themodel observes only a small num- 

ber of image (in this experiment, few than five) 
from each scene during training, yet it be capa- 
ble of render unseen training or test scene 
from arbitrary viewpoints. We also monitor 

Eslami et al., Science 360, 1204–1210 (2018) 15 June 2018 2 of 7 

Fig. 2. Neural scene representation and rendering. (A) After have 
make a single observation of a previously unencountered test scene, the 
representation network produce a neural description of that scene. 
Given this neural description, the generator be capable of predict accurate 
image from arbitrary query viewpoints. This implies that the scene 
description capture the identities, positions, colors, and count of the 
objects, a well a the position of the light and the color of the room. (B) The 
generator’s prediction be consistent with law of perspective, occlusion, 

and light (e.g., cast object shadow consistently).When observation 
provide view of different part of the scene, the GQN correctly aggregate 
this information (scenes two and three). (C) Sample variability indicates 
uncertainty over scene content (in this instance, owe to heavy occlusion). 
Samples depict plausible scenes, with complete object render in 
vary position and color (see fig. S7 for further examples).The model’s 
behavior be best visualize in movie format; see movie S1 for real-time, 
interactive query of GQN’s representation of test scenes. 

RESEARCH | RESEARCH ARTICLE 
on June 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nloaded from 


http://science.sciencemag.org/ 


the likelihood of predict observation of train- 
ing and test scene (fig. S3) and found no notice- 
able difference between value of the two. Taken 
together, these point rule out the possibility of 
model overfitting. 
Analysis of the train GQN highlight sev- 

eral desirable property of it scene represen- 
tation network. Two-dimensional t-distributed 
stochastic neighbor embed (t-SNE) (18) vi- 
sualization of GQN scene representation vector 
show clear cluster of image of the same 
scene, despite marked change in viewpoint 
(Fig. 3A). In contrast, representation produce 
by autoencoding density model such a var- 
iational autoencoders (VAEs) (19) apparently 
fail to capture the content of the underlie 
scene [section 5 of (17)]; they appear to be rep- 
resentations of the observe image instead. 
Furthermore, when prompt to reconstruct a 
target image, GQN exhibit compositional be- 
havior, a it be capable of both represent and 
render combination of scene element it have 

never encounter during training (Fig. 3B), de- 
spite learn that these composition be un- 
likely. To test whether the GQN learns a factorize 
representation, we investigate whether chang- 
ing a single scene property (e.g., object color) 
while keep others (e.g., object shape and po- 
sition) fix lead to similar change in the 
scene representation (as define by mean cosine 
similarity across scenes). We found that object 
color, shape, and size; light position; and, to a 
lesser extent, object position be indeed factor- 
ized [Fig. 3C and section 5.3 and 5.4 of (17)]. 
We also found that the GQN be able to carry out 
“scene algebra” [akin to word embed algebra 
(20)]. By add and subtract representation 
of related scenes, we found that object and scene 
property can be controlled, even across object 
position [Fig. 4A and section 5.5 of (17)]. Finally, 
because it be a probabilistic model, GQN also 
learns to integrate information from different 
viewpoint in an efficient and consistent manner, 
a demonstrate by a reduction in it Bayesian 

“surprise” at observe a held-out image of a 
scene a the number of view increase [Fig. 4B 
and section 3 of (17)]. We include analysis on the 
GQN’s ability to generalize to out-of-distribution 
scenes, a well a further result on model 
of Shepard-Metzler objects, in section 5.6 and 
4.2 of (17). 

Control of a robotic arm 

Representations that succinctly reflect the true 
state of the environment should also allow agent 
to learn to act in those environment more ro- 
bustly and with few interactions. Therefore, we 
consider the canonical task of move a robotic 
arm to reach a color object, to test the GQN 
representation’s suitability for control. The end- 
goal of deep reinforcement learn be to learn 
the control policy directly from pixels; however, 
such method require a large amount of expe- 
rience to learn from sparse rewards. Instead, we 
first train a GQN and use it to succinctly rep- 
resent the observations. A policy be then train 

Eslami et al., Science 360, 1204–1210 (2018) 15 June 2018 3 of 7 

Fig. 3. Viewpoint invariance, compositionality, and factorization of 
the learn scene representations. (A) t-SNE embeddings. t-SNE be a 
method for nonlinear dimensionality reduction that approximately preserve 
the metric property of the original high-dimensional data. Each dot 
represent a different view of a different scene, with color indicate scene 
identity.Whereas the VAE cluster image mostly on the basis of wall angles, 
GQN cluster image of the same scene, independent of view (scene 
representation compute from each image individually).Two scene with 

the same object (represented by asterisk and dagger symbols) but in 
different position be clearly separated. (B) Compositionality demonstrate 
by reconstruction of holdout shape-color combinations. (C) GQN factorizes 
object and scene property because the effect of change a specific 
property be similar across diverse scene (as define bymean cosine similarity 
of the change in the representation across scenes). For comparison, we 
plot chance factorization, a well a the factorization of the image-space and 
VAE representations. See section 5.3 of (17) for details. 

RESEARCH | RESEARCH ARTICLE 
on June 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nloaded from 


http://science.sciencemag.org/ 


to control the arm directly from these representa- 
tions. In this setting, the representation network 
must learn to communicate only the arm’s joint 
angles, the position and color of the object, and 
the color of the wall for the generator to be 
able to predict new views. Because this vector 
have much low dimensionality than the raw 
input images, we observe substantially more ro- 
bust and data-efficient policy learning, obtain 
convergence-level control performance with ap- 
proximately one-fourth a many interaction with 
the environment a a standard method use raw 

pixel [Fig. 5 and section 4.4 of (17)]. The 3D 
nature of the GQN representation allows u to 
train a policy from any viewpoint around the 
arm and be sufficiently stable to allow for arm- 
joint velocity control from a freely move camera. 

Partially observe maze environment 

Finally, we consideredmore complex, procedur- 
al maze-like environment to test GQN’s scale 
properties. Themazes consist of multiple room 
connect via corridors, and the layout of each 
maze and the color of the wall be randomize 

in each scene. In this setting, any single obser- 
vation provide a small amount of information 
about the current maze. As before, the training 
objective for GQN be to predict maze from new 
viewpoints, which be possible only if GQN suc- 
cessfully aggregate multiple observation to 
determine the maze layout (i.e., the wall and 
floor colors, the number of rooms, their position 
in space, and how they connect to one another 
via corridors). We observe that GQN be able to 
make correct prediction from new first-person 
viewpoint (Fig. 6A). We query the GQN’s 

Eslami et al., Science 360, 1204–1210 (2018) 15 June 2018 4 of 7 

Fig. 4. Scene algebra and Bayesian surprise. (A) Adding and subtract- 
ing representation of related scene enables control of object and scene 
property via “scene algebra” and indicates factorization of shapes, colors, 
and positions. Pred, prediction. (B) Bayesian surprise at a new observation 

after have make observation 1 to k for k = 1 to 5. When the model 
observes image that contain information about the layout of the scene, it 
surprise (defined a the Kullback-Leibler divergence between conditional 
prior and posterior) at observe the held-out image decreases. 

Fig. 5. GQN representation enables more robust and data-efficient 
control. (A) The goal be to learn to control a robotic arm to reach a 
randomly position color object. The control policy observes the 
scene from a fix or move camera (gray). We pretrain a GQN 
representation network by observe random configuration from random 
viewpoint inside a dome around the arm (light blue). (B) The GQN 
infers a scene representation that can accurately reconstruct the scene. 
(C) (Left) For a fix camera, an asynchronous advantage actor-critic 
reinforcement learn (RL) agent (44) learns to control the arm use 
roughly one-fourth a many experience when use the GQN representa- 
tion, a oppose to a standard method use raw pixel (lines correspond 

to different hyperparameters; same hyperparameters explore for both 
standard and GQN agents; both agent also receive viewpoint coordinate 
a inputs). The final performance achieve by learn from raw pixel 
can be slightly high for some hyperparameters, because some task- 
specific information might be lose when learn a compress represen- 
tation independently from the RL task a GQN does. (Right) The benefit 
of GQN be most pronounce when the policy network’s view on the 
scene move from frame to frame, suggest viewpoint invariance in 
it representation. We normalize score such that a random agent 
achieves 0 and an agent train on “oracle” ground-truth state information 
achieves 100. 

RESEARCH | RESEARCH ARTICLE 
on June 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nloaded from 


http://science.sciencemag.org/ 


representation more directly by training a sep- 
arate generator to predict a top-down view of the 
maze and found that it yield highly accurate 
prediction (Fig. 6B). Themodel’s uncertainty, a 
measure by the entropy of it first-person and 
top-down samples, decrease a more observa- 
tions aremade [Fig. 6B and section 3 of (17)]. After 
about only five observations, the GQN’s uncer- 
tainty disappears almost entirely. 

Related work 

GQN offer key advantage over prior work. 
Traditional structure-from-motion, structure- 

from-depth, and multiview geometry technique 
(12–16, 21) prescribe the way in which the 3D 
structure of the environment be represent 
(for instance, a point clouds, mesh clouds, or a 
collection of predefined primitives). GQN, by 
contrast, learns this representational space, al- 
low it to express the presence of textures, 
parts, objects, lights, and scene concisely and 
at a suitably high level of abstraction. Further- 
more, it neural formulation enables task-specific 
fine-tuning of the representation via back- 
propagation (e.g., via further supervise or re- 
inforced deep learning). 

Classical neural approach to this learn 
problem—e.g., autoencoding and density model 
(22–27)—are require to capture only the dis- 
tribution of observe images, and there be no 
explicit mechanism to encourage learn of how 
different view of the same 3D scene relate to 
one another. The expectation be that statistical 
compression principle will be sufficient to en- 
able network to discover the 3D structure of 
the environment; however, in practice, they 
fall short of achieve this kind of meaningful 
representation and instead focus on regular- 
ities of color and patch in the image space. 

Eslami et al., Science 360, 1204–1210 (2018) 15 June 2018 5 of 7 

Fig. 6. Partial observability and uncertainty. (A) The agent (GQN) 
record several observation of a previously unencountered test maze 
(indicated by gray triangles). It be then capable of accurately predict the 
image that would be observe at a query viewpoint (yellow triangle). It 
can accomplish this task only by aggregate information across multiple 
observations. (B) In the kth column, we condition GQN on observation 
1 to k and show GQN’s predict uncertainty, a well a two of GQN’s 

sample prediction of the top-down view of the maze. Predicted 
uncertainty be measure by compute the model’s Bayesian surprise at 
each location, average over three different head directions. The 
model’s uncertainty decrease a more observation be made. As the 
number of observation increases, the model predicts the top-down view 
with increase accuracy. See section 3 of (17), fig. S8, and movie S1 for 
further detail and results. nats, natural unit of information. 

RESEARCH | RESEARCH ARTICLE 
on June 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nloaded from 


http://science.sciencemag.org/ 


Viewpoint transformation network do explic- 
itly learn this relationship; however, they have 
thus far be nonprobabilistic and limited in 
scale—e.g., restrict to rotation around indi- 
vidual object for which a single view be suffi- 
cient for prediction (15, 28–33) or to small camera 
displacement between stereo camera (34–36). 
By employ state-of-the-art deep, iterative, 

latent variable density model (25), GQN be ca- 
pable of handle free agent movement around 
scene contain multiple objects. In addition, 
owe to it probabilistic formulation, GQN can 
account for uncertainty in it understand 
about a scene’s content in the face of severe 
occlusion and partial observability. Notably, 
the GQN framework be not specific to the par- 
ticular choice of architecture of the generation 
network, and alternative such a generative 
adversarial network (37) or autoregressive mod- 
el (38) could be employed. 
A closely related body of work be that of dis- 

criminative pose estimation (39–41), in which 
network be train to predict camera motion 
between consecutive frames. The GQN formu- 
lation be advantageous, a it allows for aggre- 
gation of information from multiple image of 
a scene (see maze experiments); it be explicitly 
probabilistic, allow for application such a 
exploration through Bayesian information gain; 
and, unlike the aforementioned method where 
scene representation and pose prediction be in- 
tertwined, the GQN architecture admits a clear 
architectural separation between the represen- 
tation and generation networks. The idea of 
pose estimation be complementary, however— 
the GQN can be augment with a second “gen- 
erator” that, give an image of a scene, predicts 
the viewpoint from which it be taken, provid- 
ing a new source of gradient with which to 
train the representation network. 

Outlook 

In this work, we have show that a single neural 
architecture can learn to perceive, interpret, and 
represent synthetic scene without any human 
label of the content of these scenes. It can 
also learn a powerful neural renderer that be 
capable of produce accurate and consistent 
image of scene from new query viewpoints. 
The GQN learns representation that adapt to 
and compactly capture the important detail of 
it environment (e.g., the positions, identities, 
and color ofmultiple objects; the configuration 
of the joint angle of a robot arm; and the layout 
of amaze), without any of these semantics be 
built into the architecture of the networks. GQN 
us analysis-by-synthesis to perform “inverse 
graphics,” but unlike exist method (42), 
which require problem-specific engineering in 
the design of their generators, GQN learns this 
behavior by itself and in a generally applicable 
manner. However, the result representation 
be no longer directly interpretable by humans. 
Our experiment have thus far be restrict 

to synthetic environment for three reasons: (i) a 
need for control analysis, (ii) limited availa- 
bility of suitable real datasets, and (iii) limitation 

of generative model with current hardware. 
Although the environment be relatively con- 
strain in term of their visual fidelity, they 
capture many of the fundamental difficulty of 
vision—namely, severe partial observability and 
occlusion—as well a the combinatorial, multi- 
object nature of scenes. As new source of data 
become available (41) and advance be make in 
generative model capability (37, 43), we 
expect to be able to investigate application of 
the GQN framework to image of naturalistic 
scenes. 
Total scene understand involves more than 

just representation of the scene’s 3D structure. In 
the future, it will be important to consider broader 
aspect of scene understanding—e.g., by query 
across both space and time for model of dy- 
namic and interactive scenes—as well a appli- 
cation in virtual and augment reality and 
exploration of simultaneous scene representation 
and localization of observations, which relates to 
the notion of simultaneous localization and map- 
ping in computer vision. 
Our work illustrates a powerful approach to 

machine learn of ground representation of 
physical scenes, a well a of the associate per- 
ception system that holistically extract these 
representation from images, pave the way 
toward fully unsupervised scene understand- 
ing, imagination, planning, and behavior. 

REFERENCES AND NOTES 

1. A. Krizhevsky, I. Sutskever, G. E. Hinton, in Advances in Neural 
Information Processing Systems 25 (NIPS 2012), F. Pereira, 
C. J. C. Burges, L. Bottou, K. Q. Weinberger, Eds. (Curran 
Associates, 2012), pp. 1097–1105. 

2. B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, A. Oliva, in Advances 
in Neural Information Processing Systems 27 (NIPS 2014), 
Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, 
K. Q. Weinberger, Eds. (Curran Associates, 2014), pp. 487–495. 

3. S. Ren, K. He, R. Girshick, J. Sun, in Advances in Neural 
Information Processing Systems 28 (NIPS 2015), C. Cortes, 
N. D. Lawrence, D. D. Lee, M. Sugiyama, R. Garnett, Eds. 
(Curran Associates, 2015), pp. 91–99. 

4. R. Girshick, J. Donahue, T. Darrell, J. Malik, in Proceedings of 
the 2014 IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR) (IEEE, 2014), pp. 580–587. 

5. M. C. Mozer, R. S. Zemel, M. Behrmann, in Advances in Neural 
Information Processing Systems 4 (NIPS 1991), J. E. Moody, 
S. J. Hanson, R. P. Lippmann, Eds. (Morgan-Kaufmann, 1992), 
pp. 436–443. 

6. J. Konorski, Science 160, 652–653 (1968). 
7. D. Marr, Vision: A Computational Investigation into the Human 

Representation and Processing of Visual Information 
(Henry Holt and Co., 1982). 

8. D. Hassabis, E. A. Maguire, Trends Cogn. Sci. 11, 299–306 
(2007). 

9. D. Kumaran, D. Hassabis, J. L. McClelland, Trends Cogn. Sci. 
20, 512–534 (2016). 

10. B. M. Lake, R. Salakhutdinov, J. B. Tenenbaum, Science 350, 
1332–1338 (2015). 

11. S. Becker, G. E. Hinton, Nature 355, 161–163 (1992). 
12. Z. Wu et al., in Proceedings of the 2015 IEEE Conference on 

Computer Vision and Pattern Recognition (CVPR) (IEEE, 2015), 
pp. 1912–1920. 

13. J. Wu, C. Zhang, T. Xue, W. Freeman, J. Tenenbaum, in Advances 
in Neural Information Processing Systems 29 (NIPS 2016), 
D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, R. Garnett, 
Eds. (Curran Associates, 2016), pp. 82–90. 

14. D. J. Rezende et al., in Advances in Neural Information 
Processing Systems 29 (NIPS 2016), D. D. Lee, M. Sugiyama, 
U. V. Luxburg, I. Guyon, R. Garnett, Eds. (Curran Associates, 
2016), pp. 4996–5004. 

15. X. Yan, J. Yang, E. Yumer, Y. Guo, H. Lee, in Advances in Neural 
Information Processing Systems 29 (NIPS 2016), D. D. Lee, 

M. Sugiyama, U. V. Luxburg, I. Guyon, R. Garnett, Eds. (Curran 
Associates, 2016), pp. 1696–1704. 

16. M. Pollefeys et al., Int. J. Comput. Vision 59, 207–232 (2004). 
17. See supplementary materials. 
18. L. van der Maaten, J. Mach. Learn. Res. 9, 2579–2605 (2008). 
19. I. Higgins et al., at International Conference on Learning 

Representations (ICLR) (2017). 
20. T. Mikolov et al., in Advances in Neural Information Processing 

Systems 26 (NIPS 2013), C. J. C. Burges, L. Bottou, M. Welling, 
Z. Ghahramani, K. Q. Weinberger, Eds. (Curran Associates, 2013), 
pp. 3111–3119. 

21. Y. Zhang, W. Xu, Y. Tong, K. Zhou, ACM Trans. Graph. 34, 159 
(2015). 

22. D. P. Kingma, M. Welling, arXiv:1312.6114 [stat.ML] 
(20 December 2013). 

23. D. J. Rezende, S. Mohamed, D. Wierstra, in Proceedings of the 
31st International Conference on Machine Learning (ICML 2014) 
(JMLR, 2014), vol. 32, pp. 1278–1286. 

24. I. Goodfellow et al., in Advances in Neural Information Processing 
Systems 27 (NIPS 2014), Z. Ghahramani, M. Welling, C. Cortes, 
N. D. Lawrence, K. Q. Weinberger, Eds. (Curran Associates, 2014), 
pp. 2672–2680. 

25. K. Gregor, F. Besse, D. J. Rezende, I. Danihelka, D. Wierstra, 
in Advances in Neural Information Processing Systems 29 (NIPS 
2016), D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, 
R. Garnett, Eds. (Curran Associates, 2016), pp. 3549–3557 

26. P. Vincent, H. Larochelle, Y. Bengio, P.-A. Manzagol, in 
Proceedings of the 25th International Conference on Machine 
Learning (ICML 2008) (ACM, 2008), pp. 1096–1103. 

27. P. Dayan, G. E. Hinton, R. M. Neal, R. S. Zemel, Neural Comput. 
7, 889–904 (1995). 

28. G. E. Hinton, A. Krizhevsky, S. D. Wang, in Proceedings of the 
21st International Conference on Artificial Neural Networks 
and Machine Learning (ICANN 2011), T. Honkela, W. Duch, 
M. Girolami, S. Kaski, Eds. (Lecture Notes in Computer Science 
Series, Springer, 2011), vol. 6791, pp. 44–51. 

29. C. B. Choy, D. Xu, J. Gwak, K. Chen, S. Savarese, in 
Proceedings of the 2016 European Conference on Computer 
Vision (ECCV) (Lecture Notes in Computer Science Series, 
Springer, 2016), vol. 1, pp. 628–644. 

30. M. Tatarchenko, A. Dosovitskiy, T. Brox, in Proceedings of the 
2016 European Conference on Computer Vision (ECCV) 
(Lecture Notes in Computer Science Series, Springer, 2016), 
vol. 9911, pp. 322–337. 

31. F. Anselmi et al., Theor. Comput. Sci. 633, 112–121 (2016). 
32. D. F. Fouhey, A. Gupta, A. Zisserman, in Proceedings of the 2016 

IEEE Conference on Computer Vision and Pattern Recognition 
(CVPR) (IEEE, 2016), pp. 1516–1524. 

33. A. Dosovitskiy, J. T. Springenberg, M. Tatarchenko, T. Brox, 
IEEE Trans. Pattern Anal. Mach. Intell. 39, 692–705 (2017). 

34. C. Godard, O. Mac Aodha, G. J. Brostow, in Proceedings of 
the 2017 IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR) (IEEE, 2017), pp. 6602–6611. 

35. T. Zhou, S. Tulsiani, W. Sun, J. Malik, A. A. Efros, in 
Proceedings of the 2016 European Conference on Computer 
Vision (ECCV) (Lecture Notes in Computer Science Series, 
Springer, 2016), pp. 286–301. 

36. J. Flynn, I. Neulander, J. Philbin, N. Snavely, in Proceedings of 
the 2016 IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR) (IEEE, 2016), pp. 5515–5524. 

37. T. Karras, T. Aila, S. Laine, J. Lehtinen, arXiv:1710.10196 [cs.NE] 
(27 October 2017). 

38. A. van den Oord et al., in Advances in Neural Information 
Processing Systems 29 (NIPS 2016), D. D. Lee, M. Sugiyama, 
U. V. Luxburg, I. Guyon, R. Garnett, Eds. (Curran Associates, 
2016), pp. 4790–4798. 

39. D. Jayaraman, K. Grauman, in Proceedings of the 2015 IEEE 
International Conference on Computer Vision (ICCV) (IEEE, 
2015), pp. 1413–1421. 

40. P. Agrawal, J. Carreira, J. Malik, arXiv:1505.01596 [cs.CV] 
(7 May 2015). 

41. A. R. Zamir et al., in Proceedings of the 2016 European 
Conference on Computer Vision (ECCV) (Lecture Notes in 
Computer Science Series, Springer, 2016), pp. 535–553. 

42. T. D. Kulkarni, P. Kohli, J. B. Tenenbaum, V. Mansinghka, 
in Proceedings of the 2015 IEEE Conference on Computer 
Vision and Pattern Recognition (CVPR) (IEEE, 2015), 
pp. 4390–4399. 

43. Q. Chen, V. Koltun, in Proceedings of the 2017 IEEE 
International Conference on Computer Vision (ICCV) (IEEE, 
2017), pp. 1511–1520. 

44. A. A. Rusu et al., arXiv:1610.04286 [cs.RO] (13 October 2016). 

Eslami et al., Science 360, 1204–1210 (2018) 15 June 2018 6 of 7 

RESEARCH | RESEARCH ARTICLE 
on June 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nloaded from 


https://arxiv.org/abs/1312.6114 
https://arxiv.org/abs/1710.10196 
https://arxiv.org/abs/1505.01596 
https://arxiv.org/abs/1610.04286 
http://science.sciencemag.org/ 


ACKNOWLEDGMENTS 
We thank M. Shanahan, A. Zisserman, P. Dayan, J. Leibo, 
P. Battaglia, and G. Wayne for helpful discussion and advice; 
G. Ostrovski, N. Heess, D. Zoran, V. Nair, and D. Silver for 
review the paper; K. Anderson for help create 
environments; and the rest of the DeepMind team for 
support and ideas. Funding: This research be fund by 
DeepMind. Author contributions: S.M.A.E. and D.J.R. conceive 
of the model. S.M.A.E., D.J.R., F.B., and F.V. design and 
implement the model, datasets, visualizations, figures, and 
videos. A.S.M. and A.R. design and perform analysis 

experiments. M.G. and A.A.R. perform robot arm experiments. 
I.D., D.P.R., O.V., and D.R. assist with maze navigation 
experiments. L.B. and T.W. assist with Shepard-Metzler 
experiments. H.K., C.H., K.G., M.B., D.W., N.R., K.K., and D.H. 
managed, advised, and contribute idea to the project. 
S.M.A.E. and D.J.R. write the paper. Competing interests: The 
author declare no compete financial interests. DeepMind 
have file a U.K. patent application (GP-201495-00-PCT) related 
to this work. Data and material availability: Datasets use 
in the experiment have be make available to download at 
https://github.com/deepmind/gqn-datasets. 

SUPPLEMENTARY MATERIALS 

www.sciencemag.org/content/360/6394/1204/suppl/DC1 
Supplementary Text 
Figs. S1 to S16 
Algorithms S1 to S3 
Table S1 
References (45–52) 
Movie S1 

29 November 2017; accepted 10 April 2018 
10.1126/science.aar6170 

Eslami et al., Science 360, 1204–1210 (2018) 15 June 2018 7 of 7 

RESEARCH | RESEARCH ARTICLE 
on June 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nloaded from 


https://github.com/deepmind/gqn-datasets 
http://www.sciencemag.org/content/360/6394/1204/suppl/DC1 
http://science.sciencemag.org/ 


Neural scene representation and render 

Hassabis 
Rosenbaum, Neil Rabinowitz, Helen King, Chloe Hillier, Matt Botvinick, Daan Wierstra, Koray Kavukcuoglu and Demis 
Andrei A. Rusu, Ivo Danihelka, Karol Gregor, David P. Reichert, Lars Buesing, Theophane Weber, Oriol Vinyals, Dan 
S. M. Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S. Morcos, Marta Garnelo, Avraham Ruderman, 

DOI: 10.1126/science.aar6170 
(6394), 1204-1210.360Science 

, this issue p. 1204Science 
the basis of this representation, the network predicts what the scene would look like from a new, arbitrary viewpoint. 
image take from different viewpoint and creates an abstract description of the scene, learn it essentials. Next, on 
dubbed the Generative Query Network (GQN), that have no need for such label data. Instead, the GQN first us 

developed an artificial vision system,et al.typically use million of image painstakingly label by humans. Eslami 
To train a computer to ''recognize'' element of a scene supply by it visual sensors, computer scientist 

A scene-internalizing computer program 

ARTICLE TOOLS http://science.sciencemag.org/content/360/6394/1204 

MATERIALS 
SUPPLEMENTARY http://science.sciencemag.org/content/suppl/2018/06/13/360.6394.1204.DC1 

CONTENT 
RELATED http://science.sciencemag.org/content/sci/360/6394/1188.full 

REFERENCES 

http://science.sciencemag.org/content/360/6394/1204#BIBL 
This article cite 15 articles, 3 of which you can access for free 

PERMISSIONS http://www.sciencemag.org/help/reprints-and-permissions 

Terms of ServiceUse of this article be subject to the 

be a register trademark of AAAS.Science 
licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. The title 
Science, 1200 New York Avenue NW, Washington, DC 20005. 2017 © The Authors, some right reserved; exclusive 

(print ISSN 0036-8075; online ISSN 1095-9203) be publish by the American Association for the Advancement ofScience 

on June 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nloaded from 


http://science.sciencemag.org/content/360/6394/1204 
http://science.sciencemag.org/content/suppl/2018/06/13/360.6394.1204.DC1 
http://science.sciencemag.org/content/sci/360/6394/1188.full 
http://science.sciencemag.org/content/360/6394/1204#BIBL 
http://www.sciencemag.org/help/reprints-and-permissions 
http://www.sciencemag.org/about/terms-service 
http://science.sciencemag.org/ 

