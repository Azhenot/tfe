


































































Mutual information, neural network and the renormalization group 


Articles 
https://doi.org/10.1038/s41567-018-0081-4 

1Institute for Theoretical Physics, ETH Zurich, Zurich, Switzerland. 2Racah Institute of Physics, Hebrew University of Jerusalem, Jerusalem, Israel. 
*e-mail: maciejk@ethz.ch 

Machine learn have be captivate public attention lately due to groundbreaking advance in automate translation, image and speech recognition1, game-playing2 and achiev- 
ing super-human performance in task in which human excel 
while more traditional algorithmic approach struggled3. The 
application of those technique in physic be very recent, initially 
leverage the trademark prowess of machine learn in classifica- 
tion and pattern recognition and apply them to classify phase 
of matter4–8, study amorphous materials9,10, or exploit the neural 
networks’ potential a efficient nonlinear approximators of arbitrary 
functions11,12 to introduce a new numerical simulation method for 
quantum systems13,14. However, the excite possibility of employ 
machine learn not a a numerical simulator, or a hypothesis tester, 
but a an integral part of the physical reason process be still largely 
unexplored and, give the stagger pace of progress in the field of 
artificial intelligence, of fundamental importance and promise. 

The renormalization group (RG) approach have be one of the 
conceptually most profound tool of theoretical physic since it 
inception. It underlies the seminal work on critical phenomena15, 
and the discovery of asymptotic freedom in quantum chromody- 
namics16, and of the Kosterlitz–Thouless phase transition17,18. The 
RG be not a monolith, but rather a conceptual framework compris- 
ing different techniques: real-space RG19, functional RG20 and den- 
sity matrix RG21, among others. While all of those scheme differ 
quite substantially in their details, style and applicability, there be 
an underlie physical intuition that encompasses all of them—the 
essence of RG lie in identify the ‘relevant’ degree of freedom 
and integrate out the ‘irrelevant’ one iteratively, thereby arrive 
at a universal, low-energy effective theory. However potent the RG 
idea, those relevant degree of freedom need to be identify first22,23. 
This be often a challenge conceptual step, particularly for strongly 
interact systems, and may involve a sequence of mathematical 
mapping to models, whose behaviour be good understood24,25. 

Here we introduce an artificial neural network algorithm itera- 
tively identify the physically relevant degree of freedom in 
a spatial region and perform an RG coarse-graining step. The 
input data be sample of the system configuration drawn from 

a Boltzmann distribution; no further knowledge about the micro- 
scopic detail of the system be provided. The internal parameter 
of the network, which ultimately encode the degree of freedom 
of interest at each step, be optimize (‘learned’, in neural network 
parlance) by a training algorithm base on evaluate real-space 
mutual information (RSMI) between spatially separate regions. 
We validate our approach by study the Ising and dimer model 
of classical statistical physic in two dimensions. We obtain the RG 
flow and extract the Ising critical exponent. The robustness of the 
RSMI algorithm to physically irrelevant noise be demonstrated. 

The identification of the important degree of freedom, and the 
ability to execute a real-space RG procedure19, have not only quanti- 
tative but also conceptual significance: it allows one to gain insight 
into the correct way of think about the problem at hand, raise 
the prospect that machine-learning technique may augment the 
scientific inquiry in a fundamental fashion. 

The RSMI algorithm 
Before go into more detail, let u provide a bird’s eye view of our 
method and results. We begin by phrasing the problem in probabi- 
listic/information-theoretic terms, a language also use in ref 26–30. 
To this end, we consider a small ‘visible’ spatial area V , which 
together with it environment E form the system X , and we define 
a particular conditional probability distribution ∣Λ H VP ( ), which 
describes how the relevant degree of freedom H (‘dubbed hiddens’) 
in V depend on both V and E . We then show that the sought-after 
conditional probability distribution be found by an algorithm maxi- 
mizing an information-theoretic quantity, the mutual information, 
and that this algorithm lends itself to a natural implementation 
use artificial neural networks. We describe how RG be practically 
perform by coarse-graining with respect to ∣Λ H VP ( ) and iterate 
the procedure. Finally, we provide a verification of our claim by 
consider two paradigmatic model of statistical physics: the Ising 
model—for which the RG procedure yield the famous Kadanoff 
block spins—and the dimer model, whose relevant degree of free- 
dom be much less trivial. We reconstruct the RG flow of the Ising 
model and extract the critical exponent. 

Mutual information, neural network and the 
renormalization group 
Maciej Koch-Janusz 1* and Zohar Ringel2 

Physical system differ in their microscopic detail often display strikingly similar behaviour when probed at macroscopic 
scales. Those universal properties, largely determine their physical characteristics, be reveal by the powerful renormal- 
ization group (RG) procedure, which systematically retains ‘slow’ degree of freedom and integrates out the rest. However, 
the important degree of freedom may be difficult to identify. Here we demonstrate a machine-learning algorithm capable of 
identify the relevant degree of freedom and execute RG step iteratively without any prior knowledge about the system. 
We introduce an artificial neural network base on a model-independent, information-theoretic characterization of a real-space 
RG procedure, which performs this task. We apply the algorithm to classical statistical physic problem in one and two dimen- 
sions. We demonstrate RG flow and extract the Ising critical exponent. Our result demonstrate that machine-learning tech- 
niques can extract abstract physical concept and consequently become an integral part of theory- and model-building. 

© 2018 Macmillan Publishers Limited, part of Springer Nature. All right reserved. 

NaTuRe PhySIcS | www.nature.com/naturephysics 

mailto:maciejk@ethz.ch 
http://orcid.org/0000-0002-2903-5202 
http://www.nature.com/naturephysics 


Articles NaTuRe PHysIcs 

Consider then a classical system of local degree of freedom 
= … ≡X x x x{ , , } { }N i1 , define by a Hamiltonian energy function 

H({xi}) and associate statistical probability ∝ β−XP( ) e xH({ })i , 
where β be the inverse temperature. Alternatively (and sufficiently 
for our purposes), the system be give by Monte Carlo sample of the 
equilibrium distribution XP( ). We denote a small spatial region of 
interest by ≡V v{ }i and the remainder of the system by ≡E e{ }i , so 
that =X V E( , ). We adopt a probabilistic point of view, and treat X E, 
and so on a random variables. Our goal be to extract the relevant 
degree of freedom H from V . 

‘Relevance’ be understood here in the follow way: the degree 
of freedom that RG capture govern the long-distance behaviour 
of the theory, and therefore the experimentally measurable physi- 
cal properties; they carry the most information about the system 
at large, a oppose to local fluctuations. We thus formally define 
the random variable H a a composite function of degree of free- 
dom in V maximize the ‘mutual information’ between H and the 
environment E . This definition, a we discus in the Supplementary 
Information, be related to the requirement that the effective coarse- 
grain Hamiltonian be compact and short-ranged, which be a con- 
dition any successful standard RG scheme should satisfy. As we also 
show, it be support by numerical results. 

Mutual information, denote by Iλ, measure the total amount of 
information about one random variable contain in the other9,10,31 
(thus, it be more general than correlation coefficients). It be give in 
our set by: 

∑=Λ Λ Λ 
Λ 

 

 
 

 

 
 

H E E H 
E H 

H E 
H E 

I P 
P 

P P 
( : ) ( , )log 

( , ) 
( ) ( ) (1) 

, 

The unknown distribution Λ E HP ( , ) and it marginalization 
Λ HP ( ), depend on a set of parameter Λ (which we keep generic 

at this point), be function of V EP( , ) and of ∣Λ H VP ( ), which be the 
central object of interest. 

Finding ∣Λ H VP ( ) that maximizes IΛ under certain constraint be 
a well-posed mathematical question and have a formal solution32. 

However, since the space of probability distribution grows expo- 
nentially with the number of local degree of freedom, it is, in 
practice, impossible to use without further assumption for any 
but the small physical systems. Our approach be to exploit the 
remarkable dimensionality reduction property of artificial neural 
networks11. We use restrict Boltzmann machine (RBMs), a class 
of probabilistic network well adapt to approximate arbitrary 
data probability distributions. An RBM be compose of two layer 
of nodes, the ‘visible’ layer, correspond to local degree of free- 
dom in our setting, and a ‘hidden’ layer. The interaction between 
the layer be define by an energy function ≡ θΘ V HE E ( , )a b, , = 
− ∑ b hj j j − ∑ a vi i i − θ∑ v hij i ij j, such that the joint probability distri- 
bution for a particular configuration of visible and hidden degree 
of freedom be give by a Boltzmann weight: 

=Θ 
− θV H 

Z 
V HP ( , ) 1 e (2)E ( , )a b, , 

where Z be the normalization. The goal of the network training be 
to find parameter θij (‘weights’ or ‘filters’) and ai,bi optimize a 
chosen objective function. 

Three distinct RBMs be used. Two be train a efficient 
approximators of the probability distribution V EP( , ) and VP( ), 
use the celebrate contrastive divergence (CD) algorithm33. Their 
train parameter be use by the third network (see Fig. 1b), 
which have a different objective: to find ∣Λ H VP ( ) maximize IΛ. To 
the end we introduce the real-space mutual information (RSMI) 
network, whose architecture be show in Fig. 1a. The hidden unit 
of RSMI correspond to coarse-grained variable H. 

The parameter λΛ = a b( , , )i j i 
j of the RSMI network be train 

by an iterative procedure. At each iteration, a Monte Carlo estimate 
of function Λ H EI ( : ) and it gradient be perform for the current 
value of parameter Λ. The gradient be then use to improve 
the value of weight in the next step, use a stochastic gradient 
descent procedure. 

The train weight Λ define the probability ∣Λ H VP ( ) of a 
Boltzmann form, which be use to generate MC sample of the coarse- 
grain system. Those, in turn, become input to the next iteration of 
the RSMI algorithm. The estimate of mutual information, weight of 
the train RBMs and set of generate MC sample at every RG step 
can be use to extract quantitative information about the system in 
the form of correlation functions, critical exponent and so on, a we 
show below and in the Supplementary Information. We also empha- 
size that the parameter Λ identify relevant degree of freedom be 
re-computed at every RG step. This potentially allows RSMI to capture 
the evolution of the degree of freedom along the RG flow34. 

Validation 
To validate our approach, we consider two important classical mod- 
el of statistical physics: the Ising model, whose coarse-grained 
degree of freedom resemble the original ones, and the fully packed 
dimer model, where they be entirely different. 

Ha 

b 

B 

P( ) 

CD CD RSMI 

PΛ(H∣ ) 
λ ji 

θ 

θ( ) 
), 

( ), 

P( 

Fig. 1 | The RSMI algorithm. a, The RSMI neural network architecture. The 
hidden layer H be directly couple to the visible layer V via the weight λi 

j 
(red arrows). However, the training algorithm for the weight estimate 
mutual information between H and the environment E . The buffer B be 
introduce to filter out local correlation within V (see Supplementary 
Information). b, The workflow of the algorithm. The CD-algorithm-trained 
RBMs learn to approximate probability distribution V EP( , ) and VP( ). Their 
final parameters, denote collectively by V EΘ( , ) and VΘ( ), be input for the 
main RSMI network learn to extract H V∣ΛP ( ) by maximize IΛ. The final 
weight λi 

j of the RSMI network identify the relevant degree of freedom. 
They be show in Figs. 2 and 4 for Ising and dimer problems. 

0 1 

0 

a b 

1 
–2.5 

0 

2.5 

0 1 

0 

1 

0 1 

0 

1 

0 1 

0 

1 

0 1 

0 

1 

Fig. 2 | The weight of the RSMI network train on the Ising model. 
Visualization of the weight of the RSMI network train on the Ising model 
for a visibile area V of 2 × 2 spins. The ANN couple strongly to area with 
large absolute value of the weights. a, The weight for Nh = 1 hidden neuron: 
the ANN discovers Kadanoff blocking. b, The weight for Nh = 4 hidden 
neurons: each neuron track one original spin. 

© 2018 Macmillan Publishers Limited, part of Springer Nature. All right reserved. 

NaTuRe PhySIcS | www.nature.com/naturephysics 

http://www.nature.com/naturephysics 


ArticlesNaTuRe PHysIcs 

The Ising Hamiltonian on a two-dimensional (2D) square lattice is: 

∑= 
⟨ ⟩ 

H s s (3) 
i j 

i jI 
, 

with si = ± 1 and the summation over near neighbours. Real-space 
RG of the Ising model proceeds by the block-spin construction19, 
whereby each 2 × 2 block of spin be coarse-grained into a single 
effective spin, whose orientation be decide by a ‘majority’ rule. 

The result of the RSMI algorithm train on Ising model sam- 
ples be show in Fig. 2. We vary the number of both hidden neu- 
ron Nh and the visible units, which be arrange in a 2D area V 
of size L × L (see Fig. 1a). For a four-spin area, the network indeed 
rediscovers the famous Kadanoff block-spin: Fig. 2a show a single 
hidden unit couple uniformly to four visible spin (that is, the ori- 
entation of the hidden unit be decide by the average magnetization 
in the area). Figure 2b be a trivial but important sanity check: give 
four hidden unit to extract relevant degree of freedom from an 
area of four spins, the network couple each hidden unit to a dif- 
ferent spin, a expected. In the Supplementary Information we also 
compare the weight for area V of different size, which be general- 
izations of the Kadanoff procedure to large blocks. 

We next study the dimer model, give by an entropy-only parti- 
tion function, which count the number of dimer covering of the 
lattice (that is, subset of edge such that every vertex be the endpoint 
of exactly one edge). Figure 3a show sample dimer configuration 
(and additional spin degree of freedom add to generate noise). 
This deceptively simple description hide non-trivial physics35 and 
correspondingly, the RG procedure for the dimer model be more 
subtle, since—in contrast to the Ising case—the correct degree of 
freedom to perform RG on be not dimers, but rather look like effec- 
tive local electric fields. This be reveal by a mathematical mapping 
to a ‘height field’ h (see Fig. 3a,b and ref. 36), whose gradient behave 
like electric fields. The continuum limit of the dimer model be give 
by the follow action: 

∫ ∫= ∇ ≡S h x h xx E x[ ] d ( ( )) d ( ) (4)dim 2 2 2 2 

and therefore the coarse-grained degree of freedom be low- 
momentum (Fourier) component of the electrical field Ex,Ey in the 
x and y directions. They correspond to ‘staggered’ dimer configura- 
tions show in Fig. 3a. 

Remarkably, the RSMI algorithm extract the local electric field 
from the dimer model sample without any knowledge of those 
mappings. In Fig. 4, the weight for Nh = 2 and Nh = 4 hidden neu- 
rons, for an 8 × 8 area (similar to Fig. 3a), be shown: the pattern 
of large negative (blue) weight couple strongly to a dimer pattern 

0 1 2 3 4 5 6 7 

0 

a 

b 

1 

2 

3 

4 

5 

6 

7 

0 

1 

2 

3 

4 

5 

6 

7 

0 

1 

2 

3 

4 

5 

6 

7 

0 

1 

2 

3 

4 

5 

6 

7 

0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 

0 1 2 3 4 5 6 7 

0 

1 

2 

3 

4 

5 

6 

7 

0 

1 

2 

3 

4 

5 

6 

7 

0 1 2 3 4 5 6 7 
–2 

–1 

0 

1 

2 

Fig. 4 | The weight of the RSMI network train on dimer model data. a, Nh = 2 hidden neuron for a visible area V of 8 × 8 spins. The two filter 
recognize Ey and Ex + Ey electrical fields, respectively (compare with dimer pattern in Fig. 3a). b, The train weight for Nh = 4 hidden neurons. 

0 

a 

b 

1 0 1 

2 233 

4 5 4 5 

77 66 

0 

0 0 

0 

1 

11 

1 

3 32 2 

3 32 2 

Fig. 3 | The dimer model. a, Two sample dimer configuration (blue links), 
correspond to the Ey and Ex electrical fields, respectively. The couple 
pair of additional spin degree of freedom on vertex and face of the 
lattice (wiggly lines) be decouple from the dimer and from each other. 
Their fluctuation constitute irrelevant noise. b, An example of mapping the 
dimer model to local electric fields. The so-called stagger configuration 
on the left map to uniform non-vanishing field in the vertical direction: 
⟨ ⟩ ≠E 0y . The ‘columnar’ configuration on the right produce both Ex and Ey 
that be zero on average (see ref. 36 for detail of the mapping). 

© 2018 Macmillan Publishers Limited, part of Springer Nature. All right reserved. 

NaTuRe PhySIcS | www.nature.com/naturephysics 

http://www.nature.com/naturephysics 


Articles NaTuRe PHysIcs 
correspond to local uniform Ey field (see left panel of Fig. 3a,b). 
The large positive (yellow) weight select an identical pattern, trans- 
lated by one link. The remain neuron extract linear superposi- 
tions Ex + Ey or Ex − Ey of the fields. 

To demonstrate the robustness of the RSMI, we add physically 
irrelevant noise, form nevertheless a pronounce pattern, which 
we model by additional spin degree of freedom, strongly couple 
(ferromagnetically) in pair (wiggly line in Fig. 3a). Decoupled 
from the dimers, and from other pairs, they form a trivial system, 
whose fluctuation be short-range noise on top of the dimer model. 
Vanishing weight (green in Fig. 4a,b) on site where pair of spin 
reside prove that RSMI discard their fluctuation a irrelevant for 
long-range physics, despite their regular pattern. 

Notably, the filter obtain use our approach for the dimer 
model, which match the analytical expectation, be orthogonal to 
those obtain use Kullback–Leibler divergence. As expand on 
in the Supplementary Information, this show that standard RBMs 
minimize the Kullback–Leibler divergence do not generally per- 
form RG, thereby contradict prior claims37. 

Finally, we demonstrate that by iterate the RSMI algorithm the 
qualitative insight into the nature of relevant degree of freedom 
give rise to quantitative results. To this end, we revisit the 2D Ising 
model that (contrary to the dimer model) exhibit a non-trivial crit- 
ical point at the temperature = + ∕ −T (log(1 2 ) 2)c 

1, separate the 
paramagnetic and ferromagnetic phases. We generate Monte Carlo 
sample of the system of size 128 × 128 at value T around the criti- 
cal point, and for each one we perform up to four RG steps, by com- 
put the Λ filter use RSMI, coarse-graining the system with 
respect to those filter (effectively halve the linear dimensions) 
and reiterate the procedure. In addition to the set of Monte Carlo 
configuration for the coarse-grained system, estimate of mutual 
information a well a the filter of the CD-trained RBMs be gener- 
ated and stored. The effective temperature T of the system at each 
RG step can be evaluate entirely intrinsically either from correla- 
tions or the mutual information, a discuss in the Supplementary 
Information. Using the RBM filters, spin–spin correlation (for 
instance, next-nearest neighbour) can be computed. By compare 
these with know analytical results38, an additional cross-check of 
the effective temperature can be obtained. 

In Fig. 5, the effective T be plot against ξ ξ∕log ( )2 128 , where ξ 
and ξ128 be the current and 128 × 128 systems’ correlation lengths, 
respectively (this have the meaning of an RG step for integer values). 
The RG flow of the 2D Ising model be recovered: system start 
with T < Tc flow towards ever-decreasing T (that is, an order 
state), while the one with T > Tc flow towards a paramagnet. In fact, 
the position of the critical point can be estimate with 1% accuracy 
just from the divergent flow. Furthermore, we evaluate the correla- 
tion length exponent ν, define by ξ ∝ τ−ν. Using the finite-size data 
collapse (see Supplementary Fig. 4), it value, equal to the negative 
slope, be estimate to be ν ≈ 1.0 ± 0.15, consistent with the exact ana- 
lytical result ν = 1. 

Future direction 
Artificial neural network base on RSMI optimization have 
prove capable of extract complex information about physi- 
cally relevant degree of freedom and use it to perform a real- 
space RG procedure. The RSMI algorithm we propose allows for 
the study of the existence and location of critical points, and RG 
flow in their vicinity, a well a estimation of correlation func- 
tions, critical exponent and so on. This approach be an example 
of a new paradigm in apply machine learn in physics: the 
internal data representation discover by suitably design algo- 
rithms be not just technical mean to an end, but instead be a 
clear reflection of the underlie structure of the physical system 
(see also ref. 39). Thus, in spite of their ‘black box’ reputation, the 
innards of such architecture may teach u fundamental lessons. 

This raise the prospect of employ machine learn in science 
in a collaborative fashion, exploit the machines’ power to distil 
subtle information from vast data, and human creativity and back- 
ground knowledge40. 

Numerous further research direction can be pursued. Most 
directly, equilibrium system with less understood relevant 
degree of freedom—for example, disorder and glassy sys- 
tems—can be investigated9,10. The ability of the RSMI algorithm 
to re-compute the relevant degree of freedom at every RG step 
potentially allows one to study their evolution along the (more 
complicated) RG flow34. Furthermore, although we study clas- 
sical systems, the extension to the quantum domain be possible 
via the quantum-to-classical mapping of Euclidean path integral 
formalism. A more detailed analysis of the mutual-informa- 
tion-based RG procedure may prove fruitful from a theoretical 
perspective. Finally, application of RSMI beyond physic be 
possible, since it offer a neural network implementation of a 
variant of the information bottleneck method32, successful in 
compression and cluster analyses41; it can also be use a a 
local-noise-filtering pre-training stage for other machine-learn- 
ing algorithms. 

Data availability. The data that support the plot within this paper 
and other finding of this study be available from the correspond- 
ing author upon request. 

Received: 11 May 2017; Accepted: 13 February 2018; 
Published: xx xx xxxx 

References 
1. LeCun, Y., Bengio, Y. & Hinton, G. E. Deep learning. Nature 521, 

436–444 (2015). 
2. Silver, D. et al. Mastering the game of Go with deep neural network and tree 

search. Nature 529, 584–589 (2016). 
3. Hershey, J. R., Rennie, S. J., Olsen, P. A. & Kristjansson, T. T. Super-human 

multi-talker speech recognition: A graphical model approach. Comput. 
Speech Lang. 24, 45–66 (2010). 

4. Carrasquilla, J. & Melko, R. G. Machine learn phase of matter. Nat. Phys. 
13, 431–434 (2017). 

5. Torlai, G. & Melko, R. G. Learning thermodynamics with Boltzmann 
machines. Phys. Rev. B 94, 165134 (2016). 

6. van Nieuwenburg, E. P. L., Liu, Y.-H. & Huber, S. D. Learning phase 
transition by confusion. Nat. Phys. 13, 435–439 (2017). 

7. Wang, L. Discovering phase transition with unsupervised learning. 
Phys. Rev. B 94, 195105 (2016). 

8. Ohtsuki, T. & Ohtsuki, T. Deep learn the quantum phase transition in 
random electron systems: application to three dimensions. J. Phys. Soc. Jpn 
86, 044708 (2017). 

T/Tc 

R 
G 

s 
te 

p 

ParamagnetFerromagnet 

0.94 0.96 0.98 1.00 1.02 1.04 1.06 1.08 

0.0 

0.5 

1.0 

1.5 

2.0 

2.5 

3.0 

1.03 1.02 1.01 1.007 

1.002 0.99 0.98 

Fig. 5 | RG flow for the 2D Ising model. The temperature T (in unit 
of Tc) a a function of the RG step for system with initial Monte Carlo 
temperature (denoted by line of different colour) below and above Tc. 
See Supplementary Information for details. 

© 2018 Macmillan Publishers Limited, part of Springer Nature. All right reserved. 

NaTuRe PhySIcS | www.nature.com/naturephysics 

http://www.nature.com/naturephysics 


ArticlesNaTuRe PHysIcs 
9. Ronhovde, P.et al Detecting hidden spatial and spatio-temporal structure in 

glass and complex physical system by multiresolution network clustering. 
Eur. Phys. J. E 34, 105 (2011). 

10. Ronhovde, P.et al Detection of hidden structure for arbitrary scale in 
complex physical systems. Sci. Rep. 2, 329 (2012). 

11. Hinton, G. E. & Salakhutdinov, R. R. Reducing the dimensionality of data 
with neural networks. Science 313, 504–507 (2006). 

12. Lin, H. W. & Tegmark, M. Why do deep and cheap learn work so well? 
J. Stat. Phys. 168, 1223–1247 (2017). 

13. Carleo, G. & Troyer, M. Solving the quantum many-body problem with 
artificial neural networks. Science 355, 602–606 (2017). 

14. Deng, D.-L., Li, X. & Sarma, S. D. Machine learn topological states. 
Phys. Rev. B 96, 195145 (2017). 

15. Wilson, K. G. The renormalization group: Critical phenomenon and the Kondo 
problem. Rev. Mod. Phys. 47, 773–840 (1975). 

16. Politzer, H. D. Reliable perturbative result for strong interactions? 
Phys. Rev. Lett. 30, 1346–1349 (1973). 

17. Berezinskii, V. L. Destruction of long-range order in one-dimensional and 
two-dimensional system have a continuous symmetry group I. Classical 
systems. Sov. J. Exp. Theor. Phys. 32, 493 (1971). 

18. Kosterlitz, J. M. & Thouless, D. Ordering, metastability and phase transition 
in two-dimensional systems. J. Phys. C 6, 1181 (1973). 

19. Kadanoff, L. P. Scaling law for Ising model near T(c). Physics 2, 
263–272 (1966). 

20. Wetterich, C. Exact evolution equation for the effective potential. Phys. Lett. B 
301, 90–94 (1993). 

21. White, S. R. Density matrix formulation for quantum renormalization groups. 
Phys. Rev. Lett. 69, 2863–2866 (1992). 

22. Ma, S.-k, Dasgupta, C. & Hu, C.-k Random antiferromagnetic chain. 
Phys. Rev. Lett. 43, 1434–1437 (1979). 

23. Corboz, P. & Mila, F. Tensor network study of the Shastry–Sutherland model 
in zero magnetic field. Phys. Rev. B 87, 115144 (2013). 

24. Capponi, S., Chandra, V. R., Auerbach, A. & Weinstein, M. p6 chiral 
resonate valence bond in the kagome antiferromagnet. Phys. Rev. B 87, 
161118 (2013). 

25. Auerbach, A. Interacting Electrons and Quantum Magnetism 
(Springer, New York, NY, 1994). 

26. Gaite, J. & O’Connor, D. Field theory entropy, the h theorem, and the 
renormalization group. Phys. Rev. D 54, 5163–5173 (1996). 

27. Preskill, J. Quantum information and physics: some future directions. 
J. Mod. Opt. 47, 127–137 (2000). 

28. Apenko, S. M. Information theory and renormalization group flows. Phys. A 
391, 62–77 (2012). 

29. Machta, B. B., Chachra, R., Transtrum, M. K. & Sethna, J. P. Parameter space 
compression underlies emergent theory and predictive models. Science 342, 
604–607 (2013). 

30. Beny, C. & Osborne, T. J. The renormalization group via statistical inference. 
New. J. Phys. 17, 083005 (2015). 

31. Stephan, J.-M., Inglis, S., Fendley, P. & Melko, R. G. Geometric mutual 
information at classical critical points. Phys. Rev. Lett. 112, 127204 (2014). 

32. Tishby, N., Pereira, F. C. & Bialek, W. The information bottleneck method. In 
Proc. 37th Allerton Conf. on Communication, Control and Computation 
(eds Hajek, B. & Sreenivas, R. S.) 49, 368–377 (University of Illinois, 2001). 

33. Hinton, G. E. Training product of expert by minimize contrastive 
divergence. Neural Comput. 14, 1771–1800 (2002). 

34. Ludwig, A. W. W. & Cardy, J. L. Perturbative evaluation of the conformal 
anomaly at new critical point with application to random systems. 
Nucl. Phys. B 285, 687–718 (1987). 

35. Fisher, M. E. & Stephenson, J. Statistical mechanic of dimer on a plane 
lattice. II. Dimer correlation and monomers. Phys. Rev. 132, 
1411–1431 (1963). 

36. Fradkin, E. Field Theories of Condensed Matter Physics (Cambridge 
Univ. Press, Cambridge, 2013). 

37. Mehta, P. & Schwab, D. J. An exact mapping between the variational 
renormalization group and deep learning. Preprint at 
abs/1410.3831 (2014). 

38. McCoy, B. M. & Wu, T. T. The Two-Dimensional Ising Model (Harvard Univ. 
Press, Cambridge, MA, 1973). 

39. Schoenholz, S. S., Cubuk, E. D., Sussman, D. M., Kaxiras, E. & Liu, A. J. A 
structural approach to relaxation in glassy liquids. Nat. Phys. 12, 
469–471 (2016). 

40. Jordan, M. I. & Mitchell, T. M. Machine learning: Trends, perspectives, and 
prospects. Science 349, 255–260 (2015). 

41. Slonim, N. & Tishby, N. Document cluster use word cluster via the 
information bottleneck method. In Proc. 23rd Annual International ACM 
SIGIR Conf. on Research and Development in Information Retrieval, SIGIR ’00 
208–215 (ACM, 2000). 

acknowledgement 
We thank S. Huber and P. Fendley for discussions. M.K.-J. gratefully acknowledges 
the support of the Swiss National Science Foundation. Z.R. be support by the 
European Union's Horizon 2020 research and innovation programme under the Marie 
Sklodowska-Curie grant agreement no. 657111. 

author contribution 
M.K.-J. and Z.R. contribute equally to this work. 

compete interest 
The author declare no compete interests. 

additional information 
Supplementary information be available for this paper at https://doi.org/10.1038/ 
s41567-018-0081-4. 

Reprints and permission information be available at www.nature.com/reprints. 

Correspondence and request for material should be address to M.K. 

Publisher’s note: Springer Nature remains neutral with regard to jurisdictional claim in 
publish map and institutional affiliations. 

© 2018 Macmillan Publishers Limited, part of Springer Nature. All right reserved. 

NaTuRe PhySIcS | www.nature.com/naturephysics 

https://doi.org/10.1038/s41567-018-0081-4 
https://doi.org/10.1038/s41567-018-0081-4 
http://www.nature.com/reprints 
http://www.nature.com/naturephysics 

Mutual information, neural network and the renormalization group 
The RSMI algorithm 
Validation 
Future direction 
Data availability. 

Acknowledgements 
Fig. 1 The RSMI algorithm. 
Fig. 2 The weight of the RSMI network train on the Ising model. 
Fig. 3 The dimer model. 
Fig. 4 The weight of the RSMI network train on dimer model data. 
Fig. 5 RG flow for the 2D Ising model. 




