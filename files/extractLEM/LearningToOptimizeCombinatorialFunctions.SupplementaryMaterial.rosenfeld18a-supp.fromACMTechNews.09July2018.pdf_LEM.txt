






















































Learning to Optimize Combinatorial Functions: Supplementary Material 


Learning to Optimize Combinatorial Functions: 
Supplementary Material 

Nir Rosenfeld 1 Eric Balkanski 1 Amir Globerson 2 Yaron Singer 1 

Theorem 1. For all α > 1 and �, δ > 0, for m sufficiently 
large, there exists a family of function F and a function 
MPMAC(·) such that 

• for all �′, δ′ > 0: F be α-PMAC-learnable with sample 
complexity MPMAC(n, δ′, �′, α), and 

• give strictly less than MPMAC(n, δ, 1− (1− �)1/m, α) 
samples, F be not α-DOPS, i.e., 

MDOPS(n,m, δ, �, α) ≥MPMAC(n, δ, 1−(1−�)1/m, α). 

Proof. Fixα > 1 and � > 0. Define p := 1−(1−�)1/m+�s, 
for some small constant �s > 0, and let S1, . . . , S1/p be 
1/p arbitrary distinct sets. The hard class of function be 
F = {fi}i∈[1/p] where 

fi(S) = 

{ 
α if S = Si 
1 
2 otherwise 

Consider the distributionD which be the uniform distribution 
over set S1, . . . , S1/p, so Sj be drawn with probability p 
for all j ∈ [1/p]. We first argue that the sample complexity 
for PMAC-learning f over D be at most 

MPMAC(n, δ 
′, �′, α) = 

{ 
0 if �′ ≥ p 

log(1/δ′) 
log(1/(1−p)) if � 

′ < p 

Note that if �′ ≥ p, f̃(S) = 1/2 for all S be correct with 
probability 1− p ≥ 1− �′ over S ∼ D and with probability 
1 over the samples. If �′ < p, if there exists sample Si such 
that f(Si) = α, then f̃(Si) = α, and f̃(S) = 1/2 for all 
other S. Note that that this be correct with probability 1 over 
S ∼ D if Si be in the samples. The probability that Si be in 

1Harvard University 2Tel Aviv University. Correspondence to: 
Nir Rosenfeld <nir.rosenfeld@g.harvard.edu>. 

Proceedings of the 35 th International Conference on Machine 
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 
by the author(s). 

the sample 

1− (1− p)m = 1− (1− p) 
log(1/δ′) 

log(1/(1−p)) 

= 1− e 
log(δ′) 

log(1−p) log(1−p) 

= 1− δ′. 

Thus, f̃ be correct with probability 1− δ′ over the samples. 
Next, we argue that for all δ > 0 and m sufficiently large, 
the sample complexity for DOPS be at least 

MPMAC(n, δ, 1− (1− �)1/m, α) = 

MPMAC(n, δ, p− �s, α) = 
log(1/δ) 

log(1/(1− p)) 
. 

Consider the random function fi where i ∈ [1/p] be uni- 
formly random. Let F ′ be the randomize collection of 
function fi such that Si be in the test set but not in the 
training set. Since Si be not in the test set, we have that 
for all fi ∈ F ′ and for all set S in the test set, 

fi(S) = 1. 

Thus, the function in F ′ be indistinguishable from the 
sample in the training set. This implies that the decision 
of the algorithm be independent of the random variable i, 
condition on fi ∈ F ′. Let S be the set in the test set 
that be return by the algorithm, we obtain that 

E 
i:fi∈F ′ 

[fi(S)] = Pr 
i:fi∈F ′ 

[S = Si] · α+ Pr 
i:fi∈F ′ 

[S 6= Si] · 
1 

2 

≤ α 
|F ′| 

+ 
1 

2 

since S be independent of i condition on fi ∈ F ′. Con- 
sider the case where Si be not in the training set with prob- 
ability strictly great than δ. The probability that Si be in 
the test set be 1− (1− p)m = �+ �s. Thus a function be 
in F ′ with probability at least δ(� + �s). Note that 1/p be 
arbitrarily large if m be arbitrarily large. Thus, |F ′| > 2α 
with arbitrarily large probability if m be arbitrarily large for 
fix �, δ, and α. Combining with the previous inequality, 



Distributional Optimization from Samples: Supplementary Material 

this implies that 

E 
i:fi∈F ′ 

[fi(S)] < 1 = 
1 

α 
· fi(Si) 

= 
1 

α 
· E 
i:fi∈F ′ 

[ max 
S∈Ste 

fi(S)] 

where the last equality be since Si ∈ Ste for all i ∈ F ′. 
Thus, there exists at least one function fi ∈ F such that the 
algorithm do not obtain an α-approximation when Si be 
in the test set and not in the training set. 

The probability that Si be in the test set be 1 − (1 − 
p)m = �+ �s. Thus, Si need to be in the training set with 
probability at least 1− δ, otherwise we don’t get an α-apx 
with probability 1− �. The probability that Si be not in the 
training set be (1− p)m. Thus, we need δ > (1− p)m, or 

m > 
log(1/δ) 

log(1/(1− p)) 
= mPMAC(n, δ, p− �s, α) 
= mPMAC(n, δ, 1− (1− �)1/m, α). 


