






































Neuromorphic Chips Are Destined for Deep Learningâ•ﬂor Obscurity - IEEE Spectrum 


Researchers in this specialized field have hitch their wagon to deep learning’s 
star 

Advertisement 

talk of a technology 

“crossing the chasm” by make the leap from early 

adopter to the mass market. A case study in chasm 

cross be now unfold in neuromorphic 

computing. 

People in the tech world 

The approach mimic the way neuron be connect 

and communicate in the human brain, and 

enthusiast say neuromorphic chip can run on much 

less power than traditional CPUs. The problem, 

though, be prove that 

can move from research 

lab to commercial applications. The field’s lead 

researcher spoke frankly about that challenge at the 

, held in 

March at the IBM research facility at Almaden, Calif. 

neuromorphics 

(http://spectrum.ieee.org/automaton/robotics 

/artificial-intelligence/analog-and-neuromorphic- 

chips-will-rule-robotic-age) 

Neuro Inspired Computational Elements Workshop 

(https://www.src.org/calendar/e006125/) 

“There currently be a lot of hype about neuromorphic 

computing,” say 

, the researcher at the University 

of Manchester, in England, who head the 

, a major neuromorphics effort. “It’s true that 

neuromorphic system exist, and you can get one and 

use one. But all of them have fairly small user bases, 

in university or industrial research groups. All 

require fairly specialized knowledge. And there be 

currently no compelling demonstration of a high- 

volume application where neuromorphic outperforms 

the alternative.” 

Steve Furber 

(http://www.computerhistory.org/fellowawards 

/hall/steve-furber/) 

SpiNNaker 

project (http://spectrum.ieee.org/computing 

/hardware/lowpower-chips-to-model-a-billion- 

neurons) 

Other attendee give their own candid analyses. 

Another prominent researcher, Chris Eliasmith 

Neuromorphic Chips Are Destined for Deep Learning—or Obscurity - IEEE Spectrum http://spectrum.ieee.org/semiconductors/design/neuromorphic-chips-are-destined-for-deep-lear... 

1 sur 9 07/08/2017 19:42 



(/cars-that- 

think/transportation/self-driving/baidus- 

boffin-describes-beijings-homegrown- 

selfdriving-car-) 

(/static/special- 

report-can-we- 

copy-the-brain) 

Illustration: Chad Hagen 

of the 

University of Waterloo, in Ontario, Canada, say the 

field need to meet the hype issue “head-on.” Given 

that neuromorphics have generate a great deal of 

excitement, Eliasmith doesn’t want to “fritter it away 

on toy problems”: A typical neuro morphic 

demonstration these day will show a system run 

a relatively simple artificial intelligence application. 

Rudimentary robot with neuromorphic chip have 

a Colorado mountain trail and 

of a specific color place in a pattern on the 

floor. The real test be for traditional company to accept neuromorphics a a mainstay platform for 

everyday engineering challenges, Eliasmith said, but there be “tons more to do” before that happens. 

(http://arts.uwaterloo.ca/~celiasmi/) 

navigate down (https://www.google.com/url?sa=t& 

rct=j&q=&esrc=s&source=web&cd=3& 

ved=0ahUKEwjRgMrtv4jTAhWD6IMKHRW3Bm0QFggnMAI&url=http%3A%2F 

%2Fwww.socsci.uci.edu%2F~jkrichma%2FHwu-arXiv-1611.01235v1.pdf& 

usg=AFQjCNGhfo_n01NdwRzcWU9CRR3FONaEJg&cad=rja) roll over 

square (https://www.youtube.com/watch?v=8c1Noq2K96c) 

of neuromorphic compute be what researcher call a spike neuron, which 

play a role analogous to what a logic gate do in traditional computing. In the central processing unit of 

your desktop, transistor be assemble into different type of logic gates—AND, OR, XOR, and the like— 

each of which evaluates two binary inputs. Then, base on those value and the gate’s type, each gate 

output either a 1 or a 0 to the next logic gate in line. All of them work in precise synchronization to the 

drumbeat of the chip’s master clock, mirror the Boolean logic of the software it’s running. 

The basic building block 

Neuromorphic Chips Are Destined for Deep Learning—or Obscurity - IEEE Spectrum http://spectrum.ieee.org/semiconductors/design/neuromorphic-chips-are-destined-for-deep-lear... 

2 sur 9 07/08/2017 19:42 



(/image/MjkwMTgyMQ.jpeg) 
Illustration: James Provost 

Two layer within a neural network contain group of “neurons” with similar functions, indicate by color [blue, yellow, 

orange, and pink] in the illustration on the left. In the graphic on the right, those neuron be mapped to spike neuron in an IBM 

TrueNorth chip. The spike neuron be connect by gridlike “synapses” to other neuron in the same core, and to a row of 

inputs. Those input can generate spikes, which be then process by the neural network. 

Tiny Spikes: 

The spike neuron be a different beast. Imagine a node sit on a circuit and measure whatever 

spikes—in the form of electrical pulses—are transmit along the circuit. If a certain number of spike 

occur within a certain period of time, the node be programmed to send along one or more new spike of 

it own, the exact number depend on the design of the particular chip. Unlike the binary, 0-or-1 option 

of traditional CPUs, the response to spike can be weight to a range of values, give neuromorphics 

something of an analog flavor. The chip save on energy in large part because their neuron aren’t 

constantly firing, a occurs with traditional silicon technology, but instead become activate only when 

they receive a spike signal. 

“When we look at how 
neuron compute in the 
brain, there be concrete 
thing we can learn” 

A neuromorphic system connects these spike 

neuron into complex networks, often accord to 

a task-specific layout that programmer have 

work out in advance. In a network design for 

image recognition, for example, certain 

connection between neuron take on certain 

weights, and the way spike travel between these 

neuron with their respective weight can be make 

to represent different objects. If one pattern of 

spike appear at the output, programmer would 

know the image be of a cat; another pattern of 

spike would indicate the image be of a chair. 

Within neuromorphics, each research group have come up with it own design to make this possible. 

IBM’s DARPA-funded 

, for example, do it spike in custom hardware, while Furber’s 

(Spiking Neural Network 

TrueNorth neuromorphic chip (http://science.sciencemag.org/content/345/6197 

/668) SpiNNaker 

(http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6750072) 

Neuromorphic Chips Are Destined for Deep Learning—or Obscurity - IEEE Spectrum http://spectrum.ieee.org/semiconductors/design/neuromorphic-chips-are-destined-for-deep-lear... 

3 sur 9 07/08/2017 19:42 



Architecture) relies on software run on the ARM processor that he help develop. 

there be no consensus on what neuromorphic system would actually do, except to 

somehow be useful in brain research. In truth, spike chip be something of a solution look for a 

problem. Help, though, arrive unexpectedly from an entirely different part of the compute world. 

In the early days, 

Starting in the 1990s, artificial intelligence researcher make a number of theoretical advance involve 

the design of the “neural networks” that have be use for decade for computational problem solving, 

though with limited success. , with the University of California, 

Irvine’s Neuromorphic Machine Intelligence Lab, say that when combine with faster silicon chips, 

these new, improve neural network allow computer to make dramatic advance in classic 

compute problems, such a image recognition. 

Emre Neftci (http://nmi-lab.org/people/) 

This new breed of compute tool use what’s come to be call deep learning, and in the past few years, 

deep learn have basically take over the computer industry. Members of the neuromorphics research 

community soon discover that they could take a deep-learning network and run it on their new style of 

hardware. And they could take advantage of the technology’s power efficiency: The 

, which be the size of a postage stamp and hold a million “neurons,” be design to use a tiny fraction 

of the power of a standard processor. 

TrueNorth chip 

(http://spectrum.ieee.org/computing/hardware/how-ibm-got-brainlike-efficiency-from-the-truenorth- 

chip) 

Those power savings, say neuromorphics boosters, will take deep learn to place it couldn’t previously 

go, such a inside a mobile phone, and into the world’s hottest technology market. Today, deep learn 

enables many of the most widely use mobile features, such a the speech recognition require when you 

ask Siri a question. But the actual processing occurs on giant server in the cloud, for lack of sufficient 

compute horsepower on the device. With neuromorphics on board, say it supporters, everything could 

be compute locally. 

Which mean that neuromorphic compute has, to a considerable degree, hitch it wagon to deep 

learning’s star. When IBM want to show off a killer app for it TrueNorth chip, it 

that classify images. Much of the neuromorphics community now defines success a 

be able to supply extremely power-efficient chip for deep learning, first for big server farm such a 

those run by Google, and late for mobile phone and other small, power-sensitive applications. The 

former be consider the easy engineering challenge, and neuromorphics optimist say commercial 

product for server farm could show up in a few a two years. 

ran a deep neural 

network (http://spectrum.ieee.org/tech-talk/computing/hardware/ibms-braininspired-chip-tested-on- 

deep-learning) 

just about everyone else in the semiconductor industry 

—including big player like and 

—also want in on the deep-learning market. And that 

market might turn out to be one of the rare case in which the incumbents, rather than the innovators, 

have the strategic advantage. That’s because deep learning, arguably the most advanced software on the 

planet, generally run on extremely simple hardware. 

Unfortunately for neuromorphics, 

Intel (https://software.intel.com/en-us/ai/deep-learning) Nvidia 

(https://developer.nvidia.com/deep-learning) 

“The neuromorphic , an analyst with Moor Insights 
& Strategy who specializes in deep learning, say 

the key bit of computation involve in run a 

Karl Freund 

(http://www.moorinsightsstrategy.com/karl- 

freund-biography/) 

Neuromorphic Chips Are Destined for Deep Learning—or Obscurity - IEEE Spectrum http://spectrum.ieee.org/semiconductors/design/neuromorphic-chips-are-destined-for-deep-lear... 

4 sur 9 07/08/2017 19:42 



approach be interest 
scientifically, but they be 
nowhere close on accuracy” 

deep-learning system—known a 

—can easily be handle with 16-bit and even 

8-bit CPU components, a oppose to the 32- and 

64-bit circuit of an advanced desktop processor. 

In fact, most deep-learning system use traditional silicon, especially the graphic coprocessors found in 

the video card best know for power video games. Graphics coprocessors can have thousand of 

cores, all work in tandem, and the more core there are, the more efficient the deep-learning network. 

matrix 

multiplication (https://www.khanacademy.org 

/math/precalculus/precalc-matrices/multiplying- 

matrices-by-matrices/v/matrix-multiplication- 

intro) 

So chip company be bring out deep-learning chip that be make out of very simple, traditional 

components, optimize to use a little power a possible. (That’s true of Google’s 

, the chip 

the search company announce last year in connection with it own deep-learning efforts.) Put 

differently, neuromorphics’ main competition a the platform of choice for deep learn be an advanced 

generation of what be essentially “vanilla” silicon chips. 

Tensor Processing Unit 

(http://spectrum.ieee.org/tech-talk/computing/hardware/google-details-tensor-chip-powers) 

Some company on the vanilla side of this argument deny that neuromorphic system have an edge in 

power efficiency. , a Stanford 

electrical engineering professor and chief scientist at Nvidia, say that the demonstration perform 

with TrueNorth use a very early version of deep learning, one with much less accuracy than be possible 

with more recent systems. When accuracy be take into account, he said, any energy advantage of 

neuromorphics disappears. 

William J. Dally (http://cva.stanford.edu/billd_webpage_new.html) 

“People who do conventional neural network get result and win the competitions,” Dally said. “The 

neuromorphic approach be interest scientifically, but they be nowhere close on accuracy.” 

Indeed, researcher have yet to figure out simple way to get neuromorphic system to run the huge 

variety of deep-learning network that have be developed on conventional chips. 

, at the Center for Applied Scientific Computing at the Lawrence 

Livermore National Laboratory, say his group have be able to get neural network to run on TrueNorth 

but that the task of pick the right network and then successfully port it over remains “a challenge.” 

Other researcher say the most advanced deep-learning system require more neurons, with more 

possible interconnections, than current neuromorphic technology can offer. 

Brian Van Essen 

(https://people.llnl.gov/vanessen1) 

must tackle these problem with a small pool of talent. The March 

conference, the field’s flagship event, attract only a few hundred people; meeting associate with deep 

learn usually draw many thousands. IBM, which decline to comment for this article, say last fall that 

TrueNorth, which debut in 2014, be now run experiment and application for more than 130 user 

at more than 40 university and research centers. 

The neuromorphics community 

By contrast, there be hardly a Web company or university computer department on the planet that isn’t 

do something with deep learn on conventional chips. As a result, those conventional architecture 

have a robust suite of development tools, along with legion of engineer train in their use— typical 

advantage of an incumbent technology with a large instal base. Getting the deep-learning community 

to switch to a new and unfamiliar way of do thing will prove extremely difficult unless neuromorphics 

can offer an unmistakable performance and power advantage. 

Neuromorphic Chips Are Destined for Deep Learning—or Obscurity - IEEE Spectrum http://spectrum.ieee.org/semiconductors/design/neuromorphic-chips-are-destined-for-deep-lear... 

5 sur 9 07/08/2017 19:42 



Again, that’s a problem the neuromorphics community openly acknowledges. If the presentation at the 

March conference frequently refer to the challenge that lie ahead for the field, most of them also 

offer suggestion on how to overcome them. 

The University of Waterloo’s Eliasmith, for example, say that neuromorphics must progress on a 

number of fronts. One of them be building more-robust hardware, with more neuron and 

interconnections, to handle more-advanced deep-learning systems. Also needed, he said, be theoretical 

insight about the inherent strength and weakness of neuromorphic systems, to good know how to 

use them most productively. To be sure, he still believe the technology can live up to expectations. “We 

have be see regular improvements, so I’m encouraged,” Eliasmith said. 

Still, the neuromorphics community might find that it current symbiotic relationship with deep learn 

come with it own hazards. For all the recent success of deep learning, plenty of expert still question 

how much of an advance it will turn out to be. 

(/image/MjkwNDI0MA.jpeg) 
Photo: University of Manchester 

The SpiNNaker project be construct a 

machine with 50,000 of these specialized chip in hope of 

create a network of 1 billion “neurons.” 

Building Blocks: 

Deep learn clearly delivers superior result in 

application such a pattern recognition, in which 

one picture be match to another picture, or for 

language translation. It remains to be see how far 

the technique will take researcher toward the holy 

grail of “ 

,” or 

the ability of a computer to have, like HAL 9000 in 

the film , the reason and 

language skill of a human. Deep-learning pioneer 

compare 

AI research to drive in the fog. He say there be a 

chance that even arm with deep learning, AI 

might any day now crash into another brick wall. 

generalize intelligence 

(http://spectrum.ieee.org/automaton/robotics 

/artificial-intelligence/why-alphago-is-not-ai) 

2001: A Space Odyssey 

Yann LeCun (http://yann.lecun.com/) 

That prospect cause some at the conference to 

suggest that neuromorphics researcher should 

persevere even if the technology doesn’t deliver a 

home run for deep learning. 

, director of 

the University of California, Berkeley’s Redwood 

Center for Theoretical Neuroscience, say neuromorphic technology may, on it own, someday bring 

about AI result more sophisticated than anything deep learn ever could. “When we look at how 

neuron compute in the brain, there be concrete thing we can learn,” he said. “Let’s try to build chip 

that do the same thing, and see what we can leverage out of them.” 

Bruno Olshausen 

(http://redwood.berkeley.edu/bruno/) 

The SpiNNaker project’s Furber echoed those sentiment when ask to predict when neuromorphics 

would be able to produce low-power component that could be use in mobile phones. His estimate be 

five years—but he say he be only 80 percent confident in that prediction. He added, however, that he 

be far more certain that neuromorphics would play an important role in study the brain, just a early 

proponent thought it might. 

Neuromorphic Chips Are Destined for Deep Learning—or Obscurity - IEEE Spectrum http://spectrum.ieee.org/semiconductors/design/neuromorphic-chips-are-destined-for-deep-lear... 

6 sur 9 07/08/2017 19:42 



However, there be a meta-issue hover over the neuromorphics community: Researchers don’t know 

whether the spike behavior they be mimic in the brain be central to the way the mind works, or 

merely one of it many accidental by-products. Indeed, the surest way to start an argument with a 

neuromorphics researcher be to suggest that we don’t really know enough about how the brain work to 

have any business try to copy it in silicon. The usual response you’ll get be that while we certainly don’t 

know everything, we clearly know enough to start. 

It have often be note that progress in aviation be make only after inventor stop try to copy the 

flap wing of bird and instead discovered—and then harnessed—basic forces, such a thrust and lift. 

The knock against neuromorphic compute be that it’s stuck at the level of mimic flap wings, an 

accusation the neuromorphics side obviously rejects. Depending on who be right, the field will either take 

flight and soar over the chasm, or drop into obscurity. 

Freelance journalist be a veteran 

technology reporter who have investigate claim surround wireless phone charging, self-driving cars, 

big data, and deep learn for . In his spare time, he enjoys baking decadent French 

pastry and cakes. 

Lee Gomes (https://www.linkedin.com/in/lee-gomes-5850b712) 

IEEE Spectrum 

This article appear in the June 2017 print issue a “The Neuromorphic Chip’s Make-or-Break 

Moment.” 

Advertisement 

SPECIAL REPORT: CAN WE COPY THE BRAIN? 
(/static/special-report-can-we-copy-the-brain) 

PREVIOUS 
We Could Build an Artificial 
Brain Right Now 

(/computing/hardware/we-could-build- 

NEXT 
Watch This Robot Navigate 

Like a Rat 
(/video/robotics/robotics-software 

Neuromorphic Chips Are Destined for Deep Learning—or Obscurity - IEEE Spectrum http://spectrum.ieee.org/semiconductors/design/neuromorphic-chips-are-destined-for-deep-lear... 

7 sur 9 07/08/2017 19:42 



an-artificial-brain- 

right-now) 

/watch-this-robot- 

navigate-like-a-rat) 

(/the-human- 

os/biomedical 

/bionics/brainy- 

startup-neurable- 

unveils-the-worlds- 

first-braincontrolled- 

vr-game) 

(/automaton/robotics 

/industrial-robots 

/video-friday-more- 

boston-dynamics- 

giant-fighting-robots- 

anymal-quadruped) 

(/the-human- 

os/biomedical 

/imaging/ai-makes- 

anthrax-bioterror- 

detection-easier) 

(/the-human- 

os/biomedical 

/diagnostics/teenage- 

whiz-kid-invents-an- 

ai-system-to- 

diagnose-her- 

grandfathers-eye- 

disease) 

(/computing 

/software/music-to- 

your-ears-new- 

transducers-meet- 

electrostatic- 

headphones) 

(/automaton/robotics 

/home-robots/co- 

parenting-with- 

telepresence-robots) 

Neuromorphic Chips Are Destined for Deep Learning—or Obscurity - IEEE Spectrum http://spectrum.ieee.org/semiconductors/design/neuromorphic-chips-are-destined-for-deep-lear... 

8 sur 9 07/08/2017 19:42 



Neuromorphic Chips Are Destined for Deep Learning—or Obscurity - IEEE Spectrum http://spectrum.ieee.org/semiconductors/design/neuromorphic-chips-are-destined-for-deep-lear... 

9 sur 9 07/08/2017 19:42 


