















































Energy-Efficient Hybrid Stochastic-Binary Neural Networks for Near-Sensor Computing 




Energy-Efficient Hybrid Stochastic-Binary Neural 

Networks for Near-Sensor Computing 
Vincent T. Lee†, Armin Alaghi†, John P. Hayes*, Visvesh Sathe‡, Luis Ceze† 

†Department of Computer Science and Engineering, University of Washington, Seattle, WA, 98198 
*Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, 48109 

‡Department of Electrical Engineering, University of Washington, Seattle, WA, 98195 

{vlee2, armin}@cs.washington.edu, jhayes@eecs.umich.edu, sathe@uw.edu, luisceze@cs.washington.edu 

Abstract— Recent advance in neural network (NNs) exhibit 

unprecedented success at transform large, unstructured data 

stream into compact higher-level semantic information for task 

such a handwrite recognition, image classification, and speech 

recognition. Ideally, system would employ near-sensor com- 

putation to execute these task at sensor endpoint to maximize 

data reduction and minimize data movement. However, near- 

sensor compute present it own set of challenge such a 

operating power constraints, energy budgets, and communication 

bandwidth capacities. In this paper, we propose a stochastic- 

binary hybrid design which split the computation between the 

stochastic and binary domain for near-sensor NN applications. In 

addition, our design us a new stochastic adder and multiplier 

that be significantly more accurate than exist adder and 

multipliers. We also show that retrain the binary portion of the 

NN computation can compensate for precision loss introduce 

by shorter stochastic bit-streams, allow faster run time at 

minimal accuracy losses. Our evaluation show that our hybrid 

stochastic-binary design can achieve 9.8× energy efficiency 

savings, and application-level accuracy within 0.05% compare 

to conventional all-binary designs. 

Keywords—neural networks, stochastic compute 

I. INTRODUCTION 

Sensors and actuator be critical for enable electronic circuit 
to interact with the physical world. Information acquire from 
sensor have become essential to application from home 
automation to medical implant to environmental surveillance. 
It be predict that the world soon will have an average of 1,000 
sensor per person [8][11] which translates to a huge amount of 
raw data acquisition. The sheer volume of unstructured sensor 
data threatens to overwhelm storage and network communica- 
tion capacities, which be increasingly limited by aggressive 
power and energy budgets. 

To reduce the storage and communication demand of raw 
sensor data, near-sensor compute have recently emerge a a 
design space for reduce these overhead [20]. Near-sensor 
compute proposes offload portion of the application to 
compute unit or accelerator co-located with the sense 
device. The key insight be that by offload certain portion of 
computation such a image feature extraction (of an image- 
processing pipeline) to sensor end points, high level semantic 
information can be transmit in place of large unstructured 
data streams. Of particular interest be neural network (NNs) 
which be a widely use class of algorithm for processing raw 
unstructured data. NNs excel at reason about raw data 
stream in application such a object detection, handwrite 
recognition, and speech processing. Recent work by Du et al. 

[12] show how a near-sensor NN accelerator can dramatically 
reduce the energy cost of the system. 

This paper present a near-sensor stochastic-binary NN 
design which combine stochastic compute (SC) with 
conventional “binary” processing and sensor data acquisition to 
improve energy efficiency and power consumption. SC be a re- 
emerge computation technique that performs computation on 
unary bit-streams represent probability [14]. SC circuit be 
often cheaper than binary arithmetic circuit [25]. For instance, 
multiplication in SC can be implement by a single AND gate. 
The primary tradeoff for SC's simplicity be increase computa- 
tion time, which lead to high energy consumption for high 
precision calculation [2][22]. However, for application that 
can tolerate reduce precision, SC can achieve compelling 
power and energy efficiency gains. Finally, stochastic circuit 
be small in size and more error tolerant, make them suitable 
for tiny sensor operating in harsh environment [3][13]. 

Stochastic NNs have be extensively study in the prior 
literature [7][9][15]. However, past work proposes fully 
stochastic design that have number length exceed 1,000 
clock cycle [7][15], which lead to high energy consumption. 
In addition, error introduce by multiple level of SC circuit 
compound a more level be execute [22]. In this paper, we 
present a stochastic-binary hybrid NN system that exploit the 
benefit of SC, while mitigate many of it drawbacks. We only 
employ SC in the first layer of an NN, so it operates directly on 
the sensor data thereby avoid the issue of compound 
error over multiple layers. We employ a new, significantly 
more accurate SC adder and a deterministic number generation 
scheme to further reduce energy consumption. Finally, we 
compare our design’s accuracy to that of exist SC designs, 
and show our design have good energy efficiency than compete 
binary implementations. 

Our contribution be a follows: 

1. A novel stochastic adder for convolutional NNs which 
increase speed and/or accuracy, lead to a reduce 
energy cost compare to previous SC NN designs. 

2. A hybrid stochastic-binary NN design which combine 
signal acquisition and SC in the first NN layer, and us 
binary for the remain layer to avoid compound 
accuracy losses. 

3. Showing that retrain these remain NN layer can 
compensate for precision loss introduce by SC. 

The rest of the paper be organize a follows. Section II provide 
background on SC and NNs. Section III introduces the new 





stochastic adder design. Section IV present our hybrid NN 
design, and result be discuss in Sections V and VI. 

II. BACKGROUND 

This section briefly review the relevant concept of stochastic 

compute and neural networks. 

A. Stochastic Computing 

Stochastic compute be an alternative method of compute first 
propose in the 1960s [14]. In SC, number be encode a bit- 
stream that be interpret a probabilities. For instance, the bit- 
stream X = 001011… denotes a stochastic number (SN) with 
value pX = 0.5 because the probability of see a 1 at a randomly 
chosen position of X be 0.5. This interpretation allows arithmetic 
function to be implement via simple logic gates. For instance, 
the AND gate in Fig. 1a performs multiplication on uncorrelated 
inputs. The SC probability pX or unipolar range [0, 1], do not 
include negative numbers, which be usually need for NNs. 
As a result, NNs often use bipolar numbers, where the value of 

X be interpret a 2pX  1, and therefore have range [1, 1]. The 
precision of SC be mainly determine by the length N of the bit- 
stream. A bit-stream of length N encodes a number at log2 N bit 
of precision. For example, a unipolar bit-stream of length 16 can 
encode the range [0, 15] which be equivalent to the range of a 
binary number with log216 = 4 bit of precision. 

In this work, we use four SC primitives: adders, multipliers, 
stochastic number generator (SNGs), and stochastic-to-digital 
converter (Fig. 1). These component operate on unipolar 
numbers; they may implement a different function when 
interpret in the bipolar domain. To perform conventional 
stochastic addition, two bit-streams X and Y be apply to the 
data input of a multiplexer with the select bit driven by a bit- 
stream R of unipolar value pR = 0.5 (Fig. 1b). The output bit- 
stream encodes pZ = 0.5(pX + pY). Notice the scale factor of 0.5, 
a necessary feature of SC, keep the probability in the unit 
interval [0, 1]. When compound over many additions, the 
scale factor can lead to severe loss of precision. Similar 
precision loss also occur with SC multiplication, which be 
realize with an AND gate (Fig. 1a) since pZ = pX × pY. One way 
to improve the quality of a function be to increase the length of 
the input bit-streams. However, since each bit of additional 
precision require a double of bit-stream length this quickly 
lead to excessive run times. As a result, researcher have 
propose alternative design that approximate the add operation. 
One example be to use an OR gate a an adder, which only work 
accurately if both input be close to zero [21]. Hence, all 
exist SC adder design need additional uncorrelated random 
number source and/or have limited accuracy. The need for extra 
random number source becomes severe when many number 
be to be added. Ideally, we would like an adder that operates 

accurately on many input in short period of time, without 
require additional uncorrelated number sources. 

Binary-to-stochastic converters, which be commonly 
refer to a stochastic number generator (SNGs), and 
stochastic-to-binary converter be SC primitive that allow 
conversion between the binary and stochastic domains. An SNG 
comprises a comparator and a random number generator (Fig. 
1c). For a give number pX, the SNG will produce a 1 with that 
probability if the random number be less than pX. Converting 
analog signal to the stochastic domain can be achieve by 
replace the SNG comparator with an analog one. In this paper, 
we use an analog-to-stochastic converter to convert the sensor 
data directly to stochastic encodings, without the need for 
analog-to-digital converter (ADCs). We also use a set of SNGs 
to generate the NN weights. 

The choice of SNG configuration affect the accuracy and 
consequently the energy consumption of the SC circuit. Table 1 
show the mean square error (MSE) of a 4-bit and 8-bit SC 
multiplier for the follow SNG schemes: (i) use the same 
linear feedback shift register (LFSR) for both inputs, (ii) use a 
separate LFSR for each input, (iii) use low-discrepancy 
sequence [4], and (iv) use a ramp-compare analog-to- 
stochastic converter [13] for one input, and a low-discrepancy 
sequence for the other. For this work, we employ the last number 
generation scheme a it provide the best accuracy. The MSEs 
be calculate by exhaustively test the multiplier for every 
possible input value. 

To convert from stochastic to binary, we simply count the 1 
in the bit-stream by use a binary counter (Fig. 1d). In our work, 
we use asynchronous counter because they allow u to clock 
the SC part of the circuit faster. It be sufficient to apply a new 
input to an asynchronous counter, even if the previous input 
have not propagate through the counter. The delay of a 
synchronous counter, on the other hand, be relatively large, so it 
cannot keep up with the speed of the SC circuit feed it. Unlike 
the asynchronous counter, a synchronous counter fails if the next 
input arrives before the previous input be propagated. 

B. Neural Networks 

NNs come in a wide range of network topologies, and generally 

consist of an input layer, an output layer, and a number of 

hidden layer in between [24]. A layer be compose of neurons, 

each of which have a set of inputs, an output, and an activation 

function f(x), e.g., a rectify linear unit. Each neuron be 

connect to neuron in the previous layer; a connection be 

define by a weight that be multiply by the previous neuron’s 

output. These value be sum with other connections’ 

output and pass to an activation function. For instance, give 

a neuron y that be connect to k neuron in the previous layer 

(c) (d) 

B 
k 

x 

(b) 

x 

(a) 

AND gate 

y 
z 

pZ = pX × pY 

Binary counterBinary counter 

B = i x, iϵ[0, 2 
k] 

Multiplexer 

pZ = (pX + pY)/2 

x 

y 

z 

r 

0 

1 

0 

1 

Multiplexer 

pZ = (pX + pY)/2 

x 

y 

z 

r 

0 

1 

Comparatork 
A 

B 

A < B 

Binary number B 
pX = B/2 

k 

k 

Random no. 

generator 
x 

Comparatork 
A 

B 

A < B 

Binary number B 
pX = B/2 

k 

k 

Random no. 

generator 
x 


Fig. 1. Unipolar stochastic arithmetic primitives: (a) multiplier, (b) scale adder, (c) comparator-based stochastic number generator, and (d) 

stochastic-to-binary converter implement a a binary counter. 





with output value �⃗� = {x0, x1, …, xk1} and connection weight 
�⃗⃗⃗� = {w0, w1, …, wk1} respectively, the output of neuron y be 

define a 𝑦𝑜𝑢𝑡 = 𝑓(∑ 𝑥𝑖𝑤𝑖 
𝑘−1 
𝑖=0 ). 

Neuron connection topology can either be fully connect 

or locally connect to the previous layer. In fully connect 

layers, each neuron be connect to every neuron of the previous 

layer. In the locally connect case, neuron be connect to a 

subset of neuron in the previous layer. Locally connect 

layer be often refer to a convolutional layer because their 

connection from the previous layer take the form of a window. 

The result operation be mathematically equivalent to a 

convolution where the convolutional kernel be simply a matrix 

of the connection weights. Finally, NNs also may have max 

pool layers, which be locally connect layer that 

subsample a window in the previous layer and output the 

maximum value. 

To determine the weight for each layer, NNs be train 

over an input training set use backpropagation [24]. This be a 

technique that iterates over the training dataset and gradually 

adjusts the weight base on the gradient of the error in the 

NN’s output function. The error metric varies across 

application but a commonly use one for NN classification be 

the cross-entropy loss. One iteration over the entire training set 

be know a an epoch. Training be often supplement by 

dropout which be a training technique that randomly remove 

connection during the training process at certain layer to 

prevent overfitting. Once the training process converges to a set 

of weights, a test set be use to evaluate the quality of the NN 

model. The quality metric varies across application but a 

commonly use metric be classification accuracy base on the 

output of the NN model. 

Using SC for NNs have a well-established history [7][17] 

date back to the 1990s. Recent work proposes fully stochastic 

NN design use FPGA fabric and full custom ASICs [16]. 

Similarly, Ardakani et al. [6] propose an SC NN for digit 

recognition which outperforms binary design by use shorter 

bit-streams (down to length 16). To the best of our knowledge, 

this be the only SC NN design that outperforms, albeit 

marginally, it binary counterpart in term of energy efficiency. 

However, unlike our approach, prior SC work us older, fully 

connect NN topology with only two hidden layer which 

be small and less accurate than current state-of-the art NN 

topology like LeNet-5 (used in our evaluation). Finally, fully 

stochastic NNs need longer bit-streams (N = 256 to 1024) to 

achieve reasonable accuracy. In contrast, our work do not 

execute the entire NN in the stochastic domain. Instead, we 

execute the first layer use SC, then allow high precision 

binary unit to finish the NN calculation. 

III. STOCHASTIC ADDER DESIGN 

Unlike the basic stochastic multiplier, the conventional 

stochastic add operation have undesirable property such a the 

enforce scale factor and an extra bit-stream. Furthermore, 

the discard of some bit of each number (through 

multiplexing) lead to accuracy loss, which compound with 

multiple additions. 

We now propose a new stochastic adder that be more accurate 
and do not require additional random inputs. But first we 
introduce a simple circuit that implement the SC function pC = 
pA/2. A rudimentary implementation be to use the multiplier of 
Fig. 1a where we assign A to one input, and a randomly 
generate bit-stream B of value 1/2 to the other. Note that for the 
multiplication to work accurately, B have to be uncorrelated to A. 
Fig. 2a show another implementation of the same function, in 
which a bit-stream B with value 1/2 be generate from the bit- 
stream of A without require an additional input. A toggle flip- 
flop (TFF), which switch it output between 0 and 1 when it 
input be 1, be use for this purpose. The area cost of a TFF be no 
more than a random number generator that be require for 
generate 1/2. More importantly, the bit-stream generate by 
the TFF be always uncorrelated with it input bit-stream. This 
mean that there be no constraint on the auto-correlation of the 
input bit-stream, unlike common sequential SC circuit that do 
not function a intend if the input be auto-correlated [7]. 

Fig. 2b show our propose TFF-based adder. At each clock 
cycle, if the value at X and Y be equal, they propagate to the 
output. Otherwise, the state of the TFF be output and the TFF be 
toggled. Suppose the adder operates on two bit-streams of length 
20. Recall for adds, there be a 0.5 normalization constant, so the 
expect result be Z = 0.5(1/2 + 4/5) = 13/20 compute a 
follows: 

X = 0110 0011 0101 0111 1000 (1/2) 
Y = 1011 1111 0101 0111 1111 (4/5) 
Z = 0110 1011 0101 0111 1101 (13/20) 

The result of the adder be always accurate if the bit-stream 
length N be sufficient to represent it. Otherwise, the output will 
be round off to the near representable number. The 
direction of round depends on the initial state S0 of the TFF. 

a 

(a) (b) 

b 

c 

T Q 

T Q 

0 

1 

x 
y 

z 

X = 0100 1010 (3/8) 

Y = 0010 0010 (1/4) 

Z0 = 0010 0010 (1/4) 

Z1 = 0100 1010 (3/8) 

(c) 


Fig. 2. (a) Stochastic circuit with pC = pA/2, (b) propose TFF-based 

stochastic adder with pZ = (pX + pY)/2, and (c) example of it operation 

with two different initial states. 

Table 1. MSE of stochastic multiplier for different RNG method 

(lower be better) 

Number generation scheme 8-Bit Prec. 4-Bit Prec. 

One LFSR + shift version 2.78×103 2.99×103 

Two LFSRs 2.57×104 1.60×103 

Low-discrepancy sequence [4] 1.28×105 1.01×103 

Ramp-compare [13] + [4] 8.66×106 7.21×104 

Table 2. MSE of stochastic addition for different SNG method 

(lower be better) 

Implementation 8-Bit Prec. 4-Bit Prec. 

Old adder 

(Fig. 1b) 

Random + LFSR 3.24×104 5.55×103 

Random + TFF 5.49×104 5.49×103 

LFSR + TFF 1.06×104 2.66×103 

New adder (Fig. 2b) 1.91×106 4.88×104 







If S0 = 0, a in the example above, the result will be round to 
the small of the two neighbor numbers. Fig. 2c show how 
S0 affect the result. Z0 and Z1 be the output of the circuit with 
S0 = 0 and 1, respectively. The expect result be Z = 0.5(3/8 + 
1/4) = 5/16. Since N = 8 be not sufficient to represent 5/16 
exactly, the result be round to either 1/4 or 3/8. 

To quantify the accuracy of our propose adder, we compare 
it to the adder of Fig. 1b with three different SNG config- 
urations: (i) random bit-streams use for the data input and an 
LFSR use for the select input, (ii) random bit-streams for the 
data input and a TFF that toggle every cycle for the select 
input, and (iii) an LFSR use for the data input and a TFF for 
the select inputs. While the first configuration be more 
commonly used, we try two more configuration that provide 
a slight improvement. However, a see in Table 2, our propose 
adder achieves significantly good accuracy. Once again, the 
MSEs be calculate by exhaustively test the adder for every 
possible input value. 

IV. STOCHASTIC-BINARY NEURAL NETWORK DESIGN 

We now present our stochastic-binary hybrid design for near- 

sensor NN computation. Fig. 3 give an overview of the 

propose neural network layer and system design. To evaluate 

it utility, we will use it to implement the first layer of the 

LeNet-5 NN topology [19]. 

A. Signal Acquisition 

Image sensor capture light intensity and convert it to analog 

signals, which be convert to digital number for processing. 

In this work, we use part of a ramp-compare analog-to-digital 

converter (ADC) to convert the analog signal to the stochastic 

domain. The conversion circuit show in Fig. 3 be functionally 

equivalent to an SNG (Fig. 1c), with some modifications: (i) the 

input be analog, and (ii) a ramp signal be apply to the second 

input of the comparator rather than a random number generator. 

Despite become heavily auto-correlated, the bit-stream 

generate by this conversion circuit be still usable for our SC 

design, because the propose adder circuit be insensitive to 

input auto-correlation. Previous work have show such analog- 

to-stochastic converter be comparable, in term of cost and 

performance to regular ADCs [3][13]. Furthermore, prior work 

[26] have show such conversion operate on the order of 100 

pJ, which be much low than the energy consume by 

computation (100s of nJ/image). Thus, we do not include the 

cost of sensor data conversion in our evaluations. 

B. Stochastic Convolutional Neural Network Layer 

The stochastic NN layer consists of 784 stochastic dot-product 

unit show in Fig. 3 which process the sensor input in parallel. 

Because there be 32 different first layer kernels, we perform 

parallel convolution 32 time per image. The convolution 

engine perform a basic dot-product operation follow by 

stochastic-to-binary conversion and an activation function. 

More precisely, each convolution engine implements: 

𝑔(�⃗�, �⃗⃗⃗�) = 𝑠𝑖𝑔𝑛(�⃗� ∘ �⃗⃗⃗�) 

where �⃗� and �⃗⃗⃗� denote input window and kernel weights, 
respectively, and ∘ denotes the dot-product operation (�⃗� ∘ �⃗⃗⃗� = 
∑ 𝑥𝑖𝑤𝑖 

𝑘−1 
𝑖=0 ). The activation function simply output the sign of 

the dot product result and output either 1, 0, or 1. The weight 

input be share among all convolution engines, so the cost of 

generate them be amortize across all units. 

Since the computation involves negative numbers, the 

bipolar SC domain [1, 1] be a natural choice [7]. However, by 

employ bipolar SC, the decision point of activation 

function map to bit-streams with maximum fluctuation (i.e., 

unipolar value 0.5). This increase power usage and decrease 

accuracy. Therefore, we adopt a different approach which us 

only unipolar operation by divide the weight into positive 

and negative bit-streams �⃗⃗⃗�𝑝𝑜𝑠 and �⃗⃗⃗�𝑛𝑒𝑔. We then perform two 

unipolar dot product operations, 𝑔𝑝𝑜𝑠 = �⃗� ∘ �⃗⃗⃗�𝑝𝑜𝑠 and 𝑔𝑛𝑒𝑔 = 

�⃗� ∘ �⃗⃗⃗�𝑛𝑒𝑔, follow by two asynchronous counter to convert the 


Fig. 3. System diagram of our propose near-sensor stochastic NN. Bottom – LeNet-5 NN topology. Middle – system pipeline. Top – 

microarchitecture. Purple, grey, and blue region denote analog, stochastic, and binary domains, respectively. 





result 𝑔𝑝𝑜𝑠 and 𝑔𝑛𝑒𝑔 to the binary domain. Finally, the binary 

activation function be implement by a simple comparator. As 

show in Fig. 3, the rest of the NN operates in the binary 

domain. 

V. EXPERIMENTAL RESULTS 

This section present the result of experiment with the 

propose SC NN design. We mainly compare our design with 

a similar all-binary implementation, but when possible we also 

provide comparison with exist SC-based NNs. 

A. Experimental Setup 

We use the MNIST database [18], a standard machine learn 

benchmark for handwritten digit recognition, to evaluate 

accuracy. The benchmark consists of M = 70,000 image of 

handwritten digit (0 to 9); each image us a 28×28 8-bit 

greyscale encoding. A subset of 60,000 image be use to train 

the NN, while the remain 10,000 image be use to test it 

accuracy. Classification accuracy be define a the ratio of 

correctly classify test image to the total number of test 

images. Then the misclassification rate be define a one minus 

the classification accuracy. These metric be often multiply 

by 100 and report a a percentage. All NN training be 

perform use the TensorFlow framework [1], and the Keras 

library [10] use a NVIDIA Titan X GPU. For each stochastic 

design, we built a custom C++ model to evaluate it accuracy. 

Previous work on SC NNs [6][16] evaluates NN topology 

with only fully connect layer and achieves misclassification 

rate between 1.95% and 2.41%. On the other hand, our work 

us the LeNet-5 topology which have both convolutional and 

fully connect layers, and achieves misclassification rate 

around 1%. In practice, the number of convolutional layer 

kernel and the size of the kernel use in LeNet-5 vary; for our 

evaluation, we use a variant provide by the Keras library 

which have the topology show in Fig. 3. 

B. Accuracy Results and Neural Network Retraining 

A key tradeoff in SC be reduce precision to enhance 

performance. To quantify the impact of reduce precision on 

classification accuracy, we build separate NN model which 

execute the first layer of LeNet-5 at different precision level (2 

to 8 bits). We also replace the standard rectify linear 

activation function with a sign function, which do not impose 

a significant accuracy loss, but have a much simpler implement- 

tation in SC. We do not execute subsequent layer in the 

stochastic domain since precision loss would compound and 

require longer bit-streams to achieve accurate results. 

For comparison, we evaluate how precision reduction 

affect the fully binary implementation. Our experiment show 

that simply quantize the first layer weight and replace the 

activation function with sign detection reduces classification 

accuracy by several percentage point (up to 6.85% mis- 

classification rate for 4-bit precision). However, by retrain 

the rest of the NN weights, the NN model be able to recover 

from the noise introduce by loss in precision and the new 

activation function (Table 3). Interestingly, we find that we can 

reduce precision down to 3 or 4 bit and still achieve excellent 

misclassification rate (below 1%) after retraining. Since the 

training process be also noisy, the classification accuracy do 

not always exhibit monotonically decrease behavior a 

precision be reduced. 

Bit reduction of SC design exhibit similar accuracy losses, 

but lead to exponential run time reduction and energy savings. 

However, stochastic convolution present unique challenges. 

SC can be inexact at near-zero input values, and output value 

be sensitive to errors. Prior work [5] show that a non-trivial 

percentage of NN value be near zero, so we use weight scale 

and soft thresholding a propose by Kim et al. [16] to mitigate 

these errors. Weight scale normalizes the value of each 

convolution kernel to use the full dynamic range [1, 1] while 

soft thresholding force a result to zero if it be within some 

threshold. Finally, we also employ the retrain technique 

introduce early in the binary domain of the design. 

We now compare the result classification accuracy use 

SC with our new adder and multiplier, and the conventional 

adder and multiplier introduce early in Fig. 1 that be use 

in prior work. Table 3 show misclassification rate (lower be 

better) for each design. The result indicate that our new adder 

and multiplier generally achieve low misclassification rate 

than those in prior SC work (up to 2.92% better). We be also 

able to achieve misclassification rate which be within 0.05% 

and 0.25% of the binary design for 8-bit and 4-bit precision 

respectively. Further, the result show that retrain the NN 

model can compensate for noise introduce by both precision 

reduction and SC. In particular, for our more accurate adder and 

multiplication scheme there be less noise that the retrain 

process must compensate for than the old adder. Note that the 

benefit of the retrain be only possible because we can 

operate in the high precision binary domain. Finally, our 

result confirm that there be significant opportunity for precision 

Table 3. Misclassification rate for full binary and hybrid stochastic-binary designs, and throughput-normalized power, energy efficiency, and 

area result for binary and stochastic convolution designs. 

Design 8 Bits 7 Bits 6 Bits 5 Bits 4 Bits 3 Bits 2 Bits 

Misclassification 

Rate (%) 

Binary 0.89% 0.86% 0.89% 0.74% 0.79% 0.79% 1.30% 

Old SC 2.22% 3.91% 1.30% 1.55% 1.63% 2.71% 4.89% 

This Work 0.94% 0.99% 1.04% 1.12% 1.04% 2.20% 43.82% 

Normalized 

Power (mW) 

Binary 40.95 mW 72.80 mW 121.52 mW 204.96 mW 325.36 mW 501.76 mW 683.20 mW 

This Work 33.17 mW 33.55 mW 33.26 mW 33.01 mW 33.20 mW 29.96 mW 28.35 mW 

Energy Efficiency 

(nJ / frame) 

Binary 670.92 nJ 596.38 nJ 497.74 nJ 419.76 nJ 333.17 nJ 256.90 nJ 174.90 nJ 

This Work 543.42 nJ 274.82 nJ 136.22 nJ 67.60 nJ 34.00 nJ 15.34 nJ 7.26 nJ 

Area 

(mm2) 

Binary 1.313 mm2 1.094 mm2 0.891 mm2 0.710 mm2 0.543 mm2 0.391 mm2 0.255 mm2 

This Work 1.321 mm2 1.282 mm2 1.240 mm2 1.200 mm2 1.166 mm2 1.110 mm2 1.057 mm2 





reduction in SC, which translates to exponential reduction in 

bit-stream length and good run times, which we explore next. 

VI. POWER, AREA, AND ENERGY EVALUATION 

We synthesize, place-and-route, and measure power use 

Synopsys Design Compiler, IC Compiler, and PrimeTime for 

our design; we also use a 65nm TSMC library. For comparison, 

we evaluate a slide window convolution engine a our binary 

baseline design [23]. Activity factor for power measurement 

be record use trace base on MNIST test image and 

weight from the TensorFlow model. 

Table 3 show the throughput-normalized power, energy 

efficiency, and design area for both stochastic and binary 

convolution designs. Power measurement be throughput- 

normalize relative to the stochastic design. For instance, a 

binary design operating at 0.25× the throughput and 2× the 

power relative to a stochastic design would have a throughput- 

normalize power of 8× relative to the stochastic design. Since 

run time of stochastic design decrease exponentially with 

low precision, we find that the binary design must operate at 

exponentially high frequency and power to match the increase 

in throughput. Finally, we find the area and energy cost of the 

SC number generator be high than a single SC dot product 

unit, but the cost be share and amortize over many units. 

Since the actual operating frequency will vary across 

application demands, we contrast the throughput-normalized 

power between the stochastic and binary designs. Throughput- 

normalize power be more representative of energy efficiency 

since it be more agnostic to the difference in frequency and 

number of parallel unit in the design. In term of energy 

efficiency, our design break even with binary design at 8-bit 

precision, and be 9.8× more energy efficient at 4-bit precision. 

Furthermore, it achieves these gain with good classification 

accuracy than prior work. 

Finally, we see that our stochastic convolution design 

achieves reasonable area overhead relative to the binary one. 

The stochastic convolution engine exhibit virtually no change 

in resource utilization since precision in SC only affect the 

length of the bit-streams. However, binary design benefit from 

linear area reduction since reduce precision narrow the 

datapath. We find that our design achieves roughly the same 

area a the binary design at 8-bit precision but be 2× large than 

the binary design at 4-bit precision. 

VII. CONCLUSIONS 

We present a convolutional NN system which employ a 

hybrid stochastic-binary design for near-sensor computing. The 

design employ near-sensor SC use a novel stochastic adder 

which be significantly more accurate than previous adder 

designs. Our simulation show that with this adder, the hybrid 

NN achieves up to 2.92% good accuracy than previous SC 

designs, and 9.8× good energy efficiency for convolution over 

all-binary designs. Finally, we show that retrain the binary 

domain portion of the NN can compensate for precision loss 

from SC. As NNs become increasingly commonplace in 

modern applications, the energy efficiency gain offer by SC 

will be invaluable for meeting the aggressive power and energy 

budget of next generation sensor and embed devices. 

VIII. ACKNOWLEDGEMENTS 

This work be support in part by the National Science 

Foundation under Grant CCF-1318091 and Grant CCF- 

1518703, and generous gift from Oracle Labs and Microsoft. 

REFERENCES 

[1] M. Adabi et al., TensorFlow: Large-scale Machine Learning on 
Heterogeneous Systems [Online], Available: http://tensorflow.org/, 
[Accessed: 17-Sep-2016]. 

[2] J. M. de Aguiar and S. P. Khatri, “Exploring the viability of stochastic 
computing,” Proc. ICCD, pp. 391-394, 2015. 

[3] A. Alaghi et al., “Stochastic circuit for real-time image-processing 
applications,” Proc. DAC, pp. 1-6, 2013. 

[4] A. Alaghi and J. P. Hayes, “Fast and accurate computation use 
stochastic circuits,” Proc. DATE, pp. 1-4, 2014. 

[5] J. Albericio et al., “Cnvlutin: Ineffectual-neuron-free deep neural 
network computing,” Proc. ISCA, pp. 1-13, 2016. 

[6] A. Ardakani et al., “VLSI implementation of deep neural network use 
integral stochastic computing,” Proc. ISTC, pp. 216-220, 2016. 

[7] B. D. Brown and H. C. Card, “Stochastic neural computation. I. 
Computational elements,” IEEE Trans. Comp., pp. 891-905, 2001. 

[8] J. Bryzek, “Roadmap to a $ trillion MEMS market,” MEMS Technology 
Symposium, Vol. 23, 2012. 

[9] V. Canals et al., “A new stochastic compute methodology for efficient 
neural network implementation,” IEEE Trans. Neural Networks and 
Learning Systems, pp. 551-564, 2016. 

[10] F. Chollet., Keras [Online], Available: https://github.com/fchollet/keras, 
[Accessed: 17-Sep-2016]. 

[11] H. G. Chen et al., “ASP vision: optically compute the first layer of 
convolutional neural network use angle sensitive pixels,” Proc. 
CVPR, 2016. 

[12] Z. Du et al., “ShiDianNao: shift vision processing closer to the 
sensor,” SIGARCH Comput. Archit. News, pp. 92-104, 2015. 

[13] D. Fick et al., “Mixed-signal stochastic computation demonstrate in an 
image sensor with integrate 2d edge detection and noise filtering,” 
Proc. Custom Integrated Circuits Conference (CICC), pp. 1-4, 2014. 

[14] B.R. Gaines, “Stochastic compute systems,” Advances in Information 
Systems Science, vol. 2, pp. 37-172, 1969. 

[15] Y. Ji et al., “A hardware implementation of a radial basis function neural 
network use stochastic logic,” Proc. DATE, pp. 880-883, 2015. 

[16] K. Kim et al., “Dynamic energy-accuracy trade-off use stochastic 
compute in deep neural networks,” Proc. DAC, pp. 124:1-6 , 2016. 

[17] Y.-C. Kim and M. A. Shanblatt, “Architecture and statistical model of a 
pulse-mode digital multilayer neural network,” IEEE Trans. Neural 
Networks, pp. 1109-1118, 1995. 

[18] Y. LeCun et al., The MNIST Database of Handwritten Digits [Online], 
http://yann.lecun.com/exdb/mnist/, [Accessed: 17-Sep-2016]. 

[19] Y. LeCun et al., “Gradient-based learn apply to document 
recognition,” Proc. IEEE, vol. 86, pp. 2278-2324, 1998. 

[20] R. LiKamWa et al., “RedEye: analog ConvNet image sensor architecture 
for continuous mobile vision," Proc. ISCA, 2016. 

[21] B. Li et al., “Using stochastic compute to reduce the hardware 
requirement for a restrict Boltzmann machine classifier,” Proc. 
FPGA, pp. 36-41, 2016. 

[22] B. Moons and M. Verhelst, “Energy-efficiency and accuracy of 
stochastic compute circuit in emerge technologies,” IEEE Jour. 
Emerging and Selected Topics in Circuits Syst., pp. 475-486, 2014. 

[23] A. E. Nelson, “Implementation of image processing algorithm on 
FPGA hardware,” M.S. thesis, EE Dept.,Vanderbilt Univ., 2000. 

[24] M.A. Nielsen, Neural Networks and Deep Learning, Determination 
Press, 2015. http://neuralnetworksanddeeplearning.com/. 

[25] W. Qian et al., “An architecture for fault-tolerant computation with 
stochastic logic,” IEEE Trans. Comp., vol. 60, pp. 93-105, 2011. 

[26] N. Verma and A. P. Chandrakasan, “An ultra low energy 12-bit rate- 
resolution scalable SAR ADC for wireless sensor nodes,” IEEE Journal 
of Solid-State Circuits, pp. 1196-1205, 2007. 




