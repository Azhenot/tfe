






































Machine Learningâ•Žs â•ŸAmazingâ•Ž Ability to Predict Chaos 


Machine Learning’s ‘Amazing’ Ability to Predict Chaos 

ByNatalie WolchoverApril 18, 2018 

chaos theory 

In new computer experiments, artificial-intelligence algorithm can tell the future of chaotic systems. 

Half a century ago, the pioneer of chaos theory discover that the “butterfly effect” make long-term prediction impossible. Even the small perturbation to a complex system (like the weather, the economy or just about anything else) can 
touch off a concatenation of event that lead to a dramatically divergent future. Unable to pin down the state of these system precisely enough to predict how they’ll play out, we live under a veil of uncertainty. 

But now the robot be here to help. 

In a series of result report in the journal Physical Review Letters and Chaos, scientist have use machine learn — the same computational technique behind recent success in artificial intelligence — to predict the future evolution of 
chaotic system out to stunningly distant horizons. The approach be be laud by outside expert a groundbreaking and likely to find wide application. 

“I find it really amaze how far into the future they predict” a system’s chaotic evolution, say Herbert Jaeger, a professor of computational science at Jacobs University in Bremen, Germany. 

The finding come from veteran chaos theorist Edward Ott and four collaborator at the University of Maryland. They employ a machine-learning algorithm call reservoir compute to “learn” the dynamic of an archetypal chaotic system 
call the Kuramoto-Sivashinsky equation. The evolve solution to this equation behaves like a flame front, flicker a it advance through a combustible medium. The equation also describes drift wave in plasma and other phenomena, and 
serf a “a test bed for study turbulence and spatiotemporal chaos,” say Jaideep Pathak, Ott’s graduate student and the lead author of the new papers. 

Jaideep Pathak, Michelle Girvan, Brian Hunt and Edward Ott of the University of Maryland, who (along with Zhixin Lu, now of the University of Pennsylvania) have show that machine learn be a powerful tool for predict chaos. 

Faye Levine/University of Maryland 

After training itself on data from the past evolution of the Kuramoto-Sivashinsky equation, the researchers’ reservoir computer could then closely predict how the flamelike system would continue to evolve out to eight “Lyapunov times” into the 
future, eight time further ahead than previous method allowed, loosely speaking. The Lyapunov time represent how long it take for two almost-identical state of a chaotic system to exponentially diverge. As such, it typically set the horizon 
of predictability. 

“This be really very good,” Holger Kantz, a chaos theorist at the Max Planck Institute for the Physics of Complex Systems in Dresden, Germany, say of the eight-Lyapunov-time prediction. “The machine-learning technique be almost a good a 
know the truth, so to say.” 

The algorithm know nothing about the Kuramoto-Sivashinsky equation itself; it only see data record about the evolve solution to the equation. This make the machine-learning approach powerful; in many cases, the equation describe a 
chaotic system aren’t known, cripple dynamicists’ effort to model and predict them. Ott and company’s result suggest you don’t need the equation — only data. “This paper suggests that one day we might be able perhaps to predict weather 
by machine-learning algorithm and not by sophisticated model of the atmosphere,” Kantz said. 

Besides weather forecasting, expert say the machine-learning technique could help with monitoring cardiac arrhythmia for sign of impend heart attack and monitoring neuronal fire pattern in the brain for sign of neuron spikes. More 
speculatively, it might also help with predict rogue waves, which endanger ships, and possibly even earthquakes. 

Ott particularly hope the new tool will prove useful for give advance warn of solar storms, like the one that erupt across 35,000 mile of the sun’s surface in 1859. That magnetic outburst create aurora borealis visible all around the 
Earth and blew out some telegraph systems, while generate enough voltage to allow other line to operate with their power switch off. If such a solar storm lash the planet unexpectedly today, expert say it would severely damage Earth’s 
electronic infrastructure. “If you knew the storm be coming, you could just turn off the power and turn it back on later,” Ott said. 

He, Pathak and their colleague Brian Hunt, Michelle Girvan and Zhixin Lu (who be now at the University of Pennsylvania) achieve their result by synthesize exist tools. Six or seven year ago, when the powerful algorithm know a “deep 
learning” be start to master AI task like image and speech recognition, they start reading up on machine learn and think of clever way to apply it to chaos. They learn of a handful of promising result predate the deep-learning 
revolution. Most importantly, in the early 2000s, Jaeger and fellow German chaos theorist Harald Haas make use of a network of randomly connect artificial neuron — which form the “reservoir” in reservoir compute — to learn the 
dynamic of three chaotically coevolving variables. After training on the three series of numbers, the network could predict the future value of the three variable out to an impressively distant horizon. However, when there be more than a few 
interact variables, the computation become impossibly unwieldy. Ott and his colleague need a more efficient scheme to make reservoir compute relevant for large chaotic systems, which have huge number of interrelate variables. 
Every position along the front of an advance flame, for example, have velocity component in three spatial direction to keep track of. 

It take year to strike upon the straightforward solution. “What we exploit be the locality of the interactions” in spatially extend chaotic systems, Pathak said. Locality mean variable in one place be influence by variable at nearby place 
but not by place far away. “By use that,” Pathak explained, “we can essentially break up the problem into chunks.” That is, you can parallelize the problem, use one reservoir of neuron to learn about one patch of a system, another reservoir 
to learn about the next patch, and so on, with slight overlap of neighbor domain to account for their interactions. 

Parallelization allows the reservoir compute approach to handle chaotic system of almost any size, a long a proportionate computer resource be dedicate to the task. 

If we have ignorance we should use the machine learn to fill in the gap where the ignorance resides. 

Edward Ott 

Ott explain reservoir compute a a three-step procedure. Say you want to use it to predict the evolution of a spread fire. First, you measure the height of the flame at five different point along the flame front, continue to measure the 
height at these point on the front a the flicker flame advance over a period of time. You feed these data-streams in to randomly chosen artificial neuron in the reservoir. The input data trigger the neuron to fire, trigger connect 
neuron in turn and send a cascade of signal throughout the network. 

The second step be to make the neural network learn the dynamic of the evolve flame front from the input data. To do this, a you feed data in, you also monitor the signal strength of several randomly chosen neuron in the reservoir. 
Weighting and combine these signal in five different way produce five number a outputs. The goal be to adjust the weight of the various signal that go into calculate the output until those output consistently match the next set of 
input — the five new height measure a moment late along the flame front. “What you want be that the output should be the input at a slightly late time,” Ott explained. 

To learn the correct weights, the algorithm simply compare each set of outputs, or predict flame height at each of the five points, to the next set of inputs, or actual flame heights, increase or decrease the weight of the various signal each 
time in whichever way would have make their combination give the correct value for the five outputs. From one time-step to the next, a the weight be tuned, the prediction gradually improve, until the algorithm be consistently able to predict 
the flame’s state one time-step later. 

“In the third step, you actually do the prediction,” Ott said. The reservoir, have learn the system’s dynamics, can reveal how it will evolve. The network essentially asks itself what will happen. Outputs be fed back in a the new inputs, whose 
output be fed back in a inputs, and so on, make a projection of how the height at the five position on the flame front will evolve. Other reservoir work in parallel predict the evolution of height elsewhere in the flame. 

In a plot in their PRL paper, which appear in January, the researcher show that their predict flamelike solution to the Kuramoto-Sivashinsky equation exactly match the true solution out to eight Lyapunov time before chaos finally wins, 
and the actual and predict state of the system diverge. 

The usual approach to predict a chaotic system be to measure it condition at one moment a accurately a possible, use these data to calibrate a physical model, and then evolve the model forward. As a ballpark estimate, you’d have to 
measure a typical system’s initial condition 100,000,000 time more accurately to predict it future evolution eight time further ahead. 

The machine-learning technique be almost a good a know the truth. 

Machine Learning’s ‘Amazing’ Ability to Predict Chaos https://www.quantamagazine.org/machine-learnings-amazing-ability-to-... 

1 sur 2 20-04-18 à 19:16 



Holger Kantz 

That’s why machine learn be “a very useful and powerful approach,” say Ulrich Parlitz of the Max Planck Institute for Dynamics and Self-Organization in Göttingen, Germany, who, like Jaeger, also apply machine learn to low- 
dimensional chaotic system in the early 2000s. “I think it’s not only work in the example they present but be universal in some sense and can be apply to many process and systems.” In a paper soon to be publish in Chaos, Parlitz and a 
collaborator apply reservoir compute to predict the dynamic of “excitable media,” such a cardiac tissue. Parlitz suspect that deep learning, while be more complicate and computationally intensive than reservoir computing, will also 
work well for tackle chaos, a will other machine-learning algorithms. Recently, researcher at the Massachusetts Institute of Technology and ETH Zurich achieve similar result a the Maryland team use a “long short-term memory” neural 
network, which have recurrent loop that enable it to store temporary information for a long time. 

Since the work in their PRL paper, Ott, Pathak, Girvan, Lu and other collaborator have come closer to a practical implementation of their prediction technique. In new research accepted for publication in Chaos, they show that improve 
prediction of chaotic system like the Kuramoto-Sivashinsky equation become possible by hybridize the data-driven, machine-learning approach and traditional model-based prediction. Ott see this a a more likely avenue for improve 
weather prediction and similar efforts, since we don’t always have complete high-resolution data or perfect physical models. “What we should do be use the good knowledge that we have where we have it,” he said, “and if we have ignorance we 
should use the machine learn to fill in the gap where the ignorance resides.” The reservoir’s prediction can essentially calibrate the models; in the case of the Kuramoto-Sivashinsky equation, accurate prediction be extend out to 12 
Lyapunov times. 

The duration of a Lyapunov time varies for different systems, from millisecond to million of years. (It’s a few day in the case of the weather.) The shorter it is, the touchier or more prone to the butterfly effect a system is, with similar state 
depart more rapidly for disparate futures. Chaotic system be everywhere in nature, go haywire more or less quickly. Yet strangely, chaos itself be hard to pin down. “It’s a term that most people in dynamical system use, but they kind of 
hold their nose while use it,” say Amie Wilkinson, a professor of mathematics at the University of Chicago. “You feel a bit cheesy for say something be chaotic,” she said, because it grab people’s attention while have no agreed-upon 
mathematical definition or necessary and sufficient conditions. “There be no easy concept,” Kantz agreed. In some cases, tune a single parameter of a system can make it go from chaotic to stable or vice versa. 

Wilkinson and Kantz both define chaos in term of stretch and folding, much like the repeat stretch and fold of dough in the make of puff pastries. Each patch of dough stretch horizontally under the roll pin, separate 
exponentially quickly in two spatial directions. Then the dough be fold and flattened, compress nearby patch in the vertical direction. The weather, wildfires, the stormy surface of the sun and all other chaotic system act just this way, 
Kantz said. “In order to have this exponential divergence of trajectory you need this stretching, and in order not to run away to infinity you need some folding,” where fold come from nonlinear relationship between variable in the systems. 

The stretch and compress in the different dimension correspond to a system’s positive and negative “Lyapunov exponents,” respectively. In another recent paper in Chaos, the Maryland team report that their reservoir computer could 
successfully learn the value of these characterize exponent from data about a system’s evolution. Exactly why reservoir compute be so good at learn the dynamic of chaotic system be not yet well understood, beyond the idea that the 
computer tune it own formula in response to data until the formula replicate the system’s dynamics. The technique work so well, in fact, that Ott and some of the other Maryland researcher now intend to use chaos theory a a way to good 
understand the internal machination of neural networks. 

Machine Learning’s ‘Amazing’ Ability to Predict Chaos https://www.quantamagazine.org/machine-learnings-amazing-ability-to-... 

2 sur 2 20-04-18 à 19:16 


