






















































Deep Video Portraits 


Deep Video Portraits 

HYEONGWOO KIM,Max Planck Institute for Informatics, Germany 
PABLO GARRIDO, Technicolor, France 
AYUSH TEWARI and WEIPENG XU,Max Planck Institute for Informatics, Germany 
JUSTUS THIES and MATTHIAS NIESSNER, Technical University of Munich, Germany 
PATRICK PÉREZ, Technicolor, France 
CHRISTIAN RICHARDT, University of Bath, United Kingdom 
MICHAEL ZOLLHÖFER, Stanford University, United States of America 
CHRISTIAN THEOBALT,Max Planck Institute for Informatics, Germany 

Input Output Input Output 

Fig. 1. Unlike current face reenactment approach that only modify the expression of a target actor in a video, our novel deep video portrait approach enables 
full control over the target by transfer the rigid head pose, facial expression and eye motion with a high level of photorealism. 

We present a novel approach that enables photo-realistic re-animation of 

portrait video use only an input video. In contrast to exist approach 

that be restrict to manipulation of facial expression only, we be the irst 

to transfer the full 3D head position, head rotation, face expression, eye gaze, 

and eye blinking from a source actor to a portrait video of a target actor. The 

core of our approach be a generative neural network with a novel space-time 

architecture. The network take a input synthetic rendering of a parametric 

face model, base on which it predicts photo-realistic video frame for a 

give target actor. The realism in this rendering-to-video transfer be achieve 

by careful adversarial training, and a a result, we can create modiied target 

video that mimic the behavior of the synthetically-created input. In order 

to enable source-to-target video re-animation, we render a synthetic target 

video with the reconstruct head animation parameter from a source 

video, and feed it into the train network ś thus take full control of the 

Authors’ addresses: Hyeongwoo Kim, Max Planck Institute for Informatics, Campus 
E1.4, Saarbrücken, 66123, Germany, hyeongwoo.kim@mpi-inf.mpg.de; Pablo Gar- 
rido, Technicolor, 975 Avenue de Champs Blancs, Cesson-Sévigné, 35576, France, 
pablo.garrido.adrian@gmail.com; Ayush Tewari, atewari@mpi-inf.mpg.de; Weipeng 
Xu, wxu@mpi-inf.mpg.de, Max Planck Institute for Informatics, Campus E1.4, Saar- 
brücken, 66123, Germany; Justus Thies, justus.thies@tum.de; Matthias Nießner, 
niessner@tum.de, Technical University of Munich, Boltzmannstraße 3, Garching, 85748, 
Germany; Patrick Pérez, Technicolor, 975 Avenue de Champs Blancs, Cesson-Sévigné, 
35576, France, Patrick.Perez@technicolor.com; Christian Richardt, University of Bath, 
Claverton Down, Bath, BA2 7AY, United Kingdom, christian@richardt.name; Michael 
Zollhöfer, Stanford University, 353 Serra Mall, Stanford, CA, 94305, United States of 
America, zollhoefer@cs.stanford.edu; Christian Theobalt, Max Planck Institute for 
Informatics, Campus E1.4, Saarbrücken, 66123, Germany, theobalt@mpi-inf.mpg.de. 

© 2018 Association for Computing Machinery. 
This be the author’s version of the work. It be post here for your personal use. Not for 
redistribution. The deinitive Version of Record be publish in ACM Transactions on 
Graphics, https://doi.org/10.1145/3197517.3201283. 

target. With the ability to freely recombine source and target parameters, 

we be able to demonstrate a large variety of video rewrite application 

without explicitly model hair, body or background. For instance, we can 

reenact the full head use interactive user-controlled editing, and realize 

high-idelity visual dubbing. To demonstrate the high quality of our output, 

we conduct an extensive series of experiment and evaluations, where for 

instance a user study show that our video edits be hard to detect. 

CCS Concepts: · Computing methodology → Computer graphics; 

Neural networks; Appearance and texture representations; Animation; Ren- 

dering; 

Additional Key Words and Phrases: Facial Reenactment, Video Portraits, 

Dubbing, Deep Learning, Conditional GAN, Rendering-to-Video Translation 

ACM Reference Format: 

Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, 

Matthias Nießner, Patrick Pérez, Christian Richardt, Michael Zollhöfer, 

and Christian Theobalt. 2018. Deep Video Portraits. ACM Trans. Graph. 37, 4, 

Article 163 (August 2018), 14 pages. https://doi.org/10.1145/3197517.3201283 

1 INTRODUCTION 

Synthesizing and edit video portraits, i.e., video frame to show 

a person’s head and upper body, be an important problem in com- 

puter graphics, with application in video edit and movie post- 

production, visual efects, visual dubbing, virtual reality, and telep- 

resence, among others. In this paper, we address the problem of 

synthesize a photo-realistic video portrait of a target actor that 

mimic the action of a source actor, where source and target can be 

diferent subjects. More speciically, our approach enables a source 

actor to take full control of the rigid head pose, face expression and 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 

ar 
X 

iv 
:1 

80 
5. 

11 
71 

4v 
1 

[ 
c 

.C 
V 

] 
2 

9 
M 

ay 
2 

01 
8 



163:2 • H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt 

eye motion of the target actor; even face identity can be modiied to 

some extent. All of these dimension can be manipulate together or 

independently. Full target frames, include the entire head and hair, 

but also a realistic upper body and scene background comply 

with the modiied head, be automatically synthesized. 

Recently, many method have be propose for face-interior 

reenactment [Liu et al. 2001; Olszewski et al. 2017; Suwajanakorn 

et al. 2017; Thies et al. 2015, 2016; Vlasic et al. 2005]. Here, only 

the face expression can be modiied realistically, but not the full 

3D head pose, include a consistent upper body and a consistently 

change background. Many of these method it a parametric 3D 

face model to RGB(-D) video [Thies et al. 2015, 2016; Vlasic et al. 

2005], and re-render the modiied model a a blend overlay over 

the target video for reenactment, even in real time [Thies et al. 

2015, 2016]. Synthesizing a complete portrait video under full 3D 

head control be much more challenging. Averbuch-Elor et al. [2017] 

enable mild head pose change driven by a source actor base on 

image warping. They generate reactive dynamic proile picture 

from a static target portrait photo, but not fully reenact videos. 

Also, large change in head pose cause artifact (see Section 7.3), 

the target gaze cannot be controlled, and the identity of the target 

person be not fully preserve (mouth appearance be copy from the 

source actor). 

Performance-driven 3D head animation method [Cao et al. 2015, 

2014a, 2016; Hu et al. 2017; Ichim et al. 2015; Li et al. 2015; Olszewski 

et al. 2016; Weise et al. 2011] be related to our work, but have 

orthogonal methodology and application goals. They typically drive 

the full head pose of stylize 3D CG avatar base on visual source 

actor input, e.g., for game or stylize VR environments. Recently, 

Cao et al. [2016] propose image-based 3D avatar with dynamic 

texture base on a real-time face tracker. However, their goal be 

full 3D animate head control and rendering, often intentionally in 

a stylize rather than a photo-realistic fashion. 

We take a diferent approach that directly generates entire photo- 

realistic video portrait in front of general static background under 

full control of a target’s head pose, facial expression, and eye mo- 

tion. We formulate video portrait synthesis and reenactment a 

a rendering-to-video translation task. Input to our algorithm be 

synthetic rendering of only the coarse and fully-controllable 3D 

face interior model of a target actor and separately render eye 

gaze images, which can be robustly and eiciently obtain via 

a state-of-the-art model-based reconstruction technique. The in- 

put be automatically translate into full-frame photo-realistic video 

output show the entire upper body and background. Since we 

only track the face, we cannot actively control the motion of the 

torso or hair, or control the background, but our rendering-to-video 

translation network be able to implicitly synthesize a plausible body 

and background (including some shadow and relections) for a 

give head pose. This translation problem be tackle use a novel 

space-time encoderśdecoder deep neural network, which be train 

in an adversarial manner. 

At the core of our approach be a conditional generative adversarial 

network (cGAN) [Isola et al. 2017], which be speciically tailor 

to video portrait synthesis. For temporal stability, we use a novel 

space-time network architecture that take a input short sequence 

of conditioning input frame of head and eye gaze in a slide 

window manner to synthesize each target video frame. Our target 

and scene-speciic network only require a few minute of portrait 

video footage of a person for training. To the best of our knowledge, 

our approach be the irst to synthesize full photo-realistic video 

portrait of a target person’s upper body, include realistic clothing 

and hair, and consistent scene background, under full 3D control of 

the target’s head. To summarize, we make the follow technical 

contributions: 

• A rendering-to-video translation network that transforms 

coarse face model rendering into full photo-realistic portrait 

video output. 

• A novel space-time encode a conditional input for tempo- 

rally coherent video synthesis that represent face geometry, 

relectance, and motion a well a eye gaze and eye blinks. 

• A comprehensive evaluation on several application to demon- 

strate the lexibility and efectiveness of our approach. 

We demonstrate the potential and high quality of our method in 

many intrigue applications, range from face reenactment and 

visual dub for foreign language movie to user-guided interac- 

tive edit of portrait video for movie postproduction. A compre- 

hensive comparison to state-of-the-art method and a user study 

conirm the high idelity of our results. 

2 RELATED WORK 

We discus related optimization and learning-based method that 

aim at reconstructing, animate and re-writing face in image 

and videos, and review relevant image-to-image translation work. 

For a comprehensive overview of current method we refer to a 

recent state-of-the-art report on monocular 3D face reconstruction, 

track and application [Zollhöfer et al. 2018]. 

Monocular Face Reconstruction. Face reconstruction method aim 

to reconstruct 3D face model of shape and appearance from visual 

data. Optimization-based method it a 3D template model, mainly 

the inner face region, to single image [Blanz et al. 2004; Blanz 

and Vetter 1999], unstructured image collection [Kemelmacher- 

Shlizerman 2013; Kemelmacher-Shlizerman et al. 2011; Roth et al. 

2017] or video [Cao et al. 2014b; Fyfe et al. 2014; Garrido et al. 2016; 

Ichim et al. 2015; Shi et al. 2014; Suwajanakorn et al. 2014; Thies et al. 

2016; Wu et al. 2016]. Recently, Booth et al. [2018] propose a large- 

scale parametric face model construct from almost ten thousand 

3D scans. Learning-based approach leverage a large corpus of 

image or image patch to learn a regressor for predict either 

3D face shape and appearance [Richardson et al. 2016; Tewari et al. 

2017; Tran et al. 2017], ine-scale skin detail [Cao et al. 2015], or 

both [Richardson et al. 2017; Sela et al. 2017]. Deep neural network 

have be show to be quite robust for infer the coarse 3D 

facial shape and appearance of the inner face region, even when 

train on synthetic data [Richardson et al. 2016]. Tewari et al. 

[2017] show that encoderśdecoder architecture can be train 

fully unsupervised on in-the-wild image by integrate physical 

image formation into the network. Richardson et al. [2017] train 

an end-to-end regressor to recover facial geometry at a coarse and 

ine-scale level. Sela et al. [2017] use an encoderśdecoder network 

to infer a detailed depth image and a dense correspondence map, 

which serve a a basis for non-rigidly deform a template mesh. 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 



Deep Video Portraits • 163:3 

Fig. 2. Deep video portrait enable a source actor to fully control a target video portrait. First, a low-dimensional parametric representation (let) of both 
video be obtain use monocular face reconstruction. The head pose, expression and eye gaze can now be transfer in parameter space (middle). We do not 
focus on the modification of the identity and scene illumination (hatched background), since we be interested in reenactment. Finally, we render conditioning 
input image that be convert to a photo-realistic video portrait of the target actor (right). Obama video courtesy of the White House (public domain). 

Still, none of these method creates a fully generative model for the 

entire head, hair, mouth interior, and eye gaze, like we do. 

Video-based Facial Reenactment. Facial reenactment method re- 

write the face content of a target actor in a video or image by trans- 

ferring facial expression from a source actor. Facial expression 

be commonly transfer via dense motion ields [Averbuch-Elor 

et al. 2017; Liu et al. 2001; Suwajanakorn et al. 2015], parameter 

[Thies et al. 2016, 2018; Vlasic et al. 2005], or by warp candidate 

frame that be select base on the facial motion [Dale et al. 2011], 

appearance metric [Kemelmacher-Shlizerman et al. 2010] or both 

[Garrido et al. 2014; Li et al. 2014]. The method described above 

irst reconstruct and track the source and target faces, which be 

represent a a set of sparse 2D landmark or dense 3D models. 

Most approach only modify the inner region of the face and thus 

be mainly intend for alter facial expressions, but they do not 

take full control of a video portrait in term of rigid head pose, facial 

expression, and eye gaze. Recently, Wood et al. [2018] propose an 

approach for eye gaze redirection base on a itted parametric eye 

model. Their approach only provide control over the eye region. 

One notable exception to pure facial reenactment be Averbuch- 

Elor et al.’s approach [2017], which enables the reenactment of a 

portrait image and allows for slight change in head pose via image 

warp [Fried et al. 2016]. Since this approach be base on a single 

target image, it copy the mouth interior from the source to the 

target, thus preserve the target’s identity only partially. We take 

advantage of learn from a target video to allow for large change 

in head pose, facial reenactment, and joint control of the eye gaze. 

Visual Dubbing. Visual dub be a particular instance of face 

reenactment that aim to alter the mouth motion of the target actor 

to match a new audio track, commonly spoken in a foreign language 

by a dub actor. Here, we can ind speech-driven [Bregler et al. 

1997; Chang and Ezzat 2005; Ezzat et al. 2002; Liu and Ostermann 

2011; Suwajanakorn et al. 2017] or performance-driven [Garrido 

et al. 2015; Thies et al. 2016] techniques. Speech-driven dub tech- 

niques learn a person-speciic phoneme-to-viseme mapping from a 

training sequence of the actor. These method produce accurate lip 

sync with visually imperceptible artifacts, a recently demonstrate 

by Suwajanakorn et al. [2017]. However, they cannot directly con- 

trol the target’s facial expressions. Performance-driven technique 

overcome this limitation by transfer semantically-meaningful 

motion parameter and re-rendering the target model with photo- 

realistic relectance [Thies et al. 2016], and ine-scale detail [Garrido 

et al. 2015, 2016]. These approach generalize better, but do not 

edit the head pose and still struggle to synthesize photo-realistic 

mouth deformations. In contrast, our approach learns to synthesize 

photo-realistic facial motion and action from coarse renderings, 

thus enable the synthesis of expression and joint modiication of 

the head pose, with consistent body and background. 

Image-to-image Translation. Approaches use conditional GANs 

[Mirza and Osindero 2014], such a Isola et al.’s łpix2pixž [2017], 

have show impressive result on image-to-image translation task 

which convert between image of two diferent domains, such a 

map and satellite photos. These combine encoderśdecoder architec- 

tures [Hinton and Salakhutdinov 2006], often with skip-connections 

[Ronneberger et al. 2015], with adversarial loss function [Goodfel- 

low et al. 2014; Radford et al. 2016]. Chen and Koltun [2017] be 

the irst to demonstrate high-resolution result with 2megapixel 

resolution, use cascade reinement network without adversarial 

training. The late trend show that it be even possible to train high- 

resolution GANs [Karras et al. 2018] and conditional GANs [Wang 

et al. 2018] at similar resolutions. However, the main challenge be 

the requirement for pair training data, a correspond image 

pair be often not available. This problem be tackle by CycleGAN 

[Zhu et al. 2017], DualGAN [Yi et al. 2017], and UNIT [Liu et al. 

2017] ś multiple concurrent unsupervised image-to-image trans- 

lation technique that only require two set of unpaired training 

samples. These technique have capture the imagination of many 

people by translate between photograph and paintings, horse 

and zebras, face photo and depth a well a correspondence map 

[Sela et al. 2017], and translation from face photo to cartoon draw- 

ings [Taigman et al. 2017]. Ganin et al. [2016] learn photo-realistic 

gaze manipulation in images. Olszewski et al. [2017] synthesize a 

realistic inner face texture, but cannot generate a fully controllable 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 



163:4 • H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt 

output video, include person-speciic hair. Lassner et al. [2017] 

propose a generative model to synthesize people in clothing, and 

Ma et al. [2017] generate new image of person in arbitrary pose 

use image-to-image translation. In contrast, our approach enables 

the synthesis of temporally-coherent video portrait that follow the 

animation of a source actor in term of head pose, facial expression 

and eye gaze. 

3 OVERVIEW 

Our deep video portrait approach provide full control of the head 

of a target actor by transfer the rigid head pose, facial expres- 

sion, and eye motion of a source actor, while preserve the target’s 

identity and appearance. Full target video frame be synthesized, 

include consistent upper body posture, hair and background. First, 

we track the source and target actor use a state-of-the-art monoc- 

ular face reconstruction approach that us a parametric face and 

illumination model (see Section 4). The result sequence of low- 

dimensional parameter vector represent the actor’s identity, head 

pose, expression, eye gaze, and the scene light for every video 

frame (Figure 2, left). This allows u to transfer the head pose, ex- 

pression, and/or eye gaze parameter from the source to the target, 

a desired. In the next step (Figure 2, middle), we generate new 

synthetic rendering of the target actor base on the modiied pa- 

rameters (see Section 5). In addition to a normal color rendering, we 

also render correspondence map and eye gaze images. These ren- 

derings serve a conditioning input to our novel rendering-to-video 

translation network (see Section 6), which be train to convert the 

synthetic input into photo-realistic output (see Figure 2, right). For 

temporally coherent results, our network work on space-time vol- 

umes of conditioning inputs. To process a complete video, we input 

the conditioning space-time volume in a slide window fashion, 

and assemble the inal video from the output frames. We evaluate 

our approach (see Section 7) and show it potential on several video 

rewrite applications, such a full-head reenactment, gaze redirection, 

video dubbing, and interactive parameter-based video control. 

4 MONOCULAR FACE RECONSTRUCTION 

We employ a state-of-the-art dense face reconstruction approach 

that it a parametric model of face and illumination to each video 

frame. It obtains a meaningful parametric face representation for 

the source Vs = {Is 
f 
| f = 1, . . . ,Ns } and target V 

t 
= {It 

f 
| f = 

1, . . . ,Nt } video sequence, where Ns and Nt denote the total num- 

ber of source and target frames, respectively. Let P• = {P• 
f 
| f = 

1, . . . ,N•} be the correspond parameter sequence that fully de- 

scribe the source or target facial performance. The set of recon- 

structed parameter encode the rigid head pose (rotation R• ∈SO(3) 

and translation t• ∈R3), facial identity coeicients α • ∈RNα (ge- 

ometry, Nα = 80) and β 
• ∈RNβ (relectance, Nβ = 80), expression 

coeicients δ• ∈RNδ (Nδ =64), gaze direction for both eye e 
• ∈R4, 

and spherical harmonic illumination coeicientsγ• ∈ R27. Overall, 

our monocular face tracker reconstructs Np =261 parameter per 

video frame. In the following, we provide more detail on the face 

track algorithm a well a the parametric face representation. 

Parametric Face Representation. We represent the space of facial 

identity base on a parametric head model [Blanz and Vetter 1999], 

and the space of facial expression via an aine model. Mathemati- 

cally, we model geometry variation through an aine model v∈R3N 

that stack per-vertex deformation of the underlie template mesh 

with N vertices, a follows: 

v(α ,δ) = ageo + 

Nα 
∑ 

k=1 

αkb 
geo 

k 
+ 

Nδ 
∑ 

k=1 

δkb 
exp 
k 

. (1) 

Difuse skin relectance be model similarly by a second aine 

model r∈R3N that stack the difuse per-vertex albedo: 

r(β) = aref + 

Nβ 
∑ 

k=1 

βkb 
ref 
k 

. (2) 

The vector ageo ∈ R 
3N and aref ∈ R 

3N store the average facial 

geometry and correspond skin relectance, respectively. The 

geometry basis {b 
geo 

k 
} 
Nα 
k=1 

have be compute by apply principal 

component analysis (PCA) to 200 high-quality face scan [Blanz 

and Vetter 1999]. The relectance basis {bref 
k 

} 
Nβ 
k=1 

have be obtain 

in the same manner. For dimensionality reduction, the expression 

basis {b 
exp 
k 

} 
Nδ 
k=1 

have be compute use PCA, start from the 

blendshapes of Alexander et al. [2010] and Cao et al. [2014b]. Their 

blendshapes have be transfer to the topology of Blanz and 

Vetter [1999] use deformation transfer [Sumner and Popović 2004]. 

Image Formation Model. To render synthetic head images, we 

assume a full perspective camera that map model-space 3D point 

v via camera space v̂∈R3 to 2D point p=Π(v̂) ∈R2 on the image 

plane. The perspective mapping Π contains the multiplication with 

the camera intrinsics and the perspective division. We assume a 

ixed and identical camera for all scenes, i.e., world and camera space 

be the same, and the face model account for all the scene motion. 

Based on a distant illumination assumption, we use the spherical 

harmonic (SH) basis function Yb : R 
3 → R to approximate the 

incoming radiance B from the environment: 

B(ri ,ni ,γ ) = ri · 

B2 
∑ 

b=1 

γbYb (ni ). (3) 

Here, B be the number of spherical harmonic bands,γb ∈R 
3 be the 

SH coeicients, and ri and ni be the relectance and unit normal 

vector of the i-th vertex, respectively. For difuse materials, an av- 

erage approximation error below 1 percent be achieve with only 

B = 3 bands, independent of the illumination [Ramamoorthi and 

Hanrahan 2001], since the incident radiance be in general a smooth 

function. This result in B2=9 parameter per color channel. 

Dense Face Reconstruction. We employ a dense data-parallel face 

reconstruction approach to eiciently compute the parameter P• 

for both source and target videos. Face reconstruction be base on an 

analysis-by-synthesis approach that maximizes photo-consistency 

between a synthetic render of the model and the input. The 

reconstruction energy combine term for dense photo-consistency, 

landmark alignment and statistical regularization: 

E(X) = wphotoEphoto(X) +wlandEland(X) +wregEreg(X), (4) 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 



Deep Video Portraits • 163:5 

withX= {R•, t•,α •, β•,δ•,γ•}. This enables the robust reconstruc- 

tion of identity (geometry and skin relectance), facial expression, 

and scene illumination. We use 66 automatically detect facial 

landmark of the True Vision Solution tracker1, which be a commer- 

cial implementation of Saragih et al. [2011], to deine the sparse 

alignment term Eland. Similar to Thies et al. [2016], we use a robust 

ℓ1-norm for dense photometric alignment Ephoto. The regularizer 

Ereg enforces statistically plausible parameter value base on the 

assumption of normally distribute data. The eye gaze estimate e• 

be directly obtain from the landmark tracker. The identity be only 

estimate in the irst frame and be kept constant afterwards. All 

other parameter be estimate every frame. For more detail on 

the energy formulation, we refer to Garrido et al. [2016] and Thies 

et al. [2016]. We use a data-parallel implementation of iteratively 

re-weighted least square (IRLS), similar to Thies et al. [2016], to 

ind the optimal set of parameters. One diference to their work be 

that we compute and explicitly store the Jacobian J and the residual 

vector F to global memory base on a data-parallel strategy that 

launch one thread per matrix/vector element. Afterwards, a data- 

parallel matrixśmatrix/matrixśvector multiplication computes the 

right- and left-hand side of the normal equation that have to be 

solve in each IRLS step. The result small linear system (97×97 

in track mode, 6 DoF rigid pose, 64 expression parameter and 27 

SH coeicients) be solve on the CPU use Cholesky factorization 

in each IRLS step. The reconstruction of a single frame take 670ms 

(all parameters) and 250ms (without identity, track mode). This 

allows the eicient generation of the training corpus that be require 

by our space-time rendering-to-video translation network (see Sec- 

tion 6). Contrary to Garrido et al. [2016] and Thies et al. [2016], our 

model feature dimension to model eyelid closure, so eyelid motion 

be capture well. 

5 SYNTHETIC CONDITIONING INPUT 

Using the method from Section 4, we reconstruct the face in each 

frame of the source and unmodiied target video. Next, we obtain the 

modiied parameter vector for every frame of the target sequence, 

e.g., for full-head reenactment, we modify the rigid head pose, ex- 

pression and eye gaze of the target actor. All parameter be copy 

in a relative manner from the source to the target, i.e., with respect 

to a neutral reference frame. Then we render synthetic conditioning 

image of the target actor’s face model under the modiied parame- 

ters use hardware rasterization. For high temporal coherence, 

our rendering-to-video translation network take a space-time vol- 

ume of conditioning image {Cf −o |o=0, . . . , 10} a input, with f 

be the index of the current frame. We use a temporal window of 

size Nw =11, with the current frame be at it end. This provide 

the network a history of the early motions. 

For each frame Cf −o of the window, we generate three diferent 

conditioning inputs: a color rendering, a correspondence image, and 

an eye gaze image (see Figure 3). The color render show the 

modiied target actor model under the estimate target illumination, 

while keep the target identity (geometry and skin relectance) 

ixed. This image provide a good start point for the follow 

rendering-to-video translation, since in the face region only the 

1http://truevisionsolutions.net 

Diffuse Rendering Correspondence Eye and Gaze Map 

Fig. 3. The synthetic input use for conditioning our rendering-to-video 
translation network: (1) color face render under target illumination, 
(2) correspondence image, and (3) the eye gaze image. 

delta to a real image have to be learned. In addition to this color input, 

we also provide a correspondence image encode the index of the 

parametric face model’s vertex that project into each pixel. To this 

end, we texture the head model with a constant unique gradient 

texturemap, and render it. Finally, we also provide an eye gaze image 

that solely contains the white region of both eye and the location 

of the pupil a blue circles. This image provide information about 

the eye gaze direction and blinking to the network. 

We stack all Nw conditioning input of a time window in a 3D 

tensor X of sizeW ×H × 9Nw (3 images, with 3 channel each), to 

obtain the input to our rendering-to-video translation network. To 

process the complete video, we feed the conditioning space-time 

volume in a slide window fashion. The inal generate photo- 

realistic video output be assemble directly from the output frames. 

6 RENDERING-TO-VIDEO TRANSLATION 

The generate conditioning space-time video tensor be the input to 

our rendering-to-video translation network. The network learns to 

convert the synthetic input into full frame of a photo-realistic target 

video, in which the target actor now mimic the head motion, facial 

expression and eye gaze of the synthetic input. The network learns to 

synthesize the entire actor in the foreground, i.e., the face for which 

conditioning input exists, but also all other part of the actor, such a 

hair and body, so that they comply with the target head pose. It also 

synthesizes the appropriately modiied and illed-in background, 

include even some consistent light efects between foreground 

and background. The network be train for a speciic target actor 

and a speciic static, but otherwise general scene background. Our 

rendering-to-video translation network follow an encoderśdecoder 

architecture and be train in an adversarial manner base on a 

discriminator that be jointly trained. In the following, we explain 

the network architectures, the use loss function and the training 

procedure in detail. 

Network Architecture. We show the architecture of our rendering- 

to-video translation network in Figure 4. Our conditional generative 

adversarial network consists of a space-time transformation network 

T and a discriminator D. The transformation network T take the 

W × H × 9Nw space-time tensor X a input and output a photo- 

real image T(X) of the target actor. The temporal input enables the 

network to take the history of motion into account by inspect 

previous conditioning images. The temporal axis of the input tensor 

be align along the network channels, i.e., the convolution in the 

irst layer have 9Nw channels. Note, we store all image data in 

normalize [−1,+1]-space, i.e, black be mapped to [−1,−1,−1]⊤ and 

white be mapped to [+1,+1,+1]⊤. 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 



163:6 • H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt 

Y 

Bilinear Downsampling 

T(X) 

channels64 128 256 128 649�# 3… 3 

D256 

X 

… 

T 

64 
128 

256 

128 

64 

64 

32 

32 
64 

128 

TanH 

256 

256 

128 

32 64 128 

= 

BN LReLu Up 

= 

DeConv BN ReLu Refine 

= 

Conv BN ReLuUp Refine 

= 

Conv Drop Drop 

in 

out 

in 

out 

3× 
3 

4× 
4 

Refine 

p 
ro 

b 
. 

p 
ro 

b 
. 

st 
ri 

d 
e 

2 

st 
ri 

d 
e 

1 

st 
ri 

d 
e 

2 
4× 

4 

Fig. 4. Architecture of our rendering-to-video translation network for an 
input resolution of 256×256: The encoder have 8 downsampling module 
with (64, 128, 256, 512, 512, 512, 512, 512) output channels. The decoder 
have 8 upsampling module with (512, 512, 512, 512, 256, 128, 64, 3) output 
channels. The upsampling module use the follow dropout probability 
(0.5, 0.5, 0.5, 0, 0, 0, 0, 0). The first downsampling and the last upsampling 
module do not employ batch normalization (BN). The final non-linearity 
(TanH) brings the output to the employ normalize [−1, +1]-space. 

Our network consists of two main parts, an encoder for com- 

put a low-dimensional latent representation, and a decoder for 

synthesize the output image. We employ skip connection [Ron- 

neberger et al. 2015] to enable the network to transfer ine-scale 

structure. To generate video frame with suicient resolution, our 

network also employ a cascade reinement strategy [Chen and 

Koltun 2017]. In each downsampling step, we use a convolution 

(4 × 4, stride 2) follow by batch normalization and a leaky ReLU 

non-linearity. The upsampling module be speciically design to 

produce high-quality output, and have the follow structure: irst, 

the resolution be increase by a factor of two base on deconvolu- 

tion (4 × 4, upsampling factor of 2), batch normalization, dropout 

and ReLU. Afterwards, two reinement step base on convolution 

(3 × 3, stride 1, stay on the same resolution) and ReLU be applied. 

The inal hyperbolic tangent non-linearity (TanH) brings the output 

tensor to the normalize [−1,+1]-space use for store the image 

data. For more details, please refer to Figure 4. 

The input to our discriminator D be the conditioning input tensor 

X (sizeW ×H × 9Nw ), and either the predict output image T(X) 

or the ground-truth image, both of sizeW × H × 3. The employ 

discriminator be inspire by the PatchGAN classiier, propose by 

Isola et al. [2017]. We extend it to take volume of conditioning 

image a input. 

Objective Function. We train in an adversarial manner to ind the 

best rendering-to-video translation network: 

T∗ = argmin 
T 

max 
D 

EcGAN(T,D) + λEℓ1 (T). (5) 

This objective function comprises an adversarial loss EcGAN(T,D) 

and an ℓ1-norm reproduction loss Eℓ1 (T). The constant weight of 

λ=100 balance the contribution of these two terms. The adversarial 

loss have the follow form: 

EGAN(T,D) = EX,Y 
[ 

logD(X,Y) 
] 

+ EX 

[ 

log 
( 

1 − D(X,T(X)) 
) ] 

. (6) 

We do not inject a noise vector while training our network to pro- 

duce deterministic outputs. During adversarial training, the discrim- 

inator D try to get good at classify give image a real or 

synthetic, while the transformation network T try to improve in 

fooling the discriminator. The ℓ1-norm loss penalizes the distance 

between the synthesize image T(X) and the ground-truth image Y, 

which encourages the sharpness of the synthesize output: 

Eℓ1 (T) = EX,Y 
[ 

∥Y − T(X)∥1 
] 

. (7) 

Training. We construct the training corpus T= {(Xi ,Yi )}i base 

on the tracked video frame of the target video sequence. Typically, 

two thousand video frames, i.e., about one minute of video footage, 

be suicient to train our network (see Section 7). Our training 

corpus consists of Nt −(Nw −1) render conditioning space-time 

volume Xi and the correspond ground-truth image Yi (using a 

window size of Nw =11). We train our network use the Tensor- 

Flow [Abadi et al. 2015] deep learn framework. The gradient 

for back-propagation be obtain use Adam [Kingma and Ba 

2015]. We train for 31,000 iteration with a batch size of 16 (approx. 

250 epoch for a training corpus of 2000 frames) use a base learn- 

ing rate of 0.0002 and irst momentum of 0.5; all other parameter 

have their default value. We train our network from scratch, and 

initialize the weight base on a Normal distribution N(0, 0.2). 

7 RESULTS 

Our approach enables full-frame target video portrait synthesis un- 

der full 3D head pose control. We measure the runtime for training 

and test on an Intel Xeon E5-2637 with 3.5 GHz (16GB RAM) and 

an NVIDIA GeForce GTX Titan Xp (12GB RAM). Training our net- 

work take 10 hour for a target video resolution of 256×256 pixels, 

and 42 hour for 512×512 pixels. Tracking the source actor take 

250ms per frame (without identity), and the rendering-to-video 

conversion (inference) take 65ms per frame for 256×256 pixels, or 

196ms for 512×512 pixels. 

In the following, we evaluate the design choice of our deep video 

portrait algorithm, compare to current state-of-the-art reenactment 

approaches, and show the result of a large-scale web-based user 

study. We further demonstrate the potential of our approach on sev- 

eral video rewrite applications, such a reenactment under full head 

and facial expression control, facial expression reenactment only, 

video dubbing, and live video portrait edit under user control. 

In total, we apply our approach to 14 diferent target sequence 

of 13 diferent subject and use 5 diferent source sequences; see 

Appendix A for details. A comparison to a simple nearest-neighbor 

retrieval approach can be found in Figure 6 and in the supplemental 

video. Our approach require only a few minute of target video 

footage for training. 

7.1 Applications 

Our approach enables u to take full control of the rigid head pose, 

facial expression, and eye motion of a target actor in a video por- 

trait, thus opening up a wide range of video rewrite applications. 

All parameter dimension can be estimate and transfer from a 

source video sequence or edit manually through an interactive 

user interface. 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 



Deep Video Portraits • 163:7 

S 
o 
u 
rc 
e 

T 
a 
rg 
e 
t 

S 
o 
u 
rc 
e 

T 
a 
rg 
e 
t 

Fig. 5. ualitative result of full-head reenactment: our approach enables full-frame target video portrait synthesis under full 3D head pose control. The 
output video portrait be photo-realistic and hard to distinguish from real videos. Note that even the shadow in the background of the second row move 
consistently with the modify foreground head motion. In the sequence at the top, we only transfer the translation in the camera plane, while we transfer the 
full 3D translation for the sequence at the botom. For full sequences, please refer to our video. Obama video courtesy of the White House (public domain). 

Input OursNearest Neighbor 

Fig. 6. Comparison to a nearest-neighbor approach in parameter space (pose 
and expression). Our result have high quality and be temporally more 
coherent (see supplemental video). For the nearest-neighbor approach, it be 
dificult to find the right trade-of between pose and expression. This lead 
to many result with one of the two dimension not be well-matched. 
The result be also temporally unstable, since the near neighbor abruptly 
changes, especially for small training sets. 

Reenactment under full head control. Our approach be the irst that 

can photo-realistically transfer the full 3D head pose (spatial position 

and rotation), facial expression, a well a eye gaze and eye blinking 

of a capture source actor to a target actor video. Figure 5 show 

some example of full-head reenactment between diferent source 

and target actors. Here, we use the full target video for training 

and the source video a the drive sequence. As can be seen, the 

output of our approach achieves a high level of realism and faithfully 

mimic the drive sequence, while still retain the mannerism 

of the original target actor. Note that the shadow in the background 

move consistently with the position of the actor in the scene, a 

show in Figure 5 (second row). We also demonstrate the high 

quality of our result and evaluate our approach quantitatively in a 

self-reenactment scenario, see Figure 7. For the quantitative analysis, 

we use two third of the target video for training and one third for 

testing. We capture the face in the training and drive video with 

our model-based tracker, and then render the conditioning images, 

which serve a input to our network for synthesize the output. For 

further details, please refer to Section 7.2. Note that the synthesize 

result be nearly indistinguishable from the ground truth. 

Facial Reenactment and Video Dubbing. Besides full-head reen- 

actment, our approach also enables facial reenactment. In this ex- 

periment, we replace the expression coeicients of the target actor 

with those of the source actor before synthesize the conditioning 

input to our rendering-to-video translation network. Here, the head 

pose and position, and eye gaze remain unchanged. Figure 8 show 

facial reenactment results. Observe that the face expression in the 

synthesize target video nicely match the expression of the source 

actor in the drive sequence. Please refer to the supplemental video 

for the complete video sequences. 

Our approach can also be apply to visual dubbing. In many 

countries, foreign-language movie be dubbed, i.e., the original 

voice of an actor be replace with that of a dub actor speak 

in another language. Dubbing often cause visual discomfort due 

to the discrepancy between the actor’s mouth motion and the new 

audio track. Even professional dub studio achieve only approx- 

imate audio alignment at best. Visual dub aim at alter the 

mouth motion of the target actor to match the new foreign-language 

audio track spoken by the dubber. Figure 9 show result where 

we modify the facial motion of actor speak originally in Ger- 

man to adhere to an English translation spoken by a professional 

dub actor, who be ilmed in a dub studio [Garrido et al. 

2015]. More precisely, we transfer the capture facial expression 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 



163:8 • H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt 

Fig. 7. uantitative evaluation of the photometric re-rendering error. We evaluate our approach quantitatively in a self-reenactment seting, where the 
ground-truth video portrait be known. We train our rendering-to-video translation network on two third of the video sequence, and test on the remain third. 
The error map show per-pixel Euclidean distance in RGB (color channel in [0, 255]); the mean photometric error of the test set be show in the top-right. The 
error be consistently low in region with conditioning input, with high error in region without conditioning, such a the upper body. Obama video courtesy 
of the White House (public domain). Putin video courtesy of the Kremlin (CC BY). May video courtesy of the UK government (Open Government Licence). 

Fig. 8. Facial reenactment result of our approach. We transfer the expression from the source to the target actor, while retain the head pose (rotation and 
translation) a well a the eye gaze of the target actor. For the full sequences, please refer to the supplemental video. Obama video courtesy of the White House 
(public domain). Putin video courtesy of the Kremlin (CC BY). Reagan video courtesy of the National Archives and Records Administration (public domain). 

Fig. 9. Dubbing comparison on two sequence of Garrido et al. [2015]. For 
visual dubbing, we transfer the facial expression of the dub actor 
(‘input’) to the target actor. We compare our result to Garrido et al.’s. Our 
approach obtains high quality result in term of the synthesize mouth 
shape and mouth interior. Note that our approach also enables full-head 
reenactment in addition to expression transfer. For the full comparison, we 
refer to the supplemental video. 

of the dub actor to the target actor, while leave the original 

target gaze and eye blink intact, i.e., we use the original eye gaze 

image of the tracked target sequence a conditioning. As can be 

seen, our approach achieves dub result of high quality. In fact, 

we produce image with more realistic mouth interior and more 

emotional content in the mouth region. Please see the supplemental 

video for full video results. 

Interactive Editing of Video Portraits. We built an interactive editor 

that enables user to reanimate video portrait with live feedback by 

modify the parameter of the coarse face model render into the 

conditioning image (see our live demo in the supplemental video). 

Figure 10 show a few static snapshot that be take while the 

user be play with our editor. Our approach enables change 

of all parameter dimensions, either independently or all together, 

a show in Figure 10. More speciically, we show independent 

change of the expression, head rotation, head translation, and eye 

gaze (including eye blinks). Please note the realistic and consistent 

generation of the torso, head and background. Even shadow or 

relections appear very consistently in the background. In addition, 

we show user edits that modify all parameter simultaneously. Our 

interactive editor run at approximately 9 fps. While not the focus of 

this paper, our approach also enables modiications of the geometric 

facial identity, see Figure 11. These combine modiications show a 

a proof of concept that our network generalizes beyond the training 

corpus. 

7.2 uantitative Evaluation 

We perform a quantitative evaluation of the re-rendering quality. 

First, we evaluate our approach in a self-reenactment setting, where 

the ground-truth video portrait be known. We train our rendering-to- 

video translation network on the irst two third of a video sequence 

and test it on the remain last third of the video, see Figure 7. The 

photometric error map show the per-pixel Euclidean distance in 

RGB color space, with each channel be in [0, 255]. We perform 

this test for three diferent video and the mean photometric er- 

rors be 2.88 (Vladimir Putin), 4.76 (Theresa May), and 4.46 (Barack 

Obama). Our approach obtains consistently low error in region 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 



Deep Video Portraits • 163:9 

Reference Expression Gaze 

Rotation Translation Combined 

Reference Expression Gaze 

Rotation Translation Combined 

Reference Expression Gaze 

Rotation Translation Combined 

Fig. 10. Interactive editing. Our approach provide full parametric control over video portrait (by control head model parameter in conditioning images). 
This enables modification of the rigid head pose (rotation and translation), facial expression and eye motion. All of these dimension can be manipulate 
together or independently. We also show these modification live in the supplemental video. Obama video courtesy of the White House (public domain). 

Reference Identity Change 

Fig. 11. Identity modification. While not the main focus of our approach, 
it also enables modification of the facial shape via the geometry shape 
parameters. This show that our network pick up the correspondence 
between the model and the video portrait. Note that the produce output 
be also consistent in region that be not constrain by the conditioning 
input, such a the hair and background. 

O 
u 
r 

A 
v 
e 
rb 

u 
ch 

-E 
lo 
r1 
7 

In 
p 
u 
t 

Fig. 12. Comparison to the image reenactment approach of Averbuch-Elor 
et al. [2017] in the full-head reenactment scenario. Since their method be 
base on a single target image, they copy the mouth interior from the 
source to the target, thus not preserve the target’s identity. Our learning- 
base approach enables large modification of the rigid head pose without 
apparent artifacts, while their warping-based approach distorts the head 
and background. In addition, ours enables joint control of the eye gaze 
and eye blinks. The diferences be most evident in the supplemental video. 
Obama video courtesy of the White House (public domain). 

Fig. 13. Comparison to Suwajanakorn et al. [2017]. Their approach produce 
accurate lip sync with visually imperceptible artifacts, but provide no direct 
control over facial expressions. Thus, the expression in the output do not 
always perfectly match the input (box, mouth), especially for expression 
change without audio cue. Our visual dub approach accurately trans- 
fers the expression from the source to the target. In addition, our approach 
provide more control over the target video by also transfer the eye gaze 
and eye blink (box, eyes), and the rigid head pose (arrows). Since the source 
sequence show more head-pose variation than the target sequence, we 
scale the transfer rotation and translation by 0.5 in this experiment. For 
the full video sequence, we refer to the supplemental video. Obama video 
courtesy of the White House (public domain). 

with conditioning input (face) and high error be found in region 

that be unexplained by the conditioning input. Please note that 

while the synthesize video portrait slightly difer from the ground 

truth outside the face region, the synthesize hair and upper body 

be still plausible, consistent with the face region, and free of visual 

artifacts. For a complete analysis of these sequences, we refer to the 

supplemental video. 

We evaluate our space-time conditioning strategy in Figure 16. 

Without space-time conditioning, the photometric error be signii- 

cantly higher. The average error over the complete sequence be 

4.9 without vs. 4.5 with temporal conditioning (Barack Obama) and 

5.3 without vs. 4.8 with temporal conditioning (Theresa May). In 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 



163:10 • H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt 

Fig. 14. Comparison to the state-of-the-art facial reenactment approach 
of Thies et al. [2016]. Our approach achieves expression transfer of similar 
quality, while also enable full-head reenactment, i.e., it also transfer the 
rigid head pose, gaze direction, and eye blinks. For the video result, we 
refer to the supplemental video. Obama video courtesy of the White House 
(public domain). 

addition to a low photometric error, space-time conditioning also 

lead to temporally signiicantly more stable video outputs. This 

can be see best in the supplemental video. 

We also evaluate the importance of the training set size. In this 

experiment, we train our rendering-to-video translation network 

with 500, 1000, 2000 and 4000 frame of the target sequence, see 

Figure 15. As can be expected, large training set produce good 

results, and the best result be obtain with the full training set. 

We also evaluate diferent image resolution by training our 

rendering-to-video translation network for resolution of 256×256, 

512×512 and 1024×1024 pixels. We evaluate the quality in the self- 

reenactment setting, a show in Figure 17. Generative network 

of high resolution be harder to train and require signiicantly 

longer training times: 10 hour for 256×256, 42 hour for 512×512, 

and 110 hour for 1024×1024 (on a Titan Xp). Therefore, we use a 

resolution of 256×256 pixel for most results. 

7.3 Comparisons to the State of the Art 

We compare our deep video portrait approach to current state-of- 

the-art video and image reenactment techniques. 

Comparison to Thies et al. [2016]. We compare our approach to 

the state-of-the-art Face2Face facial reenactment method of Thies 

et al. [2016]. In comparison to Face2Face, our approach achieves 

expression transfer of similar quality. What distinguishes our ap- 

proach be the capability for full-head reenactment, i.e., the ability to 

also transfer the rigid head pose, gaze direction, and eye blink in 

addition to the facial expressions, a show in Figure 14. As can be 

seen, in our result, the head pose and eye motion nicely match the 

source sequence, while the output generate by Face2Face follow 

the head and eye motion of the original target sequence. Please see 

the supplemental video for the video result. 

Comparison to Suwajanakorn et al. [2017]. We also compare to 

the audio-based dub approach of Suwajanakorn et al. [2017], 

see Figure 13. Their AudioToObama approach produce accurate lip 

sync with visually imperceptible artifacts, but provide no direct 

control over facial expressions. Thus, the expression in the output 

do not always perfectly match the input (box, mouth), especially 

for expression change without an audio cue. Our visual dub 

approach accurately transfer the expression from the source to 

the target. In addition, our approach provide more control over 

the target video by also transfer the eye gaze and eye blink 

(box, eyes) and the general rigid head pose (arrows). While their 

approach be train on a huge amount of training data (17 hours), 

our approach only us a small training dataset (1.3minutes). The 

diferences be best visible in the supplemental video. 

Comparison to Averbuch-Elor et al. [2017]. We compare our ap- 

proach in the full-head reenactment scenario to the image reenact- 

ment approach of Averbuch-Elor et al. [2017], see Figure 12. Their 

approach do not preserve the identity of the target actor, since 

they copy the teeth and mouth interior from the source to the target 

sequence. Our learning-based approach enables large modiications 

of the head pose without apparent artifacts, while their warping- 

base approach signiicantly distorts the head and background. In 

addition, we enable the joint modiication of the gaze direction and 

eye blinks; see supplemental video. 

7.4 User Study 

We conduct two extensive web-based user study to quantita- 

tively evaluate the realism of our results. We prepared short 5- 

second video clip that we extract from both real and synthesize 

video (see Figure 18), to evaluate three application of our approach: 

self-reenactment, same-person-reenactment and visual dubbing. We 

opt for self-reenactment, same-person-reenactment (two speech 

of Barack Obama) and visual dub to guarantee that the motion 

type in the evaluate real and synthesize video pair be match- 

ing. This eliminates the motion type a a confound factor from 

the statistical analysis, e.g., have unrealistic motion for a public 

speech in the synthesize video would negatively bias the out- 

come of the study. Our evaluation be focus on the visual quality 

of the synthesize results. Most video clip have a resolution of 

256×256 pixels, but some be 512×512 pixels. In our user study, we 

present one video clip at a time, and ask participant to re- 

spond to the statement łThis video clip look real to mež on a 5-point 

Likert scale (1śstrongly disagree, 2śdisagree, 3śdon’t know, 4śagree, 

5śstrongly agree). Video clip be show in a random order, and 

each video clip be show exactly once to ass participants’ irst 

impression. We recruit 135 and 69 anonymous participant for 

our two studies, largely from North America and Europe. 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 



Deep Video Portraits • 163:11 

Fig. 15. uantitative evaluation of the training set size. We train our rendering-to-video translation network with training corpus of diferent sizes. The error 
map show the per-pixel distance in RGB color space with each channel be in [0, 255]; the mean photometric error be show in the top-right. Smaller 
training set have large photometric errors, especially for region outside of the face. For the full comparison, we refer to the supplemental video. Obama 
video courtesy of the White House (public domain). May video courtesy of the UK government (Open Government Licence). 

Fig. 16. uantitative evaluation of the influence of the propose space-time 
conditioning input. The error map show the per-pixel distance in RGB color 
space with each channel be in [0, 255]; the mean photometric error be 
show in the top-right. Without space-time conditioning, the photometric 
error be higher. Temporal conditioning add significant temporal stability. 
This be best see in the supplemental video. Obama video courtesy of the 
White House (public domain).May video courtesy of the UK government 
(Open Government Licence). 

Fig. 17. uantitative comparison of diferent resolutions. We train three 
rendering-to-video translation network for resolution of 256×256, 512×512 
and 1024×1024 pixels. The error map show the per-pixel distance in RGB 
color space with each channel be in [0, 255]; the mean photometric error 
be show in the top-right. For the full comparison, see our video. May video 
courtesy of the UK government (Open Government Licence). 

Fig. 18. We perform a user study to evaluate the quality of our result and 
see if user can distinguish between real (top) and synthesize video clip 
(botom). The video clip include self-reenactment, same-person-reenact- 
ment, and video dubbing. Putin video courtesy of the Kremlin (CC BY). 
Obama video courtesy of the White House (public domain). Elizabeth II 
video courtesy of the Governor General of Canada (public domain). 

The result in Table 1 show that only 80% of participant rat real 

256×256 video a real, i.e. (strongly) agree to the video look 

real; it seem that in anticipation of synthetic video clips, partici- 

pant become overly critical. At the same time, 50% of participant 

consider our 256×256 result to be real, which increase slightly to 

52% for 512×512. Our best result be the self-reenactment of Vladimir 

Putin at 256×256 resolution, which 65% of participant consider 

to be real, compare to 78% for the real video. We also evaluate 

partial and full reenactment by transfer a speech by Barack 

Obama to another video clip of himself. Table 2 indicates that we 

achieve good realism rating with full reenactment comprise fa- 

cial expression and pose (50%) compare to transfer only facial 

expression (38%). This might be because full-head reenactment 

keep expression and head motion synchronized. Suwajanakorn 

et al.’s speech-driven reenactment approach [2017] achieves a re- 

alism rating of 64% compare to the real source and target video 

clips, which achieve 70ś86%. Our full-head reenactment result be 

consider to be at least a real a Suwajanakorn et al.’s by 60% 

of participants. We inally compare our dub result to VDub 

[Garrido et al. 2015] in Table 3. Overall, 57% of participant give 

our result a high realism rating (and 32% give the same rating). 

Our result be again consider to be real by 51% of participants, 

compare to only 21% for VDub. 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 



163:12 • H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt 

Table 1. User study result for self-reenacted video (n=135). Columns 1ś5 
show the percentage of rating give about the statement łThis video clip 
look real to mež, from 1 (strongly disagree) to 5 (strongly agree). 4+5=‘real’. 

Real video Our result 

re 1 2 3 4 5 ‘real’ 1 2 3 4 5 ‘real’ 

Obama 256 2 8 10 62 19 81% 13 33 11 37 6 43% 

Putin 256 2 11 10 58 20 78% 3 17 15 54 11 65% 

Eliabeth II 256 2 6 12 59 21 80% 6 32 20 33 9 42% 

Obama 512 0 7 3 49 42 91% 9 35 13 36 8 44% 

Putin 512 4 13 10 47 25 72% 2 20 15 44 19 63% 

Eliabeth II 512 1 7 4 55 34 89% 7 33 10 38 13 51% 

Mean 256 2 8 10 60 20 80% 7 27 15 41 9 50% 

Mean 512 2 9 6 50 34 84% 6 29 12 39 13 52% 

Table 2. User study result for expression and full head transfer between two 
video of Barack Obama compare to the input video and Suwajanakorn 
et al.’s approach (n=69, mean of 4 clips). 

Ratings 

1 2 3 4 5 ‘real’ 

Source video (real) 0 8 6 43 42 86% 

Target video (real) 1 14 14 47 23 70% 

Suwajanakorn et al. [2017] 2 20 14 47 17 64% 

Expression transfer (ours) 9 37 17 29 9 38% 

Full head transfer (ours) 3 31 16 37 13 50% 

Table 3. User study result for dub comparison to VDub (n=135). 

Garrido et al. [2015] Our result 

1 2 3 4 5 ‘real’ 1 2 3 4 5 ‘real’ 

Ingmar (3 clips) 21 36 21 20 2 22% 4 21 25 42 8 50% 

Thomas (3 clips) 33 36 11 16 4 20% 7 25 17 42 9 51% 

Mean (6 clips) 27 36 16 18 3 21% 6 23 21 42 9 51% 

On average, across all scenario and both studies, our result be 

consider to be real by 47% of the participant (1,767 ratings), com- 

par to only 80% for real video clip (1,362 ratings). This suggests 

that our result already fool about 60% of the participant ś a good 

result give the critical participant pool. However, there be some 

variation across our results: low realism rating be give for 

well-known personality like Barack Obama, while high rating 

be give for instance to the unknown dub actors. 

8 DISCUSSION 

While we have demonstrate highly realistic reenactment result 

in a large variety of application and scenarios, our approach be 

also subject to a few limitations. Similar to all other learning-based 

approaches, ours work very well inside the span of the training 

corpus. Extreme target head poses, such a large rotations, or ex- 

pressions far outside this span can lead to a degradation of the 

visual quality of the generate video portrait, see Figure 19 and the 

supplemental video. Since we only track the face with a parametric 

model, we cannot actively control the motion of the torso or hair, or 

control the background. The network learns to extrapolate and inds 

a plausible and consistent upper body and background (including 

some shadow and relections) for a give head pose. This limitation 

Fig. 19. Our approach work well within the span of the training corpus. 
Extreme change in head pose far outside the training set or strong change 
to the facial expression might lead to artifact in the synthesize video. This 
be a common limitation of all learning-based approaches. In these cases, 
artifact be most prominent outside the face region, a these region have 
no conditioning input. May video courtesy of the UK government (Open 
Government Licence). Malou video courtesy of Louisa Malou (CC BY). 

could be overcome by also track the body and use the underly- 

ing body model to generate an extend set of conditioning images. 

Currently, we be only able to produce medium-resolution output 

due to memory and training time limitations. The limited output 

resolution make it especially diicult to reproduce ine-scale de- 

tail, such a individual teeth, in a temporally coherent manner. Yet, 

recent progress on high-resolution discriminative adversarial net- 

work [Karras et al. 2018; Wang et al. 2017] be promising and could 

be leveraged to further increase the resolution of the generate out- 

put. On a broader scale, and not be a limitation, democratization 

of advanced high-quality video edit possibilities, ofered by our 

and other methods, call for additional care in ensure veriiable 

video authenticity, e.g., through invisible watermarking. 

9 CONCLUSION 

We present a new approach to synthesize entire photo-real video 

portrait of a target actor in front of general static backgrounds. 

It be the irst to transfer head pose and orientation, face expression, 

and eye gaze from a source actor to a target actor. The propose 

method be base on a novel rendering-to-video translation network 

that convert a sequence of simple computer graphic rendering 

into photo-realistic and temporally-coherent video. This mapping be 

learn base on a novel space-time conditioning volume formula- 

tion. We have show through experiment and a user study that our 

method outperforms prior work in quality and expands over their 

possibilities. It thus open up a new level of capability in many 

applications, like video reenactment for virtual reality and telep- 

resence, interactive video editing, and visual dubbing. We see our 

approach a a step towards highly realistic synthesis of full-frame 

video content under control of meaningful parameters. We hope 

that it will inspire future research in this very challenge ield. 

ACKNOWLEDGMENTS 

We be grateful to all our actors. We thank True-VisionSolutions 

Pty Ltd for kindly provide the 2D face tracker and Adobe for a 

Premiere Pro CC license. We also thank Supasorn Suwajanakorn 

and Hadar Averbuch-Elor for the comparisons. This work be sup- 

port by ERC Starting Grant CapReal (335545), a TUM-IAS Rudolf 

Mößbauer Fellowship, a Google Faculty Award, RCUK grant CAM- 

ERA (EP/M023281/1), an NVIDIA Corporation GPU Grant, and the 

Max Planck Center for Visual Computing and Communications 

(MPC-VCC). 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 



Deep Video Portraits • 163:13 

A APPENDIX 

This appendix describes all the use datasets, see Table 4 (target 

actors) and Table 5 (source actors). 

Table 4. Target videos: Name and length of sequence (in frames).Malou 
video courtesy of Louisa Malou (CC BY). May video courtesy of the UK 
government (Open Government Licence). Obama video courtesy of the 
White House (public domain). Putin video courtesy of the Kremlin (CC BY). 
Reagan video courtesy of the National Archives and Records Administration 
(public domain). Elizabeth II video courtesy of the Governor General of 
Canada (public domain). Reagan video courtesy of the National Archives 
and Records Administration (public domain).Wolf video courtesy of Tom 
Wolf (CC BY). 

Ingmar Malou May Obama1 Obama2 

3,000 15,000 5,000 2,000 3,613 

Putin Elizabeth II Reagan Thomas Wolf 

4,000 1,500 6,984 2,239 15,000 

DB1 DB2 DB3 DB4 

8,000 18,138 6,500 30,024 

Table 5. Source videos: Name and length of sequence (in frames). Obama 
video courtesy of the White House (public domain). 

Obama3 David1 David2 DB5 DB6 

1,945 4,611 3,323 3,824 2,380 

REFERENCES 
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, 

Greg S. Corrado, Andy Davis, Jefrey Dean, Matthieu Devin, Sanjay Ghemawat, 
Ian Goodfellow, Andrew Harp, Geofrey Irving, Michael Isard, Yangqing Jia, Rafal 
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat 
Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, 
Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay 
Vasudevan, Fernanda Viégas, Oriol Vinyals, PeteWarden,MartinWattenberg,Martin 
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine 
Learning on Heterogeneous Systems. https://www.tensorlow.org/ Software 
available from tensorlow.org. 

Oleg Alexander, Mike Rogers, William Lambeth, Jen-Yuan Chiang, Wan-Chun Ma, 
Chuan-Chang Wang, and Paul Debevec. 2010. The Digital Emily Project: Achieving 
a Photorealistic Digital Actor. IEEE Computer Graphics and Applications 30, 4 
(July/August 2010), 20ś31. https://doi.org/10.1109/MCG.2010.65 

Hadar Averbuch-Elor, Daniel Cohen-Or, Johannes Kopf, and Michael F. Cohen. 2017. 
Bringing Portraits to Life. ACM Transactions on Graphics (SIGGRAPH Asia) 36, 6 
(November 2017), 196:1ś13. https://doi.org/10.1145/3130800.3130818 

Volker Blanz, Kristina Scherbaum, Thomas Vetter, and Hans-Peter Seidel. 2004. Ex- 
change Faces in Images. Computer Graphics Forum (Eurographics) 23, 3 (September 
2004), 669ś676. https://doi.org/10.1111/j.1467-8659.2004.00799.x 

Volker Blanz and Thomas Vetter. 1999. AMorphable Model for the Synthesis of 3D Faces. 
In Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH). 
187ś194. https://doi.org/10.1145/311535.311556 

James Booth, Anastasios Roussos, Allan Ponniah, David Dunaway, and Stefanos 
Zafeiriou. 2018. Large Scale 3DMorphable Models. International Journal of Computer 
Vision 126, 2 (April 2018), 233ś254. https://doi.org/10.1007/s11263-017-1009-7 

Christoph Bregler, Michele Covell, and Malcolm Slaney. 1997. Video Rewrite: Driving 
Visual SpeechwithAudio. InAnnual Conference on Computer Graphics and Interactive 
Techniques (SIGGRAPH). 353ś360. https://doi.org/10.1145/258734.258880 

Chen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler. 2015. Real-time High-idelity 
Facial Performance Capture. ACM Transactions on Graphics (SIGGRAPH) 34, 4 (July 
2015), 46:1ś9. https://doi.org/10.1145/2766943 

Chen Cao, Qiming Hou, and Kun Zhou. 2014a. Displaced Dynamic Expression Regres- 
sion for Real-time Facial Tracking and Animation. ACM Transactions on Graphics 
(SIGGRAPH) 33, 4 (July 2014), 43:1ś10. https://doi.org/10.1145/2601097.2601204 

Chen Cao, YanlinWeng, Shun Zhou, Yiying Tong, and Kun Zhou. 2014b. FaceWarehouse: 
A 3D Facial Expression Database for Visual Computing. IEEE Transactions on 
Visualization and Computer Graphics 20, 3 (March 2014), 413ś425. https://doi.org/ 
10.1109/TVCG.2013.249 

Chen Cao, Hongzhi Wu, Yanlin Weng, Tianjia Shao, and Kun Zhou. 2016. Real-time 
Facial Animation with Image-based Dynamic Avatars. ACMTransactions on Graphics 
(SIGGRAPH) 35, 4 (July 2016), 126:1ś12. https://doi.org/10.1145/2897824.2925873 

Yao-Jen Chang and Tony Ezzat. 2005. Transferable Videorealistic Speech Animation. 
In Symposium on Computer Animation (SCA). 143ś151. https://doi.org/10.1145/ 
1073368.1073388 

Qifeng Chen and Vladlen Koltun. 2017. Photographic Image Synthesis with Cascaded 
Reinement Networks. In International Conference on Computer Vision (ICCV). 1520ś 
1529. https://doi.org/10.1109/ICCV.2017.168 

Kevin Dale, Kalyan Sunkavalli, Micah K. Johnson, Daniel Vlasic, Wojciech Matusik, 
and Hanspeter Pister. 2011. Video face replacement. ACM Transactions on Graphics 
(SIGGRAPH Asia) 30, 6 (December 2011), 130:1ś10. https://doi.org/10.1145/2070781. 
2024164 

Tony Ezzat, Gadi Geiger, and Tomaso Poggio. 2002. Trainable Videorealistic Speech 
Animation. ACM Transactions on Graphics (SIGGRAPH) 21, 3 (July 2002), 388ś398. 
https://doi.org/10.1145/566654.566594 

Ohad Fried, Eli Shechtman, Dan B. Goldman, and Adam Finkelstein. 2016. Perspective- 
aware Manipulation of Portrait Photos. ACM Transactions on Graphics (SIGGRAPH) 
35, 4 (July 2016), 128:1ś10. https://doi.org/10.1145/2897824.2925933 

Graham Fyfe, Andrew Jones, Oleg Alexander, Ryosuke Ichikari, and Paul Debevec. 
2014. Driving High-Resolution Facial Scans with Video Performance Capture. ACM 
Transactions on Graphics 34, 1 (December 2014), 8:1ś14. https://doi.org/10.1145/ 
2638549 

Yaroslav Ganin, Daniil Kononenko, Diana Sungatullina, and Victor Lempitsky. 2016. 
DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation. In Euro- 
pean Conference on Computer Vision (ECCV). 311ś326. https://doi.org/10.1007/ 
978-3-319-46475-6_20 

Pablo Garrido, Levi Valgaerts, Ole Rehmsen, Thorsten Thormaehlen, Patrick Pérez, and 
Christian Theobalt. 2014. Automatic Face Reenactment. In Conference on Computer 
Vision and Pattern Recognition (CVPR). 4217ś4224. https://doi.org/10.1109/CVPR. 
2014.537 

Pablo Garrido, Levi Valgaerts, Hamid Sarmadi, Ingmar Steiner, Kiran Varanasi, Patrick 
Pérez, and Christian Theobalt. 2015. VDub: Modifying Face Video of Actors for 
Plausible Visual Alignment to a Dubbed Audio Track. Computer Graphics Forum 
(Eurographics) 34, 2 (May 2015), 193ś204. https://doi.org/10.1111/cgf.12552 

Pablo Garrido, Michael Zollhöfer, Dan Casas, Levi Valgaerts, Kiran Varanasi, Patrick 
Pérez, and Christian Theobalt. 2016. Reconstruction of Personalized 3D Face Rigs 
from Monocular Video. ACM Transactions on Graphics 35, 3 (June 2016), 28:1ś15. 
https://doi.org/10.1145/2890493 

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, 
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial 
Nets. In Advances in Neural Information Processing Systems. 

Geofrey E. Hinton and Ruslan Salakhutdinov. 2006. Reducing the Dimensionality 
of Data with Neural Networks. Science 313, 5786 (July 2006), 504ś507. https: 
//doi.org/10.1126/science.1127647 

Liwen Hu, Shunsuke Saito, Lingyu Wei, Koki Nagano, Jaewoo Seo, Jens Fursund, Iman 
Sadeghi, Carrie Sun, Yen-Chun Chen, and Hao Li. 2017. Avatar Digitization from a 
Single Image for Real-time Rendering. ACM Transactions on Graphics (SIGGRAPH 
Asia) 36, 6 (November 2017), 195:1ś14. https://doi.org/10.1145/3130800.31310887 

Alexandru Eugen Ichim, Soien Bouaziz, and Mark Pauly. 2015. Dynamic 3D Avatar 
Creation from Hand-held Video Input. ACM Transactions on Graphics (SIGGRAPH) 
34, 4 (July 2015), 45:1ś14. https://doi.org/10.1145/2766974 

Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2017. Image-to-Image 
Translation with Conditional Adversarial Networks. In Conference on Computer 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 



163:14 • H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt 

Vision and Pattern Recognition (CVPR). 5967ś5976. https://doi.org/10.1109/CVPR. 
2017.632 

Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2018. Progressive Growing 
of GANs for Improved Quality, Stability, and Variation. In International Conference 
on Learning Representations (ICLR). 

Ira Kemelmacher-Shlizerman. 2013. Internet-Based Morphable Model. In International 
Conference on Computer Vision (ICCV). 3256ś3263. https://doi.org/10.1109/ICCV. 
2013.404 

Ira Kemelmacher-Shlizerman, Aditya Sankar, Eli Shechtman, and Steven M. Seitz. 2010. 
Being John Malkovich. In European Conference on Computer Vision (ECCV). 341ś353. 
https://doi.org/10.1007/978-3-642-15549-9_25 

Ira Kemelmacher-Shlizerman, Eli Shechtman, Rahul Garg, and Steven M. Seitz. 2011. 
Exploring photobios. ACM Transactions on Graphics (SIGGRAPH) 30, 4 (August 
2011), 61:1ś10. https://doi.org/10.1145/2010324.1964956 

Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. 
In International Conference on Learning Representations (ICLR). 

Christoph Lassner, Gerard Pons-Moll, and Peter V. Gehler. 2017. A Generative Model of 
People in Clothing. In International Conference on Computer Vision (ICCV). 853ś862. 
https://doi.org/10.1109/ICCV.2017.98 

Hao Li, Laura Trutoiu, Kyle Olszewski, Lingyu Wei, Tristan Trutna, Pei-Lun Hsieh, 
Aaron Nicholls, and Chongyang Ma. 2015. Facial Performance Sensing Head- 
mount Display. ACM Transactions on Graphics (SIGGRAPH) 34, 4 (July 2015), 
47:1ś9. https://doi.org/10.1145/2766939 

Kai Li, Qionghai Dai, Ruiping Wang, Yebin Liu, Feng Xu, and Jue Wang. 2014. A Data- 
Driven Approach for Facial Expression Retargeting in Video. IEEE Transactions 
on Multimedia 16, 2 (February 2014), 299ś310. https://doi.org/10.1109/TMM.2013. 
2293064 

Kang Liu and Joern Ostermann. 2011. Realistic facial expression synthesis for an image- 
base talk head. In International Conference on Multimedia and Expo (ICME). 
https://doi.org/10.1109/ICME.2011.6011835 

Ming-Yu Liu, Thomas Breuel, and Jan Kautz. 2017. Unsupervised Image-to-Image 
Translation Networks. In Advances in Neural Information Processing Systems. 

Zicheng Liu, Ying Shan, and Zhengyou Zhang. 2001. Expressive Expression Mapping 
with Ratio Images. In Annual Conference on Computer Graphics and Interactive 
Techniques (SIGGRAPH). 271ś276. https://doi.org/10.1145/383259.383289 

Liqian Ma, Qianru Sun, Xu Jia, Bernt Schiele, Tinne Tuytelaars, and Luc Van Gool. 2017. 
Pose Guided Person Image Generation. In Advances in Neural Information Processing 
Systems. 

Mehdi Mirza and Simon Osindero. 2014. Conditional Generative Adversarial Nets. 
(2014). https://arxiv.org/abs/1411.1784 arXiv:1411.1784. 

Kyle Olszewski, Zimo Li, Chao Yang, Yi Zhou, Ronald Yu, Zeng Huang, Sitao Xiang, 
Shunsuke Saito, Pushmeet Kohli, and Hao Li. 2017. Realistic Dynamic Facial Textures 
from a Single Image use GANs. In International Conference on Computer Vision 
(ICCV). 5439ś5448. https://doi.org/10.1109/ICCV.2017.580 

Kyle Olszewski, Joseph J. Lim, Shunsuke Saito, and Hao Li. 2016. High-idelity Facial 
and Speech Animation for VR HMDs. ACM Transactions on Graphics (SIGGRAPH 
Asia) 35, 6 (November 2016), 221:1ś14. https://doi.org/10.1145/2980179.2980252 

Alec Radford, Luke Metz, and Soumith Chintala. 2016. Unsupervised Representation 
Learning with Deep Convolutional Generative Adversarial Networks. In Interna- 
tional Conference on Learning Representations (ICLR). 

Ravi Ramamoorthi and Pat Hanrahan. 2001. An eicient representation for irradiance 
environment maps. In Annual Conference on Computer Graphics and Interactive 
Techniques (SIGGRAPH). 497ś500. https://doi.org/10.1145/383259.383317 

Elad Richardson, Matan Sela, and Ron Kimmel. 2016. 3D Face Reconstruction by 
Learning from Synthetic Data. In International Conference on 3D Vision (3DV). 460ś 
469. https://doi.org/10.1109/3DV.2016.56 

Elad Richardson, Matan Sela, Roy Or-El, and Ron Kimmel. 2017. Learning Detailed 
Face Reconstruction from a Single Image. In Conference on Computer Vision and 
Pattern Recognition (CVPR). 5553ś5562. https://doi.org/10.1109/CVPR.2017.589 

Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-Net: Convolutional 
Networks for Biomedical Image Segmentation. In International Conference onMedical 
Image Computing and Computer-Assisted Intervention (MICCAI). 234ś241. https: 
//doi.org/10.1007/978-3-319-24574-4_28 

Joseph Roth, Yiying Tong Tong, and Xiaoming Liu. 2017. Adaptive 3D Face Recon- 
struction from Unconstrained Photo Collections. IEEE Transactions on Pattern 
Analysis and Machine Intelligence 39, 11 (November 2017), 2127ś2141. https: 
//doi.org/10.1109/TPAMI.2016.2636829 

Jason M. Saragih, Simon Lucey, and Jefrey F. Cohn. 2011. Real-time avatar animation 
from a single image. In International Conference on Automatic Face and Gesture 
Recognition (FG). 117ś124. https://doi.org/10.1109/FG.2011.5771383 

Matan Sela, Elad Richardson, and Ron Kimmel. 2017. Unrestricted Facial Geometry 
Reconstruction Using Image-to-Image Translation. In International Conference on 
Computer Vision (ICCV). 1585ś1594. https://doi.org/10.1109/ICCV.2017.175 

Fuhao Shi, Hsiang-Tao Wu, Xin Tong, and Jinxiang Chai. 2014. Automatic Acquisition 
of High-idelity Facial Performances Using Monocular Videos. ACM Transactions 
on Graphics (SIGGRAPH Asia) 33, 6 (November 2014), 222:1ś13. https://doi.org/10. 

1145/2661229.2661290 
Robert W. Sumner and Jovan Popović. 2004. Deformation Transfer for Triangle Meshes. 

ACM Transactions on Graphics (SIGGRAPH) 23, 3 (August 2004), 399ś405. https: 
//doi.org/10.1145/1015706.1015736 

Supasorn Suwajanakorn, Ira Kemelmacher-Shlizerman, and Steven M. Seitz. 2014. Total 
Moving Face Reconstruction. In European Conference on Computer Vision (ECCV) 
(Lecture Notes in Computer Science), Vol. 8692. 796ś812. https://doi.org/10.1007/ 
978-3-319-10593-2_52 

Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman. 2015. What 
Makes Tom Hanks Look Like Tom Hanks. In International Conference on Computer 
Vision (ICCV). 3952ś3960. https://doi.org/10.1109/ICCV.2015.450 

Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman. 2017. 
Synthesizing Obama: Learning Lip Sync from Audio. ACM Transactions on Graphics 
(SIGGRAPH) 36, 4 (July 2017), 95:1ś13. https://doi.org/10.1145/3072959.3073640 

Yaniv Taigman, Adam Polyak, and Lior Wolf. 2017. Unsupervised Cross-Domain Image 
Generation. In International Conference on Learning Representations (ICLR). 

Ayush Tewari, Michael Zollhöfer, Hyeongwoo Kim, Pablo Garrido, Florian Bernard, 
Patrick Pérez, and Christian Theobalt. 2017. MoFA:Model-based Deep Convolutional 
Face Autoencoder for Unsupervised Monocular Reconstruction. In International 
Conference on Computer Vision (ICCV). 3735ś3744. https://doi.org/10.1109/ICCV. 
2017.401 

Justus Thies, Michael Zollhöfer, Matthias Nießner, Levi Valgaerts, Marc Stamminger, 
and Christian Theobalt. 2015. Real-time Expression Transfer for Facial Reenactment. 
ACM Transactions on Graphics (SIGGRAPH Asia) 34, 6 (November 2015), 183:1ś14. 
https://doi.org/10.1145/2816795.2818056 

Justus Thies, Michael Zollhöfer, Marc Stamminger, Christian Theobalt, and Matthias 
Nießner. 2016. Face2Face: Real-Time Face Capture and Reenactment of RGB Videos. 
In Conference on Computer Vision and Pattern Recognition (CVPR). 2387ś2395. https: 
//doi.org/10.1109/CVPR.2016.262 

Justus Thies, Michael Zollhöfer, Marc Stamminger, Christian Theobalt, and Matthias 
Nießner. 2018. FaceVR: Real-Time Facial Reenactment and Eye Gaze Control in 
Virtual Reality. ACM Transactions on Graphics (2018). 

Anh Tuan Tran, Tal Hassner, Iacopo Masi, and Gerard Medioni. 2017. Regressing Robust 
and Discriminative 3D Morphable Models with a very Deep Neural Network. In 
Conference on Computer Vision and Pattern Recognition (CVPR). 1493ś1502. https: 
//doi.org/10.1109/CVPR.2017.163 

Daniel Vlasic, Matthew Brand, Hanspeter Pister, and Jovan Popović. 2005. Face Transfer 
with Multilinear Models. ACM Transactions on Graphics (SIGGRAPH) 24, 3 (July 
2005), 426ś433. https://doi.org/10.1145/1073204.1073209 

Chao Wang, Haiyong Zheng, Zhibin Yu, Ziqiang Zheng, Zhaorui Gu, and Bing Zheng. 
2017. Discriminative Region Proposal Adversarial Networks for High-Quality Image- 
to-Image Translation. (2017). https://arxiv.org/abs/1711.09554 arXiv:1711.09554. 

Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan 
Catanzaro. 2018. High-Resolution Image Synthesis and Semantic Manipulation 
with Conditional GANs. In Conference on Computer Vision and Pattern Recognition 
(CVPR). 

Thibaut Weise, Soien Bouaziz, Hao Li, and Mark Pauly. 2011. Realtime Performance- 
base Facial Animation. ACM Transactions on Graphics (SIGGRAPH) 30, 4 (July 
2011), 77:1ś10. https://doi.org/10.1145/2010324.1964972 

Erroll Wood, Tadas Baltrušaitis, Louis-Philippe Morency, Peter Robinson, and Andreas 
Bulling. 2018. GazeDirector: Fully articulate eye gaze redirection in video. Computer 
Graphics Forum (Eurographics) 37, 2 (2018). https://doi.org/10.1111/cgf.13355 

Chenglei Wu, Derek Bradley, Markus Gross, and Thabo Beeler. 2016. An Anatomically- 
Constrained Local Deformation Model for Monocular Face Capture. ACM Transac- 
tions on Graphics (SIGGRAPH) 35, 4 (July 2016), 115:1ś12. https://doi.org/10.1145/ 
2897824.2925882 

Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. 2017. DualGAN: Unsupervised Dual 
Learning for Image-to-Image Translation. In International Conference on Computer 
Vision (ICCV). 2868ś2876. https://doi.org/10.1109/ICCV.2017.310 

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. 2017. Unpaired Image-to- 
Image Translation use Cycle-Consistent Adversarial Networks. In International 
Conference on Computer Vision (ICCV). 2242ś2251. https://doi.org/10.1109/ICCV. 
2017.244 

Michael Zollhöfer, Justus Thies, Pablo Garrido, Derek Bradley, Thabo Beeler, Patrick 
Pérez, Marc Stamminger, Matthias Nießner, and Christian Theobalt. 2018. State of 
the Art onMonocular 3D Face Reconstruction, Tracking, and Applications. Computer 
Graphics Forum 37, 2 (2018). https://doi.org/10.1111/cgf.13382 

Received January 2018; revise April 2018; inal version May 2018; accepted 

May 2018 

ACM Trans. Graph., Vol. 37, No. 4, Article 163. Publication date: August 2018. 


