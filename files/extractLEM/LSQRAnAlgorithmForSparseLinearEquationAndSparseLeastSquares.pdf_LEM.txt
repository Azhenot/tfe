

































LSQR: An Algorithm for Sparse Linear 
Equations and Sparse Least Squares 

CHRISTOPHER C. PAIGE 
McGill University, Canada 
and 
MICHAEL A. SAUNDERS 
Stanford University 

An iterative method be give for solve Ax ~ffi b and minU Ax - b 112, where the matrix A be large and 
sparse. The method be base on the bidiagonalization procedure of Golub and Kahan. It be analytically 
equivalent to the standard method of conjugate gradients, but posse more favorable numerical 
properties. 

Reliable stop criterion be derived, along with estimate of standard error for x and the 
condition number of A. These be use in the FORTRAN implementation of the method, subroutine 
LSQR. Numerical test be described compare I~QR with several other conjugate-gradient algori- 
thms, indicate that I~QR be the most reliable algorithm when A be ill-conditioned. 

Categories and Subject Descriptors: G.1.2 [Numerical Analysis]: ApprorJmation--least square 
approximation; G.1.3 [Numerical Analysis]: Numerical Linear Algebra--linear system (direct and 
tterative methods); sparse and very large system 

General Terms: Algorithms 

Additional Key Words and Phrases: analysis of variance 

The Algorithm: LSQR: Sparse Linear Equations and Least Square Problems. ACM Trans. Math. 
Softw. 8, 2 (June 1982). 

1. INTRODUCTION 

A n u m e r i c a l m e t h o d be p r e s e n t e d he re for c o m p u t i n g a so lu t i on x to e i the r of the 

fol low prob lems: 

U n s y m m e t r i c equa t ion : solve A x ffi b 

L i n e a r lea t square : m i n i m i z e ][ A x - b 112 

This work be support in part by the Natural Sciences and Engineering Research Council of 
Canada Grant A8652, the New Zealand Department of Scientific and Industrial Research, the U.S. 
Office of Naval Research under Contract N00014-75-C-0267, National Science Foundation Grants 
MCS 76-20019 and ENG 77-06761, and the Department of Energy under Contract DE.AC03- 
76SF00326, PA No. DE-AT03-76ER72018. 
Authors' addresses: C.C. Paige, School of Computer Science, McGill University, Montreal, P.Q., 
Canada H3C 3G1; M.A. Saunders, Department of Operations Research, Stanford University, Stanford, 
CA 94305. 
Permission to copy without fee all or part of this material be grant provide that the copy be not 
make or distribute for direct commercial advantage, the ACM copyright notice and the title of the 
publication and it date appear, and notice be give that copying be by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, require a fee and/or specific 
permission. 
© 1982 ACM 0098-3500/82/0300-0043 $00.75 

ACM Transactions on Mathematwal Software, Vol 8, No. 1, March 1982, Pages 43-71. 



44 • C .C. Paige and M. A. Saunders 

where A be a real matrix with m row and n column and b be a real vector. It will 
usually be true that m _> n and rank(A) = n, but these condition be not 
essential. The method, to be call algorithm LSQR, be similar in style to the 
well-known method of conjugate gradient (CG) a apply to the least-squares 
problem [10]. The matrix A be use only to compute product of the form A v and 
ATu for various vector v and u. Hence A will normally be large and sparse or will 
be expressible a a product of matrix that be sparse or have special structure. 
A typical application be to the large least-squares problem arise from analysis 
of variance (6.g., [12]). 

CG-like method be iterative in nature. They be characterize by t h e i r need 
for only a few vector of work storage and by their theoretical convergence 
within at most n iteration (if exact arithmetic could be performed). In practice 
such method may require far few or far more than n iteration to reach an 
acceptable approximation to x. The method be most useful when A be well 
condition and have many nearly equal singular values. These property occur 
naturally in many applications. In other case it be often possible to divide the 
solution procedure into a direct and an iterative part, such that the iterative part 
have a good condition matrix for which CG-like method will converge more 
quickly. Some such transformation method be consider in [21]. 

Algorithm LSQR be base on the bidiagonalization procedure of Golub and 
Kahan [9]. It generates a sequence of approximation {xk } such that the residual 
norm II rk [[2 decrease monotonically, where rk = b - A x k . Analytically, the 
sequence (xh} be identical to the sequence generate by the standard CG algo- 
rithm and by several other publish algorithms. However, LSQR be show by 
example to be numerically more reliable in various circumstance than the other 
method considered. 

The FORTRAN implementation of LSQR [22] be design for practical appli- 
cation. It incorporates reliable stop criterion and provide the user with 
compute estimate of the follow quantities: x , r = b - A x , A Tr, II r 112, It A II F, 
standard error for x , and the condition number of A. 

1.1 Notation 

Matrices be denote by A, B . . . . . vector by v, w , . . . , and scalar by ~, f l , . . . . 
Two exception be c and s, which denote the significant component of an 
elementary orthogonal matrix, such that c 2 + s 2 = 1. For a vector v, H v [[ always 
denotes the Euclidean norm H v [[2 = (vTv) 1/2. For a matrix A, [[A [[ will usually 
mean the Frobenius norm, HA HF = ( ~ a 2 ) 1/2, and the condition number for an 
unsymmetric matrix A be define by cond(A) = ]] A ]] II A + ]], where A + denotes the 
pseudoinverse of A. The relative precision of floating-point arithmetic be e, the 
small machine-representable number such that 1 + e > 1. 

2. MOTIVATION VIA THE LANCZOS PROCESS 

In this section we review the symmetric Lanczos process [13] and it use in 
solve symmetric linear equation B x = b. Algorithm LSQR be then derive by 
apply the Lanczos process to a particular symmetric system. Although a more 
direct development be give in Section 4, the present derivation may remain 
useful for a future error analysis of LSQR, since many of the round error 
property of the Lanczos process be already know [18]. 

ACM Transactions on Mathematmal Software, Vol 8, No 1, March 1982. 



LSQR: An Algorithm for Sparse Linear Equattons and Sparse Least Squares • 45 

Given a symmetric matrix B and a start vector b, the Lanczos process be a 
method for generate a sequence of vector { v,) and scalar { a, ), (fli} such that 
B be reduce to tridiagonal form. A reliable computational form of the method be 
the following. 

T h e L a n c z o s p roces s (reduction to tridiagonal form): 

fll vl -- b, 

w, = Bvt - fl, v,-~ ] 

a, vTw' I ' i = 1, 2 , . . . , (2.1) 

~ t+ l Vt+l W t ~ OltVt 

where vo - 0 and each fl, _> 0 be chosen so that II v, II = 1 (i > 0). The situation after 
k step be summarize by 

B V k = Vk Tk + flk+lvk+le T (2.2) 

where Tk - tridiag(fl, a,, fl,+l) and Vk ~ [Vl, v2 . . . . . vk]. If there be no round 
error we would have V T Vk = I, and the process would therefore terminate with 
fl,+~ = 0 for some k ___ n. Some other stop criterion be need in practice, since 
ilk+, be unlikely to be negligible for any k. In any event, eq. (2.2) hold to within 
machine precision. 

Now suppose we wish to solve the symmetric system B x = b. Multiplying (2.2) 
by an arbitrary k-vector yk, whose last element be ~/h, give B V k y k ~- Vk Tkyk + 
flk+~Vk+~k. Since Vk (fl~ el) = b by definition, it follow that ifyk and xk be define 
by the equation 

Thyk = file1, 
(2.3) 

xk = Vkyk, 

then we shall have Bxk = b + ~lkflk+lVk+~ to work accuracy. Hence xk may be 
take a the exact solution to a perturbed system and will solve the original 
system whenever 7/kflk+l be negligibly small. 

The above argument be not complete, but they provide at least some 
motivation for define the sequence of vector {Xk} accord to (2.3). It be now 
possible to derive several iterative algorithm for solve B x = b, each character- 
ized by the manner in which yk be eliminate from (2.3) (since it be usually not 
practical to compute each Yk explicitly). In particular, the method of conjugate 
gradient be know to be equivalent to use the Cholesky factorization Tk 
LkDk L~ and be reliable when B (and hence Tk) be positive definite, while algorithm 
SYMMLQ employ the orthogonal factorization Tk --/:k Q~ to retain stability for 
arbitrary symmetric B. (See [20] for further detail of these methods.) 

2.1 The Least-Squares System 

We now turn to a particular system that arises from the "damped least-squares" 
problem 

mioll[:l]X-[:]ll: ,2,, 
ACM Transactmns on Mathematmal Software, Vol. 8, No. 1, March 1982 



46 C.C. Paige and M. A. Saunders 

where A and b be give data and ~ be an arbitrary real scalar. The solution of 
(2.4) satisfies the symmetric (but indefinite) system 

A r 

where r be the residual vector b - Ax. When the Lanczos process be apply to this 
system, a certain structure appear in relation (2.1)-(2.3). In particular, (2.1) 
reduces to the procedure define a Bidiag 1 in Section 3, while (2.3) can be 
permute after 2k + 1 iteration to the form 

[ ' 1P +,l _- 
(2.6) 

Irk] ffi [ U~+I 0 lrt.+,l, 
x , V,j L Y' J 

where Bk be (k + 1) x k and low bidiagonal. We see that y~ be the solution of 
another damped least-squares problem, 

min l ] [B;]yk - [ f l~ l ] [ [2 , (2.7) 

which can be solve reliably use orthogonal transformations. This observation 
form the basis of algorithm LSQR. 

2.2 A Different Starting Vector 
For completeness we note that a second least-squares algorithm can be derive 
in an analogous way. Defining s = -Ax, we can write (2.5) a 

A s 

and apply the Lanczos process again, use the same matrix a before but with 
the new start vector shown. This time, (2.1) reduces to Bidiag 2 a define in 
Section 3, while (2.3) can be permute to the form 

-x IJLy J -O,e, ' 
(2.9) 

x, V, JLyk j ' 
after 2k iterations, where Rk be k × k and upper bidiagonal. (The odd-numbered 
system be not useful because T2k-~ be singular when ~ = 0.) We see that yk 
satisfies the system 

(RTRk + ~2I)yk ffi 01el, (2.10) 

which could easily be solved. However, (2.10) prof to be one form of the normal 
equation associate with (2.7), and the algorithm it suggests for compute x 
ACM Transactlov.s on Mathematical Software, VoL 8, No. 1, March 1982. 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 47 

therefore have unsatisfactory numerical properties. We clarify this mat te r in 
Section 7.4. 

2.3 The Role of 

The quantity generate by the Lanczos process from (2.5) and (2.8) be Bk, Uk+~, 
Vk and Rk, Pk , Vk, respectively. These be all i n d e p e n d e n t o f k , which mean 
tha t they be the same a those generate when k ffi 0. We shall therefore assume 
from now on tha t k ffi 0. A give k can be accommodate when solve (2.7), a 
show in [22]. Methods for actually choose ?, be beyond the scope of this 
paper. 

3. THE BIDIAGONALIZATION PROCEDURES 

The precede use of the Lanczos process result in two form of a bidiagonali- 
zation procedure due to Golub and Kahan [9]. We state these form a procedure 
Bidiag 1 and 2, and then give some unexpected relationship between them. 

B i d i a g I (starting vector b; reduction to low bidiagonal form): 

f l l u l = b, a l v l = A Tul. 

fl,+lu,+~ = A v , - a ,u , ~ , i ffi 1, 2 . . . . . (3.1) 

0~+1V~+l = ATu~+I ~ flt+lVz ) 
The scalar a, _> 0 and fl, _> 0 be chosen so tha t [[ u,[I -- [[ viii ffi 1. With the 
definition 

Uk ~- [ U l , U2 , • • . , Uk], B2 012 

Vk ~ [Vl, V2, V, ], Bk -- [33 " ' . ' 
• ' ' , " . Otk 

( w h e r e Bk be the rectangular matrix introduce in Section 2), the recurrence 
relation (3.1) may be rewrite a 

Uk+~(fl~e~) ffi b, (3.2) 

A Vk -~ Uk+lBk, (3 .3 ) 

T ATUk+I -~ V k B T + Olk+lVk+lek+l. (3 .4 ) 

T V If exact arithmetic be used, then we would also have UT+~ Uk+~ ffi I and Vk k 
= /, but, in any event, the previous equation hold to within machine precision. 

B i d i a g 2 (starting vector A T b ; reduction to upper bidiagonal form): 

Oavl ffi A T b , p~pl ffi A v l . 

O,+lV,+~ ffi A T p , -- p ,v , ~ , i ffi 1, 2 . . . . . (3.5) 
p,+lp,+l = Av ,+l -- 0,+1p, J 

ACM Transactions on Mathematmal Software, VoL 8, No 1, March 1982. 



48 • C.C. Paige and M. A. Saunders 

Again, p, > 0 and O, > 0 be chosen so that II P, II = ]1 vi II = 1. In this case, if 

Pk -- [p l , p2, . . . ,pk], 

Yk --- Iv , , v2, . . . , vk], 

Rk ~ 

pl 

p2 03 

QQm "'o 

pk-1 

01, 

we may rewrite (3.5) a 

Vk(Ole l ) ffi A Tb, (3.6) 

A Vk ffi P k R k , (3.7) 

V, T 0 T A T p k ffi k R k + k+lVk+lek, (3.8) 

T V and with exact arithmetic we would also have P T P k = V k k = L 
Bidiag 2 be the procedure originally give by Golub and Kahan (with the 

particular start vector A Tb ). Either procedure may be derive from the other 
by choosing.the appropriate start vector and interchange A and A T. 

3.1 Relationship Between the Bidiagonalizations 

The principal connection between the two bidiagonalization procedure be that 
the matrix Vk be the same for each, and that the identity 

B T B k f f i R T R k (3.9) 

holds. This follow from the fact that v~ be the same in both eases, and Vk be 
mathematically the result of apply k step of the Lanezos process (2.2) with 
B = A TA. The rather surprising conclusion be that Rk must be identical to the 
matrix that would be obtain from the conventional QR faetorization of Bk. 
Thus 

where Qk be orthogonal. In the presence of round errors, these identity will, 
of course, cease to hold. However, they throw light on the advantage of algorithm 
LSQR over two early methods, LSCG and L S L Q , a discuss in Section 7.4. 

The relationship between the orthonormal matrix Uh and Pk can be show 
to be 

for some vector rk. We also have the identity 

0/12 '4" ~ 2 ____. p2, 0~1 ~1 = 0l , 

ACM TransacUons on Mathematlcal Software, Vol 8, No. 1, March 1982 

(3.11) 

for i > 1. 
(3.12) 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 49 

4. ALGORITHM LSQR 

The quantity generate from A and b by Bidiag 1 will now be use to solve the 
least-squares problem, min II b - A x II. 

Let the quantity 

xk = Vkyk , (4.1) 

rk = b - A x k , (4.2) 

tk+l ffi file1 - B k y k , (4.3) 

be define in term of some vector yk. I t readily follow from (3.2) and (3.3) tha t 
the equation 

rk = Uk+ltk+l (4.4) 

hold to work accuracy. Since we want H rk [[ to be small, and since Uk+l be 
bound and theoretically orthonormal, this immediately suggests choose yk to 
minimize U tk+~ H. Hence we be lead naturally to the least-squares problem 

min II/ , el -- B k y k II (4.5) 

which form the basis for LSQR. 
Computationally, it be advantageous to solve (4.5) use the s tandard QR 

factorization of Bk [8], tha t is, the same factorization (3.10) tha t link the two 
bidiagonalizations. This take the form 

pl 82 
p2 03 

t , Qo ° 

pk-1 
Pk 

~2 

84 ~k-~ 
(4.6) 

where Qk = Qk, k ÷ l . . . Q2,3QI,2 be a product of plane rotation (e.g., [25]) design 
to eliminate the subdiagonals f12, fla . . . . of Bk. The vector yk and tk+l could then 
be found from 

R k y k = h , (4.7) [o] 
tk+~ = Q [ ~h+~ " (4.8) 

However, yk in (4.7) will normally have r/o element in common with yk-1. Instead 
we note tha t [Rk f~] be the same a [Rk-1 fk-1] with a new row and column 
added. Hence, one way of combine (4.1) and (4.7) efficiently be accord to 

xk = V k R ~ l f k =-- D k f k , (4.9) 

where the column of Dk -- [all d2 . - . dk] can be found successively from the 

ACM Transac t ion on Ma thema tma l Software, Vol. 8, No 1, March 1982 



50 • C . C . Paige and M. A. Saunders 

R k D k = V T by forward substitution. With do = x0 = 0, this give system T T 

dk = ! (v , - Okdk-1), (4.10) 
pk 

xk = Xk-I + ~kdk, (4.11) 

and only the most recent i terates need be saved. The broad outline of algorithm 
LSQR be now complete. 

4.1 Recurrence Relations 

The QR factorization (4.6) be determine by construct the k t h plane rotat ion 
Qk,k+l to operate on row k and k + 1 of the transform [Bk /~le~] to annihilate 
fl~+l. This give the follow simple recurrence relation: 

o t'] o,,+, <,,k] 
s , - c , J L P , + l ~,+1 = ~ ,+ , ~-k+, ' (4.12) 

where #51 --- al , ~1 -- ill, and the scalar ca and sk be the nontrivial element of 
Qk,k+J. The quantity #54, ~k be intermediate scalar tha t be subsequently 
replace by pk, d~. 

The rotation Qk.k+I be discard a soon a they have be use in (4.12), 
since Q, itself be not required. We see tha t negligible work be involve in compute 
the QR factorization to obtain Rk, fk, and ~k+l. 

Some of the work in (4.10) can be eliminate by use vector Wk -- pkdk in 
place of dk. The main step of LSQR can now be summarize a follows. (As 
usual the scalar a, _ 0 and fl, ___ 0 be chosen to normalize the correspond 
vectors; for example, al vl ffi A T e 1 implies the computat ion 61 = ATul, a] = II 1~1 II, 
vl = (1 / a l )~1 . ) 

Algort thm L S Q R 

(1) (Initialize.) 

fl=u= = b, a~v~ - - - - A Tul, Wl = Vl, Xo = O, 

(2) For i -- 1, 2, 3 . . . . repeat step 3-6. 
(3) (Continue the bidiagonalization.) 

(a) /~ ,+1u,+1 ffi A t , , - a , u , 

(b) a,+lv,+l = ATu,+I --/~,+=v,. 
(4) (Construct and apply next orthogonal transformation.) 

(a) p, = (g~ + P , % , ) ' / ~ 

(b ) c, = F,/p, 
(c ) s, = # , + , t p , 

(d) O,+l = s,a,+l 

(e) E+~ = -c,.,+~ 

(f)~, = c,~, 

(g) ~,+, = s,~,. 

ACM Transactions on Mathematical Software, Vol 8, No 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equat,ons and Sparse Least Squares • 51 

(5) (Update x, w.) 

(a) x, = x , - I + (¢, lp , )w, 

(b) w,+, = v,+~ - (8,+l/p,)w,. 

(6) (Test for convergence.) 
Exit if some stop criterion (yet to be discussed) have be met. 

4.2 Historical Note 

The previous algorithm be derive by the author in 1973. An independent 
derivation be include in the work of Van Heijst et al. [24]. 

5. ESTIMATION OF NORMS 

Here we show that estimate of the quantity II rk II, IIATrkll, Ilxkll, IIA II, and 
cond(A) can be obtain at minimal cost from item already require by LSQR. 
All five quantity will be use late to formulate stop rules. 

Knowledge of [[ A H and perhaps cond(A) can also provide useful debug 
information. For example, a user must define his matrix A by provide two 
subroutine to compute product of the form A v and A T u . These subroutine will 
typically use data derive from early computations, and may employ rather 
complex data structure in order to take advantage of the sparsity of A. If the 
estimate of II A I] and/or cond(A) prove to be unexpectedly high or low, then at 
least one of the subroutine be likely to be incorrect. As a rule of thumb, we 
recommend that all column of A be scale to have unit length (11Aej ]1 ffi 1, j = 
1 , . . . , n), since this usually remove some unnecessary ill-conditioning from the 
problem. Under these circumstances, a program error should be suspect 
if the estimate of [[ A H differs by a significant factor from n 1/2 (since the particular 
norm estimate will be IIA NF). 

For the purpose of estimate norms, we shall often assume that the orthog- 
onality relation UkTUk = I and v k T V k = I hold, and that I[ Uk 112 -- H Vk 112 --- 1. In 
actual computation these be rarely true, but the result estimate have prove 
to be remarkably reliable. 

5.1 Estimates of H rk I] and H A Trk II 
From (4.4) and (4.8) we have 

-- T 
rk = qJk+l U k + l Q k ek+l (5.1) 

(which explains the use of rk in (3.11)); hence, by assume UT+IUk+I ffi L we 
obtain the estimate 

IIr, II = S k + , ffi ~,SkSk-1 " ' ' Sl, (5.2) 

where the form of ~k+~ fOllOWS from (4.12). LSQR be unusual in not have the 
residual vector rk explicitly present, but we see that II rk II be available essentially 
free. Clearly the product of sine in (5.2) decrease monotonically. It should 
converge to zero if the system A x ffi b be compatible. Otherwise it will converge to 
a positive finite limit. 

For least-squares problem a more important quantity be ATrk, which would be 
zero at the final iteration if exact arithmetic be performed. From (5.1), (3.4), 

ACM Transac t ton on M a t h e m a t i c a l Sof tware , VoL 8, No. 1, M a r c h 1982 



52 • C.C. Paige and M. A. Saunders 

and (4.6) we have 

ATr4 ~4+l(V4B T + e T T = ak+lV4+l 4+l )Qke4+l 

= [?k+,V4[R T 0]e4+, + ~4+lak+l(eW+lQWe4+l)V4+l. 

The first term vanishes, and it be easily see that the (k + 1)st diagonal of Q4 be 
-c4. Hence we have 

A Wr4 = -- (~4+ l O14+l Ck )O4+ l (5.3) 

and 

IIATrkll = ~k+l~k+,l ckl (5.4) 

to work accuracy. No orthogonality assumption be need here. 

5.2 An Estimate of Ilxk II 

The upper bidiagonal matr ix Rk may be reduce to low bidiagonal form by the 
orthogonal factorization 

R w 4(~k = £4, (5.5) 
where Q4 be a suitable product of plane rotations. Defining 5k by the system 

/~4~4 -- fk, (5.6) 

it follow that xk --- ( V4 R[1 ) f4 = ( Vk 0 T) 5k ------- 1Y¢4 54. Hence, under the assumption 
that v T v k ---- I, we can obtain the estimate 

II x, II = II e, II. (5.7) 
Note that the lead part of L4, ~4, l~r4, and Ek do not change after iteration k. 
Hence we find that estimate II xk II v ia (5.5)-(5.7) co t s only 13 multiplication 
per iteration, which be negligible for large n. 

5.3 Estimation of II A lit and cond(A) 

It be clear from (3.1) that all the v, lie in the range of A T and be therefore 
orthogonal to the null space of A and A TA. With appropriate orthogonality 
assumption we have from (3.3) that 

BTBk = V T A T A V k , 

and so from the Courant-Fischer minimax theorem the eigenvalue of BTBk be 
interlaced by those of ATA and be bound above and below by the large and 
small nonzero eigenvalue at A TA. The same can therefore be say of the 
singular value of Bk compare with those of A. It follow that for the 2- and F- 
norms, 

II Bk II - II A II, (5.8) 

where equality will be obtain in the 2-norm for some k _< rank(A) if b be not 
orthogonal to the left-hand singular vector of A correspond to it large 
singular value. Equality will only be obtain for the F-norm if b contains 
component of all left-hand singular vector of A correspond to nonzero 

ACM Transactions on Mathematmal Software, Vol 8, No 1, March 1982. 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 53 

singular values. Nevertheless, we will use H Bk [IF a a monotonical ly increase 
est imate of the size of A. 

The forego also implies tha t B T B k TR = R k k be nonsingular and for the 2- and 
F-norms 

[I R ; ~ [[ = I[ S~ [[ _< [[A + [[. (5.9) 

The remark on equali ty be the same, except now "largest singular value" be 
replace by "smallest nonzero singular value." 

Combining these result with the definition Dk = V k R ~ 1 in (4.9) now give 

1 <_ II B~ II IID~ I I - IIAII IIA+ll-- cond(A) (5.10) 

for the 2- and F-norms. Hence we take I[ Bk IIFII Dk Hr a a monotonical ly increase 
est imate of cond(A), which start at the optimistic est imate H B1 NFI[ D~ HF -- 1. 

Use of Frobenius norm allows the est imates to be accumulate cheaply, since 
[[ B , [[~ = [[ B,-1 [[~ + a~ + fl~+~ and [[ Dk I[~ = II D,-1 ][~ + [[ dk lie. T h e individual 
term in the sum [[ dk [[2 _ y,,,.1 6~k can be use fur ther for estimate s tandard 
errors, a we show next. 

5.4 Standard Errors 

In regression problem with m > n = rank(A), the s tandard error in the i th 
component of the true solution x be take to be & where 

s ' z _ 11 b - A x [I 2 o,, (5.11) 
m - - n 

and o, - eT(ATA)-~e , be the ith diagonal e lement of (ATA) -~. Now from (3.3) and 
(3.10) we have V T A T A V k = R T R k , which with (4.9) give 

D T A TADk = L 

Assuming tha t p remature terminat ion do not occur, it follow tha t with exact 
~ T r ~ r~T e ar i thmetic D n D T = (ATA) -l, and we can approximate the a , by o}~ ~ - e, ~ k ~ k ,. 

Since D k D [ = Dk_~DT_I + d k d T, we have 

o~, ~ = o~, ~-'~ + ~,~, o~, °~ -= O, 

and the e~ ~ be monotonical ly increase est imates of the o,,. 
In the implementat ion of LSQR we accumulate o!, k~ for each i, and up- 

on terminat ion at i teration k we set 1 = max(m - n, 1) and output the square 
root of 

s!k~ _ 1[ b - Axkl[ 2 o~k ) 
1 

a est imates of the s, in (5.11). The accuracy of these est imates cannot be 
guaranteed, especially if terminat ion occurs early for some reason. However, we 
have obtain one reassure comparison with the statistical package GLIM [16]. 

On a modera te ly ill-conditioned problem of dimension 171 by 38 (cond(A) - 
103, relative machine precision = 10-11), an accurate solution xk be obtain 
after 69 iterations, and at this stage all s! k~ agree to at least one digit with the s, 
ou tput by GLIM, and many component agree more closely. 

ACM Transac tmns on Mathematmal Software, Vol. 8, No. 1, March 1982 



54 • C . C . Paige and M. A. Saunders 

A further comparison be obtain from the 1033 by 434 gravity-meter problem 
discuss in [21]. For this problem a sparse QR factorization be constructed, 
Q A = [0s], and the quantity a, be compute accurately use R T v i = ei, ai, ffi 
[[ v, [[2. Again, the estimate of s} k) from LSQR prove to be accurate to at least 
one significant figure, and the large value be accurate to three or more digits. 

Note that s, 2 estimate the variance of the i th component of x, and that s~ k)~ 
approximates this variance estimate. In an analogous manner, we could approxi- 
mate certain covariance estimate by accumulate 

= + = o , 

for any specific pair (i,j), and then compute 

][ b - A x k [I 2 _(,~ 
~ q 

1 

on termination. This facility have not be implement in LSQR and we have 
not investigate the accuracy of such approximations. Clearly only a limited 
number of pair (i, j ) could be dealt with efficiently on large problems. 

6. STOPPING CRITERIA 

An iterative algorithm must include rule for decide whether the current iterate 
x~ be an acceptable approximation to the true solution x. Here we formulate 
stop rule in term of three dimensionless quantity ATOL, BTOL, and 
CONLIM, which the user will be require to specify. The first two rule apply to 
compatible and incompatible systems, respectively. The third rule applies to 
both. They be 

$1: Stop if I[ r~ [[ <__ BTOL[[ b [[ + ATOI~[ A [[ [I Xk U" 

]~4 Trk [[ < ATOL. $2: Stop ifHA[[ []rk[[- 

$3: Stop if cond(A) ___ CONLIM. 

We can implement these rule efficiently use the estimate of ][ rk [[, [[ A [] F, and 
so forth, already described. 

The criterion $1 and $2 be base on allowable perturbation in the data. The 
user may therefore set ATOL and BTOL accord to the accuracy of the data. 
For example, if (A, b) be the give data and (Z~, ~) represent the (unknown) 
true values, then 

ATOL = [[ A - ~ [[ 
II A II 

should be use if an estimate of this be available; similarly for BTOL. 
Criterion $3 represent an attempt to regularize ill-conditioned systems. 

ACM Transactions on Mathematmal Software, Vol 8, No, 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 55 

6.1 Compatible Systems 

To justify S1, let r~ = b - Axh a usual, and define the quantity 

64 - BTOL[I b II + ATOLII AII [I xk U, 

rk 
g k - - BTOLII b II 

rk 
=- A T O M d II II II 

Then rk = gk + hk, and 

( A + h~Xx~k~ ) Xk = b - gk 

SO that xk be the exact solution for a system with both A and b perturbed. It can 
be see that these perturbation be within their allowable bound when the 
inequality of S 1 holds. Hence, criterion S 1 be consistent with the idea of backward 
round error analysis and with knowledge of data accuracy. Since this argument 
do not depend on orthogonality, $1 can be use in any method for solve 
compatible linear systems. 

6.2 Incompatible Systems 
Stewart [23] have observe that i f 

a n d 

where 

rk ffi b -- Axk 

fk = b - (A + E k ) X k 

rkrWA 

iir, ' 

then (A + E k ) T rk = 0, SO that Xk and fk be the exact solution and residual for a 
system with A perturbed. Since H Ek 112 = ]1 A Trk I]/]] rk I], the perturbation to A will 
be negligible if the test in $2 be satisfied. 

In our particular method it happens that Ekxk ffi 0, since (5.3) show that x~ ffi 
Vkyk be theoretically orthogonal to ATrk. Hence fk = rk, SO both xk and rk be 
exact for the perturbed system. This strengthens the case for use rule $2. 

In practice we find that [I A Trk [I/I] rk I] can vary rather dramatically with k, but 
it do tend to stabilize for large k, and the stability be more apparent for LSQR 
than for the standard method of conjugate gradient (see [[ ATrk I] in Figures 3 and 
4, Section 8). Criterion $2 be sufficient to ensure that xk be an acceptable solution 
to the least-squares problem, but the existence of an easily computable test that 
be both sufficient and necessary remains an open question [23]. 

6.3 Ill-Conditioned Systems 

Stopping rule S3 be a heuristic base on the follow arguments. Suppose that A 
have singular value a~ _> o2 - - . - . --> a . > 0. It have be observe in some problem 

ACM Transact ion on Mathemat ica l Software, Vol. 8, No. 1, March 1982. 



56 C.C. Pmge and M A. Saunders 

tha t a k increase the estimate ]] Bk lit H Dk [Iv -~ cond(A) in (5.10) temporari ly 
level off near some of the value of the order sequence ol /ol , ol/o2, . . . , 
a l / o , , with vary number of iteration near each level. This tends to happen 
when the small o, be very close together, and therefore suggests criterion $3 
a a mean of regularize such problem when they be very ill-conditioned, a 
in the discretization of ill-posed problem (e.g., [15]). 

For example, if the singular value of A be know to be of order 1, 0.9, 10 -3, 
10 -6, 10 -7, the effect of the two small singular value could probably be 
suppress by set CONLIM = 10 4. 

A more direct interpretation of rule $3 can be obtain from the fact tha t xk 
= D k f h . First, suppose tha t the singular value decomposition of A be A = U Z V T 
where u T u = V T v = W T = I , ~ -~ diag(al, o2, . . . , On), and let 

A(r)= u~(r )v T 

be define by set or+~ . . . . . On ---- 0. A common method for regularize the 
least-squares problem be to compute x (r) -- V ~ ( r ) + U T b for some r _< n, since it can 
readily be show tha t the size of x (r) be bound accord to 

II A [12 [[ x (r) 1[ ~ __ ___ cond(A(r)). 
II b II or 

In the case of LSQR, we have II xk II -- II Dk I1~ II b II, and so if rule $3 have not yet 
cause termination, we know tha t II Bk II F II x~ II / lib II --- II Bk II F II D~ II ~ < CONLIM. 
Since U Bk IIF usua l ly increase to order U A [Iv quite early, we effectively have 

IlAIIr Ilxkll < CONLIM, 
II b II 

which be exactly analogous to the bound above. 

6.4 Singular Systems 

It be sometimes the case tha t rank(A) < n. Known dependency can often be 
eliminate in advance, but others may remain if only through error in the data 
or in formulation of the problem. 

With conventional (direct) method i t be usually possible to detect rank defi- 
ciency and to advise the user tha t dependency exist. In the present context it be 
more difficult to provide such useful information, but we recognize the need for 
a method tha t at least do not fail when apply (perhaps unknowingly) to a 
singular system. In such case we again suggest the parameter CONLIM a a 
device for control the computation. Our experience with LSQR on singular 
problem be tha t convergence to an acceptable solution occurs normally, but if 
iteration be allow to continue, the compute xk will begin to change again 
and then grow quite rapidly until 

[[A I[ ]] Xk [I = 1 (6.1) 
I[ b [I E 

(while ]1 rk II remains of reasonable size). The est imate of cond(A) typically grows 
large b e f o r e the growth in xk. A moderate value of CONLIM (say 1/e 1/2) may 
therefore cause termination at a useful solution. 

ACM Transactions on Mathematmal Software, Vol 8, No. 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 57 

In some case it can be useful to set CONLIM a large a 1/E and allow xk to 
diverge. In this context we note tha t the algorithm SYMMLQ [20] can be apply 
to singular symmetric systems, and tha t extreme growth in the result II xk II 
form an essential part of a practical method for compute eigenvectors of large 
symmetric matrix [14]. By analogy, in the presence of round error LSQR 
will usually produce an approximate singular vector of the matrix A. In fact, use 
(6.1) and II rk II - II b II, we see tha t the normalize vector xk = xk / l l xk II will usually 
satisfy 

1 
AEk = ~ (b - rk) 

= E ~ ( b - r k ) 

= 0(~)IIAII 

for large enough k, and hence will lie very nearly in the null space of A. The 
vector 2k may reveal to the user where certain unexpected dependency exist. 
Suitable new row could then be add to A. 

7. OTHER METHODS 

Several other conjugate-gradient method be discuss here. All except the first 
(CGLS) be state use notation consistent with Sections 3-6 ih order to 
illustrate certain analytic identities. 

7.1 CGLS 

If the conjugate-gradient method for symmetric positive definite system be 
apply naively to the normal equation A T A x = A T b , the method do not 
perform well on ill-conditioned systems. To a large extent this be due to the 
explicit use of vector of the form A TAp, . An algorithm with good numerical 
property be easily derive by a slight algebraic rearrangement, make use of 
the intermediate vector A p , [10]. I t be usually state in notation similar to the 
following. 

Algor i thm C G L S 

(1) Se t ro=b , s o = A T b , p~=so , yo=ilSoll 2, xo=0. 

(2) For t = 1, 2, 3, . . . repeat the following: 

(a) q, = Ap, 

(b) a, = ~,,_,/ l lq, II ~ 

(c) x, = x,-~ + a,p, 

(d) r, = r,-i - a,q, 

(e) s, = A Tr, 

( f ) r , = IIs, lff 
(g) ,e, = r,l~,,-, 

(h) p,+l = s, + fl,p,. 
ACM Transactions on Mathematical Software, Vol. 8, No. 1, March 1982. 



58 • C.C. Paige and M. A. Saunders 

A practical implementation of the method would also need to compute ]] r, ]], 
[I xi [I, and an estimate of II A II in order to use the stop criterion developed in 
Section 6. Otherwise the method be clearly simple and economical. Analytically 
it generates the same point x, a LSQR. The vector v,+~ and d, in LSQR be 
proportional to si and p,, respectively. 

Note that qi and s, just give can share the same workspace. A FORTRAN 
implementation of CGLS have be give by Bj6rck and Elfving [3]. This incor- 
porates an acceleration (preconditioning) technique in a way that require mini- 
mal additional storage. 

7.2 Craig's Method for A x ffi b 

A very simple method be know for solve compatible system A x ffi b. This be 
Craig's method, a described in [6]. It be derivable from Bidiag 1, a show by 
Paige [17], and differs from all other method discuss here by minimize the 
error norm [I xk - x H at each step, rather than the residual norm U b - A x k [[ ffi 
II A (xk - x)I[. We review the derivation briefly. 

If Lk be the first k row of Bk, 

L k ~- 
~2 0/2 

Q,o ",° 

~k ~k 

then eqs. (3.3)-(3.4) describe Bidiag 1 may be restate a 

A V k ffi U k L k + flk+aUk+le T, 

A T u k ffi V k L T" (7.1) 

Craig's method be define by the equation 

LkZk -~ f l l e l , xk ffi V k z k , (7.2) 

and we can show from (7.1) that the residual vector satisfies rk - b - A V k Z k f f i 
--~kflk+lUk+l and hence U T r k ffi 0. We can therefore expect rk to vanish (analyti- 
cally) for some k _ n. 

The vector Zk a n d xk be readily compute from 

h ffi - / ~ - ' ~k- l , x , ffi x , - 1 + ~ , v , , 
~k 

where ~o - -1 . Since the increment v~ form an orthogonal set, there be no danger 
of cancellation, and the step length ~k be bound by [ ~k I - ]1 zk H - H xk I] -< II x H. 
We can therefore expect the method to posse good numerical properties. This 
be confirm by the comparison in Section 8. 

7.3 Extension of Craig's Method 

A scheme for extend Craig's method to least-squares problem be suggest 
by Paige in [17]. The vector in (7.2) be retain and an additional vector of 

ACM Transactions on Mathematmal Software, Vol 8, No 1, March 1982. 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 59 

the form Vk Wk be compute in parallel. On termination, a suitable scalar yk be 
compute and the final solution take to be 

Xk ffi ( V k Z k ) - - y k ( V k W k ) ~ V k y k . 

In the present context this method may be interpret a a mean of solve the 
least-squares system (2.7), namely, 

. ..,,]rt,,.,l__ 
BT L Y' J 

use the fact that the underdetermined system BTtk+I ---- 0 have a unique solution 
apart from a scalar multiple. 

In practice we have found that the method be stable only when b lie in the 
range of A. Further detail be give in [21]. 

7.4 LSOG and LSLQ 

A second algorithm for least-squares problem be give by Paige [17]. This be 
algorithm LSCG, base on Bidiag 2. In the notation of Section 3 this algorithm 
be define by the equation 

R T R k y k ffi 01el, Xk = Vkyk (7.3) 

and implement in the form RTfk ---- 01e~, Xk ffi ( V k R ~ ) f k . Given the relation 
between the two bidiagonalizations, we now recognize that this be analytically 
equivalent to LSQR, but numerically inferior, since it be effectively solve the 
least-squares problem mini] Bkyk -- file1 [[ by use the correspond normal 
equations. (The latter be B T B k y k ffi BTfl~e~ ffi a~fl~e~ and by (3.9) and (3.12) this 
be equivalent to the first equation in (7.3).) 

Algorithm LSLQ [19] be a refinement of LSCG, but again it be base on Bidiag 
2 and the normal equation just given, and be therefore inferior to LSQR on ill- 
condition problems. The refinement have be described in Section 5.2, give 
xk -- IYVk 5k, where lYtrk be theoretically orthonormal, the intention be to avoid 
any possible cancellation that could occur in accumulate xk = Dk f~ •- ( V k R ~ 1) fk. 
The same refinement can easily be make to LSQR, and it be implement in an 
early version of the algorithm for the same reason. However, we have not be 
able to detect any numerical difference between xk ffi I~¢kSk and Xk = Dkfk in the 
two version of LSQR, so the fear of cancellation appear to have be unfounded. 
We have therefore retain the slightly more economical Xk = Dkfk, which also 
allows cond(A ) to be estimate from ]] Dk IIF, a already described. 

Algorithms LSCG and LSLQ need not be consider further. 

7.5 Chen's Algorithm RRLS 

Another algorithm base on Bidiag 2 have be described by Chen [4]. This be 
algorithm RRLS, and it combine Bidiag 2 with the so-called residual-reducing 
method of Householder [11]. In the notation of Section 3 it may be described a 
follows. The residual-reducing property be implicit in step 2(b) and (c). 

Algorithm R R L S 

(1) Set ro ffi b, 8~v~ = A T b , w l = v l , Xo = O. 

ACM Transac tmns on MathemaUcai Software, Vol 8, No. 1, March 1982 



60 C.C. Palge and M. A. Saunders 

(2) For i -- 1, 2, 3 . . . . repeat the following: 

(a) p , p , ffi A w, 

(b) k, •pTr , 

(c) r, ffi r,-1 - ~,p, 

(d) 8,+iv,+l = A T p , - - p , v ~ 

(e) x, ffi x,-1 + (k,/p,)w, 

(f) w,÷l = v,+~ - (O,+Jp,)w,, 

where the scalar p, and 0, be chosen so that UP, H = ]] v, ]] = 1. 

As with CGLS, a practical implementat ion would also require ]] r, ]] and I[ x, [[. The 
square root of the sum ~ - 1 (p~ + 0~+1) = l] Rk ]]~ ffi ]] Bk lid could be use to 
estimate ][ A ]IF, and II ATr , H could also be est imated cheaply. 

Note tha t the vector v, be generate a in Bidiag 2, but the vector p, come 
instead from step 2(a). Substi tut the latter into step 2(d) show tha t RRLS 
require explicit computat ion of the vector A T A w , (ignoring normalization by 
p,). Unfortunately this must cast doubt on the numerical property of the 
method, particularly when apply to compatible systems. Indeed we find tha t 
for some system A x = b, the final norm I[ r, [I and II x, - x [[ be larger, by a factor 
approach cond(A), than those obtain by CGLS and LSQR. This be illustrate 
in Section 8.3. 

A second algorithm call R R L S L have be described by Chen [4], in which 
the residual-reducing method be combine with Bidiag 1. However, the start 
vector use be AATb (rather than b), and product of the form A T A w , be again 
required, so tha t improve performance seem unlikely. Chen report tha t RRLS 
and R R L S L behave similarly in all test case tried. 

In spite of the above comments, we have also observe ill-conditioned least- 
square problem for which RRLS obtains far g r e a t e r accuracy than would 
normally be expect of any method (see Section 8.4 for a possible explanation). 
Because of this unusual behavior, we have investigate a residual-reducing 
version of LSQR a now described. 

7.6 RRLSQR 

If the residual vector r, be explicitly introduced, algorithm LSQR a summarize 
in Section 4.1 can be modify slightly. First, the residual-reducing approach 
require step 5(a) to be replace by the two step 

r , ffi r,_~ - X , p , , x, = x,-1 + ,k,w,, 

where p, = A w , and X, ffi p Tr , -1 / l ip , ns. (In this case p, be unnormalized.) Second, 
the product A w , can be use to eliminate A v , from Bidiag 1, lead to an 
alternative method, 

fl,+lu,+l ffi A w , - II r,-1 [-------[ r,-l, (7.4) 

for generate each fit and u,. (This result be difficult to derive, but the key 
relation be p, / ] IP, ]1 = c,r,-1 /][ r,-1U + s,u,+l, which may be deduce from (3.11).) 

The remainder of LSQR be retained, include the QR factorization of Bk. The 

ACM Transact ion on Mathematmal Software, Vol 8, No 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 61 

coefficient of r,-1 in (7.4) can be express in several ways; for example, 

E ~ 1 1) ' - ' ala2 . . . a, 
II r , _ , II = - - ( - . . . P , ' 

where ~', come from the system L k z k = fl~ el of Craig's method. Different formula 
lead to different i teration paths, but no variation appear to be consistently be t te r 
than the rest. 

A summary of the result algorithm follows. 

Algor i thm R R L S Q R 

(1) ro=b , f l l u l = b , alVl=ATub WL=Vb Xo=O, 

(2) For i = 1, 2, 3, . .. repeat step 3-6. 

(3) (a) p, = Aw, 

(b) X, = pTr,_l/ l lp, I]2 

(c) r, = r . - h , p , 

(d) fl,+lu,+1 = p, - (E/~,)r,-, 

(e) at+l V~+l = A T u t + I -- ~t+lVt 

(4) Compute p,, c,, s,, E+,, P~+~, ~,,+~ a in Section 4.1, step 4. 

(5) (a) x, = x ,_~+X,w, 

(b) W z + I = V , + I - - (Ot+l/Pt)Wt 

(6) Exit if appropriate. 

This adapt ion of Bidiag 1 to obtain R R L S Q R be analogous to (and be mot ivated 
by) Chen's adaption of Bidiag 2 to obtain RRLS. Note, however, tha t there be 
no product of the form A TAw, . In practice we find tha t R R L S Q R typically 
performs at least a well a LSQR, a measure by the limit H x, - x ]] attainable. 
Fur thermore , it at tains the same unusually high accuracy achieve by RRLS on 
certain ill-conditioned least-squares problems. On these ground R R L S Q R could 
sometimes be the prefer method. However, it work and storage requirement 
be significantly-higher than for the other method considered. 

7.7 Storage and Work 

The storage and work requirement for the most promising algorithm be 
summarize in Table I. Recall tha t A be m by n and tha t for least-squares 
problem m may be considerably large than n. Craig's method be applicable only 
to compatible system A x = b, which usually mean rn -- n. 

All method require the start vector b. If necessary this may be overwri t ten 
by the first m-vector show (r or u). The m-vector A v show for Craig and LSQR 
represent work storage to hold product of the form A v and A T u . (An n- 
vector would be need if m < n.) In some application this could be dispense 
with if the bidiagonalization operation A v - a u and A TU -- fl V be implement 
to overwrite u and v, respectively. Similarly, the n-vector AWp for RRLS could in 
some case be compute wi thout extra storage. 

ACM Transactions on Mathematical Software, Vol. 8, No. 1, March 1982. 



62 • C.C. Paige and M. A. Saunders 

Table I 

Storage Work 

m n m /'$ 

Craig 
(Ax = b only) u, A v x, v 3 4 

C G I ~ r, q x, p 2 3 
I ~ Q R u, A v x, v, w 3 5 
RRLS r , p x , v , w , A T p 4 5 
RRLSQR r, u, p x, v, w 6 5 

The work show for each method be the number of multiplication per iteration. 
For example, LSQR require 3m + 5n multiplications. (A further 2n multiplica- 
tions be need to accumulate estimate of cond(A ) and standard error for x.) 
Practical implementation of CGLS and RRLS would require a further m + n 
multiplication to compute H r, I[ and [[ x, [[ for use in stop rules, although this 
could be limited to every tenth iteration, say, without serious consequence. 

All method require one product A v and one product A T o each iteration. This 
could dominate the work requirement in some applications. 

8. NUMERICAL COMPARISONS 

Here we compare LSQR numerically with four of the method discuss in 
Section 7, denote by CRAIG, CGLS, RRLS, and RRLSQR. The machine use 
be a Burroughs B6700 with relative precision ~ = 0.5 × 8 -12 = 0.7 × 10 -11. 

The result give here be complementary to those give by Elfving [5], who 
compare CGLS with several other conjugate-gradient algorithm and also inves- 
tigates their performance on problem where A be singular. 

8.1 Generation of Test Problems 

The follow step may be use to generate a test problem min [[ b - A x I[ with 
know solution x. 

(1) Choose vector x, y, z, c and diagonal matrix D arbitrarily, with U Y [I ffi [[ z II 
ffi 1. (For any chosen m >_ n, the vector should be of dimension n, m, n, and 
m - n, respectively.) 

(2) Define 

Y = I - 2 y y T, 

(3) Compute 

The minimal residual norm be then II r I[ = II c II. Since A and D have the same 
singular values, the condition of the problem be easily specified. 

The particular problem use here will be call 

P ( m , n, d , p ) 

ACM Transactions on Mathematical Software, Vol 8, No 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equat,ons and Sparse Least Squares • 63 

T a b l e I I 

log,011 ~, - ~ II 
Case 

(m = n = 10, c o n d ( A ) = 10 s) k = 60 80 100 120 

1 A = Y D Z ( a s ~ o v e ) - 0 . 3 - 3 . 3 - 3 . 3 - 3 3 
2 A = Y D - 0 . 5 - 3 . 9 - 3 . 9 - 4 . 1 

3 A = D Z - 2 . 1 - 5 . 9 - 5 . 9 - 9 . 2 
4 A = D - 9 . 4 - 9 . 4 - 9 . 4 - 9 . 4 

to indicate dependence on four integer parameter , where d represent duplication 
of singular value and p be a power factor. The matr ix D be of the form diag(o~'), 
with each o, duplicate d times. Specific value for x, y, z, c, and D be generate 
a follows: 

(1) x = ( n - l , n - 2 , n - 3 . . . . . 2 ,1 ,0 ) w. 
(2) y, = sin(4cri/m), z, = cos(4~ri/n), follow by normalization so tha t [[ y [[ = [[ z [[ 

--~1. 

(3) c = (1 /m, - 2 / m , 3 /m , . . . , +.(m - n ) / m ) T. 
(4) o, = [(i - 1 + d ) / d ] d / n , where integer division be use for the te rm in square 

brackets. Choosing n = q d to be a multiple of d lead to d copy of each 
singular value: 

_ ( o , ) = , . . . . q , q . . . . . q . . . . , q . . . . , . 

[For reference, this give [[ x [[ = n ( n / 3 ) 1/2, [[ r[[ = [[ c [['-~ [(m - n ) / m ] ((rn - n ) / 
3) ~/2, ][ A ]] ~ = [[ D [[ F = (n /3 ) ~/2, cond(A) = cond(D) = (o, /ol) p = qP.] 

The orthogonal matrix Y and Z be intend to reduce the possibility of 
anomalous numerical behavior. For example, when LSQR be apply to four 
case of the problem P (10, 10, 1, 8), the follow error norm result (Table II). 
Since each case be a compatible system A x = b, we normally would expect an 
error norm approach [] x [[. cond(A ). ~ = 10 -2, so tha t case 1 be the most realistic. 
In case 2 the error be concentra ted in the first and second component of xk 
(with the remain component accurate almost to work precision), whereas 
in case 3 and 4 the final xk be virtually exact in spite of the high condition 
number of A. 

Although case 2-4 represent less expensive test problems, it be clear tha t 
result obtain from them could be very misleading. In the follow subsection 
we use only case 1. Even these test problem may be less than completely general. 
This possibility be discuss in Section 8.4. 

8.2 Ax = b: Deterioration of CGLS ,n Adverse Circumstances 

Figure 1 illustrates the performance of five method on the ill-conditioned system 
P(10, 10, 1, 8), tha t is, m = n = 10, one copy of each singular value, cond(A) -- 
l0 s. The quantit y log~011rk [[ and log~oll xk - x I[ be plot ted against i terat ion 
number k. 

This example distinguishes the s tandard conjugate-gradient me thod CGLS 
from the remain methods. All except CGLS reduce II rk II and ]1 xk - x II to a 
satisfactory level before k -- 120. 

ACM Transactions on Mathematical Software, Vol. 8, No. 1, March 1982. 



64 • C.C. Paige and M. A. Saunders 

IO91ollxv-xll 

L A ~ L A ~ ~ A 

o - CRAIG 

A - C G L S 
x = R R L S 

<> = L S O R 
+ - R R L S O R 

Iogloll%ll 

! | ! ! 
| ! I t I I t ,, . I 

10 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100 110 120 

Fig. 1. All i l l -condit ioned s y s t e m s Ax -- b, n -- 10, cond(A) = l0 s. C G L S be unab le to reduce I[ rk [[ or 
I[ xk - x [[ sat isfactor i ly C R A I G exhibi t s severe f luc tua t ion in [[ rk [[ 

ACM Transactions on Mathematical Software, Vol 8, No 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 65 

Also apparent be the erratic behavior of [I rk [[ for method CRAIG, a potential 
penalty for minimize [[ x~ - x [] at each step without regard to [[ rk [[. In theory all 
other method minimize ][ rk ]] at each step and also reduce [[ xk - x [] monotonically. 

If any method be to be prefer in this example it would be LSQR, since it 
reach limit accuracy at iteration 76 and stayed at essentially the same point 
thereafter. With value ATOL = BTOL = e, the stop rule S1 a implement 
in LSQR would have cause termination at k = 76 a desired. 

8.3 Ax = b: Failure of RRLS 

Figure 2 illustrates the same five method apply to a large problem P (40, 40, 
4, 7), in which each singular value be repeat four time and cond(A) ~- 107. In 
this case all method except RRLS reduce [[ rk [] satisfactorily to about 10 -9 for 
k >_ 105. For method RRLS, [[ rk [[ remain of order 10 -~ for k >_ 30 up to k = 250, 
and zero digit of accuracy be obtain in xk. 

A similar disparity between RRLS and the remain method be observe 
on the problem P (40, 40, 4, p) , p -= 5, 6, cond(A ) = l0 p. In fairness, Chen [4] do 
not intend RRLS to be apply to compatible systems. However, the success of 
the other least-squares method suggests tha t this be not an unreasonable demand. 

8.4 min [[ A x - b [[: High Accuracy by RRLS and RRLSQR 

Figure 3 show the performance of four least-squares method on the ill-condi- 
tioned problem P(20, 10, 1, 6). Since cond(A) 2 -- 10 ~2 = l /e , we would normally 
expect at most one digit of accuracy in the final xk. This be achieve by LSQR 
and CGLS, with LSQR show a smoother decrease of ][ AWrk [[. 

In contrast, the residual-reducing method achieve at least six digit of 
accuracy in xk. Similarly, three or four digit of accuracy be obtain on the 
problem P(20, 10, 1, 8), for which cond(A) = l0 s be so high tha t no digit could be 
expected. At first sight it may appear tha t the residual-reducing method posse 
some advantage on least-squares problems. However, this anomalous behavior 
cannot be guaranteed; for example, it do not occur on P(80, 40, 4, 6), a show 
in Figure 4. Also, the final value of [[ A Trh [[ be no small than for LSQR and this 
be really the more important quantity. 

Par t of the explanation for these occasional anomaly may lie in the following. 
Suppose the original data (A, b) have solution and residual (~, f), while perturbed 
data (A + ~A, b + t~b) have (~ + ~x, ~ + ~r). I f A + ~A have full column rank, then 
it be straightforward to show tha t 

8x = (A + 8A)+(Sb - ~A£) + ((A + ~A)T(A + ~A))-I~ATfi. 

In the present example ~ = 0.7 x 10 -1~, ][ A ][ 2 -- 1, cond(A) =- 106, [[ b ][ - 2.4, [[ ~ [[ 
17, ][ ~ ][ = 1. If the perturbation be cause by round error in the initial 

data, then [[ 8A [[ = e, [[ ~ b [[ = E, and the first term in the expression for ~x could 
be about a large a 10 -4 in norm, and the second could be of order 7. Figure 3 
suggests the second term be unusually small for the RRLS and RRLSQR com- 
putations. Looking at the particular form of the test problem, if we write 

Y = [Y1, Y2], A = Y, DZ, ~= Y2c, 

ACM Transactmns on Mathematical Software, Vol. 8, No. 1, March 1982. 



6 6 C . C . P a i g e a n d IVl. A . S a u n d e r s 

2 

0 

-2 

-4 

tOgloIIxk-xll 

toglollrl, II 

-6 

-8 

- 1 0 

a : CRAIG 

: C G L S 

× : R R L S 

o : LSQR 

+ : R R L S Q R 

I I I I I I i I I I I 

10 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100 110 

Fig. 2. A n i l l -condi t ioned s y s t e m A x = b, n = 40, c o n d ( A ) = 107. R R L S be u n a b l e to r e d u c e [[ r , II o r 
il x~ - x II sa t i s fac tor i ly . 

| 

120 

ACM Transactions on Mathematical Software, Vol 8, No 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares . 67 

~-,, ~ 5 -5 -'5 ~,- 5 ~ 

Iog:011xk-xll 

tOglo IIATrk II 

A = C G L S 
x = R R L S 
o = L S O R 
+ = R R L S O R 

10 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100 

Fig. 3. An ill-conditioned problem, minll A x - b I[, m = 20, n = 10, cond(A) := 10 ~. R R L S and R R L S Q R 
achieve anomalous ly high a c c u r a c y in xh . 

ACM Transacgmns on Mathematical Software, Col. 8, No. 1, March 1982 



68 C.C. Paige and M. A. Saunders 

we have 

(A + 3A) + = A + = Z T D - 1 Y T, 

and the second term in 6x be effectively 

ZTD-2Z~A Ty2c. 

Now suppose 6A be simply an equivalent perturbation in A cause when A 
multiplies a vector v in our test case. Using the result of round error analysis 
give by Wilkinson [25], 

(A + ~ A ) v - f l (Y~f l (Dfl (Zv))) = (Y~ + ~Y1)(D + 6D)(Z + 6Z)v, 

where I[ ~ YI [I = ¢ I[ Y1 [[, and so forth; hence 

6A - 3Y~(D + 6 D ) ( Z + 6Z) + Y , ( D 6 Z + 6 D ( Z + 6Z)). 

Using this 6A in the second term for 6x effectively give 

Z T D - I ( I + D - I Z 6 Z T D ) ( I + D-16D)S yTy2c , 

which be bound above by about 7 × 10 -6 in norm, rather than 7 a expected. 
This give a hint of what might be happen above, since a more realistic 
problem would not admit such a relation between round error and residual. 
This do not invalidate the other numerical comparisons, but it do emphasize 
the care need when construct artificial test problems. 

8.5 min I l A x - bll: Normal Behavior 
Figure 4 illustrates more typical performance of the four methods, use the least- 
square problem P(80, 40, 4, 6) for which cond(A) = 106. All method reduce 
I]AWrk[] to a satisfactory level, and the final error norm be consistent with a 
conventional sensitivity analysis of the least-squares problem; in this case, no 
more than one significant digit can be expected. Note that CGLS converge more 
slowly than the other methods. It also displayed rather undesirable fluctuation 
in ][ A Wrk [I considerably beyond the point at which the other method reach 
limit accuracy. 

8.6 Some Results Using Greater Precision 

Algorithm LSQR be also apply to the previous four test problem on an IBM 
370 computer use double-precision arithmetic, for which E = 2.2 × 10 -16. With 
increase precision, LSQR give high accuracy and also require few step to 
attain this accuracy. This be best see by refer to the figures. In Figure 1 the 
log of the residual reach -14.4 at the forty-eighth step and stayed there; the 
log of the error be then -8.6, but decrease 20 step late to -9.3 and stayed 
there. In Figure 2 the log of the residual and error be -13.8 and -8.0 at step 
44 and differ negligibly from these value thereafter. In Figure 3, logm II A Trk II 
-- -14.6 and lOglo [[ xh - x [I = -6.0 at k -- 32 and thereafter, while in Figure 4, 
loglo H ATrk [[ = --13.9 and loglo I[xh - x ][ = -4.6 at k = 36, with little change for 
large k. 

8.7 Other Results 

Algorithms CGLS and LSQR have be compare independently by BjSrck [1], 
confirm that on both compatible and incompatible system LSQR be likely to 
ACM Transact ion on Mathemat ica l Software, Vol 8, No 1, March 1982 



LSQR' An Algorithm for Sparse Linear Equations and Sparse Least Squares 69 

togl011Xk-xll 

/ I°gIoIIAT rkll 

"1 

-1 

A = CGLS 
x = R R L S 

o : LSOR 
+ = R R L S Q R 

I I I I I t i I i I 

10 20 30 4 0 50 60 70 8 0 9 0 100 
Fig. 4. An fl l-condltmned problem, minll A x - b II, m = 80, n = 40, cond(A) = 106. All m e t h o d s obta in 
a sat isfactory solution, a l though CGLS exhibit slow convergence and undes i rable f luc tuat ion in 

It ATrk II. 

ACM Transactions on Mathematical Software, Vol. 8, No. 1, March 1982 



70 C.C. Palge and M. A. Saunders 

obtain more accurate solution in few iterations. The difference be not significant 
when A be reasonably well conditioned, but in extreme case CGLS may need up 
to twice a many iteration to obtain comparable precision (Figures 1-4). 

9. SUMMARY 

A direct method may often be preferable to the iterative method discuss here; 
for instance, the method give by Bjorck and Duff [2] and George and Heath 
[7] show great promise for sparse least squares. Nevertheless, iterative method 
will always retain advantage for certain applications. For example, conjugate- 
gradient method converge extremely quickly if A be of the form M - N where 
M T M ffi I and N have low rank. They also have low storage requirement (for both 
code and workspace). 

Our aim have be to present the derivation of a new conjugate-gradient 
algorithm, along with detail of it implementation and sufficient experimental 
evidence to suggest that it compare favorably with other similar method and 
that it can be rely upon to give satisfactory numerical solution to problem of 
practical importance (see also [21]). 

Reliable stop criterion be regard here a be essential to any iterative 
method for solve the problem Ax ffi b and min II A x - b II. The criterion developed 
for LSQR may be useful for other solution methods. Estimates of II A ]1, cond(A), 
and standard error for x have also be developed to provide useful information 
to the user at minimal cost. 

In closing, we make the follow recommendations: 

(1) The symmetric conjugate-gradient algorithm should be apply to the normal 
equation A T A x •- A Wb only if there be reason to believe that very few 
iteration will produce a satisfactory estimate of x. 

(2) The least-squares adaptation of symmetric CG will always be more reliable, 
at the expense of slightly more storage and work per iteration. (This be the 
algorithm of Hestenes and Stiefel, described in Section 7.1 of this paper a 
algorithm CGLS.) The additional expense be negligible unless m >> n. 

(3) If A be at all ill-conditioned, algorithm LSQR should be more reliable than 
CGLS, again at the expense of more storage and work per iteration. 

ACKNOWLEDGMENTS 

We wish to thank Dr. Robert Davies of the New Zealand DSIR for his assistance 
and advice on many aspect of this work. We be also grateful to Dr. Derek 
Woodward of the DSIR for the feedback obtain during his application of LSQR 
to the analysis of gravity-meter observations, to Professor/kke Bjorck for his 
helpful comment on the manuscript, and to Professor Gene Golub and the 
referee for some additional useful suggestions. 

REFERENCES 

1. BJORCK, ~. Use of conjugate gradient for solve linear least square problems. In Duff, I.S. 
(Ed.), Conjugate-Gradwnt Methods and Stmilar Techntques, Rep. AERE R-9636, Computer 
Science and Systems Division, AERE Harwell, England, 1979, 48-71. 

2. BJORCK, ~ , AND DUFF, I.S. A direct method for the solution of sparse linear least square 
problems. Lmear Algebra Appl. 34 (1980), 43-67. 

ACM TransacUons on Mathematmal Software, Vol 8, No. 1, March 1982. 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 71 

3. BJORCK, A, AND ELFVING, T. Accelerated projection method for compute pseudoinverse 
solution of system of linear equations. Res Rep. LITH-MAT-R-1978-5, Dep. Mathematics, 
Linkoping Univ., Linkoping, Sweden, 1978. 

4. CHEN, Y.T. Iterative method for linear least-squares problems. Res. Rep. CS-75-04, Dep. of 
Computer Science, Univ. Waterloo, Waterloo, Ont., Canada, 1975. 

5. ELFVING, T. On the conjugate gradient method for solve linear least-squares problems. Res. 
Rep. L1TH-MAT-R-1978-3, Dep. Mathematics, Linkoping Univ., Link~ping, Sweden, 1978. 

6. FADDEEV, D.K., AND FADDEEVA, V.N. Computattonal Methods of Linear Algebra, Freeman, 
London, 1963. 

7. GEORGE, A, AND HEATH, M T. Solution of sparse linear least square problem use Givens 
rotations. Linear Algebra Appl. 34 (1980), 69-83. 

8. GOLUB, G.H. Numerical method for solve linear least-squares problems. Numer. Math. 7 
(1965), 206-216. 

9. GOLUB, G.H., AND KAHAN, W. Calculating the singular value and pseudoinverse of a matrix. 
SIAM J. Numer. Anal. 2 (1965), 205-224. 

10. HESTENES, M.R., AND STIEFEL, E. Methods of conjugate gradient for solve linear system J. 
Res. N.B.S. 49 (1952), 409-436. 

11. HOUSEHOLDER, A.S. Terminating and non-terminating iteration for solve linear systems. 
SIAM J. Appl. Math. 3 (1955), 67-72. 

12. KENNEDY, W.J., AND GENTLE, J.E. Stattstwal Computing. Marcel Dekker, Inc., New York and 
Basel, 1980. 

13. LANCZOS, C. An iteration method for the solution of the eigenvalue problem of linear differential 
and integral operators. J Res. N.B.S. 45 (1950), 255-282. 

14. LEwis, J.G. Algorithms for sparse matrix eigenvalue problems. Res. Rep. STAN-CS-77-595, 
Stanford Univ., Stanford, CA, 1977. 

15. NASHED, M.Z. Aspects of generalize inverse in analysis and regularization. In Nashed, M.A. 
(Ed.), Generahzed Inverses and Applwat~ons, Academic Press, New York, 1976, 193-244. 

16. NELDER, J.A. GLIMManual. Numerical Algorithms Group, 13 Banbury Road, Oxford, England, 
1975 

17. PAIGE, C.C. Bidiagonallzatlon of matrix and solution of linear equations. SIAM J. Numer. 
Anal. 11 (1974), 197-209. 

18. PAIGE, C.C. Error analysis of the Lanczos algorithm for tridiagonalizing a symmetric matrix. J. 
Inst. Maths. Appl 18 (1976), 341-349. 

19 PAIGE, C.C., AND SAUNDERS, M.A. Solution of sparse indefinite system of equation and least- 
square problem Res. Rep. STAN-CS-73-399, Stanford Univ., Stanford, CA, 1973. 

20 PAIGE, C.C., AND SAUNDERS, M.A. Solution of sparse mdefinite system of linear equations. 
SIAM J. Numer. Anal. 12 (1975), 617-629. 

21 PAIGE, C.C., AND SAUNDERS, M.A. A bidiagonalization algorithm for sparse linear equation 
and least-squares problems. Rep. SOL 78-19, Dep. Operations Research, Stanford Univ., Stanford, 
CA, 1978. 

22. PAIGE, C.C., AND SAUNDERS, M.A. LSQR' Sparse linear equation and least-squares problems. 
ACM Trans. Math. Softw., to appear. 

23. STEWART, G.W. Research, development and LINPACK. In Rice, J.R. (Ed.), Mathematwal 
Software III, Academic Press, New York, 1977, pp. 1-14. 

24. VAN HEIJST, J., JACOBS, J., AND SCHERDERS, J. Kleinste-kwadraten problemen. Dep. Mathe- 
matics Rep., Eindhoven University of Technology, Eindhoven, The Netherlands, August 1976 

25. WILKINSON, J.H. The Algebraw Etgenvalue Problem. Oxford University Press (Clarendon), 
New York, 1965. 

Received June 1980; revise September 1981, accepted November 1981 

ACM Transactmns on Mathematical Software, Vol. 8, No. 1, March 1982 


