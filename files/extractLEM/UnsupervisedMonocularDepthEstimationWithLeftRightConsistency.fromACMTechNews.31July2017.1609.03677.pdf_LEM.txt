


















































Unsupervised Monocular Depth Estimation with Left-Right Consistency 

Clément Godard Oisin Mac Aodha Gabriel J. Brostow 
University College London 

http://visual.cs.ucl.ac.uk/pubs/monoDepth/ 

Abstract 

Learning base method have show very promising result 
for the task of depth estimation in single images. However, 
most exist approach treat depth prediction a a supervise 
regression problem and a a result, require vast quantity 
of correspond ground truth depth data for training. Just 
record quality depth data in a range of environment be a 
challenge problem. In this paper, we innovate beyond exist 
approaches, replace the use of explicit depth data during 
training with easier-to-obtain binocular stereo footage. 

We propose a novel training objective that enables our convo- 
lutional neural network to learn to perform single image depth 
estimation, despite the absence of ground truth depth data. Ex- 
ploiting epipolar geometry constraints, we generate disparity 
image by training our network with an image reconstruction 
loss. We show that solve for image reconstruction alone re- 
sults in poor quality depth images. To overcome this problem, 
we propose a novel training loss that enforces consistency be- 
tween the disparity produce relative to both the left and right 
images, lead to improve performance and robustness com- 
par to exist approaches. Our method produce state of the 
art result for monocular depth estimation on the KITTI drive 
dataset, even outperform supervise method that have be 
train with ground truth depth. 

1. Introduction 
Depth estimation from image have a long history in computer 

vision. Fruitful approach have rely on structure from motion, 
shape-from-X, binocular, and multi-view stereo. However, most 
of these technique rely on the assumption that multiple obser- 
vations of the scene of interest be available. These can come 
in the form of multiple viewpoints, or observation of the scene 
under different light conditions. To overcome this limitation, 
there have recently be a surge in the number of work that pose 
the task of monocular depth estimation a a supervise learn 
problem [32, 10, 36]. These method attempt to directly predict 
the depth of each pixel in an image use model that have be 
train offline on large collection of ground truth depth data. 
While these method have enjoy great success, to date they 

Figure 1. Our depth prediction result on KITTI 2015. Top to bottom: 
input image, ground truth disparities, and our result. Our method be 
able to estimate depth for thin structure such a street sign and poles. 

have be restrict to scene where large image collection 
and their correspond pixel depth be available. 

Understanding the shape of a scene from a single image, 
independent of it appearance, be a fundamental problem in 
machine perception. There be many application such a 
synthetic object insertion in computer graphic [29], synthetic 
depth of field in computational photography [3], grasp 
in robotics [34], use depth a a cue in human body pose 
estimation [48], robot assist surgery [49], and automatic 2D 
to 3D conversion in film [53]. Accurate depth data from one 
or more camera be also crucial for self-driving cars, where 
expensive laser-based system be often used. 

Humans perform well at monocular depth estimation by 
exploit cue such a perspective, scale relative to the 
know size of familiar objects, appearance in the form of 
light and shade and occlusion [24]. This combination of 
both top-down and bottom-up cue appear to link full scene 
understand with our ability to accurately estimate depth. In 
this work, we take an alternative approach and treat automatic 
depth estimation a an image reconstruction problem during 
training. Our fully convolutional model do not require any 
depth data, and be instead train to synthesize depth a an 
intermediate. It learns to predict the pixel-level correspondence 
between pair of rectify stereo image that have a know 
camera baseline. There be some exist method that also 
address the same problem, but with several limitations. For 
example they be not fully differentiable, make training 
suboptimal [16], or have image formation model that do 

1 

ar 
X 

iv 
:1 

60 
9. 

03 
67 

7v 
3 

[ 
c 

.C 
V 

] 
1 

2 
A 

pr 
2 

01 
7 



not scale to large output resolution [53]. We improve upon 
these method with a novel training objective and enhance 
network architecture that significantly increase the quality 
of our final results. An example result from our algorithm be 
illustrate in Fig. 1. Our method be fast and only take on the 
order of 35 millisecond to predict a dense depth map for a 
512×256 image on a modern GPU. Specifically, we propose 
the follow contributions: 
1) A network architecture that performs end-to-end unsuper- 
vised monocular depth estimation with a novel training loss that 
enforces left-right depth consistency inside the network. 
2) An evaluation of several training loss and image formation 
model highlight the effectiveness of our approach. 
3) In addition to show state of the art result on a challenge 
drive dataset, we also show that our model generalizes to three 
different datasets, include a new outdoor urban dataset that 
we have collect ourselves, which we make openly available. 

2. Related Work 
There be a large body of work that focus on depth 

estimation from images, either use pair [46], several 
overlap image capture from different viewpoint [14], 
temporal sequence [44], or assume a fix camera, static 
scene, and change light [52, 2]. These approach be 
typically only applicable when there be more than one input 
image available of the scene of interest. Here we focus on 
work related to monocular depth estimation, where there be 
only a single input image, and no assumption about the scene 
geometry or type of object present be made. 

Learning-Based Stereo 

The vast majority of stereo estimation algorithm have a data 
term which computes the similarity between each pixel in the 
first image and every other pixel in the second image. Typically 
the stereo pair be rectify and thus the problem of disparity (i.e. 
scale inverse depth) estimation can be pose a a 1D search 
problem for each pixel. Recently, it have be show that instead 
of use hand define similarity measures, treat the match 
a a supervise learn problem and training a function to 
predict the correspondence produce far superior result 
[54, 31]. It have also be show that pose this binocular 
correspondence search a a multi-class classification problem 
have advantage both in term of quality of result and speed 
[38]. Instead of just learn the match function, Mayer 
et al. [39] introduce a fully convolutional [47] deep network 
call DispNet that directly computes the correspondence 
field between two images. At training time, they attempt to 
directly predict the disparity for each pixel by minimize a 
regression training loss. DispNet have a similar architecture to 
their previous end-to-end deep optical flow network [12]. 

The above method rely on have large amount of accurate 
ground truth disparity data and stereo image pair at training 
time. This type of data can be difficult to obtain for real world 

scenes, so these approach typically use synthetic data for 
training. Synthetic data be become more realistic, e.g. [15], 
but still require the manual creation of new content for every 
new application scenario. 

Supervised Single Image Depth Estimation 

Single-view, or monocular, depth estimation refers to the 
problem setup where only a single image be available at test 
time. Saxena et al. [45] propose a patch-based model know 
a Make3D that first over-segments the input image into patch 
and then estimate the 3D location and orientation of local 
plane to explain each patch. The prediction of the plane 
parameter be make use a linear model train offline on 
a dataset of laser scans, and the prediction be then combine 
together use an MRF. The disadvantage of this method, and 
other planar base approximations, e.g. [22], be that they can 
have difficulty model thin structure and, a prediction 
be make locally, lack the global context require to generate 
realistic outputs. Instead of hand-tuning the unary and pairwise 
terms, Liu et al. [36] use a convolutional neural network (CNN) 
to learn them. In another local approach, Ladicky et al. [32] 
incorporate semantics into their model to improve their per 
pixel depth estimation. Karsch et al. [28] attempt to produce 
more consistent image level prediction by copying whole depth 
image from a training set. A drawback of this approach be that 
it require the entire training set to be available at test time. 

Eigen et al. [10, 9] show that it be possible to produce 
dense pixel depth estimate use a two scale deep network 
train on image and their correspond depth values. Unlike 
most other previous work in single image depth estimation, 
they do not rely on hand craft feature or an initial over- 
segmentation and instead learn a representation directly from 
the raw pixel values. Several work have built upon the success 
of this approach use technique such a CRFs to improve accu- 
racy [35], change the loss from regression to classification [5], 
use other more robust loss function [33], and incorporate 
strong scene prior in the case of the related problem of surface 
normal estimation [50]. Again, like the previous stereo methods, 
these approach rely on have high quality, pixel aligned, 
ground truth depth at training time. We too perform single 
depth image estimation, but train with an add binocular color 
image, instead of require ground truth depth. 

Unsupervised Depth Estimation 

Recently, a small number of deep network base method for 
novel view synthesis and depth estimation have be proposed, 
which do not require ground truth depth at training time. Flynn et 
al. [13] introduce a novel image synthesis network call Deep- 
Stereo that generates new view by select pixel from nearby 
images. During training, the relative pose of multiple camera 
be use to predict the appearance of a held-out nearby image. 
Then the most appropriate depth be select to sample color 
from the neighbor images, base on plane sweep volumes. 



At test time, image synthesis be perform on small overlap 
patches. As it require several nearby pose image at test time 
DeepStereo be not suitable for monocular depth estimation. 

The Deep3D network of Xie et al. [53] also address the 
problem of novel view synthesis, where their goal be to generate 
the correspond right view from an input left image (i.e. the 
source image) in the context of binocular pairs. Again use an 
image reconstruction loss, their method produce a distribution 
over all the possible disparity for each pixel. The result 
synthesize right image pixel value be a combination of the 
pixel on the same scan line from the left image, weight 
by the probability of each disparity. The disadvantage of 
their image formation model be that increase the number 
of candidate disparity value greatly increase the memory 
consumption of the algorithm, make it difficult to scale their 
approach to big output resolutions. In this work, we perform 
a comparison to the Deep3D image formation model, and show 
that our algorithm produce superior results. 

Closest to our model in spirit be the concurrent work of 
Garg et al. [16]. Like Deep3D and our method, they train 
a network for monocular depth estimation use an image 
reconstruction loss. However, their image formation model be 
not fully differentiable. To compensate, they perform a Taylor 
approximation to linearize their loss result in an objective 
that be more challenge to optimize. Similar to other recent 
work, e.g. [43, 56, 57], our model overcomes this problem by 
use bilinear sample [27] to generate images, result in 
a fully (sub-)differentiable training loss. 

We propose a fully convolutional deep neural network 
loosely inspire by the supervise DispNet architecture of 
Mayer et al. [39]. By pose monocular depth estimation 
a an image reconstruction problem, we can solve for the 
disparity field without require ground truth depth. However, 
only minimize a photometric loss can result in good quality 
image reconstruction but poor quality depth. Among other 
terms, our fully differentiable training loss include a left-right 
consistency check to improve the quality of our synthesize 
depth images. This type of consistency check be commonly 
use a a post-processing step in many stereo methods, e.g. 
[54], but we incorporate it directly into our network. 

3. Method 
This section describes our single image depth prediction 

network. We introduce a novel depth estimation training loss, 
feature an inbuilt left-right consistency check, which enables 
u to train on image pair without require supervision in the 
form of ground truth depth. 

3.1. Depth Estimation a Image Reconstruction 

Given a single image I at test time, our goal be to learn a 
function f that can predict the per-pixel scene depth, d̂=f(I). 
Most exist learn base approach treat this a a 
supervise learn problem, where they have color input 

H x W x D 

2H x 2W x D/2 

UC 

C 

C 

S 

S 

S 

S 

US 

US 

C 

SC 

Figure 2. Our loss module output left and right disparity maps, dl 

and dr. The loss combine smoothness, reconstruction, and left-right 
disparity consistency terms. This same module be repeat at each of 
the four different output scales. C: Convolution, UC: Up-Convolution, 
S: Bilinear Sampling, US: Up-Sampling, SC: Skip Connection. 

image and their correspond target depth value at training. It 
be presently not practical to acquire such ground truth depth data 
for a large variety of scenes. Even expensive hardware, such 
a laser scanners, can be imprecise in natural scene feature 
movement and reflections. As an alternative, we instead pose 
depth estimation a an image reconstruction problem during 
training. The intuition here be that, give a calibrate pair of 
binocular cameras, if we can learn a function that be able to 
reconstruct one image from the other, then we have learn 
something about the 3D shape of the scene that be be imaged. 

Specifically, at training time, we have access to two image 
Il and Ir, correspond to the left and right color image from 
a calibrate stereo pair, capture at the same moment in time. 
Instead of try to directly predict the depth, we attempt to 
find the dense correspondence field dr that, when apply to the 
left image, would enable u to reconstruct the right image. We 
will refer to the reconstruct image Il(dr) a Ĩr. Similarly, we 
can also estimate the left image give the right one, Ĩl=Ir(dl). 
Assuming that the image be rectify [19], d corresponds to 
the image disparity - a scalar value per pixel that our model 
will learn to predict. Given the baseline distance b between the 
camera and the camera focal length f , we can then trivially 
recover the depth d̂ from the predict disparity, d̂=bf/d. 

3.2. Depth Estimation Network 

At a high level, our network estimate depth by infer 
the disparity that warp the left image to match the right one. 
The key insight of our method be that we can simultaneously 
infer both disparity (left-to-right and right-to-left), use only 
the left input image, and obtain good depth by enforce them 
to be consistent with each other. 

Our network generates the predict image with backward 
mapping use a bilinear sampler, result in a fully differen- 
tiable image formation model. As illustrate in Fig. 3, naı̈vely 
learn to generate the right image by sample from the left 



Target 

Output 

Disparity 

Sampler 

CNN 

Naïve 

Input 

No LR Ours 

Figure 3. Sampling strategy for backward mapping. With naı̈ve 
sample the CNN produce a disparity map align with the target 
instead of the input. No LR corrects for this, but suffers from artifacts. 
Our approach us the left image to produce disparity for both 
images, improve quality by enforce mutual consistency. 

one will produce disparity align with the right image (target). 
However, we want the output disparity map to align with the 
input left image, meaning the network have to sample from the 
right image. We could instead train the network to generate the 
left view by sample from the right image, thus create a left 
view align disparity map (No LR in Fig. 3). While this alone 
works, the infer disparity exhibit ‘texture-copy’ artifact and 
error at depth discontinuity a see in Fig. 5. We solve this by 
training the network to predict the disparity map for both view 
by sample from the opposite input images. This still only 
require a single left image a input to the convolutional layer 
and the right image be only use during training (Ours in Fig. 3). 
Enforcing consistency between both disparity map use this 
novel left-right consistency cost lead to more accurate results. 

Our fully convolutional architecture be inspire by Disp- 
Net [39], but feature several important modification that 
enable u to train without require ground truth depth. Our net- 
work, be compose of two main part - an encoder (from cnv1 to 
cnv7b) and decoder (from upcnv7), please see the supplementary 
material for a detailed description. The decoder us skip con- 
nections [47] from the encoder’s activation blocks, enable it to 
resolve high resolution details. We output disparity prediction 
at four different scale (disp4 to disp1), which double in spatial 
resolution at each of the subsequent scales. Even though it only 
take a single image a input, our network predicts two disparity 
map at each output scale - left-to-right and right-to-left. 

3.3. Training Loss 

We define a loss Cs at each output scale s, form the 
total loss a the sum C= 

∑4 
s=1Cs. Our loss module (Fig. 2) 

computes Cs a a combination of three main terms, 

Cs=αap(C 
l 
ap+C 

r 
ap)+αds(C 

l 
ds+C 

r 
ds)+αlr(C 

l 
lr+C 

r 
lr), 

(1) 
where Cap encourages the reconstruct image to appear 
similar to the correspond training input, Cds enforces 

smooth disparities, and Clr prefers the predict left and right 
disparity to be consistent. Each of the main term contains 
both a left and a right image variant, but only the left image 
be fed through the convolutional layers. 

Next, we present each component of our loss in term of the 
left image (e.g.Clap). The right image versions, e.g.C 

r 
ap, require 

to swap left for right and to sample in the opposite direction. 

Appearance Matching Loss During training, the network 
learns to generate an image by sample pixel from the 
opposite stereo image. Our image formation model us the 
image sampler from the spatial transformer network (STN) [27] 
to sample the input image use a disparity map. The STN us 
bilinear sample where the output pixel be the weight sum of 
four input pixels. In contrast to alternative approach [16, 53], 
the bilinear sampler use be locally fully differentiable and 
integrates seamlessly into our fully convolutional architecture. 
This mean that we do not require any simplification or 
approximation of our cost function. 

Inspired by [55], we use a combination of an L1 and 
single scale SSIM [51] term a our photometric image 
reconstruction cost Cap, which compare the input image Ilij 
and it reconstruction Ĩlij, whereN be the number of pixels, 

Clap= 
1 

N 

∑ 

i,j 

α 
1−SSIM(Ilij,Ĩlij) 

2 
+(1−α) 

∥∥∥Ilij−Ĩlij 
∥∥∥. (2) 

Here, we use a simplify SSIM with a 3×3 block filter instead 
of a Gaussian, and set α=0.85. 

Disparity Smoothness Loss We encourage disparity to be 
locally smooth with an L1 penalty on the disparity gradient ∂d. 
As depth discontinuity often occur at image gradients, similar 
to [21], we weight this cost with an edge-aware term use the 
image gradient ∂I, 

Clds= 
1 

N 

∑ 

i,j 

∣∣∂xdlij 
∣∣e−‖∂xIlij‖+ 

∣∣∂ydlij 
∣∣e−‖∂yIlij‖. (3) 

Left-Right Disparity Consistency Loss To produce more 
accurate disparity maps, we train our network to predict both 
the left and right image disparities, while only be give 
the left view a input to the convolutional part of the network. 
To ensure coherence, we introduce an L1 left-right disparity 
consistency penalty a part of our model. This cost attempt 
to make the left-view disparity map be equal to the project 
right-view disparity map, 

Cllr= 
1 

N 

∑ 

i,j 

∣∣∣dlij−drij+dlij 
∣∣∣. (4) 

Like all the other terms, this cost be mirror for the right-view 
disparity map and be evaluate at all of the output scales. 



Method Dataset Abs Rel Sq Rel RMSE RMSE log D1-all δ<1.25 δ<1.252 δ<1.253 

Ours with Deep3D [53] K 0.412 16.37 13.693 0.512 66.85 0.690 0.833 0.891 
Ours with Deep3Ds [53] K 0.151 1.312 6.344 0.239 59.64 0.781 0.931 0.976 
Ours No LR K 0.123 1.417 6.315 0.220 30.318 0.841 0.937 0.973 
Ours K 0.124 1.388 6.125 0.217 30.272 0.841 0.936 0.975 
Ours CS 0.699 10.060 14.445 0.542 94.757 0.053 0.326 0.862 
Ours CS + K 0.104 1.070 5.417 0.188 25.523 0.875 0.956 0.983 
Ours pp CS + K 0.100 0.934 5.141 0.178 25.077 0.878 0.961 0.986 
Ours resnet pp CS + K 0.097 0.896 5.093 0.176 23.811 0.879 0.962 0.986 
Ours Stereo K 0.068 0.835 4.392 0.146 9.194 0.942 0.978 0.989 

Lower be good 

Higher be good 

Table 1. Comparison of different image formation models. Results on the KITTI 2015 stereo 200 training set disparity image [17]. For training, 
K be the KITTI dataset [17] and CS be Cityscapes [8]. Our model with left-right consistency performs the best, and be further improve with the 
addition of the Cityscapes data. The last row show the result of our model train and test with two input image instead of one (see Sec. 4.3). 

At test time, our network predicts the disparity at the fine 
scale level for the left image dl, which have the same resolution 
a the input image. Using the know camera baseline and focal 
length from the training set, we then convert from the disparity 
map to a depth map. While we also estimate the right disparity 
dr during training, it be not use at test time. 

4. Results 
Here we compare the performance of our approach to both 

supervise and unsupervised single view depth estimation 
methods. We train on rectify stereo image pairs, and do 
not require any supervision in the form of ground truth depth. 
Existing single image datasets, such a [41, 45], that lack 
stereo pairs, be not suitable for evaluation. Instead we evaluate 
our approach use the popular KITTI 2015 [17] dataset. To 
evaluate our image formation model, we compare to a variant 
of our algorithm that us the original Deep3D [53] image 
formation model and a modify one, Deep3Ds, with an add 
smoothness constraint. We also evaluate our approach with and 
without the left-right consistency constraint. 

4.1. Implementation Details 

The network which be implement in TensorFlow [1] con- 
tains 31 million trainable parameters, and take on the order of 
25 hour to train use a single Titan X GPU on a dataset of 30 
thousand image for 50 epochs. Inference be fast and take less 
than 35 ms, or more than 28 frame per second, for a 512×256 
image, include transfer time to and from the GPU. Please 
see the supplementary material and our code1 for more details. 

During optimization, we set the weight of the different 
loss component to αap=1 and αlr=1. The possible output 
disparity be constrain to be between 0 and dmax use a 
scale sigmoid non-linearity, where dmax = 0.3× the image 
width at a give output scale. As a result of our multi-scale 
output, the typical disparity of neighbor pixel will differ 
by a factor of two between each scale (as we be upsampling 
the output by a factor of two). To correct for this, we scale the 
disparity smoothness termαds with r for each scale to get equiv- 
alent smooth at each level. Thus αds=0.1/r, where r be the 

1Available at https://github.com/mrharicot/monodepth 

downscaling factor of the correspond layer with respect to 
the resolution of the input image that be pass into the network. 

For the non-linearities in the network, we use exponential 
linear unit [7] instead of the commonly use rectify liner unit 
(ReLU) [40]. We found that ReLUs tend to prematurely fix 
the predict disparity at intermediate scale to a single value, 
make subsequent improvement difficult. Following [42], 
we replace the usual deconvolutions with a near neighbor 
upsampling follow by a convolutions. We train our model 
from scratch for 50 epochs, with a batch size of 8 use Adam 
[30], where β1 =0.9, β2 =0.999, and �=10−8. We use an 
initial learn rate of λ=10−4 which we kept constant for the 
first 30 epoch before halve it every 10 epoch until the end. 
We initially experiment with progressive update schedules, 
a in [39], where low resolution image scale be optimize 
first. However, we found that optimize all four scale at once 
lead to more stable convergence. Similarly, we use an identical 
weight for the loss of each scale a we found that weight 
them differently lead to unstable convergence. We experiment 
with batch normalization [26], but found that it do not produce 
a significant improvement, and ultimately exclude it. 

Data augmentation be perform on the fly. We flip the input 
image horizontally with a 50% chance, take care to also 
swap both image so they be in the correct position relative 
to each other. We also add color augmentations, with a 
50% chance, where we perform random gamma, brightness, 
and color shift by sample from uniform distribution in 
the range [0.8,1.2] for gamma, [0.5,2.0] for brightness, and 
[0.8,1.2] for each color channel separately. 

Resnet50 For the sake of completeness, and similar to [33], 
we also show a variant of our model use Resnet50 [20] a 
the encoder, the rest of the architecture, parameter and training 
procedure stay identical. This variant contains 48 million 
trainable parameter and be indicate by resnet in result tables. 

Post-processing In order to reduce the effect of stereo disoc- 
clusions which create disparity ramp on both the left side of the 
image and of the occluders, a final post-processing step be per- 
form on the output. For an input image I at test time, we also 

https://github.com/mrharicot/monodepth 


Input GT Eigen et al. [10] Liu et al. [36] Garg et al. [16] Ours 

Figure 4. Qualitative result on the KITTI Eigen Split. The ground truth velodyne depth be very sparse, we interpolate it for visualization 
purposes. Our method do good at resolve small object such a the pedestrian and poles. 

compute the disparity map d′l for it horizontally flip image 
I′. By flip back this disparity map we obtain a disparity 
map d′′l , which aligns with dl but where the disparity ramp be 
locate on the right of occluders a well a on the right side of the 
image. We combine both disparity map to form the final result 
by assign the first 5% on the left of the image use d′′l and 
the last 5% on the right to the disparity from dl. The central 
part of the final disparity map be the average of dl and d′l. This 
final post-processing step lead to both good accuracy and less 
visual artifact at the expense of double the amount of test time 
computation. We indicate such result use pp in result tables. 

4.2. KITTI 

We present result for the KITTI dataset [17] use two 
different test splits, to enable comparison to exist works. In it 
raw form, the dataset contains 42,382 rectify stereo pair from 
61 scenes, with a typical image be 1242×375 pixel in size. 

KITTI Split First we compare different variant of our 
method include different image formation model and differ- 
ent training sets. We evaluate on the 200 high quality disparity 
image provide a part of the official KITTI training set, which 
cover a total of 28 scenes. The remain 33 scene contain 
30,159 image from which we keep 29,000 for training and the 
rest for evaluation. While these disparity image be much good 
quality than the reprojected velodyne laser depth values, they 
have CAD model insert in place of move cars. These CAD 
model result in ambiguous disparity value on transparent 
surface such a car windows, and issue at object boundary 
where the CAD model do not perfectly align with the images. 
In addition, the maximum depth present in the KITTI dataset be 
on the order of 80 meters, and we cap the maximum prediction 
of all network to this value. Results be compute use the 
depth metric from [10] along with the D1-all disparity error 
from KITTI [17]. The metric from [10] measure error in both 
meter from the ground truth and the percentage of depth that 
be within some threshold from the correct value. It be important 
to note that measure the error in depth space while the ground 
truth be give in disparity lead to precision issues. In particular, 
the non-thresholded measure can be sensitive to the large error 
in depth cause by prediction error at small disparity values. 

In Table 1, we see that in addition to have poor scale prop- 

erties (in term of both resolution and the number of disparity it 
can represent), when train from scratch with the same network 
architecture a ours, the Deep3D [53] image formation model 
performs poorly. From Fig. 6 we can see that Deep3D produce 
plausible image reconstruction but the output disparity be in- 
ferior to ours. Our loss outperforms both the Deep3D baseline 
and the addition of the left-right consistency check increase per- 
formance in all measures. In Fig. 5 we illustrate some zoom 
in comparisons, clearly show that the inclusion of the left- 
right check improves the visual quality of the results. Our result 
be further improve by first pre-training our model with addi- 
tional training data from the Cityscapes dataset [8] contain 
22,973 training stereo pair capture in various city across Ger- 
many. This dataset brings high resolution, image quality, and 
variety compare to KITTI, while have a similar setting. We 
cropped the input image to only keep the top 80% of the image, 
remove the very reflective car hood from the input. Interest- 
ingly, our model train on Cityscapes alone do not perform 
very well numerically. This be likely due to the difference in 
camera calibration between the two datasets, but there be a clear 
advantage to fine-tuning on data that be related to the test set. 

Eigen Split To be able to compare to exist work, we also 
use the test split of 697 image a propose by [10] which 
cover a total of 29 scenes. The remain 32 scene contain 
23,488 image from which we keep 22,600 for training and the 
rest for evaluation, similarly to [16]. To generate the ground 
truth depth images, we reproject the 3D point view from the 
velodyne laser into the left input color camera. Aside from only 
produce depth value for less than 5% of the pixel in the 
input image, error be also introduce because of the rotation 

Ours NoLR Ours NoLR Ours NoLR 

Ours 

NoLR 

Figure 5. Comparison between our method with and without the left- 
right consistency. Our consistency term produce superior result on 
the object boundaries. Both result be show without post-processing. 



Method Supervised Dataset Abs Rel Sq Rel RMSE RMSE log δ<1.25 δ<1.252 δ<1.253 

Train set mean No K 0.361 4.826 8.102 0.377 0.638 0.804 0.894 
Eigen et al. [10] Coarse ◦ Yes K 0.214 1.605 6.563 0.292 0.673 0.884 0.957 
Eigen et al. [10] Fine ◦ Yes K 0.203 1.548 6.307 0.282 0.702 0.890 0.958 
Liu et al. [36] DCNF-FCSP FT * Yes K 0.201 1.584 6.471 0.273 0.68 0.898 0.967 
Ours No LR No K 0.152 1.528 6.098 0.252 0.801 0.922 0.963 
Ours No K 0.148 1.344 5.927 0.247 0.803 0.922 0.964 
Ours No CS + K 0.124 1.076 5.311 0.219 0.847 0.942 0.973 
Ours pp No CS + K 0.118 0.923 5.015 0.210 0.854 0.947 0.976 
Ours resnet pp No CS + K 0.114 0.898 4.935 0.206 0.861 0.949 0.976 
Garg et al. [16] L12 Aug 8× cap 50m No K 0.169 1.080 5.104 0.273 0.740 0.904 0.962 
Ours cap 50m No K 0.140 0.976 4.471 0.232 0.818 0.931 0.969 
Ours cap 50m No CS + K 0.117 0.762 3.972 0.206 0.860 0.948 0.976 
Ours pp cap 50m No CS + K 0.112 0.680 3.810 0.198 0.866 0.953 0.979 
Ours resnet pp cap 50m No CS + K 0.108 0.657 3.729 0.194 0.873 0.954 0.979 
Our pp uncropped No CS + K 0.134 1.261 5.336 0.230 0.835 0.938 0.971 
Ours resnet pp uncropped No CS + K 0.130 1.197 5.222 0.226 0.843 0.940 0.971 

Lower be good 

Higher be good 

Table 2. Results on KITTI 2015 [17] use the split of Eigen et al. [10]. For training, K be the KITTI dataset [17] and CS be Cityscapes [8]. The 
prediction of Liu et al. [36]* be generate on a mix of the left and right image instead of just the left input images. For a fair comparison, we 
compute their result relative to the correct image. As in the provide source code, Eigen et al. [10]◦ result be compute relative to the velodyne 
instead of the camera. Garg et al. [16] result be take directly from their paper. All results, except [10], use the crop from [16]. We also show 
our result with the same crop and maximum evaluation distance. The last two row be compute on the uncropped ground truth. 

In 
pu 

t 

D 
ee 

p3 
D 

D 
ee 

p3 
D 

s 
O 

ur 
s 

Reconstruction error (x2) Disparities 
Figure 6. Image reconstruction error on KITTI. While all method 
output plausible right views, the Deep3D image formation model 
without smoothness constraint do not produce valid disparities. 

of the Velodyne, the motion of the vehicle and surround 
objects, and also incorrect depth reading due to occlusion at 
object boundaries. To be fair to all methods, we use the same 
crop a [10] and evaluate at the input image resolution. With the 
exception of Garg et al.’s [16] results, the result of the baseline 
method be recomputed by u give the authors’s original 
prediction to ensure that all the score be directly comparable. 
This produce slightly different number than the previously 
publish ones, e.g. in the case of [10], their prediction be 
evaluate on much small depth image (1/4 the original size). 
For all baseline method we use bilinear interpolation to resize 
the prediction to the correct input image size. 

Table 2 show quantitative result with some example 
output show in Fig. 4. We see that our algorithm outperforms 
all other exist methods, include those that be train with 
ground truth depth data. We again see that pre-training on the 
Cityscapes dataset improves the result over use KITTI alone. 

4.3. Stereo 

We also implement a stereo version of our model, see 
Fig. 8, where the network’s input be the concatenation of both 
left and right views. Perhaps unsurprisingly, the stereo model 
outperforms our monocular network on every single metric, 
especially on the D1-all disparity measure, a can be see 
in Table 1. This model be only train for 12 epoch a it 
becomes unstable if train for longer. 

4.4. Make3D 

To illustrate that our method can generalize to other datasets, 
here we compare to several fully supervise method on the 
Make3D test set of [45]. Make3D consists of only RGB/Depth 
pair and no stereo images, thus our method cannot train on this 
data. We use our network train only on the Cityscapes dataset 
and despite the dissimilarity in the datasets, both in content 
and camera parameters, we still achieve reasonable results, 
even beating [28] on one metric and [37] on three. Due to the 
different aspect ratio of the Make3D dataset we evaluate on a 
central crop of the images. In Table 3, we compare our output to 
the similarly cropped result of the other methods. As in the case 
of the KITTI dataset, these result would likely be improve 
with more relevant training data. A qualitative comparison to 
some of the related method be show in Fig. 7. While our 
numerical result be not a good a the baselines, qualitatively, 
we compare favorably to the supervise competition. 

4.5. Generalizing to Other Datasets 

Finally, we illustrate some further example of our model 
generalize to other datasets in Figure 9. Using the model only 
train on Cityscapes [8], we test on the CamVid drive 
dataset [4]. In the accompany video and the supplementary 
material we can see that despite the difference in location, 
image characteristics, and camera calibration, our model still 



Input GT Karsch et al. [28] Liu et al. [37] Laina et al. [33] Ours 

Figure 7. Our method achieves superior qualitative result on Make3D despite be train on a different dataset (Cityscapes). 

In 
pu 

tl 
ef 

t 
St 

er 
eo 

M 
on 

o 

Figure 8. Our stereo results. While the stereo disparity map contains 
more detail, our monocular result be comparable. 

Method Sq Rel Abs Rel RMSE log10 
Train set mean* 15.517 0.893 11.542 0.223 
Karsch et al. [28]* 4.894 0.417 8.172 0.144 
Liu et al. [37]* 6.625 0.462 9.972 0.161 
Laina et al. [33] berHu* 1.665 0.198 5.461 0.082 
Ours with Deep3D [53] 17.18 1.000 19.11 2.527 
Ours 11.990 0.535 11.513 0.156 
Ours pp 7.112 0.443 8.860 0.142 

Table 3. Results on the Make3D dataset [45]. All method marked 
with an * be supervise and use ground truth depth data from the 
Make3D training set. Using the standard C1 metric, error be only 
compute where depth be less than 70 meter in a central image crop. 

produce visually plausible depths. We also capture a 60,000 
frame dataset, at 10 frame per second, take in an urban 
environment with a wide angle consumer 1080p stereo camera. 
Finetuning the Cityscapes pre-trained model on this dataset 
produce visually convincing depth image for a test set that 
be capture with the same camera on a different day, please 
see the video in the supplementary material for more results. 

Figure 9. Qualitative result on Cityscapes, CamVid, and our own 
urban dataset capture on foot. For more result please see our video. 

4.6. Limitations 

Even though both our left-right consistency check and post- 
processing improve the quality of the results, there be still some 
artifact visible at occlusion boundary due to the pixel in the 
occlusion region not be visible in both images. Explicitly rea- 
soning about occlusion during training [23, 25] could improve 
these issues. It be worth note that depend how large the base- 
line between the camera and the depth sensor, fully supervise 
approach also do not always have valid depth for all pixels. 

Our method require rectify and temporally align 
stereo pair during training, which mean that it be currently 
not possible to use exist single-view datasets for training 
purpose e.g. [41]. However, it be possible to fine-tune our 
model on application specific ground truth depth data. 

Finally, our method mainly relies on the image reconstruc- 
tion term, meaning that specular [18] and transparent surface 
will produce inconsistent depths. This could be improve with 
more sophisticated similarity measure [54]. 

5. Conclusion 
We have present an unsupervised deep neural network for 

single image depth estimation. Instead of use align ground 
truth depth data, which be both rare and costly, we exploit the 
ease with which binocular stereo data can be captured. Our 
novel loss function enforces consistency between the predict 
depth map from each camera view during training, improve 
predictions. Our result be superior to fully supervise 
baselines, which be encourage for future research that do 
not require expensive to capture ground truth depth. We have 
also show that our model can generalize to unseen datasets 
and still produce visually plausible depth maps. 

In future work, we would like to extend our model to 
videos. While our current depth estimate be perform 
independently per frame, add temporal consistency [28] 
would likely improve results. It would also be interest to 
investigate sparse input a an alternative training signal [58, 6]. 
Finally, while our model estimate per pixel depth, it would be 
interest to also predict the full occupancy of the scene [11]. 

Acknowledgments We would like to thank David Eigen, Ravi Garg, 
Iro Laina and Fayao Liu for provide data and code to recreate the 
baseline algorithms. We also thank Stephan Garbin for his lua skill and 
Peter Hedman for his LATEX magic. We be grateful for EPSRC funding 
for the EngD Centre EP/G037159/1, and for project EP/K015664/1 
and EP/K023578/1. 



References 
[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, 

G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. Tensorflow: 
Large-scale machine learn on heterogeneous distribute 
systems. arXiv preprint arXiv:1603.04467, 2016. 5 

[2] A. Abrams, C. Hawley, and R. Pless. Heliometric stereo: Shape 
from sun position. In ECCV, 2012. 2 

[3] J. T. Barron, A. Adams, Y. Shih, and C. Hernández. Fast 
bilateral-space stereo for synthetic defocus. CVPR, 2015. 1 

[4] G. J. Brostow, J. Fauqueur, and R. Cipolla. Semantic object 
class in video: A high-definition ground truth database. Pattern 
Recognition Letters, 2009. 7 

[5] Y. Cao, Z. Wu, and C. Shen. Estimating depth from monocular 
image a classification use deep fully convolutional residual 
networks. arXiv preprint arXiv:1605.02305, 2016. 2 

[6] W. Chen, Z. Fu, D. Yang, and J. Deng. Single-image depth 
perception in the wild. In NIPS, 2016. 8 

[7] D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and 
accurate deep network learn by exponential linear unit (elus). 
arXiv preprint arXiv:1511.07289, 2015. 5 

[8] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be- 
nenson, U. Franke, S. Roth, and B. Schiele. The cityscape dataset 
for semantic urban scene understanding. In CVPR, 2016. 5, 6, 7 

[9] D. Eigen and R. Fergus. Predicting depth, surface normal 
and semantic label with a common multi-scale convolutional 
architecture. In ICCV, 2015. 2 

[10] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from 
a single image use a multi-scale deep network. In NIPS, 2014. 
1, 2, 6, 7 

[11] M. Firman, O. Mac Aodha, S. Julier, and G. J. Brostow. 
Structured Prediction of Unobserved Voxels From a Single Depth 
Image. In CVPR, 2016. 8 

[12] P. Fischer, A. Dosovitskiy, E. Ilg, P. Häusser, C. Hazırbaş, 
V. Golkov, P. van der Smagt, D. Cremers, and T. Brox. Flownet: 
Learning optical flow with convolutional networks. In ICCV, 
2015. 2 

[13] J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Deepstereo: 
Learning to predict new view from the world’s imagery. In 
CVPR, 2016. 2 

[14] Y. Furukawa and C. Hernández. Multi-view stereo: A tutorial. 
Foundations and Trends in Computer Graphics and Vision, 2015. 
2 

[15] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig. Virtual world a 
proxy for multi-object track analysis. In CVPR, 2016. 2 

[16] R. Garg, V. Kumar BG, and I. Reid. Unsupervised CNN for 
single view depth estimation: Geometry to the rescue. In ECCV, 
2016. 1, 3, 4, 6, 7 

[17] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous 
driving? the kitti vision benchmark suite. In CVPR, 2012. 5, 6, 7 

[18] C. Godard, P. Hedman, W. Li, and G. J. Brostow. Multi-view 
reconstruction of highly specular surface in uncontrolled 
environments. In 3DV, 2015. 8 

[19] R. Hartley and A. Zisserman. Multiple view geometry in 
computer vision. Cambridge university press, 2003. 3 

[20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn for 
image recognition. In CVPR, 2016. 5 

[21] P. Heise, S. Klose, B. Jensen, and A. Knoll. Pm-huber: 
Patchmatch with huber regularization for stereo matching. In 
ICCV, 2013. 4 

[22] D. Hoiem, A. A. Efros, and M. Hebert. Automatic photo pop-up. 
TOG, 2005. 2 

[23] D. Hoiem, A. N. Stein, A. A. Efros, and M. Hebert. Recovering 
occlusion boundary from a single image. In ICCV, 2007. 8 

[24] I. P. Howard. Perceiving in depth, volume 1: basic mechanisms. 
Oxford University Press, 2012. 1 

[25] A. Humayun, O. Mac Aodha, and G. J. Brostow. Learning to 
Find Occlusion Regions. In CVPR, 2011. 8 

[26] S. Ioffe and C. Szegedy. Batch normalization: Accelerating 
deep network training by reduce internal covariate shift. arXiv 
preprint arXiv:1502.03167, 2015. 5 

[27] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu. 
Spatial transformer networks. In NIPS, 2015. 3, 4 

[28] K. Karsch, C. Liu, and S. B. Kang. Depth transfer: Depth 
extraction from video use non-parametric sampling. PAMI, 
2014. 2, 7, 8 

[29] K. Karsch, K. Sunkavalli, S. Hadap, N. Carr, H. Jin, R. Fonte, 
M. Sittig, and D. Forsyth. Automatic scene inference for 3d 
object compositing. TOG, 2014. 1 

[30] D. Kingma and J. Ba. Adam: A method for stochastic 
optimization. arXiv preprint arXiv:1412.6980, 2014. 5 

[31] L. Ladickỳ, C. Häne, and M. Pollefeys. Learning the match 
function. arXiv preprint arXiv:1502.00652, 2015. 2 

[32] L. Ladickỳ, J. Shi, and M. Pollefeys. Pulling thing out of 
perspective. In CVPR, 2014. 1, 2 

[33] I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab. 
Deeper depth prediction with fully convolutional residual 
networks. In 3DV, 2016. 2, 5, 8 

[34] I. Lenz, H. Lee, and A. Saxena. Deep learn for detect 
robotic grasps. The International Journal of Robotics Research, 
2015. 1 

[35] B. Li, C. Shen, Y. Dai, A. van den Hengel, and M. He. Depth and 
surface normal estimation from monocular image use regres- 
sion on deep feature and hierarchical crfs. In CVPR, 2015. 2 

[36] F. Liu, C. Shen, G. Lin, and I. Reid. Learning depth from single 
monocular image use deep convolutional neural fields. PAMI, 
2015. 1, 2, 6, 7 

[37] M. Liu, M. Salzmann, and X. He. Discrete-continuous depth 
estimation from a single image. In CVPR, 2014. 7, 8 

[38] W. Luo, A. Schwing, and R. Urtasun. Efficient deep learn 
for stereo matching. In CVPR, 2016. 2 

[39] N. Mayer, E. Ilg, P. Häusser, P. Fischer, D. Cremers, A. Dosovit- 
skiy, and T. Brox. A large dataset to train convolutional network 
for disparity, optical flow, and scene flow estimation. In CVPR, 
2016. 2, 3, 4, 5 

[40] V. Nair and G. E. Hinton. Rectified linear unit improve restrict 
boltzmann machines. In ICML, 2010. 5 

[41] P. K. Nathan Silberman, Derek Hoiem and R. Fergus. Indoor 
segmentation and support inference from rgbd images. In ECCV, 
2012. 5, 8 

[42] A. Odena, V. Dumoulin, and C. Olah. Deconvolution and 
checkerboard artifacts. Distill, 2016. 5 



[43] V. Patraucean, A. Handa, and R. Cipolla. Spatio-temporal 
video autoencoder with differentiable memory. arXiv preprint 
arXiv:1511.06309, 2015. 3 

[44] R. Ranftl, V. Vineet, Q. Chen, and V. Koltun. Dense monocular 
depth estimation in complex dynamic scenes. In CVPR, 2016. 2 

[45] A. Saxena, M. Sun, and A. Y. Ng. Make3d: Learning 3d scene 
structure from a single still image. PAMI, 2009. 2, 5, 7, 8 

[46] D. Scharstein and R. Szeliski. A taxonomy and evaluation of 
dense two-frame stereo correspondence algorithms. IJCV, 2002. 
2 

[47] E. Shelhamer, J. Long, and T. Darrell. Fully convolutional 
network for semantic segmentation. PAMI, 2016. 2, 4 

[48] J. Shotton, T. Sharp, A. Kipman, A. Fitzgibbon, M. Finocchio, 
A. Blake, M. Cook, and R. Moore. Real-time human pose 
recognition in part from single depth images. Communications 
of the ACM, 2013. 1 

[49] D. Stoyanov, M. V. Scarzanella, P. Pratt, and G.-Z. Yang. 
Real-time stereo reconstruction in robotically assist minimally 
invasive surgery. In MICCAI, 2010. 1 

[50] X. Wang, D. Fouhey, and A. Gupta. Designing deep network 
for surface normal estimation. In CVPR, 2015. 2 

[51] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image 
quality assessment: from error visibility to structural similarity. 
Transactions on Image Processing, 2004. 4 

[52] R. J. Woodham. Photometric method for determine surface 
orientation from multiple images. Optical engineering, 1980. 2 

[53] J. Xie, R. Girshick, and A. Farhadi. Deep3d: Fully automatic 
2d-to-3d video conversion with deep convolutional neural 
networks. In ECCV, 2016. 1, 2, 3, 4, 5, 6, 8 

[54] J. Žbontar and Y. LeCun. Stereo match by training a 
convolutional neural network to compare image patches. JMLR, 
2016. 2, 3, 8 

[55] H. Zhao, O. Gallo, I. Frosio, and J. Kautz. Is l2 a good loss 
function for neural network for image processing? arXiv 
preprint arXiv:1511.08861, 2015. 4 

[56] T. Zhou, P. Krähenbühl, M. Aubry, Q. Huang, and A. A. Efros. 
Learning dense correspondence via 3d-guided cycle consistency. 
CVPR, 2016. 3 

[57] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros. View 
synthesis by appearance flow. In ECCV, 2016. 3 

[58] D. Zoran, P. Isola, D. Krishnan, and W. T. Freeman. Learning 
ordinal relationship for mid-level vision. In ICCV, 2015. 8 



Unsupervised Monocular Depth Estimation with Left-Right Consistency 

Supplementary Material 

1. Model architecture 

“Encoder” 
layer k s chns in out input 
conv1 7 2 3/32 1 2 left 
conv1b 7 1 32/32 2 2 conv1 
conv2 5 2 32/64 2 4 conv1b 
conv2b 5 1 64/64 4 4 conv2 
conv3 3 2 64/128 4 8 conv2b 
conv3b 3 1 128/128 8 8 conv3 
conv4 3 2 128/256 8 16 conv3b 
conv4b 3 1 256/256 16 16 conv4 
conv5 3 2 256/512 16 32 conv4b 
conv5b 3 1 512/512 32 32 conv5 
conv6 3 2 512/512 32 64 conv5b 
conv6b 3 1 512/512 64 64 conv6 
conv7 3 2 512/512 64 128 conv6b 
conv7b 3 1 512/512 128 128 conv7 

“Decoder” 
layer k s chns in out input 
upconv7 3 2 512/512 128 64 conv7b 
iconv7 3 1 1024/512 64 64 upconv7+conv6b 
upconv6 3 2 512/512 64 32 iconv7 
iconv6 3 1 1024/512 32 32 upconv6+conv5b 
upconv5 3 2 512/256 32 16 iconv6 
iconv5 3 1 512/256 16 16 upconv5+conv4b 
upconv4 3 2 256/128 16 8 iconv5 
iconv4 3 1 128/128 8 8 upconv4+conv3b 
disp4 3 1 128/2 8 8 iconv4 
upconv3 3 2 128/64 8 4 iconv4 
iconv3 3 1 130/64 4 4 upconv3+conv2b+disp4* 
disp3 3 1 64/2 4 4 iconv3 
upconv2 3 2 64/32 4 2 iconv3 
iconv2 3 1 66/32 2 2 upconv2+conv1b+disp3* 
disp2 3 1 32/2 2 2 iconv2 
upconv1 3 2 32/16 2 1 iconv2 
iconv1 3 1 18/16 1 1 upconv1+disp2* 
disp1 3 1 16/2 1 1 iconv1 

Table 1: Our network architecture, where k be the kernel size, s the stride, chns the number of input and output channel for each 
layer, input and output be the downscaling factor for each layer relative to the input image, and input corresponds to the input of 
each layer where + be a concatenation and ∗ be a 2× upsampling of the layer. 

2. Post-Processing 
The post-processed disparity map corresponds to the per-pixel weight sum of two components: dl the disparity of the input 

image, d′′l the flip disparity of the flip input image. 
We define the per-pixel weight map wl for dl a 

wl(i,j)= 

 
 
 

1 if j≤0.1 
0.5 if j>0.2 
5∗(0.2−i)+0.5 else, 

where i,j be normalize pixel coordinates, and the weight map w′l for d′′l be obtain by horizontally flip wl. 
The final disparity be calculate as, 

d=dlwl+d′′lw′l. 

1 



Figure 1: Example of a post-processed disparity map. From left to right: The disparity dl, d′′l, d, and the weight map wl. 

3. Deep3D Smoothness Loss 
In the main paper we also compare to our enhance version of the Deep3D [1] image formation model that include smoothness 

constraint on the disparities. Deep3D output an intensity image a a weight sum of offset copy of the input image. The weight 
wi, see in Fig. 2a, can be see a a discrete probability distribution over the disparity for each pixel, a they sum to one. Thus, 
smoothness constraint cannot be apply directly onto these distributions. However, we see (in Fig. 2c) that if the probability mass 
be concentrate into one disparity, i.e. max(w)≈1, then the sum of the cumulative sum of the weight be equal to the position of 
the maximum. To encourage the network to concentrate probability at single disparities, we add a cost, 

Cmax= 
1 

N 
‖max(wi)−1‖2, (1) 

and it associate weight αmax=0.02. Assuming the maximum of each distribution be one, such a in Fig. 2b, meaning the network 
only pick one disparity per pixel, we can see that the (sum of the) cumulative sum cs(wi) of the distribution (Fig. 2c) directly relates 
to the location of the maximum disparity: 

d=argmax 
i 

(wi)=n− 
n∑ 

i 

cs(wi), (2) 

where n be the maximum number of disparities. 
In the example present in Fig. 2, we can see that the maximum be locate at disparity 3, and that Equation 2 give u d=8−6=3. 
We use this observation to build our smoothness constraint for the Deep3D image formation model. 
We can then directly apply smoothness constraint on the gradient of the cumulative sum of the weight at each pixel, so 

Cds= 
1 

N 

∑ 

i,j 

|∂xcs(w)ij|+|∂ycs(w)ij|, (3) 

and it associate weight αds=0.1. 

1 0 1 2 3 4 5 6 7 8 
disparity 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

pr 
ob 

ab 
ili 

ty 

(a) Probability over disparity 

1 0 1 2 3 4 5 6 7 8 
disparity 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

pr 
ob 

ab 
ili 

ty 

(b) Probability over disparity 

1 0 1 2 3 4 5 6 7 8 
disparity 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

pr 
ob 

ab 
ili 

ty 

(c) Cumulative sum of Fig. 2b 

Figure 2: Deep3Ds per-pixel disparity probabilities. 

4. More KITTI Qualitative Results 
In Fig. 3 we show some additional qualitative comparison for the KITTI dataset use the Eigen split. 



In 
pu 

t 
G 

T 
Ei 

ge 
n 

et 
al 

. 
Li 

u 
et 

al 
. 

G 
ar 

g 
et 

al 
. 

O 
ur 

s 
In 

pu 
t 

G 
T 

Ei 
ge 

n 
et 

al 
. 

Li 
u 

et 
al 

. 
G 

ar 
g 

et 
al 

. 
O 

ur 
s 

Figure 3: Additional qualitative result on the KITTI Eigen Split. As ground truth velodyne depth be very sparse, we interpolate 
it for visualization purposes. 

5. Disparity Error Maps 
As we train our model with color augmentations, we can apply the same principle at test time and analyze the distribution of 

the results. We apply 50 random augmentation to each test image and show the standard deviation of the disparity per pixel 
(see Figure 4). We can see that the network get confuse with close-by objects, texture-less regions, and occlusion boundaries. 
Interestingly, one test image be capture in a tunnel, result in a very dark image. Our network clearly show high uncertainty 
for this sample a it be very different from the rest of the training set. 

References 
[1] J. Xie, R. Girshick, and A. Farhadi. Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks. In ECCV, 

2016. 2 



In 
pu 

t 
D 

be 
pa 

rit 
y 

σ 

Figure 4: Uncertainty of our model on the KITTI dataset. From top to bottom: input image, predict disparity, and standard 
deviation of multiple different augmentations. We can see that there be uncertainty in low texture region and at occlusion boundaries. 


