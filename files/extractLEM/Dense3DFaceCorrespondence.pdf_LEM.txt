




































untitled 


Dense 3D Face Correspondence 
Syed Zulqarnain Gilani , Ajmal Mian, Faisal Shafait, and Ian Reid 

Abstract—We present an algorithm that automatically establishes dense correspondence between a large number of 3D faces. 

Starting from automatically detect sparse correspondence on the outer boundary of 3D faces, the algorithm triangulates exist 

correspondence and expands them iteratively by match point of distinctive surface curvature along the triangle edges. After 

exhaust keypoint matches, further correspondence be establish by generate evenly distribute point within triangle by 

evolve level set geodesic curve from the centroid of large triangles. A deformable model (K3DM) be construct from the dense 

correspond face and an algorithm be propose for morph the K3DM to fit unseen faces. This algorithm iterates between rigid 

alignment of an unseen face follow by regularize morph of the deformable model. We have extensively evaluate the propose 

algorithm on synthetic data and real 3D face from the FRGCv2, Bosphorus, BU3DFE and UND Ear database use quantitative and 

qualitative benchmarks. Our algorithm achieve dense correspondence with a mean localisation error of 1.28 mm on synthetic face 

and detect 14 anthropometric landmark on unseen real face from the FRGCv2 database with 3 mm precision. Furthermore, our 

deformable model fitting algorithm achieve 98.5 percent face recognition accuracy on the FRGCv2 and 98.6 percent on Bosphorus 

database. Our dense model be also able to generalize to unseen datasets. 

Index Terms—Dense correspondence, 3D face, morphing, keypoint detection, level sets, geodesic curves, deformable model 

Ç 

1 INTRODUCTION 

ONE of the canonical task in shape analysis be to find ameaningful mapping between two or more shape [1]. 
The process, call shape correspondence, be a pre-requisite 
for many computer vision, computer graphic and medical 
image analysis applications. The requisite density of corre- 
spondence be often dictate by the underlie shape and tar- 
get application. Sometimes, sparse correspondence be 
sufficient to infer shape semantics bymatching representative 
points, for example the four corner of a rectangle or emblem- 
atic point on key joint of a human body. However, sparse 
correspondence be often inadequate in case of articulate 
shape [2], [3] where part of the shape can bend indepen- 
dently or in the correspondence of anatomical shape which 
can deform in an elastic manner [4]. In such circumstances, 
dense correspondence be require to guarantee representation 
of global shape changes, for instance in case of morph or 
attribute transfer. Furthermore, very subtle change within a 
class of shape can be detect only if the correspondence 
between these shape be dense [6]. 

In this paper we be concerned with the task of find 
dense correspondence between a very large number of simi- 
lar shapes; in our case 3D scan of human faces. We do so 

because this further enables u to generate highly accurate 3D 
morphable model that can be use for information transfer 
between the training set and a test face or between two test 
face by morph the 3D model to fit the test face(s). For 
example, give the location of anthropometric landmark [7] 
on the 3Dmorphablemodel, these landmark can be automat- 
ically localize on previously unseen test face [8]. Further- 
more, dense correspondence and morphable model can be 
use for 3D face recognition [9], [10]. Other application 
include facial morphometric measurement such a gender 
score [5] and asymmetry for syndrome diagnosis [6], statis- 
tical shape model [11], [12], shape interpolation [13], non- 
rigid shape registration [3], [14], [15], deformation analy- 
si [16] and recognition [17], [18], [19]. 

While it be possible to manually annotate a small number 
(�30) of correspondence for a few 3D faces, it be not feasible 
to manually identify dense correspondence (�6,000) 
between hundred of 3D faces. The literature also proposes 
compute dense correspondence by extend manually 
annotate sparse one [9], [20]. However, with the advent of 
huge 3D face database like the Facebase Consortium [21] or 
Raine dataset [22], [23], this strategy too have become impracti- 
cal and call for fully automatic algorithms. Automatically 
establish dense correspondence between the 3D face of 
two different person be an extremely challenge task 
because the facial shape varies significantly amongst individ- 
uals depend on their identity, gender, ethnicity and age [7] 
a well a their facial expression and pose. The problem of 
dense 3D point-to-point correspondence can be formulate 
a follows. Given a set of N 3D faces, Fj ¼ ½xp; yp; zp�T ; 
j ¼ 1; . . . ;N; p ¼ 1; . . . ; Pj, the aim be to establish a dense bijec- 
tive mapping f : Fi ! Fjði 6¼ jÞ over k vertex where 
1 < < k < minðPi; PjÞ. Correspondences should cover all 
region of the face for high fidelity and should follow the 
same triangulation for shape consistency. 

� S.Z. Gilani, A. Mian, and F. Shafait be with the School of Computer Sci- 
ence and Software Engineering, The University of Western Australia, 35 
Stirling Highway, Crawley, WA, 6009, Australia. E-mail: syedzulqarnain. 
gilani@research.uwa.edu.au, {ajmal.mian, faisal.shafait}@uwa.edu.au. 

� I. Reid be with the School of Computer Science, University of Adelaide, 
Ingkarni Wardli, North Terrace Campus, Adelaide, SA 5005, Australia. 
E-mail: ian.reid@adelaide.edu.au. 

Manuscript receive 15 June 2016; revise 2 May 2017; accepted 3 July 2017. 
Date of publication 10 July 2017; date of current version 12 June 2018. 
(Corresponding author: Syed Zulqarnain Gilani.) 
Recommended for acceptance by R. Yang. 
For information on obtain reprint of this article, please send e-mail to: 
reprints@ieee.org, and reference the Digital Object Identifier below. 
Digital Object Identifier no. 10.1109/TPAMI.2017.2725279 

1584 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018 

0162-8828� 2017 IEEE. Personal use be permitted, but republication/redistribution require IEEE permission. 
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. 

https://orcid.org/0000-0002-7448-2327 
https://orcid.org/0000-0002-7448-2327 
https://orcid.org/0000-0002-7448-2327 
https://orcid.org/0000-0002-7448-2327 
https://orcid.org/0000-0002-7448-2327 
mailto: 
mailto: 
mailto: 
mailto: 


Existing dense correspondence technique have one or 
more of the follow limitations: (1) They need manually 
annotate landmark on 3D face for initialization. (2) They 
use image texture match to find 3D shape correspon- 
dence. (3) They correspond all face to a single reference face 
neglect the global proximity of the 3D faces. (4) They 
have not be test on complete benchmark database such 
a the FRGCv2 [24] or Bosphorus [25] datasets for face recog- 
nition and landmark identification. (5) They have no explicit 
mechanism of update the dense correspondencemodel. 

In this context, we propose a fully automatic algorithm 
for establish dense correspondence simultaneously 
between a large number of 3D faces. Our algorithm do 
not require any manual intervention and relies solely on 3D 
shape match to encode accurate facial morphology. We 
organize the 3D face into a minimum span tree base 
on bending energy require to deform one shape into the 
other so that correspondence can be propagate in a reli- 
able way. We propose a mechanism for automatic initializa- 
tion of a sparse set of correspondence on the outer 
boundary of the 3D faces. We form a triangulation of these 
correspondences, and iteratively add to the set of point by 
match point of distinctive surface curvature along (and 
close to) the triangulate edges. After exhaust the possi- 
bilities for such matches, we further expand the set of 
match by generate point distribute evenly within tri- 
angle by evolve level set geodesic curve from the cent- 
roids of large triangles. The outcome of our algorithm be a 
Keypoint-based 3D Deformable Model (K3DM). 

Our second major contribution be a deformable model fit- 
ting algorithm where K3DM be use to morph into unseen 
query faces. Starting from the mean face, the fitting algo- 
rithm iterates between two steps. The query face be trans- 
form rigidly to align with the model and the model be 
deform use regularize least square to fit the query 
face. This algorithm converges in a few iteration and be 
robust to noise, outlier points, miss points, pose and 
expression variations. 

Our final contribution be an algorithm for augment the 
K3DM. Given the K3DM and a new batch of M faces, we 
construct a minimum span tree use the near face 
to the K3DM a the root node. The K3DM be augment by 
add one face at a time, start with the root node, and 
each time update the model and deform the update 
model to good fit the next face in the span tree. 

Evaluating dense correspondence technique be challeng- 
ing due to the inherent difficulty of obtain ground-truth 
data. In the exist literature, evaluation have mostly 
be perform on a sparse set of anthropometric facial 
landmark [26], [27], [28] since these can be manually 
labelled. However, evaluation on only a few (� 20) anthro- 
pometric point do not show how well dense correspond- 
ences have generalize to the whole face. Thus, subjective 
evaluation be frequently perform [29] by visually 
inspect the quality of morph between face [4], [30]. 
In this paper, we show how synthetic 3D face (Facegen 
Modeller) can be use to quantitatively evaluate dense cor- 
respondences on a large set of point (� 1; 000). Using the 
present deformable face model, we perform extensive 
experiment for landmark localization (Section 6.1) and 
face recognition (Section 6.2) use real face from the 

FRGCv2 [24] and BU3DFE [31] databases. Results show 
that our algorithm outperforms state-of-the-art application- 
specific algorithm in each of these areas. 

2 RELATED WORK 

Existing 3D correspondence technique can be grouped into 
descriptor based, model base and optimization base [1]. 

Descriptor Based Techniques. These technique match local 
3D point signature derive from the curvatures, shape 
index and normals. However, they be often highly sensitive 
to surface noise and sample density [33] of the underlie 
geometry [34]. More significantly for our purpose, the den- 
sity of correspond point be typically low result in cor- 
respondences between a very sparse set of anthropometric 
landmarks. 

One of the early works, in this category, for establish 
dense correspondence be propose by Sun and Abidi [30], 
[32] who project geodesic contour around a 3D facial 
point onto their tangential plane and use them a feature 
to match two surfaces. The approach, with minor modifica- 
tions, be employ by Salazar et al. [35] to establish point 
correspondence on 3D face in BU3DFE database. Lu and 
Jain [36] present a multimodal approach for facial feature 
extraction. Using a face landmark model, the author 
detect seven correspond point on 3D face use 
shape index from range image and cornerness from inten- 
sity images. Segundo et al. [37] combine surface curvature 
and depth relief curve for landmark detection in 3D face of 
the FRGCv2 and BU3DFE databases. They extract feature 
from the mean and Gaussian curvature for detect five 
landmark in the nose and eye (high curvature) regions. 

Creusot et al. [27] present a machine learn approach 
to detect 14 correspond landmark on 3D faces. They 
train multiple LDA classifier on a set of 200 face and a 
landmark model use a myriad of local descriptors. Each 
landmark detection be treat a a two class classification 
problem and the final result be fused. This method work 
well for neutral expression face of the FRGCv2 and Bospho- 
ru databases. Perakis et al. [26] propose a method to detect 
landmark under large pose variation use a statistical 
Facial Landmark Model (FLM) for the full face and another 
two FLMs for profile view of the face. Keypoints be 
detect use Shape Index and Spin Images and then 
match on the basis ofminimumcombined normalize Pro- 
crustes and Spin Image similarity distance from all three 
FLMs. Thismethodwas use to detect eight correspondence 
in the FRGCv2 and UND Ear databases. Later, the author 
propose a technique [38] for fuse feature from 2D and 3D 
data to detect these landmarkswith good accuracy than [26]. 

Some method have also be propose for generate 
sparse correspondence for 3D face recognition [44], [45], [46], 
[47]. However, these method be base on keypoint corre- 
spondences that be repeatable only on the same identity. 

Model Based Techniques. These approach create a 
morphable model use a sparse set of correspondence 
and then extend them to dense correspondences. 

Employing a Point Distribution Model couple with 3D 
point signature detection, Nair and Cavallaro [8] estimate 
the location of 49 correspond landmark on faces. They 
test their algorithm on 2,350 face of the BU3DFE [31] 

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1585 



database and report a rather high mean landmark locali- 
zation error. 

Blanz and Vetter [29] propose a dense correspondence 
algorithm use optical flow on the texture and the 3D cylin- 
drical coordinate of the face point assume that the face 
be spatially aligned. They construct a 3D morphable face 
model from 100 male and female face each. An arbitrary 
face be chosen a a reference and the remain scan be 
register to it by iterate between optical flow base corre- 
spondence and morphable model fitting. One potential pit- 
fall of the texture base dense correspondence [29] be that 
facial texture be not always consistent with the underlie 
3D facial morphology, e.g., the shape and location of eye- 
brows. Moreover, this algorithm require seven manually 
annotate facial landmark for initialization. Later, in [9], 
[39] the author use the 3Dmorphablemodel for face recog- 
nition. Experiments be perform on only 150 pair of 3D 
face [39] from FRGCv2 database, although the total number 
of scan in the database be 4,007. The seminal work of Blanz 
andVetter [29]was extend by Paysan et al. [40] in the Basel 
Face Model (BFM) which use an improve mesh registra- 
tion algorithm [41]. The author havemade their dense corre- 
spondence model publicly available which have enable u to 
draw comparison with their model. 

Passalis et al. [42] propose an Annotated Face Model 
(AFM) base on an average facial 3D mesh. The model be 
create by manually annotate a sparse set of anthropo- 
metric landmark [7] on 3D face scan and then segment 
it into different annotate areas. Later, Kakadiaris et al. [43] 
propose elastic registration use this AFM by shift the 
manually annotate facial point accord to elastic con- 
straints to match the correspond point of 3D target 
model in the gallery. Face recognition be perform by 
compare the wavelet coefficient of the deform image 
obtain from morphing. Passalis et al. [18] further 
improve the AFM by incorporate facial symmetry to 
perform pose invariant face recognition. However, the algo- 
rithm depends on detection of at least five facial landmark 
on a side pose scan. 

Level set curve be evolve in [28] to automatically 
extract seed point and correspondence be establish 
by minimize the bending energy between patch around 
seed point of different faces. A morphable model base on 
the dense correspond point be then fit to unseen 
query face for transfer of correspondences. The accuracy of 
landmark localization in this method depends on the num- 
ber and accuracy of initial seed points. 

Optimization Based Techniques. These method optimize 
an objective function to find a mapping between fiducial 
points. Non-rigid ICP (NICP) be one such technique which 
formulates deformable registration a an optimization prob- 
lem consist of a mesh smoothness term and several data 
fitting term [41], [78]. These algorithm require accurate 
global initialization point range from 14 point [41] to 68 
point [77]. These point be either manually annotate [29], 
[41] or detect automatically use texture [77]. An exten- 
sion to this method remove the need for fiducial point but 
assumes a partial overlap of facial region [78], [79]. The 
alignment between two face be perform with a global 
rigid transformation follow by per-vertex affine transfor- 
mations that bring the non-rigid shape into full alignment. 

Such method be more suit for time vary deforma- 
tions of the same identity and often do not result in a bijec- 
tive (one-to-one) mapping of the vertices. Booth et al. [77] 
construct a dense correspondence model of several face 
from a propriety dataset by register the scan to a tem- 
plate mesh use NICP algorithm [41] initialize by 68 fidu- 
cial landmark detect use texture. Bolkart et al. [80] 
present dense correspondence a an optimization prob- 
lem and use the Minimum Description Length (MDL) [11] 
a the objective function. The author of method that be 
base on NICP [40], [41], [77], [78], [79] or other alternative 
optimization technique [80] have not report facial land- 
mark localization results. Hence, it be difficult to perform a 
direct objective comparison with these methods. 

3 DENSE 3D FACE CORRESPONDENCE 

The overall idea of our system for dense correspondence 
between 3D face scans, be to begin with a set of automati- 
cally extract seed point that represent point match 
across all face in the dataset, and gradually densify the set 
of matches. Fig. 1 depicts the overall flow of our system. 
Here we give an overview of how this proceeds, and then 
expand the detail in the section below. 

We first organize the face into a tree (Section 3.1) base on 
similarity. We then seek a set of reliable seed match 
(Section 3.2) from which to begin an iterative densification 
process. Each iteration of the densification process 
(Section 3.3) begin by select the current best set of 
match (comprising nq of the full set of nmatches) and form- 
ing a triangulation of these points. Taking each edge of 
the result triangulation in turn, we extract a narrow patch 

Fig. 1. Block diagram of the present dense 3D face correspondence 
algorithm. 

1586 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018 



center on the edge from a pair of face that be adjacent 
in the tree. For each of these patch we find point of dis- 
tinctive curvature (Section 3.4) - these will be new candi- 
date matches, or keypoints - and compute a 38-dimensional 
descriptor of the local surface around each keypoint. Using 
constrain near neighbor we then determine point 
that match well between the pair of patch (i.e., their 
descriptor match and they be within a proximity thresh- 
old in the patch). We repeat this process for all parent/ 
child pair throughout the tree, and eliminate all keypoints 
that be not consistently match throughout the tree. The 
remain keypoints that be successfully match across 
all face in the dataset be add to the current set of 
matches. At the end of one iteration, when we have cycle 
through all the triangulate edges, we choose a new best 
set of nq match and repeat the process. 

Once the search for keypoints be exhausted, further corre- 
spondences on facial area devoid of discriminative point 
be establish by first evolve level set curve and sam- 
pling equidistant vertex (See Section 3.6). Feature vector 
of these vertex on the reference face be then match with 
the remain face to establish correspondence a previ- 
ously stated. 

The outcome of this process be a set of densely corre- 
sponding 3D face which we call the Keypoint-based 3D 
Deformable Model (K3DM). 

3.1 Preprocessing and Organizing Faces 

The nose tip of a 3D face be detect automatically follow 
Mian et al. [67]. Centering a sphere at the nose tip , the face 
be cropped. The pose of the 3D face be iteratively correct to 
a canonical form use the Hotelling transform [53]. Next, 
hole be fill and noise be remove use the gridfit algo- 
rithm [76]. 

Next,we pre-organise the face dataset into a graph (in fact, 
a tree) in which similar face be “close” to one another. Let 
G ¼ ðVg; EgÞ be a direct graph where each node Vg be a 3D 
face F from the dataset and each edgeEg connects two node 
ðvi; vjÞ of the graph. Each edge of the graph hasweightw 

wðvi; vjÞ ¼ 
bij þ bji 

2 
; (1) 

where bij be the amount of bending energy require to 
deform face Fi to Fj and be measure use the 2D thin-plate 

spline model [48]. Note that bij 6¼ bji and bii ¼ 0. Since, the 
face be already roughly aligned, their near neighbor 
point be take a approximate correspondence for the 
purpose of calculate the bending energy. From G, we con- 
struct a minimum span tree P ¼ ðVt; EtÞ use 
Kruskal’s algorithm. The node with the maximum number 
of child be take a the root node. 

The purpose of this pre-organisation be to increase the 
likelihood of find point match between pair of faces. 
A naive approach would be to arbitrarily choose a single (or 
average) face a reference and find it correspondence to 
others in the dataset. But such an approach ignores the 
proximity between the face instance and the global infor- 
mation underlie the population. The process and a sam- 
ple graph be show in Fig. 2. 

3.2 Sparse Correspondence Initialization 

We initialize the correspondence by first automatically 
establish a sparse set of seed points. We restrict these 
seed point to those that lie on the roughly ellipse-shaped 
2D convex hull of the face, i.e., the 2D-hull when the 3D 
mesh be project into the x� y plane. We sample these 
point at regular angular interval of d ¼ p=36 (see 
Fig. 3), where the angle d be measure at the nose tip. 
There be of course no guarantee that in the finite resolu- 
tion mesh of the face there will be a point at an exact 
multiple of p=36, but for each face we choose the near 
point. This yield a set of 72 3D seed point for each 3D 
face in the dataset which be use in the first iteration of 
the triangulation and densification process, a described 
in the next section. 

3.3 Triangulation and Geodesic Patch Extraction 

The main part of our algorithm be an iteration that take the 
best set of match that have be establish to date, and 
grows the number of correspondences. For the first itera- 
tion, we use the sparse set of correspondence establish 
a in the previous section, while for subsequent iteration 
we determine the best set of nq match from the full set of 
nmatches a described in Section 3.5. 

In each iteration, give nq correspondence between N 
faces, we perform a 2D Delaunay triangulation of the mean 
x� y location of the nq current best matches. This triangu- 
lation be then use consistently across all faces. We then 
pick a pair of parent/child node from the Minimum Span- 
ning Tree P, Fj and Fk. For both face in the pair, we extract 

Fig. 2. The direct graphG ¼ ðVg; EgÞ (Left) and the Minimum Spanning 
Tree (MST)P ¼ ðVt; EtÞ (Right) construct from five example image of 
FRGCv2. 

Fig. 3. (a) Vertices of the 2D-convex hull of the projection. (b) Points 
sample at angular interval of p=36. (c) Initial sparse correspondence 
project on four identity of the FRGCv2 dataset. 

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1587 



a narrow surface patch S ¼ f½xi; yi; zi�T ; i ¼ 1; . . . ;mg � F, 
center on a geodesic curve define by each triangle 
edge (see Fig. 4). For the sake of simplicity we call this a geo- 
desic patch. 

The (projected) length of the patch be the same a the 
length of the edge. The “narrow” width be set with reference 
to the scale of the original face mesh resolution. More specif- 
ically, we set the width to be 5r where r be the average 
mesh-edge length in the vicinity of the endpoint of the edge 
(note that here the mesh-edges refer to the edge in the orig- 
inal dataset, not the edge of the triangulation use for the 
densification). This make the extraction of the geodesic 
patch scale invariant. The value of r for real 3D face cap- 
tured with the Minolta or the 3dMDface scanner typically 
range from 1-3 mm. 

Finally, we bring the patch Sj;Sk into approximate 
alignment use non-rigid registration [49], [50]. The pro- 
ce be show in Fig. 4. 

3.4 Keypoint Detection on Geodesic Patches 

Our aim now be to establish accurate correspondence 
between a patch on one face Sj and it correspond patch 
on the other face Sk. We do this in a fairly standard manner 
by find distinctive keypoints, generate a descriptor of 
the local surface around each point, and establish 
match between point on each patch whose descriptor 
be sufficiently close. 

More specifically, to find keypoints we consider the sur- 
face distinctiveness at each point in the patch. We do so by 
calculate the covariance of all the point within a neigh- 
borhood of 5r of the current point, and mark a key- 
point any point for which the ratio of the large two 
eigenvalue of the covariance exceeds a threshold. Note that 
if the neighborhood be uniform these eigenvalue will be 
equal, and therefore the point be unsuitable a a keypoint. 

Fig. 5 show keypoints detect by our algorithm in the 
tenth iteration on four different identity of the FRGCv2 
database. 

We use the keypoints detect on surface patch Sj for 
feature extraction and match only if an adequate number 
of keypoints be detect (we use a minimum of three), oth- 
erwise, Sj be not consider to be sufficiently descriptive 
and the match be not sought within the patch. 

3.5 Feature Extraction and Matching 

We denote by CCj ¼ ½xi; yi; zi�T ; i ¼ 1; . . . ; nC the set of key- 
point detect on the surface Sj, where nC be the number 
of keypoints (likewise for CCk). For each keypoint we extract 
a feature vector x which describe the local surface (within 
5r) use a set of 3D signature and histogram base descrip- 
tors. These descriptor have be widely use in the litera- 
ture [27], [51], [52] for automatic object recognition and for 
landmark detection. We use a combination of many descrip- 
tor since the surface patch be quite small and a single 
descriptor may not capture sufficient information. The list 
of descriptor be give below: 

� The spatial location ½xi; yi; zi�T . 
� The surface normal ½nx; ny; nz�T . 
� The seven invariant moment [53] of the 3 3 histo- 

gram of theXY; YZ andXZ planes. 
� The central moment mmn of order mþ n of the histo- 

gram matrixH 

mmn ¼ 
X’ 
i¼1 

X’ 
j¼1 

ði� �iÞmðj� �jÞnHði; jÞ; (2) 

where ’ be the total number of point in H, 
�i ¼ P’i¼1 P’j¼1 iHði; jÞ and �j ¼ P’i¼1 P’j¼1 jHði; jÞ. 

� The mean of the two principle curvature �k1 and �k2 
calculate at each point on the extract local surface 

� The Gaussian CurvatureK ¼ k1k2 
� The Mean Curvature H ¼ k1þk22 
� The Shape Index. We use two variant of the shape 

index which vary from 0 to 1 and �1 to 1 
respectively, 
sa ¼ 12 � 1p arctan k1þk2k1�k2 ; 0 � sa � 1 and 
sb ¼ 2p arctan k1þk2k1�k2 ; �1 � sb � 1. 

� The Curvedness c ¼ 
ffiffiffiffiffiffiffiffiffiffi 
k2 
1 
þk2 

2 
2 

q 
� The Log-Curvedness 

cl ¼ 2p log 
ffiffiffiffiffiffiffiffiffiffi 
k2 
1 
þk2 

2 
2 

q 
, 

� The Willmore Energy ew ¼ H2 �K, 
� The Shape Curvedness c ¼ sb:cl 
� The Log Difference Mapml ¼ lnðK �H þ 1Þ. 
Using these descriptors, the dimensionality of the final 

feature vector x be 38. These feature be extract over a 
small enough local surface center at the keypoint such 
that they be repeatable across identities. In contrast, the fea- 
ture vector extract by Mian et al. [44], [54] take the range 
value of a large surface (typically 20 mm radius) sur- 
round each keypoint. Hence, their feature be repeat- 
able only over the same identity. One of the prerequisite of 
the technique that use depth value a feature [44], [51], 

Fig. 4. Illustration of geodesic patch extraction. (a) Two 3D face with tri- 
angulation over a few correspond point from the 2nd iteration. Geo- 
desic surface patch be extract between two sample point show in 
red colour. (b) Pointclouds of the geodesic surface patch before and 
after registration. 

Fig. 5. Illustration of keypoints (not correspond points) detect along 
geodesic patch in the tenth iteration of our algorithm. Notice the 
repeatability of keypoints across the identities. 

1588 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018 



[52], [54] be to define a local reference frame for pose invari- 
ant matching. In our case, the feature be quasi pose invari- 
ant and hence do not require a local reference frame. This be 
because the pose of each training face have be iteratively 
correct to a canonical form during preprocessing and the 
feature be extract from a very small patch. 

Next, we perform constrained-NN search between the 
feature vector xj from Sj and xk from Sk, such that the cor- 
respond point lie within a proximity of 2r to each other, 
and their match score dðxj; xkÞ - take to be Euclidean 
distance between their feature descriptor - be less than a 
threshold kq. The quality of correspondence varies directly 
with kq. Higher value of kq will result in poor correspond- 
ing point with large errors, whereas low value of kq may 
reject valid correspondence and hence adversely effect the 
correspondence density. Fig. 6 show the effect of kq on the 
correspondence found in our experiment on the synthetic 
dataset. As we increase the value of kq, the mean localiza- 
tion error and it standard deviation (SD) increases. Fig. 7 
show the outcome of this step on two identities. 

This process be repeat for all surface patch in a pair 
of faces, and for all pair of face in the MST. Only point 
that be match throughout the MST in the pairwise 
scheme be retain and these be add to the set of cor- 
respondences obtain for the previous iteration. We then 
select from the full set of correspondence those which 
have the small match score dðxj; xkÞ. We denote the 
number of select correspondence by nq and use a value 
of nq ¼ 80 in our experiments. In order to adequately cover 

the whole face for the subsequent iteration, we add 
the original seed point to the nq points. Next, we obtain 
a triangulation of these point on the mean face of the 
dataset and extract geodesic surface patch a described 
in Section 3.3, repeat the process. 

3.6 Densifying Matches in Uniform Regions 

Keypoints, by their very definition concentrate around 
region of high curvature/discrimination, such a the 
mouth, nose, and eyes. In this section we describe how we 
establish correspondence in more uniform region where 
keypoints cannot be found. A simple approach to establish 
dense correspondence in these area would be to sample 
them uniformly within triangle of the Delaunay triangula- 
tion. This approach have be use in 2D by Munsell 
et al. [55] who pre-organized the shape instance base on a 
similarity measure and then establish correspondence 
between pair of shape by mapping the point from the 
source instance to the target instance after minimize a 
bending energy. However, a uniform sample in the trian- 
gle only result in uniform sample on the face in planar 
regions. Instead, we adopt a sample strategy that respect 
the underlie surface distance (geodesics) on each face. 

After triangulation of the final set of best quality corre- 
sponding points, we select large triangle with area great 
than ta. We set ta to be the mean area of all triangle in the 
connectivity, an effective and expeditious choice. From the 
centroid of each triangle, we evolve a level-set curve, in 
which the front speed be set to be uniform along a (radial) 
geodesic. For convenience we refer to these curve a “level- 
set geodesics”. We follow the Fast Marching Method [56] 
and use the implementation give by Peyre [57]. We then 
sample the point along the curve at regular interval to 
ensure equidistant point (see Fig. 8). Because the evolution 

Fig. 6. The effect of correspondence quality threshold kq in the synthetic dataset in the first iteration. (Left) Graph of kq versus the mean and SD of 
correspondence localization error. (Middle) kq versus the number of correspondence established. (Right) kq versus the maximum localization error. 
For all our experiment we have set kq ¼ 2r show in the graph in a magenta circle. 

Fig. 7. Correspondence establish in 1st, 4th, 13th and 18th iteration of 
our algorithm on the first two identity of FRGCv2. Notice how well the 
point correspond across the identities. 

Fig. 8. Correspondence establishment on smooth surfaces. Two face 
from an order pair with triangulation over nq best quality correspond 
points. Blue dot indicate the centroid of large triangles. Level set base 
evolution of geodesic curve for the two sample triangles, magnify on 
the right. 

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1589 



speed of the curve be uniform along geodesics, we obtain 
a uniform sample on the surface; this be in contrast to 
uniform sample within the triangle which would not nec- 
essarily be uniform on the surface itself. Although these 
point be not keypoints, they be repeatable on all 3D face 
across identity because they be extract from triangle 
whose vertex be correspond to each other across the 
dataset. Furthermore, these point be extract at equal 
interval over a small regionwhich be smooth. 

Given this set of point sample uniformly on the sur- 
face, we extract feature vector and perform pairwise 
match a before. Points whose feature vector be close 
enough be retain a matches, with the rest discarded. 
This be not an iterative process and point be sample only 
once from each triangle meeting the threshold criterion. 
Fig. 8 visually illustrates the process. 

An alternative method for densifying the match in the 
uniform region could be to register the source and target 
face use the NICP algorithm [41], [78] initialize by the 
correspondence establish in the previous section. Once 
the target face have deform to the source face, densification 
of correspondence be achieve by mapping the vertex in 
uniform region of the source face to that of the target face. 
This approach require tweak the NICP parameter and 
the iterative optimization process for non-rigid face defor- 
mation tends to be computationally expensive. Our result 
in Section 6.1 also show that our feature match approach 
achieves high accuracy and therefore, we use this 
approach for the remain part of the paper. 

4 K3DM FITTING AND AUGMENTATION 

The output of the dense correspondence algorithm be the set of 
N densely correspond 3D face eFj. Our objective now be to 
develop a compact deformable model base on these densely 
correspond faces. To do so we take a standard PCA-based 

approach, and we call the result our Keypoint-based 3D 

deformable model. More formally, let �� ¼ ½ef1;ef2; . . . ; efN; �, 
where ef ¼ ½x1; . . . ; xp; y1; . . . ; yp; z1; . . . ; zp�T and p ¼ 1; . . . ; P . 
The rowmeanmm� of the K3DM be give by, 

mm� ¼ 
1 

N 

XN 
i¼1 

efi: (3) 
The row-normalized model m ¼ �� mm� can be model 

by a multivariate Gaussian distribution and it eigenvalue 
decomposition be give by, 

USVT ¼ ��m; (4) 
where US be the principal component (PCs), the column 
of V be their correspond loadings, and S be a diagonal 
matrix of eigenvalues. We use only the first n column of U 
which correspond to 98 percent of the energy. 

We propose to deform the statistical model give in (4) 
into a query face Q in a two step iterative process, i.e., regis- 
tration and morphing. Algorithm 1 give the detail of fit- 
ting the deformable model to a query face. Note that we use 
Q and M for the point cloud of the query face and model 
and use q and m for their vectorized version respectively. 
The query face after vectorization can be parametrized by 
the statistical model such that mi ¼ Uaai þ mm�, where the 

vector aai contains the parameter which be use to vary 
the shape of the model in the ith iteration and mi be the vec- 
torized form of the model represent the query face. In 
the initialization step aai be set to zero and the deformable 
model Mi be characterize by the mean face of the K3DM. 
Each iteration begin with a registration step where the 
input face Q be register to the model Mi. This step essen- 
tially entail find an approximate correspondence 
between the model and the query face and a rigid transfor- 
mation. Correspondence be establish by search for the 
Nearest Neighbor (NN) of each point of Mi in Q use the 
k-d tree data structure [58]. Let d represent the NN euclid- 
ean distance between the correspond query face and the 
model such that dj ¼ kfQji �Mijk2. We define outlier a 
point on eQ whose NN distance with Mi be great than a 
threshold tc where tc ¼ dþ 3sd and exclude them from reg- 
istration. This step ensures that the outlier do not affect the 
registration process. Next, the query face be translate to the 
mean of the model and be rotate to align with Mi. We 
denote the correspond and register query face byQr. 

In the next step, the modelMi be deform to fit the regis- 
tered query faceQr such that, 

âai ( min 
aai 

U 
aai þ mm� � qr 
�� �� 

2 
þ�kaai � aai�1k2; (5) 

andmi ¼ Uâai þ mm�. The 
denotes that only those point (rows 
ofU andmm�) be consideredwhich satisfy the threshold tc. The 
second term in (5) put a constraint on deform the model. 
The apply condition be intuitive because we want to partially 
deform themodel in each iteration such that themodel approx- 
imates the query face in small steps. The iterative procedure be 
terminate when the residual error kmi � qrk2 � �f . In all of 
our experiment � be set to 0.8 and �f ¼ 10�4. Fig. 9 show 
the K3DMfitting result on three datasets. 

Algorithm 1. K3DM Fitting 

Require: �m�m ¼ ½ef1;ef2; . . . ;efN � � mm� and Query Face 
Q ¼ ½xp; yp; zp�T where p ¼ 1; . . . ; Pq. 

Initialization: 
1: Iteration: i ¼ 0 and �0 ¼ 1 
2: USVT ¼ ��m 
3: aai ¼ 0 andmi ¼ Uaai þ mm� 
4: while �i > �f do 
5: Update iteration: i ¼ iþ 1 
6: eQ ¼ Q(Mi (NN use k-d tree) 
7: eQ0 ¼ feQkeQi �Mik2 < dþ 3sdg 
8: Qr ¼ eQ0Rþ t (Registration step) 
9: U 
( {Uk row of U correspond to eQ0} 
10: âai(minaai U 
aai þ mm� � qr 

�� �� 
2 
þ�kaai � aai�1k2 

11: mi ¼ Uâai þ mm� 
12: �i ¼ kmi � qrk2 
13: end while 
14: return Qr; âa;m 

From a practical perspective, there be usually a need to 
augment an exist dense correspondence model with new 
3D faces. In the following, we present a K3DM augmenta- 
tion algorithm to achieve this objective. Given the K3DM 
and a batch of M new 3D faces, we compute the bending 
energy require to deform the mean face of the K3DM to 

1590 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018 



each of the new faces. This information be employ to orga- 
nize theM face in a Minimum Spanning Tree a outline in 
Section 3.1. Traversing from the root node (mean face), the 
K3DM be morph into each child node use the model fit- 
ting procedure give in Algorithm 1. The result corre- 
sponded 3D face of the input identity be add to the 
K3DM. Algorithm 2 give the detail of our model augmen- 
tation technique. 

Algorithm 2. K3DM Augmentation 

Require: �� ¼ ½ef1;ef2; . . . ;efN � and a batch of input 3D face 
FM ¼ ff1; f2; . . . ; fMg,M � 1. 

Initialization: 
1: Pre-organize the M face in a Minimum Spanning Tree 

P ¼ ðVt; EtÞ 
2: for each 3D face fi in P do 
3: efi ¼ fit K3DMð��; fiÞ 
4: �� ¼ ½ef1;ef2; . . . ;efN;efi� 
5: Increment number of face in the model 
6: mm� ¼ 1N 

PN 
n¼1 n 

7: end for 
8: return �� ¼ ½ef1;ef2; . . . ;efNþM � 

5 EXPERIMENTAL SETUP 

We have carry out extensive experiment on synthetic and 
real data. Below be the detail of the datasets used, evalua- 
tion criterion and the experiment performed. 

5.1 Datasets Used 

Our synthetic dataset consists of 100 3D face generate 
from the Facegen software.1 Facegen have be use by sci- 
entists in the field of neuroscience and social cognition to 
generate synthetic face for replicate human stimulus [59], 
[60]. The 100 face be in perfect correspondence with each 
other and hence provide the ground truth. Each face have 
3,727 vertex and 7,179 triangles. For experiment on real 
3D faces, we use the FRGCv2 [24], Bosphorus [25], 
BU3DFE [31] and side pose scan of the UND Ear database 
Collections F [61] and G [62]. Some sample image and 
detail of these datasets be give in Fig. 10. The purpose of 
use such diverse datasets be to evaluate the perfor- 
mance of our propose technique for partial data, occlusion, 
expression and pose invariance. 

5.2 Evaluation Criteria 

Fig. 11 show qualitative result of our dense correspon- 
dence algorithm. The smooth transition between different 
face be indicative of accurate correspondence [4], [20]. We 
have include a video of morphings in the supplementary 

material, which can be found on the Computer Society Digi- 
tal Library at http://doi.ieeecomputersociety.org/10.1109/ 
TPAMI.2017.2725279. 

Objective evaluation of dense correspondence algorithm 
on real data be difficult due to the unavailability of the 
ground-truth shape correspondence [63]. One solution be to 
use synthetic data where correspondence be know a pri- 
ori. We use the synthetic 3D face dataset a ground truth 
for our evaluations. To the best of our knowledge, this be the 
first time synthetic 3D face image have be use to evalu- 
ate result of a dense correspondence algorithm in term of 
mean localization error of the correspondences. This dataset 
and protocol be also use to evaluate the efficacy of indi- 
vidual module of our algorithm. 

In the case of real data, the accuracy of the dense corre- 
spondence can be measure together with the deformable 
model fitting algorithm by measure the accuracy of land- 
mark localization and face recognition. Results be expect 
to be good when the underlie model have accurate 
dense correspondences. Hence, we use our dense corre- 
spondence model and fitting algorithm in these applica- 
tions and evaluate the results. In all tables, we have 
highlight the best and the second best result in that cate- 
gory. Note that the main focus of this paper be to propose a 
dense 3D face correspondence algorithm. Experiments on 
landmark localization and face recognition have be car- 
ried out to validate the accuracy of the correspondences. 

We create separate dense correspondence model from 
the FRGCv2, Bosphorus and BU3DFE datasets and denote 

Fig. 9. K3DM fitting result on three datasets. The first scan for each 
dataset be the raw input while the second scan be the fit model. The 60 
degree side pose scan have be rotate to highlight the partial data. 

Fig. 10. Sample image and detail of our four experimental datasets. 

Fig. 11. Qualitative result of our dense correspondence algorithm on 
the first three identity of FRGCv2. The first face in each row be the 
source and the last face be the target.1. Singular Inversions, “Facegen Modeller”, www.facegen.com 

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1591 

http://doi.ieeecomputersociety.org/10.1109/TPAMI.2017.2725279 
http://doi.ieeecomputersociety.org/10.1109/TPAMI.2017.2725279 
www.facegen.com 


them with K3DMFR, K3DMBO and K3DMBU respectively. 
We compare our result to the Basel Face Model (BFM) 
propose by Paysan et al. [40]. We also establish dense cor- 
respondence use our NICP variant for densifying the 
initial keypoints base feature match (see Section 3.6). 
The algorithm be initialize by the correspondence found 
in Section 3.5 and the variant be refer a K3DM-NICP. 

6 RESULTS AND ANALYSIS 

6.1 Landmark Localization 

Synthetic Dataset. First, we present the evaluation of our 
algorithm on synthetic data. We establish dense correspon- 
dence on 100 synthetic 3D face use our propose algo- 
rithm and report the mean and the standard deviation 
(SD) of the localization error with respect to the ground 
truth. The original synthetic dataset contains 3,727 vertex 
for each 3D face. Our propose method be successful in 
establish dense correspondence over 2,246 vertex (60 
percent of the original) with a mean localization error of 
1:28 mm and SD �2:2 mm. Correspondence within 10 mm 
be establish on 99:33 percent vertices. Fig. 12a show a 
plot of the cumulative distribution of correspondence 
within a give error distance. We also establish dense cor- 
respondence over 2,341 vertex of the synthetic dataset 
use our K3DM-NICP variant (see Section 3.6). That 
method result in a mean localization error of 1:30 mm 
with �2:3 mm standard deviation. 

To ascertain the contribution of different components, we 
repeat our experiment by remove different component 
(see Fig. 1) from our algorithm. The result in Table 1 show 
that the combination of all components/modules give the 
best results. 

FRGCv2 Dataset. We construct a dense correspondence 
K3DM from the first neutral scan of the first 200 identi- 
tie (100 male and female each) of this dataset. The 

remain 1,956 scan of 266 identity be use a test 
data. Next, we construct a K3DM from the neutral scan 
of the next 200 identity (100 male and female each) and 
use the 2,051 scan correspond to the first training set 
for testing. This way, we be able to perform landmark 
detection on all 4,007 scan of FRGCv2, each time ensur- 
ing that the identity use for make the K3DM be not 
present in the test data. 

We establish dense correspondence between 9,309 verti- 
ce on the FRGCv2 dataset (K3DMFR) and report the mean 
and SD of the Landmark Localization Error (�L) on 14 fidu- 
cial point consider to be biologically significant [65]. 
These anthropometric landmark be annotate only on the 
mean face and transfer to each densely correspond 
scan in the dataset. Manual annotation provide by 
Szeptycki et al. [66] and Creusot et al. [27] be use a 
ground truth for comparison. 

A comparison of the mean and SD of landmark localiza- 
tion error of our propose algorithm with the state-of-the- 
art in Table 2 show that our result outperform them by a 
significant margin. K3DMBU be construct from 100 neu- 
tral expression scan and 100 angry expression level-1 
scans. K3DMBO be construct from the first neutral scan 
of 105 identities. The K3DMFR achieves the best perfor- 
mance and even the cross domain K3DMs and the K3DM- 
NICP variant outperform exist state-of-the-art. Cumula- 
tive localization error plot use K3DMFR be show 
graphically in Fig. 12b. 

Bosphorus Dataset. We construct two K3DMBO (100 face 
each) from the neutral scan of the Bosphorus dataset [25] 
such that the model and test identity be mutually exclu- 
sive. Note that there be 299 neutral expression scan in the 
dataset. We manually annotate 14 fiducial landmark on the 
mean face of K3DM and transfer the information to other 
scan after model fitting. Fig. 12c show the cumulative 
detection rate of the 14 landmarks. Table 3 detail landmark 
localization result on the three category of the Bosphorus 
dataset. It be evident that our algorithm performs signifi- 
cantly good than the state-of-the-art under occlusions, 
rotation and expression variation. Creusot et al. [27] and 
Sukno et al. [64] train their algorithm on 99 neutral 
scans. They do not report result on these 99 scan and 
the scan with yaw rotation of �90�. On the contrary we 
report the landmarking result on all 4,666 scan of the 
database include the scan with large yaw variation. 
Landmark annotation provide by [25], [27] be use a 
ground truth. For this experiment, K3DMFR be create 
from the neutral scan of first 100 male and female (each) 

Fig. 12. Results of dense correspondence: (a-d) Cumulative localization error distribution plot on the Synthetic (2,246 vertices), FRGCv2 (14 land- 
marks), Bosphorus (14 landmarks) and BU3DFE (12 landmarks) datasets. 

TABLE 1 
Module Wise Mean and SD of Localization Error 
(mm) on 2,246 Vertices of the Synthetic Dataset 

Excluded Module(s) mean � std 
Organising face into a graph 2.16 � 2.8 
Keypoint detection 3.06 � 5.1 
Feature match 3.61 � 6.8 
Keypoint detection and feature match 4.78 � 7.3 
Selecting best match in each iteration 2.61 � 3.4 
No module exclude 1.28 � 2.2 

1592 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018 



identity of FRGCv2 while K3DMBU be the same a use 
in experiment on FRGCv2 dataset. 

BU3DFE Dataset. We construct dense correspondence 
model from the neutral a well a intensity level-1 anger 
expression scan of 100 identity of the BU3DFE data- 
set [31]. We ensure mutually exclusive test and training 
identity while landmark localization use a K3DMBU . 
Comparative result on 12 anthropometric landmark [7] on 
all 2,500 scan of the dataset be give in Table 4. Ground 
truth landmark location be provide with the dataset [31]. 

Fig. 12d show the commutative error detection rate of the 
12 landmarks. K3DMFR be create from the scan of the 
first 100 male and 100 female identity of FRGC. Our result 
be good than the state-of-the-art for both the models. 

UND Ear Dataset. To evaluate the landmark localization 
performance of our algorithm on side pose scan contain 
self occlusions, we perform experiment on the UND Ear 
Database [61], [62]. We follow the exact protocol outline 
by [18], [38] for a fair comparison. The dataset be divide into 
45 and 60 degree left and right pose scan namely DB45L, 

TABLE 2 
Comparative Results of the Mean and SD (mm) of Landmark Localization Error on FRGCv2 Dataset 

Author Images Ex(L) Ex( R ) En(L) En( R) N Prn Ac* Ch* Ls Li Pg Sn Mean 

Lu [36] 676 9.5 � 17.1 10.3 � 18.1 8.2 �17.2 8.3 � 17.2 - 8.3 � 19.4 - 6.1 � 17.4 - - - - 8.1 � 17.7 
Segundo [37] 4007 - - 3.7 � 2.3 3.4 � 2.3 - 2.8 � 1.4 5.3 � 1.9 - - - - 4.1 � 1.9 
Perakis [26] 975 5.6 � 3.1 5.8 � 3.4 4.2 � 2.2 4.4 � 2.5 - - 4.1 � 2.2 5.5 � 2.4 - - 4.9 � 3.7 - 5.0 � 2.7 
Cruesot [27] 4007 5.9 � 3.1 6.0 � 3.0 4.3 � 2.4 4.3 � 2.0 4.2 � 2.0 3.4 � 2.0 4.8 � 3.6 5.5 � 3.5 4.2 � 3.2 5.5 � 3.9 7.3 � 7.4 3.7 � 3.1 5.0 � 3.3 
Perakis [38] 975 4.7 5.4 4.0 4.1 3.7 4.3 - 4.1 - - 4.3 

Sukno [64] 4007 4.7 � 2.7 4.6 � 2.7 3.5 � 1.7 3.6 � 1.7 2.5 � 1.6 2.3 � 1.7 2.6 � 1.4 3.9 � 2.8 3.3 � 1.8 4.6 � 3.4 4.9 � 3.5 2.7 � 1.1 3.5 � 2.4 
Gilani [28] 4007 4.5 � 2.9 3.7 � 2.8 3.1 � 2.1 2.7 � 2.1 3.6 � 2.0 2.7 � 2.5 4.2 � 3.2 4.8 � 2.1 3.3 � 3.7 4.0 � 3.8 4.2 � 3.3 4.1 � 3.1 3.9 � 2.8 
BFM [40] 4007 2.2 � 2.5 2.7 � 1.8 2.5 � 2.1 2.9 � 2.2 3.2 � 2.2 2.3 � 2.0 8.3 � 2.9 2.6 � 2.9 2.6 � 2.2 3.8 � 3.7 4.2 � 3.8 3.8 � 3.6 3.7 � 2.7 
K3DM-NICP 4007 2.8 � 2.2 2.5 � 1.8 2.7 � 1.8 2.6 � 1.1 2.6 � 1.7 2.4 � 1.9 3.3 � 2.5 2.7 � 1.8 2.6 � 3.2 4.2 � 3.4 4.2 � 3.3 3.5 � 1.4 3.3 � 2.3 
K3DMFR 4007 2.6 � 2.1 2.4 � 1.7 2.4 � 1.6 2.4 � 0.9 2.5 � 1.5 2.2 � 1.8 3.0 � 2.4 2.5 � 1.8 2.4 � 3.1 4.1 � 3.3 4.1 � 3.3 3.4 � 1.1 2.9 � 2.1 
K3DMBU 4007 2.7 � 2.4 2.3 � 1.9 2.4 � 1.9 2.5 � 1.8 2.8 � 1.8 2.6 � 1.8 6.1 � 2.7 4.2 � 3.1 2.9 � 3.3 4.6 � 3.9 4.1 � 3.4 3.6 � 2.9 3.6 � 2.6 
K3DMBO 4007 2.6 � 2.2 2.4 � 1.9 2.8 � 2.0 2.9 � 2.0 3.2 � 2.2 2.3 � 2.1 8.3 � 3.4 3.1 � 2.7 2.5 � 2.4 3.5 � 3.7 4.1 � 3.9 3.8 � 3.6 3.8 � 2.7 

* Results have be average for left and right corner of nose and mouth. 
A ‘-’ denotes that the author have not detect this particular landmark. Ex/En-outer/inner eye corner, N-nosebridge saddle, Prn-nosetip, Ac-nose curvature, Ch- 
mouth corner, Ls/Li upper/lower lip midpoint, Pg-chintip, Sn-nasal base. 

TABLE 3 
Comparison of Landmark Localization Results with the State-of-the-Art on Bosphorus Dataset 

Mean of Localization Error (mm) 

Author Images Ex(L) Ex( R ) En(L) En( R) N Prn Ac(L) Ac ( R) Ch( L) Ch( R) Ls Li Pg Sn Mean 

E 
x 
p 
re 
s 
io 
n Cruesot et al. [27] 2803 6.20 4.10 5.09 4.18 6.33 4.47 4.22 4.07 4.06 8.00 7.66 5.36 8.83 15.23 6.27 

Sukno et al. [64] 2803 5.19 4.92 2.94 2.76 2.22 2.33 3.03 3.01 6.12 6.03 4.00 6.54 7.58 2.81 4.25 
BFM [40] 2920 3.58 3.62 2.77 2.65 2.17 2.90 3.80 4.63 5.86 6.01 3.99 6.77 8.12 3.59 4.32 
K3DMBO 2920 3.57 4.01 2.35 2.40 2.32 2.82 2.50 2.99 4.85 4.91 3.32 5.03 6.02 2.35 3.53 

R 
o 
ta 
ti 
o 
n 

Cruesot et al. [27] 1155 5.42 4.12 5.18 3.65 5.17 4.89 3.52 3.43 4.05 4.29 3.84 3.81 4.68 9.47 4.68 
Sukno et al. [64] 1155 4.48 4.95 2.97 3.23 3.40 4.36 3.36 3.37 3.76 3.75 3.47 5.01 7.77 4.19 4.15 

BFM [40] 1365 4.63 4.96 5.30 5.16 3.81 5.08 4.81 5.49 4.49 5.28 5.43 6.40 7.10 3.14 5.08 
K3DMBO 1365 4.84 5.09 3.31 3.85 2.68 3.19 2.73 3.20 4.53 4.91 4.13 5.84 6.22 3.80 4.14 

O 
cc 
lu 
si 
o 
n Cruesot et al. [27] 381 8.13 5.45 5.60 4.99 7.78 4.72 5.34 4.85 4.10 5.62 4.81 4.30 5.44 11.05 5.87 

Sukno et al. [64] 381 6.63 6.28 3.82 3.87 4.12 3.83 4.40 4.67 4.75 5.07 3.61 4.81 7.63 3.76 4.80 
BFM [40] 381 4.95 4.42 3.96 3.52 2.49 3.32 4.57 4.77 3.61 3.75 3.36 4.40 5.54 2.45 3.94 
K3DMBO 381 4.64 4.51 3.10 2.95 2.69 3.18 2.55 3.01 4.36 4.22 2.89 4.14 5.00 2.90 3.58 

All 

Cruesot et al. [27] 4339 6.09 4.18 5.14 4.08 6.10 4.60 4.15 3.94 4.05 6.83 6.37 4.81 7.35 13.20 5.78 
Sukno et al. [64] 4339 5.13 5.05 3.03 2.98 2.70 3.00 3.24 3.25 5.37 5.34 3.82 5.98 7.63 3.26 4.27 

BFM [40] 4666 3.93 4.03 3.41 3.34 2.68 3.57 4.07 4.86 5.34 5.65 4.37 6.50 7.64 3.38 4.48 
K3DMBO 4666 3.94 4.15 2.62 2.80 2.46 2.96 2.55 3.04 4.73 4.86 3.53 5.21 6.01 2.75 3.70 
K3DMBU 4666 4.04 4.25 3.21 3.12 2.50 3.27 3.65 4.34 5.16 5.45 4.25 6.25 7.26 3.16 4.27 
K3DMFR 4666 4.13 4.27 3.33 3.24 2.60 3.51 3.91 4.61 5.26 5.56 4.32 6.43 7.48 3.26 4.42 

Standard Deviation of Localization Error (mm) 

Images Ex(L) Ex( R ) En(L) En( R) N Prn Ac(L) Ac ( R) Ch( L) Ch( R) Ls Li Pg Sn Mean 

All 

Cruesot et al. [27] 4339 5.02 3.79 4.43 3.49 5.22 4.61 3.45 3.11 2.95 5.35 5.17 3.95 8.36 10.37 4.95 
Sukno et al. [64] 4339 4.01 3.86 2.15 2.33 2.27 2.56 2.37 2.42 5.06 4.75 3.51 6.86 7.16 2.37 3.69 

BFM [40] 4666 2.84 2.97 3.30 3.60 2.58 3.44 2.63 2.86 4.23 4.22 3.93 6.76 6.98 3.09 3.82 
K3DMBO 4666 2.69 2.82 2.06 2.23 1.63 1.61 1.45 1.59 3.13 3.03 2.99 4.23 4.28 2.23 2.57 
K3DMBU 4666 2.87 3.01 2.98 3.17 2.42 3.04 2.36 2.56 4.01 3.99 3.75 6.13 6.20 2.88 3.53 
K3DMFR 4666 2.93 3.07 3.10 3.30 2.52 3.26 2.54 2.72 4.10 4.07 3.80 6.31 6.39 2.97 3.65 

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1593 



DB45R,DB60L andDB60R. This be a very challenge dataset 
due to large yaw rotations, noisy scan and self occlusions. 
The dense correspondence model be create from 200 neutral 
expression scan of FRGCv2. Eight landmark include the 
two inner and outer eye corners, nose tip, mouth corner and 
chin tip be annotate on the mean face of K3DMFR. The 
mean and SD of landmark localization error for all 8 point be 
comparedwith the state-of-the-art in Table 5. 

6.2 Face Recognition 

Imaging of face be consider to be one of the most impor- 
tant biometrics because it can be do passively and be 
highly distinctive between individuals. 3D face recognition 
have address many shortcoming of it counterpart in the 
2D domain [67]. We consider this application apt to test the 
quality of the present algorithms. Note that our main aim 
be to evaluate our propose correspondence and model fit- 
ting algorithm a oppose to present a face recognition 
system per se. 

FRGCv2 Dataset. We follow the FRGCv2 protocol [24] of 
face recognition and include only one scan of each individ- 
ual (466 in total) in the gallery. To demonstrate the effective- 
ness of our K3DM Augmentation algorithm, we construct a 
dense correspondence model on the first available neutral 
scan of the first 200 identity of FRGCv2 dataset. The con- 
structed model be then augment with first available neu- 
tral scan of the remain 266 identity of the database 
use Algorithm 2. The probe set consists of the remain 
(3,541) scan of all identities. Note that there be only one 
scan per identity for 56 individual in the dataset. All of 
these identity appear only in the gallery. The complete 
dataset be further classify into “neutral” and “non- 
neutral” expression subclass follow the protocol out- 
line in [44] to evaluate the effect of expression on 
deformable model fitting and face recognition. 

We employ a holistic and region base approach to 
model fitting and face recognition. It be well know that 
the generalization of a model can be increase by divid- 
ing face into independent subregions that be morph 

independently [29]. This technique have be use exten- 
sively for face recognition [9], [67] and recently for match- 
ing offspring to their parent [68]. We also use this 
approach and perform face recognition by morph the 
complete face a well a the eye and nose region We 
define these region on the mean face which be sufficient 
to transfer the information to all the face in the dense 
correspondence model. 

The full K3DMFR and the eye and nose model be sepa- 
rately morph and fit to each query face in the probe to 
obtain model parameter (a-Step 12 in Algorithm 1). Next, 
the parameter from the whole face and the region be 
concatenate to form the feature vector for face recognition. 
We then perform feature selection use the GEFS algo- 
rithm [69], [70] on the training data set of FRGCv2 contain- 
ing 953 facial scans. Note that these scan be not use in 
test the face recognition algorithm. The select feature 
of each query face be match with those of the gallery 
face in the model. The query face be assign the identity of 
the gallery face with which it have the small distance 

df ¼ co �1 eaaTMeaaQkeaaT 
M 
k2keaaQk2, where eaaM be the select feature of 

each face in K3DM and eaaQ be the select feature of the 
query face. 

Fig. 14 show the process of model fitting in PCA space. 
The dense correspondence model be iteratively fit on the 
query face, which in the figure be an extreme expression scan 
of the first identity. The model fitting start from the mean 
face and in each iteration the fit query model traverse 
closer to it gallery face in the PCA space. Face recognition be 
perform when the fitting residual error �f be less than 10 

�5. 
Figs. 13a and b show the result CMC and ROC curves. 
Rank-1 identification rate for neutral probe be 99.85 percent 
while 100 percent accuracy be achieve at Rank-8. In the 
more difficult scenario of neutral versus non-neutral, the 
Rank-1 identification rate be 96.3 percent. A similar trend be 
observe in the verification rate at 0.1 percent FAR. Table 6 

TABLE 4 
Comparison of Landmark Localization Results (Mean � SD) with the State-of-the-Art on BU3DFE Dataset 

Author Images Ex(L) En(L) N Ex( R ) En( R) Prn Ac Ch Ls Li Mean 

Nair et al. [8] 2350 - 12.1 - - 11.9 8.8 - - - - 10.9 
Segundo et al. [37] 2500 - 6.3 � 4.8 - - 6.3 � 5.0 1.9 � 1.1 6.6 � 3.4 - - - 4.4� 3.5 
Salazar et al. [35] 350 9.6 � 6.1 6.8 � 4.5 - 8.5 � 5.8 6.1 � 4.2 5.9 � 2.7 6.8 � 3.2 - - - 5.9� 4.3 
Gilani et al. [28] 2500 4.4 � 2.7 4.8 � 2.6 4.5 � 2.7 4.4 � 2.7 3.3 � 2.7 2.9 � 2.0 4.3 � 2.7 5.7 � 3.7 4.2 � 2.7 6.9 � 6.3 3.7� 3.1 
K3DMBU 2500 3.8 � 2.2 2.2 � 1.5 2.9 � 2.1 3.3 � 2.2 2.4 � 1.6 2.5 � 1.7 2.3 � 1.6 4.6 � 3.3 3.6 � 2.3 6.4 � 6.1 2.8� 2.5 
K3DMFR 2500 4.0 � 2.4 2.8 � 1.6 4.3 � 2.6 3.6 � 2.4 2.7 � 1.7 2.6 � 1.8 2.9 � 1.8 5.4 � 3.5 3.8 � 2.4 7.0 � 6.6 3.1� 2.7 

TABLE 5 
Comparative Landmark Localisation Results 

(mm) on UND Side Pose Scans 

Database DB45L DB45R DB60L DB60R 

Yaw Est [26] �45� � 9� 44� � 8� �59� � 8� 57� � 7� 
# Scans 118 118 87 87 
Passalis et al. [18] 6.02 � 2.45 5.83 � 2.49 6.08 � 2.53 5.87 � 2.4 
Perakis et al. [26] 4.75 � 1.91 5.03 �1.92 5.30 �2.49 4.95 � 1.80 
K3DMFR 4.04 � 1.77 4.31 � 1.90 4.36 � 2.25 4.24 � 1.28 

Fig. 13. (Left) ROC curve for identification and (Right) verification task 
on FRGCv2 database use our dense correspondence model and fitting 
algorithms. 

1594 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018 



compare our algorithm with the state-of-the-art. In most 
cases, our result be good than the state-of-the-art depict 
the high quality of the dense correspondencemodel. 

Bosphorus Dataset. Experiments be perform on the 
more versatile Bosphorus dataset to demonstrate the 
expression, pose and occlusion invariant face recognition 
capability of our propose model. K3DMBO be form 
from the first available neutral scan of each identity in the 
dataset and a holistic approach to face recognition be 
adopted. We follow the model fitting and parameter match- 
ing technique a mention for FRGCv2. Comparative 
result be give in Table 7. Our propose technique signifi- 
cantly outperforms the state-of-the-art in pose invariant 
face recognition, while at the same time it handle expres- 
sion and occlusions. 

UND Ear Dataset. We perform face recognition experi- 
ments on this dataset to demonstrate the ability of K3DM to 
handle pose variation and self occlusions. The dataset be 
divide into three subset follow the protocol set by [18]. 
UND00LR contains 466 subject of FRGCv2 in the gallery. 

Two 45 degree side scan each (left and right) for 39 sub- 
jects and two 60 degree side scan each (left and right) for 
32 subject make the probe set. These subject be common 
between FRGCv2 and UND Ear databases. UND45LR be 
compose of 45 degree side scan from 118 subjects. The 
K3DMFR make from 200 scan be fit on the left side scan 
to get the gallery parameter and then fit to the right 
side scan to get the probe parameters. A similar protocol be 
follow for UND60LR which contains 60 degree side 
scan from 87 subjects. Comparative Rank-1 recognition 
result be give in Table 8. Note that while Smeets 
et al. [45] report > 98 percent face recognition result on 
the side pose scan of this dataset, their performance on 
pose variation in the Bosphorus dataset be significantly 
low at 84.2 percent. 

Cross Domain Face Recognition. To compare K3DM with 
the state-of-the-art Basel Face Model(BFM) [40] we perform 
cross domain face recognition experiment on FRGCv2 and 
Bosphorus datasets a they include all the challenge of 
expressions, occlusion and pose variation. For FRGCv2 we 
use K3DMBO (created from 105 neutral scan of Bosphorus 
database) and K3DMBU (created from 100 neutral and 100 
angry level-1 scans) while for Bosphorus dataset we use 
K3DMFR and K3DMBU . All three model be fit to each 
scan in FRGCv2 and Bosphorus datasets. The model param- 
eters of the first neutral scan of each identity in each data- 
base be use a gallery features. Table 9 detail the Rank-1 
recognition result from this experiment which show that 
K3DM outperforms the BFM. 

Fig. 14. Iterative model fitting. The 466 FRGCv2 identity be show a 
red star in the first three PC space. The model be morph iteratively 
into the query face until the residual error be negligible. Notice how the fit- 
ting process take the query face through a non-linear path (inset image) 
and remove the extreme facial expression to generate it equivalent 
neutral expression model. 

TABLE 6 
Comparison of 3D Face Recognition Results with the 
State-of-the-Art in Terms of Rank-1 Identification Rate 

(I-Rate) and Verification Rate (V-Rate) at 0.1 Percent FAR 

Author Neutral Non-neutral All 

I-Rate V-Rate I-Rate V-Rate I-Rate V-Rate 

Mian et al. [44] 99.4% 99.9% 92.1% 96.6% 96.1% 98.6% 
Kakadiaris et al. [43] - 99.0% - 95.6% 97.0% 97.3% 
Al-Osaimi et al. [71] 97.6% 98.4% 95.2% 97.8% 96.5% 98.1% 
Queirolo et al. [72] - 99.5% - 94.8% 98.4% 96.6% 
Drira et al. [73] 99.2% - 96.8% - 97.7% 97.1% 
Smeets et al. [45] - - - - 89.6% 79.0% 
Li et al. [47] - - - - 96.3% - 
K3DMFR 99.9% 99.9% 96.9% 96.6% 98.5% 98.7% 

TABLE 7 
Comparison of Rank-1 Recognition Results (in Percentage) with the State-of-the-Art on Bosphorus Dataset 

Author 

Expressions Poses Occlusions 

All 4,543AU Expr All YR< 90 YR90 PR CR All Eye Mouth Glasses Hair All 

2,150 647 2,797 525 210 419 211 1,365 105 105 104 67 381 

Alyz et al. [74] - - - - - - - - 93.6 93.6 97.8 89.6 93.6 - 
Colombo et al. [75] - - - - - - - - 91.1 74.7 94.2 90.4 87.6 - 
Drira et al. [73] - - - - - - - - 97.1 78.0 94.2 81.0 87.0 - 
Berretti et al. [46] - - 95.7 81.6 45.7 98.3 93.4 88.6 - - - - 93.2 93.4 
Smeetset al. [45] - - 97.7 - 24.3 - - 84.2 - - - - - 93.7 
Li et al. [47] 99.2 96.6 98.8 84.1 47.1 99.5 99.1 91.1 100.0 100.0 100.0 95.5 99.2 96.6 
K3DMBO 99.0 96.7 98.5 99.8 95.2 100.0 99.1 99.0 99.0 96.1 100.0 97.3 98.1 98.6 

AU=Action Units; YR=Yaw Rotation; PR= Pitch Rotation; CR= Cross Rotation. 

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1595 



7 DISCUSSION AND CONCLUSIONS 

We have propose an algorithm that simultaneously estab- 
lishes dense correspondence between a large number of 3D 
faces. Based on the dense correspondences, a deformable 
face model be constructed. We also propose morphable 
model fitting and update algorithm that be useful for 
landmark identification and face recognition. Thorough 
experiment be perform on synthetic and real 3D faces. 
Comparison with exist state-of-the-art show that our 
algorithm consistently achieves good or comparable perfor- 
mance on both the task on all datasets. It be interest to 
note that while the face recognition algorithm propose by 
Li et al. [47] performs well on Bosphorus database, it do 
not fare that well on FRGCv2. Similar trend can be observe 
in case of Smeets et al. [45] for face recognition and 
Sukno et al. [64] for landmark localization. To the best of our 
knowledge this be the first paper that have report consistent 
comparable result on a variety of application on four 
public datasets. 

Although the dense correspondence model assumes 
frontal and neutral pose scans, for landmark localization 
and face recognition it demonstrates robustness to occlusion 
a well a pose and expression variation during the fitting 
process. Hence the three propose algorithm present a uni- 
fied solution to a variety of application under expression, 
occlusion and pose variation. The model can handle pose 
variation up to �90�. 

With regard to the computational complexity, it may be 
note that the model building process have to be do off- 
line. The algorithm iterates over geodesic patch between 
vertex for each image. Building a dense correspondence 
model on 105 identity of Bosphorus database in approxi- 
mately 30 iteration take over 48 hour on a Core–i7 
machine with 8 GB RAM use MATLAB. However, the 
model fitting process on an unseen face take less than 
seven seconds. 

ACKNOWLEDGMENTS 

This research be support by ARC Discovery Grants 
DP110102399 and DP160101458. I. Reid gratefully acknowl- 
edge the financial support of the Australian Research 
Council through grant CE140100016 and FL130100102. 

REFERENCES 
[1] O. Van Kaick , H. Zhang, G. Hamarneh, and D. Cohen-Or, “A 

survey on shape correspondence,” Comput. Graph. Forum, vol. 30, 
no. 6, pp. 1681–1707, 2011. 

[2] V. Jain, H. Zhang, and O. van Kaick, “Non-rigid spectral corre- 
spondence of triangle meshes,” Int. J. Shape Model., vol. 13 no. 1, 
pp. 101–124, 2007. 

[3] W. Chang and M. Zwicker, “Automatic registration for articu- 
lated shapes,” Comput. Graph. Forum, vol. 27, no. 5, pp. 1459– 
1468, 2008. 

[4] H. Zhang, A. Sheffer, D. Cohen, Q. Zhou, O. van Kaick, and 
A. Tagliasacchi, “Deformation-driven shape correspondence,” 
Comput. Graph. Forum, vol. 27, no. 5, pp. 1431–1439, 2008. 

[5] S. Z. Gilani, K. Rooney, F. Shafait, M. Walters, and A. Mian, 
“Geometric facial gender scoring: Objectivity of perception,” PloS 
One, vol. 9, no. 6, 2014, Art. no. e99483. 

[6] P. Hammond, “The use of 3D face shape model in dys- 
morphology,” Archives Disease Childhood, vol. 92, no. 12, pp. 1120– 
1126, 2007. 

[7] L. Farkas, “Anthropometry of the head and face in clinical 
practice,” in Anthropometry of the Head and Face, 2nd ed. Ann 
Arbor, MI, USA: Univ. Michigan, 1994, pp. 71–111. 

[8] P. Nair and A. Cavallaro, “3-D face detection, landmark localiza- 
tion, and registration use a point distribution model,” IEEE 
Trans. Multimedia, vol. 11, no. 4, pp. 611–623, Jun. 2009. 

[9] V. Blanz and T. Vetter, “Face recognition base on fitting a 3D 
morphable model,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 25, 
no. 9, pp. 1063–1074, Sep. 2003. 

[10] S. Z. Gilani, A. Mian, and P. Eastwood, “Deep, dense and accu- 
rate 3D face correspondence for generate population specific 
deformable models,” Pattern Recognit., vol. 69, pp. 238–250, 
2017. 

[11] R. H. Davies, C. J. Twining, T. F. Cootes, J. C. Waterton, and 
C. J. Taylor, “3D statistical shape model use direct optimisation 
of description length,” inProc. Eur. Conf. Comput. Vis., 2002, pp. 3–20. 

[12] T. Heimann and H.-P. Meinzer, “Statistical shape model for 3D 
medical image segmentation: A review,”Med. Image Anal., vol. 13, 
no. 4, pp. 543–563, 2009. 

[13] M. Alexa, “Recent advance in mesh morphing,” Comput. Graph. 
Forum, vol. 21, no. 2, pp. 173–198, 2002. 

[14] D. Aiger, N. Mitra, and D. Cohen, “4-points congruent set for 
robust pairwise surface registration,” ACM Trans. Graph., vol. 27, 
no. 3, 2008, Art. no. 85. 

[15] B. Brown and S. Rusinkiewicz, “Global non-rigid alignment of 3-D 
scans,” ACM Trans. Graph., vol. 26, no. 3, 2007, Art. no. 21. 

[16] H.Mirzaalian, G.Hamarneh, and T. Lee, “A graph-based approach 
to skin mole match incorporate template-normalized coor- 
dinates,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2009, 
pp. 2152–2159. 

[17] T. Funkhouser and P. Shilane, “Partial match of 3D shape 
with priority-driven search,” in Proc. Eurographics Symp. Geometry 
Process., 2006, vol. 256, pp. 131–142. 

[18] G. Passalis, P. Perakis, T. Theoharis, and I. A. Kakadiaris, “Using 
facial symmetry to handle pose variation in real-world 3D face 
recognition,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 10, 
pp. 1938–1951, Oct. 2011. 

[19] U. Prabhu, J. Heo, andM. Savvides, “Unconstrained pose-invariant 
face recognition use 3D generic elastic models,” IEEE Trans. 
Pattern Anal. Mach. Intell., vol. 33, no. 10, pp. 1952–1961, Oct. 2011. 

[20] V. Kraevoy and A. Sheffer, “Cross-parameterization and compati- 
ble remeshing of 3D models,” ACM Trans. Graph., vol. 23, pp. 861– 
869, 2004. 

[21] H. Hochheiser, et al., “The facebase consortium: A comprehensive 
program to facilitate craniofacial research,” Develop. Biology, 
vol. 355, no. 2, pp. 175–182, 2011. 

[22] A. J. Whitehouse, et al., “Prenatal testosterone exposure be related 
to sexually dimorphic facial morphology in adulthood,” Proc. Roy. 
Soc. B, vol. 282, no. 1816, 2015, Art. no. 7. 

TABLE 8 
Comparative of Rank-1 Recognition Results on 

Partial Faces of UND Side Pose Scans 

Database UND00LR UND45LR UND60LR 

# Scans 608 236 174 
Passalis et al. [18] 76.8% 86.4% 81.6% 
Smeets et al. [45] - 98.3% 100.0% 
K3DMFR 86.0% 95.8% 98.6% 

TABLE 9 
Comparison of Rank-1 Recognition on FRGCv2 and 
Bosphorus Datasets Using Cross Domain Models 

FRGCv2 

Method Neutral Expressions Poses Occlusions All 

BFM [40] 87.7% 65.6% - - 76.4% 
K3DMBU 92.7% 69.0% - - 80.5% 
K3DMBO 92.1% 62.9% - - 77.1% 

Bosphorus 

BFM [40] - 81.1% 86.1% 86.6% 82.7% 
K3DMFR - 85.6% 86.5% 89.3% 85.8% 
K3DMBU - 90.3% 92.8% 90.7% 90.7% 

1596 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018 



[23] S. Z. Gilani, et al., “Sexually dimorphic facial feature vary accord- 
ing to level of autistic-like trait in the general population,” J. Neu- 
rodevelopmental Disorders, vol. 7, no. 1, 2015, Art. no. 14. 

[24] P. Phillips, et al., “Overview of the face recognition grand 
challenge,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern 
Recognit., 2005, pp. 947–954. 

[25] A. Savran, et al., “Bosphorus database for 3D face analysis,” in 
Proc. Eur. Workshop Biometrics Identity Manage., 2008, pp. 47–56. 

[26] P. Perakis, G. Passalis, T. Theoharis, and I. A. Kakadiaris, “3D 
facial landmark detection under large yaw and expression var- 
iations,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 7, 
pp. 1552–1564, Jul. 2013. 

[27] C. Creusot, N. Pears, and J. Austin, “A machine-learning 
approach to keypoint detection and landmarking on 3D meshes,” 
Int. J. Comput.Vis., vol. 102, no. 1–3, pp. 146–179, 2013. 

[28] S. Z. Gilani, F. Shafait, and A. Mian, “Shape-based automatic 
detection of a large number of 3D facial landmarks,” in Proc. IEEE 
Conf. Comput. Vis. Pattern Recognit., 2015, pp. 4639–4648. 

[29] V. Blanz and T. Vetter, “A morphable model for the synthesis of 
3D faces,” in Proc. ACM Conf. Comput. Graph. Interactive Techn., 
1999, pp. 187–194. 

[30] Y. Sun, J. Paik, A. Koschan, D. Page, and M. Abidi, “Point fin- 
gerprint: A new 3D object representation scheme,” IEEE Trans. 
Syst. Man Cybern. Part B: Cybern., vol. 33, no. 4, pp. 712–717, 
Aug. 2003. 

[31] L. Yin, X. Wei, Y. Sun, J. Wang, and M. J. Rosato, “A 3D facial 
expression database for facial behavior research,” in Proc. 7th Int. 
Conf. Automatic Face Gesture Recognit., 2006, pp. 211–216. 

[32] Y. Sun and M. A. Abidi, “Surface match by 3D point’s finger- 
print,” in Proc. 8th IEEE Int. Conf. Comput. Vis., 2001, pp. 263–269. 

[33] S. Wang, Y. Wang, M. Jin, X. D. Gu, and D. Samaras, “Conformal 
geometry and it application on 3D shape matching, recognition, 
and stitching,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, 
no. 7, pp. 1209–1220, Jul. 2007. 

[34] J. Novatnack and K. Nishino, “Scale-dependent/invariant local 
3D shape descriptor for fully automatic registration of multiple 
set of range images,” in Proc. 10th Eur. Conf. Comput. Vis., 2008, 
pp. 440–453. 

[35] A. Salazar, S. Wuhrer, C. Shu, and F. Prieto, “Fully automatic 
expression-invariant face correspondence,” Mach. Vis. Appl., 
vol. 25, no. 4, pp. 859–879, 2014. 

[36] X. Lu and A. K. Jain, “Automatic feature extraction for multiview 
3D face recognition,” in Proc. 7th Int. Conf. Automatic Face Gesture 
Recognit., 2006, pp. 585–590. 

[37] M. Segundo, L. Silva, P. Bellon, and C. C. Queirolo, “Automatic 
face segmentation and facial landmark detection in range 
images,” IEEE Trans. Syst. Man Cybern. Part B: Cybern., vol. 40, 
no. 5, pp. 1319–1330, Oct. 2010. 

[38] P. Perakis, T. Theoharis, and I. A. Kakadiaris, “Feature fusion for 
facial landmark detection,” Pattern Recognit., vol. 47, no. 9, 
pp. 2783–2793, 2014. 

[39] V. Blanz, K. Scherbaum, and H.-P. Seidel, “Fitting a morphable 
model to 3D scan of faces,” in Proc. 11th IEEE Int. Conf. Comput. 
Vis., 2007, pp. 1–8. 

[40] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter, 
“A 3D face model for pose and illumination invariant face recog- 
nition,” in Proc. Int. Conf. Adv. Video Signal Based Surveillance, 2009, 
pp. 296–301. 

[41] B. Amberg, S. Romdhani, and T. Vetter, “Optimal step nonrigid 
ICP algorithm for surface registration,” in Proc. IEEE Conf. Com- 
put. Vis. Pattern Recognit., 2007, pp. 1–8. 

[42] G. Passalis, I. Kakadiaris, T. Theoharis, G. Toderici, and N. Mur- 
tuza, “Evaluation of 3D face recognition in the presence of facial 
expressions: An annotate deformable model approach,” in Proc. 
IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops, 
2005, pp. 171–171. 

[43] I. A. Kakadiaris, et al., “Three-dimensional face recognition in the 
presence of facial expressions: An annotate deformable model 
approach,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 4, 
pp. 640–649, Apr. 2007. 

[44] A. Mian, M. Bennamoun, and R. Owens, “Keypoint detection and 
local feature match for textured 3D face recognition,” Int. 
J. Comput. Vis., vol. 79, no. 1, pp. 1–12, 2008. 

[45] D. Smeets, J. Keustermans, D. Vandermeulen, and P. Suetens, 
“MeshSIFT: Local surface feature for 3D face recognition under 
expression variation and partial data,” Comput. Vis. Image Under- 
standing, vol. 117, no. 2, pp. 158–169, 2013. 

[46] S. Berretti, N. Werghi, A. Del Bimbo , and P. Pala, “Matching 3D 
face scan use interest point and local histogram descriptors,” 
Comput. Graph., vol. 37, no. 5, pp. 509–525, 2013. 

[47] H. Li, D. Huang, J.-M. Morvan, Y. Wang, and L. Chen, “Towards 
3D face recognition in the real: A registration-free approach use 
fine-grained match of 3D keypoint descriptors,” Int. J. Comput. 
Vis., vol. 113, no. 2, pp. 128–142, 2014. 

[48] F. L. Bookstein, “Principal warps: Thin-plate spline and the 
decomposition of deformations,” IEEE Trans. Pattern Anal. Mach. 
Intell., vol. 11, no. 6, pp. 567–585, Jun. 1989. 

[49] D. Rueckert, L. Sonoda, C. Hayes, D. L. G. Hill, M. O. Leach, and 
D. J. Hawkes, “Nonrigid registration use free-form deforma- 
tions: Application to breast MR images,” IEEE Trans. Med. Imag., 
vol. 18, no. 8, pp. 712–721, Aug. 1999. 

[50] D.-J. Kroon, “Finite iterative closest point,” MATLAB Central 
File Exchange, 2009, https://au.mathworks.com/matlabcentral/ 
fileexchange/24301-finite-iterative-closest-point 

[51] Y. Guo, F. Sohel, M. Bennamoun, M. Lu, and J. Wan, “Rotational 
projection statistic for 3D local surface description and object rec- 
ognition,” Int. J. Comput. Vis., vol. 105, no. 1, pp. 63–86, 2013. 

[52] F. Tombari, S. Salti, and L. Di Stefano, “Unique signature of histo- 
gram for local surface description,” in Proc. 11th Eur. Conf. 
Comput. Vis., 2010, pp. 356–369. 

[53] R. C. Gonzalez and R. E. Woods, Digital Image Processing, 2nd ed. 
Upper Saddle River, NJ USA: Prentice Hall, 2002. 

[54] A. Mian, M. Bennamoun, and R. Owens, “On the repeatability and 
quality of keypoints for local feature-based 3D object retrieval 
from clutter scenes,” Int. J. Comput. Vis., vol. 89, no. 2/3, 
pp. 348–361, 2010. 

[55] B. C. Munsell, A. Teml, and S. Wang, “Fast multiple shape corre- 
spondence by pre-organizing shape instances,” in Proc. IEEE Conf. 
Comput. Vis. Pattern Recognit., 2009, pp. 840–847. 

[56] J. A. Sethian, “Evolution, implementation, and application of level 
set and fast march method for advance fronts,” J. Comput. 
Physics, vol. 169, no. 2, pp. 503–555, 2001. 

[57] G. Peyr�e, “The numerical tour of signal processing-advanced 
computational signal and image processing,” IEEE Comput. Sci. 
Eng., vol. 13, no. 4, pp. 94–97, Jul./Aug. 2011. 

[58] J. L. Bentley, “Multidimensional binary search tree use for asso- 
ciative searching,” Commun. ACM, vol. 18, no. 9, 1975, Art. no. 509. 

[59] A. Todorov, S. Baron, and N. Oosterhof, “Evaluating face trust- 
worthiness: A model base approach,” Social Cognitive Affect. Neu- 
roscience, vol. 3, no. 2, pp. 119–127, 2008. 

[60] N. N. Oosterhof and A. Todorov, “The functional basis of face 
evaluation,” Proc. Nat. Acadmey Sci. United States America, vol. 105, 
no. 32, pp. 11087–11092, 2008. 

[61] P. Yan and K. Bowyer, “Empirical evaluation of advanced ear bio- 
metrics,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Rec- 
ognit. Workshops, 2005, pp. 41–41. 

[62] P. Yan and K. W. Bowyer, “An automatic 3D ear recognition sys- 
tem,” in Proc. 3rd Int. Symp. 3D Data Process. Vis. Transmiss., 2006, 
vol. 6, pp. 326–333. 

[63] B. C. Munsell, P. Dalal, and S. Wang, “Evaluating shape corre- 
spondence for statistical shape analysis: A benchmark study,” 
IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 11, pp. 2023– 
2039, Nov. 2008. 

[64] F. M. Sukno, J. L. Waddington, and P. F. Whelan, “3-D facial land- 
mark localization with asymmetry pattern and shape regression 
from incomplete local features,” IEEE Trans. Cybern., vol. 45, no. 9, 
pp. 1717–1730, Sep. 2015. 

[65] S. Z. Gilani, F. Shafait, and A. Mian, “Biologically significant facial 
landmarks: How significant be they for gender classification?” in 
Proc. Int. Conf. Digit. Image Comput.: Techn. Appl., 2013, pp. 1–8. 

[66] P. Szeptycki, M. Ardabilian, and L. Chen, “A coarse-to-fine curva- 
ture analysis-based rotation invariant 3D face landmarking,” in 
Proc. IEEE 3rd Int. Conf. Biometrics: Theory Appl. Syst., 2009, pp. 1–6. 

[67] A. Mian, M. Bennamoun, and R. Owens, “An efficient multimodal 
2D-3D hybrid approach to automatic face recognition,” IEEE 
Trans. Pattern Anal. Mach. Intell., vol. 29, no. 11, pp. 1927–1943, 
Nov. 2007. 

[68] A. Dehghan, E. Ortiz, R. Villegas, and M. Shah, “Who do I look 
like? Determining parent-offspring resemblance via gate 
autoencoders,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. 
Workshops, 2014, pp. 171–171. 

[69] S. Z. Gilani and A. Mian, “Perceptual difference between men 
and women: A 3D facial morphometric perspective,” in Proc. 22nd 
Int. Conf. Pattern Recognit., 2014, pp. 2413–2418. 

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1597 

https://au.mathworks.com/matlabcentral/fileexchange/24301-finite-iterative-closest-point 
https://au.mathworks.com/matlabcentral/fileexchange/24301-finite-iterative-closest-point 


[70] S. Z. Gilani, F. Shafait, and A. Mian, “Gradient base efficient fea- 
ture selection,” in Proc. IEEE Winter Conf. Appl. Comput. Vis., 2014, 
pp. 191–197. 

[71] F. Al-Osaimi, M. Bennamoun, and A. Mian, “An expression defor- 
mation approach to non-rigid 3D face recognition,” Int. J. Comput. 
Vis., vol. 81, no. 3, pp. 302–316, 2009. 

[72] C. Queirolo, L. Silva, O. Bellon, andM. Segundo, “3D face recogni- 
tion use simulated anneal and the surface interpenetration 
measure,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 2, 
pp. 206–219, Feb. 2010. 

[73] H. Drira, B. Ben Amor, A. Srivastava, M. Daoudi, and R. Slama, 
“3D face recognition under expressions, occlusions, and pose var- 
iations,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 9, 
pp. 2270–2283, Sep. 2013. 

[74] N. Alyuz, B. Gokberk, and L. Akarun, “A 3D face recognition sys- 
tem for expression and occlusion invariance,” in Proc. IEEE 2nd 
Int. Conf. Biometrics: Theory Appl. Syst., 2008, pp. 1–7. 

[75] A. Colombo, C. Cusano, and R. Schettini, “Three-dimensional 
occlusion detection and restoration of partially occlude faces,” 
J. Math. Imag. Vis., vol. 40, no. 1, pp. 105–119, 2011. 

[76] J. D Erico, “Surface fitting use gridfit,” MATLAB Central 
File Exchange, 2008, https://au.mathworks.com/matlabcentral/ 
fileexchange/8998-surface-fitting-using-gridfit 

[77] J. Booth, A. Roussos, S. Zafeiriou and D. Dunaway, “A 3D morph- 
able model learnt from 10,000 faces,” in Proc. IEEE Conf. Comput. 
Vis. Pattern Recognit., 2016, pp. 5543–5552. 

[78] H. Li, R. W. Sumner, and M. Pauly, “Global correspondence opti- 
mization for non-rigid registration of depth scans,” Comput. 
Graph. Forum, vol. 27, no. 5, pp. 1421–1430, 2008, 

[79] H. Li, B. Adams, L. J. Guibas, and M. Pauly, “Robust single-view 
geometry and motion reconstruction,” ACM Trans. Graph., vol. 28, 
no. 5, 2009, Art. no. 175. 

[80] T. Bolkart and S. Wuhrer, “A robust multilinear model learn 
framework for 3D faces,” in Proc. IEEE Conf. Comput. Vis. Pattern 
Recognit., 2016, pp. 4911–4919. 

[81] V. G. Kim, Y. Lipman, and T. Funkhouser, “Blended intrinsic 
maps,” ACM Trans. Graph., vol. 30, no. 4, 2011, Art. no. 79. 

Syed Zulqarnain Gilani receive the MS degree 
in EE from the National University of Sciences 
and Technology (NUST), Pakistan, in 2009 and 
secure the President’s Gold Medal. He receive 
the PhD degree from the University of Western 
Australia. His research interest include 3D facial 
morphometrics with application to syndrome 
delineation and machine learning. 

Ajmal Mian be an Associate Professor of Com- 
puter Science at The University of Western Aus- 
tralia. He have receive several award include 
the West Australian Early Career Scientist of the 
Year Award, the Vice-chancellors Mid-career 
Research Award and the Outstanding Young 
Investigator Award. He have receive two presti- 
gious fellowship and seven major grant from 
the Australian Research Council and the National 
Health and Medical Research Council with total 
funding of $3.0 Million. His research interest 

include computer vision, machine learning, 3D face analysis, human 
action recognition and remote sensing. 

Faisal Shafait currently work a an Associate 
Professor at School of Electrical Engineering 
and Computer Science (SEECS), NUST, Paki- 
stan. He be also an Adjunct Senior Lecturer at 
the School of Computer Science at The Univer- 
sity of Western Australia. Formerly, he be a 
Senior Researcher at the German Research 
Center for Artificial Intelligence (DFKI), Germany 
and a visit researcher at Google, California. 
He receive his Ph.D. in computer engineering 
from Kaiserslautern University of Technology, 

Germany in 2008. His research interest include machine learn and 
pattern recognition. 

Ian Reid receive the DPhil degree from the Uni- 
versity of Oxford, in 1991. He be a professor of 
computer science with the University of Adelaide. 
He have be employ in the Robotics Research 
Group, conduct research in computer vision, 
include hold an EPSRC Advanced Research 
Fellowship (1997-2000), and he have be a Uni- 
versity lecturer since 2000. In 2005, he be 
award the title of reader and in 2010 the title of 
professor. 

" For more information on this or any other compute topic, 
please visit our Digital Library at www.computer.org/publications/dlib. 

1598 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018 

https://au.mathworks.com/matlabcentral/fileexchange/8998-surface-fitting-using-gridfit 
https://au.mathworks.com/matlabcentral/fileexchange/8998-surface-fitting-using-gridfit 















<< 
/ASCII85EncodePages false 
/AllowTransparency false 
/AutoPositionEPSFiles true 
/AutoRotatePages /None 
/Binding /Left 
/CalGrayProfile (Gray Gamma 2.2) 
/CalRGBProfile (sRGB IEC61966-2.1) 
/CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2) 
/sRGBProfile (sRGB IEC61966-2.1) 
/CannotEmbedFontPolicy /Warning 
/CompatibilityLevel 1.4 
/CompressObjects /Off 
/CompressPages true 
/ConvertImagesToIndexed true 
/PassThroughJPEGImages true 
/CreateJobTicket false 
/DefaultRenderingIntent /Default 
/DetectBlends true 
/DetectCurves 0.0000 
/ColorConversionStrategy /sRGB 
/DoThumbnails true 
/EmbedAllFonts true 
/EmbedOpenType false 
/ParseICCProfilesInComments true 
/EmbedJobOptions true 
/DSCReportingLevel 0 
/EmitDSCWarnings false 
/EndPage -1 
/ImageMemory 1048576 
/LockDistillerParams true 
/MaxSubsetPct 100 
/Optimize true 
/OPM 0 
/ParseDSCComments false 
/ParseDSCCommentsForDocInfo true 
/PreserveCopyPage true 
/PreserveDICMYKValues true 
/PreserveEPSInfo false 
/PreserveFlatness true 
/PreserveHalftoneInfo true 
/PreserveOPIComments false 
/PreserveOverprintSettings true 
/StartPage 1 
/SubsetFonts false 
/TransferFunctionInfo /Remove 
/UCRandBGInfo /Preserve 
/UsePrologue false 
/ColorSettingsFile () 
/AlwaysEmbed [ true 
/Algerian 
/Arial-Black 
/Arial-BlackItalic 
/Arial-BoldItalicMT 
/Arial-BoldMT 
/Arial-ItalicMT 
/ArialMT 
/ArialNarrow 
/ArialNarrow-Bold 
/ArialNarrow-BoldItalic 
/ArialNarrow-Italic 
/ArialUnicodeMS 
/BaskOldFace 
/Batang 
/Bauhaus93 
/BellMT 
/BellMTBold 
/BellMTItalic 
/BerlinSansFB-Bold 
/BerlinSansFBDemi-Bold 
/BerlinSansFB-Reg 
/BernardMT-Condensed 
/BodoniMTPosterCompressed 
/BookAntiqua 
/BookAntiqua-Bold 
/BookAntiqua-BoldItalic 
/BookAntiqua-Italic 
/BookmanOldStyle 
/BookmanOldStyle-Bold 
/BookmanOldStyle-BoldItalic 
/BookmanOldStyle-Italic 
/BookshelfSymbolSeven 
/BritannicBold 
/Broadway 
/BrushScriptMT 
/CalifornianFB-Bold 
/CalifornianFB-Italic 
/CalifornianFB-Reg 
/Centaur 
/Century 
/CenturyGothic 
/CenturyGothic-Bold 
/CenturyGothic-BoldItalic 
/CenturyGothic-Italic 
/CenturySchoolbook 
/CenturySchoolbook-Bold 
/CenturySchoolbook-BoldItalic 
/CenturySchoolbook-Italic 
/Chiller-Regular 
/ColonnaMT 
/ComicSansMS 
/ComicSansMS-Bold 
/CooperBlack 
/CourierNewPS-BoldItalicMT 
/CourierNewPS-BoldMT 
/CourierNewPS-ItalicMT 
/CourierNewPSMT 
/EstrangeloEdessa 
/FootlightMTLight 
/FreestyleScript-Regular 
/Garamond 
/Garamond-Bold 
/Garamond-Italic 
/Georgia 
/Georgia-Bold 
/Georgia-BoldItalic 
/Georgia-Italic 
/Haettenschweiler 
/HarlowSolid 
/Harrington 
/HighTowerText-Italic 
/HighTowerText-Reg 
/Impact 
/InformalRoman-Regular 
/Jokerman-Regular 
/JuiceITC-Regular 
/KristenITC-Regular 
/KuenstlerScript-Black 
/KuenstlerScript-Medium 
/KuenstlerScript-TwoBold 
/KunstlerScript 
/LatinWide 
/LetterGothicMT 
/LetterGothicMT-Bold 
/LetterGothicMT-BoldOblique 
/LetterGothicMT-Oblique 
/LucidaBright 
/LucidaBright-Demi 
/LucidaBright-DemiItalic 
/LucidaBright-Italic 
/LucidaCalligraphy-Italic 
/LucidaConsole 
/LucidaFax 
/LucidaFax-Demi 
/LucidaFax-DemiItalic 
/LucidaFax-Italic 
/LucidaHandwriting-Italic 
/LucidaSansUnicode 
/Magneto-Bold 
/MaturaMTScriptCapitals 
/MediciScriptLTStd 
/MicrosoftSansSerif 
/Mistral 
/Modern-Regular 
/MonotypeCorsiva 
/MS-Mincho 
/MSReferenceSansSerif 
/MSReferenceSpecialty 
/NiagaraEngraved-Reg 
/NiagaraSolid-Reg 
/NuptialScript 
/OldEnglishTextMT 
/Onyx 
/PalatinoLinotype-Bold 
/PalatinoLinotype-BoldItalic 
/PalatinoLinotype-Italic 
/PalatinoLinotype-Roman 
/Parchment-Regular 
/Playbill 
/PMingLiU 
/PoorRichard-Regular 
/Ravie 
/ShowcardGothic-Reg 
/SimSun 
/SnapITC-Regular 
/Stencil 
/SymbolMT 
/Tahoma 
/Tahoma-Bold 
/TempusSansITC 
/TimesNewRomanMT-ExtraBold 
/TimesNewRomanMTStd 
/TimesNewRomanMTStd-Bold 
/TimesNewRomanMTStd-BoldCond 
/TimesNewRomanMTStd-BoldIt 
/TimesNewRomanMTStd-Cond 
/TimesNewRomanMTStd-CondIt 
/TimesNewRomanMTStd-Italic 
/TimesNewRomanPS-BoldItalicMT 
/TimesNewRomanPS-BoldMT 
/TimesNewRomanPS-ItalicMT 
/TimesNewRomanPSMT 
/Times-Roman 
/Trebuchet-BoldItalic 
/TrebuchetMS 
/TrebuchetMS-Bold 
/TrebuchetMS-Italic 
/Verdana 
/Verdana-Bold 
/Verdana-BoldItalic 
/Verdana-Italic 
/VinerHandITC 
/Vivaldii 
/VladimirScript 
/Webdings 
/Wingdings2 
/Wingdings3 
/Wingdings-Regular 
/ZapfChanceryStd-Demi 
/ZWAdobeF 
] 
/NeverEmbed [ true 
] 
/AntiAliasColorImages false 
/CropColorImages true 
/ColorImageMinResolution 150 
/ColorImageMinResolutionPolicy /OK 
/DownsampleColorImages true 
/ColorImageDownsampleType /Bicubic 
/ColorImageResolution 150 
/ColorImageDepth -1 
/ColorImageMinDownsampleDepth 1 
/ColorImageDownsampleThreshold 1.50000 
/EncodeColorImages true 
/ColorImageFilter /DCTEncode 
/AutoFilterColorImages false 
/ColorImageAutoFilterStrategy /JPEG 
/ColorACSImageDict << 
/QFactor 0.76 
/HSamples [2 1 1 2] /VSamples [2 1 1 2] 
>> 
/ColorImageDict << 
/QFactor 0.40 
/HSamples [1 1 1 1] /VSamples [1 1 1 1] 
>> 
/JPEG2000ColorACSImageDict << 
/TileWidth 256 
/TileHeight 256 
/Quality 15 
>> 
/JPEG2000ColorImageDict << 
/TileWidth 256 
/TileHeight 256 
/Quality 15 
>> 
/AntiAliasGrayImages false 
/CropGrayImages true 
/GrayImageMinResolution 150 
/GrayImageMinResolutionPolicy /OK 
/DownsampleGrayImages true 
/GrayImageDownsampleType /Bicubic 
/GrayImageResolution 300 
/GrayImageDepth -1 
/GrayImageMinDownsampleDepth 2 
/GrayImageDownsampleThreshold 1.50000 
/EncodeGrayImages true 
/GrayImageFilter /DCTEncode 
/AutoFilterGrayImages false 
/GrayImageAutoFilterStrategy /JPEG 
/GrayACSImageDict << 
/QFactor 0.76 
/HSamples [2 1 1 2] /VSamples [2 1 1 2] 
>> 
/GrayImageDict << 
/QFactor 0.40 
/HSamples [1 1 1 1] /VSamples [1 1 1 1] 
>> 
/JPEG2000GrayACSImageDict << 
/TileWidth 256 
/TileHeight 256 
/Quality 15 
>> 
/JPEG2000GrayImageDict << 
/TileWidth 256 
/TileHeight 256 
/Quality 15 
>> 
/AntiAliasMonoImages false 
/CropMonoImages true 
/MonoImageMinResolution 1200 
/MonoImageMinResolutionPolicy /OK 
/DownsampleMonoImages true 
/MonoImageDownsampleType /Bicubic 
/MonoImageResolution 600 
/MonoImageDepth -1 
/MonoImageDownsampleThreshold 1.50000 
/EncodeMonoImages true 
/MonoImageFilter /CCITTFaxEncode 
/MonoImageDict << 
/K -1 
>> 
/AllowPSXObjects false 
/CheckCompliance [ 
/None 
] 
/PDFX1aCheck false 
/PDFX3Check false 
/PDFXCompliantPDFOnly false 
/PDFXNoTrimBoxError true 
/PDFXTrimBoxToMediaBoxOffset [ 
0.00000 
0.00000 
0.00000 
0.00000 
] 
/PDFXSetBleedBoxToMediaBox true 
/PDFXBleedBoxToTrimBoxOffset [ 
0.00000 
0.00000 
0.00000 
0.00000 
] 
/PDFXOutputIntentProfile (None) 
/PDFXOutputConditionIdentifier () 
/PDFXOutputCondition () 
/PDFXRegistryName () 
/PDFXTrapped /False 

/CreateJDFFile false 
/Description << 
/CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002> 
/CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002> 
/DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e> 
/DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e> 
/ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e> 
/FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e> 
/ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 5.0 e versioni successive.) 
/JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002> 
/KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e> 
/NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.) 
/NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e> 
/PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e> 
/SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e> 
/SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e> 
/ENU (Use these setting to create PDFs that match the "Suggested" setting for PDF Specification 4.0) 
>> 
>> setdistillerparams 
<< 
/HWResolution [600 600] 
/PageSize [612.000 792.000] 
>> setpagedevice 

