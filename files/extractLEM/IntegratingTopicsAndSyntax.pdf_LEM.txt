




































composite4.dvi 


Integrating Topics and Syntax 

Thomas L. Griffiths Mark Steyvers 
gruffydd@mit.edu msteyver@uci.edu 

Massachusetts Institute of Technology University of California, Irvine 
Cambridge, MA 02139 Irvine, CA 92614 

David M. Blei Joshua B. Tenenbaum 
blei@cs.berkeley.edu jbt@mit.edu 

University of California, Berkeley Massachusetts Institute of Technology 
Berkeley, CA 94720 Cambridge, MA 02139 

Abstract 

Statistical approach to language learn typically focus on either 
short-range syntactic dependency or long-range semantic dependency 
between words. We present a generative model that us both kind of 
dependencies, and be capable of simultaneously find syntactic class 
and semantic topic despite have no knowledge of syntax or seman- 
tic beyond statistical dependency. This model be competitive on task 
like part-of-speech tag and document classification with model that 
exclusively use short- and long-range dependency respectively. 

1 Introduction 

A word can appear in a sentence for two reasons: because it serf a syntactic function, or 
because it provide semantic content. Words that play different role be treat differently 
in human language processing: function and content word produce different pattern of 
brain activity [1], and have different developmental trend [2]. So, how might a language 
learner discover the syntactic and semantic class of words? Cognitive scientist have 
show that unsupervised statistical method can be use to identify syntactic class [3] 
and to extract a representation of semantic content [4], but none of these method capture 
the interaction between function and content words, or even recognizes that these role 
be distinct. Here we explore how statistical learning, with no prior knowledge of either 
syntax or semantics, can discover the difference between function and content word and 
simultaneously organize word into syntactic class and semantic topics. 

Our approach relies on the different kind of dependency between word produce by 
syntactic and semantic constraints. Syntactic constraint result in relatively short-range de- 
pendencies, span several word but not go beyond the limit of a sentence. Seman- 
tic constraint result in long-range dependencies: different sentence within a document 
be likely to have similar content, and use similar words. We present an algorithm that cap- 
tures the interaction between short- and long-range dependencies, base upon a generative 
model for text in which a hidden Markov model (HMM) determines when to emit a word 
from a topic model. The different capacity of the two component of the model result in a 
factorization of a sentence into function words, handle by the HMM, and content words, 
handle by the topic model. Each component divide word into finer group accord 
to a different criterion: the function word be divide into syntactic classes, and the con- 

In: Advances in Neural Information Processing Systems, 17 



tent word be divide into semantic topics. In addition to produce clean syntactic and 
semantic class and identify function and content words, our composite model be com- 
petitive in quantitative tasks, such a part-of-speech tag and document classification, 
with model specialized to detect only one kind of dependency. 

The plan of the paper be a follows. First, we introduce the approach, consider the 
general question of how syntactic and semantic generative model might be combined, 
and argue that a composite model be necessary to capture the different role that word 
can play in a document. We then define a generative model of this form, and describe 
a Markov chain Monte Carlo algorithm for inference in this model. Finally, we present 
result illustrate the quality of the recover syntactic class and semantic topics. 

2 Combining syntactic and semantic generative model 

A probabilistic generative model specifies a simple stochastic procedure by which data 
might be generated, usually make reference to unobserved random variable that express 
latent structure. Once defined, this procedure can be invert use statistical inference, 
compute distribution over latent variable condition on a dataset. Such an approach be 
appropriate for model language, where word be generate from the latent structure of 
the speaker’s intentions, and be widely use in statistical natural language processing (e.g., 
[5]). 

Probabilistic model of language be typically driven exclusively by either short-range or 
long-range dependency between words. HMMs and probabilistic context-free grammar 
(e.g., [5]) generate document purely base on syntactic relation among unobserved word 
classes, while “bag-of-words” model like naive Bayes or topic model (e.g., [6]) generate 
document base on semantic correlation between words, independent of word order. By 
consider only one of the factor influence the word that appear in documents, these 
approach be force to ass all word on a single criterion: an HMM will group noun 
together, a they play the same syntactic role even though they vary across contexts, and a 
topic model will assign determiner to topics, even though they bear little semantic content. 

A major advantage of generative model be modularity. A generative model for text spec- 
ifies a probability distribution over word in term of other probability distribution over 
words, and different model be thus easily combined. We can produce a model that ex- 
press both the short- and long-range dependency of word by combine two model 
that be each sensitive to one kind of dependency. However, the form of combination must 
be chosen carefully. In a mixture of syntactic and semantic models, each word would ex- 
hibit either short-range or long-range dependencies, while in a product of model (e.g. [7]), 
each word would exhibit both short-range and long-range dependencies. Consideration of 
the structure of language reveals that neither of these model be appropriate. In fact, only 
a subset of word – the content word – exhibit long-range semantic dependencies, while 
all word obey short-range syntactic dependencies. This asymmetry can be capture in a 
composite model, where we replace one of the probability distribution over word use in 
the syntactic model with the semantic model. This allows the syntactic model to choose 
when to emit a content word, and the semantic model to choose which word to emit. 

2.1 A composite model 

We will explore a simple composite model, in which the syntactic component be an HMM 
and the semantic component be a topic model. The graphical model for this composite be 
show in Figure 1(a). The model be define in term of three set of variables: a sequence 
of word w = {w1, . . . , wn}, with each wi be one of W words, a sequence of topic 
assignment z = {z1, . . . zn}, with each zi be one of T topics, and a sequence of 
class c = {c1, . . . , cn}, with each ci be one of C classes. One class, say ci = 1, be 
designate the “semantic” class. The zth topic be associate with a distribution over word 



network 
neural 

network 
output 

... 

image 
image 
object 
object 

... 

support 
vector 
svm 
... 

kernel 

in 
with 
for 
on 
... 

use 
train 

obtain 
described 

... 

0.5 0.4 0.1 

0.7 

0.9 

0.8 

0.2 

neural network train with svm image 

image 

output 

network forused image 

kernelwithobtained 

described with object 

s s s s1 42 3 

w w w w1 42 3 

z z z z 

θ 

1 42 3 

(a) (b) 

Figure 1: The composite model. (a) Graphical model. (b) Generating phrases. 

φ(z), each class c 6= 1 be associate with a distribution over word φ(c), each document 
d have a distribution over topic θ(d), and transition between class ci−1 and ci follow a 
distribution π(si−1). A document be generate via the follow procedure: 

1. Sample θ(d) from a Dirichlet(α) prior 
2. For each word wi in document d 

(a) Draw zi from θ(d) 

(b) Draw ci from π(ci−1) 

(c) If ci = 1, then draw wi from φ(zi), else draw wi from φ(ci) 

Figure 1(b) provide an intuitive representation of how phrase be generate by the com- 
posite model. The figure show a three class HMM. Two class be simple multinomial 
distribution over words. The third be a topic model, contain three topics. Transitions 
between class be show with arrows, annotate with transition probabilities. The top- 
ic in the semantic class also have probabilities, use to choose a topic when the HMM 
transition to the semantic class. Phrases be generate by follow a path through the 
model, choose a word from the distribution associate with each syntactic class, and a 
topic follow by a word from the distribution associate with that topic for the seman- 
tic class. Sentences with the same syntax but different content would be generate if the 
topic distribution be different. The generative model thus act like it be play a game 
of “Madlibs”: the semantic component provide a list of topical word (shown in black) 
which be slot into template generate by the syntactic component (shown in gray). 

2.2 Inference 

The EM algorithm can be apply to the graphical model show in Figure 1, treat the 
document distribution θ, the topic and class φ, and the transition probability π a 
parameters. However, EM produce poor result with topic models, which have many pa- 
rameters and many local maxima. Consequently, recent work have focus on approximate 
inference algorithm [6, 8]. We will use Markov chain Monte Carlo (MCMC; see [9]) to 
perform full Bayesian inference in this model, sample from a posterior distribution over 
assignment of word to class and topics. 

We assume that the document-specific distribution over topics, θ, be drawn from a 
Dirichlet(α) distribution, the topic distribution φ(z) be drawn from a Dirichlet(β) dis- 
tribution, the row of the transition matrix for the HMM be drawn from a Dirichlet(γ) 
distribution, the class distribution φ(c) be drawn from a Dirichlet(δ) distribution, and all 
Dirichlet distribution be symmetric. We use Gibbs sample to draw iteratively a topic 
assignment zi and class assignment ci for each word wi in the corpus (see [8, 9]). 

Given the word w, the class assignment c, the other topic assignment z 
−i, and the 

hyperparameters, each zi be drawn from: 

P (zi|z−i, c,w) ∝ P (zi|z−i) P (wi|z, c,w−i) 

∝ 

{ 

n 
(di) 
zi + α 

(n 
(di) 
zi + α) 

n 
(zi) 
wi 

+β 

n(zi)+Wβ 

ci 6= 1 
ci = 1 



where n(di)zi be the number of word in document di assign to topic zi, n 
(zi) 
wi be the number 

of word assign to topic zi that be the same a wi, and all count include only word for 
which ci = 1 and exclude case i. We have obtain these conditional distribution by use 
the conjugacy of the Dirichlet and multinomial distribution to integrate out the parameter 
θ, φ. Similarly condition on the other variables, each ci be drawn from: 
P (ci|c−i, z,w) ∝ P (wi|c, z,w−i) P (ci|c−i) 

∝ 

 

 

 

 

 

n 
(ci) 
wi 

+δ 

n 
(ci) 
· 

+Wδ 

(n 
(ci−1) 
ci 

+γ)(n 
(ci) 
ci+1 

+I(ci−1=ci)·I(ci=ci+1)+γ) 

n 
(ci) 
· 

+I(ci−1=ci)+Cγ 
ci 6= 1 

n 
(zi) 
wi 

+β 

n 
(zi) 
· 

+Wβ 

(n 
(ci−1) 
ci 

+γ)(n 
(ci) 
ci+1 

+I(ci−1=ci)·I(ci=ci+1)+γ) 

n 
(ci) 
· 

+I(ci−1=ci)+Cγ 
ci = 1 

where n(zi)wi be a before, n 
(ci) 
wi be the number of word assign to class ci that be the 

same a wi, exclude case i, and n 
(ci−1) 
ci be the number of transition from class ci−1 

to class ci, and all count of transition exclude transition both to and from ci. I(·) be an 
indicator function, take the value 1 when it argument be true, and 0 otherwise. Increasing 
the order of the HMM introduces additional term into P (ci|c−i), but do not otherwise 
affect sampling. 

3 Results 

We test the model on the Brown corpus and a concatenation of the Brown and TASA 
corpora. The Brown corpus [10] consists of D = 500 document and n = 1, 137, 466 word 
tokens, with part-of-speech tag for each token. The TASA corpus be an untagged collection 
of educational material consist of D = 37, 651 document and n = 12, 190, 931 word 
tokens. Words appear in few than 5 document be replace with an asterisk, but 
punctuation be included. The combine vocabulary be of size W = 37, 202. 

We dedicate one HMM class to sentence start/end marker {.,?,!}. In addition to run 
the composite model with T = 200 and C = 20, we examine two special cases: T = 200, 
C = 2, be a model where the only HMM class be the start/end and semantic classes, 
and thus equivalent to Latent Dirichlet Allocation (LDA; [6]); and T = 1, C = 20, be 
an HMM in which the semantic class distribution do not vary across documents, and 
simply have a different hyperparameter from the other classes. On the Brown corpus, we 
ran sampler for LDA and 1st, 2nd, and 3rd order HMM and composite models, with three 
chain of 4000 iteration each, take sample at a lag of 100 iteration after a burn-in of 
2000 iterations. On Brown+TASA, we ran a single chain for 4000 iteration for LDA and 
the 3rd order HMM and composite models. We use a Gaussian Metropolis proposal to 
sample the hyperparameters, take 5 draw of each hyperparameter for each Gibbs sweep. 

3.1 Syntactic class and semantic topic 

The two component of the model be sensitive to different kind of dependency among 
words. The HMM be sensitive to short-range dependency that be constant across docu- 
ments, and the topic model be sensitive to long-range dependency that vary across docu- 
ments. As a consequence, the HMM allocates word that vary across context to the se- 
mantic class, where they be differentiate into topics. The result of the algorithm, take 
from the 20th sample on Brown+TASA, be show in Figure 2. The model cleanly sep- 
arates word that play syntactic and semantic roles, in sharp contrast to the result of the 
LDA model, also show in the figure, where all word be force into topics. The syntactic 
category include prepositions, pronouns, past-tense verbs, and punctuation. While one 
state of the HMM, show in the eighth column of the figure, emits common nouns, the 
majority of noun be assign to the semantic class. 

The designation of word a syntactic or semantic depends upon the corpus. For compar- 
ison, we apply a 3rd order composite model with 100 topic and 50 class to a set of 



the the the the the a the the the 
blood , , of a the , , , 

, and and , of of of a a 
of of of to , , a of in 

body a in in in in and and game 
heart in land and to water in drink ball 
and tree to class picture be story alcohol and 
in tree farmer government film and be to team 
to with for a image matter to bottle to 
be on farm state lens be a in play 

blood forest farmer government light water story drug ball 
heart tree land state eye matter story drug game 

pressure forest crop federal lens molecule poem alcohol team 
body land farm public image liquid character people * 
lung soil food local mirror particle poetry drinking baseball 

oxygen area people act eye gas character person player 
vessel park farm state glass solid author effect football 
artery wildlife wheat national object substance poem marijuana player 

* area farm law object temperature life body field 
breathing rain corn department lens change poet use basketball 

the in he * be say can time , 
a for it new have make would way ; 

his to you other see use will year ( 
this on they first make come could day : 
their with i same do go may part ) 
these at she great know found have number 
your by we good get call must kind 
her from there small go do place 
my a this little take have 

some into who old find do 

Figure 2: Upper: Topics extract by the LDA model. Lower: Topics and class from the 
composite model. Each column represent a single topic/class, and word appear in order 
of probability in that topic/class. Since some class give almost all probability to only a 
few words, a list be terminate when the word account for 90% of the probability mass. 

D = 1713 NIPS paper from volume 0-12. We use the full text, from the Abstract to 
the Acknowledgments or References section, exclude section headers. This result in 
n = 4, 312, 614 word tokens. We replace all word appear in few than 3 paper 
with an asterisk, lead to W = 17, 268 types. We use the same sample scheme a 
Brown+TASA. A selection of topic and class from the 20th sample be show in Figure 
3. Words that might convey semantic information in another setting, such a “model”, “al- 
gorithm”, or “network”, form part of the syntax of NIPS: the consistent use of these word 
across document lead them to be incorporate into the syntactic component. 

3.2 Identifying function and content word 

Identifying function and content word require use information about both syntactic 
class and semantic context. In a machine learn paper, the word “control” might be an 
innocuous verb, or an important part of the content of a paper. Likewise, “graph” could 
refer to a figure, or indicate content related to graph theory. Tagging class might indicate 
that “control” appear a a verb rather than a noun, but decide that “graph” refers to a 
figure require use information about the content of the rest of the document. 

The factorization of word between the HMM and the LDA component provide a simple 
mean of assess the role that a give word play in a document: evaluate the posterior 
probability of assignment to the LDA component. The result of use this procedure to 
identify content word in sentence excerpt from NIPS paper be show in Figure 4. 
Probabilities be evaluate by average over assignment from all 20 samples, and take 
into account the semantic context of the whole document. As a result of combine short- 
and long-range dependencies, the model be able to pick out the word in each sentence that 
concern the content of the document. Selecting the word that have high probability of 



image data state membrane chip expert kernel network 
image gaussian policy synaptic analog expert support neural 
object mixture value cell neuron gate vector network 
object likelihood function * digital hme svm output 
feature posterior action current synapse architecture kernel input 

recognition prior reinforcement dendritic neural mixture # training 
view distribution learn potential hardware learn space input 

# em class neuron weight mixture function weight 
pixel bayesian optimal conductance # function machine # 
visual parameter * channel vlsi gate set output 

in be see use model network however # 
with be show train algorithm value also * 
for have note obtain system result then i 
on becomes consider described case model thus x 

from denotes assume give problem parameter therefore t 
at be present found network unit first n 

use remains need present method data here - 
into represent propose define approach function now c 
over exists describe generate paper problem hence r 

within seem suggest show process algorithm finally p 

Figure 3: Topics and class from the composite model on the NIPS corpus. 

1. 

In contrast to this approach, we study here how the overall network activity can control single cell 
parameter such a input resistance, a well a time and space constants, parameter that be crucial for 
excitability and spariotemporal (sic) integration. 

The integrate architecture in this paper combine feed forward control and error feedback adaptive 

control use neural networks. 

2. 

In other words, for our proof of convergence, we require the softassign algorithm to return a doubly 
stochastic matrix a *sinkhorn theorem guarantee that it will instead of a matrix which be merely close 
to be doubly stochastic base on some reasonable metric. 

The aim be to construct a portfolio with a maximal expect return for a give risk level and time 
horizon while simultaneously obey *institutional or *legally require constraints. 

3. 

The left graph be the standard experiment the right from a training with # samples. 

The graph G be call the *guest graph, and H be call the host graph. 

Figure 4: Function and content word in the NIPS corpus. Graylevel indicates posterior 
probability of assignment to LDA component, with black be highest. The boxed word 
appear a a function word and a content word in one element of each pair of sentences. 
Asterisked word have low frequency, and be treat a a single word type by the model. 

be assign to syntactic HMM class produce template for write NIPS papers, into 
which content word can be inserted. For example, replace the content word that the 
model identifies in the second sentence with content word appropriate to the topic of the 
present paper, we could write: The integrate architecture in this paper combine simple 
probabilistic syntax and topic-based semantics use generative models. 

3.3 Marginal probability 

We assess the marginal probability of the data under each model, P (w), use the har- 
monic mean of the likelihood over the last 2000 iteration of sampling, a standard method 
for evaluate Bayes factor via MCMC [11]. This probability take into account the com- 
plexity of the models, a more complex model be penalize by integrate over a latent 
space with large region of low probability. The result be show in Figure 5. LDA out- 
performs the HMM on the Brown corpus, but the HMM out-performs LDA on the large 
Brown+TASA corpus. The composite model provide the best account of both corpora, 



1st 2nd 3rd 1st 2nd 3rd 
−6e+06 

−5.5e+06 

−5e+06 

−4.5e+06 

−4e+06 

LDA 

HMM 

Composite 

Brown 

M 
ar 

gi 
na 

l l 
ik 

el 
ih 

oo 
d 

1st 2nd 3rd 1st 2nd 3rd 
−8e+07 

−7e+07 

−6e+07 

−5e+07 

−4e+07 

LDA 

HMM 

Composite 

Brown+TASA 

M 
ar 

gi 
na 

l l 
ik 

el 
ih 

oo 
d 

Figure 5: Log marginal probability of each corpus under different models. Labels on 
horizontal axis indicate the order of the HMM. 

1st 2nd 3rd 1st 2nd 3rd 1st 2nd 3rd 1st 2nd 3rd 
0 

0.2 

0.4 

0.6 

A 
dj 

u 
te 

d 
R 

an 
d 

In 
de 

x 

Brown Brown+TASA Brown Brown+TASA 

DC HMM Composite 
0 

0.2 

0.4 

0.6 

0.8 
1000 most frequent word 

A 
dj 

u 
te 

d 
R 

an 
d 

In 
de 

x 

Figure 6: Part-of-speech tag for HMM, composite, and distributional cluster (DC). 

be able to use whichever kind of dependency information be most predictive. Using 
a higher-order transition matrix for either the HMM or the composite model produce lit- 
tle improvement in marginal likelihood for the Brown corpus, but the third-order model 
perform best on Brown+TASA. 

3.4 Part-of-speech tag 

Part-of-speech tag – identify the syntactic class of a word – be a standard task in 
computational linguistics. Most unsupervised tag method use a lexicon that identifies 
the possible class for different words. This simplifies the problem, a most word belong 
to a single class. However, genuinely unsupervised recovery of parts-of-speech have be 
use to ass statistical model of language learning, such a distributional cluster [3]. 

We assess tag performance on the Brown corpus, use two tagsets. One set con- 
sisted of all Brown tags, exclude those for sentence markers, leave a total of 297 tags. 
The other set collapse these tag into ten high-level designations: adjective, adverb, con- 
junction, determiner, foreign, noun, preposition, pronoun, punctuation, and verb. We eval- 
uated tag performance by use the Adjusted Rand Index [12], to measure the con- 
cordance between the tag and the class assignment of the HMM and composite model 
in the 20th sample. The Adjusted Rand Index range from −1 to 1, with an expectation 
of 0. Results be show in Figure 6. Both model produce class assignment that be 
strongly concordant with part-of-speech, although the HMM give a slightly good match 
to the full tagset, and the composite model give a closer match to the top-level tags. This be 
partly because all word that vary strongly in frequency across context get assign to the 
semantic class in the composite model, so it miss some of the fine-grained distinction 
express in the full tagset. Both the HMM and the composite model perform good 
than the distributional cluster method described in [3], which be use to form the 1000 
most frequent word in Brown into 19 clusters. Figure 6 compare this cluster with the 
class for those word from the HMM and composite model train on Brown. 

3.5 Document classification 

The 500 document in the Brown corpus be classify into 15 groups, from editorial jour- 
nalism to romance fiction. We assess the quality of the topic recover by the LDA and 



composite model by training a naive Bayes classifier on the topic vector produce by the 
two models. We compute classification accuracy use 10-fold cross validation for the 
20th sample from a single chain. The two model perform similarly. Baseline accuracy, 
choose class accord to the prior, be 0.09. Trained on Brown, the LDA model give 
an accuracy of 0.51, while 1st, 2nd, and 3rd order composite model give 0.45, 0.41, 0.42 
respectively. Trained on Brown+TASA, the LDA model give 0.54, while the 1st. 2nd, and 
3rd order composite model give 0.48, 0.48, 0.46 respectively. The slightly low accuracy 
of the composite model may result from have few data in which to find correlations: it 
only see the word allocate to the semantic component, which account for approximately 
20% of the word in the corpus. 

4 Conclusion 

The composite model we have described capture the interaction between short- and long- 
range dependency between words. As a consequence, it be able to simultaneously learn 
syntactic class and semantic topic and identify the role that word play in documents, 
and be competitive in part-of-speech tag and classification with model that specialize 
in only one form of dependency. Clearly, such a model do not do justice to the depth 
of syntactic or semantic structure, or their interaction. However, it illustrates how a sensi- 
tivity to different kind of statistical dependency might be sufficient for the first stage of 
language acquisition, discover the syntactic and semantic building block that form the 
basis for learn more sophisticated representations. 

Acknowledgements. The TASA corpus appear courtesy of Tom Landauer and Touchstone Applied 
Science Associates, and the NIPS corpus be provide by Sam Roweis. This work be support by 
the DARPA CALO program and NTT Communication Science Laboratories. 

References 
[1] H. J. Neville, D. L. Mills, and D. S. Lawson. Fractionating language: Different neural sub- 

sytems with different sensitive periods. Cerebral Cortex, 2:244–258, 1992. 

[2] R. Brown. A first language. Harvard University Press, Cambridge, MA, 1973. 

[3] M. Redington, N. Chater, and S. Finch. Distributional information: A powerful cue for acquir- 
ing syntactic categories. Cognitive Science, 22:425–469, 1998. 

[4] T. K. Landauer and S. T. Dumais. A solution to Plato’s problem: the Latent Semantic Anal- 
ysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 
104:211–240, 1997. 

[5] C. Manning and H. Schütze. Foundations of statistical natural language processing. MIT Press, 
Cambridge, MA, 1999. 

[6] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation. Journal of Machine 
Learning Research, 3:993–1022, 2003. 

[7] N. Coccaro and D. Jurafsky. Towards good integration of semantic predictor in statistical 
language modeling. In Proceedings of ICSLP-98, volume 6, page 2403–2406. 1998. 

[8] T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy 
of Science, 101:5228–5235, 2004. 

[9] W.R. Gilks, S. Richardson, and D. J. Spiegelhalter, editors. Markov Chain Monte Carlo in 
Practice. Chapman and Hall, Suffolk, 1996. 

[10] H. Kucera and W. N. Francis. Computational analysis of present-day American English. Brown 
University Press, Providence, RI, 1967. 

[11] R. E. Kass and A. E. Rafferty. Bayes factors. Journal of the American Statistical Association, 
90:773–795, 1995. 

[12] L. Hubert and P. Arabie. Comparing partitions. Journal of Classification, 2:193–218, 1985. 


