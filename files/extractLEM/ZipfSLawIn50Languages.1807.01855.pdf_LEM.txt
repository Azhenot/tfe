














































1 / 18 





Zipf‚Äôs law in 50 languages:* 
it structural pattern, linguistic interpretation, and cognitive motivation 



Shuiyuan YU1, Chunshan Xu2, 3, Haitao LIU3, 4 



1 School of Computer, Communication University of China, Beijing,100024, China 

2 School of Foreign Studies, Anhui Jianzhu University, Hefei, 230601, China. 

3 Department of Linguistics, Zhejiang University, Hangzhou, 310058, China 

4 Ningbo Institute of Technology, Zhejiang University, Ningbo, 315100, China 



Abstract Zipf‚Äôs law have be found in many human-related fields, include 

language, where the frequency of a word be persistently found a a power law function 

of it frequency rank, know a Zipf‚Äôs law. However, there be much dispute whether it 

be a universal law or a statistical artifact, and little be know about what mechanism 

may have shape it. To answer these questions, this study conduct a large scale 

cross language investigation into Zipf‚Äôs law. The statistical result show that Zipf‚Äôs 

law in 50 language all share a 3-segment structural pattern, with each segment 

demonstrate distinctive linguistic property and the low segment invariably 

bending downwards to deviate from theoretical expectation. This find indicates 

that this deviation be a fundamental and universal feature of word frequency 

distribution in natural languages, not the statistical error of low frequency words. A 

computer simulation base on the dual-process theory yield Zipf‚Äôs law with the same 

structural pattern, suggest that Zipf‚Äôs law of natural language be motivate by 

common cognitive mechanisms. These result show that Zipf‚Äôs law in language be 

motivate by cognitive mechanism like dual-processing that govern human verbal 

behaviors. 



Keywords Zipf‚Äôs law, language universals, 3-segment structural pattern, dual- 

processing mechanism 




* Correspondence to: Haitao Liu, lhtzju@yeah.net 



2 / 18 





Introduction 

About 80 year ago, George Kingsley Zipf report an observation that the frequency 

of a word seem to be a power law function of it frequency rank, formulate a 

f(r) ‚àù ùëüùõº , where f be word frequency, r be the rank of frequency, and ùõº be the 

exponent(1, 2). This linguistic regularity be late term a Zipf‚Äôs law. In subsequent 

years, similar power law function have be widely found in various domains, 

draw attention of scientist with different academic backgrounds‚Äîit may reflect a 

universal law underlie various natural, social and cognitive phenomenon (3-5). To 

specify the mechanism motivate such power-law functions, much work have be 

do from different perspective (6-19). However, a universally acknowledge 

conclusion be yet to obtain, which reflect a failure to fully understand the role and the 

significance of these power function in natural, social and cognitive spheres. 

In an effort to explore the motivation of Zipf‚Äôs laws, we conduct a case study on 

natural languages. Though this law be first observe in language, it significance in 

verbal communication and deep-level motivation be far from be clear. We know 

neither why this law have be persistently found in sample of different languages, 

and with different sizes, nor what underlie mechanism may have shape it. As a 

result, it be often propose that this law have no linguistic significance and cognitive 

motivations: simple and meaningless statistical process (16), such a random type 

model (6, 8, 20), can result in power-law distribution similar to Zipf‚Äôs law a well. 

However, a law usually describes the fundamental nature of something. The failure to 

pin down deep level motivation actually suggests that Zipf‚Äôs law be not a law: it 

shed no light on the fundamental nature of verbal communication and the cognitive 

mechanism underlie it. To trace the possible cognitive mechanism that shape this 

law, it be necessary to find a universally valid linguistic interpretation of Zipf‚Äôs law, or 

rather it universal significance in verbal communication, which mean that Zipf‚Äôs 

curve (word frequency a a function of rank on a log-log scale) should exhibit, across 

different languages, similar structural pattern that have similar linguistic significance. 

Hence, corpus-based investigation be need that cover language a diverse a 

possible. Nevertheless, previous study be generally limited to a rather limited 

range of language and paid little attention to it underlie cognitive mechanism and 

significance in language communication. 



3 / 18 



Based on text from 50 languages, this quantitative study reveals that Zipf‚Äôs law 

found in these language all exhibit a 3-segment structural pattern, with the low 

segment invariably bending downward to deviate from the theoretical expectation, 

which have linguistic significance and can be construe a motivate by dual-process 

model‚Äîa cognitive mechanism common among human beings. We further 

demonstrate that computer simulation base on this cognitive mechanism can 

generate Zipf‚Äôs curve that be structurally similar to those found in human languages. 

These finding suggest that Zipf‚Äôs law found in human-related field be likely to 

have deep-level motivations, especially the cognitive ones, and that big-data analysis 

base on corpus may provide valuable mean to uncover language law and to trace 

cognitive law hidden behind. 



Materials and method 

Data 

Language material use in our study be take from BNC and Leipzig Corpus. In 

these corpora, we choose 50 languages. The sample of each language contains over 

300,000 sentences, range between 80,000 and 43,000,000 word tokens. These 

language belong to such typological branch a Indo-European, Uralic, Altaic, 

Caucasian, Sino-Tibet, Dravidian, Afro-asiatic, etc. Of these 50 languages, 25 include 

text of both news and wiki. 

To avoid the stochastic influence of text genre on overall word frequency, we 

conduct text-based randomization upon the material from BNC. The Leipzig Corpus 

have already be randomize on the basis of sentences. 

Piecewise fitting of frequency-rank curve of 50 language to different power-law 

function 

The rank-frequency curve found in these 50 language seem all to exhibit a 3- 

segment structural pattern. We therefore fit these 3 segment respectively to the 

follow power law functions: y = a ‚àô xùëè, y = a ‚àô xùëè, and y = a ‚àô (x + c)ùëè. In other 

words, for the fitting function of the low segment that cover the word with low 

frequency, the definition domain be not solely confine to the frequency rank. In 

addition, to prevent the influence of too many word with extremely low frequency, 

the fitting of the low segment be base on logarithmically equal size spaces, not 



4 / 18 



linearly equal size space, and offset C be add to the fitting function to indicate the 

steepness and the direction of the bending. 

The influence of text genre on the 3 segment of the curve 

25 language be chosen in Leipzig corpus to find out the influence of the genre on 

the exponent of fitting function of these curves. For each language, the sample 

cover two genres, include both wiki text and news texts. For each genre, there be 

more than 300, 000 sentences. The exponent derive from text of wiki be pair 

with the exponent derive from text of news to conduct Wilcoxon rank sum test. If 

the P-value of test be small than alpha, the null hypothesis be reject that parameter 

of fitting function be not influence by genres. 

Relations between linguistic typology and function exponent of each curve 

segment. To study the effect of typological differences, we chose, among the 50 

languages, 37 language that fall into 7 typological branch that each contains at 

least 3 of these 37 languages. In other words, these 37 language be unevenly divide 

into 7 group to conduct a F-test, which can reveal whether typological difference 

affect exponent of fitting functions. 

The growth rate of word frequency in upper segment, middle segment, and 

low segment. To investigate how word frequency grows with increase sample 

size, the follow formula be use to calculate the average growth rate of word 

frequency: 1 
ùëÅ‚àí1 

‚àë ùëì(ùëõ+1)/ùëì(ùëõ) 
ùëê(ùëõ+1)/ùëê(ùëõ) 

ùëÅ‚àí1 
ùëõ=1 , in which f(n) be the frequency of a word in language 

sample whose size be n word tokens. For each languageÔºåùëÜ1Ôπ¶ùëÜ2Ôπ¶ùëÜ3Ôπ¶‚Ä¶ ‚Ä¶ ùëÜùëõ be 

sample with different sizes; N(S) be the number of word token in a sample; the 

relation among these sample be ùëÜ1 ‚äÇ ùëÜ2 ‚äÇ ùëÜ3 ‚äÇ, ‚Ä¶ ‚Ä¶ ‚äÇ ùëÜùëõ and 2 C(ùëÜ1) = 

C(ùëÜ2), 2C(ùëÜ2) = C(ùëÜ3), ‚Ä¶ ‚Ä¶ 2C(ùëÜùëõ‚àí1) = ùê∂(ùëÜùëõ). 

We also investigate the relation between the number of word type and the number of 

word token (sample size) through linearly fitting the logarithm of number of word 

type to the logarithm of the number of word token of each sample in a language. 

The monomial coefficient of the fitting function be the Heaps‚Äô exponent. To examine 

the influence of genre and typology on Heaps‚Äô exponent, a F-tests be conduct on 

the news text and wiki text of 25 languages, and on the sample of 37 language that 

fall into 7 typological branches. 



5 / 18 



Computer simulation of dual-generating mechanism model .This model include 

two mechanism to simulate the growth of word tokens, one for the 3000 high- 

frequency word that be cover in the upper and the middle segment of the curve, 

and the other for low-frequency word and new word that be cover in the low 

segment of the curve. For high-frequency words, the increase of sample size will 

cause their total frequency to escalate accord to a fix probability a found in 

existent samples. In other words, total probability of high frequency word (their 

relative frequency) be not much influence by the variation in sample size. The 

remainder probability (1 minus the total probability of high frequency words) be 

assign to low frequency word and new words. For example, if the total probability 

of high frequency word be 0.8, the frequency of these word will stably increase in 

accordance with the increase of sample size, and thus the total probability will remain 

steadily a about 0.8. At the same time, the remainder 0.2 probability will be assign 

to low frequency words. Since the total probability be persistently 0.2, more new 

word that may appear in a large sample will reduce the average occurrence 

probability (or the mean relative frequency) of each word, which shape the 

downward bending of the low segment. 



Results 

Zipf‚Äôs law reflect the power law relation between word frequency and it rank. Since 

Zipf‚Äôs law be found, in our study, invariably to exhibit a 3-segment structural pattern, 

we be mainly concerned with dynamic pattern and the limit value of the power 

exponent of these 3 segments, that is, the power exponent of the upper segment 

(hereafter exponent 1), the power exponent of the middle segment (hereafter exponent 

2), and the power exponent of the low segment (hereafter exponent 3). The result 

of fitting variation of exponent 1 and 2 in sample of increase size to power law, 

with adjust R2=0.8506 and 0.9177, show that, when the sample size approach 

infinity, increase sample size result in zero difference of exponent 1 and 2 go to, 

and that exponent 3 have a limit value, with the average, the maximal, and the 

minimal goodness of fitting be respectively 0.9603, 0.5277, and 0.9996. In other 

words, with increase sample size, exponent 1 and 2 gradually reach constants, 

exponent 3 go to a limit value. Since exponent 3 be always significantly small 



6 / 18 



than exponent 1 and 2, the downward deviation of the low be a universal 

phenomenon in natural languages. 

As see in Figure 1, for all the 50 languages, the frequency-rank curve all deviate 

from the theoretical expectation of Zipf‚Äôs law. For any of these 50 languages, the 

Zipf‚Äôs curve can be dissect into 3 segments. Piecewise fitting of Zipf‚Äôs curve to 

power-law function in 50 language indicates that the minimal adjust R2s of the 3 

segment be 0.9619, 0.9908, and 0.9482; the maximal adjust R2s be 0.9976, 

0.9998, and 0.9966; the mean adjust R2s be 0.9879, 0.9976, and 0.9831.The upper 

segment of the curve be consistently unsmooth, roughly cover word whose 

frequency rank be within 200 (the 200 most frequent words); the middle segment be 

smoother, cover the word whose frequency rank be between 200 and 2000 (or 

3000 in some languages); the low segment be also smooth, but slope more steeply 

(that is, this segment bend downward), cover the rest of the word in the sample. 



Figure 1Ôºé The frequency-rank curve found in 50 language of Leipzig corpus. The 

upper segment of each curve (the segment before the 1st vertical line) be unsmooth; the 

middle segment of each curve (the segment between the 1st and the 2nd vertical lines) 

be smooth, with the gradient be roughly 1. The low segment of each curve (the 

segment after the 2nd vertical line) be also smooth, but bend downward to deviate 

from the expect line. 



10 0 10 2 10 4 10 6 10 8 

Rank 

10 0 

10 2 

10 4 

10 6 

10 8 

Fr 
eq 

ue 
nc 

y 



7 / 18 



In addition, the 3 segment also seem to register distinctive dynamic properties. As 

can be see in Figure 2, the sample size have different influence on the 3 segments: 1) 

exponent 1 and exponent 2 remain stable despite the growth of sample size; 2) 

exponent 3 and the power exponent of the entire curve decrease with the growth of 

sample size, go to a limit value when the sample size be approach infinity. In 

other words, these 3 segment exhibit different dynamic properties. Due to limited 

space, Figure 2 demonstrates only the statistic of English. In fact, for the exponent 

of the 3 segment in 50 languages, the Pearson's linear correlation coefficient be 

0.2543 between exponent 1 and 2, 0.0746 between exponent 1 and 3, -0.2384 between 

exponent 2 and 3. So Zipf‚Äôs curve of 50 language all show similar patterns. And 

statistical test indicate no correlation between exponent of any two segment of the 

curves. 

Apart from the sample size, linguistic factors, such a text genre and typological 

difference, seem to also have different effect on the 3 segments. Thus, 25 language 

with both wiki text and news text be chosen to study the influence of genre on the 

3 segments. On the basis of these 25 languages, this study, first of all, investigate the 

Ochiai coefficient of word that be share by text of both genre in each segment. 

The mean value be respectively 0.6362, 0.524, and 0.3477 in the 3 segments. In 

other words, in the upper segment, about 36% of word be different across the two 

genres, while for the low segment, more than 65% of word be different. Such a 

difference may have much to do the different topic that different text genre usually 

involve. Then, we use nonparametric test to investigate the influence of genre on 

exponent 1, 2, and 3, since little be know of the distribution pattern of the 

exponent of thesis languages. A Wilcoxon rank sum test be conduct to investigate 

the influence of genre on the exponent of each segment, which indicates that the P- 

value of exponent 1, 2, 3 and Heaps‚Äô be respectively 0.3033, 0.0598, 0.3417, and 

0.1160. Obviously, the test only yield low p-value for exponent 2. In other words, if 

we set significance level a 0.1, exponent 1, 3 and Heaps‚Äô be insensitive to genre 

difference while exponent 2 be sensitive. To investigate the influence of typological 

difference on exponent 1, 2, 3 and Heaps‚Äô exponent, we conduct F-test on 40 

language that fall into 7 typological branches. The result indicate that the F-values 

and P-values of Exponent 1, 2, 3, and Heaps‚Äô exponent be 22.98 and 1.7786e-10, 

13.36 and 1.26e-07, 0.8742 and 0.5243, 0.7701 and 0.5988, which suggests 



8 / 18 



significant influence of typological difference on exponent 1 and 2. . Therefore, it 

can be see that genre difference influence exponent 2, and typological difference 

influence both exponent 1 and 2. For the upper segment of the curves, most word 

remain the same regardless of different text genres. For the low segment, on the 

contrary, most word be different across different genres. However, the exponent of 

segment 1 and 3 be insensitive to genres, suggest that semantics have little 

influence on the overall frequency distribution of word in these two segments. What 

be more, these segment seem to cover word of different categories: the upper 

segment mainly include the function word while the middle and the low segment 

mainly cover content words. 



Figure 2Ôºé The relation between exponent 1, 2, 3 and the sample size a found in 

English sample of Leipzig corpus 



The different dynamic property of the 3 segment can be see more clearly in the 

fact that, for different segments, the increase of sample size will bring about different 

growth rate of word frequency. The statistic of 50 language indicate that the 

frequency of most word in the upper and the middle segment increase in proportion 

to the sample size, while the frequency of word in the low segment do not. In 

other words, the upper and the middle segment of curve rise in accordance with the 

growth of the sample size, but the low segment rise much slower, lead to the 

downward bending of the curve. 

0 0.5 1 1.5 2 2.5 

Sample size 10 7 

-1.2 

-1 

-0.8 

-0.6 

-0.4 

E 
xp 

on 
en 

t 

Exponent 1 

Exponent 2 

Exponent 3 



9 / 18 



For large scale language samples, especially those that have be shuffled, the 

probability of word occur in it be expect to be stable, and a a result, their 

frequency be expect to be in a constant proportion to sample size (the number of 

word tokens). To investigate the growth rate (dynamic properties) of word frequency 

in each segment, we calculate the growth rate of word frequency in relation to 

double sample sizes. The result show that, for most word in the upper and the 

middle segments, the growth rate cluster around 2 when sample size doubles. That is, 

frequency of most word grow in proportion with the increase of sample sizes. In 

contrary, for most word in the low segment, the growth rate cluster around 1, 

which mean that, in this segment, the frequency of most word grow much slow 

than the increase of sample sizes. Since the frequency of most word in the low 

segment fail to increase proportionally with the growth of the sample size, the relative 

frequency of word in this segment be likely to diminish in large samples, or rather, 

the temporal interval between two occurrence of a word be likely to widen in large 

samples. 

In short, word frequency distribution in natural language be characterize by a 

tendency for Zipf‚Äôs curve of large sample to bend increasingly downward. This 

phenomenon can be explain in term of the cognitive mechanism of the dual- 

process model (21-25) that suggests two quite different generate mechanism for 

high-frequency word and low-frequency words. To test this hypothesis, we conduct a 

computer simulation to find out whether a model include two different generate 

mechanism be necessary to generate the universally observe downward bending. In 

the simulation, new words‚Äô occurrence probability be predict by Heaps‚Äô law and 

statistical fitting. According to Heaps‚Äô law, there be a good power-law relation 

between the size of a text (the number of word tokens) and the number of word types. 

Our statistical investigation into Heaps‚Äô exponent of 50 language suggest that the 

distribution of Heaps‚Äô exponent be insensitive to genre and typological differences. 

The differential form of the function of heaps‚Äô law be the ratio of increase of word 

type to the increase of word tokens, i.e. the occurrence rate of new words. The 

Heaps‚Äô exponent be small than 1, and hence, the exponent of the correspond 

differential form be small than 0, or, decrease with the increase of sample sizes. In 

other words, new word do not grow at a steady rate, but slow and slower, with 

increase sample size. 



10 / 18 



Presently there be two influential model to account for the frequency distribution of 

words, both base on a single generate process. One be Simon model (12, 26), 

which assigns no fix probability to high-frequency words, prescribe that new 

word appear with equal probabilities. This model have be attract scholar 

because it can generate power law distribution of word and have explanatory power 

outside of linguistics. The other one be random type model, which generates word 

frequency distribution by randomly hit the key of typewriter and calculate the 

frequency of string separate by spaces. This model, though mathematically simple, 

be capable of generate power law distributions. Figure 3 illustrates the simulation 

result of these two models. As show in Figure 3, both model produce frequency- 

rank relation that, though in agreement with power-law functions, lack the downward 

bending found in natural language (27). In other words, the frequency distribution of 

word in natural language can be explain by neither of model that be respectively 

base on only one generate process. However, the propose model base on the 

mechanism of dual-process yield curve with downward bending similar to those 

found in natural languages. 



Figure 3. The frequency distribution generate by Simon model, random-typing 

model, and the model of our study. 



Discussion 

10 0 10 2 10 4 10 6 

Rank 

10 0 

10 2 

10 4 

10 6 

Fr 
eq 

ue 
nc 

y 

Real data 

Our model 

Simon model 

Monkey type model 



11 / 18 



The statistical result suggest that Zipf‚Äôs curve of natural language seem to 

universally have a 3-segment structural pattern: the upper segment, the middle 

segment, and the low segment, a persistently found in 50 languages. Since each 

segment have it distinctive linguistic properties, Zipf‚Äôs curve with this peculiar 3- 

segment structural pattern be likely to reflect a linguistic law, not a mere statistical 

artifact. In fact, Zipf and many other scholar have long notice that Zipf‚Äôs curve may 

consist of segment with different properties, a reflect in the observe deviation 

from expect curve (28-29). Some other scholar have try two-segment piecewise 

fitting in English and obtain result that be capable of linguistic interpretation (13, 

30). But their two-segment piecewise fitting be all base on one language sample, 

and hence cannot universally reveal the dynamic property of these different 

segments. Based on 50 different languages, our finding probably suggest universal 

linguistic pattern that be underpinned by universal motivation such a common 

human cognition. 

Since the 3 segment present different linguistic properties, it be possible to 

linguistically account for how this pattern be shaped. The upper segment of the curve 

cover mostly function words‚Äîthe fundamental syntactic mean in sentence 

organization and comprehension. Therefore, their frequency in one language be 

expect to be stable regardless of text genre and sample sizes. However, syntactic 

mean (e.g. function words, inflection, and word order) may weigh differently in 

typologically different languages, which be a possible linguistic explanation for our 

finding that typological difference have influence on exponent 1. Exponent 2 

seem sensitive to text genre. Linguistically, this may have much to do with basic 

category or concept (31) because the middle segment consists mainly of most 

frequent content words. Texts of different genre may cover very different field and 

topics, which may rely differently on these basic category (concepts) in semantic 

presentation. As a result, the content word denote these concept may account for 

different proportion of word in text of different genres, or rather, their relative 

frequency may vary with different text genres. In addition, typological difference 

also affect the exponent of the middle segment, which may imply that typologically 

different language may somewhat differ in the use of basic categories. However, in 

one language, the basic category and their role in semantic presentation be rather 

stable in a specific field, and hence the relative frequency of these word should be 



12 / 18 



stable, regardless of the text size. This be probably a linguistic explanation for the 

find that exponent 2 be sensitive to difference in both typology and genre, but 

insensitive to difference in corpus size. Used quite infrequently, and in many case 

only once, the word cover in the low segment of the curve be largely neither 

function word nor basic category content words. For these words, their relative 

frequency be not stable, with the average decrease with large sample sizes. What 

be noticeable be that the limit value of exponent 3, insensitive to text genres, sample 

size or typological differences, be likely to be a linguistic universal share by 

different languages. 

Universal linguistic pattern be believe to be driven by more fundamental 

mechanisms, such a those of human cognition. As a result, the above linguistic 

pattern a reflect by the 3-segment structural pattern may also be accountable in 

term of common human cognitive mechanism, which may further explain why this 

law universally exists in various language and in many other human related areas. . 

High-level human cognitive activity be probably characterize by the dual-process 

model. Type-1 process feature promptness, automaticity, effortlessness, freedom 

from conscious attention, etc.; Type-2 process feature slowness, control attention, 

conscious effort, etc. (32). The upper and the middle segment of Zipf‚Äôs curve mainly 

cover those high-frequency words, whose frequency rise in proportion to increase in 

sample size. This dynamic property mean that there be no need to introduce any 

temporal parameter or other constraint into formal probability model to account for 

their probability of occurrence. Cognitively, the freedom from extra constraint mean 

little or no effort for activation of these high-frequency words. Such effortlessness and 

promptness characterize Type-1 process. In contrast, the low segment mainly cover 

those low-frequency words, whose relative frequency be temporally unstable, 

decrease with the growth of sample sizes. Therefore, temporal parameter or other 

constraint be need to account for variable relative frequency of low-frequency 

words. For these words, the relation between frequency and number of word type 

(frequency spectral) be also a power law function (1, 33). This can be consider a 

the 1/f noise, a phenomenon widely found in both natural and social worlds. The 

frequency of low-frequency word be subject to model through non-stationary and 

non-ergodic stochastic process (34-35). Such extra constraint cognitively 



13 / 18 



correspond to extra mental effort to activate or process these words. This effort- 

demand processing be typical of type-2 process. 

In fact, numerous psychological study have evince that word frequency bear 

closely on nearly all psychological/cognitive process related to words, include 

intelligibility (36-37), recognizability (38), pronunciation (39), memory retention (40), 

name time, and semantic categorization time (41), which be call word frequency 

effect. That is, the processing of high-frequency word call for less 

cognitive/psychological cost than low-frequency word (42-43). These finding be 

consistent with the finding of this study that high-frequency word and low- 

frequency word have very different dynamic properties. The probable reason be that 

language system may rely heavily on the former for efficient Type-1 process, and at 

the same time, limit the growth rate of the latter to moderately regulate Type-2 

process. 

Governed by the principle of least effort, human cognition should mold language in 

such way that Type-1 process can be fully utilized. The organization and 

comprehension of sentence depend heavily on syntactic mean such a function 

words, which may play key role in trigger Type-1 process so a to automatize 

syntactic organization and parsing. Cognitively, the access of function words, thanks 

to their high frequency, be largely effortless and automatic. As a result, function word 

may contribute to automatic and quick syntactic parsing. More sentence mean more 

function words, and hence their relative frequency in one language should be stable 

regardless of genre and sample sizes. However, different syntactic mean (unction 

words, inflection, and word order) may weigh differently in typologically different 

languages, which mean the overall frequency of function word may bear on 

typology. 

Nevertheless, sentence comprehension be more than syntactic parsing‚Äîit ultimately 

aim to build semantic representations. Cognitively, the basic category play vital and 

fundamental role in our knowledge of world and semantic representation, frequently 

used, and cognitively easy to access. That is, these word may also be subject to 

Type-1 process. In brief, their frequency should steadily grow in proportion to the 

increase of sample sizes. However, since different genre may rely, to different 

degrees, on basic categories, the frequency of these word be subject to influence of 

different genres. 



14 / 18 



Most word cover in the low segment of the curve denote those concept less 

fundamental in the world knowledge and thus be use infrequently. It is, hence, 

inefficient to constantly store these word in mind a independent units‚Äîit would be 

more economical to provisionally assemble a low-frequency word when needed, an 

operation call for conscious attention and efforts. In other words, these word be 

largely subject to Type-2 process. Despite their assume high processing cost, these 

word will not significantly deteriorate the general efficiency of language processing 

because they account for merely a quite limited proportion of word token in a 

language sample. 

In brief, it be Type-1 process that shape the upper and the middle segment of Zipf‚Äôs 

curve, and Type-2 process, the low segment. If human being share similar 

cognitive mechanism, such a dual-process model, and if human language be largely 

driven by cognition, it may be predict that the 3-segment structural should be found 

in various languages, which be what have be found in our work on 50 languages. 



Conclusions 



Through the first large-scale cross-language investigation, our work have found that 

Zipf‚Äôs law in 50 language all share a 3-segment structural pattern, with the low 

segment invariably bending downward to deviate from theoretical expectation, and 

each segment demonstrate distinctive linguistic property and different bias in 

use. Further computer simulation suggest the fundamental cognitive mechanism of 

dual-processing a a deep level motivation for the 3-segment structure. That is, Zipf‚Äôs 

law found in natural language be probably the result of the general constraint of 

human cognition. These finding suggest that Zipf‚Äôs law found in human-related 

field be likely to be human driven, have deep-level motivation such a general 

cognitive mechanisms, and that big-data analysis into language may provide valuable 

mean to uncover language regularity and to trace cognitive law hidden behind 

them. 



Acknowledgments This work be partly support by the National Social Science 

Foundation of China (Grant No. 11&ZD188). 





15 / 18 



References 



1. Zipf, G. (1936). The psycho-biology of language: an introduction to dynamic 

philology. Boston: Houghton Mifflin. 

2. Zipf, G. (1949). Human Behavior and the Principle of Least Effort. New York: 

Addison-Wesley. 

3. Newman, M. (2005). Power laws, Pareto distribution and Zipf‚Äôs law. 

Contemporary physics, 46(5), 323‚Äì351. 

4. Piantadosi, S. T. (2014). Zipf's word frequency law in natural language: A critical 

review and future directions. Psychonomic bulletin & review, 21, 1112-1130. 

5. Xavier Gabaix (2016). Power Laws in Economics: An Introduction. Journal of 

Economic Perspectives, 30(1):185-206. 

6. Miller, G. (1957). Some effect of intermittent silence. The American Journal of 

Psychology, 311‚Äì314. 

7. Huberman, B.A., Adamic, L.A. Evolutionary dynamic of the World Wide Web. 

Nature 1999, 399, 131 

8. Li, Wentian (1992). Random Texts Exhibit Zipf's-Law-like Word Frequency 

Distribution. IEEE Transaction on Information Theory, Vol. 38(6), 1842‚Äì1845. 

9. Parker-Rhodes, A., & Joyce, T. (1956). A theory of word-frequency distribution. 

Nature, 178, 1308. 

10. Manin, D. (2009). Mandelbrot's Model for Zipf‚Äôs Law: Can Mandelbrot‚Äôs Model 

Explain Zipf‚Äôs Law for Language? Journal of Quantitative Linguistics, 16(3), 

274‚Äì285. 

11. Mason, W., & Suri, S. (2012). Conducting behavioral research on amazon‚Äôs 

mechanical turk. Behavior research methods, 44(1), 1‚Äì23. 

12. Simon, H. A. (1955). On a class of skew distribution functions. Biometrika, 425‚Äì 

440. 

13. Ferrer i Cancho, R., & Sol√©, R. (2003). Least effort and the origin of scale in 

human language. Proceedings of the National Academy of Sciences of the 

United States of America, 100(3), 788. 

14. Corominas-Murtra, B., & Sol√©, R. V. (2010). Universality of zipf‚Äôs law. Physical 

Review E, 82(1), 011102. 

15. Baek, S. K., Bernhardsson, S., & Minnhagen, P. (2011). Zipf‚Äôs law unzipped. 

New Journal of Physics, 13(4), 043004. 



16 / 18 



16. Rybski D, Buldyrev SV, Havlin S, Liljeros F, Makse HA (2009). Scaling law of 

human interaction activity. Proceedings of the National Academy of Sciences of 

the United States of America, 106(31):12640-12645 

17. Mandelbrot, B.B. On the theory of word frequency and on related markovian 

model of discourse. In Structure of Language and Its Mathematical Aspects: 

Proceedings of Symposia on Applied Mathematics Volume 3; Jakobson, R., Ed.; 

Amer. Math. Soc.: Providence, RI, USA, 1961; pp. 190-219. 

18. Mandelbrot, B. (1982). The Fractal Geometry of Nature. San Francisco: Freeman 

19. Frank, S. A. (2009). The common pattern of nature. Journal of evolutionary 

biology, 22(8), 1563‚Äì1585. 

20. Conrad, B., &Mitzenmacher, M. (2004). Power law for monkey type 

randomly: the case of unequal probabilities. IEEE Transactions on Information 

Theory 50(7), 1403‚Äì1414. 

21. Barrett, L. F.; Tugade, M. M.; Engle, R. W. (2004). Individual difference in 

work memory capacity and dual-process theory of the mind. Psychological 

Bulletin. 130: 553‚Äì573. 

22. Evans, J. (2003). In two minds: dual-process account of reasoning. Trends in 

Cognitive Sciences, 7 (10): 454‚Äì459. 

23. Jonathan St. B. T. Evans and Keith E. Stanovich (2013). Dual-Process Theories 

of Higher Cognition: Advancing the Debate. Perspectives on Psychological 

Science, 8(3) 223‚Äì 241, 

24. Kahneman, D (2003). A perspective on judgment and choice. American 

Psychologist, 58: 697‚Äì720. 

25. Van Lancker Sidtis, D. (2004). When novel sentence spoken or heard for the 

first time in the history of the universe be not enough: Toward a dual-process 

model of language. International Journal of Language and Communication 

Disorders, 39 (1), 1 ‚Äì 44. 

26. Yule, G. U. (1944). The statistical study of literary vocabulary. Cambridge 

University Press 

27. Ferrer-i-Cancho R, & Elvev√•g B (2010). Random Texts Do Not Exhibit the Real 

Zipf‚Äôs Law-Like Rank Distribution. PLoS ONE, 5(3): e9411. 

28. Mandelbrot, B. (1953). An informational theory of the statistical structure of 

language. In Communication Theory. Ed. W. Jackson. London: Butterworth, 486- 

504. 



17 / 18 



29. Font-Clos, F., Boleda, G. & Corral, A. (2013). A scale law beyond Zipf‚Äôs law 

and it relation to Heaps‚Äô law. New Journal of Physics, 15(9), 093033. 

30. Gerlach, M. & Altmann, E. G. (2013). Stochastic model for the vocabulary 

growth in natural languages. Physical Review X, 3, 021006. 

31. Manin, D. (2008). Zipf‚Äôs law and avoidance of excessive synonymy. Cognitive 

Science, 32(7), 1075‚Äì1098. 

32. Stanovich, K. E., & Toplak, M. E. (2012). Defining feature versus incidental 

correlate of Type 1 and Type 2 processing. Mind and Society, 11, 3-13. 

33. Chen, Y.-S. & Chong, P. (1992). Mathematical model of empirical law in 

computer applications: A case study. Computers & Mathematics with 

Applications, 24(7), 77-87. 

34. Correll, J. (2008). 1/f noise and effort on implicit measure of bias, Journal of 

Personality and Social Psychology 94, 48‚Äì59. 

35. D.L. Gilden, T. Thornton, M.W. Mallon (1995). 1/f noise in Human Cognition, 

Science, 267: 1837‚Äì1839. 

36. Brown, H; Rubenstein, C.R (1961). Test of response bias explanation of word- 

frequency effect. Science, 133: 280‚Äì28. 

37. Howes, D. H. (1957). On the relation between the intelligibility and frequency of 

occurrence of English words. Journal of the Acoustical Society of America, 29: 

296‚Äì305. 

38. Segui, J, Mehler, J, Frauenfelder, U, Morton, J (1982). The word frequency effect 

and lexical access. Neuropsychologia, 20: 615‚Äì627 

39. David A Balota, James I Chumbley (1985).The locus of word-frequency effect 

in the pronunciation task: Lexical access and/or production? Journal of Memory 

and Language, Vol. 24, 89‚Äì106. 

40. Hulme, C., Roodenrys, S., Schweickert, R., Brown, G. D., Martin, S., & Stuart, G. 

(1997). Word-frequency effect on short-term memory tasks: evidence for a 

redintegration process in immediate serial recall. Journal of Experimental 

Psychology: Learning, Memory, and Cognition, 23, 1217-1232. 

41. Forster, K. I., & Chambers, S. M. (1973). Lexical access and name time. 

Journal of Verbal Learning and Verbal Behavior, 12, 627‚Äì635. 

42. Reder, L. M., Nhouyvanisvong, A., Schunn, C. D., Ayers, M. S., Angstadt, P., & 

Hiraki, K. (2000). A mechanistic account of the mirror effect for word frequency: 

A computational model of remember‚Äìknow judgment in a continuous 



18 / 18 



recognition paradigm. Journal of Experimental Psychology: Learning, Memory, 

and Cognition, 26, 294‚Äì320. 

43. Diana, Rachel A. & Lynne M. Reder (2006). The Low-Frequency Encoding 

Disadvantage: Word Frequency Affects Processing Demands. Journal of 

Experimental Psychology: Learning, Memory, and Cognition 32, 805‚Äì815. 





(2016-10-26) 


