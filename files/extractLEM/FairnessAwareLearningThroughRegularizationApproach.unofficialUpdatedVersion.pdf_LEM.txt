





































Fairness-aware Learning through Regularization Approach 

Toshihiro Kamishima∗,Shotaro Akaho∗, and Jun Sakuma† 
∗National Institute of Advanced Industrial Science and Technology (AIST), 

AIST Tsukuba Central 2, Umezono 1–1–1, Tsukuba, Ibaraki, 305–8568 Japan, 
Email: mail@kamishima.net (http://www.kamishima.net/) and s.akaho@aist.go.jp 

†University of Tsukuba; and Japan Science and Technology Agency, 
1-1-1 Tennodai, Tsukuba, Japan; and 4-1-8, Honcho, Kawaguchi, Saitama, 332-0012 Japan 

Email: jun@cs.tsukuba.ac.jp 

Abstract—With the spread of data mining technology and 
the accumulation of social data, such technology and data be 
be use for determination that seriously affect people’s 
lives. For example, credit score be frequently determine 
base on the record of past credit data together with statistical 
prediction techniques. Needless to say, such determination 
must be socially and legally fair from a viewpoint of social 
responsibility; namely, it must be unbiased and nondiscrimi- 
natory in sensitive features, such a race, gender, religion, and 
so on. Several researcher have recently begin to attempt the 
development of analysis technique that be aware of social fair- 
ness or discrimination. They have show that simply avoid 
the use of sensitive feature be insufficient for eliminate bias 
in determinations, due to the indirect influence of sensitive 
information. From a privacy-preserving viewpoint, this can be 
interpret a hiding sensitive information when classification 
result be observed. In this paper, we first discus three 
cause of unfairness in machine learning. We then propose 
a regularization approach that be applicable to any prediction 
algorithm with probabilistic discriminative models. We further 
apply this approach to logistic regression and empirically show 
it effectiveness and efficiency. 

Keywords-fairness, discrimination, privacy, classification, lo- 
gistic regression, information theory 

I. INTRODUCTION 

Data mining technique be be increasingly use for 
serious determination such a credit, insurance rates, em- 
ployment applications, and so on. Their emergence have 
be make possible by the accumulation of vast store of 
digitize personal data, such a demographic information, 
financial transactions, communication logs, tax payments, 
and so on. Additionally, the spread of off-the-shelf mining 
tool have make it easy to analyze these store data. 
Such determination often affect people’s life seriously. 
For example, credit score be frequently determine base 
on the record of past credit data together with statistical 
prediction techniques. 

Needless to say, such serious determination must be 
socially and legally fair from a viewpoint of social responsi- 
bility; that is, they must be unbiased and nondiscriminatory 
in relation to sensitive feature such a race, gender, religion, 
and so on. Blindness to such factor must be ensure 
in determination that affect people’s life directly. Thus, 

sensitive feature must be carefully treat in the process 
and algorithm for machine learning. 

In some cases, some feature must be carefully process 
for reason other than avoid discrimination. One such 
reason would be contract between service provider and 
customers. Consider the case in which personal information 
about customer demographic be collect to recommend 
item at an e-commerce site. If the site collect these data 
under a privacy policy that restricts the use of the data for 
the purpose of recommendation, personal information must 
not be use for the selection of customer to be provide 
personalize discount coupons. In this case, the use of 
unrestricted data would be problematic. Because purchasing 
log be influence by recommendation base on personal 
information, careful consideration would be require for the 
use of such data. 

Several researcher have recently begin to attempt the 
development of analytic technique that be aware of social 
fairness or discrimination [1], [2]. They have show that the 
simple elimination of sensitive feature from calculation 
be insufficient for avoid inappropriate determination pro- 
cesses, due to the indirect influence of sensitive information. 
For example, when determine credit scoring, the feature 
of race be not used. However, if people of a specific race 
live in a specific area and address be use a a feature for 
training a prediction model, the train model might make 
unfair determination even though the race feature be not 
explicitly used. Such a phenomenon be call a red-lining 
effect [2] or indirect discrimination [1], and we describe 
it in detail in section II-A. New analytic technique have 
be devise to deal with fairness. For example, Calders 
and Verwer propose a naive Bayes that be modify so a 
to be less discriminatory [2], and Pedreschi et al. discuss 
discriminatory association rule [1]. 

In this paper, we formulate cause of unfairness in 
machine learning, develop widely applicable and efficient 
technique to enhance fairness, and evaluate the effective- 
ness and efficiency of our techniques. First, we discus 
the cause of unfairness in machine learning. In previous 
works, several notion of fairness have be propose and 
successfully exploited. Though these work focus on 

Toshihiro Kamishima 
T. Kamishima, S. Akaho, and J. Sakuma “Fairness-aware Learning through Regularization Approach” Proceedings of The 3rd IEEE International Workshop on Privacy Aspects on Data Mining (2011) 
UNOFFICIAL UPDATED VERSION 



resultant unfairness, we consider unfairness in term of 
it causes. We describe three type of cause: prejudice, 
underestimation, and negative legacy. Prejudice involves a 
statistical dependence between sensitive feature and other 
information; underestimation be the state in which a classifier 
have not yet converged; and negative legacy refers to the 
problem of unfair sample or label in the training data. 
We also propose measure to quantify the degree of these 
cause use mutual information and the Hellinger distance. 

Second, we then focus on indirect prejudice and develop 
a technique to reduce it. This technique be implement 
a regularizers that restrict the learners’ behaviors. This 
approach can be apply to any prediction algorithm with 
discriminative probabilistic models, such a logistic regres- 
sion. In solve classification problem that pay attention 
to sensitive information, we have to consider the trade- 
off between the classification accuracy and the degree of 
resultant fairness. Our method provide a way to control 
this trade-off by adjust the regularization parameter. We 
propose a prejudice remover regularizer, which enforces a 
determination’s independence from sensitive information. As 
we demonstrate, such a regularizer can be built into a logistic 
regression model. 

Finally, we perform experiment to test the effectiveness 
and efficiency of our methods. We compare our method 
with the two-naive-Bayes on a real data set use in a previ- 
ous study [2]. We evaluate the effectiveness of our approach 
and examine the balance between prediction accuracy and 
fairness. 

Note that in the previous work, a learn algorithm that be 
aware of social discrimination be call discrimination-aware 
mining. However, we hereafter use the terms, ‘unfairness’ / 
‘unfair’, instead of the ‘discrimination’ / ‘discriminatory’ for 
two reasons. First, a described above, these technology 
can be use for comply with laws, regulations, or con- 
tract that be irrelevant to discrimination. Second, because 
the term discrimination be frequently use for the meaning 
of classification in the machine learn literature, use this 
term becomes highly confusing. Worse yet, in this paper, we 
target a discriminative model, i.e., logistic regression. 

We discus cause of unfairness in section II and propose 
our method for enhance fairness in section III. Our 
method be empirically compare with two-naive-Bayes in 
section IV. Section V show related work, and section VI 
summarizes our conclusions. 

II. FAIRNESS IN DATA ANALYSIS 

After introduce an example of the difficulty in fairness- 
aware learning, we show three cause of unfairness and 
quantitative measure for the degree of these causes. 

A. Illustration of the Difficulties in Fairness-aware Learning 

We here introduce an example from the literature to 
show the difficulty in fairness-aware learn [2], which 

be a simple analytical result for the data set described in 
section IV-B. The researcher perform a classification 
problem to predict whether the income of an individual 
would be high or low. 

The sensitive feature, S, be gender, which take a value, 
Male or Female, and the target class, Y, indicate whether 
his/her income be High or Low. The sensitive feature, S, 
be gender, which take a value, Male or Female, and the 
target class, Y , indicate whether his/her income be High 
or Low. There be some other non-sensitive features, X . 
The ratio of Female record comprise about 1/3 of the data 
set; that is, the number of Female record be much small 
than that of Male records. Additionally, while about 30% of 
Male record be classify into the High class, only 11% 
of Female record were. Therefore, Female–High record 
be the minority in this data set. 

In this data set, we describe how Female record tend 
to be classify into the Low class unfairly. Calders and 
Verwer define a discrimination score (hereafter refer to 
a the Calders-Verwer score (CV score) by subtract the 
conditional probability of the positive class give a sensitive 
value from that give a non-sensitive value. In this example, 
a CV score be define a 

Pr[Y =High|S=Male]− Pr[Y =High|S=Female]. 

The CV score calculate directly from the original data be 
0.19. After training a naive Bayes classifier from data involv- 
ing a sensitive feature, the CV score on the predict class 
increase to about 0.34. This show that Female record be 
more frequently misclassified to the Low class than Male 
records; and thus, Female–High individual be consider 
to be unfairly treated. This phenomenon be mainly cause 
by an Occam’s razor principle, which be commonly adopt 
in classifiers. Because infrequent and specific pattern tend 
to be discard to generalize observation in data, minority 
record can be unfairly neglected. Even if the sensitive 
feature be remove from the training data for a naive Bayes 
classifier, the resultant CV score be 0.28, which still show 
an unfair treatment for minorities. This be cause by the 
indirect influence of sensitive features. This event be call 
by a red-lining effect [2], a term that originates from the 
historical practice of draw red line on a map around 
neighborhood in which large number of minority be 
know to dwell. Consequently, simply remove sensitive 
feature be insufficient, and affirmative action have to be 
adopt to correct the unfairness in machine learning. 

B. Three Causes of Unfairness 

In this section, we discus the social fairness in data 
analysis. Previous work [1], [2] have focus on unfairness 
in the resultant determinations. To look more carefully 
at the problem of fairness in machine learning, we shall 
examine the underlie cause or source of unfairness. 
We suppose that there be at least three possible causes: 



prejudice, underestimation, and negative legacy. Note that 
these be not mutually exclusive, and two or more cause 
may compositely lead to unfair treatments. 

Before present these three cause of unfairness, we 
must introduce several notations. Here, we discus super- 
vised learning, such a classification and regression, which 
be aware of unfairness. Y be a target random variable to 
be predict base on the instance value of features. The 
sensitive variable, S, and non-sensitive variable, X , corre- 
spond to sensitive and non-sensitive features, respectively. 
We further introduce a prediction modelM[Y |X, S], which 
model a conditional distribution of Y give X and S. With 
this model and a true distribution over X and S, Pr∗[X, S], 
we define 

Pr[Y, X, S] =M[Y |X, S]Pr∗[X, S]. (1) 

Applying marginalization and/or Bayes’ rule to this equa- 
tion, we can calculate other distributions, such a Pr[Y, S] 
or Pr[Y |X]. We use P̃r[·] to denote sample distributions. 
P̂r[Y, X, S] be define by replace a true distribution in (1) 
with it correspond sample distribution: 

P̂r[Y, X, S] =M[Y |X, S]P̃r[X, S], (2) 

and induced distribution from P̂r[Y,X, S] be denote by 
use P̂r[·]. 

1) Prejudice: Prejudice mean a statistical dependence 
between a sensitive variable, S, and the target variable, Y , 
or a non-sensitive variable, X . There be three type of 
prejudices: direct prejudice, indirect prejudice, and latent 
prejudice. 

The first type be direct prejudice, which be the use of a 
sensitive variable in a prediction model. If a model with a 
direct prejudice be use in classification, the classification 
result clearly depend on sensitive features, thereby gen- 
erating a database contain direct discrimination [1]. To 
remove this type of prejudice, all that we have to do be 
simply eliminate the sensitive variable from the prediction 
model. We then show a relation between such this direct 
prejudice and statistical dependence. After eliminate the 
sensitive variable, equation (1) can be rewrite a 

Pr[Y, X, S] =M[Y |X]Pr∗[S|X]Pr∗[X]. 

This equation state that S and Y be conditionally inde- 
pendent give X , i.e., Y ⊥⊥ S | X . Hence, we can say that 
when the condition Y 6⊥⊥ S |X be not satisfied, the prediction 
model have a direct prejudice. 

The second type be an indirect prejudice, which be statis- 
tical dependence between a sensitive variable and a target 
variable. Even if a prediction model lack a direct prejudice, 
the model can have an indirect prejudice and can make an 
unfair determination. We give a simple example. Consider 
the case that all Y , X , and S be real scalar variables, and 
these variable satisfy the equations: 

Y = X + εY and S = X + εS , 

where εY and εS be mutually independent 
random variables. Because Pr[Y, X, S] be equal to 
Pr[Y |X] Pr[S|X] Pr[X], these variable satisfy the 
condition Y ⊥⊥ S | X , but do not satisfy the condition 
Y⊥⊥S. Hence, the adopt prediction model do not have 
a direct prejudice, but may have an indirect prejudice. If 
the variance of εY and εS be small, Y and S become 
highly correlated. In this case, even if a model do not 
have a direct prejudice, the determination clearly depends 
on sensitive information. Such resultant determination 
be call indirect discrimination [1] or a red-lining effect 
[2] a described in section II-A. To remove this indirect 
prejudice, we must use a prediction model that satisfies the 
condition Y⊥⊥S. 

We next show an index to quantify the degree of indirect 
prejudice, which be straightforwardly define a the mutual 
information between Y and S. However, because a true 
distribution in (1) be unknown, we adopt sample distribution 
in equation (2) over a give sample set, D: 

PI = 
∑ 

(y,s)∈D 

P̂r[y, s] ln 
P̂r[y, s] 

P̂r[s]P̂r[s] 
. (3) 

We refer to this index a a (indirect) prejudice index (PI for 
short). For convenience, the application of the normalization 
technique for mutual information [3] lead to a normalize 
prejudice index (NPI for short): 

NPI = PI/( 
√ 

H(Y )H(S)), (4) 

where an entropy function H(X) be define a 
− 

∑ 
x∈D P̂r[x] ln P̂r[x]. The range of this NPI be [0, 1]. 

The third type of prejudice be latent prejudice, which be a 
statistical dependence between a sensitive variable, S, and a 
non-sensitive variable, X . Consider an example that satisfies 
the equations: 

Y = X1 + εY , X = X1 + X2, and S = X2 + εS , 

where εY⊥⊥εS and X1⊥⊥X2. Clearly, the condition 
Y ⊥⊥ S | X and Y⊥⊥S be satisfied, but X and S be 
not mutually independent. This dependence doesn’t cause a 
sensitive information to influence the final determination, but 
it would be exploit for training learners; thus, this might 
violate some regulation or laws. Recall our example about 
personal information in section I. The use of raw purchasing 
log may violate contract with customers, because the 
log be influence by recommendation base on personal 
information, even if it be irrelevant to the final selection 
of customers. Removal of potential prejudice be achieve 
by make X and Y independent from S simultaneously. 
Similar to a PI, the degree of a latent prejudice can be 
quantify by the mutual information between X and S. 

2) Underestimation: Underestimation be the state in 
which a learn model be not fully converge due to the 
finiteness of the size of a training data set. Given a learn 



algorithm that can acquire a prediction model without indi- 
rect prejudice, it will make a fair determination if infinite 
training example be available. However, if the size of the 
training data set be finite, the learn classifier may lead to 
more unfair determination than that observe in the training 
sample distribution. Though such determination be not 
intentional, they might awake suspicion of unfair treatment. 
In other words, though the notion of convergence at infinity 
be appropriate in a mathematical sense, it might not be in a 
social sense. We can quantify the degree of underestimation 
by assess the resultant difference between the training 
sample distribution over D, P̃r[·], and the distribution in- 
duced by a model, P̂r[·]. Along this line, we define the 
underestimation index (UEI) use the Hellinger distance: 

UEI = 
(1 

2 

∑ 
y,s∈D 

(√ 
P̂r[y, s]− 

√ 
P̃r[y, s] 

)2)1/2 
= 

( 
1− 

∑ 
y,s∈D 

√ 
P̂r[Y, S]P̃r[Y, S] 

)1/2 
. (5) 

Note that we do not adopt the KL-divergence because it can 
be infinite and this property be inconvenient for an index. 

3) Negative Legacy: Negative legacy be unfair sample 
or label in the training data. For example, if a bank have 
be refuse credit to minority people without assess 
them, the record of minority people be less sample in 
a training data set. A sample selection bias be cause by 
such bias sample depend on the feature of samples. 
It be know that the problem of a sample selection bias 
can be avoid by adopt specific type of classification 
algorithm [4]. However, it be not easy to detect the existence 
of a sample selection bias only by observe training data. 
On the other hand, if a bank have be unfairly reject 
the loan of the people who should have be approved, 
the label in the training data would become unfair. This 
problem be serious because it be hard to detect and correct. 
However, if other information, e.g., a small-sized fairly 
label data set, can be exploited, this problem can be 
correct by technique such a transfer learn [5]. 

Regulations or law that demand the removal of potential 
prejudice be rare. We investigate UEIs in the experimental 
section of this paper, but we don’t especially focus on 
underestimation. As described above, avoid a negative 
legacy can be difficult if no additional information be avail- 
able. We therefore focus on the development of a method to 
remove indirect prejudice. 

III. PREJUDICE REMOVAL TECHNIQUES 

We here propose a technique to reduce indirect prejudice. 
Because this technique be implement a a regularizer, 
which we call a prejudice remover, it can be apply to 
wide variety of prediction algorithm with probabilistic 
discriminative models. 

A. General Framework 

We focus on classification and built our regularizers 
into logistic regression models. Y , X , and S be random 
variable correspond to a class, non-sensitive features, 
and a sensitive feature, respectively. A training data set 
consists of the instance of these random variables, i.e., 
D = {(y,x, s)}. The conditional probability of a class 
give non-sensitive and sensitive feature be model by 
M[Y |X, S;Θ], where Θ be the set of model parameters. 
These parameter be estimate base on the maximum 
likelihood principle; that is, the parameter be tune so a 
to maximize the log-likelihood: 

`(D;Θ) = 
∑ 

(yi,xi,si)∈D 

lnM[yi|xi, si;Θ]. (6) 

We adopt two type of regularizers. The first regular- 
izer be a standard one to avoid over-fitting. We use an 
L2 regularizer ‖Θ‖22. The second regularizer, R(D,Θ), be 
introduce to enforce fair classification. We design this 
regularizer to be easy to implement and to require only 
modest computational resources. By add these two regu- 
larizers to equation (6), the objective function to minimize 
be obtained: 

−`(D;Θ) + ηR(D,Θ) + λ 
2 
‖Θ‖22, (7) 

where λ and η be positive regularization parameters. 
We dealt with a classification problem in which the target 

value Y be binary {0, 1}, X take a real vectors, x, and S 
take a discrete value, s, in a domain S. We use a logistic 
regression model a a prediction model: 

M[y|x, s;Θ] = yσ(x>ws) + (1− y)(1− σ(x>ws)), (8) 

where σ(·) be a sigmoid function, and the parameter be 
weight vector for x, Θ = {ws}s∈S . Note that a constant 
term be include in x without loss of generality. We next 
introduce a regularizer to reduce the indirect prejudice. 

B. Prejudice Remover 

A prejudice remover regularizer directly try to reduce 
the prejudice index and be denote by RPR. Recall that the 
prejudice index be define a 

PI = 
∑ 
Y,S 

P̂r[Y, S] ln 
P̂r[Y, S] 

P̂r[S]P̂r[Y ] 

= 
∑ 

Y,X,S 

M[Y |X, S;Θ]P̃r[X, S] ln P̂r[Y, S] 
P̂r[S]P̂r[Y ] 

. 

∑ 
X,S P̃r[X, S] can be replace with 

∑ 
(xi,si)∈D, and the 

argument of logarithm can be rewrite a P̂r[Y |si]/P̂r[Y ], 
by reduce P̂r[S]. We obtain∑ 

(xi,si)∈D 

∑ 
y∈{0,1} 

M[y|xi, si;Θ] ln 
P̂r[y|si] 
P̂r[y] 

. 



The straightforward way to compute P̂r[y|s] be to marginal- 
ize M[y|X, s;Θ]P̂r[X, s] over X . However, if the domain 
of X be large, this marginalization be computationally heavy. 
We hence take a drastically simple approach. We replace 
X with x̄s, which be a sample mean vector of x over a set 
of training sample whose correspond sensitive feature be 
equal to s, {(yi,xi, si) ∈ D s.t. si = s}, and we get 

P̂r[y|s] =M[y|x̄s, s;Θ], (9) 

P̂r[y] = 
∑ 
s∈S 

P̂r[s]M[y|x̄s, s;Θ]. (10) 

Finally, the prejudice remover regularizer RPR(D,Θ) is∑ 
(xi,si)∈D 

∑ 
y∈{0,1} 

M[y|xi, si;Θ] ln 
P̂r[y|si] 
P̂r[y] 

, (11) 

where P̂r[y|s] and P̂r[y] be equation (9) and (10), respec- 
tively. This regularizer becomes large when a class be de- 
termined mainly base on sensitive features; thus, sensitive 
feature become less influential to the final determination. 
In the case of logistic regression, the objective function (7) 
to minimize be rewrite a 

− 
∑ 

(yi,xi,si) 

lnM[yi|xi, si;Θ]+RPR(D,Θ)+ 
λ 

2 

∑ 
s∈S 

‖ws‖22, (12) 

where M[y|x, s;Θ] be equation (8) and RPR(D,Θ) be 
equation (11). In our experiment, this objective function be 
minimize by a conjugate gradient method start from the 
initial condition w = 0, ∀s ∈ S , and we obtain an optimal 
parameter set, {w∗s}. 

The probability of Y = 1 give a sample without a class 
label, (xnew, snew) can be predict by 

Pr[Y =1|xnew, snew; {w∗s}] = σ(x>neww∗snew). 

IV. EXPERIMENTS 

We compare our method with Calders and Verwer’s 
method on the real data set use in a previous study [2]. 

A. Calders-Verwer’s 2-Naive-Bayes 

We briefly introduce Calders and Verwer’s 2-naive-Bayes 
method (CV2NB), which be found to be the best method 
in the previous study use the same dataset [2]. This 
method target a binary classification problem. The number 
of sensitive feature be one and the feature be binary. The 
generative model of this method be 

Pr[Y,X, S] =M[Y, S] 
∏ 

i 

M[Xi|Y, S]. (13) 

M[Xi|Y, S] model a conditional distribution of Xi give 
Y and S, and the parameter of these model be estimate 
by the similar way in the estimation of parameter of a naive 
Bayes model.M[Y, S] model a joint distribution Y and S. 
Because Y and S be not mutually independent, the final 

1 Calculate a CV score, disc, of the predict class by the current model. 
2 while disc > 0 
3 numpos be the number of positive sample classify by the current model. 
4 if numpos < the number of positive sample in D then 
5 N(Y =1, S=0)← N(Y =1, S=0) + ∆N(Y =0, S=1) 
6 N(Y =0, S=0)← N(Y =0, S=0)−∆N(Y =0, S=1) 
7 else 
8 N(Y =0, S=1)← N(Y =0, S=1) + ∆N(Y =1, S=0) 
9 N(Y =1, S=1)← N(Y =1, S=1)−∆N(Y =1, S=0) 
10 if any of N(Y, S) be negative then 

cancel the previous update of N(Y, S) and abort 
11 Recalculate Pr[Y |S] and a CV score, disc base on update N(Y, S) 

Figure 1. naive Bayes modification algorithm 
NOTE: N(Y =y, S=s) denotes the number of sample in D, 
whose class and sensitive feature be y and s, respectively. In 
our experiment, ∆ be set to 0.01 a in the original paper. 

determination might be unfair. While each feature depends 
only on a class in the case of the original naive Bayes, every 
non-sensitive feature, Xi, depends on both Y and S in the 
case of CV2NB. It be a if two naive Bayes classifier be 
learn depend on each value of the sensitive feature; 
that be why this method be name by the 2-naive-Bayes. 
To make the classifier fair, M[Y, S] be initialize by the 
sample distribution P̃r[Y, S], and this model be modify by 
the algorithm in Figure 1. A model parameter M(y, s) 
be derive by N(y, s)/ 

∑ 
Y,S N(y 

′, s′). This algorithm be 
design so a to update Pr[Y, S] gradually until a CV score 
becomes positive. Note that we slightly modify the original 
algorithm by add line 10 in Figure 1, which guarantee 
the parameters, N(Y, S), to be non-negative, because the 
original algorithm may fail to stop. 

B. Experimental Conditions 

We summarize our experimental conditions. We test a 
previously use real data set [2], a show in section II-A. 
This set include 16281 data in an adult.test file of the 
Adult/Census Income distribute at the UCI Repository 
[6]. The target variable indicates whether or not income be 
large than 50M dollars, and the sensitive feature be gender. 
Thirteen non-sensitive feature be discretized by the pro- 
cedure in the original paper. In the case of the naive Bayes, 
parameter of models,M[Xi|Y, S], be estimate by a MAP 
estimator with multinomial distribution and Dirichlet priors. 
In the case of our logistic regression, discrete variable be 
represent by 0/1 dummy variable cod by a so-called 
1-of-K scheme. The regularization parameter for the L2 
regularizer, λ, be fix to 1, because the performance of 
pure logistic regression be less affected by this parameter in 
our preliminary experiments. We test six methods: logistic 
regression with a sensitive feature (LR), logistic regression 
without a sensitive feature (LRns), logistic regression with 
a prejudice remover regularizer (PR), naive Bayes with 
a sensitive feature (NB), naive Bayes without a sensitive 
feature (NBns), and Caldars and Verwer’s 2-naive-Bayes 
(CV2NB). We show the mean of the statistic obtain by 
the five-fold cross-validation. 



Table I 
A SUMMARY OF EXPERIMENTAL RESULTS 

method Acc NMI NPI UEI CVS PI / MI 

LR 0.851 0.267 5.21E-02 0.040 0.189 2.10E-01 
LRns 0.850 0.266 4.99E-04 0.036 -0.033 1.06E-03 
PR η=0 0.850 0.265 4.94E-02 0.038 0.185 2.01E-01 
PR η=0.1 0.850 0.264 4.11E-02 0.036 0.170 1.68E-01 
PR η=0.3 0.774 0.149 7.53E-03 0.127 -0.095 5.47E-02 
PR η=1 0.720 0.124 1.29E-05 0.148 -0.004 1.12E-04 
PR η=10 0.676 0.013 2.13E-01 0.259 -0.472 1.84E+01 

NB 0.822 0.246 1.12E-01 0.068 0.332 4.90E-01 
NBns 0.826 0.249 7.17E-02 0.043 0.267 3.11E-01 
CV2NB 0.813 0.191 3.64E-06 0.082 -0.002 2.05E-05 

NOTE: 〈n1〉E〈n2〉 denotes n1 × 10n2 . 

C. Experimental Results 

Table I show accuracy (Acc), NPI and UEI in section II, 
and CV score (CVS). MI denotes mutual information be- 
tween sample label and predict labels, NMI be obtain 
by normalize this MI in a process similar to NPI. PI / MI 
quantifies a prejudice index that be sacrifice by obtain a 
unit of information about the correct label. This can be use 
to measure the efficiency of the trade-off between prediction 
accuracy and prejudice removal. A small PI / MI value 
indicates high efficiency in this trade-off. 

We first compare the performance of our method with that 
of baseline in Table I. Compared with NBns, our method 
be superior both in accuracy and NPI at η = 0.1. Because 
LRns successfully remove prejudice without sacrifice 
accuracy unlike NBns, our PR at η = 1 be good in 
PI / MI, but accuracy be fairly degraded. Note that two 
methods, PR at η = 0 and LR, behave similarly, because 
our PR be almost equivalent to LR if the prejudice remover 
be eliminate by set η = 0. 

We next move on to the influence of the parameter, η, 
which control the degree of prejudice removal. We expect 
that the large the η, the more prejudice would be removed, 
whereas accuracy might be sacrificed. According to Table I, 
a η increased, our PR generally become degrade in accu- 
racy, but be also not fully improve in prejudice removal. 

To further investigate the change of performance depend- 
ing on this parameter η, we demonstrate the variation in 
accuracy (Acc), normalize prejudice index (NPI), and the 
trade-off efficiency between accuracy and prejudice removal 
(PI / MI) in Figure 2. We focus on our PR method. Overall, 
the change be rather unstable in all statistics. The reason 
for this instability would be the sub-optimality in solution 
stem from the lack of convexity of the objective func- 
tion (12) and the approximation by replace the marginal 
value of X with their sample means. The increase of η 
generally damage accuracy because a prejudice remover 
regularizer be design to remove prejudice by sacrifice 
correctness in prediction. NPI peaked at η = 1, though 

0.60 

0.65 

0.70 

0.75 

0.80 

0.85 

0.01 0.1 1 10 

(a) accuracy (Acc) 

10 
−6 

10 
−5 

10 
−4 

10 
−3 

10 
−2 

10 
−1 

1 
0.01 0.1 1 10 

(b) normalize prejudice index (NPI) 

10 
−5 

10 
−4 

10 
−3 

10 
−2 

10 
−1 

1 

10 

0.01 0.1 1 10 

(c) trade-off efficiency between accuracy and prejudice removal 
(PI / MI) 

Figure 2. The change in performance accord to the parameter η 

NOTE: Horizontal ax represent the parameter η, and vertical 
ax represent statistic in each subtitle. Solid, chain, dotted, 
and broken line indicate the statistic of PR, CV2NB, LRns, 
and NBns, respectively. 



we expect that more prejudice would be remove a η 
increased. We postulate that this would be due to the approx- 
imation in the marginalization of X; further investigation be 
require for this point. The peak in trade-off efficiency be 
observe at η = 1, but accuracy be fairly damage at this 
point. 

We next compare our PR with other methods. The 
performance of CV2NB be fairly good, and our PR be 
inferior except for accuracy at the low range of η. When 
compare to the baseline LRns, by tune the parameter η, 
our PR could exceed in all statistics. However, it fail to 
exceed in both accuracy and prejudice removal at the same 
η. 

In summary, our PR could successfully reduce indirect 
prejudice, but accuracy be sacrifice for this reduction. We 
must further improve the efficiency in the trade-off between 
accuracy and prejudice removal. 

V. RELATED WORK 

Several analytic technique that be aware of fairness or 
discrimination have recently receive attention. Pedreschi 
et al. emphasize the unfairness in association rule whose 
consequents include serious determination [1], [7]. They 
advocate the notion of α-protection, which be the condition 
that association rule be fair. Given a rule whose conse- 
quent exhibit negative determination, it would be unfair if 
the confidence of the rule substantially increase by add a 
condition related to a sensitive feature to the antecedent part 
of the rule. The α-protection constrains the rule so that the 
ratio of this increase be at most α. They also suggest the 
notion of direct discrimination and indirect discrimination. 
A direct discriminatory rule directly contains a sensitive 
condition in it antecedent, and while an indirect discrimi- 
natory rule doesn’t directly contain a sensitive condition, the 
rule be consider to be unfair in the context of background 
knowledge that include sensitive information. Their work 
have since be extend [8]. Various kind of index for 
evaluate discriminatory determination be propose and 
their statistical significance have be discussed. A system for 
find such unfair rule have be propose [9]. Calders and 
Verwer propose several method to modify naive Bayes 
for enhance fairness a described in section IV-A [2]. 
Luong et al. propose a notion of situation testing, wherein a 
determination be consider unfair if different determination 
be make for two individual all of whose feature be equal 
except for sensitive one [10]. Such unfairness be detect 
by compare the determination for record whose sensitive 
feature be different, but be neighbor in non-sensitive 
feature space. If a target determination differs, but non- 
sensitive feature be completely equal, then a target variable 
depends on a sensitive variable. Therefore, this situation 
test have connection to our indirect prejudice. Dwork et al. 
argue a data transformation for the purpose of export 
data while keep aware of fairness [11]. A data set held 

by a data owner be transform and pass to a vendor who 
classifies the transform data. The transformation preserve 
the neighborhood relation of data and the equivalence 
between the expectation of data mapped from sensitive 
individual and from non-sensitive ones. In a sense that 
consider the neighborhood relations, this approach be 
related to the above notion of situation testing. Because 
their proposition 2.2 implies that the classification result 
be roughly independent from the membership in a sensitive 
group, their approach have relation to our idea of prejudice. 

In a broad sense, fairness-aware learn be a kind of 
cost-sensitive learn [12]. That be to say, the cost of 
enhance fairness be take into account. Fairness in machine 
learn can be interpret a a sub-notion of legitimacy, 
which mean that model can be deployed in the real 
world [13]. Gondek and Hofmann devise a method for 
find cluster that be not relevant to a give group 
[14]. If a give group contains sensitive information, this 
method can be use for cluster data into fair clusters. 
Independent component analysis might be use to maintain 
the independence between feature [15]. 

The removal of prejudice be closely related to privacy- 
preserve data mining [16], which be a technology for min- 
ing useful information without expose individual private 
records. The privacy protection level be quantify by mutual 
information between the public and private realm [17]. In 
our case, the degree of indirect prejudice be quantify by mu- 
tual information between classification result and sensitive 
features. Due to the similarity of these two us of mutual 
information, the design goal of fairness-aware learn can 
be consider the protection of sensitive information when 
expose classification results. 

Regarding underestimation, the concept of anytime algo- 
rithms in planning or decision make [18] might be useful. 

As described in section II-B, the problem of negative 
legacy be closely related to transfer learning. Transfer learn- 
ing be “the problem of retain and apply the knowledge 
learn in one or more task to efficiently develop an effec- 
tive hypothesis for a new task” [5]. Among many type of 
transfer learning, the problem of a sample selection bias [4] 
would be related to the negative legacy problem. Sample 
selection bias mean that the sample be not at random, but 
bias depend on some feature value of data. Another 
related approach to transfer learn be weight sample 
accord the degree of usefulness for the target task [19]. 
Using these approaches, if give a small amount of fairly 
label data, other data set that might be unfairly label 
would be correctly processed. 

VI. CONCLUSIONS AND FUTURE WORK 

The contribution of this paper be a follows. First, 
we propose three cause of unfairness: prejudice, under- 
estimation, and negative legacy. Prejudice refers to the 
dependence between sensitive information and the other 



information, either directly or indirectly. We further clas- 
sified prejudice into three type and developed a way to 
quantify them by mutual information. Underestimation be 
the state in which a classifier have not yet converged, thereby 
produce more unfair determination than those observe 
in a sample distribution. Negative legacy be the problem of 
unfair sample or label in the training data. Second, 
we developed technique to reduce indirect prejudice. We 
propose a prejudice remover regularizer, which enforces 
a classifier’s independence from sensitive information. Our 
method can be apply to any algorithm with probabilistic 
discriminative model and be simple to implement. Third, 
we show experimental result of logistic regression with 
our prejudice remover regularizer. The experimental result 
show the effectiveness and efficiency of our methods. We 
further propose a method to evaluate the trade-off between 
the prediction accuracy and fairness. 

Research on fairness-aware learn be just beginning; 
thus, there be many problem yet to be solved; for example, 
the definition of fairness in data analysis, measure for 
fairness, and maintain other type of law or regulations. 
The type of analytic method be severely limited at present. 
Our method can be easily apply to regression, but fairness- 
aware cluster and rank method be also needed. 

The use of data mining technology in our society 
will only become great with time. Unfortunately, their 
result can occasionally damage people’s life [20]. On the 
other hand, data analysis be crucial for enhance public 
welfare. For example, exploit personal information have 
prove to be effective for reduce energy consumption, 
improve the efficiency of traffic control, prevent in- 
fectious diseases, and so on. Consequently, method of 
data exploitation that do not damage people’s lives, such 
a fairness/discrimination-aware learning, privacy-preserving 
data mining, or adversarial learning, together comprise the 
notion of socially responsible mining, which it should be- 
come an important concept in the near future. 

ACKNOWLEDGMENT 

We wish to thank Dr. Sicco Verwer for provide detail 
information about his work. This work be support by the 
grant-in-aid 14658106, 16700157, and 21500154 of the 
Japan society for the promotion of science. 

REFERENCES 

[1] D. Pedreschi, S. Ruggieri, and F. Turini, “Discrimination- 
aware data mining,” in Proc. of The 14th Int’l Conf. on 
Knowledge Discovery and Data Mining, 2008. 

[2] T. Calders and S. Verwer, “Three naive bayes approach for 
discrimination-free classification,” Data Mining and Knowl- 
edge Discovery, vol. 21, pp. 277–292, 2010. 

[3] A. Strehl and J. Ghosh, “Cluster ensemble — a knowledge 
reuse framework for combine multiple partitions,” Journal 
of Machine Learning Research, vol. 3, pp. 583–617, 2002. 

[4] B. Zadrozny, “Learning and evaluate classifier under sam- 
ple selection bias,” in Proc. of The 21st Int’l Conf. on Machine 
Learning, 2004, pp. 903–910. 

[5] “NIPS workshop — inductive transfer: 10 year later,” 2005, 
http://iitrl.acadiau.ca/itws05/. 

[6] A. Frank and A. Asuncion, “UCI machine learn reposi- 
tory,” University of California, Irvine, School of Information 
and Computer Sciences, 2010, http://archive.ics.uci.edu/ml. 

[7] S. Ruggieri, D. Pedreschi, and F. Turini, “Data mining for 
discrimination discovery,” ACM Transactions on Knowledge 
Discovery from Data, vol. 4, no. 2, 2010. 

[8] D. Pedreschi, S. Ruggieri, and F. Turini, “Measuring discrim- 
ination in socially-sensitive decision records,” in Proc. of the 
SIAM Int’l Conf. on Data Mining, 2009, pp. 581–592. 

[9] S. Ruggieri, D. Pedreschi, and F. Turini, “Dcube: Discrimina- 
tion discovery in databases,” in Proc of The ACM SIGMOD 
Int’l Conf. on Management of Data, 2010, pp. 1127–1130. 

[10] B. T. Luong, S. Ruggieri, and F. Turini, “k-nn a an imple- 
mentation of situation test for discrimination discovery and 
prevention,” in Proc. of The 17th Int’l Conf. on Knowledge 
Discovery and Data Mining, 2011, pp. 502–510. 

[11] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, 
“Fairness through awareness,” arxiv.org:1104.3913, 2011. 

[12] C. Elkan, “The foundation of cost-sensitive learning,” in 
Proc. of the 17th Int’l Joint Conf. on Artificial Intelligence, 
2001, pp. 973–978. 

[13] C. Perlich, S. Kaufman, and S. Rosset, “Leakage in data 
mining: Formulation, detection, and avoidance,” in Proc. of 
The 17th Int’l Conf. on Knowledge Discovery and Data 
Mining, 2011, pp. 556–563. 

[14] D. Gondek and T. Hofmann, “Non-redundant data clustering,” 
in Proc. of The 4th IEEE Int’l Conf. on Data Mining, 2004, 
pp. 75–82. 

[15] R. S. Sutton and A. G. Barto, Reinforcement Learning: An 
Introduction. MIT Press, 1998. 

[16] C. C. Aggarwal and P. S. Yu, Eds., Privacy-Preserving Data 
Mining: Models and Algorithms. Springer, 2008. 

[17] S. Venkatasubramanian, “Measures of anonimity,” in Privacy- 
Preserving Data Mining: Models and Algorithms, C. C. 
Aggarwal and P. S. Yu, Eds. Springer, 2008, ch. 4. 

[18] S. Zilberstein, “Using anytime algorithm in intelligent sys- 
tems,” AI Magazine, vol. 17, no. 3, pp. 73–86, 1996. 

[19] W. Dai, Q. Yang, G.-R. Xue, and Y. Yu, “Boosting for 
transfer learning,” in Proc. of The 24th Int’l Conf. on Machine 
Learning, 2007, pp. 193–200. 

[20] D. Boyd, “Privacy and publicity in the context of big data,” 
in Keynote Talk of The 19th Int’l Conf. on World Wide Web, 
2010. 




