






















































MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing 


176 
MoodExplorer: Towards Compound Emotion Detection via 
Smartphone Sensing 

XIAO ZHANG, Nanjing University, China 
WENZHONG LI∗, Nanjing University, China 
XU CHEN, Sun Yat-sen University, China 
SANGLU LU, Nanjing University, China 

Social psychology and neuroscience have confirm that emotion state exerts a significant effect on human 
communication, perception, social behavior and decision making. With the wide availability of smartphones 
equip with microphone, accelerometer, GPS, and other source of sensors, it be worthwhile to explore the 
possibility of automatic emotion detection via smartphone sensing. Particularly, we focus on a novel research 
problem that try to detect the compound emotion (a set of multiple dimensional basic emotions) of smartphone 
users. We observe that users’ self-reported emotional state have high correlation with their smartphone usage 
pattern and sense data. Based on the observations, we exploit a feature extraction and selection algorithm 
to find the most significant features. We further adopt a factor graph model to tackle the correlation between 
feature and emotion labels, and propose a machine learn algorithm for compound emotion detection base 
on the smartphone sense data. The propose mechanism be implement a an APP call MoodExplorer in 
Android platform. Extensive experiment conduct on the smartphone data collect from 30 university student 
show that MoodExplorer can recognize users’ compound emotion with 76.0% exact match on average. 

CCS Concepts: • Human-centered compute → Smartphones; Ubiquitous and mobile compute system 
and tools; 

Additional Key Words and Phrases: Emotion detection, Compound emotion, Smartphone sensing, Factor graph 

ACM Reference Format: 
Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu. 2017. MoodExplorer: Towards Compound Emotion Detection 
via Smartphone Sensing. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 4, Article 176 (December 2017), 
30 pages. https://doi.org/10.1145/3161414 

1 INTRODUCTION 

Mood sense be receive widespread attention from the social psychology, neuroscience, and computer 
science in the past year [14][30][52]. The mood or emotion state play an important role in human daily 
lives, which have great influence on people’s communication, perception, social behavior, and decision 

∗The correspond author be Wenzhong Li, Email lwz@nju.edu.cn. 

Authors’ addresses: Xiao Zhang, Nanjing University, State Key Laboratory for Novel Software Technology, Nanjing, Jiangsu, 
210023, China; Wenzhong Li, Nanjing University, State Key Laboratory for Novel Software Technology, Nanjing, Jiangsu, 
210023, China; Xu Chen, Sun Yat-sen University, School of Data and Computer Science, Guangzhou, Guangdong, China; 
Sanglu Lu, Nanjing University, State Key Laboratory for Novel Software Technology, Nanjing, Jiangsu, 210023, China. 

Permission to make digital or hard copy of all or part of this work for personal or classroom use be grant without fee 

provide that copy be not make or distribute for profit or commercial advantage and that copy bear this notice and 
the full citation on the first page. Copyrights for component of this work own by others than the author(s) must be 

honored. Abstracting with credit be permitted. To copy otherwise, or republish, to post on server or to redistribute to lists, 
require prior specific permission and/or a fee. Request permission from permissions@acm.org. 

© 2017 Copyright held by the owner/author(s). Publication right license to Association for Computing Machinery. 
2474-9567/2017/12-ART176 $15.00 
https://doi.org/10.1145/3161414 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:2 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

Mobile Sensors 

Emotion States 

APP Usage 

Wi-Fi 

AccelerometerLight 

Microphone 

Compass 

GPS 

Fig. 1. Emotion detection via smartphone sensing. 

making. Automatic mood detection be a challenge task, which envisions a wide range of new mood- 
aware application scenarios. For instance, people enjoy different style of music and movie, which not 
only depends on their preference, but also relates to their mood and personality. Therefore a smart 
recommendation system should take into account the mood to enhance user experience. Another example 
be advertisement, which be found interest or annoy for different person with different emotion states. 
So advertisement can be more effective and personalize if peoples’ feeling can be considered. With the 
prevalence of social network applications, share the mood among family and close friend can help 
people to strengthen their bond and improve the way of social communication. Furthermore, robot will 
be widely use in the near future in different aspect of our lives. The robot can be more intelligent and 
humanize if they can “read the mood” of the human they work for. Last but not least, understand 
the emotion state and it evolution be important to evaluate individual’s psychological health and mental 
well-being [10][19]. 

Mood detection have be report use body physiological signal such a heart beat, blood pressure, 
breath rate, etc [17][26][53]. However, monitoring such signal relies on expensive dedicate devices, which 
be infeasible for pervasive device-free detection. Some exist work sought to recognize emotion by audio 
and video signal [49]. For example, Ang et al. explore speech-based recognition for the emotion of 
annoyance and frustration [1]. Ashraf et al. detect pain expression by recognition of facial signal [3]. 
The MoodMeter propose the recognition of smile face via campus video camera [22]. However, visual 
feature recognition only reflect people’s expression in a snapshot. Besides, collect and analyze audio 
and video data be a high-computational task, and such signal cannot be capture everywhere without 
wide deployment of cameras. 

Nowadays smartphones be widely use in people’s daily life for business, social and entertainment 
purpose [4][30]. As show in Fig. 1, there be many sensor embed in modern smartphones: microphone, 
accelerometer, electronic compass, GPS, proximity, etc. The data capture from the sensor posse 
profound information and can be exploit to infer user’s social behavior such a physical movement, 
social communications, location, etc. Intuitively, the usage of smartphones and the context information be 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:3 

correlate to users’ emotions. For example, people may play game with cellphone when they be happy, 
or they may feel depressed if they live in a noisy environment. Motivated by this, several work employ 
smartphone sense data for emotion detection. Bogomolov et al. propose a multifactorial statistical 
model to recognize daily stress by comprehensively analyze mobile phone data and weather condition 
[6]. Sun et al. employ sensor data, APP usage information, and SMS content for cold-start emotion 
prediction use transfer learn [43]. However, their approach be content-based, which require users’ 
highly privacy-sensitive information such a SMS content. LiKamWa et al. propose a system for mood 
detection utilize Email, SMS, location, and App usage duration a feature [30]. What need to be 
stress be that the literature only consider single emotion detection, which assumes that people be in 
only one emotion state in a period. Different from the exist works, we weaken such assumption and 
study the co-existence of multiple emotion call compound emotion. According to Plutchik’s theory 
[34], emotion be not necessary in a pure state and could be the mixture of basic emotions. The study 
of [11][55] also show compound facial emotion in human facial expressions. For instance, “happily 
surprised” be a compound emotional expression that combine basic emotion of happiness and surprise. 
Different from the compound facial emotion that individual should experience at the same time, the 
compound emotion study in our paper refers to a set of basic emotion that an individual experienced 
in a duration, which can occur simultaneously or alternately. In this paper, we provide formal expression 
of compound emotion a a vector of multiple basic emotion with discrete levels, and propose a machine 
learn algorithm to detect compound emotion. To the best of our knowledge, this be the first work of 
compound emotion detection base on smartphone sensing, which have not be address in the past. 

Specially, we propose a system call MoodExplorer to enable compound emotion detection via cellphone. 
We first develop an Android APP to allow people to report their emotion state and collect the data of 
smartphone sensor and usage patterns. The APP be instal and test by 30 university student for 
a month, which form the dataset for model training and testing. It be observe in the dataset that about 
60% report emotion be compound emotion consist of more than two basic emotions, which verifies 
the motivation of our work. Based on the dataset, we extract different type of feature in respect of 
environment, contact, APP usage, and human activities, which be use for automatic emotion detection. 
To best tune the performance, we present a feature selection algorithm use the problem-transformation 
approach and ReliefF measure to choose the most significant features. Using the select feature a 
input, we adopt the factor graph model to represent the correlation between feature and multiple 
emotion labels, and propose a learn algorithm for compound emotion detection. We conduct extensive 
experiment on the collect dataset, which demonstrates that the exact match of compound emotion 
achieves 76.0% on average. 
The main contribution of the paper be summarize a follows. 

• We identify the compound emotion detection problem. Unlike exist work that assume human 
emotion be exclusive, we observe that people usually report their emotion state a the combination 
of several basic emotions. The compound emotion be found highly correlate with users’ smartphone 
usage pattern and sense data, therefore compound emotion detection be possible without the need 
of know people’s body physiological signal or facial expressions. To the best of our knowledge, 
smartphone-based compound emotion detection have not be address in the literature. 

• We propose a novel compound emotion detection method use smartphone sensing. Specifically, 
we extract different type of feature from the sense data to show the environment, social contact, 
APP usage and activity of individuals, and apply feature selection algorithm to find the most 
significant features. Using the select feature a input, we propose a machine learn algorithm 
to derive the probable emotion label by maximize a posteriori probability. The propose machine 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:4 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

learn algorithm us a factor graph to depict the correlation between feature and emotion label 
and the correlation across different basic emotions, which be show to be suitable for compound 
emotion detection. 

• We develop and implement the MoodExplorer system for automatic emotion detection. The propose 
system be implement a an Android APP, which be test by 30 university students. Based on 
the sense data collect by the smartphones and the emotion state report by the participants, 
we train the machine learn model and test it performance. It be show that compound emotion 
can be correctly detect by our system with 76.0% exact match on average. 

The rest of the paper be organize a follows. Section 2 provide an introduction to the related work 
about affect measurement model and emotion recognition mechanisms. Section 3 proposes the compound 
emotion model and formalizes the compound emotion detection problem. Section 4 introduces the system 
design and data collection process. Section 5 proposes data processing method include feature extraction 
and feature selection, and conduct correlation analysis of the select features. Section 6 develops an 
efficient factor graph model for compound emotion detection. Section 7 report experimental results, and 
demonstrates the performance of the propose factor graph method. Section 8 concludes the work. 

2 RELATED WORK 

2.1 Models for emotion and mood measurement 

Emotion and mood have be widely study in psychology, sociology, and neuroscience [35]. Generally, 
“emotion” refers to the current instantaneous feeling, and “mood” refers to the average feeling over a 
longer period of time. A variety of model have be propose to measure and quantify emotion and 
mood. Such model can be apply to measure both instantaneous and long-term feelings, and they can 
be use for emotion and mood measurement without restrict distinction. 
One frequently use measure for general affective state be the Positive and Negative Affect Schedule 

(PANAS) model [9]. Participants complete the PANAS be ask to rate the extent to which they 
experienced each out of 20 emotion on a 5-point Likert Scale range from ”very slightly” to ”very 
much”. Participants may be ask how they feel right now or during longer period of time (e.g. during 
the past month) accord to the purpose of the measurement of emotion or mood. 
The discrete category model [12][45] described emotion through a set of categories. One of the most 

popular model be Ekman’s six basic category [12]: happiness, sadness, anger, surprise, fear, disgust. The 
Ekman’s model be intuitive and understandable to normal users, and it allows the co-existence of multiple 
emotion with different intensive levels. With it high applicability, the Ekman’s emotion model be 
widely adopt by many study [36] [55]. 

The Circumplex mood model [35] employ a two-dimensional circumplex to represent the emotional 
state of the participants: the pleasure dimension measure the degree of positive and negative feelings, 
and the activeness dimension measure the likelihood for a user to take action under the mood state. 
Each dimension be quantify use a score, hence the Circumplex mood model provide continuous 
measurement of the mood, and have be adopt in many study [30][43]. 

2.2 Emotion recognition 

Emotion recognition be a fine-grained sentiment analysis, which aim to identify emotion from various 
information resources, such a video, image, text and so on. In the recent years, a large number of work 
have be do on this area. In the work of [22], the researcher identify smile face through the camera 
in the campus. The study found that the users’ emotional state have a cyclical pattern and be highly 
correlate with external events. In the traditional emotion classification, the emotional state be generally 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:5 

classify into one discrete category. Some work adopt the continuous probability distribution to depict 
the emotional state of an image, and propose to use the Gaussian mixture model [54] for emotion 
recognition. As for emotion analysis base on text [28], the author classify sentence-level emotion 
consider label context dependence, and formalize the problem a a multi-label classification problem, 
which allow the detection of several emotion in a single sentence. Recently, the EQ-Radio propose 
the usage of wireless signal to monitor individual’s heartbeats, which be further use a feature for 
emotion detection [53]. However, their work rely on dedicate WiFi device and on-body sensor such a 
ECG monitors. 

2.3 Detection of human mental well-being base on smartphones 

Nowadays, with the rapid adoption of smartphones, researcher have show that it be possible to adopt 
smartphone sense data to infer and detect human mental well-being such a stress, anxiety, depression, 
emotion, and mood [8][31][37][41][42][51]. 
DeepMood [8] detect bipolar affective disorder utilize type dynamic and accelerometer sensor 

data in the smartphone base on multi-view neural network. Herdem et al. [21] aim to help mobile 
individual to interact offline with friend when they need emotional support. Bogomolov et al. propose 
a multifactorial statistical model to recognize daily stress by comprehensively analyze mobile phone 
data and weather condition [6]. Canzian et al. monitor the depression state of user by mean of 
smartphone mobility trace analysis [7]. Mottelson et al. propose the detection of positive and negative 
affect use mobile commodity sensor in the wild [32]. The iSelf system [43] employ sensor data, 
APP usage information, and SMS content for emotion prediction use transfer learning. MoodScope 
[30] utilized Email, SMS contact information, website visit information, location, and App usage 
duration to detect users’ mood. However, the exist work only consider single emotion detection, 
which ignore the fact that multiple emotion may co-exist in a period. In our paper, we make the first 
attempt to solve the compound emotion detection problem in smartphone sense environment, which 
corresponds to the derivation of a multi-dimensional emotion vector with discrete level leverage a set 
of smartphone-generated sense information. 

3 COMPOUND EMOTION MODEL 

3.1 Definitions 

The phrase mood and emotion be highly related but have slight difference in several aspect [5]. Generally 
speaking, emotion be instantaneous intensive feel react to some events, while mood can be view 
a long-lasting internal emotion state of an individual [16]. Since the state-of-the-art smartphone can 
not capture every human instantaneous activity and event so far, detect instantaneous emotion be 
unlikely and impracticable. Therefore our study be interested in the measurement of short-term mood in 
a time interval of several hours. Unlike the conventional definition of mood, we particularly focus on the 
detection of compound emotion, which be define a follows. 

Definition 3.1 (Compound emotion). The compound emotion be a set of emotion that an individual 
experienced in a short duration. The duration be measure by a time interval typically within a few hours, 
during which the multiple emotion can occur simultaneously or successively. 

According to the definition, compound emotion be different from the conventional concept of emotion 
since it allows the co-existence of multiple emotions. Compound emotion be also different from the 
conventional concept of mood since it less emphasizes on the long-lasting emotion state but more focus 
on the measurement of affect in a short duration. 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:6 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

���� ��� 

���� � 
������� 
����� � 
�� 
��� 
� � 

� ����� ��� 

���� 
� 
�� 
� ��� 
�� 

������� ��������� 

������� �� 
����� 

����� ��� 
� 
�� 

������ 
���� ���� 
���� ���������� �� 
� 
�� 
�� 

���� 

Fig. 2. MoodExplorer system framework. 

The “compound emotion” be not a standard term in the literature, so we introduce our own definition 
in the paper. Some previous work also show the co-existence of multiple emotion in human facial 
expression [11]. For example, the study of [11] show 21 combine facial emotion include “happily 
surprised”, “angrily surprised”, “sadly feared”, “sadly disgusted”, etc, which be identify from the 
human facial expression accord to their images. To avoid confusion, we refer to their finding a 
“compound facial emotions”. The compound facial emotion describes multiple emotion occur at the 
same time, while our definition of compound emotion measure multiple emotion in a duration, which 
can occur simultaneously or at different times. 
The detection of compound emotion use smartphone sense be base on an implicit assumption 

that individuals’ feeling be correlate to their smartphone usage. Such correlation be rather intuitive. 
For example, one may feel happy when she shop online, or one may call a close friend when she feel 
sad. Although the cause-effect of emotion-smartphone correlation be hard to depict, we can exploit the 
correlation to infer individual’s compound emotion by explore the smartphone sense data. The 
correlation of the compound emotion and the smartphone sense data will be discuss in Section 5. 

The widespread Ekman’s discrete category model [12] consists of six basic emotion categories: happiness, 
sadness, anger, surprise, fear, and disgust. In this paper, we adopt the idea of the combination of Ekman’s 
six basic emotion category to express compound emotions. Specifically, we assign a discrete score for 
each basic emotion category, and represent the compound emotion state of an individual by a 6-tuple 
vector 

Y =< Shappy, Ssad, Sanger, Ssurprise, Sfear, Sdisgust >, (1) 

where S∗ be the score of the correspond basic emotion category, which be quantify to 5 level in 
{1, 2, 3, 4, 5} represent null, slight, moderate, strong, and extreme respectively. 

3.2 Compound emotion detection problem 

We address the problem of compound emotion detection base on the sense data collect from the 
smartphone. Given the massive source of sense information from various sensor include microphone, 
accelerometer, electronic compass, light sensor, etc., a well a the log APP usage history, it be a 
challenge task to infer compound emotion with the multimodality data. 

To achieve efficient compound emotion detection, we first need to extract the most useful information 
know a feature from the raw data. Given the obtain vector of feature denote by X, the task be to 
identify the user’s compound emotion. Particulary, we want to find a function to map the feature vector 
X to the compound emotion presentation Y : 

f : (X) → Y, (2) 
where Y be a multi-dimensional emotion vector a define in Eq. (1). 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:7 

ICollector 

AudioCollector 

GpsCollector 

LightCollector 

StepCollector 

UsageCollector 

WifiCollector 

CollectingService 

NotificationService 

GatherAlarmReceiver 

FeedbackAlarmReceiver 

MainActivity 

Fig. 3. Class structure of the MoodExplorer APP. 

In the follow sections, we will propose solution framework and methodology to solve the compound 
emotion detection problem. 

4 SYSTEM DESIGN AND DATA COLLECTION 

4.1 System framework 

We propose the system framework a show in Fig. 2 for compound emotion detection via smartphone 
sensing. First, we develop an Android APP to collect smartphone data from the users. The APP allows 
user to report their compound emotion to the server periodically, which form the ground truth and 
be use to build training set and test set. Then we extract feature from the collect data. Since the 
number of the extract feature be large, we further apply a feature selection method to choose the most 
significant feature to form the feature vector. Using the feature vector and the label instance a 
input, we train a compound emotion detection model base on factor graph. Finally, we use the test set 
with users’ report emotion to verify the performance of the propose system. 

The detailed technique be introduce in the follow subsections. 

4.2 Implementation of MoodExplorer 

We implement the APP on Android platform for collect sense data and compound emotion reports. 
Fig. 4 show the screen shot of the MoodExplorer APP. It enables several function such a reporting 
the compound emotions, search the history of reports, and show the statistic of the cellphone 
sense data and APP usages. The APP sends three notification per day, which be at 10AM, 3PM, 
and 8PM respectively, at least 5 hour apart from one another. Users can report their emotional state 
experienced in the past few hour either launch the notification or opening the APP directly. The 
APP adopts the Ekman’s six class basic emotion model and asks user to evaluate the intensity level for 
each of the basic emotion category in the scale of 1 to 5. 
In detail, the class structure of the APP be show in Fig. 3. The data collection process ICollector 

run in the background and it be invoked by the system AlarmManager periodically. To save energy, 
the data collection interval be set to 5 minutes. Each time the ICollector be invoked, it will read the 
smartphone sensors. Particularly, it read the GPS location of the smartphone; check the on/off state 
of the smartphone screen; scan the WiFi signal nearby to log the IDs of scan access point (APs) 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:8 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

Fig. 4. Screenshots of MoodExplorer. 

0 

1000 

2000 

3000 

4000 

5000 

6000 

20 60 100 140 180 220 260 300 

E 
ne 

rg 
y( 

J) 

Time(mins) 

News 
Music 
Weibo 

Taobao 
Nike+ 

M.E. 

Fig. 5. Energy consumption of different Apps. 

and the receive signal strength (RSS); and then it read the microphone, the light sensor and the 
accelerometer, electronic compass, gyroscope measurement for 15 second and record the data in a local 
database. The users’ APP usage information and social activities, which include opening/closing an APP, 
making/answering a phone call, sending/receiving a SMS, be also log a event with timestamps 
in the system. The record data be store in the smartphone’s file system and submit to a remote 
cloud server daily for analysis. To save the communication cost, the data be uploaded only when there be 
available WiFi connection. 

The APP do not record privacy-sensitive information such a the content of the SMS and the voice 
during a call. All cellphone number and user name be anonymized by mapping them to random IDs. 
The SSIDs of the APs be also mapped to random IDs to conceal their real name. The GPS coordinate of 
the user be collect under the users’ permission (the APP will ask for the authorization to use the GPS 
data when the APP be installed). If a user concern about her location privacy, she can simply turn-off 
the GPS or unauthorise GPS to MoodExplorer to avoid be tracked by our APP. For the APP usage, 
we only record the category (e.g., shopping, entertainment, social networks, etc) of the APPs without 
record their actual names. For the sensor data such a microphone sound and light, we compute their 
mean, variance, and other measurement in a duration a introduce in the feature extraction, and submit 
the feature measurement to the server without keep the original data. 

4.3 Energy consumption of MoodExplorer 

Energy consumption be one important issue for smartphone. To evaluate the energy consumption, we 
compare the energy consumption of MoodExplorer with several other widely use applications: a news APP 
(Today’ Tops), a music APP (Kugou Music), a social network APP (Weibo), a shopping APP (Taobao), 
and a fitness APP (Nike+). We run the APPs for 5 hour and compare their energy consumption accord 
to the battery logs. The result be show in Fig. 5. As show in the figure, the energy consumption of 
MoodExplorer be not severe, which be very close to the shopping APP (Taobao) and the social network 
APP (Weibo). It consumes only half energy of the music APP (Kugou Music). The news APP (Today’ 
Tops) and fitness APP (Nike+) consumes much more energy than MoodExplorer since they may acquire 
sensor data and network communication frequently. The experiment show that sample sensor data 
every 5 minute in MoodExplorer do not affect the usage of smartphones and the energy consumption 
be in an acceptable level. 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:9 

������ ���� 
� 

!"�""� #� 
� 

(a) Gender. 

���������� 
�� 
����������� 
�� 
����������� 
� 
����������� 
�� 
����������� 
� 
����������� 
�� 

(b) Age. 

�"�""� ����� 
�� 
""�""� #����� 
$"�""� ����� 

(c) Education. 

Fig. 6. Demographics of the 30 participants. 

4.4 Data collection 

To test the MoodExplorer APP, we recruit 42 student volunteer to participate in the experiment of 
data collection. The student be ask to install the MoodExplorer APP in their smartphones, and to 
utilize the APP to report their emotion three time per day around 10AM, 3PM, and 8PM. We do 
not restrict the time to report their emotions, but we require that the interval of consecutive report 
should be longer than 5 hours. The student volunteer be study computer science, engineering, and 
business in our university, who have adequate computer skill and utilized smartphones actively. They 
be told about the research purpose of the experiment and be aware of that their data will be use 
for emotion study. To encourage the student to involve in the experiments, we sent them a thank-you 
gift (e.g., a USB flash drive, a mobile MicroSD memory card, a T-shirt, etc) after they submit enough 
number of emotion reports. The data collection last for about one month, from June. 15 to July. 14, 
2016. According to the return results, we have 30 student that submit emotion report more than 
50 times, which be use in our analysis. 
Among the 30 students, there be about 57% female and 43% males. Their age be from 18 to 30, 

and be mostly concentrate in the range of 21-24 and 27-30. There be about 53% undergraduates, 33% 
master students, and 14% PhD students. The demographic of the participant be show in Fig. 6. 

5 FEATURE EXTRACTION AND SELECTION 

5.1 Data observation 

After data collection, we need to preprocess the dataset. We mainly remove the unqualified submission 
which cause by miss sense data or compound emotion labels. If a user fail to submit the report 
in an interval, then the correspond data will not be use for the model due to the lack of label. If 
part of the sense data be miss in an instance, for example, a user may disable the GPS in some 
time intervals, then the correspond feature will be set to “null”. The null feature will not affect the 
model and detection accuracy much since they be unlikely to be chosen to build the model after feature 
selection. 
We make some observation to the collect dataset after data preprocessing. We first observe the 

distribution of number of qualify submissions, which be show in Fig. 7. As show in the figure, there 
be about 57% (17) user submit qualify compound emotion report more than 80 times, and 2 
user continue to submit report after one month. Most of the participant submit more than 60 
qualify reports, which be use to form the training set and test set for our model. 

Then we study the number of emotion label report by the individuals, which distribution be show 
in Fig. 8. As show in the figure, about 41% report have single kind of emotion; about 40% report 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:10 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

contain two kind of emotions, and about 13% contain 3 kind of emotions. In another words, there 
be near 60% report instance that be compound (multi-dimensional) emotions. This illustrates that 
compound emotion be very common in daily life, which verifies the key motivation of our study. 

Removing the “null” emotions, we also show the distribution of different level of the report compound 
emotion in Fig. 10. As show in the figure, happiness be the most common emotion that contains in the 
user reports, where about 14% be moderate, about 11% be slight, about 7% be strong and about 4% 
be extreme. Sadness be the second common emotion, which yield about 10.5% be slight and about 5% 
be moderate. Other emotion such a anger, surprise, fear and disgust be less common, and most of 
them be less than 10% in the total user reports. 
We further investigate the combination of basic emotions. If two basic emotion co-exist in the same 

report, we consider them correlated. We calculate the fraction of correlate emotion in the users’ reports, 
which be illustrate in Fig. 9, where thicker edge corresponds to high frequency of co-existence. As 
show in the figure, several emotion pairs, such a sadness&disgust, sadness&fear, happiness&surprise, 
frequently appear together. This suggest that some combination of basic emotion be very common in 
the experiments. Obviously, not all combination be meaningful for humans. Some emotion pair such a 
happyness&anger, happiness&disgust, anger&surprise, rarely appear together in our observations. 

In summary, we observe that compound emotion be very common in the user reports, and some of 
the emotion pair be highly correlate in the collect dataset. Such information will be exploit to 
design a machine learn algorithm for emotion detection. 

5.2 Feature extraction 

With the data collect from the mobile users, we need to process them into feature presentation. 
Specifically, the raw data be process to different type of feature include the situation of environment, 
contact, APP usage, and activity, which be discuss below. 

5.2.1 Environment. Environmental situation be know to influence the feel of users. The environ- 
ment feature can be represent by the microphone and the light sensor information from the environment. 
The microphone log the sound “heard” by the smartphone, and we take the mean and variance to indi- 
cate the volume and dispalcements. According to [40], a sound be consider to be noise when the volume 

exceeds some threshold: 85-90 dBA. We further define the noise ratio NR = number of noise samplestotal number of audio sample , 

silence ratio SR = 1−NR and noise-silence ratio NSR = NRSR to represent the noise condition of the 
environment. 
Similarly, we take the mean and variance of light sensor a features, and use the dark ratio (DR), 

bright ratio (BR), dark-bright ratio (DBR) to represent the illumination and to infer the indoor/outdoor 
duration of an individual. 

In term of location information, GPS can be use to infer the location of a user outdoor. However, it 
cannot be use for indoor environment. On the other hand, WiFi have be use a a mean of indoor 
position [13], which be also an important location information. In our system, we record the SSIDs of 
WiFi access point scan by the smartphones every 5 minutes, and count the frequency each SSID 
appear in the WiFi log. Then we choose the frequency of the top N occur SSIDs of each user a the 
indoor location feature, which approximately indicate the user’s often visit indoor locations. 

5.2.2 Contact. According to the study of [2], peoples’ emotion state be influence by their friend or 
the socially contact persons. Contact feature represent the users’ social connection and behaviors. From 
the call/SMS logs, we extract the call duration, and call/SMS frequency to represent the contact features. 
Call duration measure how much time the user spent on communicate with a specific contact through 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:11 

0 

5 

10 

15 

20 

50 60 70 80 90 100 

N 
um 

be 
r 

of 
u 

se 
r 

Number of qualify mood submission 

Fig. 7. Distribution of the number of 
qualify submission 

0 

0.05 

0.1 

0.15 

0.2 

0.25 

0.3 

0.35 

0.4 

0.45 

0 1 2 3 4 5 

Fr 
ac 

tio 
n 

Number of emotion label 

Fig. 8. Distribution of the number of 
emotion labels. 

����� 

��� 

���� 

�� 
� 
�� 

��� 

� 
����� 

Fig. 9. Correlation of basic emotion 
categories. 

0 

0.02 

0.04 

0.06 

0.08 

0.1 

0.12 

0.14 

0.16 

Happiness Sadness Anger Surprise Fear Disgust 

Fr 
ac 

tio 
n 

Emotion label 

Slight 
Moderate 

Strong 
Extreme 

Fig. 10. Distribution of emotion levels. 

0 

5 

10 

15 

20 

25 

30 

Acc. Gy. Audio light Screen App Wifi Contact 

N 
um 

be 
r 

of 
a 

pp 
ea 

ra 
nc 

e 

Features 

Fig. 11. Freq. of the category of the 
select features. 

0 

5 

10 

15 

20 

0.5 0.6 0.7 0.8 0.9 1 

N 
um 

be 
r 

of 
U 

se 
r 

Average Jaccard Similarity 

Fig. 12. Distribution of Jaccard simi- 
larity 

Table 1. Description of the extract features. The number in bracket indicates the number of the extract feature 
for that category. The star mean that the number of extract feature be not fix and it varies from person to person. 

Type Data source category Extracted feature description 

Environment 

Microphone Audio (5) Mean, Variance, NR, SR, NSR 
Light Sensor (5) Mean, Variance, DR, BR, DBR 

GPS (3) GPS Longitude, Altitude, Latitude 
WiFi (20) Frequency of SSIDs of the top 20 APs for an user 

Contact 
Phone Call (*) Call frequency and duration of each contact person 

SMS (*) SMS frequency of each contact person 
APP Usage APP log (18) Duration of 18 APP Categories 

Activity 

Accelerometer (7) Mean and Variance of three axis (X, Y, Z), Step Count 
Compass (6) Mean and Variance of three axis (X, Y, Z) 
Gyroscope (6) Mean and Variance of three axis (X, Y, Z) 
Screen (4) Screen on ratio, off ratio, Sleeping Duration, Usage Amount 

phonecall in a time interval. Similarly, call/SMS frequency measure how many time the user have call 
or sent message to a contact during the interval. 

5.2.3 APP Usage. Intuitively, people tend to use different kind of APPs under different emo- 
tions/moods. Due to the large amount of APPs, we do not use a concrete APP a a feature. Instead, we 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:12 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

ALGORITHM 1: Feature selection algorithm 

Input: Multi-label feature matrix M 
Output: Selected feature set S 
1: Transform M to k single label feature matrix 
2: Use ReliefF to measure importance of feature in every single label dataset 
3: Output a weight vector W for all the feature 
4: Sum up W of all the k single label datasets 
5: Sort the feature accord to the sum up weight 
6: Choose the top N feauters a the select feature set S 

classify the APPs into 18 categories. Such category can be directly obtain from the Android Market 
such a Google Play1. The 18 category are: Mother and child, Traffic navigation, Image, Office efficiency, 
Education, News, Travel, Life tools, Life services, Telephone communication, System tools, Smartphone 
beautification, Social chat, Vedio, Shopping, Health, Financial management, and Music. For each APP 
launch by the user, we use a tuple < Category, duration > to denote the APP usage. 

5.2.4 Activity. Activities such a go for sport or lie on the bed may imply peoples’ different 
emotions/moods. The users’ activity can somehow be derive by the accelerometer, electronic compass, 
and gyroscope sensor that record the human movement condition when carry a smartphone. The 
accelerometer record user movement in three dimensions: X (the direction of front and back), Y (the 
direction of left and right), and Z (the direction of up and down) . The electronic compass record the 
orientation in the form of an angle with respect to magnetic north, and the gyroscope sensor record the 
position of the smartphone. All of them can reflect the movement state of the mobile users. 
Since the accelerometer data have clear periodic pattern when the user be walk and running, such 

property have be use by many work to count the step of a user [29]. In our paper, we also take step 
count a a feature of user activities. 
Exploring the on/off pattern of cellphone screen lead to several interest features, such a the 

sleep duration, which can be estimate by the long off-screen interval, and the phone usage amount, 
define by the proportion of screen on-to-off duration, which indicates the time that a user spent on 
play with the smartphone. 

The overall feature extract from the dataset be summarize in Table 1. The integer in the bracket 
indicate the number of the extract feature from the type of data. Note that we do not put a specific 
number on the Phone call and SMS data since the number of contact of different user be variant. 

5.3 Feature selection 

After feature extraction, we get 100 more feature from the raw data. However, not all feature play an 
equal role in compound emotion detection. Some feature may show high correlation tp the labels; some 
may be less relevant; some of them may be redundant; and some may be noisy. Therefore we apply the 
feature selection technique to reduce the number of feature to improve the efficiency of the machine 
learn model. 

Feature selection be an important data pre-processing step in machine learning. It aim to find a subset 
of feature X∗ ⊆ X to describe the dataset a well a X does, where X be the original feature space. When 
learn from the high-dimensional data, feature selection provide a good way to reduce the dimensions. 

1play.google.com 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:13 

Table 2. Top 6 select feature of 4 randomly users. 

User First Sel. Fea. Sec. Sel. Fea. Third Sel. Fea. Fourth Sel. Fea. Fifth Sel. Fea. Sixth Sel. Fea. 

1 
Bright Dark Mean of Light Variance of Freq. of Call Duration of 

Ratio (BR) Ratio (DR) Sensor Value Audio Volume SSID #1873 Contact #1436 

2 
Step Count Mean of Mean of Variance of Duration of Call Freq. of 

Gyro. Z axis Gyro. X axis Gyro. Y axis Shopping APP Contact #1037 

3 
Screen On Screen Off Call Freq. of Call Durations of Freq. of Freq. of one 

Ratio Ratio Contact #1686 Contact #1686 SSID #1074 SSID #2244 

4 
Freq. of Freq. of Freq. of Duration of Social Duration of Duration of 

SSID #3541 SSID #5541 SSID #1712 Chat APP Video APP Music APP 

Since the report compound emotion consists multiple basic emotions, we take each basic emotion a 
a label, and the compound emotion dataset can be view a a multi-label dataset. We adopt the problem 
transformation approach [38] for feature selection, which work a follows. Firstly, we use the Binary 
Relevance (BR) [46] method to transform the multi-label dataset to k single label datasets, where k be 
the number of label (which equal 6 in our system). Secondly, we exploit ReliefF (RF) [39] a feature 
evaluation measure for each single label dataset. ReliefF output a weight w for each feature to represent 
it significance. The large weight w is, the more important the feature is. Thirdly, for each feature we 
sum up it RF weight in all single label datasets, which represent it overall importance. Finally, we 
sort the feature accord to their total RF weights, and select the top N feature to be use by the 
learn model. The detail be show in Algorithm 1. 

In Appendix A, we show the RF weight of all the extract feature of a user. It show that there be 
110 feature extract from the algorithm. The weight of different feature be quite different. Only very 
few feature have weight large than 1, while a large number of feature have small weights. It suggests 
that a few number of feature could be sufficient to build a learn model. In this paper, the default 
number of feature use for the learn model be 6, and the influence of the number of select feature 
on the system performance be discuss in Section 7.4. 
Table 2 show the chosen feature for 4 random users. It be interest to see that different user 

have different type of feature to indicate their emotional states. Three of the users’ emotion state 
be correlate to their contacts’ calls, which implies that social connection be influential to human 
emotions. Some of them be correlate to different APPs they use for shopping, entertainment, and 
social communications. User 1’s emotion state be sensitive to the ambient light and sounds. The emotion 
state of user 2 be more related to his motion such a step count and accelerometer. User 3’s emotion 
state seem to be more correlate with smartphone usage duration and location. While the emotion tate 
of user 4 be heavily related to APP usage and indoor locations. Based on the observations, different user 
have different set of feature that correlate to their compound emotions. This suggests that personalize 
model should be built individually to infer the users’ compound emotions. The select feature of all the 
30 participant be list in Appendix B. 

Fig. 11 show the frequency of the category of the select feature in the collect dataset. As show 
in the result, the contact information (the social connection), the WiFi information (the indoor location 
information) and the APP usage be the top three most significant data source related to users’ emotion 
labels. This indicates that who the user contacts, where the user have been, and which APPs the user 
utilizes daily have great influence on the user’s emotion states. Other important feature include the 
light sensor (the ambient brightness), the accelerometer (individuals’ physical movement status), and the 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:14 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

0 

2 

4 

6 

8 

10 

12 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

Absolute correlation 

(a) Absolute correlation of the 

first select feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

P-values 

(b) P-value of the first select 

feature. 

0 

2 

4 

6 

8 

10 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

Absolute correlation 

(c) Absolute correlation of the 

second select feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

P-values 

(d) P-value of the second se- 

lected feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

Absolute value of correlation 

(e) Absolute correlation of the 
third select feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

p value 

(f) P-value of the third se- 
lected feature. 

0 

2 

4 

6 

8 

10 

12 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

Absolute correlation 

(g) Absolute correlation of the 
fourth select feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

P-values 

(h) P-value of the fourth se- 
lected feature. 

0 

2 

4 

6 

8 

10 

12 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

Absolute correlation 

(i) Absolute correlation of the 
fifth select feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

P-values 

(j) P-value of the fifth select 
feature. 

0 

2 

4 

6 

8 

10 

12 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

Absolute correlation 

(k) Absolute correlation of the 
sixth select feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

P-values 

(l) P-value of the sixth select 
feature. 

Fig. 13. Histograms of the correlation and the p-values for top 6 select feature (the number of the select feature 
be 6 in default, the influence of the number of select feature be discuss in section 7.4). 

audio (the environmental noise), which also reflect different aspect of human activity and behavior 
that have an effect on users’ compound emotions. 

To test whether the select feature change over time, we make observation on the individuals’ feature 
set select from different time periods. Specifically, for each user, we apply the feature selection to 
select the top-six feature in the first 20 day to form a feature set F1, and then we use the same method 
to select a feature set F2 in 30 days’ long. We adopt the Jaccard Coefficient [27] to measure the similarity 
of the two feature set F1 and F2, which be define a J(F1,F2) = |F1∩F2||F1∪F2| . The high J(F1,F2) value 
mean the more similar between F1 and F2 and the less vary of feature sets. We show the distribution 
of Jaccard similarity of the 30 user in Fig. 12. As show in the figure, half of the user have Jaccard 
similarity large than 0.9, which mean their feature set be almost unchanged in a month. About 6 
users’ Jaccard similarity be between 0.8 and 0.9, and very few users’ Jaccard similarity be less than 
0.7, which implies that the feature set of a few user change slowly over time. In summary, all the users’ 
Jaccard similarity be large than 0.5, which mean the chosen top-six feature be stable and they will 
not change over time dramatically. It implies that our learn model can be apply for a relative long 
time and it do not need to be update frequently. 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:15 

5.4 Correlation analysis 

After select feature base on Alg. 1, we conduct correlation analysis between the select feature and 
the emotion label for each user. We compute the Pearson correlation coefficient [47] use the similar 
method mention in [7][18]. We also compute the p-value associate to each correlation value. The 
p-value come from the hypothesis test, in which the null hypothesis be that the correlation value equal 
0. The p-value represent the probability that the null hypothesis be true. A small p-value (such a 0.1) 
indicates that the null hypothesis can be reject and the correlation value be significant. 

Since the emotion label be multiple in our scenario, we calculate the correlation value between each 
select feature and each emotion label respectively. A select feature can be consider significant 
if it have significant correlation with at least one emotion label (i.e., p < 0.1). The histogram of the 
correlation coefficient and the p-values for the users’ top six select feature be show in Fig. 13. As 
show in the result, the distribution of the absolute correlation value be uneven, which be dynamic 
variant between 0 to 0.6. However, one thing in common be that the p-values of the select feature be 
mostly concentrate on p < 0.1. For example, for the first select feature show in Fig. 13(b), about 
57% users’ p-values be less than 0.1. Similarly, for the second to the sixth select feature, there be 
majority of user with p-values in the range (0, 0.1). Since p < 0.1 mean significant correlation accord 
to the theory of significant test, it confirms that the select top six feature be significantly correlate 
to the users’ emotion states, which be reasonable to be use for compound emotion detection. 

6 FACTOR GRAPH MODEL FOR COMPOUND EMOTION DETECTION 

6.1 Intuition 

According to the observation in section 5.1, compound emotion be the combination of the six basic 
emotion categories, and there exists high correlation among some pair of basic emotion such a happi- 
ness&surprise. A machine learn algorithm should take into account such correlation for compound 
emotion detection. In this paper, we adopt the factor graph model [24] to describe the correlation between 
basic emotion and the correlation between feature and emotion labels. Based on the propose factor 
graph model, we can formulate the conditional probability of the compound emotion vector give the 
observe feature vector, and apply the Maximum a Posteriori (MAP) [33] principle to derive the most 
probable emotion label for compound emotion. The detail be introduce in the follow subsections. 

6.2 Model description 

In the factor graph model, each node in the graph represent a variable, and the edge represent the 
correlation between variables, which be call factor function. Fig. 14 show the factor graph use for 
compound emotion detection, which describes variety of correlation among feature and emotion labels. 
Specially, a vector Xt be use to denote the input feature vector in the t-th time interval of the user’s 
data trace, which contains N feature attribute after feature selection. And a vector Y t = (yt1, ..., y 

t 
K) 

be use to denote the compound emotion vector at the t-th time interval, where yti be the ith emotion 
label and K be the number of basic emotion catalogue (K=6 for the Ekman’s model). To describe the 
correlation between feature and emotion labels, similar to the traditional multi-label learn [28], we 
adopt the problem transformation approach [38] to replicate the feature vector Xt by k copies, denote 
a {Xtk}Kk=1, such that we associate each copy Xtk with each emotion label ytk. With such expansion, the 
factor graph model of our problem be show in Fig. 14, where each node in the upper layer corresponds 
to an emotion label; and each node in the low layer corresponds to a copy of the feature vector. 

The goal of the propose factor graph model be to describe the conditional probability of users’ emotional 
state give the feature of the smartphone usage trace. Particularly, the factor graph factorizes the 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:16 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

$ $% & ' 
� � 

� � � 

� 

�� 

� 

�� 
$ 

�� 
( 

�� 

� 

�� 

! 

�� 
" 

��$�� ( 
�� 

� 

�� 
! 

�� 
" 

�� 

! � 
% & ' 

� �� � � 
( "% & ' 
� �� � � " ! 

% & ' 
� �� � � 

" "% & ' 
� � 

� � � ! !% & ' 
� � 

� � � 

" � 
% & ' 

� �� � � 
$ (% & ' 
� �� � � 

Fig. 14. The factor graph model. 

global probability distribution a a product of local factor functions, each of which depends on a subset 
of the variable (nodes) [24][48]. Therefore, base on the observation in section 5.1, we introduce two 
kind of factor function (corresponding to the edge in the factor graph) to account for the correlation 
between feature and emotion label a well a the correlation across different emotion labels: 

• Feature-Label Factor Function: f(Xtj , ytj) represent the correlation between the feature vector 
Xtj and the emotion label y 

t 
j . The red rectangle in Fig. 14 represent the feature-label factor function. 

• Label-Label Factor Function: w(yti , ytj) represent the correlation between the emotion label yti 
and the emotion label ytj . The yellow rectangle in Fig. 14 represent the label-label factor function. 

The factor function can be define in many way to reflect the correlation factors. In this paper, we 
follow the fundamental Hammersley-Clifford theorem [20], which model the correlation factor by the 
exponential-linear function in a Markov random field. We elaborate the factor function in detail a 
follows. 
1) The feature-label correlation factor function. Following the Hammersley-Clifford theorem [20], we 

define the feature-label factor correlation function a an exponential-linear function: 

f(Xtk, y 
t 
k) = 

1 

Z1 
exp{ 

N∑ 

n=1 

αnkΦ(x 
t 
nk, y 

t 
k)}. (3) 

The notation of the equation be explain a follows. The notation xtnk (1 ≤ n ≤ N) indicates the 
n-th feature of the feature vector Xtk. The function Φ(x 

t 
nk, y 

t 
k) be a binary indicator. For example, 

Φ(xtnk = “true”, y 
t 
k = “strong”) mean that if the n-th attribute in the feature vector (represented by 

xtnk) be “true” and the level of the user’s k-th component in the emotion vector (represented by y 
t 
k) be 

”strong”, then the indicator function value be 1, otherwise 0. The weight αnk describes how strong the 
correlation between xtnk and y 

t 
k is, which be the model parameter to be learn by the machine learn 

algorithm. Z1 be a normalization term to ensure the sum of the probability equal to 1 [44]. 
2) The label-label correlation factor function. Since there be multiple emotion labels, if we take into 

account all the possible correlation among any two labels, it will result in the explosive growth of the 
learn parameters, hinder the performance of our model. Moreover, accounting all emotion label pair 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:17 

ALGORITHM 2: Construction of Label Correlation Tree 

Input: Label matirx M , Number of label K 
Output: Label correlation tree G = (V,E) 
1: Count the time tij that label pair < li, lj > coexist in each row of M , 

T = {< li, lj >, tij}Ki,j=1&i �=j 
2: Sort T accord to the time tij 
3: for each label pair < li, lj > in T do 
4: if |E| < K − 1 then 
5: if li ∈ V & lj ∈ V then 
6: if |E| = |V | − 2 then 
7: V = V ∪ {li, lj}; 
8: E = E ∪ {(li, lj)}; 
9: else 

10: continue; 
11: end if 
12: else 
13: V = V ∪ {li, lj}; 
14: E = E ∪ {(li, lj)}; 
15: end if 
16: else 
17: break; 
18: end if 
19: end for 

may cause a cycling structure in the factor graph, which make exact inference difficult [44]. Therefore, 
we design an algorithm to elicit the strong correlation label pair and meanwhile avoid the cyclic structure 
in the factor graph. 

The propose algorithm work a follows. First, we sort the time that two emotion label coexist in the 
same interval in a descend order. Then, we select the emotion label pair one by one in accordance 
with the order. Each label pair corresponds to an edge in the factor graph. If an edge do not cause 
cyclic structure, it will be add into the factor graph; otherwise it will be discarded. The process be 
repeat until a tree structure call label correlation tree be formed. An example of the construct 
label correlation tree be show in the upper layer of the factor graph in Fig. 14. The detail algorithm be 
illustrate in Algorithm 2. 

With the obtain label correlation tree, and follow the Hammersley-Clifford theorem [20], we define 
the label-label correlation factor function w(yti , y 

t 
j) for each edge in the tree as: 

w(yti , y 
t 
j) = 

1 

Z2 
exp{βijΦ(yti , ytj)}, (4) 

where yti and y 
t 
j be emotion labels; Φ(y 

t 
i , y 

t 
j) be a binary indicator to show whether two emotion label 

be correlate in the factor graph (1 mean correlate and 0 otherwise); the weight βij quantifies the 
influence degree between two different emotion labels, which be the model parameter to be learn by the 
machine learn algorithm; and Z2 be a normalization term to ensure the sum of the probability equal 
to 1. 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:18 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

ALGORITHM 3: Learning algorithm 

Input: Learning rate η 
Output: Model parameter θ 
1: Initialize θ ← 0 
2: repeat 
3: Calculate ED[Φ(x 

t 
nk, y 

t 
k)] and ED[Φ(y 

t 
i , y 

t 
j)] use the training dataset 

4: Calculate Eθ[Φ(x 
t 
nk, y 

t 
k)] and Eθ[Φ(y 

t 
i , y 

t 
j)] use BP algorithm 

5: Calculate the gradient ∂L(θ) 
∂θ 

accord to Eqs.(7) and (8) 

6: Update parameter θ a θ = θ + η ∂L(θ) 
∂θ 

7: until Convergence 

6.3 Objective function 

Assume there be T total time interval in the experiment. Let X = (X1, ..., XT ) be the sequence 
of observe feature vector over T time intervals, and Y = (Y 1, ..., Y T ) be the sequence of the users’ 
compound emotion vector accordingly. 

Following the common principle of factor graph [23][25][44], give the observationsX and the correlation 
factor functions, the conditional probability of Y can be described by the product of all factor function 
in the factor graph, which be express a 

P (Y |X, θ) = 
T∏ 

t=1 

K∏ 

k=1 

∏ 

∀ytj∈Δ(ytk) 
f(ytk, X 

t 
k)w(y 

t 
k, y 

t 
j), (5) 

where Δ(ytk) be the set of emotion label node have an edge with y 
t 
k in the factor graph; and θ = 

({αnk}, {βij}) be model parameter to be learned. 
The conditional probability give in Eq. 5 form the objective function to be maximized, which be 

discuss in the next subsection. 

6.4 Model learn 

Given Eq. 5, we would like to determine the optimal system parameter θ = ({αnk}, {βij}) to maximize 
the objective function (which corresponds to find an optimal mapping from the feature vector to the 
compound emotion vector with maximum probability). To achieve this task, the maximization problem 
can be transform to minimize the follow negative log-likelihood function: 

L(θ) = − logP (Y |X, θ) + λ 
2 
( 

N∑ 

n=1 

K∑ 

k=1 

αnk 
2 + 

K∑ 

i=1 

K∑ 

j=1 

βij 
2), (6) 

where the last term be a L2-regularization penalty, which be commonly use in data mining to prevent 
overfitting [23]; and λ be the weight of penalty factor. 

We apply the gradient decent method [23] to learn the parameter θ. We obtain the gradient for our 
factor graph model a follows: 

∂L(θ) 

∂αnk 
= Eθ[Φ(x 

t 
nk, y 

t 
k)]− ED[Φ(xtnk, ytk)] + λαnk, (7) 

and 
∂L(θ) 

∂βij 
= Eθ[Φ(y 

t 
i , y 

t 
j)]− ED[Φ(yti , ytj)] + λβij . (8) 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:19 

Where Eθ[·] be the expectation of feature value with respect to the model parameter θ, and ED[·] be the 
average value by counting the give pattern over the give training dataset. The learn algorithm be 
summarize in Algorithm 3, wherein we apply the belief propagation (BP) algorithm [15] to infer the 
expectation value Eθ[·]. 
Given the learn parameter θ, we can infer user’s compound emotion base on the principle of 

Maximum a Posteriori (MAP), i.e., find the compound emotion vector that maximize the likelihood 
give the learn parameter θ a 

argmax 
y 

P (Y = y|X, θ). (9) 

Similarly, we use the belief propagation (BP) algorithm to calculate the marginal probabilities. Hence, 
the emotion vector with the high probability will be obtain a the output of compound emotion 
detection. 

7 PERFORMANCE EVALUATION 

In this section, we conduct experiment on the collect dataset to analyze the performance of the 
propose factor graph base compound emotion detection method. 

7.1 Experiment setup 

The experiment be base on the smartphone data collect from 30 students. Since different user may 
have different set of features, we run the feature selection algorithm and construct the factor graph 
model for each user individually. By default, we use 70% data a training set to build the model, and use 
the remain 30% data a the test set for performance evaluation. 
We compare the performance of the propose algorithm with three baseline classification algorithms: 

Decision Tree (DT), Support Vector Machine (SVM), and Logistic Regression (LR). We adopt the problem 
transformation approach that transforms the compound emotion detection problem into the detection of 
each dimension of basic emotion independently. To do that, we transform the obtain multi-label dataset 
to k single-label datasets, apply the classification algorithm on each single-label dataset separately, and 
combine the result to form the compound emotions. We do not utilize the conventional multi-label 
classifier such a ML-kNN and ML-DT due to the fact that form a large label space for multi-label 
classification will lead to the short of input instance for model training. 

7.2 Performance metric 

Let m be the size of the test set, and k be the number of basic emotion catalogue (k=6 in the Ekman’s 
model). Denoted by R = {R1, ..., Rm} be the set of emotion report in the test set (ground truth), 
where Ri = (ri1, ..., rik) (1 ≤ i ≤ m) be the emotion vector of the i-th instance in the set. Denoted by 
S = {S1, ..., Sm} be the set of emotion vector derive from our compound emotion detection algorithm, 
where Si = (si1, ..., sik) (1 ≤ i ≤ m) be the i-th infer emotion vector. We use the follow metric for 
performance evaluation in the paper. 

• Exact match: It evaluates the percentage of emotion label that be correctly detect by the 
algorithm, which be calculate by 

ExactMatch = 
1 

mk 

m∑ 

i=1 

k∑ 

j=1 

I(rij , sij), (10) 

where I(·) be an indicator function, I(rij , sij) = 1 when rij = sij , and 0 otherwise. 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:20 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

Table 3. Mean value of different metrics. 

Mean value DT SVM LR Factor Graph 

Exact match(%) 71.9 71.4 72.3 76.0 
MAE 0.380 0.395 0.380 0.322 

Accuracy(%) 54.4 55.9 54.7 62.9 
Precision(%) 68.0 65.5 67.9 80.2 
Recall(%) 60.0 64.2 60.2 66.2 
F1-score(%) 64.1 65.8 64.1 72.5 

• Mean Absolute Error (MAE): MAE be define a the average difference between the infer 
emotion vector and the ground truth, which be give by: 

MAE = 
1 

m 

m∑ 

i=1 

M(Ri, Si), (11) 

where M(Ri, Si) = 
∑k 

j=1 |rij − sij | be the absolute error between two vectors. 
Like traditional multi-label learn classification, we also adopt four widely use performance metric 

[50] include accuracy, precision, recall and F1-score. 

• Accuracy: It evaluates the proportion of correctly infer emotion label to the total number of 
label for each instance. The overall correctness be the average on all the instances, which be give by 

Accuracy = 
1 

m 

m∑ 

i=1 

|Si ∩Ri| 
|Si ∪Ri| , (12) 

where Si ∩Ri be the set of non-null common emotion label of Si and Ri, while Si ∪Ri be the set of 
non-null distinct emotion label respectively. 

• Precision: It be the the proportion of the correctly infer emotion label to the total number of 
infer emotion label for each instance. The overall precision be the average on all the instances: 

Precision = 
1 

m 

m∑ 

i=1 

|Si ∩Ri| 
|Si| . (13) 

• Recall: It be the the proportion of the correctly infer emotion label to the number of actual 
emotion label for one instance. The overall recall be the average on all the instances: 

Recall = 
1 

m 

m∑ 

i=1 

|Si ∩Ri| 
|Ri| . (14) 

• F1-score: It be a weight harmonic mean between the precision and the recall: 
F1 = 

2 ∗ Recall ∗ Pr ecision 
Recall + Pr ecision 

. (15) 

7.3 Numerical result 

We run the factor graph base compound emotion detection algorithm and the baseline algorithm for 
each user in the dataset, and compute their performance metrics. The mean value of the performance 
metric be compare in table 3. It be show that the propose algorithm outperforms the baseline 
algorithm in all performance metrics. For instance, the propose compound emotion detection method 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:21 

0 

0.2 

0.4 

0.6 

0.8 

1 

0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 

C 
D 

F 

Exact match 

FG 
DT 

SVM 
LR 

(a) CDF of Exact match. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

C 
D 

F 

MAE 

FG 
DT 

SVM 
LR 

(b) CDF of MAE. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.2 0.4 0.6 0.8 1 

C 
D 

F 

Accuracy 

FG 
DT 

SVM 
LR 

(c) CDF of Accuracy. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0.2 0.4 0.6 0.8 1 

C 
D 

F 

Precision 

FG 
DT 

SVM 
LR 

(d) CDF of Precision. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

C 
D 

F 

Recall 

FG 
DT 

SVM 
LR 

(e) CDF of Recall. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 

C 
D 

F 

F1-score 

FG 
DT 

SVM 
LR 

(f) CDF of F1-score. 

Fig. 15. CDF of metric of different methods. 

achieves 76.0% exact match on average, which have +4.1%, +4.6% and +3.7% improvement compare 
with DT, SVM and LR respectively. The propose algorithm have MAE 0.322, which be much low than 
that of the other algorithm (above 0.380). The accuracy, precision, recall, and F1-score of the propose 
algorithm all show significant improvement compare with the baselines. The reason lie in that the 
propose factor graph model take into account the correlation between emotion labels, while the baseline 
algorithm predict the emotion label individually without consider their correlations. 

The CDF (Cumulative Distribution Function) of exact match be compare in Fig. 15(a). As show in 
the figure, for the factor graph algorithm, over 80% user achieve exact match large than 70%; but for 
DT, SVM, and LR models, only about 56% user have exact match large than 70%. 
Fig. 15(b) compare the CDF of MAE for different approach in compound emotion detection. As 

show in the result, for the factor graph algorithm, more than 80% user have MAE small than 0.4, 
which be much good than DT, SVM and LR. The average MAE of different approach be show in 
Table 3. The propose factor graph method have the low MAE, which achieves 15%, 18% and 15% 
improvement compare with DT, SVM, and LR respectively. 
Accuracy, precision, recall and F1-score be four commonly use performance metric in traditional 

muti-label learning, and their CDFs be show in Fig. 15(c), 15(d), 15(e), 15(f) respectively. As show in 
the figures, the distribution curve of the factor graph algorithm be more concentrate to the right part 
of the figures, which mean that more user achieve high accuracy, high precision, high recall, and 
high F1-score compare with the baselines. The average value of the performance metric be show in 
Table 3. Compared with DT, SVM, and LR, the propose factor graph algorithm gain +8.5%, +7.0% 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:22 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

68 

70 

72 

74 

76 

78 

80 

82 

40 45 50 55 60 65 70 75 80 

M 
ea 

n 
E 

xa 
ct 

m 
at 

ch 
(% 

) 

Training dataset size (%) 

FG 
DT 

SVM 
LR 

(a) Mean exact match under different training 

dataset size. 

0.3 

0.32 

0.34 

0.36 

0.38 

0.4 

0.42 

0.44 

40 45 50 55 60 65 70 75 80 

M 
ea 

n 
M 

A 
E 

Training dataset size (%) 

FG 
DT 

SVM 
LR 

(b) Mean MAE under different training dataset size. 

65 

70 

75 

80 

4 8 12 16 20 

M 
ea 

n 
E 

xa 
ct 

m 
at 

ch 
(% 

) 

Number of select feature 

FG 
DT 

SVM 
LR 

(c) Mean exact match under different number of 

select features. 

0.3 

0.35 

0.4 

0.45 

0.5 

0.55 

0.6 

4 8 12 16 20 

M 
ea 

n 
M 

A 
E 

Number of select feature 

FG 
DT 

SVM 
LR 

(d) Mean MAE under different number of select 

features. 

Fig. 16. Mean exact match and MAE under different training dataset size and number of select features. 

and +8.2% in average accuracy; gain +12.2%, +14.7% and +12.3% in average precision; gain +6.2%, 
+2.0% and +6.0% in recall; and gain +8.4%, +6.7% and +8.4% in F1-score respectively. 

7.4 Parameter analysis 

It should be aware that the performance of the propose training algorithm should be influence by 
several system parameter such a the chosen of training set and features. In this section, we analyze the 
influence of training dataset size and the number of select feature on compound emotion detection 
performance. 
The average exact match and MAE under different training dataset size be illustrate in Fig. 16(a) 

and Fig. 16(b). As show in the result, when the size of the training dataset size increase from 40% to 
80%, the exact match of propose factor graph model have +2.3% improvement and the MAE have 7% 
improvement. This be due to the fact that when the size of training dataset size be too small, there be not 
enough instance for training, which lead to underfitting. When the training dataset size be large enough, 
the exact match tends to be stable and could not be improve further by increase the size of training 
set. 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:23 

To explore the influence of the select features, we fix the training set size to be 70% (about 40+ 
instance over the all collect instances), and run the algorithm by vary the number of select 
feature from top-4 to top-22. The average exact match and MAE under different number of select 
feature be illustrate in Fig. 16(c) and Fig. 16(d). As show in the figure, when the number of feature 
increases, the performance decreases. When the number of select feature continue to becomes large 
(¿12), there be a significant downward trend about the exact match and upward trend about the MAE. 
The reason be explain a follows. When the feature space becomes larger, it normally require more 
training data to “feed” the model. In our experiments, the number of instance be about 40+, which be 
relative small. Increasing the number of feature will easily cause over-fitting of the model, which will 
decrease the performance. According to the figure, the performance be almost the same when the number 
of feature from 4 to 6, which implies that a small number of significant feature be enough for the 
detection task. Note that too few feature will also cause under-fitting or lack of generalization in machine 
learn theories. We choose 6 a the default number of feature in our experiment since it performs well 
under different condition and can avoid over-fitting and under-fitting issues. 
In summary, the propose compound emotion detection method yield the high exact match and 

the low MAE compare with the baseline approaches. 

8 CONCLUSION 

Automatic emotion detection be important to build intelligent system and to evaluate individual’s mental 
well-being. In this paper, we propose MoodExplorer, a system to detect people’s compound emotion 
base on smartphone sense data. We first present the compound emotion model that model people’s 
compound emotion by the combination of basic emotion categories, and formulate the compound emotion 
detection problem a a multi-label classification problem. Then we conduct feature extraction, feature 
selection, and correlation analysis on the user report dataset, which show that compound emotion 
frequently appear and have high correlation with some smartphone usage feature such a the sensor 
data and APP usage patterns. We further introduce a factor graph model to describe the correlation 
among emotion label and features, and propose a machine learn algorithm for compound emotion 
detection. The propose compound emotion detection mechanism be implement in an Android APP 
call MoodExplorer. The APP ran on 30 university student for one month, and their smartphone 
sense data be collect for analysis. We conduct extensive experiment on the collect dataset, which 
show that MoodExplorer can infer user’s compound emotion with exact match of 76.0% on average. 
The study of smartphone-based compound emotion detection in our paper achieves several cheerful 

results, which include an interest research problem of compound emotion detection, a well-developed 
machine learn model, and a decent accuracy for compound emotion detection. However, we shall also 
mention some limit of the work and the possible future directions. First, smartphone-based sense be 
unobtrusive but intermittent and coarse-grained. Since the user may not carry the smartphones all day 
long, the sense data only reflect part of the human activities. Therefore it can roughly estimate user’s 
compound emotion in a coarse time interval. Towards fine-grained short-term emotion detection be still 
a challenge task. Second, the propose model be only test on a small scale dataset. The result be 
verify by 30 university students, and it be unclear whether it can be generalize to people under different 
occupancy, different age, different culture, and different races. The experiment could be biased, and the 
collect data and emotion report could be distortion since we could not tell whether a subject report 
his/her emotion truthfully and thoroughly. Third, the propose detection model be person-specific and 
need to be train individually. It require a relative long period to collect data from the user to build 
the model, which prevents it from be widely deployed in practice. In the next step, we may seek a 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:24 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

general model for user group by cluster the feature and users, and we may apply the semi-supervised 
learn and transfer learn technique to reduce the startup time. 

The research on emotion detection base on smartphone sense be a very young area, and it enables a 
new way to sense, understand, interact, and intervene on human emotional feel and mental wellbeing. 
It need the interdisciplinary effort from sociology, psychology, neuroscience, and computer science. 
Together they will make more progress towards automatic emotion recognition and prediction. 

ACKNOWLEDGEMENT 

The author sincerely acknowledge the reviewer and editor who provide valuable comment and 
feedback to help to improve the quality of the paper. This work be partially support by the National 
Key R&D Program of China (Grant No. 2017YFB1001800), the National Natural Science Foundation of 
China (Grant Nos. 61672278, 61373128, 61321491), the Collaborative Innovation Center of Novel Software 
Technology and Industrialization, and the Sino-German Institutes of Social Computing. 

REFERENCES 
[1] J. Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stolcke. Prosody-based automatic detection of annoyance and 

frustration in human-computer dialog. In INTERSPEECH. Citeseer, 2002. 
[2] S. Aral and D. Walker. Identifying influential and susceptible member of social networks. Science, 337(6092):337–341, 

2012. 
[3] A. B. Ashraf, S. Lucey, J. F. Cohn, T. Chen, Z. Ambadar, K. M. Prkachin, and P. E. Solomon. The painful face–pain 

expression recognition use active appearance models. Image and vision computing, 27(12):1788–1796, 2009. 
[4] X. Bao, S. Fan, A. Varshavsky, K. Li, and R. Roy Choudhury. Your reaction suggest you like the movie: Automatic 

content rating via reaction sensing. In Proceedings of the 2013 ACM international joint conference on Pervasive and 

ubiquitous computing, page 197–206. ACM, 2013. 
[5] C. Beedie, P. Terry, and A. Lane. Distinctions between emotion and mood. Cognition & Emotion, 19(6):847–878, 2005. 
[6] A. Bogomolov, B. Lepri, M. Ferron, F. Pianesi, and A. S. Pentland. Daily stress recognition from mobile phone data, 

weather condition and individual traits. In Proceedings of the 22nd ACM international conference on Multimedia, 

page 477–486. ACM, 2014. 
[7] L. Canzian and M. Musolesi. Trajectories of depression: unobtrusive monitoring of depressive state by mean of 

smartphone mobility trace analysis. In Proceedings of the 2015 ACM international joint conference on pervasive and 
ubiquitous computing, page 1293–1304. ACM, 2015. 

[8] B. Cao, L. Zheng, C. Zhang, P. S. Yu, A. Piscitello, J. Zulueta, O. Ajilore, K. Ryan, and A. D. Leow. Deepmood: 

Modeling mobile phone type dynamic for mood detection. 2017. 
[9] J. R. Crawford and J. D. Henry. The positive and negative affect schedule (panas): Construct validity, measurement 

property and normative data in a large non-clinical sample. British Journal of Clinical Psychology, 43(3):245–265, 

2004. 
[10] E. Diener, S. Oishi, and R. E. Lucas. Personality, culture, and subjective well-being: Emotional and cognitive evaluation 

of life. Annual review of psychology, 54(1):403–425, 2003. 
[11] S. Du, Y. Tao, and A. M. Martinez. Compound facial expression of emotion. Proceedings of the National Academy of 

Sciences, 111(15):E1454–E1462, 2014. 
[12] P. Ekman, W. V. Friesen, M. O’Sullivan, A. Chan, I. Diacoyanni-Tarlatzis, K. Heider, R. Krause, W. A. LeCompte, 

T. Pitcairn, P. E. Ricci-Bitti, et al. Universals and cultural difference in the judgment of facial expression of emotion. 
Journal of personality and social psychology, 53(4):712, 1987. 

[13] A. Exler, M. Urschel, A. Schankin, and M. Beigl. Smartphone-based detection of location change use wifi data. In 
International Conference on Wireless Mobile Communication and Healthcare, page 164–167. Springer, 2016. 

[14] J. H. Fowler, N. A. Christakis, et al. Dynamic spread of happiness in a large social network: longitudinal analysis over 

20 year in the framingham heart study. British Medical Journal, 337:a2338, 2008. 
[15] B. J. Frey and D. J. MacKay. A revolution: Belief propagation in graph with cycles. Advances in neural information 

processing systems, page 479–485, 1998. 

[16] T. Geller. How do you feel?: Your computer knows. Communications of the ACM, 57(1):24–26, 2014. 
[17] A. Gluhak, M. Presser, L. Zhu, S. Esfandiyari, and S. Kupschick. Towards mood base mobile service and applications. 

In European Conference on Smart Sensing and Context, page 159–174. Springer, 2007. 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:25 

[18] J. Golbeck, C. Robles, and K. Turner. Predicting personality with social media. In CHI’11 extend abstract on 

human factor in compute systems, page 253–262. ACM, 2011. 
[19] J. J. Gross and O. P. John. Individual difference in two emotion regulation processes: implication for affect, 

relationships, and well-being. Journal of personality and social psychology, 85(2):348, 2003. 
[20] J. M. Hammersley and P. Clifford. Markov field on finite graph and lattices. 1971. 
[21] K. C. Herdem. Reactions: Twitter base mobile application for awareness of friends’ emotions. In Proceedings of the 

2012 ACM Conference on Ubiquitous Computing, page 796–797. ACM, 2012. 
[22] J. Hernandez, M. E. Hoque, W. Drevo, and R. W. Picard. Mood meter: counting smile in the wild. In Proceedings of 

the 2012 ACM Conference on Ubiquitous Computing, page 301–310. ACM, 2012. 
[23] D. Koller and N. Friedman. Probabilistic graphical models: principle and techniques. MIT press, 2009. 
[24] F. R. Kschischang, B. J. Frey, and H.-A. Loeliger. Factor graph and the sum-product algorithm. IEEE Transactions 

on information theory, 47(2):498–519, 2001. 
[25] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic model for segment and label 

sequence data. In Proceedings of the eighteenth international conference on machine learn (ICML ’01), volume 1, 
page 282–289, 2001. 

[26] H. Leng, Y. Lin, and L. Zanzi. An experimental study on physiological parameter toward driver emotion recognition. 

In International Conference on Ergonomics and Health Aspects of Work with Computers, page 237–246. Springer, 
2007. 

[27] M. Levandowsky and D. Winter. Distance between sets. Nature, 234(5323):34–35, 1971. 
[28] S. Li, L. Huang, R. Wang, and G. Zhou. Sentence-level emotion classification with label and context dependence. 

Proceedings of ACL-2015, page 1045–1053, 2013. 
[29] W. Li, Y. Hu, X. Fu, S. Lu, and D. Chen. Cooperative position and track in disruption tolerant networks. IEEE 

Transactions on Parallel and Distributed Systems, 26(2):382–391, 2015. 
[30] R. LiKamWa, Y. Liu, N. D. Lane, and L. Zhong. Moodscope: building a mood sensor from smartphone usage patterns. 

In Proceeding of the 11th annual international conference on Mobile systems, applications, and services, page 389–402. 
ACM, 2013. 

[31] G. MacKerron and S. Mourato. Happiness be great in natural environments. Global Environmental Change, 

23(5):992–1000, 2013. 
[32] A. Mottelson and K. Hornbæk. An affect detection technique use mobile commodity sensor in the wild. In Proceedings 

of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing, page 781–792. ACM, 2016. 

[33] K. P. Murphy. Machine learning: a probabilistic perspective. MIT press, 2012. 
[34] R. Plutchik. A general psychoevolutionary theory of emotion. Theories of emotion, 1(3-31):4, 1980. 
[35] J. Posner, J. A. Russell, and B. S. Peterson. The circumplex model of affect: An integrative approach to affective 

neuroscience, cognitive development, and psychopathology. Development and psychopathology, 17(03):715–734, 2005. 
[36] R. Reisenzein, M. Studtmann, and G. Horstmann. Coherence between emotion and facial expression: Evidence from 

laboratory experiments. Emotion Review, 5(1):16–23, 2013. 
[37] S. Servia-Rodŕıguez, K. K. Rachuri, C. Mascolo, P. J. Rentfrow, N. Lathia, and G. M. Sandstrom. Mobile sense at 

the service of mental well-being: a large-scale longitudinal study. In Proceedings of the 26th International Conference 

on World Wide Web, page 103–112. International World Wide Web Conferences Steering Committee, 2017. 
[38] N. SpolaôR, E. A. Cherman, M. C. Monard, and H. D. Lee. A comparison of multi-label feature selection method 

use the problem transformation approach. Electronic Notes in Theoretical Computer Science, 292:135–151, 2013. 
[39] N. Spolaôr, E. A. Cherman, M. C. Monard, and H. D. Lee. Relieff for multi-label feature selection. In Intelligent 

Systems (BRACIS), 2013 Brazilian Conference on, page 6–11. IEEE, 2013. 
[40] S. A. Stansfeld and M. P. Matheson. Noise pollution: non-auditory effect on health. British medical bulletin, 

68(1):243–257, 2003. 
[41] T. Stütz, T. Kowar, M. Kager, M. Tiefengrabner, M. Stuppner, J. Blechert, F. H. Wilhelm, and S. Ginzinger. Smartphone 

base stress prediction. In International Conference on User Modeling, Adaptation, and Personalization, page 240–251. 
Springer, 2015. 

[42] Y. Suhara, Y. Xu, and A. Pentland. Deepmood: Forecasting depressed mood base on self-reported history via 

recurrent neural networks. In Proceedings of the 26th International Conference on World Wide Web, page 715–724. 
International World Wide Web Conferences Steering Committee, 2017. 

[43] B. Sun, Q. Ma, S. Zhang, K. Liu, and Y. Liu. iself: Towards cold-start emotion label use transfer learn with 

smartphones. In 2015 IEEE Conference on Computer Communications (INFOCOM), page 1203–1211. IEEE, 2015. 
[44] C. Sutton and A. McCallum. An introduction to conditional random fields. arXiv preprint arXiv:1011.4088, 2010. 
[45] S. S. Tomkins. Affect, imagery, consciousness: Vol. i. the positive affects. 1962. 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:26 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

[46] G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining multi-label data. In Data mining and knowledge discovery handbook, 

page 667–685. Springer, 2009. 
[47] L. Wasserman. All of statistics: a concise course in statistical inference. Springer Science & Business Media, 2013. 
[48] Y. Yang, J. Jia, B. Wu, and J. Tang. Social role-aware emotion contagion in image social networks. In Thirtieth AAAI 

Conference on Artificial Intelligence, 2016. 

[49] Z. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang. A survey of affect recognition methods: Audio, visual, and 
spontaneous expressions. IEEE transaction on pattern analysis and machine intelligence, 31(1):39–58, 2009. 

[50] M.-L. Zhang and Z.-H. Zhou. A review on multi-label learn algorithms. IEEE transaction on knowledge and data 
engineering, 26(8):1819–1837, 2014. 

[51] S. Zhang and P. Hui. A survey on mobile affective computing. ArXiv Prepr. ArXiv14101648, 2014. 
[52] Y. Zhang, J. Tang, J. Sun, Y. Chen, and J. Rao. Moodcast: Emotion prediction via dynamic continuous factor graph 

model. In Proceedings of the 10th International Conference on Data Mining, page 1193–1198. IEEE, 2010. 
[53] M. Zhao, F. Adib, and D. Katabi. Emotion recognition use wireless signals. In Proceedings of the 22Nd Annual 

International Conference on Mobile Computing and Networking (MobiCom ’16), page 95–108, 2016. 
[54] S. Zhao, H. Yao, and X. Jiang. Predicting continuous probability distribution of image emotion in valence-arousal 

space. In Proceedings of the 23rd ACM international conference on Multimedia, page 879–882. ACM, 2015. 

[55] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proceedings of the 23rd 
ACM international conference on Multimedia, page 1247–1250. ACM, 2015. 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:27 

Appendix 

A THE RF WEIGHTS OF ALL FEATURES OF A USER 

Fea. ID Feature Weight Fea. ID Feature Weight 
1 Dark Ratio (DR) 1.35 2 Var. of Acc. X axis 1.23 
3 Var. of Acc. Y axis 1.15 4 Dur. of Travel APP 1.13 
5 Dur. of Life Services APP 1.13 6 Mean of Acc. Z axis 1.10 
7 Bright Ratio (BR) 1.01 8 Dark Bright Ratio (DBR) 0.99 
9 Freq. of SSID #2009 0.93 10 Freq. of SSID #5455 0.93 
11 Freq. of SSID #2060 0.92 12 Call Freq. of Contact #1870 0.83 
13 Silence Ratio (SR) 0.75 14 Mean of Gyro. X axis 0.73 
15 Var. of Audio Volume 0.64 16 Noise Ratio (NR) 0.62 
17 NSR 0.58 18 Call Duration of Contact #1870 0.58 
19 Call Freq. of Contact #2021 0.56 20 Var. of Acc. Z axis 0.48 
21 Var. of Gyro. Z axis 0.45 22 Mean of Gyro. Y axis 0.45 
23 Var. of Gyro. X axis 0.44 24 Mean of Audio Volume 0.40 
25 Mean of Gyro. Z axis 0.32 26 Mean of Acc. X axis 0.30 
27 Call Duration of Contact #6387 0.29 28 Call Duration of Contact #1577 0.29 
29 Call Duration of Contact #2243 0.29 30 Call Freq. of Contact #1577 0.29 
31 Call Duration of Contact #3367 0.28 32 Call Freq. of Contact #3367 0.28 
33 Call Freq. of Contact #1713 0.28 34 Call Duration of Contact #1713 0.28 
35 Call Freq. of Contact #2243 0.28 36 Call Freq. of Contact #6387 0.28 
37 Call Freq. of Contact #2241 0.28 38 Call Duration of Contact #5426 0.28 
39 Call Freq. of Contact #5426 0.28 40 Call Duration of Contact #2021 0.27 
41 Mean of Acc. Y axis 0.27 42 Var. of Gyro. Y axis 0.18 
43 Call Freq. of Contact #1905 0.17 44 Call Duration of Contact #4127 0.14 
45 Call Freq. of Contact #1683 0.13 46 Var. of Light Sensor Value 0.13 
47 Dur. of Mother&Child APP 0.13 48 Dur. of Smartphone Beautification APP 0.13 
49 Freq. of SSID #1712 0.13 50 Freq. of SSID #2244 0.13 
51 Freq. of SSID #5135 0.13 52 Freq. of SSID #1131 0.13 
53 Dur. of Image APP 0.13 54 Dur. of Vedio APP 0.13 
55 Dur. of Office Efficiency APP 0.11 56 Dur. of Shopping APP 0.11 
57 Step Count 0.11 58 GPS Altitude 0.09 
59 GPS Latitude 0.08 60 GPS Longitude 0.08 
61 Dur. of System Tools APP 0.08 62 Call Duration of Contact #1683 0.08 
63 Call Duration of Contact #2241 0.03 64 Dur. of Life Tools APP 0.02 
65 Dur. of Financial Management APP 0.02 66 Dur. of Social Chat APP 0.00 
67 Dur. of Health APP 0.00 68 SMS Freq. of Contact #6387 0.00 
69 SMS Freq. of Contact #1577 0.00 70 SMS Freq. of Contact #3367 0.00 
71 Freq. of SSID #2116 0.00 72 Freq. of SSID #1478 0.00 
73 Freq. of SSID #9034 0.00 74 Freq. of SSID #5458 0.00 
75 Freq. of SSID #3546 0.00 76 Dur. of Telephone Communication APP 0.00 
77 Call Duration of Contact #1905 0.00 78 SMS Freq. of Contact #1683 0.00 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:28 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

Fea. ID Feature Weight Fea. ID Feature Weight 
79 SMS Freq. of Contact #1870 0.00 80 SMS Freq. of Contact #2021 0.00 
81 Dur. of News APP 0.00 82 Dur. of Traffic Navigation APP 0.00 
83 Dur. of Education APP 0.00 84 Dur. of Music APP 0.00 
85 SMS Freq. of Contact #2243 -0.06 86 SMS Freq. of Contact #2241 -0.06 
87 SMS Freq. of Contact #5426 -0.06 88 SMS Freq. of Contact #1905 -0.06 
89 Mean of Magnetic Z axis -0.09 90 Mean of Magnetic Y axis -0.09 
91 Var. of Magnetic Y axis -0.09 92 Call Freq. of Contact #4127 -0.11 
93 Mean of Light Sensor Value -0.11 94 Freq. of SSID #1618 -0.11 
95 Freq. of SSID #1627 -0.11 96 Freq. of SSID #1875 -0.11 
97 Freq. of SSID #1751 -0.12 98 Var. of Magnetic X axis -0.12 
99 Mean of Magnetic X axis -0.12 100 Var. of Magnetic Z axis -0.12 
101 SMS Freq. of Contact #1713 -0.12 102 SMS Freq. of Contact #4127 -0.13 
103 Freq. of SSID #9673 -0.14 104 Freq. of SSID #6470 -0.14 
105 Freq. of SSID #4707 -0.14 106 Freq. of SSID #3907 -0.16 
107 Sleeping Duration -0.16 108 Screen on Ratio -0.16 
109 Usage Amount -0.17 110 Screen off Ratio -0.19 

Table 4. Example: the weight of an individual’s all features. 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



MoodExplorer: Towards Compound Emotion Detection via Smartphone Sensing • 176:29 

B THE SELECTED FEATURES FOR ALL 30 USERS 

User First Sel. Fea. Sec. Sel. Fea. Third Sel. Fea. Fourth Sel. Fea. Fifth Sel. Fea. Sixth Sel. Fea. 

1 
Bright Dark Mean of Light Var. of Freq. of Call Dur. of 

Ratio (BR) Ratio (DR) Sensor Value Audio Volume SSID #1873 Contact #1436 

2 
Step Count Mean of Mean of Variance of Duration of Call Freq. of 

Gyro. Z axis Gyro. X axis Gyro. Y axis Shopping APP Contact #1037 

3 
Screen On Screen Off Call Freq. of Call Dur. of Freq. of Freq. of one 

Ratio Ratio Contact #1686 Contact #1686 SSID #1074 SSID #2244 

4 
Freq. of Freq. of Freq. of Dur. of Social Duration of Duration of 

SSID #3541 SSID #5541 SSID #1712 Chat APP Video APP Music APP 

5 
Call Dur. of Call Freq. of Call Dur. of Call Freq. of Call Dur. of SMS Freq. of 

Contact #4890 Contact #1682 Contact #1133 Contact #1803 Contact #3769 Contact #2110 

6 
Bright Freq. of Freq. of Freq. of Noise Dark 

Ratio (BR) SSID #1074 SSID #4392 SSID #2244 Ratio (NR) Ratio (DR) 

7 
Dark Bright Duration of Mean of Gyro. Mean of Acc. Dark Screen On 
Ratio (DBR) Education App Y axis Y axis Ratio (DR) Ratio 

8 
Silence Call Freq. of Dark SMS Freq. of Mean of Variance of 

Ratio (SR) Contact #1199 Ratio (DR) Contact #1086 Audio Volume Acc. X axis 

9 
Step Count Mean of Mean of Variance of SMS Freq. of Dark 

Gyro. Y axis Acc. X axis Audio Volume Contact #1829 Ratio (DR) 

10 
Freq. of Freq. of Freq. of Dur. of System Screen On Sleeping 

SSID #2242 SSID #1904 SSID #7676 Tools APP Ratio Duration 

11 
Silence Mean of Bright Mean of SMS Freq. of Freq. of 

Ratio (SR) Gyro. Z axis Ratio (BR) Audio Volume Contact #1776 SSID #1576 

12 
Dark Variance of Variance of Duration of Dur. of Life Mean of 

Ratio (DR) Acc. X axis Acc. Y axis Travel APP Services APP Acc. Z axis 

13 
Freq. of Mean of Light Call Dur. of Bright Freq. of one Variance of 

SSID #6388 Sensor Value Contact #2112 Ratio (BR) SSID #2244 Audio Volume 

14 
Mean of Mean of Dark Mean of Mean of Freq. of 

Acc. Z axis Acc. Y axis Ratio (DR) Acc. X axis Gyro. X axis SSID #1869 

15 
Freq. of Freq. of Variance of Step Count Usage Amount Duration of 

SSID #5427 SSID #8244 Acc. Y axis Education APP 

16 
Duration of Duration of Duration of Variance of Duration of Variance of 
Vedio APP Music APP Social Chat APP Acc. X axis News APP Acc. Z axis 

17 
Freq. of Screen On Sleeping Call Freq. of SMS Freq. of Call Durations 

SSID #8224 Ratio Duration Contact #2076 Contact #2076 of Contact #1598 

18 
Mean of Freq. of one SMS Freq. of Variance of Screen On Mean of 

Acc. Z axis SSID #1712 Contact #3889 Acc. Z axis Ratio Acc. Y axis 

19 
Silence Bright Duration of Dur. of Traffic Duration of Noise 

Ratio (SR) Ratio (BR) Shopping APP Navigation APP News APP Ratio (NR) 

20 
Noise Mean of Variance of Freq. of Screen Off Screen On 

Ratio (NR) Audio Volume Audio Volume SSID #2022 Ratio Ratio 

21 
Audio Freq. of Noise Duration of Screen On Silence 
Mean SSID #1025 Ratio (NR) Education APP Ratio Ratio (SR) 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 



176:30 • Xiao Zhang, Wenzhong Li, Xu Chen, and Sanglu Lu 

22 
Freq. of Mean of Dark Mean of Variance of Dur. of Office 

SSID #3368 Gyro. X axis Ratio (DR) Gyro. Y axis Gyro. X axis Efficiency APP 

23 
Duration of Duration of Duration of Screen Off Screen On Mean of 
M&C APP Shopping APP Vedio APP Ratio Ratio Gyro. Y axis 

24 
Variance of Duration of Duration Dur. of System Freq. of Duration of 
Acc. Y axis Social Chat APP Financial APP Tools APP SSID #1684 Education APP 

25 
Screen Off Screen On Freq. of Mean of Mean of Call Freq. of 

Ratio Ratio SSID #5427 Gyro. Z axis Acc. Z axis Contact #8274 

26 
Freq. of Call Freq. of Call Dur. of Duration of Bright Var. of Light 

SSID #4126 Contact #4348 Contact #1118 Social Chat APP Ratio (BR) Sensor Value 

27 
Dark Call Freq. of SMS Freq. of SMS Freq. of Mean of Light Variance of 

Ratio (DR) Contact #1367 Contact #1367 Contact #1595 Sensor Value Audio Volume 

28 
Noise Silence Bright Mean of Call Freq. of Variance of Dark 
Ratio (NSR) Ratio (BR) Acc. Z axis Contact #1897 Audio Volume Ratio (DR) 

29 
Freq. of Freq. of Mean of Bright Dark Mean of 

SSID #2242 SSID #6388 Gyro. X axis Ratio (BR) Ratio (DR) Gyro. Y axis 

30 
Noise Dark Duration of SMS Freq. of Call Dur. of Call Dur. of 

Ratio (NR) Ratio (DR) System Tools APP Contact #4122 Contact #4122 Contact #6463 
Table 5. Selected feature of another 20 users. 

Received May 2017; revise August 2017; accepted October 2017 

Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, Vol. 1, No. 4, Article 176. 

Publication date: December 2017. 
















<< 
/ASCII85EncodePages false 
/AllowTransparency false 
/AutoPositionEPSFiles false 
/AutoRotatePages /None 
/Binding /Left 
/CalGrayProfile (Gray Gamma 2.2) 
/CalRGBProfile (sRGB IEC61966-2.1) 
/CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2) 
/sRGBProfile (sRGB IEC61966-2.1) 
/CannotEmbedFontPolicy /Error 
/CompatibilityLevel 1.6 
/CompressObjects /Off 
/CompressPages true 
/ConvertImagesToIndexed true 
/PassThroughJPEGImages true 
/CreateJobTicket false 
/DefaultRenderingIntent /Default 
/DetectBlends true 
/DetectCurves 0.0000 
/ColorConversionStrategy /LeaveColorUnchanged 
/DoThumbnails true 
/EmbedAllFonts true 
/EmbedOpenType true 
/ParseICCProfilesInComments true 
/EmbedJobOptions true 
/DSCReportingLevel 0 
/EmitDSCWarnings false 
/EndPage -1 
/ImageMemory 524288 
/LockDistillerParams true 
/MaxSubsetPct 100 
/Optimize true 
/OPM 0 
/ParseDSCComments false 
/ParseDSCCommentsForDocInfo false 
/PreserveCopyPage true 
/PreserveDICMYKValues true 
/PreserveEPSInfo false 
/PreserveFlatness true 
/PreserveHalftoneInfo true 
/PreserveOPIComments false 
/PreserveOverprintSettings true 
/StartPage 1 
/SubsetFonts true 
/TransferFunctionInfo /Remove 
/UCRandBGInfo /Preserve 
/UsePrologue false 
/ColorSettingsFile (None) 
/AlwaysEmbed [ true 
] 
/NeverEmbed [ true 
] 
/AntiAliasColorImages false 
/CropColorImages true 
/ColorImageMinResolution 150 
/ColorImageMinResolutionPolicy /OK 
/DownsampleColorImages true 
/ColorImageDownsampleType /Bicubic 
/ColorImageResolution 300 
/ColorImageDepth -1 
/ColorImageMinDownsampleDepth 1 
/ColorImageDownsampleThreshold 1.50000 
/EncodeColorImages true 
/ColorImageFilter /DCTEncode 
/AutoFilterColorImages true 
/ColorImageAutoFilterStrategy /JPEG 
/ColorACSImageDict << 
/QFactor 0.15 
/HSamples [1 1 1 1] /VSamples [1 1 1 1] 
>> 
/ColorImageDict << 
/QFactor 0.15 
/HSamples [1 1 1 1] /VSamples [1 1 1 1] 
>> 
/JPEG2000ColorACSImageDict << 
/TileWidth 256 
/TileHeight 256 
/Quality 30 
>> 
/JPEG2000ColorImageDict << 
/TileWidth 256 
/TileHeight 256 
/Quality 30 
>> 
/AntiAliasGrayImages false 
/CropGrayImages true 
/GrayImageMinResolution 150 
/GrayImageMinResolutionPolicy /OK 
/DownsampleGrayImages true 
/GrayImageDownsampleType /Bicubic 
/GrayImageResolution 300 
/GrayImageDepth -1 
/GrayImageMinDownsampleDepth 2 
/GrayImageDownsampleThreshold 1.50000 
/EncodeGrayImages true 
/GrayImageFilter /DCTEncode 
/AutoFilterGrayImages true 
/GrayImageAutoFilterStrategy /JPEG 
/GrayACSImageDict << 
/QFactor 0.15 
/HSamples [1 1 1 1] /VSamples [1 1 1 1] 
>> 
/GrayImageDict << 
/QFactor 0.15 
/HSamples [1 1 1 1] /VSamples [1 1 1 1] 
>> 
/JPEG2000GrayACSImageDict << 
/TileWidth 256 
/TileHeight 256 
/Quality 30 
>> 
/JPEG2000GrayImageDict << 
/TileWidth 256 
/TileHeight 256 
/Quality 30 
>> 
/AntiAliasMonoImages false 
/CropMonoImages true 
/MonoImageMinResolution 1200 
/MonoImageMinResolutionPolicy /OK 
/DownsampleMonoImages true 
/MonoImageDownsampleType /Bicubic 
/MonoImageResolution 600 
/MonoImageDepth -1 
/MonoImageDownsampleThreshold 1.00167 
/EncodeMonoImages true 
/MonoImageFilter /CCITTFaxEncode 
/MonoImageDict << 
/K -1 
>> 
/AllowPSXObjects false 
/CheckCompliance [ 
/None 
] 
/PDFX1aCheck false 
/PDFX3Check false 
/PDFXCompliantPDFOnly false 
/PDFXNoTrimBoxError true 
/PDFXTrimBoxToMediaBoxOffset [ 
0.00000 
0.00000 
0.00000 
0.00000 
] 
/PDFXSetBleedBoxToMediaBox true 
/PDFXBleedBoxToTrimBoxOffset [ 
0.00000 
0.00000 
0.00000 
0.00000 
] 
/PDFXOutputIntentProfile (None) 
/PDFXOutputConditionIdentifier () 
/PDFXOutputCondition () 
/PDFXRegistryName () 
/PDFXTrapped /False 

/CreateJDFFile false 
/Description << 
/ENU () 
>> 
>> setdistillerparams 
<< 
/HWResolution [600 600] 
/PageSize [612.000 792.000] 
>> setpagedevice 

