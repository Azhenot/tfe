


















































CAN: Creative Adversarial Networks 
Generating “Art” by Learning About Styles and 

Deviating from Style Norms∗ 

Ahmed Elgammal1† Bingchen Liu1 Mohamed Elhoseiny2 Marian Mazzone3 

The Art & AI Laboratory - Rutgers University 
1 Department of Computer Science, Rutgers University, NJ, USA 

2 Facebook AI Research, CA, USA 
3 Department of Art History, College of Charleston, SC, USA 

June 23, 2017 

Abstract 

We propose a new system for generate art. The system generates art by look- 
ing at art and learn about style; and becomes creative by increase the arousal 
potential of the generate art by deviate from the learn styles. We build over 
Generative Adversarial Networks (GAN), which have show the ability to learn 
to generate novel image simulate a give distribution. We argue that such net- 
work be limited in their ability to generate creative product in their original 
design. We propose modification to it objective to make it capable of generate 
creative art by maximize deviation from establish style and minimize de- 
viation from art distribution. We conduct experiment to compare the response 
of human subject to the generate art with their response to art create by artists. 
The result show that human subject could not distinguish art generate by the 
propose system from art generate by contemporary artist and show in top art 
fairs. 

1 Introduction 
Since the dawn of Artificial Intelligence, scientist have be explore the machine’s ability to 
generate human-level creative product such a poetry, stories, jokes, music, paintings, etc., a 
well a creative problem solving. This ability be fundamental to show that Artificial Intelligence 
algorithm be in fact intelligent. In term of visual art, several system have be propose to 

∗This paper be an extend version of a paper publish on the eighth International Conference on Computational 
Creativity (ICCC), held in Atlanta, GA, June 20th-June 22nd, 2017. 
†Corresponding author: Ahmed Elgammal elgammal@cs.rutgers.edu 

1 

ar 
X 

iv 
:1 

70 
6. 

07 
06 

8v 
1 

[ 
c 

.A 
I] 

2 
1 

Ju 
n 

20 
17 

mailto:elgammal@cs.rutgers.edu 
http://digihumanlab.rutgers.edu 
mailto:elgammal@cs.rutgers.edu 


automatically create art, not only in the domain of AI and computational creativity (e.g. [1, 6, 5, 10] 
), but also in computer graphic (e.g. [22]), and machine learning, (e.g. [16, 12]). 

Within the computational creativity literature, different algorithm have be propose focus 
on investigate various and effective way of explore the creative space. Several approach 
have use an evolutionary process wherein the algorithm iterates by generate candidates, evalu- 
ating them use a fitness function, and then modify them to improve the fitness score for the 
next iteration (e.g. [14, 6]). Typically, this process be do within a genetic algorithm framework. 
As point out by DiPaola and Gabora 2009, the challenge for any algorithm center on “how to 
write a logical fitness function that have an aesthetic sense”. Some early system utilized a hu- 
man in the loop with the role of guide the process (e.g. [1, 9]). In these interactive systems, the 
computer explores the creative space and the human play the role of the observer whose feedback 
be essential in drive the process. Recent system have emphasize the role of perception and 
cognition in the creative process [4, 5, 10]. 

The goal of this paper be to investigate a computational creative system for art generation with- 
out involve a human artist in the creative process, but nevertheless involve human creative 
product in the learn process. An essential component in art-generating algorithm be relate 
their creative process to art that have be produce by human artist throughout time. We believe 
this be important because a human’s creative process utilizes prior experience of and exposure to 
art. A human artist be continuously expose to other artists’ work, and have be expose to a wide 
variety of art for all of his/her life. What remains largely unknown be how human artist integrate 
their knowledge of past art with their ability to generate new forms. A theory be need to model 
how to integrate exposure to art with the creation of art. 

Colin Martindale (1943-2008) propose a psychology-based theory that explains new art creation[15]. 
He hypothesize that at any point in time, creative artist try to increase the arousal potential of 
their art to push against habituation. However, this increase have to be minimal to avoid negative 
reaction by the observer (principle of least effort). Martindale also hypothesize that style break 
happen a a way of increase the arousal potential of art when artist exert other mean within the 
role of style. The approach propose in this paper be inspire by Martindale’s principle of least 
effort and his explanation of style breaks. Among theory that try to explain progress in art, we 
find Martindale’s theory to be computationally feasible. 

Deep neural network have recently played a transformative role in advance artificial intel- 
ligence across various application domains. In particular, several generative deep network have 
be propose that have the ability to generate novel image to emulate a give training distribution 
Generative Adversarial Networks (GAN) have be quite successful in achieve this goal [8]. We 
argue that such network be limited in their ability to generate creative product in their original 
design. Inspired by Martindale’s theory, in this paper we propose modification to GAN’s objec- 
tive to make it able to generate creative art by maximize deviation from establish style while 
minimize deviation from art distribution. Figure 1 show sample of the generate images. 

2 



Figure 1: Example of image generate by CAN. The generate image vary from simple abstract 
one to complex texture and compositions. More example be show in Figures 4.2 and 7 

. 

2 Methodology 

2.1 Background 
The propose approach be motivate from the theory suggest by D. E. Berlyne (1924-1976). 
Berlyne argue that the psychophysical concept of “arousal” have a great relevance for study 
aesthetic phenomenon [3]. “Level of arousal” measure how alert or excite a human be is. The 

3 



level of arousal varies from the low level, when a person be asleep or relaxed, to the high 
level when s/he be violent, in a fury, or in a passionate situation [2]. Among different mecha- 
nisms of arousal, of particular importance and relevance to art be property of external stimulus 
pattern [3]. 

The term “arousal potential” refers to the property of stimulus pattern that lead to raise 
arousal. Besides other psychophysical and ecological property of stimulus patterns, Berlyne 
emphasize that the most significant arousal-raising property for aesthetic be novelty, surpris- 
ingness, complexity, ambiguity, and puzzlingness. He coin the term collative variable to refer 
to these property collectively. 

Novelty refers to the degree a stimulus differs from what an observer have seen/experienced be- 
fore. Surprisingness refers to the degree a stimulus disagrees with expectation. Surprisingness 
be not necessarily correlate with novelty, for example it can stem from lack of novelty. Unlike 
novelty and surprisingness which rely on inter-stimulus comparison of similarity and differences, 
complexity be an intra-stimulus property that increase a the number of independent element in 
a stimulus grows. Ambiguity refers to the conflict between the semantic and syntactic informa- 
tion in a stimulus. Puzzlingness refers to the ambiguity due to multiple, potentially inconsistent, 
meanings. 

Several study have show that people prefer stimulus with a moderate arousal potential [2, 
20]. Too little arousal potential be consider boring, and too much activates the aversion system, 
which result in negative response. This behavior be explain by the Wundt curve that correlate 
the arousal potential with the hedonic response [3, 24]. Berlyne also study arousal moderate 
mechanisms. Of particular importance in art be habituation, which refers to decrease arousal in 
response to repetition of a stimulus [3]. 

Martindale emphasize the importance of habituation in derive the art-producing system [15]. 
If artist keep produce similar work of arts, this directly reduces the arousal potential and hence 
the desirability of that art. Therefore, at any point of time, the art-producing system will try to 
increase the arousal potential of produce art. In other words, habituation form a constant pres- 
sure to change art. However, this increase have to be within the minimum amount necessary to 
compensate for habituation without fall into the negative hedonic range, accord to Wundt 
curve finding (“stimuli that be slightly rather than vastly supernormal be preferred”). Martindale 
call this the principle of “least effort”. Therefore, there be an opposite pressure that lead to a 
graduate pace of change in art. 

2.2 Art Generating Agent 
We propose a model for an art-generating agent, and then propose a function model use a 
variant of GAN to make it creative. The agent’s goal be to generate art with increase level of 
arousal potential in a constrain way without activate the aversion system and fall into the 
negative hedonic range. In other words, the agent try to generate art that be novel, but not too 
novel. This criterion be common in many computationally creative systems, however it be not easy 
to find a way to achieve that goal give the infinite possibility in the creative space. 

In our model the art-generating agent have a memory that encodes the art it have be expose 
to, and can be continuously update with the addition of new art. The agent us this encode 
memory in an indirect way while generate new art with a restrain increase in arousal potential. 
While there be several way to increase the arousal potential, in this paper we focus on building 

4 



an agent that try to increase the stylistic ambiguity and deviation from style norms, while at the 
same time, avoid move too far away from what be accepted a art. The agent try to explore 
the creative space by deviate from the establish style norm and thereby generates new art. 

There be two type of ambiguity that be expect in the generate art by the propose net- 
work; one be by design and the other one be inherent. Almost all computer-generated art might 
be ambiguous because the art generate typically do not have clear figure or an interpretable 
subject matter. Because of this, Heath et al argue that the creative machine would need to have 
perceptual ability (be able to see) in order to be able to generate plausible creative art [10]. This 
limited perceptual ability be what cause the inherent ambiguity. Typically, this type of ambiguity 
result in user be able to tell right away that the work be generate by a machine rather than 
a human artist. Even though several style of art developed in the 20th century might lack rec- 
ognizable figure or lucid subject matter, human observer usually be not fool into confuse 
computer-generated art with human-generated art. 

Because of this inherent ambiguity people always think of computer-generated art a be 
hallucination-like. The Guardian comment on a the image generate by Google DeepDream [16] 
by “Most, however, look like dorm-room mandalas, or the kind of digital psychedelia you might 
expect to find on the cover of a Terrence McKenna book”1. Others comment on it a be “daz- 
zling, druggy, and creepy” 2. This negative reaction might be explain a a result of too much 
arousal, which result in negative hedonics accord to the Wundt curve. 

The other type of ambiguity in the art generate by the propose agent be stylistic ambiguity, 
which be intentional by design. The rational be that creative artist would eventually break from 
establish style and explore new way of expression to increase the arousal potential of their art, 
a Martindale suggested. As suggest by DiPaola and Gabora, “creators often work within a very 
structure domain, follow rule that they eventually break free of” [6]. 

The propose art-generating agent be realize by a model call Creative Adversarial Network 
(CAN), which we will describe next. The network be design to generate art that do not follow 
establish art movement or styles, but instead try to generate art that maximally confuses human 
viewer a to which style it belongs to. 

2.3 GAN: Emulative and not Creative 
Generative Adversarial Network (GAN) have two sub networks, a generator and a discriminator. 
The discriminator have access to a set of image (training images). The discriminator try to dis- 
criminate between “real” image (from the training set) and “fake” image generate by the gener- 
ator. The generator try to generate image similar to the training set without see these images. 
The generator start by generate random image and receives a signal from the discriminator 
whether the discriminator find them real or fake. At equilibrium the discriminator should not be 
able to tell the difference between the image generate by the generator and the actual image 
in the training set, hence the generator succeed in generate image that come from the same 
distribution a the training set. 

Let u now assume that we train a GAN model on image of paintings. Since the generator 
be train to generate image that fool the discriminator to believe it be come from the training 

1Alex Rayner, the Guardian, March 28, 2016 
2David Auerbach, Slate, July 23, 2015 

5 



distribution, ultimately the generator will just generate image that look like already exist art. 
There be no motivation to generate anything creative. There be no force that push the generator 
to explore the creative space. Let u think about a generator that can cheat and already have access 
to sample from the training data. In that case the discriminator will right away be fool into 
believe that the generator be generate art, while in fact it be already exist art, and hence not 
novel and not creative. 

There have be extension to GANs that facilitate generate image condition on cate- 
gories (e.g., [18]) or caption (e.g., [19]). We can think of a GAN that can be design and train 
to generate image of different art style or different art genre by provide such label with train- 
ing. This might be able to generate art that look like, for example, Renaissance, Impressionism, 
or Cubism. However that do not lead to anything creative either. No creative artist will create 
art today that try to emulate the Baroque or Impressionist style, or any traditional style, unless 
do so ironically. According to Berlyne and Martindale, artist would try to increase the arousal 
potential of their art by create novel, surprising, ambiguous, and/or puzzle art. This highlight 
the fundamental limitation of use GANs in generate creative works. 

2.4 From be Emulative to be Creative 
In the propose Creative Adversarial Network (CAN), the generator be design to receive two 
signal from the discriminator that act a two contradictory force to achieve three points: 1) 
generate novel works, 2) the novel work should not too novel, i.e., it should not be too far away 
from the distribution or it will generate too much arousal, thereby activate the aversion system 
and fall into the negative hedonic range accord to the Wundt curve, 3) the generate work 
should increase the stylistic ambiguity. 

Similar to Generative Adversarial Networks (GAN), the propose network have two adversary 
networks, a discriminator and a generator. The discriminator have access to a large set of art asso- 
ciated with style label (Renaissance, Baroque, Impressionism, Expressionism, etc.) and us it to 
learn to discriminate between styles. The generator do not have access to any art. It generates 
art start from a random input, but unlike GAN, it receives two signal from the discriminator 
for any work it generates. The first signal be the discriminator’s classification of “art or not art”. 
In traditional GAN, this signal enables the generator to change it weight to generate image that 
more frequently will deceive the discriminator a to whether it be come from the same distribu- 
tion. Since the discriminator in our case be train on art, this will signal whether the discriminator 
think the generate art be come from the same distribution a the actual art it know about. In 
that sense, this signal flag whether the discriminator think the image present to it be “art or not 
art”. Since the generator only receives this signal, it will eventually converge to generate image 
that will emulate art. 

The second signal the generator receives be a signal about how well the discriminator can clas- 
sify the generate art into establish styles. If the generator generates image that the discrimina- 
tor think be art and also can easily classify into one of the establish styles, then the generator 
would have fool the discriminator into believe it generate actual art that fit within establish 
styles. In contrast, the creative generator will try to generate art that confuses the discriminator. 
On one hand it try to fool the discriminator to think it be “art,” and on the other hand it try to 
confuse the discriminator about the style of the work generated. 

These two signal be contradictory forces, because the first signal push the generator to 

6 



Figure 2: Block diagram of the CAN system. 

generate work that the discriminator accepts a “art,” however if it succeed within the rule of 
establish styles, the discriminator will also be able to classify it style. Then the second signal 
will heftily penalize the generator for do that. This be because the second signal push the 
generator to generate style-ambiguous works. Therefore, these two signal together should push 
the generator to explore part of the creative space that lay close to the distribution of art (to 
maximize the first objective), and at the same time maximizes the ambiguity of the generate art 
with respect to how it fit in the realm of standard art styles. 

3 Technical Details 

3.1 Generative Adversarial Networks 
Generative Adversarial Network (GAN) [8] be one of the most successful image synthesis model 
in the past few years. GAN be typically train by set a game between two players. The 
first player, call the generator, G, generates sample that be intend to come from the same 
probability distribution a the training data (i.e. pdata), without have access to such data. The 
other player, denote a the discriminator, D, examines the sample to determine whether they 
be come from pdata (real) or not (fake). Both the discriminator and the generator be typically 
model a deep neural networks. The training procedure be similar to a two-player min-max game 
with the follow objective function 

min 
G 

max 
D 

V (D,G) = Ex∼pdata [logD(x)] + 

Ez∼pz [log(1−D(G(z)))], 
(1) 

where z be a noise vector sample from distribution pz (e.g., uniform or Gaussian distribution) and 
x be a real image from the data distribution pdata. In practice, the discriminator and the generator 
be alternatively optimize for every batch. The discriminator aim at maximize Eq 1 by mini- 
mizing −Ex∼pdata [logD(x)] − Ez∼pz [log(1 −D(G(z))), which improves the utility of the D a a 
fake vs. real image detector. Meanwhile, the generator aim at minimize Eq 1 by maximize 
log(D(G(z)), which work good than −log(1 − D(G(z)) since it provide strong gradients. 

7 



0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
-5 

-4 

-3 

-2 

-1 

0 

1 

Figure 3: Style ambiguity cross entropy loss function (two class case). Red curve: entropy, Blue curve: cross 
entropy with a uniform distribution (inverted here for illustration). Both function be maximize when the class be 
equiprobable. In contrast to entropy, which go to zero at the boundaries, the invert cross entropy go to negative 
infinity at the boundary, which cause hefty penalty for sample classify correctly. 

By optimize D and G alternatively, GAN be train to generate image that emulate the training 
distribution. 

3.2 Creative Adversarial Networks 
We modify the GAN loss function to achieve the vision explain in the previous section. Fig- 
ure 2 illustrates the architecture. We add a style classification loss and a style ambiguity loss. 
Maximizing the stylistic ambiguity can be achieve by maximize the style class posterior en- 
tropy. Hence, we need to design the loss such that the generator G produce an image x ∼ pdata 
and, meanwhile, maximizes the entropy of p(c|x) (i.e. style class posterior) for the generate im- 
ages. However, instead of maximize the class posterior entropy, we minimize the cross entropy 
between the class posterior and a uniform target distribution. Similar to entropy that be maximize 
when the class posterior (i.e., p(c|G(z))) be equiprobable, cross entropy with uniform target dis- 
tribution will be minimize when the class be equiprobable. So both objective will be optimal 
when the class be equiprobable. However, the difference be that the cross entropy will go up 
sharply at the boundary since it go to infinity if any class posterior approach 1 (or zero), while 
entropy go to zero at this boundary condition (see Figure 3). Therefore, use the cross entropy 
result in a hefty penalty if the generate image be classify to one of the class with high proba- 
bility. This in turn would generate very large loss, and hence large gradient if the generate image 
start to be classify to any of the style class with high confidence. Hence, we can redefine the 
cost function with a different adversarial objective a 

min 
G 

max 
D 

V (D,G) = 

Ex,ĉ∼pdata [logDr(x) + logDc(c = ĉ|x)] + 

Ez∼pz [log(1−Dr(G(z)))− 
K∑ 
k=1 

( 1 
K 

log(Dc(ck|G(z))+ 

(1− 1 
K 

)log(1−Dc(ck|G(z)) 
) 
], 

(2) 

8 



where z be a noise vector sample from distribution pz (e.g., uniform or Gaussian distribution) and 
x and ĉ be a real image and it correspond style label from the data distribution pdata. Dr(·) be 
the transformation function that try to discriminate between real art and generate images. Dc(·) 
be the the function that discriminates between different style category and estimate the style class 
posterior (i.e., Dc(ck|·) = p(ck|·)). 
Discriminator Training: In Eq 2, the discriminator D encourages maximize Eq 2 by minimiz- 
ing −Ex∼pdata [logDr(x) + logDc(c = ĉ|x)] for the real image and −Ez∼pz [log(1 − Dr(G(z)))] 
for the generate images. The discriminator be train to, not only discriminate the real art sample 
from the generate (fake) ones, but also to identify their style class though the K-way loss (where 
K be the number of style classes). Therefore, the discriminator be simultaneously learn about 
both the art distribution and art styles. 
Generator Training: The generator G encourages minimize Eq 2 by maximize log(1 − 
Dr(G(z))− 

∑K 
k=1( 

1 
K 
log(Dc(ck|G(z)) + (1− 1K )log(1−Dc(ck|G(z)). This push the generate 

image to look a real art (first term) and meanwhile to have a large cross entropy for p(c|G(z)) 
with a uniform distribution to maximize style ambiguity (second term). Note that the CAN gener- 
ator do not require any class labels, similar to unconditional generative model. 
Model Architecture: Algorithm 1 illustrates CAN training process. The Generator G and similar 
to DCGAN architecture [18], first z ∈ R100 normally sample from 0 to 1 be up-sampled to a 4× 
spatial extent convolutional representation with 2048 feature map result in a 4 × 4 × 2048 
tensor. Then a series of four fractionally-stride convolution (in some papers, wrongly call 
deconvolutions). Finally, convert this high level representation into a 256 × 256 pixel image. In 
other words, start from z ∈ R100 → 4 × 4 × 1024 → 8 × 8 × 1024 → 16 × 16 × 512 → 
32× 32× 256→ 64× 64× 128→ 128× 128× 64→ 256× 256× 3 (the generate image size). 
As described earlier, the discriminator have two loss (real/fake loss and multi-label loss). The 
discriminator in our work start by a common body of convolution layer follow by two head 
(one for the real/fake loss and one for the multi-label loss). The common body of convolution layer 
be compose of a series of six convolution layer (all with stride 2 and 1 pixel padding). conv1 (32 
4× 4 filters), conv2 (64 4× 4 filters, conv3 (128 4× 4 filters, conv4 (256 4× 4 filters, conv5 (512 
4 × 4 filters, conv6 (512 4 × 4 filters). Each convolutional layer be follow by a leaky rectify 
activation (LeakyRelU) [13, 25] in all the layer of the discriminator. After passing a image to 
the common conv D body, it will produce a feature map or size (4 × 4 × 512). The real/fake Dr 
head collapse the (4 × 4 × 512) by a fully connect to produce Dr(c|x) (probability of image 
come for the real image distribution). The multi-label probability Dc(ck|x) head be produce 
by passing the(4× 4× 512) into 3 fully collect layer size 1024, 512, K, respectively, where K 
be the number of style classes. 
Initialization and Training: The weight be initialize from a zero-centered Normal distribu- 
tion with standard deviation 0.02. We use a mini-batch size of 128 and use mini-batch stochastic 
gradient descent (SGD) for training with 0.0001 a learn rate. In the LeakyReLU, the slope of 
the leak be set to 0.2 in all models. While previous GAN work have use momentum to acceler- 
ate training, we use the Adam optimizer and train the model for 100 epoch (100 pass over 
the training data). To stabilize the training, we use Batch Normalization [11] that normalize 
the input to each unit to have zero mean and unit variance. We perform data augmentation by 
add 5 crop within for each image (bottom-left, bottom-right, mid, top-left, top-right) on our 
image dataset. The width and hight of each crop be 90% of the width and the hight of the original 
painting. 

9 



Algorithm 1 CAN training algorithm with step size α, use mini-batch SGD for simplicity. 
1: Input: mini-batch image x, match label ĉ, number of training batch step S 
2: for n = 1 to S do 
3: z ∼ N (0, 1)Z {Draw sample of random noise} 
4: x̂← G(z) {Forward through generator} 
5: srD ← Dr(x) {real image, real/fake loss } 
6: scD ← Dc(ĉ|x) {real image, multi class loss} 
7: sfG ← Dr(x̂) {fake image, real/fake loss} 
8: scG ← 

∑K 
k=1 

1 
K 
log(p(ck|x̂) + (1− 1K )(log(p(ck|x̂)) {fake image Entropy loss} 

9: LD ← log(srD) + log(scD) + log(1− s 
f 
G) 

10: D ← D − α∂LD/∂D {Update discriminator} 
11: LG ← log(sfG)− scG 
12: G← G− α∂LG/∂G {Update generator} 
13: end for 

Table 1: Artistic Styles Used in Training 
Style name Image number Style name Image number 
Abstract-Expressionism 2782 Mannerism-Late-Renaissance 1279 
Action-Painting 98 Minimalism 1337 
Analytical-Cubism 110 Naive Art-Primitivism 2405 
Art-Nouveau-Modern 4334 New-Realism 314 
Baroque 4241 Northern-Renaissance 2552 
Color-Field-Painting 1615 Pointillism 513 
Contemporary-Realism 481 Pop-Art 1483 
Cubism 2236 Post-Impressionism 6452 
Early-Renaissance 1391 Realism 10733 
Expressionism 6736 Rococo 2089 
Fauvism 934 Romanticism 7019 
High-Renaissance 1343 Synthetic-Cubism 216 
Impressionism 13060 Total 75753 

4 Results and Validation 

4.1 Training the model 
We train the network use painting from the publicly available WikiArt dataset 3. This col- 
lection (as download in 2015) have image of 81,449 painting from 1,119 artist range from 
the Fifteenth century to Twentieth century. Table 1 show the number of image use in training 
the model from each style. 

10 



Figure 4: Images from three baselines. Top Panel: Sample image generate by art-trained DC- 
GAN 64x64 baseline model (top 6 and bottom 6 ranked image be show accord to human 
subject likeness rating in Experiment I). Middle Panel: Sample image generate by art-trained 
DCGAN 256x256 baseline model (top 6 and bottom 6 ranked image be show accord to hu- 
man subject likeness rating in Experiment II). Bottom Panel: Sample image generate by style- 
classification CAN model, a variant of CAN with style classification loss and without the style 
ambiguity loss. The image show recognizable genre such a portraits, landscapes, etc. 

11 



4.2 Qualitative Validation 
Assessing the creativity of artifact generate by the machine be an open and hard question. As 
note by Colton 2008, aesthetic assessment of an artifact be different from the creativity assess- 
ment [4]. Figures 1, Figures 4.2 and 7 show sample of the generate image by the propose 
CAN model. The image generate by CAN do not look like traditional art, in term of standard 
genre (portrait, landscapes, religious paintings, still life, etc.). We also do not see any recogniz- 
able figures. Many of the image seem abstract. Is that simply because it fails to emulate the art 
distribution, or be it because it try to generate novel images? Is it at all creative? These be hard 
question which we will try to get an insight to help answer them. 

We evaluate the image generate by three baseline model in comparison to the one gener- 
ated by the propose CAN model to gain an insight into the difference qualitatively and quantita- 
tively. All the model be train on the same art dataset, a show in table 1. 

The first two baseline be variant of the original DCGAN [18] model train on art data. 
Since GAN aim to emulate the training distribution, we should expect training the model on art 
data would result in generate image that show recognizable figures, subject matters, art genres, 
and styles. The first baseline model be the original DCGAN [18] model train on art data. This 
model generates image with 64x64 resolution. Although train on art data, this model fail to 
generate image that emulate the train art. The generate sample do not show any recognizable 
figure or art genre or styles. The top panel of Figure 4 show sample of the generate images; 
here we show the six top and six bottom ranked image base on human subject evaluation in 
Experiment I below. 

The second baseline model be the original DCGAN [18] model after add two more layer 
to the generator to increase the resolution to 256x256, i.e., the generator here have the same exact 
architecture a the CAN model. We also train this model on the art collection. The generate 
sample show significant improvement; we can clearly see aesthetically appeal compositional 
structure and color contrast in the result images. However, the generate image also do not 
show any recognizable figures, subject matter or art genres. The middle panel of Figure 4 show 
sample of the generate images; here we show the six top and six bottom ranked image base on 
human subject evaluation in Experiment II below. 

The third model be a variant of the propose CAN model with the style classification loss 
(without the style ambiguity loss). In that model the discriminator learns to discriminate between 
style class along learn the art distribution. The generator have exactly the same loss a the GAN 
model, i.e., only try to deceive the discriminator by emulate the training distribution. In other 
words, unlike the two first baseline that learn about art (art/nor art), this model also learn about 
style class in art. We refer to this model by style-classification-CAN. The generate image of 
that model show significant improvement in actually emulate the art distribution, in the sense that 
we can see lot of hallucination of portraits, landscapes, architectures, religious subject matter, etc. 
We didn’t see any of that on the first two baselines. The bottom panel of Figure 4 show sample of 
the generate images, which show portraits, landscapes, architecture elements, religious themes, 
etc. This baseline show that the CAN model, without the style ambiguity, can good emulate 
the art distribution by learn about style classes, however not creative (Experiment IV below be 
design to quantitatively validate this claim). 

3https://www.wikiart.org/ 

12 



Figures 4.2 show sample of image generate by CAN. The figure show top ranked and low- 
est ranked image accord to human subjects. In contrast to the aforementioned three baselines, 
the propose CAN model generates image that can be characterize a novel and not emulate 
the art distribution, however, aesthetically appealing. Although the generate image of the CAN 
model do not show typical figures, genres, styles, or subject matter (similar to the first two baseline 
anyway), we cannot say that this be because it cannot emulate the art distribution, since, simply, re- 
move the style ambiguity loss reduces the model to the third baseline (style-classification-CAN) 
which be successful in generate such elements. So we can claim that the style ambiguity loss 
force the network to try to generate novel image and, in the same time, stay closer to the art 
distribution (by be aesthetically appealing). We will try to test this claim quantitatively by a 
series of human subject experiment in the next section. 

4.3 Quantitative Validation 
We conduct human subject experiment to evaluate aspect of the creativity of the propose 
model. The goal of these experiment be to test whether human subject would be able to distin- 
guish whether the art be generate by a human artist or by a computer system, a well a to rate 
aspect of that art. However, the hard question be which art by human artist we should use for this 
comparison. Since the goal of this study be to evaluate the creativity of the artifact produce by the 
propose system, we need to compare human response to such artifact with art that be consider 
to be novel and creative at this point in time. If we compare the produce artifact to, for example, 
Impressionist art, we would be test the ability of the system to emulate such art, and not the 
creativity of the system. Therefore we collect two set of work by real artists, a well a four 
machine-generated set a follows: 

1. Abstract Expressionist Set: A collection of 25 painting by Abstract Expressionist master 
make between 1945-2007, many of them by famous artists. This set be previously use 
in recent study to compare human and machine’s ability to distinguish between abstract 
art create by artists, child or animal [23, 21]. We use this set a a baseline set. Human 
subject be expect to easily determine that these be create by artist base on familiarity. 
We use Abstract Expressionist art in particular because they lack recognizable figure or 
lucid subject matter. Existence of figure or clear subject matter might directly bias the 
subject to conclude that such painting be do by human when contrast to the generate 
image which lack such figures. 

2. Art Basel 2016 Set: This set consists of 25 painting of various artist that be show in 
Art Basel 2016, which be the flagship art fair for contemporary art world wide. Being show 
in Art Basel 2016 be an indication that these be art work at the frontier of human creativity 
in paintings, at least a judged by the art expert and the art market. We select this set at 
random after exclude art that have clear figure or obvious brush stroke which might bias 
the subjects. The collection be show in Figure 8. 

3. DCGAN Sets: We use two set of image generate by the state-of-the art Deep Convolu- 
tion GAN (DCGAN) architecture [18], a described in the baseline model in Section 4.2. 
The first set contains 100 image generate at 64x64 resolution. The second set consists of 
76 image generate at 256x256 resolution. 

13 



Figure 5: Example of image generate by CAN. Top: Images ranked high in “likeness” accord 
to human subjects. Bottom: Images ranked the low by human subjects. 

14 



Table 2: Means and standard deviation of response of Experiment I 
Painting set Q1 (std) Q2 (std) 

CAN 53% (18%)† 3.2 (1.5) ‡ 

DCGAN [18] (64x64) 35% (15%) † 2.8 (0.54) ‡ 

Abstract Expressionist 85% (16%) 3.3 (0.43) 
Art Basel 2016 41% (29%) 2.8 (0.68) 

Artist set combine 62% (32%) 3.1 (0.63) 
All image be resize to 512x512 resolution 
† Q1t-test (CAN vs. DCGAN) p-value = 1.9932e− 15 
‡ Q2 t-test (CAN vs. DCGAN) p-value = 9.3634e− 06 

4. Style Classification CAN Set (sc-CAN): a set of 100 image generate by a variant of the 
model that involves style classifier at the discriminator, a explain in Section 4.2. We 
mainly use this set to compare the effectiveness of add style ambiguity to the propose 
loss function. 

5. CAN Set: a set of 125 image generate by the propose model. 

We use the same set of training image for all the model and we conduct three human subject 
experiment a follows. All generate image be upscaled to 512x512 resolution use a super- 
resolution algorithm. Artist image be also resize to 512x512 resolution. 

Experiment I: 
The goal of this experiment be to test the ability of the system to generate art that human user 
could not distinguish from top creative art that be be generate by artist today. We use four 
image set (Abstract Expressionist, Art Basel, CAN and DCGAN(64x64)). In this experiment each 
subject be show one image at time select from the four set of image and asked: 

Q1: Do you think the work be create by an artist or generate by a computer? The user have to 
choose one of two answers: artist or computer. 

Q2: The user ask to rate how they like the image in a scale 1 (extremely dislike) to 5 (extremely 
like). 

Results: 18 MTurk user participate in this experiment, where for each image we receive 
10 distinct responses. The result be summarize in Table 2. There be several conclusion we 
can draw from these results: 1) As expected, subject rat the Abstract Expressionist set high 
a be create by an artist (85%). 2) The propose CAN model out-performed the GAN model 
in generate image that human subject think be generate by artist (53% vs. 35%). Human 
subject also like the image generate by CAN more than GAN (3.2 vs. 2.8) We perform 
two-sample t-test to determine the statistical significance of these results, with the null hypothesis 
be that the subjects’ response for both CAN and GAN be come from the same distribution. 
t-test reject this hypothesis with p-value = 1.9932e− 15 and 9.3634e− 06 for question 1 and 2 
respectively. This test highlight that the difference be statistically significant. Of course we cannot 

15 



Figure 6: Experiment I (Q1 vs. Q2 responses) 

obviously conclude from this result that CAN be more creative than GAN. A system that would 
perfectly copy human art, without be innovative, would score high in that question. However, 
we can exclude this possibility since the generate image by both CAN and GAN be not copying 
human art a be explain in Section 4.2. 3) More interestingly, human subject rat the image 
generate by CAN high a be create by a human than the one from the Art Basel set (53% 
vs. 41%) when combine the two set of art create by artists, the image generate by CAN 
score only 9% less (53% vs. 62%). Figure 7 show the top ranked CAN image accord to 
subjects’ responses. Figure 6 show a scatter plot of the response for the two questions, which 
interestingly show weak correlation between the likeness rating and whether subject think it be 
by an artist or a computer. 

4.4 Experiment II: 
To confirm the result of experiment I we design another experiment where in each survey we 
show an image and ask subject a series of question first before ask the question whether 
the show image be generate by a human artist or computer. We hypothesize that if that question 
be ask first, this would have a high chance of subject answer it randomly, and defer it 
till after a series of other question about the image would lead to a more constructive response. 

The question be base on the collative variable a explain in Section 2.1 and specify as: 

Q1 How do you like this image: 1-extremely dislike, 2-dislike, 3-Neutral, 4-like, 5-extremely like. 

Q2 Rate the novelty of the image: 1-extremely not novel, 2-some how not novel, 3-neutral, 4- 
somehow novel, 5-extremely novel. 

Q3 Do you find the image surprising: 1-extremely not surprising, 2-some how not surprising, 
3-neutral, 4-some how surprising, 5-extremely surprising. 

Q4 Rate the ambiguity of the image. I find this image: 1-extremely not ambiguous, 2-some how 
not ambiguous, 3-neutral, 4-some how ambiguous, 5-extremely ambiguous. 

Q5 Rate the complexity of the image. I find this image: 1-extremely simple, 2-some how simple, 
3-neutral, 4-somehow complex, 5-extremely complex 

Q6 Do you think the image be create by an artist or generate by computer? 

We use the same set of image a in experiment I except that we change the GAN set to a 
new set generate by DCGAN model with output resolution 256x256, which be the same resolution 

16 



Table 3: Means and standard deviation of response of Experiment II 
Q1 (std) Q2 (std) Q3 (std) Q4 (std) Q5 (std) Q6 (std) 

Image set Likeness Novelty Surprising Ambiguity Complexity human/computer 
DCGAN [18] (256x256) 3.23 (0.53) 3.08 (0.50) 3.21 (0.59) 3.37 (0.48) 3.18 (0.63) 0.65 (0.17) 

CAN 3.30 (0.43) 3.27 (0.44) 3.13 (0.46) 3.54 (0.45) 3.34 (0.50) 0.75 (0.14) 
Abstract Expresionist 3.38 (0.43) 3.03 (0.38) 2.95 (0.50) 3.17 (0.35) 2.90 (0.35) 0.85 (0.11) 

Art Basel 2016 2.95 (0.70) 2.69 (0.59) 2.36 (0.66) 2.79 (0.59) 2.46 (0.68) 0.48 (0.23) 

a the CAN model. Both the CAN and GAN set be upsampled to 512x512. All artist set be 
resize to 512x512 a well. 

Results: We conduct this experiment use Amazon MTurk, where for each image we re- 
ceived 10 distinct responses. Table 3 summarize the results. In general there be no much difference 
in the rating for the different set for question 1 to 5. However the response to Q6 be signif- 
icantly different for each dataset. The result show that CAN out-perform GAN in generate 
image that human subject think be generate by artist (75% vs. 65%). This difference be sta- 
tistically significant (t-test p-value=1.0147e − 05 ). This result be consistent with experiment I, 
despite we change the GAN set to a high resolution version. Although the same set of CAN 
image be use in both Experiment I and II, the response to the human/computer question be 
much high in Experiment II (75% v 53%), which confirms our hypothesis about defer that 
question. The rank order between Abstract Expressionist, CAN, and Art Basel sets, in regard to 
be create by artist (Q6), be also consistent with Q1 in experiment 1. 

Experiment III: 
This goal of this experiment be to judge aspect related to whether the image generate by CAN 
can be consider art. We compare the CAN set with both the Art Expressionist and the Art Basel 
Sets. This experiment be similar to an experiment conduct by Snapper et al [23] to determine 
to what degree human subject find the work of art to be intentional, have visual structure, 
communicative, and inspirational. In this experiment an image be show and the subject be ask 
the follow four questions: 

Q1: As I interact with this painting, I start to see the artist’s intentionality: it look like it be 
compose very intentionally. 

Q2: As I interact with this painting, I start to see a structure emerging. 

Q3: Communication: As I interact with this painting, I feel that it be communicate with me. 

Q4: Inspiration: As I interact with this painting, I feel inspire and elevated. 

For each of the question the user answer in a scale from 1 (Strongly Disagree) to 5 (Strongly 
Agree). The user be ask to look at each image at least 5 second before answering. 21 user 
participate in this experiment with response from 10 distinct user for each image. 

Snapper et al hypothesize that subject would rate work by real artist high in these scale 
that work by child or animals, and indeed their experiment validate their hypothesis[23]. 

We also hypothesize that human subject would rate art by real artist high on these scale 
than those generate by the propose system. To our surprise the result show that our hypothesis 

17 



Table 4: Means and standard deviation of the response of Experiment III 
Q1 (std) Q2 (std) Q3 (std) Q4 (std) 

Painting set Intentionality Visual Structure Communication Inspiration 
CAN 3.3 (0.47) 3.2 (0.47) 2.7 (0.46) 2.5 (0.41) 

Abstract Expressionist 2.8 (0.43) 2.6 (0.35) 2.4 (0.41) 2.3 (0.27) 
Art Basel 2016 2.5 (0.72) 2.4 (0.64) 2.1 (0.59) 1.9(0.54) 

Artist set combine 2.7 (0.6) 2.5 (0.52) 2.2 (0.54) 2.1 (0.45) 

be not true! Human subject rat the image generate by the propose system high than those 
create by real artists, whether in the Abstract Expressionism set or in the Art Basel set (see 
Table 3). 

It might be debatable what a high score in each of these scale actually means, and whether 
the difference be statistically significant. However, the fact that subject found the image gen- 
erated by the machine intentional, visually structured, communicative, and inspiring, with similar 
level to actual human art, indicates that subject see these image a art! Figure 7 show several 
example generate by CAN, ranked by the response of human subject to each question. 

Experiment IV: 
The goal of this experiment be to evaluate the effect of add the style ambiguity loss to the CAN 
model, in contrast to the style classification loss, in generate novel and aesthetically appeal 
images. In other words, be it learn about style or deviate from style that cause the result to 
be creative. To ass creativity we refer to the most common definition of creativity of an artifact 
a be novel and influential [17, 7]. However, since influence be not relevant here, we use novelty 
a a proxy for creativity. In this experiment, in order to evaluate novelty we use a pool of art 
history student a sophisticated art-educated subject who can judge the novelty and aesthetic 
good than general MTurk subjects. Each subject be show pair of images, one from the CAN 
set and one from the sc-CAN model, randomly select and place in random order side by side. 
Each subject be ask two question for each pair: 

Q1 Which image do you think be more novel? 

Q2 Which image do you think be more aesthetically appealing? 

Results: The result of this experiment show that 59.47% of the time subject select CAN 
image a more novel and 60% of the time they found CAN image more aesthetically appealing. 
This indicates the effect of the style ambiguity loss in the process of generation by CAN compare 
to the style classification loss. 

5 Discussion and Conclusion 
We propose a system for generate art with creative characteristics. We demonstrate a realiza- 
tion of this system base on a novel creative adversarial network. The system be train use a 
large collection of art image from the 15th century to 21st century with their style labels. The 

18 



Figure 7: Top Ranked Images From CAN in Human Subject Experiment I and III 

19 



Figure 8: Art Basel Set: a collection of 25 painting select from Art Basel 2017 art fair. Artist 
and year in order: Richard Caldicott 2003, Jigger Cruz 2016, Leonardo Drew 2015, Cenk Akaltun 
2015, Lang Li 2014, Xuerui Zhang 2015, David Smith 1956, Kelu Ma 1989, Xie Nanxing 2013, 
Panos Tsagaris 2015, Heimo Zobernig 2014, Zao Wou-Ki 1958, Andy Warhol 1985, David Smith 
1956, Wei Ligang 2014, KONG Chun Hei 2016, Ye Yongqing 2015, Wei Ligang 2010, Xiaorong 
Pan 2015, Xuerui Zhang 2016, Xiaorong Pan 2015, Xiaorong Pan 2015, Xu Zhenbang 2015, 
Xuerui Zhang 2016, Zao Wou-Ki 1963. 

system be able to generate art by optimize a criterion that maximizes stylistic ambiguity while 
stay within the art distribution. The system be evaluate by human subject experiment which 
show that human subject regularly confuse the generate art with the human art, and some- 
time rat the generate art high on various high-level scales. 

What creative characteristic do the propose system have? Colton 2008 suggest three 
criterion that a creative system should have: the ability to produce novel artifact (imagination), 
the ability to generate quality artifact (skill), and the ability to ass it own creation [4]. Our 
propose system posse the ability to produce novel artifact because the interaction between 
the two signal that derive the generation process be design to force the system to explore creative 
space to find solution that deviate from establish style but stay close enough to the boundary 

20 



of art to be recognize a art. This interaction also provide a way for the system to self-assess it 
products. The quality of the artifact be verify by the human subject experiments, which show 
that subject not only thought these artifact be create by artists, but also rat them high on 
some scale than human art. 

One of the main characteristic of the propose system be that it learns about the history of art 
in it process to create art. However it do not have any semantic understand of art behind the 
concept of styles. It do not know anything about subject matter, or explicit model of element 
or principle of art. The learn here be base only on exposure to art and concept of styles. In 
that sense the system have the ability to continuously learn from new art and would then be able to 
adapt it generation base on what it learns. 

We leave open how to interpret the human subjects’ response that ranked the CAN art good 
than the Art Basel sample in different aspects. Is it because the user have typical style-backward 
bias? Are the subject bias by their aesthetic assessment? Would that mean that the result be 
not that creative? More experiment be definitely need to help answer these questions. 

References 
[1] Ellie Baker and Margo I Seltzer. Evolving line drawings. 1993. 

[2] Daniel E Berlyne. Arousal and reinforcement. In Nebraska symposium on motivation. Uni- 
versity of Nebraska Press, 1967. 

[3] Daniel E Berlyne. Aesthetics and psychobiology, volume 336. JSTOR, 1971. 

[4] Simon Colton. Creativity versus the perception of creativity in computational systems. In 
AAAI spring symposium: creative intelligent systems, volume 8, 2008. 

[5] Simon Colton, Jakob Halskov, Dan Ventura, Ian Gouldstone, Michael Cook, and Blanca 
Perez-Ferrer. The painting fool sees! new project with the automate painter. In Proceedings 
of the 6th International Conference on Computational Creativity, page 189–196, 2015. 

[6] Steve DiPaola and Liane Gabora. Incorporating characteristic of human creativity into an 
evolutionary art algorithm. Genetic Programming and Evolvable Machines, 10(2):97–110, 
2009. 

[7] Ahmed Elgammal and Babak Saleh. Quantifying creativity in art networks. In Proceedings 
of the 6th International Conference on Computational Creativity, 2015. 

[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil 
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in 
neural information processing systems, page 2672–2680, 2014. 

[9] Jeanine Graf and Wolfgang Banzhaf. Interactive evolution of images. In Evolutionary Pro- 
gramming, page 53–65, 1995. 

[10] Derrall Heath and Dan Ventura. Before a computer can draw, it must first learn to see. In 
Proceedings of the 7th International Conference on Computational Creativity, 2016. 

21 



[11] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training 
by reduce internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 

[12] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual loss for real-time style transfer 
and super-resolution. In European Conference on Computer Vision, page 694–711. Springer, 
2016. 

[13] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectifier nonlinearities improve neural 
network acoustic models. In Proc. ICML, volume 30, 2013. 

[14] Penousal Machado, Juan Romero, and Bill Manaris. An iterative approach to stylistic change 
in evolutionary art. 

[15] Colin Martindale. The clockwork muse: The predictability of artistic change. Basic Books, 
1990. 

[16] Alexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper into 
neural networks. Google Research Blog. Retrieved June, 20:14, 2015. 

[17] Elliot Samuel Paul and Scott Barry Kaufman. Introducing the philosophy of creativity. In 
The Philosophy of Creativity: New Essays. Oxford University Press, 2014. 

[18] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learn with 
deep convolutional generative adversarial networks. In International Conference on Learning 
Representation, 2016. 

[19] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak 
Lee. Generative adversarial text-to-image synthesis. In Proceedings of The 33rd International 
Conference on Machine Learning, 2016. 

[20] Theodore Christian Schneirla. An evolutionary and developmental theory of biphasic pro- 
ce underlie approach and withdrawal. 1959. 

[21] Lior Shamir, Jenny Nissel, and Ellen Winner. Distinguishing between abstract art by artist 
vs. child and animals: Comparison between human and machine perception. ACM Trans- 
action on Applied Perception (TAP), 13(3):17, 2016. 

[22] Karl Sims. Artificial evolution for computer graphics, volume 25. ACM, 1991. 

[23] Leslie Snapper, Cansu Oranç, Angelina Hawley-Dolan, Jenny Nissel, and Ellen Winner. Your 
kid could not have do that: Even untutored observer can discern intentionality and struc- 
ture in abstract expressionist art. Cognition, 137:154–165, 2015. 

[24] Wilhelm Max Wundt. Grundzüge de physiologischen Psychologie, volume 1. W. Engelman, 
1874. 

[25] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectify activation 
in convolutional network. arXiv preprint arXiv:1505.00853, 2015. 

22 


1 Introduction 
2 Methodology 
2.1 Background 
2.2 Art Generating Agent 
2.3 GAN: Emulative and not Creative 
2.4 From be Emulative to be Creative 

3 Technical Details 
3.1 Generative Adversarial Networks 
3.2 Creative Adversarial Networks 

4 Results and Validation 
4.1 Training the model 
4.2 Qualitative Validation 
4.3 Quantitative Validation 
4.4 Experiment II: 

5 Discussion and Conclusion 

