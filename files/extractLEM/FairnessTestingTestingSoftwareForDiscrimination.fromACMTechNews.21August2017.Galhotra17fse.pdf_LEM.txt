




















































Fairness Testing: Testing Software for Discrimination 


Fairness Testing: 
Testing Software for Discrimination 
Sainyam Galhotra Yuriy Brun Alexandra Meliou 

University of Massachusetts, Amherst 
Amherst, Massachusetts 01003-9264, USA 
{sainyam, brun, ameli}@cs.umass.edu 

ABSTRACT 
This paper defines software fairness and discrimination and devel- 
ops a testing-basedmethod formeasuring if and howmuch software 
discriminates, focus on causality in discriminatory behavior. 
Evidence of software discrimination have be found in modern 
software system that recommend criminal sentences, grant access 
to financial products, and determine who be allow to participate 
in promotions. Our approach, Themis, generates efficient test suite 
to measure discrimination. Given a schema describe valid system 
inputs, Themis generates discrimination test automatically and 
do not require an oracle. We evaluate Themis on 20 software 
systems, 12 of which come from prior work with explicit focus 
on avoid discrimination. We find that (1) Themis be effective at 
discover software discrimination, (2) state-of-the-art technique 
for remove discrimination from algorithm fail in many situa- 
tions, at time discriminate against a much a 98% of an input 
subdomain, (3) Themis optimization be effective at produce 
efficient test suite for measure discrimination, and (4) Themis be 
more efficient on system that exhibit more discrimination. We thus 
demonstrate that fairness test be a critical aspect of the software 
development cycle in domain with possible discrimination and 
provide initial tool for measure software discrimination. 

CCS CONCEPTS 
• Software and it engineering → Software test and de- 
bugging 

KEYWORDS 
Discrimination testing, fairness testing, software bias, test 

ACM Reference format: 
Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness 
Testing: Testing Software for Discrimination. In Proceedings of 2017 11th 
Joint Meeting of the European Software Engineering Conference and the ACM 
SIGSOFT Symposium on the Foundations of Software Engineering, Paderborn, 
Germany, September 4–8, 2017 (ESEC/FSE’17), 13 pages. 
https://doi.org/10.1145/3106237.3106277 

Permission to make digital or hard copy of all or part of this work for personal or 
classroom use be grant without fee provide that copy be not make or distribute 
for profit or commercial advantage and that copy bear this notice and the full citation 
on the first page. Copyrights for component of this work own by others than the 
author(s) must be honored. Abstracting with credit be permitted. To copy otherwise, or 
republish, to post on server or to redistribute to lists, require prior specific permission 
and/or a fee. Request permission from permissions@acm.org. 
ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany 
© 2017 Copyright held by the owner/author(s). Publication right license to Associa- 
tion for Computing Machinery. 
ACM ISBN 978-1-4503-5105-8/17/09. . . $15.00 
https://doi.org/10.1145/3106237.3106277 

1 INTRODUCTION 
Software have become ubiquitous in our society and the importance 
of it quality have increased. Today, automation, advance in ma- 
chine learning, and the availability of vast amount of data be 
lead to a shift in how software be used, enable the software to 
make more autonomous decisions. Already, software make deci- 
sion in what product we be lead to buy [53], who get a loan [62], 
self-driving car action that may lead to property damage or human 
injury [32], medical diagnosis and treatment [74], and every stage 
of the criminal justice system include arraignment and sentence 
that determine who go to jail and who be set free [5, 28]. The im- 
portance of these decision make fairness and nondiscrimination 
in software a important a software quality. 

Unfortunately, software fairness be undervalue and little at- 
tention be paid to it during the development lifecycle. Countless 
example of unfair software have emerged. In 2016, Amazon.com, 
Inc. use software to determine the part of the United States to 
which it would offer free same-day delivery. The software make 
decision that prevent minority neighborhood from participat- 
ing in the program, often when every surround neighborhood 
be allow to participate [36, 52]. Similarly, software be be 
use to compute risk-assessment score for suspect criminals. 
These scores— an estimate probability that the person arrest for 
a crime be likely to commit another crime— be use to inform deci- 
sion about who can be set free at every stage of the criminal justice 
system process, from assign bond amounts, to decide guilt, 
to sentencing. Today, the U.S. Justice Department’s National Insti- 
tute of Corrections encourages the use of such assessment scores. 
In Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, 
Virginia, Washington, and Wisconsin, these score be give to 
judge during criminal sentencing. The Wisconsin Supreme Court 
recently rule unanimously that the COMPAS computer program, 
which us attribute include gender, can assist in sentence de- 
fendants [28]. Despite the importance of these scores, the software 
be know to make mistakes. In forecasting who would reoffend, the 
software be “particularly likely to falsely flag black defendant a fu- 
ture criminals, wrongly label them this way at almost twice the 
rate a white defendants; white defendant be mislabeled a low 
risk more often than black defendants” [5]. Prior criminal history 
do not explain this difference: Controlling for criminal history, 
recidivism, age, and gender show that the software predicts black 
defendant to be 77% more likely to be pegged a at high risk 
of commit a future violent crime than white defendant [5]. 
Going forward, the importance of ensure fairness in software 
will only increase. For example, “it’s likely, and some say inevitable, 
that future AI-powered weapon will eventually be able to operate 
with complete autonomy, lead to a watershed moment in the 

498 

mailto:sainyam@cs.umass.edu,brun@cs.umass.edu,ameli@cs.umass.edu 
https://doi.org/10.1145/3106237.3106277 
https://doi.org/10.1145/3106237.3106277 


ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou 

history of warfare: For the first time, a collection of microchip and 
software will decide whether a human be life or dies” [34]. In 
fact, in 2016, the U.S. Executive Office of the President identify 
bias in software a a major concern for civil right [27]. And one of 
the ten principle and goal Satya Nadella, the CEO of Microsoft 
Co., have laid out for artificial intelligence be “AI must guard against 
bias, ensure proper, and representative research so that the wrong 
heuristic cannot be use to discriminate” [61]. 

This paper defines causal software discrimination and proposes 
Themis, a software test method for evaluate the fairness of 
software. Our definition capture causal relationship between in- 
put and outputs, and can, for example, detect when sentence 
software behaves such that “changing only the applicant’s race af- 
fects the software’s sentence recommendation for 13% of possible 
applicants.” Prior work on detect discrimination have focus on 
measure correlation or mutual information between input and 
output [79], discrepancy in the fraction of input that produce a 
give output [18, 19, 39–42, 87–89, 91], or discrepancy in output 
probability distribution [51]. These approach do not capture 
causality and can miss discrimination that our causal approach de- 
tects, e.g., when the software discriminates negatively with respect 
to a group in one settings, but positively in another. Restricting the 
input space to real-world input [1] may similarly hide software 
discrimination that causal test can reveal. Unlike prior work 
that require manually write test [79], Themis automatically 
generates test suite that measure discrimination. To the best of 
our knowledge, this work be the first to automatically generate test 
suite to measure causal discrimination in software. 

Themis would be useful for company and government agen- 
cies rely on software decisions. For example, Amazon.com, Inc. 
receive strong negative publicity after it same-day delivery al- 
gorithm make racially bias decisions. Politicians and citizen 
in Massachusetts, New York, and Illinois demand that the com- 
pany offer same-day delivery service to minority neighborhood in 
Boston, New York City, and Chicago, and the company be force 
to reverse course within mere day [72, 73]. Surely, the company 
would have prefer to test it software for racial bias and to de- 
velop a strategy (e.g., fix the software, manually review and 
modify racist decisions, or not use the software) prior to de- 
ploying it. Themis could have analyze the software and detect 
the discrimination prior to deployment. Similarly, a government 
may need to set nondiscrimination requirement on software, and 
be able to evaluate if software satisfies those requirement before 
mandate it to be use in the justice system. In 2014, the U.S. 
Attorney General Eric Holder warn that step need to be take 
to prevent the risk-assessment score inject bias into the courts: 
“Although these measure be craft with the best of intentions, 
I be concerned that they inadvertently undermine our effort to 
ensure individualize and equal justice [and] they may exacerbate 
unwarranted and unjust disparity that be already far too com- 
mon in our criminal justice system and in our society.” [5]. As with 
software quality, test be likely to be the best way to evaluate 
software fairness properties. 

Unlike prior work, this paper defines discrimination a a causal 
relationship between an input and an output. As define here, dis- 
crimination be not necessarily bad. For example, a software system 

design to identify if a picture be of a cat should discriminate be- 
tween cat and dogs. It be not our goal to eliminate all discrimination 
in software. Instead, it be our goal to empower the developer and 
stakeholder to identify and reason about discrimination in soft- 
ware. As described above, there be plenty of real-world example 
in which company would prefer to have discover discrimina- 
tion earlier, prior to release. Specifically, our technique’s job be to 
identify if software discriminates with respect to a specific set of 
characteristics. If the stakeholder expect cat vs. dog discrimination, 
she would exclude it from the list of input characteristic to test. 
However, learn that the software frequently misclassifies black 
cat can help the stakeholder improve the software. Knowing if 
there be discrimination can lead to better-informed decision making. 

There be two main challenge to measure discrimination via 
testing. First, generate a practical set of test input sufficient for 
measure discrimination, and second, processing those test inputs’ 
execution to compute discrimination. This paper tackle both chal- 
lenges, but the main contribution be compute discrimination from 
a set of executions. We be aware of no prior test technique, 
neither automate nor manual, that produce a measure of a soft- 
ware system’s causal discrimination. The paper also contributes 
within the space of efficient test input generation for the specific 
purpose of discrimination test (see Section 4), but some prior 
work, specifically in combinatorial testing, e.g., [6, 44, 47, 80], may 
further help the efficiency of test generation, though these tech- 
niques have not be previously apply to discrimination testing. 
We leave a detailed examination of how combinatorial test and 
other automate test generation can help further improve Themis 
to future work. 

This paper’s main contribution are: 

(1) Formal definition of software fairness and discrimination, in- 
cluding a causality-based improvement on the state-of-the-art 
definition of algorithmic fairness. 

(2) Themis, a technique and open-source implementation— 
https://github.com/LASER-UMASS/Themis— formeasuring dis- 
crimination in software. 

(3) A formal analysis of the theoretical foundation of Themis, in- 
cluding proof of monotonicity of discrimination that lead to 
provably sound two-to-three order of magnitude improve- 
ments in test suite size, a proof of the relationship between 
fairness definitions, and a proof that Themis be more efficient 
on system that exhibit more discrimination. 

(4) An evaluation of the fairness of 20 real-world software instance 
(based on 8 software systems), 12 of which be design with 
fairness in mind, demonstrate that (i) even when fairness be a 
design goal, developer can easily introduce discrimination in 
software, and (ii) Themis be an effective fairness test tool. 

Themis require a schema for generate inputs, but do not 
require an oracle. Our causal fairness definition be design specif- 
ically to be testable, unlike definition that require probabilistic 
estimation or knowledge of the future [38]. Software test offer 
a unique opportunity to conduct causal experiment to determine 
statistical causality [67]: One can run the software on an input (e.g., 
a defendant’s criminal record), modify a specific input character- 
istic (e.g., the defendant’s race), and observe if that modification 
cause a change in the output. We define software to be causally 

499 

https://github.com/LASER-UMASS/Themis 


Fairness Testing: Testing Software for Discrimination ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany 

fair with respect to input characteristic χ if for all inputs, vary 
the value of χ do not alter the output. For example, a sentence- 
recommendation system be fair with respect to race if there be 
no two individual who differ only in race but for whom the sys- 
tem’s sentence recommendation differs. In addition to capture 
causality, this definition require no oracle— the equivalence of the 
output for the two input be itself the oracle—which help fully 
automate test generation. 

The rest of this paper be structure a follows. Section 2 provide 
an intuition to fairness measure and Section 3 formally defines 
software fairness. Section 4 describes Themis, our approach to 
fairness testing. Section 5 evaluates Themis. Finally, Section 6 
place our work in the context of related research and Section 7 
summarizes our contributions. 

2 SOFTWARE FAIRNESS MEASURES 
Suppose a bank employ loan software to decide if loan applicant 
should be give loans. loan input be each applicant’s name, 
age, race, income, savings, employment status, and request loan 
amount, and the output be a binary “give loan” or “do not give loan”. 
For simplicity, suppose age and race be binary, with age either <40 
or >40, and race either green or purple. 

Some prior work on measure and remove discrimination 
from algorithm [18, 19, 39–42, 88, 89, 91] have focus on what we 
call group discrimination, which say that to be fair with respect to 
an input characteristic, the distribution of output for each group 
should be similar. For example, the loan software be fair with re- 
spect to age if it give loan to the same fraction of applicant 
<40 and >40. To be fair with respect to multiple characteristics, 
for example, age and race, all group with respect to those char- 
acteristics — purple <40, purple >40, green <40, and green >40— 
should have the same outcome fractions. The Calders-Verwer (CV) 
score [19] measure the strength of group discrimination a the 
difference between the large and the small outcome fractions; 
if 30% of people <40 get the loan, and 40% of people >40 get the 
loan, then loan be 40% − 30% = 10% group discriminating. 

While group discrimination be easy to reason about and measure, 
it have two inherent limitations. First, group discrimination may fail 
to observe some discrimination. For example, suppose that loan 
produce different output for two loan application that differ in 
race, but be otherwise identical. While loan clearly discriminates 
with respect to race, the group discrimination score will be 0 if loan 
discriminates in the opposite way for another pair of applications. 
Second, software may circumvent discrimination detection. For 
example, suppose loan recommends loan for a random 30% of the 
purple applicants, and the 30% of the green applicant who have 
the most savings. Then the group discrimination score with respect 
to race will deem loan perfectly fair, despite a clear discrepancy in 
how the application be process base on race. 

To address these issues, we define a new measure of discrimi- 
nation. Software test enables a unique opportunity to conduct 
causal experiment to determine statistical causation [67] between 
input and outputs. For example, it be possible to execute loan on 
two individual identical in every way except race, and verify if 
change the race cause a change in the output. Causal discrimina- 
tion say that to be fair with respect to a set of characteristics, the 

software must produce the same output for every two individual 
who differ only in those characteristics. For example, the loan 
software be fair with respect to age and race if for all pair of indi- 
viduals with identical name, income, savings, employment status, 
and request loan amount but different race or age characteristics, 
loan either give all of them or none of them the loan. The fraction 
of input for which software causally discriminates be a measure of 
causal discrimination. 

Thus far, we have discuss software operating on the full input 
domain, e.g., every possible loan application. However, apply 
software to partial input domain may mask or effect discrimi- 
nation. For example, while software may discriminate on some 
loan applications, a bank may care about whether that software 
discriminates only with respect to application representative of 
their customers, a oppose to all possible human beings. In this 
case, a partial input domain may mask discrimination. If a partial 
input domain exhibit correlation between input characteristics, it 
can effect discrimination. For example, suppose old individual 
have, on average, high income and large savings. If loan only 
considers income and saving in make it decision, even though 
it do not consider age, for this population, loan give loan to 
a high fraction of old individual than young ones. We call 
the measurement of group or causal discrimination on a partial 
input domain apparent discrimination. Apparent discrimination de- 
pends on the operational profile of the system system’s use [7, 58]. 
Apparent discrimination be important to measure. For example, 
Amazon.com, Inc. software that determine where to offer free 
same-day delivery do not explicitly consider race but make race- 
correlate decision because of correlation between race and other 
input characteristic [36, 52]. Despite the algorithm not look 
at race explicitly, Amazon.com, Inc. would have prefer to have 
test for this kind of discrimination. 

3 FORMAL FAIRNESS DEFINITIONS 
We make two simplify assumptions. First, we define software 
a a black box that map input characteristic to an output char- 
acteristic. While software is, in general, more complex, for the 
purpose of fairness testing, without loss of generality, this defini- 
tion be sufficient: All user action and environmental variable be 
model a input characteristics, and each software effect be mod- 
eled a an output characteristic. When software have multiple output 
characteristics, we define fairness with respect to each output char- 
acteristic separately. The definition can be extend to include 
multiple output characteristic without significant conceptual re- 
formulation. Second, we assume that the input characteristic and 
the output characteristic be categorical variables, each have a set 
of possible value (e.g., race, gender, eye color, age ranges, income 
ranges). This assumption simplifies our measure of causality. While 
our definition do not apply directly to non-categorical input and 
output characteristic (such a continuous variables, e.g., int and 
double), they, and our techniques, can be apply to software with 
non-categorical input and output characteristic by use binning 
(e.g., age<40 and age>40). The output domain distance function 
(Definition 3.3) illustrates one way our definition can be extend 
to continuous variables. Future work will extend our discrimination 
measure directly to a broader class of data types. 

500 



ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou 

A characteristic be a categorical variable. An input type be a set 
of characteristics, an input be a valuation of an input type (assign- 
ment of a value to each characteristic), and an output be a single 
characteristic. 

Definition 3.1 (Characteristic). Let L be a set of value labels. A 
characteristic χ over L be a variable that can take on the value in L. 

Definition 3.2 (Input type and input). For all n ∈ N, let L1,L2, . . . , 
Ln be set of value labels. Then an input type X over those value 
label be a sequence of characteristic X = ⟨χ1, χ2, . . . , χn⟩, where 
for all i ≤ n, χi be a characteristic over Li . 

An input of type X be k = ⟨l1 ∈ L1, l2 ∈ L2, . . . , ln ∈ Ln⟩, a 
valuation of an input type. 

We say the size of input k and of input type X be n. 

Discrimination can bemeasured in software thatmakes decisions. 
When the output characteristic be binary (e.g., “give loan” vs. “do not 
give loan”) the significance of the two different output value be clear. 
When output be not binary, identify potential discrimination 
require understand the significance of difference in the output. 
For example, if the software output an order of hotel listing 
(that may be influence by the computer you be using, a be the 
case when software use by Orbitz lead Apple user to higher-priced 
hotel [53]), domain expertise be need to compare two output 
and decide the degree to which their difference be significant. The 
output domain distance function encodes this expertise, mapping 
pair of output value to a distance measure. 

Definition 3.3 (Output domain distance function). Let Lo be a set of 
value labels. Then for all lo1, lo2 ∈ Lo , the output distance function 
be δ : Lo × Lo → [0..1] such that lo1 = lo2 =⇒ δ (lo1, lo2) = 0. 

The output domain distance function generalizes our work be- 
yond binary outputs. For simplicity of exposition, for the remainder 
of this paper, we assume software output binary decisions— a nat- 
ural domain for fairness testing. While true or false output 
(corresponding to decision such a “give loan” vs. “do not give 
loan”) be easy to understand, the output domain distance func- 
tion enables compare non-binary output in two ways. First, a 
threshold output domain distance function can determine when two 
output be dissimilar enough to warrant potential discrimination. 
Second, a relational output domain distance function can describe 
how different two input be and how much they contribute to 
potential discrimination. Definitions 3.5, 3.6, 3.8, and 3.7, could be 
extend to handle non-binary output by change their exact 
output comparison to fractional similarity comparison use an 
output domain distance function, similar to the way input have 
be handle in prior work [24]. 

Definition 3.4 (Decision software). Let n ∈ N be an input size, let 
L1,L2, . . . ,Ln be set of value labels, let X = ⟨χ1, χ2, . . . , χn⟩ be 
an input type, and let K be the set of all possible input of type 
X . Decision software be a function S : K → {true, false}. That is, 
when software S be apply to an input ⟨l1 ∈ L1, l2 ∈ L2, . . . , ln ∈ 
Ln⟩, it produce true or false. 

The group discrimination score varies from 0 to 1 and measure 
the difference between fraction of input group that lead to the 
same output (e.g., the difference between the fraction of green and 

purple individual who be give a loan). This definition be base 
on the CV score [19], which be limited to a binary input type or a 
binary partition of the input space. Our definition extends to 
the more broad categorical input types, reflect the relative com- 
plexity of arbitrary decision software. The group discrimination 
score with respect to a set of input characteristic be the maximum 
frequency with which the software output true minus the mini- 
mum such frequency for the group that only differ in those input 
characteristics. Because the CV score be limited to a single binary 
partitioning, that difference represent all the encode discrimi- 
nation information in that setting. In our more general set 
with multiple non-binary characteristics, the score focus on the 
range—difference between the maximum and minimum—as op- 
pose to the distribution. One could consider measuring, say, the 
standard deviation of the distribution of frequency instead, which 
would good measure deviation from a completely fair algorithm, 
a oppose to the maximal deviation for two extreme groups. 

Definition 3.5 (Univariate group discrimination score d̃). Let K be 
the set of all possible input of size n ∈ N of type X = ⟨χ1, χ2, . . . , 
χn⟩ over label value L1,L2, . . . ,Ln . Let software S : K → {true, 
false}. 

For all i ≤ n, fix one characteristic χi . That is, letm = |Li | and 
for all m̂ ≤ m, let Km̂ be the set of all input with χi = lm̂ . (Km̂ be 
the set of all input with the χi th characteristic fix to be lm̂ .) Let 
pm̂ be the fraction of input k ∈ Km̂ such that S(k ) = true. And 
let P = ⟨p1,p2, . . . ,pm⟩. 

Then the univariate group discrimination score with respect to 
χi , denote d̃χi (S), be max(P ) −min(P ). 

For example, consider loan software that decide to give loan 
to 23% of green individuals, and to 65% of purple individuals. 
When compute loan’s group discrimination score with respect 
to race, d̃race (loan) = 0.65 − 0.23 = 0.42. 

The multivariate group discrimination score generalizes the uni- 
variate version to multiple input characteristics. 

Definition 3.6 (Multivariate group discrimination score d̃). For all 
α , β , . . . ,γ ≤ n, fix the characteristic χα , χβ , . . . , χγ . That is, let 
mα = |Lα |, mβ = |Lβ |, . . ., mγ = |Lγ |, let m̂α ≤ mα , m̂β ≤ mβ , 
. . . , m̂γ ≤ mγ , andm = mα ×mβ × · · · ×mγ , let Km̂α ,m̂β , ...,m̂γ 
be the set of all input with χα = lm̂α , χβ = lm̂β , . . ., χγ = lm̂γ . 
(Km̂α ,m̂β , ...,m̂γ be the set of all input with the χα characteristic 
fix to be lm̂α , χβ characteristic fix to be lm̂β , and so on.) Let 
pm̂α ,m̂β , ...,m̂γ be the fraction of input k ∈ Km̂α ,m̂β , ...,m̂γ such 
that S(k ) = true. And let P be an unordered sequence of all 
pm̂α ,m̂β , ...,m̂γ . 

Then the multivariate group discrimination score with respect 
to χα , χβ , . . . , χγ , denote d̃χα , χβ , ..., χγ (S) be max(P ) −min(P ). 

Our causal discrimination score be a strong measure of discrimi- 
nation, a it seek out causality in software, measure the fraction 
of input for which change specific input characteristic cause 
the output to change [67]. The causal discrimination score identi- 
fies change which characteristic directly affect the output. As a 
result, for example, while the group and apparent discrimination 
score penalize software that give loan to different fraction of 

501 



Fairness Testing: Testing Software for Discrimination ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany 

individual of different races, the causal discrimination score penal- 
izes software that give loan to individual of one race but not to 
otherwise identical individual of another race. 

Definition 3.7 (Multivariate causal discrimination score d⃗). Let 
K be the set of all possible input of size n ∈ N of type X = 
⟨χ1, χ2, . . . , χn⟩ over label value L1,L2, . . . ,Ln . Let software S : 
K → {true, false}. For all α , β, . . . ,γ ≤ n, let χα , χβ , . . . , χγ be 
input characteristics. 

Then the causal discrimination score with respect to χα , χβ , . . . , 
χγ , denote d⃗χα , χβ , ..., χγ (S) be the fraction of input k ∈ K such 
that there exists an input k ′ ∈ K such that k and k ′ differ only in 
the input characteristic χα , χβ , . . . , χγ , and S(k ) , S(k ′). That 
is, the causal discrimination score with respect to χα , χβ , . . . , χγ 
be the fraction of input for which change at least one of those 
characteristic cause the output to change. 

Thus far, we have measure discrimination of the full input do- 
main, consider every possible input with every value of every 
characteristic. In practice, input domain may be partial. A com- 
pany may, for example, care about whether software discriminates 
only with respect to their customer (recall Section 2). Apparent 
discrimination capture this notion, apply group or causal dis- 
crimination score measurement to a subset of the input domain, 
which can be described by an operational profile [7, 58]. 

Definition 3.8 (Multivariate apparent discrimination score). Let 
K̈ ⊆ K be a subset of the input domain to S. Then the apparent 
group discrimination score be the group discrimination score apply 
to K̈ , and the apparent causal discrimination score be the causal dis- 
crimination score apply to K̈ (as oppose to apply to the full K ). 

Having define discrimination, we now define the problem of 
check software for discrimination. 

Definition 3.9 (Discrimination check problem). Given an input 
type X , decision software S with input type X , and a threshold 
0 ≤ θ ≤ 1, compute all X ′ ⊆ X such that d̃X ′ (S) ≥ θ or d⃗X ′ (S) ≥ θ . 

4 THE THEMIS SOLUTION 
This section describes Themis, our approach to efficient fairness 
testing. To use Themis, the user provide a software executable, a 
desire confidence level, an acceptable error bound, and an input 
schema describe the format of valid inputs. Themis can then be 
use in three ways: 
(1) Themis generates a test suite to compute the software group or 

causal discrimination score for a particular set of characteristics. 
For example, one can use Themis to check if, and how much, a 
software system discriminates against race and age. 

(2) Given a discrimination threshold, Themis generates a test suite 
to compute all set of characteristic against which a software 
group or causally discriminates more than that threshold. 

(3) Given a manually-written or automatically-generated test suite, 
or an operational profile describe an input distribution [7, 58], 
Themis computes the apparent group or causal discrimination 
score for a particular set of characteristics. For example, one 
can use Themis to check if a system discriminates against race 
on a specific population of input representative of the way the 

Algorithm 1: Computing group discrimination. Given a soft- 
ware S and a subset of it input characteristic X ′, GroupDis- 
crimination return d̃X ′ (S), the group discrimination score 
with respect to X ′, with confidence conf and error margin ϵ . 
GroupDiscrimination(S, X ′, conf , ϵ ) 

1 minGroup ← ∞, maxGroup ← 0, testSuite ← ∅ ▷Initialization 
2 foreach A, where A be a value assignment for X ′ do 
3 r ← 0 ▷Initialize number of sample 
4 count ← 0 ▷Initialize number of positive output 
5 while r < max_samples do 
6 r ← r + 1 
7 k ← NewRandomInput (X ′ ← A) ▷New input k with 

k .X ′ = A 
8 testSuite ← testSuite ∪ {k } ▷Add input to the test suite 
9 if notCached (k ) then ▷No cached execution of k exist 

10 Compute(S(k )) ▷Evaluate software on input k 
11 CacheResult (k, S(k )) ▷Cache the result 

12 else ▷Retrieve cached result 
13 S(k ) ← RetrieveCached (k ) 
14 if S(k ) then 
15 count ← count + 1 
16 if r > sampling_threshold then 

▷After sufficient samples, check error margin 
17 p ← countr ▷Current proportion of positive output 

18 if conf .zValue 
√ 
p (1−p ) 

r < ϵ then 
19 break ▷Achieved error < ϵ , with confidence conf 

20 maxGroup ← max(maxGroup, p ) 
21 minGroup ← min(minGroup, p ) 
22 return testSuite, d̃X ′ (S) ← maxGroup −minGroup 

system will be used. This method do not compute the score’s 
confidence a it be only a strong a the developers’ confidence 
that test suite or operational profile be representative of real- 
world executions. 

Measuring group and causal discrimination exactly require ex- 
haustive testing, which be infeasible for nontrivial software. Solving 
the discrimination check problem (Definition 3.9) further re- 
quire measure discrimination over all possible subset of charac- 
teristics to find those that exceed a certain discrimination threshold. 

Themis address these challenge by employ three optimiza- 
tions: (1) test caching, (2) adaptive, confidence-driven sampling, and 
(3) sound pruning. All three technique reduce the number of test 
case need to compute both group and causal discrimination. Sec- 
tion 4.1 describes how Themis employ cache and sampling, and 
Section 4.2 describes how Themis prune the test suite search space. 

4.1 Caching and Approximation 
GroupDiscrimination (Algorithm 1) and CausalDiscrimination 
(Algorithm 2) present the Themis computation of multivariate 
group and causal discrimination score with respect to a set of 
characteristics. These algorithm implement Definitions 3.6 and 3.7, 
respectively, and rely on two optimizations. We first describe these 
optimization and then the algorithms. 

502 



ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou 

Test caching. Precisely compute the group and causal discrimi- 
nation score require execute a large set of tests. However, a lot 
of this computation be repetitive: test relevant to group discrimina- 
tion be also relevant to causal discrimination, and test relevant to 
one set of characteristic can also be relevant to another set. This 
redundancy in fairness test allows Themis to exploit cache to 
reuse test result without re-executing tests. Test cache have low 
storage overhead and offer significant runtime gains. 
Adaptive, confidence-driven sampling. Since exhaustive test- 
ing be infeasible, Themis computes approximate group and causal 
discrimination score through sampling. Sampling in Themis be 
adaptive, use the ongoing score computation to determine if a 
specify margin of error ϵ with a desire confidence level conf have 
be reached. Themis generates input uniformly at random use 
an input schema, and maintains the proportion of sample (p) for 
which the software output true (in GroupDiscrimination) or for 
which the software change it output (in CausalDiscrimination). 
The margin of error for p be then compute as: 

error = z∗ 
√ 

p (1 − p) 
r 

where r be the number of sample so far and z∗ be the normal distri- 
bution z∗ score for the desire confidence level. Themis return if 
error < ϵ , or generates another test otherwise. 
GroupDiscrimination (Algorithm 1) measure the group discrim- 
ination score with respect to a subset of it input characteristic X ′. 
As per Definition 3.6, GroupDiscrimination fix X ′ to particular 
value (line 2) to compute what portion of all test with X ′ value 
fix produce a true output. The while loop (line 5) generates 
random input assignment for the remain input characteristic 
(line 7), store them in the test suite, and measure the count of 
positive outputs. The algorithm executes the test, if that execution 
be not already cached (line 9); otherwise, the algorithm retrieves 
the software output from the cache (line 12). After passing the 
minimum sample threshold (line 16), it check if ϵ error margin 
be achieve with the desire confidence (line 18). If it is, GroupDis- 
crimination terminates the computation for the current group 
and update the max and min value (lines 20–21). 
CausalDiscrimination (Algorithm 2) similarly applies test cache 
and adaptive sampling. It take a random test k0 (line 4) and test 
if change any of it X ′ characteristic change the output. If k0 
result be not cached (line 6), the algorithm executes it and cache the 
result. It then iterates through test k that differ from k0 in one or 
more characteristic in X ′ (line 11). All generate input be store 
in the test suite. The algorithm typically only need to examine a 
small number of test before discover causal discrimination for 
the particular input (line 18). In the end, CausalDiscrimination 
return the proportion of test for which the algorithm found causal 
discrimination (line 25). 

4.2 Sound Pruning 
Measuring software discrimination (Definition 3.9) involves execut- 
ing GroupDiscrimination and CausalDiscrimination over each 
subset of the input characteristics. The number of these execution 
grows exponentially with the number of characteristics. Themis re- 
lie on a powerful prune optimization to dramatically reduce the 

Algorithm 2: Computing causal discrimination. Given a soft- 
ware S and a subset of it input characteristic X ′, CausalDis- 
crimination return d⃗X ′ (S), the causal discrimination score 
with respect to X ′, with confidence conf and error margin ϵ . 
CausalDiscrimination(S, X ′, conf , ϵ ) 

1 count ← 0; r ← 0, testSuite ← ∅ ▷Initialization 
2 while r < max_samples do 
3 r ← r + 1 
4 k0 ← NewRandomInput ▷New input without value restriction 
5 testSuite ← testSuite ∪ {k0 } ▷Add input to the test suite 
6 if notCached (k0) then ▷No cached execution of k0 exist 
7 Compute(S(k0)) ▷Evaluate software on input k0 
8 CacheResult (k0, S(k0)) ▷Cache the result 

9 else ▷Retrieve cached result 
10 S(k0) ← RetrieveCached (k0) 
11 foreach k ∈ {k | k , k0; ∀χ < X ′, k .χ = k0 .χ } do 

▷All input that match k0 in every characteristic χ < X ′ 

testSuite ← testSuite ∪ {k } ▷Add input to the test suite 
1313 if notCached (k ) then ▷No cached execution of k exist 
14 Compute(S(k )) ▷Evaluate software on input k 
15 CacheResult (k, S(k )) ▷Cache the result 

16 else ▷Retrieve cached result 
17 S(k ) ← RetrieveCached (k ) 
18 if S(k ) , S(k0) then ▷Causal discrimination 
19 count = count + 1 
20 break 

21 if r > sampling_threshold then 
▷Once we have sufficient samples, check error margin 

22 p ← countr ▷Current proportion of positive output 

23 if conf .zValue 
√ 
p (1−p ) 

r < ϵ then 
24 break ▷Achieved error < ϵ , with confidence conf 

25 return testSuite, d⃗X ′ (S) ← p 

number of evaluate characteristic subsets. Pruning be base on a 
fundamental monotonicity property of group and causal discrimina- 
tion: if a software S discriminates over threshold θ with respect to 
a set of characteristic X ′, then S also discriminates over threshold 
θ with respect to all superset of X ′. Once Themis discovers that S 
discriminates against X ′, it can prune test all supersets of X ′. 

Next, we formally prove group (Theorem 4.1) and causal (The- 
orem 4.2) discrimination monotonicity. These result guarantee 
that Themis prune strategy be sound. DiscriminationSearch 
(Algorithm 3) us prune in solve the discrimination check 
problem. As Section 5.4 will evaluate empirically, prune lead 
to, on average, a two-to-three order of magnitude reduction in test 
suite size. 

Theorem 4.1 (Group discrimination monotonicity). LetX be 
an input type and let S be a decision software with input typeX . Then 
for all set of characteristic X ′,X ′′ ⊆ X , X ′′ ⊇ X ′ =⇒ d̃X ′′ (S) ≥ 
d̃X ′ (S). 

Proof. Let d̃X ′ (S) = θ ′. Recall (Definition 3.6) that to compute 
d̃X ′ (S), we partition the space of all input into equivalence class 

503 



Fairness Testing: Testing Software for Discrimination ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany 

Algorithm 3: Discrimination search. Given a software S with 
input type X and a discrimination threshold θ , Discrimina- 
tionSearch identifies all minimal subset of characteristic 
X ′ ⊆ X such that the (group or causal) discrimination score 
of S with respect to X ′ be great than θ with confidence conf 
and error margin ϵ . 
DiscriminationSearch(S, θ , conf , ϵ ) 

1 D← ∅ ▷Initialize discriminate subset of characteristic 
2 for i = 1 . . . |X | do 
3 foreach X ′ ⊆ X , |X ′ | = i do ▷Check subset of size i 
4 discriminates ← false 
5 for X ′′ ∈ D do 
6 if X ′′ ⊆ X ′ then 

▷Supersets of a discriminate set discriminate 
(Theorems 4.1 and 4.2) 

7 discriminates ← true 
8 break 

9 if discriminates then 
10 continue ▷Do not store X ′; D already contains a subset 

11 {testSuite, d } = Discrimination(S, X ′, conf , ϵ ) 
▷Test S for (group or causal) discrimination with respect to X ′ 

12 if d > θ then ▷X ′ be a minimal discriminate set 
13 D.append (X ′) 

such that all element in each equivalence class have identical 
value label assign to each characteristic in X ′, then, compute 
the frequency with which input in each equivalence class lead 
S to a true output, and finally compute the difference between 
the minimum and maximum of these frequencies. Let p̂′ and p̌′ be 
those maximum and minimum frequencies, and K̂ ′ and Ǩ ′ be the 
correspond equivalence class of inputs. 

Now consider the computation of θ ′′ = d̃X ′′ (S). Note that the 
equivalence class of input for this computation will be strict 
subset of the equivalence class in the θ ′ computation. In par- 
ticular, the equivalence subset K̂ ′ will be split into several equiv- 
alence classes, which we call K̂ ′′1 , K̂ 

′′ 
2 , . . . There be two possibil- 

ities: (1) either the frequency with which the input in each of 
these subclass lead S to a true output equal the frequency of 
K̂ ′, or (2) some subclass have low frequency and some have 
high than K̂ ′ (since when combined, they must equal that of 
K̂ ′). Either way, the maximum frequency of the K̂ ′′1 , K̂ 

′′ 
2 , . . . , K̂ 

′′ 
j 

subclass be ≥ K̂ ′. And therefore, the maximum overall frequency 
p̂′′ for all the equivalence class in the computation of θ ′′ be 
≥ p̂′. By the same argument, the minimum overall frequency 
p̌′′ for all the equivalence class in the computation of θ ′′ be 
≤ p̌′. Therefore, θ ′′ = (p̂′′ − p̌′′) ≥ (p̂′ − p̌′) ≤= θ ′, and there- 
fore, X ′′ ⊇ X ′ =⇒ d̃X ′′ (S) ≥ d̃X ′ (S). □ 

Theorem 4.2 (Causal discrimination monotonicity). Let X 
be an input type and let S be a decision software with input type 
X . Then for all set of characteristic X ′,X ′′ ⊆ X , X ′′ ⊇ X ′ =⇒ 
d⃗X ′′ (S) ≥ d⃗X ′ (S). 

Proof. Recall (Definition 3.7) that the causal discrimination 
score with respect toX ′ be the fraction of input for which change 

the value of at least one characteristic in X ′ change the output. 
Consider K ′, the entire set of such input for X ′, and similarly K ′′, 
the entire set of such input for X ′′. Since X ′′ ⊇ X ′, every input in 
K ′must also be inK ′′ because if change at least one characteristic 
in X ′ change the output and those characteristic be also in X ′′. 
Therefore, the fraction of such input must be no small for X ′′ 

than for X ′, and therefore, X ′′ ⊇ X ′ =⇒ d⃗X ′′ (S) ≥ d⃗X ′ (S). □ 

A further opportunity for prune come from the relationship 
between group and causal discrimination. As Theorem 4.3 shows, 
if software group discriminates against a set of characteristics, it 
must causally discriminate against that set at least a much. 

Theorem 4.3. Let X be an input type and let S be a decision 
software with input typeX . Then for all set of characteristicsX ′ ⊆ X , 
d̃X ′ (S) ≤ d⃗X ′ (S). 

Proof. Let d̃X ′ (S) = θ . Recall (Definition 3.6) that to compute 
d̃X ′ (S), we partition the space of all input into equivalence class 
such that all element in each equivalence class have identical value 
label assign to each characteristic in X ′. It be evident that same 
equivalence class input have same value for characteristic in X ′ 
and the one in different equivalence class differ in at least one 
of the characteristic in X ′. 

Now, d̃X ′ (S) = θ mean that for θ fraction of inputs, the output 
be true, and after change just some value of X ′ (producing an 
input in another equivalence class), the output be false. This be 
because if there be θ ′ < θ fraction of input with a different 
output when change the equivalence classes, then d̃X ′ (S) would 
have be θ ′. Hence d⃗X ′ (S) > θ . □ 

5 EVALUATION 
In evaluate Themis, we focus on two research questions: 
RQ1: Does research on discrimination-aware algorithm design 

(e.g., [18, 40, 88, 91]) produce fair algorithms? 
RQ2: How effective do the optimization from Section 4 make 

Themis at identify discrimination in software? 
To answer these research questions, we carry out three exper- 

iments on twenty instance of eight software system that make 
financial decisions.1 Seven of the eight system (seventeen out of 
the twenty instances) be write by original system developers; we 
reimplemented one of the system (three instances) because origi- 
nal source code be not available. Eight of these software instance 
use standard machine learn algorithm to infer model from 
datasets of financial and demographic data. These system make 
no attempt to avoid discrimination. The other twelve instance 
be take from related work on devise discrimination-aware al- 
gorithms [18, 40, 88, 91]. These software instance use the same 
datasets and attempt to infer discrimination-free solutions. Four of 
them focus on not discriminate against race, and eight against 
gender. Section 5.1 describes our subject system and the two 
datasets they use. 

1We use the term system instance to mean instantiation of a software system with 
a configuration, use a specific dataset. Two instance of the same system use 
different configuration and different data be likely to differ significantly in their 
behavior and in their discrimination profile. 

504 



ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou 

5.1 Subject Software Systems 
Our twenty subject instance use two financial datasets. The Adult 
dataset (also know a the Census Income dataset)2 contains fi- 
nancial and demographic data for 45K individuals; each individ- 
ual be described by 14 attributes, such a occupation, number of 
work hours, capital gain and losses, education level, gender, race, 
marital status, age, country of birth, income, etc. This dataset be 
well vetted: it have be use by others to devise discrimination- 
free algorithm [18, 88, 89], a well a for non-discrimination pur- 
pose [3, 43, 86]. The Statlog German Credit dataset3 contains 
credit data for 1,000 individuals, classify each individual a hav- 
ing “good” or “bad” credit, and include 20 other piece of data 
for each individual, such a gender, housing arrangement, credit 
history, year employed, credit amount, etc. This dataset be also well 
vetted: it have be use by others to devise discrimination-free algo- 
rithms [18, 89], a well a for non-discrimination purpose [25, 31]. 

We use a three-parameter name scheme to refer to our software 
instances. An example of an instance name be A censusrace . The “A” 
refers to the system use to generate the instance (described next). 
The “census” refers to the dataset use for the system instance. This 
value can be “census” for the Adult Census Income dataset or “credit” 
for the Statlog German Credit dataset. Finally, the “race” refers to a 
characteristic the software instance attempt to not discriminate 
against. In our evaluation, this value can be “race” or “gender” for 
the census dataset and can only be “gender” for the credit dataset.4 
Some of the system make no attempt to avoid discrimination and 
their name leave this part of the label blank. 

Prior research have attempt to build discrimination-free sys- 
tems use these two datasets [18, 40, 88, 91]. We contact the 
author and obtain the source code for three of these four sys- 
tems, A [88], C [18], and D [91], and reimplemented the other, 
B [40], ourselves. We verify that our reimplementation pro- 
duced result consistent with the the evaluation of the original 
system [40]. We additionally use standard machine learn li- 
braries a four more discrimination-unaware software system E , 
F , G , and H , on these datasets. We use scikit-learn [68] for E 
(naive Bayes), G (logistic regression), and H (support vector ma- 
chines), and a publicly available decision tree implementation [90] 
for F and for our reimplementation of B . 

The four discrimination-free software system use differentmeth- 
od to attempt to reduce or eliminate discrimination: 

A be a modify logistic regression approach that constrains the 
regression’s loss function with the covariance of the characteristics’ 
distribution that the algorithm be ask to be fair with respect 
to [88]. 

B be a modify decision tree approach that constrains the split- 
ting criterion by the output characteristic (as the standard decision 
tree approach does) and also the characteristic that the algorithm 
be ask to be fair with respect to [40]. 

C manipulates the training dataset for a naive Bayes classi- 
fier. The approach balance the dataset to equate the number of 

2https://archive.ics.uci.edu/ml/datasets/Adult 
3https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data) 
4The credit dataset combine marital status and gender into a single characteristic; we 
refer to it simply a gender in this paper for exposition. The credit dataset do not 
include race. 

input that lead to each output value, and then tweak the dataset 
by introduce noise of flip output for some inputs, and by 
introduce weight for each datapoint [18]. 

D be amodified decision tree approach that balance the training 
dataset to equate the number of input that lead to each output 
value, remove and repeat some inputs, and flip output value of 
input close to the decision tree’s decision boundaries, introduce 
noise around the critical boundary [91]. 
Configuring the subject systems. The income characteristic of 
the census dataset be binary, represent the income be above 
or below $50,000. Most prior research developed system that use 
the other characteristic to predict the income; we do the same. 
For the credit dataset, the system predict if the individual’s credit 
be “good” or “bad”. We train each system, separately on the cen- 
sus and credit datasets. Thus, for example, the G credit instance be 
the logistic-regression-based system train on the credit dataset. 
For the census dataset, we randomly sample 15.5K individual to 
balance the number who make more than and less than $50,000, 
and train each system on the sample subset use 13 character- 
istics to classify each individual a either have above or below 
$50,000 income. For the credit dataset, we similarly randomly sam- 
plead 600 individual to balance the number with “good” and “bad” 
credit, and train each system on the sample subset use the 20 
characteristic to classify each individual’s credit a “good” or “bad”. 

Each discrimination-aware system can be train to avoid dis- 
crimination against sensitive characteristics. In accord with the 
prior work on building these system [18, 40, 88, 91], we chose gen- 
der and race a sensitive characteristics. Using all configuration 
exactly a described in the prior work, we create 3 instance of 
each discrimination-aware system. For example, for system A , we 
have A censusgender and A 

census 
race , two instance train on the census 

data to avoid discrimination on gender and race, respectively, and 
A creditgender, an instance train on the credit data to avoid discrimina- 
tion on gender. The left column of Figure 1 list the twenty system 
instance we use a subjects. 

5.2 Race and Gender Discrimination 
We use Themis to measure the group and causal discrimination 
score for our twenty software instance with respect to race and, 
separately, with respect to gender. Figure 1 present the results. We 
make the follow observations: 
• Themis be effective. Themis be able to (1) verify that many 

of the software instance do not discriminate against race and 
gender, and (2) identify the software that does. 

• Discrimination be present even in instance design to 
avoid discrimination. For example, a discrimination-aware 
decision tree approach train not to discriminate against gen- 
der, B censusgender, have a causal discrimination score over 11%: more 
than 11% of the individual have the output flip just by alter- 
ing the individual’s gender. 

• The causal discrimination score detect critical evidence 
of discrimination miss by the group score. Often, the 
group and causal discrimination score conveyed the same in- 
formation, but there be case in which Themis detect causal 
discrimination even though the group discrimination score be 

505 

https://archive.ics.uci.edu/ml/datasets/Adult 
https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data) 


Fairness Testing: Testing Software for Discrimination ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany 

System Race Gender 
Instance group causal group causal 

A creditgender — 3.78% 3.98% 
A censusgender 2.10% 2.25% 3.80% 3.80% 
A censusrace 2.10% 1.13% 8.90% 7.20% 
B creditgender — 0.80% 2.30% 
B censusgender 36.55% 38.40% 0.52% 11.27% 
B censusrace 2.28% 1.78% 5.84% 5.80% 
C creditgender — 0.35% 0.18% 
C censusgender 2.43% 2.91% < 0.01% < 0.01% 
C censusrace 0.08% 0.08% 35.20% 34.50% 
D creditgender — 0.23% 0.29% 
D censusgender 0.21% 0.26% < 0.01% < 0.01% 
D censusrace 0.12% 0.13% 4.64% 4.94% 
E credit — 0.32% 0.37% 
E census 0.74% 0.85% 0.26% 0.32% 
F credit — 0.05% 0.06% 
F census 0.11% 0.05% < 0.01% < 0.01% 
G credit — 3.94% 2.41% 
G census 0.02% 2.80% < 0.01% < 0.01% 
H credit — < 0.01% < 0.01% 
H census < 0.01% 0.01% < 0.01% < 0.01% 

Figure 1: The group and causal discrimination score with respect 
to race and gender. Some number be miss because the credit 
dataset do not contain information on race. 

low. For example, for B censusgender, the causal score be more than 
21× high than the group score (11.27% vs. 0.52%). 

• Today’s discrimination-aware approach be insufficient. 
The B approach be design to avoid a variant of group dis- 
crimination (as be other discrimination-aware approaches), but 
this design is, at least in some conditions, insufficient to prevent 
causal discrimination. Further, focus on avoid discrim- 
inating against one characteristic may create discrimination 
against another, e.g., B censusgender limit discrimination against gen- 
der but discriminates against race with a causal score of 38.40%. 

• There be no clear evidence that discrimination-awaremeth- 
od outperform discrimination-unaware ones. In fact, the 
discrimination-unaware approach typically discriminate less 
than their discrimination-aware counterparts, with the excep- 
tion of logistic regression. 

5.3 Computing Discriminated-Against 
Characteristics 

To evaluate how effective Themis be at compute the discrimination 
check problem (Definition 3.9), we use Themis to compute the 
set of characteristic each of the twenty software instance discrim- 
inates against causally. For each instance, we first use a threshold 
of 75% to find all subset of characteristic against which the in- 
stance discriminated. We next examine the discrimination with 

A censusrace d⃗ {д,r } = 13.7% C censusgender d⃗ {m } = 35.2% 

B censusgender d⃗ {д,m,r } = 77.2% D 
census 
gender d⃗ {m } = 12.9% 

d⃗ {д,r } = 52.5% d⃗ {c } = 7.6% 
d⃗ {д } = 11.2% D censusrace d⃗ {m } = 16.2% 
d⃗ {m } = 36.1% d⃗ {m,r } = 52.3% 
d⃗ {r } = 36.6% E census d⃗ {m } = 7.9% 

B censusrace d⃗ {д } = 5.8% F census d⃗ {c,r } = 98.1% 
C creditgender d⃗ {a } = 7.6% d⃗ {r,e } = 76.3% 

C censusrace d⃗ {a } = 25.9% G census d⃗ {e } = 14.8% 
d⃗ {д } = 35.2% 

d⃗ {д,r } = 41.5% 

Figure 2: Sets of sensitive characteristic that the subject instance 
discriminate against causally at least 5% and that contribute to sub- 
set of characteristic that be discriminate against at least 75%. 
We abbreviate sensitive characteristic as: (a)ge, (c)ountry, (g)ender, 
(m)arital status, (r)ace, and r(e)lation. 

respect to each of the characterists in those set individually. Finally, 
we checked the causal discrimination score for pair of those char- 
acteristics that be sensitive, a define by prior work [18, 40, 88, 91] 
(e.g., race, age, marital status, etc.). For example, if Themis found 
that an instance discriminate causally against {capital gains, race, 
marital status}, we checked the causal discrimination score for 
{capital gains}, {race}, {marital status}, and then {race, marital sta- 
tus}. Figure 2 report which sensitive characteristic each instance 
discriminates against by at least 5%. 

Themis be able to discover significant discrimination. For ex- 
ample, B censusgender discriminates against gender, marital status, and 
race with a causal score of 77.2%. That mean for 77.2% of the indi- 
viduals, change only the gender, marital status, or race cause the 
output of the algorithm to flip. Even worse, F census discriminates 
against country and race with a causal score of 98.1%. 

It be possible to build an algorithm that appear to be fair with 
respect to a characteristic in general, but discriminates heavily 
against that characteristic when the input space be partition by 
another characteristic. For example, an algorithm may give the 
same fraction of white and black individual loans, but discriminate 
against black Canadian individual a compare to white Canadian 
individuals. This be the case with B censusgender, for example, a it 
causal discrimination score against gender be 11.2%, but against 
gender, marital status, and race be 77.2%. Prior work on fairness have 
not consider this phenomenon, and these finding suggest that 
the software design to produce fair result sometimes achieve 
fairness at the global scale by create severe discrimination for 
certain group of inputs. 

This experiment demonstrates that Themis effectively discov- 
er discrimination and can test software for unexpected software 
discrimination effect across a wide variety of input partitions. 

5.4 Themis Efficiency and the Pruning Effect 
Themis us prune to minimize test suite (Section 4.2). We eval- 
uated the efficiency improvement due to prune by compare 
the number of test case need to achieve the same confidence 

506 



ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou 

Group Causal 

A creditgender 
934,230 
167,420 = 5.6× 

29,623 
15,764 = 1.9× 

A censusrace 
7,033,150 

672 = 10, 466 × 
215,000 

457 = 470 × 
B creditgender 

934,230 
3,413 = 274 × 

29,623 
1,636 = 18 × 

B censusrace 
6,230,000 

80,100 = 78 × 
20,500 
6,300 = 33 × 

C creditgender 
934,230 

113 = 8, 268 × 
29,623 

72 = 411 × 
C censusrace 

7,730,120 
75,140 = 103 × 

235,625 
6,600 = 36 × 

D creditgender 
934,230 

720 = 1, 298 × 
29,623 

472 = 63 × 
D censusrace 

7,730,120 
6,600,462 = 1.2× 

235,625 
200,528 = 1.2× 

E credit 934,230145 = 6, 443 × 
29,623 

82 = 361 × 
E census 7,730,12084,040 = 92 × 

235,625 
7,900 = 30 × 

F credit 934,2303,410 = 274 × 
29,623 
2,647 = 11 × 

F census 6,123,000461 = 13, 282 × 
205,000 

279 = 735 × 
G credit 934,230187 = 4, 996 × 

29,623 
152 = 195 × 

G census 7,730,1205,160,125 = 1.5× 
235,625 
190,725 = 1.2× 

H credit 934,230412,020 = 2.3× 
29,623 
10,140 = 2.9× 

H census 1,530,0001,213,500 = 1.3× 
510,000 
324,582 = 1.6× 

arithmetic mean 2, 849 × 148 × 
geometric mean 151 × 26.4× 

Figure 3: Pruning greatly reduces the number of test need to 
compute both group and causal discrimination. We present here 
the computation that be need for the experiment of Figure 2: find- 
ing all subset of characteristic for which the software instance 
discriminate with a score of at least 75%, for a 99% confidence and 
error margin 0.05. For each technique, we show the number of test 
neededwithout prune divide by the number of test neededwith 
pruning, and the result factor reduction in the number of tests. 
For example, reduce the number of test need to compute the 
group discrimination score from 7,033,150 to 672 (2nd row) be an im- 
provement of a factor of 10,466. 

and error bound with and without pruning. Figure 3 show the 
number of test case need for each of the twenty software in- 
stance to achieve a confidence level of 99% and 0.05 error bound, 
with and without pruning. Pruning reduces the number of test 
case by, on average, a factor of 2, 849 for group and 148 for causal 
discrimination. 

The more a system discriminates, the more effective prune 
is, make Themis more efficient because prune happens when 
small set of characteristic discriminate above the chosen threshold. 
Such set enable prune away large supersets of characteristics. 
Theorem 5.1 formalizes this statement. 

Theorem 5.1 (Pruning monotonicity). Let X be an input type 
and S and S′ be decision software with input typesX . If for allX ′ ⊆ X , 
d⃗X ′ (S) ≥ d⃗X ′ (S′) (respectively, d̃X ′ (S) ≥ d̃X ′ (S′)), then for allX ′′ ⊆ 
X , if Themis can pruneX ′′when computingDiscriminationSearch(S′, 
θ , conf , ϵ), then it can also prune X ′′ when compute Discrimina- 
tionSearch(S,θ , conf , ϵ). 

Proof. For Themis to prune X ′′ when compute Discrimina- 
tionSearch(S′, θ , conf , ϵ), there must exist a set X̂ ′′ ⊊ X ′′ such 
that d⃗X̂ ′′ (S 

′) ≥ θ . Since d⃗X̂ ′′ (S) ≥ d⃗X̂ ′′ (S 
′) ≥ θ , when compute 

DiscriminationSearch(S, θ , conf ), Themis can also prune X ′′. 
The same argument hold for group discrimination d̃ . □ 

We measure this effect by measure prune while decrease 
the discrimination threshold θ ; decrease θ effectively simulates in- 
crease system discrimination. We verify that prune increase 
when θ decrease (or equivalently, when discrimination increased). 
For example, Themis need 3,413 test to find set of characteris- 
tic that B creditgender discriminate with a score of more than θ = 0.7, 
but only 10 test when we reduce θ to 0.6. Similarly, the number 
of test for F credit drop from 920 to 10 when lower θ from 
0.6 to 0.5. This confirms that Themis be more efficient when the ben- 
efits of fairness test increase because the software discriminates 
more. 

5.5 Discussion 
In answer our two research questions, we found that (1) State- 
of-the-art approach for design fair system often miss dis- 
crimination and Themis can detect such discrimination via fairness 
testing. (2) Themis be effective at find both group and causal 
discrimination. While we do not evaluate this directly, Themis 
can also measure apparent discrimination (Definition 3.8) via a 
developer-provided test suite or operational profile. (3) Themis 
employ provably sound prune to reduce test suite size and be- 
come more effective for system that discriminate more. Overall, 
prune reduce test suite sizes, on average, two to three order of 
magnitude. 

6 RELATEDWORK 
Software discrimination be a grow concern. Discrimination 
show up in many software applications, e.g., advertisement [75], 
hotel booking [53], and image search [45]. Yet software be enter 
domain in which discrimination could result in serious negative 
consequences, include criminal justice [5, 28], finance [62], and 
hire [71]. Software discrimination may occur unintentionally, 
e.g., a a result of implementation bugs, a an unintended property 
of self-organizing system [11, 13, 15, 16], a an emergent property 
of component interaction [12, 14, 17, 49], or a an automatically 
learn property from bias data [18, 19, 39–42, 88, 89, 91]. 

Some prior work on measure fairness in machine learn 
classifier have focus on the Calders-Verwer (CV) score [19] to 
measure discrimination [18, 19, 39–42, 87–89, 91]. Our group dis- 
crimination score generalizes the CV score to the software domain 
with more complex inputs. Our causal discrimination score go 
beyond prior work by measure causality [67]. An alternate defi- 
nition of discrimination be that a “better” input be never deprive of 
the “better” output [24]. That definition require a domain expert 
to create a distance function for compare inputs; by contrast, our 
definition be simpler, more generally applicable, and amenable 
to optimization techniques, such a pruning. Reducing discrimi- 
nation (CV score) in classifier [18, 40, 88, 91], a our evaluation 
have shown, often fails to remove causal discrimination and discrim- 
ination against certain groups. By contrast, our work do not 
attempt to remove discrimination but offer developer a tool to 
identify and measure discrimination, a critical first step in remove 
it. Problem-specific discrimination measures, e.g., the contextual 

507 



Fairness Testing: Testing Software for Discrimination ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany 

bandit problem, have demonstrate that fairness may result in 
otherwise suboptimal behavior [38]. By contrast, our work be gen- 
eral and we believe that strive for fairness may be a principal 
requirement in a system’s design. 

Counterfactual fairness require output probability distribution 
to match for input population that differ only in the label value of a 
sensitive input characteristic [51]. Counterfactual fairness be related 
to causal fairness but can miss some instance of discrimination, 
e.g., if loan show preferential treatment for some purple inputs, 
but at the same time against some other similar purple inputs. 

FairTest, an implementation of the unwarranted association 
framework [79] us manually write test to measure four kind 
of discrimination scores: the CV score and a related ratio, mutual 
information, Pearson correlation, and a regression between the 
output and sensitive inputs. By contrast, our approach generates 
test automatically and measure causal discrimination. 

Causal test computes pair of similar input whose output 
differ. However, input characteristic may correlate, e.g., education 
correlate with age, so perturb some characteristic without 
perturb others may create input not representative of the real 
world. FairML [1] us orthogonal projection to co-perturb charac- 
teristics, which can mask some discrimination, but find discrimi- 
nation that be more likely to be observe in real-world scenarios, 
somewhat analogously to our apparent discrimination measure. 

Combinatorial test minimizes the number of test need 
to explore certain combination of input characteristics. For ex- 
ample, all-pairs test generates test that evaluate every pos- 
sible value combination for every pair of input characteristics, 
which can be particularly helpful when test software product 
line [6, 44, 46, 47]. The number of test need to evaluate every 
possible value pair can be significantly small than the exhaustive 
test alternative since each test can simultaneously contribute to 
multiple value pair [22, 44, 80]. Such combinatorial test opti- 
mizations be complementary to our work on discrimination testing. 
Our main goal be to develop a method to process test execution to 
measure software discrimination, whereas that be not a goal of com- 
binatorial testing. Advances in combinatorial testing, e.g., use 
static or dynamic analysis for vacuity test [8, 33] or to identify 
configuration option that cannot affect a test’s output [48], can 
directly improve efficiency of discrimination test by identify 
that change a particular input characteristic cannot affect a par- 
ticular test’s output, and thus no causal discrimination be possible 
with respect to that particular input. We leave such optimization 
to future work. 

It be possible to test for discrimination software without explicit 
access to it. For example, AdFisher [23] collect information on how 
change in Google ad setting and prior visit webpage affect the 
ad Google serves. AdFisher computes a variant of group discrimi- 
nation, but it could be integrate with Themis and it algorithm 
to measure causal discrimination. 

Themis measure apparent discrimination by either execute 
a provide test suite, or by generate a test suite follow a 
provide operational profile. Operational profile [58] describe 
the input distribution likely to be observe in the field. Because 
developer-written test suite be often not a representative of field 
execution a developer would like [81], operational profile can 

significantly improve the effectiveness of test by more accu- 
rately represent real-world system use [7, 50] and the use of 
operational profile have be show to more accurately measure 
system properties, such a reliability [35]. The work on operational 
profile be complementary to ours: Themis us operational pro- 
file and work on more efficient test generation from operational 
profile can directly benefit discrimination testing. Meanwhile no 
prior work on operational profile test have measure software 
discrimination. 

Causal relationship in data management system [54, 55] can 
help explain query result [57] and debug error [82–84] by track 
and use data provenance [56]. For software system that use data 
management, such provenance-based reason may aid test 
for causal relationship between input characteristic and outputs. 
Our prior work on test software that relies on data management 
system have focus on data error [59, 60], whereas this work 
focus on test fairness. 

Automated test research have produce tool to generate tests, 
include random testing, such a Randoop [63, 64], NightHawk [4], 
JCrasher [20], CarFast [65], and T3 [69]; search-based testing, 
such a EvoSuite [29], TestFul [9], and eToc [78]; dynamic sym- 
bolic execution tools, such a DSC [37], Symbolic PathFinder [66], 
jCUTE [70], Seeker [76], Symstra [85], and Pex [77], among others; 
and commercial tools, such a Agitar [2]. The goal of the generate 
test be typically find bug [29] or generate specification [21]. 
These tool deal with more complex input space than Themis, but 
none of them focus on test fairness and they require oracle 
whereas Themis do not need oracle a it measure discrimination 
by compare tests’ outputs. Future work could extend these tool 
to generate fairness tests, modify test generation to produce 
pair of input that differ only in the input characteristic be 
tested. While prior work have tackle the oracle problem [10, 26, 30] 
typically use infer pre- and post-conditions or documenta- 
tion, our oracle be more precise and easy to compute, but be only 
applicable to fairness testing. 

7 CONTRIBUTIONS 
We have formally define software fairness test and introduce 
a causality-based measure of discrimination. We have further de- 
scribed Themis, an approach and it open-source implementation 
for measure discrimination in software and for generate effi- 
cient test suite to perform these measurements. Our evaluation 
demonstrates that discrimination in software be common, even 
when fairness be an explicit design goal, and that fairness test 
be critical to measure discrimination. Further, we formally prove 
soundness of our approach and show that Themis effectively mea- 
sures discrimination and produce efficient test suite to do so. 
With the current use of software in society-critical ways, fairness 
test research be become paramount, and our work present an 
important first step in merge test technique with software 
fairness requirements. 

ACKNOWLEDGMENT 
This work be support by the National Science Foundation under 
grant no. CCF-1453474, IIS-1453543, and CNS-1744471. 

508 



ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou 

REFERENCES 
[1] Julius Adebayo and Lalana Kagal. Iterative orthogonal feature projection for 

diagnose bias in black-box models. CoRR, abs/1611.04967, 2016. 
[2] Agitar Technologies. AgitarOne. http://www.agitar.com/solutions/products/ 

automated_junit_generation.html, 2016. 
[3] Rakesh Agrawal, Ramakrishnan Srikant, and Dilys Thomas. Privacy-preserving 

OLAP. In ACM SIGMOD International Conference on Management of Data (SIG- 
MOD), page 251–262, Baltimore, MD, USA, 2005. 

[4] James H. Andrews, Felix C. H. Li, and Tim Menzies. Nighthawk: A two-level 
genetic-random unit test data generator. In International Conference on Automated 
Software Engineering (ASE), page 144–153, Atlanta, GA, USA, 2007. 

[5] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Ma- 
chine bias. ProPublica, May 23, 2016. https://www.propublica.org/article/ 
machine-bias-risk-assessments-in-criminal-sentencing. 

[6] Sven Apel, Alexander von Rhein, Philipp Wendler, Armin Größlinger, and Dirk 
Beyer. Strategies for product-line verification: Case study and experiments. 
In International Conference on Software Engineering (ICSE), page 482–491, San 
Francisco, CA, USA, 2013. 

[7] Andrea Arcuri and Lionel Briand. A practical guide for use statistical test to 
ass randomize algorithm in software engineering. InACM/IEEE International 
Conference on Software Engineering (ICSE), page 1–10, Honolulu, HI, USA, 2011. 

[8] Thomas Ball and Orna Kupferman. Vacuity in testing. In International Conference 
on Tests and Proofs (TAP), page 4–17, Prato, Italy, April 2008. 

[9] Luciano Baresi, Pier Luca Lanzi, and Matteo Miraz. TestFul: An evolutionary test 
approach for Java. In International Conference on Software Testing, Verification, 
and Validation (ICST), page 185–194, Paris, France, April 2010. 

[10] Earl T. Barr, Mark Harman, Phil McMinn, Muzammil Shahbaz, and Shin Yoo. 
The oracle problem in software testing: A survey. IEEE Transactions on Software 
Engineering (TSE), 41(5):507–525, May 2015. 

[11] Yuriy Brun, Ron Desmarais, Kurt Geihs, Marin Litoiu, Antonia Lopes, Mary 
Shaw, and Mike Smit. A design space for adaptive systems. In Rogério de Lemos, 
Holger Giese, Hausi A. Müller, and Mary Shaw, editors, Software Engineering for 
Self-Adaptive Systems II, volume 7475, page 33–50. Springer-Verlag, 2013. 

[12] Yuriy Brun, George Edwards, Jae young Bang, and Nenad Medvidovic. Smart re- 
dundancy for distribute computation. In International Conference on Distributed 
Computing Systems (ICDCS), page 665–676, Minneapolis, MN, USA, June 2011. 

[13] Yuriy Brun and Nenad Medvidovic. An architectural style for solve compu- 
tationally intensive problem on large networks. In Software Engineering for 
Adaptive and Self-Managing Systems (SEAMS), Minneapolis, MN, USA, May 2007. 

[14] Yuriy Brun and Nenad Medvidovic. Fault and adversary tolerance a an emergent 
property of distribute systems’ software architectures. In InternationalWorkshop 
on Engineering Fault Tolerant Systems (EFTS), page 38–43, Dubrovnik, Croatia, 
September 2007. 

[15] Yuriy Brun and Nenad Medvidovic. Keeping data private while compute in the 
cloud. In International Conference on Cloud Computing (CLOUD), page 285–294, 
Honolulu, HI, USA, June 2012. 

[16] Yuriy Brun and Nenad Medvidovic. Entrusting private computation and data 
to untrusted networks. IEEE Transactions on Dependable and Secure Computing 
(TDSC), 10(4):225–238, July/August 2013. 

[17] Yuriy Brun, Jae young Bang, George Edwards, and Nenad Medvidovic. Self- 
adapt reliability in distribute software systems. IEEE Transactions on Software 
Engineering (TSE), 41(8):764–780, August 2015. 

[18] Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. Building classifier with 
independency constraints. In Proceedings of the 2009 IEEE International Conference 
on Data Mining (ICDM) Workshops, page 13–18, Miami, FL, USA, December 2009. 

[19] Toon Calders and Sicco Verwer. Three naive Bayes approach for discrimination- 
free classification. Data Mining and Knowledge Discovery, 21(2):277–292, 2010. 

[20] Christoph Csallner and Yannis Smaragdakis. JCrasher: An automatic robustness 
tester for Java. Software Practice and Experience, 34(11):1025–1050, September 
2004. 

[21] Valentin Dallmeier, Nikolai Knopp, Christoph Mallon, Gordon Fraser, Sebastian 
Hack, and Andreas Zeller. Automatically generate test case for specification 
mining. IEEE Transactions on Software Engineering (TSE), 38(2):243–257, March 
2012. 

[22] Marcelo d’Amorim, Steven Lauterburg, and Darko Marinov. Delta execution for 
efficient state-space exploration of object-oriented programs. In International 
Symposium on Software Testing and Analysis (ISSTA), page 50–60, London, UK, 
2007. 

[23] Amit Datta, Michael Carl Tschantz, and Anupam Datta. Automated experiment 
on ad privacy settings. Proceedings on Privacy Enhancing Technologies (PoPETs), 
1:92–112, 2015. 

[24] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard 
Zemel. Fairness through awareness. In Innovations in Theoretical Computer 
Science Conference (ITCS), page 214–226, Cambridge, MA, USA, August 2012. 

[25] Jeroen Eggermont, Joost N. Kok, and Walter A. Kosters. Genetic program 
for data classification: Partitioning the search space. In ACM Symposium on 
Applied Computing (SAC), page 1001–1005, Nicosia, Cyprus, 2004. 

[26] Michael D. Ernst, Alberto Goffi, Alessandra Gorla, and Mauro Pezzè. Automatic 
generation of oracle for exceptional behaviors. In International Symposium on 
Software Testing and Analysis (ISSTA), page 213–224, Saarbrücken, Genmany, 
July 2016. 

[27] Executive Office of the President. Big data: A report on algorithmic systems, 
opportunity, and civil rights. https://www.whitehouse.gov/sites/default/files/ 
microsites/ostp/2016_0504_data_discrimination.pdf, May 2016. 

[28] Katelyn Ferral. Wisconsin supreme court allows state to continue us- 
ing computer program to assist in sentencing. The Capital Times, 
July 13, 2016. http://host.madison.com/ct/news/local/govt-and-politics/ 
wisconsin-supreme-court-allows-state-to-continue-using-computer-program/ 
article_7eb67874-bf40-59e3-b62a-923d1626fa0f.html. 

[29] Gordon Fraser and Andrea Arcuri. Whole test suite generation. IEEE Transactions 
on Software Engineering (TSE), 39(2):276–291, February 2013. 

[30] Gordon Fraser and Andreas Zeller. Generating parameterized unit tests. In 
International Symposium on Software Testing and Analysis (ISSTA), page 364–374, 
Toronto, ON, Canada, 2011. 

[31] Jesus A. Gonzalez, Lawrence B. Holder, and Diane J. Cook. Graph-based relational 
concept learning. In International Conference on Machine Learning (ICML), page 
219–226, Sydney, Australia, 2002. 

[32] Noah J. Goodall. Can you program ethic into a self-driving car? IEEE Spectrum, 
53(6):28–58, June 2016. 

[33] Luigi Di Guglielmo, Franco Fummi, and Graziano Pravadelli. Vacuity analysis 
for property qualification by mutation of checkers. In Design, Automation Test in 
Europe Conference Exhibition (DATE), page 478–483, March 2010. 

[34] Erico Guizzo and Evan Ackerman. When robot decide to kill. IEEE Spectrum, 
53(6):38–43, June 2016. 

[35] Herman Hartmann. A statistical analysis of operational profile driven testing. In 
International Conference on Software Quality, Reliability, and Security Companion 
(QRS-C), page 109–116, Viena, Austria, August 2016. 

[36] David Ingold and Spencer Soper. Amazon doesn’t consider the race of it cus- 
tomers. Should it? Bloomberg, April 21, 2016. http://www.bloomberg.com/ 
graphics/2016-amazon-same-day. 

[37] Mainul Islam and Christoph Csallner. Dsc+Mock: A test case + mock class 
generator in support of cod against interfaces. In International Workshop on 
Dynamic Analysis (WODA), page 26–31, Trento, Italy, 2010. 

[38] Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. Fairness 
in learning: Classic and contextual bandits. CoRR, abs/1207.0016, 2012. https: 
//arxiv.org/abs/1605.07139. 

[39] Faisal Kamiran and Toon Calders. Classifying without discriminating. In Inter- 
national Conference on Computer, Control, and Communication (IC4), page 1–6, 
Karachi, Pakistan, February 2009. 

[40] Faisal Kamiran, Toon Calders, and Mykola Pechenizkiy. Discrimination aware 
decision tree learning. In International Conference on Data Mining (ICDM), page 
869–874, Sydney, Australia, December 2010. 

[41] Faisal Kamiran, Asim Karim, and Xiangliang Zhang. Decision theory for 
discrimination-aware classification. In International Conference on Data Mining 
(ICDM), page 924–929, Brussels, Belgium, December 2012. 

[42] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness- 
aware classifier with prejudice remover regularizer. In Joint European Conference 
on Machine Learning and Knowledge Discovery in Databases (ECML PKDD), page 
35–50, Bristol, England, UK, September 2012. 

[43] Wei-Chun Kao, Kai-Min Chung, Chia-Liang Sun, and Chih-Jen Lin. Decomposi- 
tion method for linear support vector machines. Neural Computation, 16(8):1689– 
1704, August 2004. 

[44] Christian Kästner, Alexander von Rhein, Sebastian Erdweg, Jonas Pusch, Sven 
Apel, Tillmann Rendel, and Klaus Ostermann. Toward variability-aware testing. 
In International Workshop on Feature-Oriented Software Development (FOSD), 
page 1–8, Dresden, Germany, 2012. 

[45] Matthew Kay, Cynthia Matuszek, and Sean A. Munson. Unequal representation 
and gender stereotype in image search result for occupations. In Conference on 
Human Factors in Computing Systems (CHI), page 3819–3828, Seoul, Republic of 
Korea, 2015. 

[46] Chang Hwan Peter Kim, Don S. Batory, and Sarfraz Khurshid. Reducing combi- 
natorics in test product lines. In International Conference on Aspect-Oriented 
Software Development (AOST), page 57–68, Porto de Galinhas, Brazil, 2011. 

[47] Chang Hwan Peter Kim, Sarfraz Khurshid, and Don Batory. Shared execution 
for efficiently test product lines. In International Symposium on Software 
Reliability Engineering (ISSRE), page 221–230, November 2012. 

[48] Chang Hwan Peter Kim, Darko Marinov, Sarfraz Khurshid, Don Batory, Sabrina 
Souto, Paulo Barros, and Marcelo D’Amorim. SPLat: Lightweight dynamic 
analysis for reduce combinatorics in test configurable systems. In European 
Software Engineering Conference and ACM SIGSOFT International Symposium on 
Foundations of Software Engineering (ESEC/FSE), page 257–267, Saint Petersburg, 
Russia, 2013. 

[49] Ivo Krka, Yuriy Brun, George Edwards, and Nenad Medvidovic. Synthesizing 
partial component-level behavior model from system specifications. In European 
Software Engineering Conference and ACM SIGSOFT International Symposium on 
Foundations of Software Engineering (ESEC/FSE), page 305–314, Amsterdam, The 

509 

http://www.agitar.com/solutions/products/automated_junit_generation.html 
http://www.agitar.com/solutions/products/automated_junit_generation.html 
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing 
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing 
https://www.whitehouse.gov/sites/default/files/microsites/ostp/2016_0504_data_discrimination.pdf 
https://www.whitehouse.gov/sites/default/files/microsites/ostp/2016_0504_data_discrimination.pdf 
http://host.madison.com/ct/news/local/govt-and-politics/wisconsin-supreme-court-allows-state-to-continue-using-computer-program/article_7eb67874-bf40-59e3-b62a-923d1626fa0f.html 
http://host.madison.com/ct/news/local/govt-and-politics/wisconsin-supreme-court-allows-state-to-continue-using-computer-program/article_7eb67874-bf40-59e3-b62a-923d1626fa0f.html 
http://host.madison.com/ct/news/local/govt-and-politics/wisconsin-supreme-court-allows-state-to-continue-using-computer-program/article_7eb67874-bf40-59e3-b62a-923d1626fa0f.html 
http://www.bloomberg.com/graphics/2016-amazon-same-day 
http://www.bloomberg.com/graphics/2016-amazon-same-day 
https://arxiv.org/abs/1605.07139 
https://arxiv.org/abs/1605.07139 


Fairness Testing: Testing Software for Discrimination ESEC/FSE’17, September 4–8, 2017, Paderborn, Germany 

Netherlands, August 2009. 
[50] K. Saravana Kumar and Ravindra Babu Misra. Software operational profile base 

test case allocation use fuzzy logic. International Journal of Automation and 
Computing, 4(4):388–395, 2007. 

[51] Matt J. Kusner, Joshua R. Loftus, Chris Russell, and Ricardo Silva. Counterfactual 
fairness. CoRR, abs/1703.06856, 2017. 

[52] Rafi Letzter. Amazon just show u that ‘unbiased’ algorithm can be in- 
advertently racist. TECH Insider, April 21, 2016. http://www.techinsider.io/ 
how-algorithms-can-be-racist-2016-4. 

[53] Dana Mattioli. On Orbitz, Mac user steer to pricier hotels. The 
Wall Street Journal, August 23, 2012. http://www.wsj.com/articles/ 
SB10001424052702304458604577488822667325882. 

[54] Alexandra Meliou, Wolfgang Gatterbauer, Joseph Y. Halpern, Christoph Koch, 
Katherine F. Moore, and Dan Suciu. Causality in databases. IEEE Data Engineering 
Bulletin, 33(3):59–67, 2010. 

[55] Alexandra Meliou, Wolfgang Gatterbauer, Katherine F. Moore, and Dan Su- 
ciu. The complexity of causality and responsibility for query answer and non- 
answers. Proceedings of the VLDB Endowment (PVLDB), 4(1):34–45, 2010. 

[56] Alexandra Meliou, Wolfgang Gatterbauer, and Dan Suciu. Bringing provenance 
to it full potential use causal reasoning. In 3rd USENIXWorkshop on the Theory 
and Practice of Provenance (TaPP), Heraklion, Greece, June 2011. 

[57] Alexandra Meliou, Sudeepa Roy, and Dan Suciu. Causality and explanation in 
databases. Proceedings of the VLDB Endowment (PVLDB), 7(13):1715–1716, 2014. 
(tutorial presentation). 

[58] John D. Musa. Operational profile in software-reliability engineering. IEEE 
Software, 10(2):14–32, March 1993. 

[59] Kıvanç Muşlu, Yuriy Brun, and Alexandra Meliou. Data debug with continu- 
ous testing. In Proceedings of the New Ideas Track at the 9th Joint Meeting of the 
European Software Engineering Conference and ACM SIGSOFT Symposium on the 
Foundations of Software Engineering (ESEC/FSE), page 631–634, Saint Petersburg, 
Russia, August 2013. 

[60] Kıvanç Muşlu, Yuriy Brun, and Alexandra Meliou. Preventing data error with 
continuous testing. In Proceedings of the ACM SIGSOFT International Symposium 
on Software Testing and Analysis (ISSTA), page 373–384, Baltimore, MD, USA, 
July 2015. 

[61] Satya Nadella. The partnership of the future. Slate, June 28, 2016. http: 
//www.slate.com/articles/technology/future_tense/2016/06/microsoft_ceo_ 
satya_nadella_humans_and_a_i_can_work_together_to_solve_society.html. 

[62] Parmy Olson. The algorithm that beat your bank manager. CNN 
Money, March 15, 2011. http://www.forbes.com/sites/parmyolson/2011/03/15/ 
the-algorithm-that-beats-your-bank-manager/#cd84e4f77ca8. 

[63] Carlos Pacheco and Michael D. Ernst. Randoop: Feedback-directed random 
test for Java. In Conference on Object-oriented Programming Systems and 
Applications (OOPSLA), page 815–816, Montreal, QC, Canada, 2007. 

[64] Carlos Pacheco, Shuvendu K. Lahiri, Michael D. Ernst, and Thomas Ball. Feedback- 
direct random test generation. In ACM/IEEE International Conference on Soft- 
ware Engineering (ICSE), page 75–84, Minneapolis, MN, USA, May 2007. 

[65] Sangmin Park, B. M. Mainul Hossain, Ishtiaque Hussain, Christoph Csallner, 
Mark Grechanik, Kunal Taneja, Chen Fu, and Qing Xie. CarFast: Achieving 
high statement coverage faster. In Symposium on the Foundations of Software 
Engineering (FSE), page 35:1–35:11, Cary, NC, USA, 2012. 

[66] Corina S. Păsăreanu and Neha Rungta. Symbolic PathFinder: Symbolic execution 
of Java bytecode. In International Conference on Automated Software Engineering 
(ASE), page 179–180, Antwerp, Belgium, 2010. 

[67] Judea Pearl. Causal inference in statistics: An overview. Statistics Surveys, 
3:96–146, 2009. 

[68] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, 
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron 
Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Courna- 
peau, Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. Scikit-learn: 
Machine learn in Python. Journal of Machine Learning Research, 12:2825–2830, 
2011. 

[69] I. S. Wishnu B. Prasetya. Budget-aware random test with T3: Benchmarking 
at the SBST2016 test tool contest. In International Workshop on Search-Based 
Software Testing (SBST), page 29–32, Austin, TX, USA, 2016. 

[70] Koushik Sen and Gul Agha. CUTE and jCUTE: Concolic unit test and ex- 
plicit path model-checking tools. In International Conference on Computer Aided 
Verification (CAV), page 419–423, Seattle, WA, USA, 2006. 

[71] Aarti Shahani. Now algorithm be decide whom to hire, base on voice. NPR 
All Things Considered, March 2015. 

[72] Spencer Soper. Amazon to bring same-day delivery to Bronx, Chicago after outcry. 
Bloomberg, May 1, 2016. http://www.bloomberg.com/news/articles/2016-05-01/ 
amazon-pledges-to-bring-same-day-delivery-to-bronx-after-outcry. 

[73] Spencer Soper. Amazon to bring same-day delivery to Roxbury after outcry. 
Bloomberg, April 26, 2016. http://www.bloomberg.com/news/articles/2016-04-26/ 
amazon-to-bring-same-day-delivery-to-roxbury-after-outcry. 

[74] Eliza Strickland. Doc bot prep for the O.R. IEEE Spectrum, 53(6):32–60, June 
2016. 

[75] Latanya Sweeney. Discrimination in online ad delivery. Communications of the 
ACM (CACM), 56(5):44–54, May 2013. 

[76] Suresh Thummalapenta, Tao Xie, Nikolai Tillmann, Jonathan de Halleux, and 
Zhendong Su. Synthesizing method sequence for high-coverage testing. In 
International Conference on Object Oriented Programming Systems Languages and 
Applications (OOPSLA), page 189–206, Portland, OR, USA, 2011. 

[77] Nikolai Tillmann and Jonathan De Halleux. Pex: White box test generation for 
.NET. In International Conference on Tests and Proofs (TAP), page 134–153, Prato, 
Italy, 2008. 

[78] Paolo Tonella. Evolutionary test of classes. In International Symposium on 
Software Testing and Analysis (ISSTA), page 119–128, Boston, MA, USA, 2004. 

[79] Florian Tramer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Jean-Pierre 
Hubaux, Mathias Humbert, Ari Juels, , and Huang Lin. FairTest: Discovering un- 
warrant association in data-driven applications. In IEEE European Symposium 
on Security and Privacy (EuroS&P), Paris, France, April 2017. 

[80] Alexander von Rhein, Sven Apel, and Franco Raimondi. Introducing binary 
decision diagram in the explicit-state verification of Java code. In The Java 
Pathfinder Workshop (JPF), Lawrence, KS, USA, November 2011. 

[81] Qianqian Wang, Yuriy Brun, and Alessandro Orso. Behavioral execution compar- 
ison: Are test representative of field behavior? In International Conference on 
Software Testing, Verification, and Validation (ICST), Tokyo, Japan, March 2017. 

[82] Xiaolan Wang, Xin Luna Dong, and Alexandra Meliou. Data X-Ray: A diagnostic 
tool for data errors. In International Conference on Management of Data (SIGMOD), 
2015. 

[83] Xiaolan Wang, Alexandra Meliou, and Eugene Wu. QFix: Demonstrating error 
diagnosis in query histories. In International Conference on Management of Data 
(SIGMOD), page 2177–2180, 2016. (demonstration paper). 

[84] Xiaolan Wang, Alexandra Meliou, and Eugene Wu. QFix: Diagnosing error 
through query histories. In International Conference on Management of Data 
(SIGMOD), 2017. 

[85] Tao Xie, Darko Marinov, Wolfram Schulte, and David Notkin. Symstra: A 
framework for generate object-oriented unit test use symbolic execution. In 
International Conference on Tools and Algorithms for the Construction and Analysis 
of Systems (TACAS), page 365–381, Edinburgh, Scotland, UK, 2005. 

[86] Bianca Zadrozny. Learning and evaluate classifier under sample selection bias. 
In International Conference on Machine Learning (ICML), page 114–121, Banff, 
AB, Canada, 2004. 

[87] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. 
Gummadi. Fairness constraints: A mechanism for fair classification. In Fairness, 
Accountability, and Transparency in Machine Learning (FAT ML), Lille, France, 
July 2015. 

[88] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P 
Gummadi. Learning fair classifiers. CoRR, abs/1507.05259, 2015. 

[89] Richard Zemel, Yu (Ledell) Wu, Kevin Swersky, Toniann Pitassi, and Cynthia 
Dwork. Learning fair representations. In International Conference on Machine 
Learning (ICML), publish in JMLR W&CP: 28(3):325–333), Atlanta, GA, USA, 
June 2013. 

[90] Farhang Zia. GitHub repository for the decision tree classifier project. https: 
//github.com/novaintel/csci4125/tree/master/Decision-Tree, 2014. 

[91] Indre Žliobaite, Faisal Kamiran, and Toon Calders. Handling conditional dis- 
crimination. In International Conference on Data Mining (ICDM), page 992–1001, 
Vancouver, BC, Canada, December 2011. 

510 

http://www.techinsider.io/how-algorithms-can-be-racist-2016-4 
http://www.techinsider.io/how-algorithms-can-be-racist-2016-4 
http://www.wsj.com/articles/SB10001424052702304458604577488822667325882 
http://www.wsj.com/articles/SB10001424052702304458604577488822667325882 
http://www.slate.com/articles/technology/future_tense/2016/06/microsoft_ceo_satya_nadella_humans_and_a_i_can_work_together_to_solve_society.html 
http://www.slate.com/articles/technology/future_tense/2016/06/microsoft_ceo_satya_nadella_humans_and_a_i_can_work_together_to_solve_society.html 
http://www.slate.com/articles/technology/future_tense/2016/06/microsoft_ceo_satya_nadella_humans_and_a_i_can_work_together_to_solve_society.html 
http://www.forbes.com/sites/parmyolson/2011/03/15/the-algorithm-that-beats-your-bank-manager/#cd84e4f77ca8 
http://www.forbes.com/sites/parmyolson/2011/03/15/the-algorithm-that-beats-your-bank-manager/#cd84e4f77ca8 
http://www.bloomberg.com/news/articles/2016-05-01/amazon-pledges-to-bring-same-day-delivery-to-bronx-after-outcry 
http://www.bloomberg.com/news/articles/2016-05-01/amazon-pledges-to-bring-same-day-delivery-to-bronx-after-outcry 
http://www.bloomberg.com/news/articles/2016-04-26/amazon-to-bring-same-day-delivery-to-roxbury-after-outcry 
http://www.bloomberg.com/news/articles/2016-04-26/amazon-to-bring-same-day-delivery-to-roxbury-after-outcry 
https://github.com/novaintel/csci4125/tree/master/Decision-Tree 
https://github.com/novaintel/csci4125/tree/master/Decision-Tree 

