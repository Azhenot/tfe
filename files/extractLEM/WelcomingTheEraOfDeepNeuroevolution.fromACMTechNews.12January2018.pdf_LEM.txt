






































Welcoming the Era of Deep Neuroevolution 


Welcoming the Era of Deep 
Neuroevolution 

By Kenneth O. Stanley & Jeff Clune 

December 18, 2017 

On behalf of an Uber AI Labs team that also include Joel Lehman, Jay 

Chen, Edoardo Conti, Vashisht Madhavan, Felipe Petroski Such, & 

Xingwen Zhang. 

In the field of deep learning, deep neural network (DNNs) with many 

layer and million of connection be now train routinely through 

stochastic gradient descent (SGD). Many assume that the ability of SGD 

to efficiently compute gradient be essential to this capability. However, 

we be release a suite of five paper that support the emerge 

realization that neuroevolution, where neural network be optimize 

through evolutionary algorithms, be also an effective method to train 

deep neural network for reinforcement learn (RL) problems. Uber 

have a multitude of area where machine learn can improve it 

operations, and develop a broad range of powerful learn 

approach that include neuroevolution will help u achieve our 

mission of develop safer and more reliable transportation solutions. 

Genetic algorithm a a competitive alternative for training 

deep neural network 

Using a new technique we invent to efficiently evolve DNNs, we be 

surprised to discover that an extremely simple genetic algorithm (GA) 

can train deep convolutional network with over 4 million parameter 

to play Atari game from pixels, and on many game outperforms 

modern deep reinforcement learn (RL) algorithm (e.g. DQN and 

A3C) or evolution strategy (ES), while also be faster due to good 

parallelization. This result be surprising both because GAs, which be 

not gradient-based, be not expect to scale well to such large 

parameter space and also because match or outperform the 

state-of-the-art in RL use GAs be not thought to be possible. We 

Welcoming the Era of Deep Neuroevolution https://eng.uber.com/deep-neuroevolution/ 

1 sur 9 12-01-18 à 19:09 



further show that modern GA enhancement that improve the power of 

GAs, such a novelty search, also work at DNN scale and can promote 

exploration to solve deceptive problem (those with challenge local 

optima) that stymie reward-maximizing algorithm such a Q-learning 

(DQN), policy gradient (A3C), ES, and the GA. 

This GA policy score 10,500 on 
Frostbite. DQN, AC3, and ES score 
less than 1,000 on this game. 

The GA play Asteroids well. It 
outperforms DQN and ES on 
average, but not A3C. 

Safe mutation through gradient computation 

In a separate paper, we show how gradient can be combine with 

neuroevolution to improve the ability to evolve recurrent and very deep 

neural networks, enable the evolution of DNNs with over one 

hundred layers, a level far beyond what be previously show possible 

through neuroevolution. We do so by compute the gradient of 

network output with respect to the weight (i.e. not the gradient of 

error a in conventional deep learning), enable the calibration of 

random mutation to treat the most sensitive parameter more 

delicately than the least, thereby solve a major problem with random 

mutation in large networks. 

Both animation show a batch of mutation of a single network that 

could solve the maze (with start at low left and goal at upper left). 

Normal mutation mostly lose the ability to reach the end while safe 

mutation largely preserve it while still yield diversity, illustrate 

the significant advantage of mutate safely. 

How ES relates to SGD 

Our paper complement an already-emerging realization, which be 

first note by a team from OpenAI, that the evolution strategy variety of 

neuroevolution can optimize deep neural network competitively on 

Deep RL tasks. However, to date, the broader implication of this result 

have remain a subject of some speculation. Setting the stage for 

further innovation with ES, we provide deeper insight into it 

relationship to SGD through a comprehensive study that examines how 

close the ES gradient approximation actually come to the optimal 

gradient for each mini-batch compute by SGD on MNIST, and also 

Welcoming the Era of Deep Neuroevolution https://eng.uber.com/deep-neuroevolution/ 

2 sur 9 12-01-18 à 19:09 



how close this approximation must be to perform well. We show that ES 

can achieve 99 percent accuracy on MNIST if enough computation be 

provide to improve it gradient approximation, hint at why ES will 

increasingly be a serious contender in Deep RL, where no method have 

privileged access to perfect gradient information, a parallel 

computation increases. 

ES be not just traditional finite difference 

Adding further understanding, a companion study confirms empirically 

that ES (with a large enough perturbation size parameter) act 

differently than SGD would, because it optimizes for the expect 

reward of a population of policy described by a probability 

distribution (a cloud in the search space), whereas SGD optimizes 

reward for a single policy (a point in the search space). This change 

cause ES to visit different area of the search space, for good or for 

bad (both case be illustrated). An additional consequence of 

optimize over a population of parameter perturbation be that ES 

acquires robustness property not attain through SGD. Highlighting 

that ES optimizes over a population of parameter also emphasizes an 

intrigue link between ES and Bayesian methods. 

Random perturbation of the weight of a walker learn by TRPO 

lead to significantly less stable gait than random perturbation of a 

walker of equivalent quality evolve by ES. The original learn 

walker be at the center of each nine-frame composite. 

Traditional finite difference (gradient descent) cannot cross a narrow 

gap of low fitness while ES easily cross it to find high fitness on the 

other side. 

ES stall a a path of high fitness narrows, while traditional finite 

difference (gradient descent) traverse the same path with no 

problem, illustrate along with the previous video the distinction and 

trade-off between the two different approaches. 

Improving exploration in ES 

An excite consequence of deep neuroevolution be that the collection of 

tool previously developed for neuroevolution now become candidate 

Welcoming the Era of Deep Neuroevolution https://eng.uber.com/deep-neuroevolution/ 

3 sur 9 12-01-18 à 19:09 



for enhance the training of deep neural networks. We explore this 

opportunity by introduce new algorithm that combine the 

optimization power and scalability of ES with method unique to 

neuroevolution that promote exploration in RL domain via a 

population of agent incentivized to act differently from one 

another. Such population-based exploration be different from the single- 

agent tradition in RL, include recent work on exploration in deep RL. 

Our experiment reveal that add this new style of exploration 

improves the performance of ES in many domain that require 

exploration to avoid deceptive local optima, include some Atari game 

and a humanoid locomotion task in the Mujoco simulator. 

ES ES w/ Exploration Reward During Training 

With our hyperparameters, ES converges quickly to a local optimum 
of not come up for oxygen because do so temporarily foregoes 
earn reward. With exploration, however, the agent learns to come 
up for oxygen and thus accrue much high reward in the future. 
Note that Salimans et al. 2017 do not report ES, with their 
hyperparameters, encounter this particular local optima, but the 
point that ES without exploration can get stuck indefinitely on some 
local optimum (and that exploration help it get unstuck) be a general 
one, a our paper shows. 

ES ES w/ Exploration Reward During Training 

The agent be tasked with run a far forward a it can. ES never 
learns to escape the deceptive trap. With a pressure to explore, 

Welcoming the Era of Deep Neuroevolution https://eng.uber.com/deep-neuroevolution/ 

4 sur 9 12-01-18 à 19:09 



however, one of the agent learns to navigate around the trap. 

Conclusions 

For neuroevolution researcher interested in move towards deep 

network there be several important considerations: first, these kind 

of experiment require more computation than in the past; for the 

experiment in these new papers, we often use hundred or even 

thousand of simultaneous CPUs per run. However, the hunger for 

more CPUs or GPUs should not be view a a liability; in the long run, 

the simplicity of scale evolution to massively parallel compute 

center mean that neuroevolution be perhaps best poise to take 

advantage of the world that be coming. 

The new result be so different from what be previously observe in 

lower-dimensional neuroevolution that they effectively overturn year 

of intuitions, in particular on the implication of search in high 

dimensions. As have be discover in deep learning, above some 

threshold of complexity, it appear that search actually becomes easy 

in high dimension in the sense that it be less susceptible to local 

optima. While the field of deep learn be familiar with this way of 

thinking, it implication be only begin to be digest in 

neuroevolution. 

The reemergence of neuroevolution be yet another example that old 

algorithm combine with modern amount of compute can work 

surprisingly well. The viability of neuroevolution be interest because 

the many technique that have be developed in the neuroevolution 

community immediately become available at DNN-scale, each offering 

different tool to solve challenge problems. Moreover, a our paper 

show, neuroevolution search differently than SGD, and thus offer an 

interest alternative approach to have in the machine learn 

toolbox. We wonder whether deep neuroevolution will experience a 

renaissance just a deep learn has. If so, 2017 may mark the 

begin of the era, and we be excite to see what will unfold in the 

year to come! 

Here be the five paper we be release today, along with a summary 

of their key findings: 

Welcoming the Era of Deep Neuroevolution https://eng.uber.com/deep-neuroevolution/ 

5 sur 9 12-01-18 à 19:09 



Deep Neuroevolution: Genetic Algorithms be a Competitive Alternative 

for Training Deep Neural Networks for Reinforcement Learning 

Evolves DNNs with a simple, traditional, population-based genetic 

algorithm that performs well on hard deep RL problems. On Atari, the 

GA performs a well a evolution strategy and deep reinforcement 

learn algorithm base on Q-learning (DQN) and policy gradient 

(A3C). 

The “Deep GA” successfully evolves network with over four million free 

parameters, the large neural network ever evolve with a traditional 

evolutionary algorithm. 

Suggests intriguingly that in some case follow the gradient be not 

the best choice for optimize performance. 

Combines DNNs with Novelty Search, an exploration algorithm 

design for task with deceptive or sparse reward functions, to solve a 

deceptive, high-dimensional problem on which reward-maximizing 

algorithm (e.g. GA and ES) fail. 

Shows the Deep GA parallelizes good than, and be thus faster than, ES, 

A3C, and DQN, and enables a state-of-the-art compact encode 

technique that can represent million-parameter DNNs in thousand of 

bytes. 

Includes result for random search on Atari. Surprisingly, on some 

game random search substantially outperforms DQN, A3C, and ES, 

although it never outperforms the GA. 

00:26 -00:09 00:35 

Copy and paste this HTML code into your webpage to embed. 

spaceplay / pause 

qunload | stop 

Welcoming the Era of Deep Neuroevolution https://eng.uber.com/deep-neuroevolution/ 

6 sur 9 12-01-18 à 19:09 



ffullscreen 

shift + ←→slower / faster 

←→seek 

. seek to previous 

Surprisingly, a DNN found through random search play Frostbite 

well, outperform DQN, A3C, and ES, but not the GA. 

Safe Mutations for Deep and Recurrent Neural Networks through 

Output Gradients 

Safe mutation through gradient (SM-G) greatly improves the efficacy 

of mutation in large, deep, and recurrent network by measure the 

sensitivity of the network to change in particular connection weights. 

Computes gradient of output with respect to weight instead of 

gradient of error or loss a in conventional deep learning, allow 

random, but safe, exploratory steps. 

Both type of safe mutation require no additional trial or rollouts in the 

domain. 

The result: deep network (over 100 layers) and large recurrent 

network now evolve effectively only through variant of SM-G. 

On the Relationship Between the OpenAI Evolution Strategy and 

Stochastic Gradient Descent 

Explores the relationship between ES and SGD by compare the 

approximate gradient compute by ES with the exact gradient 

compute by SGD in MNIST under different conditions. 

Develops fast proxy that predict ES expect performance with 

different population sizes. 

Introduces and demonstrates different way to speed up and improve 

the performance of ES. 

Limited perturbation ES significantly speed up execution on parallel 

infrastructure. 

No-mini-batch ES improves the gradient estimate by replace the 

mini-batch convention design for SGD with a different approach 

customize for ES: A random subset of the whole training batch be 

Welcoming the Era of Deep Neuroevolution https://eng.uber.com/deep-neuroevolution/ 

7 sur 9 12-01-18 à 19:09 



assign to each member of the ES population within each iteration of 

the algorithm. This ES-specific approach provide good accuracy for 

ES with equivalent computation, and the learn curve be much 

smoother than even for SGD. 

No-mini-batch ES reach 99 percent accuracy in a test run, the best 

report performance of an evolutionary approach in this supervise 

learn task. 

Overall help to show why ES would be able to compete in RL, where 

gradient information obtain through trial in the domain be less 

informative with respect to the performance objective than in 

supervise learning. 

ES Is More Than Just a Traditional Finite Difference Approximator 

Highlights an important distinction between ES and traditional finite 

difference methods, which be that ES optimizes for an optimal 

distribution of solution (as oppose to one optimal solution). 

One interest consequence: Solutions found by ES tend to be robust 

to parameter perturbation. For example, we show that Humanoid 

Walking solution from ES be significantly more robust to parameter 

perturbation than similar solution found by GAs and by TRPO. 

Another important consequence: ES be expect to solve some 

problem where conventional method would become trapped, and vice 

versa. Simple example illustrate these different dynamic between ES 

and conventional gradient-following. 

Improving Exploration in Evolution Strategies for Deep Reinforcement 

Learning via a Population of Novelty-Seeking Agents 

Adds the ability to encourage deep exploration in ES. 

Shows that algorithm that have be invent to promote exploration 

in small-scale evolve neural network via population of explore 

agent — specifically novelty search (NS) and quality diversity (QD) 

algorithm — can be hybridize with ES to improve it performance on 

sparse or deceptive deep RL tasks, while retain scalability. 

Confirms that the resultant new algorithms, NS-ES and a version of 

QD-ES call NSR-ES, avoid local optimum encounter by ES to achieve 

high performance on task range from simulated robot learn to 

Welcoming the Era of Deep Neuroevolution https://eng.uber.com/deep-neuroevolution/ 

8 sur 9 12-01-18 à 19:09 



walk around a deceptive trap to the high-dimensional pixel-to-action 

task of play Atari games. 

Adds this new family of population-based exploration algorithm to the 

deep RL toolbox. 

To be notify of future Uber AI Labs blog posts, please sign up for our 

mail list, or you can subscribe to the Uber AI Labs YouTube 

channel. If you be interested in join Uber AI Labs, please apply at 

Uber.ai. 

Header Image Credit: Eric Frank 

Categories: Uber Data / 

Tags: AI, AI Labs, Already-Emerging Realization, artificial intelligence, 

Atari, Deep Neuroevolution, DNNS, ES, Evolution, Finite Difference, 

Jeff Clune, Kenneth Stanley, Machine Learning, ML, MNIST, Mujoco, 

Neuroevolution, Novelty Search, OpenAI, Peter Dayan, Policy 

Gradients, Q-Learning, Reinforcement Learning, SGD, SM-R, 

Stochastic Gradient Descent, TRPO, Uber, Uber AI Labs, Uber 

Engineering 

Welcoming the Era of Deep Neuroevolution https://eng.uber.com/deep-neuroevolution/ 

9 sur 9 12-01-18 à 19:09 


