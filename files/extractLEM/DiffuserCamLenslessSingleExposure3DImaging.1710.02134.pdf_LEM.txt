


















































Research Article 1 

DiffuserCam: Lensless Single-exposure 3D Imaging 
NICK ANTIPA1,*, GRACE KUO1,*, REINHARD HECKEL1, BEN MILDENHALL1, 
EMRAH BOSTAN1, REN NG1, AND LAURA WALLER1,+ 

1Department of Electrical Engineering & Computer Sciences, University of California, Berkeley, California 94720, USA 
*Authors contribute equally 
+Corresponding Author: lwaller@alum.mit.edu 

Compiled October 6, 2017 

We demonstrate a compact and easy-to-build computational camera for single-shot 3D imag- 
ing. Our lensless system consists solely of a diffuser place in front of a standard image sen- 
sor. Every point within the volumetric field-of-view project a unique pseudorandom pattern 
of caustic on the sensor. By use a physical approximation and simple calibration scheme, we 
solve the large-scale inverse problem in a computationally efficient way. The caustic pattern 
enable compress sensing, which exploit sparsity in the sample to solve for more 3D voxels 
than pixel on the 2D sensor. Our 3D voxel grid be chosen to match the experimentally mea- 
sured two-point optical resolution across the field-of-view, result in 100 million voxels be 
reconstruct from a single 1.3 megapixel image. However, the effective resolution varies sig- 
nificantly with scene content. Because this effect be common to a wide range of computational 
cameras, we provide new theory for analyze resolution in such systems. 

1. INTRODUCTION 

Because optical sensor be 2D, capture 3D information re- 
quire projection onto a 2D sensor in such a way that the 3D data 
can be recovered. Scanning and multi-shot method can achieve 
high spatial resolution, but sacrifice capture speed and require 
complex hardware. In contrast, exist single-shot method be 
fast but have low resolution. Most 3D imagers require bulky 
hardware, such a large bench-top microscopes. In this work, we 
introduce a compact and inexpensive single-shot lensless system 
capable of volumetric imaging, and show that it improves upon 
the sample limit of exist single-shot system by leverage 
compress sensing. 

We present DiffuserCam, a lensless imager that us a dif- 
fuser to encode the 3D intensity of volumetric object into a 
single 2D image. The diffuser, a thin phase mask, be place a 
few millimeter in front of a traditional 2D image sensor. Each 
point source in 3D space creates a unique pseudorandom caus- 
tic pattern that cover a large portion of the sensor. Due to 
these properties, compress sense algorithm can be use 
to reconstruct more voxels than pixel captured, provide that 
the 3D sample be sparse in some domain. We recover this 3D 
information by solve a sparsity-constrained optimization prob- 
lem, use a physical model and simple calibration scheme to 

make the computation and calibration scalable. This allows u 
to reconstruct on a grid of 100 million voxels, several order of 
magnitude more than previous work. 

We demonstrate a prototype DiffuserCam system built en- 
tirely from commodity hardware. It be simple to calibrate, do 
not require precise alignment during construction and be light 
efficient (as compare to amplitude masks). We reconstruct 3D 
object on a grid of 100 million voxels (non-uniformly spaced) 
from a single 1.3 megapixel image, which provide high resolu- 
tion across a large volume. Our reconstruction show true depth 
sectioning, allow u to generate 3D rendering of the sample. 

Our system us a nonlinear reconstruction algorithm, which 
result in object-dependent performance. We quantify this by ex- 
perimentally measure the resolution of our prototype use a 
variety of test targets, and we show that the standard two-point 
resolution criterion be mislead and should be consider a 
best-case scenario. Additionally, we propose a new local condi- 
tion number analysis that explains the variable resolve power 
and show that the theory be consistent with our experiments. 

DiffuserCam us concept from lensless camera technology 
and image through complex media, integrate together via 
computational image design principles. This new device could 
enable high-resolution lensless 3D image of large and dynamic 

ar 
X 

iv 
:1 

71 
0. 

02 
13 

4v 
1 

[ 
c 

.C 
V 

] 
5 

O 
ct 

2 
01 

7 



Research Article 2 

Experimental setup 

diffuser 

sensor 

Calibration 

Algorithm 3D Reconstruction 
v̂ = argmin 

v≥0 
1 
2‖b−Hv‖22 
+λ‖Ψv‖1 

Fig. 1. DiffuserCam setup and reconstruction pipeline. Our lensless system consists of a diffuser place in front of a sensor (bumps 
on the diffuser be exaggerated for illustration). The system encodes a 3D scene into a 2D image on the sensor. A one-time calibra- 
tion consists of scan a point source axially while capture images. Images be reconstruct computationally by solve a 
nonlinear inverse problem with a sparsity prior. The result be a 3D image reconstruct from a single 2D measurement. 

3D sample in an extremely compact package. Such camera 
will open up new application in remote diagnostics, mobile 
photography and in vivo microscopy. 

To review related previous work, we start with lensless cam- 
era for 2D photography, which have show great promise be- 
cause of their small form factors. Unlike traditional cameras, in 
which a point in the scene map to a pixel on the sensor, lensless 
camera map a point in the scene to many point on the sensor, 
require computational reconstruction. A typical lensless ar- 
chitecture replaces the lens with an encode element place 
directly in front of the sensor. Incoherent 2D lensless camera 
have be demonstrate use amplitude mask [1], diffractive 
mask [2, 3], random reflective surface [4, 5], and modify mi- 
crolens array [6] a the encode element. 2D lensless image 
with coherent illumination have be demonstrate in [7, 8], and 
extend to 3D [9–12], but these method require active or coher- 
ent illumination. Our system us a similar lensless architecture 
but extends both the design and image reconstruction to enable 
3D capture without the need for coherent lighting. 

Light field cameras, also call integral imagers, passively 
capture 4D space-angle information in a single-shot [13], which 
can be use for 3D reconstructions. This concept can be built 
into a thin form factor with microlens array [14] or Fresnel 
zone plate [15]. Lenslet array-based 3D capture scheme have 
also be use in microscopy [16], where improve result be 
obtain when wave-optical effect [17, 18] or in vivo tissue scat- 
tering [18, 19] be account for. All of these system must trade 
resolution or field-of-view for single-shot capture. DiffuserCam, 
in contrast, us compress sense to capture large 3D vol- 
umes at high resolution in a single exposure. 

Diffusers be often use a a proxy for general scatter me- 
dia in the context of develop method for image through 
scatter [20–22]. These work have similar mathematical mod- 
el to our system, but instead of try to mitigate the effect 
of unwanted scattering, here we use the diffuser a an optical 
element in our system design. Therefore, we choose a thin, opti- 
cally smooth diffuser that refracts pseudorandomly (as oppose 
to true random scattering). Such diffuser have be show to 
produce high contrast pattern under incoherent illumination, 
enable light field image [23], and have also be use to 
record coherent hologram [10, 24]. Multiple scatter with 
coherent illumination have be demonstrate a an encode 
mechanism for 2D compress sense [25], but necessitates an 

inefficient transmission matrix calibration approach, limit re- 
construction to a few thousand pixels. We achieve similar bene- 
fit without need coherent illumination, and, unlike previous 
work, we use compress sense to add depth information. 
Finally, our system be design to enable simpler calibration and 
more efficient computation, allow for 3D reconstruction at 
megavoxel scale with superior image quality. 

A. System Overview 
DiffuserCam be part of the class of mask-based lensless imagers 
in which a phase or amplitude mask be place a small distance 
in front of a sensor, with no main lens. Our mask (the diffuser) 
be a thin transparent phase object with smoothly vary thick- 
ness (see Fig. 1). When illuminate by an incoherent source 
sufficiently far from the sensor, the convex bump concentrate 
light into high-frequency pseudorandom caustic pattern which 
be capture by the sensor. The caustic patterns, term Point 
Spread Functions (PSFs), vary with the 3D position of the source, 
thereby capture 3D information. 

To illustrate how the caustic encode 3D information, Fig. 2 
show simulation of caustic PSFs a a function of point source 
location in object space. A lateral shift of the point source cause 
a lateral translation of the PSF. An axial shift of the point source 
cause (approximately) a scale of the PSF. Hence, each 3D 
position in the volume generates a unique PSF. The resolution 
of our camera depends on the structure and spatial frequency 
present in the caustic patterns. Because the caustic retain high 
spatial frequency over a large range of depths, DiffuserCam 
attains good lateral resolution for object at any depth within 
the volumetric field-of-view (FoV). 

By assume that all point in the scene be incoherent with 
each other, the measurement can be model a a linear combi- 
nation of PSFs from different 3D positions. We represent this a 
matrix-vector multiplication: 

b = Hv, (1) 

where b be a vector represent the 2D sensor measurement 
and v be a vector represent the intensity of the object at every 
point in the 3D FoV, sample on a user-chosen grid (discussed in 
Section 3). H be the forward model matrix whose column consist 
of each of the caustic pattern create by the correspond 3D 
point on the object grid. The number of entry in b and the 
number of row of H be equal to the number of pixel on the 



Research Article 3 

b Depth dependence of the PSF 

a Lateral dependence of the PSF 

Fig. 2. Caustic pattern shift with lateral shift of a point 
source in the scene and scale with axial shifts. (a) Ray-traced 
rendering of caustic a the point source move laterally. For 
large shift (far right), part of the pattern be clipped by the sen- 
sor. (b) The caustic approximately magnify a the source be 
brought closer. 

image sensor, but the number of column in H be set by the 
choice of reconstruction grid. Note that this model do not 
account for partial occlusion of sources. 

In order to reconstruct the 3D image, v, from the measure 
2D image, b, we must solve Eq. (1) for v. However, if we use 
the full optical resolution of our system (see Sec. 3B), v will 
exist on a grid that contains more voxels than there be pixel on 
the sensor. In this case, H have many more column than rows, 
so the problem be underdetermined and we cannot uniquely 
recover v simply by invert Eq. (1). To remedy this, we rely 
on sparsity-based computation principle [26]. We exploit the 
fact that the object can be represent in a domain in which it 
be sparse (few non-zero elements) and solve the `1 regularize 
inverse problem: 

v̂ = argmin 
v≥0 

1 
2‖b−Hv‖22 + λ‖Ψv‖1. (2) 

Here, Ψ be a linear transform that sparsifies the 3D object, v ≥ 0 
be a nonnegativity constraint, and λ be a tune parameter. For 
object that be sparse in native 3D space, such a fluorescent 
samples, we choose Ψ to be the identity matrix. For object that 
be not natively sparse, but whose gradient be sparse, we choose 
Ψ to be the finite difference operator, so ‖Ψv‖1 be the 3D Total 
Variation (TV) semi-norm [27]. 

Equation (2) be the basis pursuit problem in compress sens- 
ing [26]. For this optimization procedure to succeed, H must 
have distributed, uncorrelated columns. The diffuser creates 
caustic that spread across many pixel in a pseudorandom fash- 
ion and contain high spatial frequencies. Therefore, any shift 
or magnification of the caustic lead to a new pattern that be 
uncorrelated with the original one. As discuss in Sections 2B 
and 2C, these two property lead to a matrix that allows u to 
reconstruct 3D image v from the measurement b = Hv. 

2. METHODS 

A. System Architecture 

The hardware setup for DiffuserCam consists of a diffuser place 
at a fix distance in front a sensor (see Fig. 3a). The convex 
bump on the diffuser surface can be thought of a a set of 

randomly-spaced microlenses that have statistically vary fo- 
cal length and f-numbers. The f-number determines the mini- 
mum feature size of the caustics, which set our optical resolu- 
tion. The average focal length determines the distance at which 
the caustic have high contrast (the caustic plane), which be 
where we place the sensor [23]. 

Our prototype be built use a PCO.edge 5.5 Color camera 
(6.5µm pixels). The diffuser be an off-the-shelf engineer dif- 
fuser (Luminit 0.5◦) with a flat input surface and an output 
surface that be described statistically a Gaussian lowpass fil- 
tered white noise with an average spatial feature size of 140µm 
and average slope magnitude of 0.7◦ (details in Supplementary 
Fig. S1). This corresponds to an average focal length of approx- 
imately 8 mm and average f-number of 50. Due to the high 
f-number, the caustic maintain high contrast over a large range 
of propagation distances. Therefore, the diffuser need not be 
place precisely at the caustic plane; in our prototype, d = 8.9 
mm from our sensor. Additionally, we affix a 5.5 × 7.5 mm 
aperture directly on the textured side of the diffuser to limit the 
support of the caustics. 

Similar to a traditional camera, the pixel pitch should Nyquist 
sample the minimum feature of the caustics. The small 
feature generate by the caustic pattern be roughly twice the 
pixel pitch on our sensor, so we perform 2x2 binning on the 
data, yield 1.3 megapixel images, before apply our 3D 
reconstruction algorithm. 

B. Convolutional Forward Model 

Recovering a 3D image require know the system transmis- 
sion matrix, H, which be extremely large. Measuring or store 
the full H would be impractical, require million of calibration 
image and operating on multi-Terabyte matrices. The convo- 
lution model outline below drastically reduces complexity of 
both calibration and computation. 

We describe the object, v, a a set of point source locate 
at (x, y, z) on a non-Cartesian grid and represent the relative 
radiant power collect by the aperture from each source a 
v(x, y, z). The caustic pattern at pixel (x′, y′) on the sensor due to 
a unit-powered point source at (x, y, z) be the PSF, h(x′, y′; x, y, z). 
Thus, b(x′, y′) represent the 2D sensor measurement record 
after the light from every point in v propagates through the 
diffuser and onto the sensor. This let u explicitly write the 
matrix-vector multiplication Hv by sum over all voxels in 
the FoV: 

b(x′, y′) = ∑ 
(x,y,z) 

v(x, y, z)h(x′, y′; x, y, z). (3) 

The convolution model amount to a shift invariance (or infi- 
nite memory effect [20, 21]) assumption, which greatly simplifies 
the evaluation of Eq. (3). Consider the caustic create by point 
source at a fix distance, z, from the diffuser. Because the 
diffuser surface be slowly vary and smooth, the paraxial ap- 
proximation holds. This implies that a lateral translation of the 
source by (∆x, ∆y) lead to a lateral shift of the caustic on the 
sensor by (∆x′, ∆y′) = (m∆x, m∆y), where m be the paraxial 
magnification. We validate this behavior in both simulation 
(see Fig. 2) and experiment (see Section 3D). For notational 
convenience, we define the on-axis caustic pattern at depth z a 
h(x′, y′; z) := h(x′, y′; 0, 0, z). Thus, the off-axis caustic pattern 
be give by h(x′, y′; x, y, z) = h(x′ + mx, y′ + my; z). Plugging 



Research Article 4 

DiffuserCam 

20 mm 

3 mm 

Each box represent 
20 x 20 voxels 

x 

z 

c Experimental 2-point resolution at z = 20 mm 

d 

Axial Distance (μm) 
center 20 mm from diffuser 

N 
ot 


re 

so 
lv 

ed 
B 

ar 
el 

y 
re 

so 
lv 

ed 
C 

le 
ar 

ly 
re 

so 
lv 

ed 

Lateral Axial 

x 
y 250 um 

x 
z 

0-60 
Lateral 

Distance (μm) 

0-4480 

1 

In 
te 

n 
ity 

(a 
. u 

.) 

0 

1 

In 
te 

n 
ity 

(a 
. u 

.) 

0 

1 

In 
te 

n 
ity 

(a 
. u 

.) 

0-60 0-448 

0-60 0-448 

50 um 

50 um 

Angle of Incidence (o) 

Pi 
xe 

l R 
e 

po 
n 

e 

x: αc = 41.5 
o 

y: αc = 30 
o 

d 

l w 

α 

β 

αc 

Sensor 

DiffuserAperture 
a b 

0 30 60 90 
0 

0.2 
0.4 
0.6 
0.8 

1 
xy 

Fig. 3. Experimentally determine field-of-view (FoV) and resolution. (a) System architecture with design parameters. (b) Angular 
pixel response of our sensor. We define the angular cutoff (αc) a the angle at which the response fall to 20%. (c) Reconstructed 
image of two point (captured separately) at vary separation laterally and axially, near the z = 20 mm depth plane. Points be 
consider resolve if they be separate by a dip of at least 20%. (d) To-scale non-uniform voxel grid for 3D reconstruction, view 
from above. The voxel grid be base on the system geometry and Nyquist-sampled two-point resolution over the entire FoV. For 
visualization purposes, each box represent 20×20 voxels, a show in red. 

into Eq. (3), the sensor measurement be then give by: 

b(x′, y′) = ∑ 
z 

∑ 
(x,y) 

v(x, y, z)h(x′ + mx, y′ + my; z) 

= C ∑ 
z 

[ 
v 
(−x′ 

m 
, 
−y′ 
m 

, z 
) 
∗ h 

( 
x′, y′; z 

)] 
. 

(4) 

Here, ∗ represent 2D discrete convolution over (x′, y′), which 
return array that be large than the originals. Hence, we crop 
to the original sensor size, denote by the linear operator C. For 
an object discretized into Nz depth slices, the number of column 
of H be Nz time large than the number of element in b (i.e. the 
number of sensor pixels), so our system be underdetermined. 

The cropped convolution model provide three benefits. First, 
it allows u to compute Hv a a linear operator in term of Nz 
images, rather than instantiate H explicitly (which would re- 
quire petabyte of memory to store). In practice, we evaluate 
the sum of 2D cropped convolution use a single circular 3D 
convolution, enable the use of 3D FFTs, which scale well to 
large array size (for details, see Supplementary Sec. 2). Second, 
it provide u with a theoretical justification of our system’s ca- 
pability for sparse reconstruction. Derivations in [28] show that 
translate copy of a random pattern provide close-to-optimal 
compress sense performance. 

The third benefit of our convolution model be that it enables 
simple calibration. Rather than measure the system response 
from every voxel (millions of image per depth), we only need to 
capture a single calibration image of the caustic pattern from an 
on-axis point source at each depth plane.1 A typical calibration 

1While the scale effect described in Sec. 1A suggests that we could use only 

thus consists of capture image a a single point source be 
move axially. This take minutes, but need only be perform 
once. The add aperture on DiffuserCam ensures that a point 
source at the minimum z distance generates caustic that just fill 
the sensor, so that the entire PSF be capture in each image (see 
Supplementary Fig. S2). 

C. Inverse Algorithm 
Our inverse problem be extremely large in scale, with million of 
input and outputs. Even with the convolution model described 
above, use project gradient technique be extremely slow 
due to the time require to compute the proximal operator of 
3D total variation [29]. To alleviate this, we use the Alternating 
Direction Method of Multipliers (ADMM) [30]. We derive a 
variable splitting that leverage the specific structure of our 
problem. 

Our algorithm us the fact that Ψ can be write a a circu- 
lar convolution for both the 3D TV and native sparsity cases. 
Additionally, we factor the forward model in Eq. (4) into a di- 
agonal component, D, and a 3D convolution matrix, M, such 
that H = DM (details in Sec. ). Thus, both the forward oper- 
ator and the regularizer can be compute in 3D Fouier space. 
This enables u to use variable-splitting [31–33] to formulate the 
constrain counterpart of Eq. (2): 

v̂ = argmin 
w≥0,u,v 

1 
2‖b−Dv‖22 + λ‖u‖1 

s.t. v = Mv, u = Ψv, w = v, 
(5) 

one image for calibration and scale it to predict PSFs at different depths, we find 
that there be subtle change in the caustic structure with depth. Hence, we obtain 
good result when we calibrate with PSFs measure at each depth. 



Research Article 5 

where v, u, and w be auxiliary variables. We solve Eq. (5) 
by follow the commonly-used augment Lagrangian argu- 
ments [34]. Using ADMM, this result in the follow scheme 
at iteration k: 

uk+1 ← T λ 
µ2 

( 
Ψvk + ηk 

/ 
µ2 

) 
vk+1 ← (DᵀD + µ1 I)−1 

( 
ξk + µ1Mvk + Dᵀb 

) 
wk+1 ← max 

( 
ρk 
/ 

µ3 + vk, 0 
) 

vk+1 ← (µ1MᵀM + µ2ΨᵀΨ + µ3 I)−1 rk 

ξk+1 ← ξk + µ1(Mvk+1 − vk+1) 
ηk+1 ← ηk + µ2(Ψvk+1 − uk+1) 
ρk+1 ← ρk + µ3(vk+1 − wk+1), 

where 

rk = (µ3wk+1 − ρk) + Ψᵀ 
( 

µ2uk+1 − ηk 
) 
+ Mᵀ 

( 
µ1vk+1 − ξk 

) 
. 

Note that Tν be a vectorial soft-thresholding operator with a 
threshold value of ν [35], and ξ, η and ρ be the Lagrange multi- 
plier associate with v, u, and w, respectively. The scalar µ1, µ2 
and µ3 be penalty parameter which we compute automatically 
use the tune strategy in [30]. 

Although our algorithm involves two large-scale matrix in- 
versions, both can be compute efficiently and in close form. 
Since D be diagonal, (DᵀD + µ1 I) be itself diagonal, require 
complexity O(n) to invert use point-wise multiplication. Ad- 
ditionally, all three matrix in (µ1MᵀM + µ2ΨᵀΨ + µ3 I) be 
diagonalize by the 3D discrete Fourier transform (DFT) matrix, 
so inversion of the entire term can be do use point-wise di- 
vision in 3D frequency space. Therefore, it inversion have good 
computational complexity, O(n3 log n), since it be dominate by 
two 3D FFTs be apply to n3 total voxels. We parallelize 
our algorithm on the CPU use C++ and Halide [36], a high 
performance program language specialized for image pro- 
cessing (A comparison of regularizers and runtimes be show in 
Supplementary Sec. 2c). 

A typical reconstruction require at least 200 iterations. Solv- 
ing for 2048 × 2048 × 128 = 537 million voxels take 26 
minute (8 second per iteration) on a 144-core workstation 
and require 85 Gigabytes of RAM. A small reconstruction 
(512× 512× 128 = 33.5 million voxels) take 3 minute (1 sec- 
ond per iteration) on a 4-core laptop with 16 Gigabytes of RAM. 

3. SYSTEM ANALYSIS 

Unlike traditional cameras, the performance of computational 
camera depends on property of the scene be image (e.g. 
the number of sources). As a consequence, standard two-point 
resolution metric may be misleading, a they do not predict 
resolve power for more complex objects. To address this, we 
propose a new local condition number metric that good predicts 
performance. We analyze resolution, FoV and the validity of the 
convolution model, then combine these analysis to determine 
the appropriate sample grid for faithfully reconstruct real- 
world objects. 

A. Field-of-View 
At every depth in the volume, the angular half-FoV be deter- 
mine by the most extreme lateral position that contributes 
to the measurement. There be two possible limit factors. 

y 

x 300 um 

e max 

0 

max 

0 
100 um 

z 

x 

y 

x 

100 um 

300 um 

f 

c d 

a b 

Separation 
Δx = 45 um, Δz = 336 um 

Separation 
Δx = 75 um, Δz = 448 um 

R 
ec 

on 
st 

ru 
ct 

ed 


2 
po 

in 
t 


R 

ec 
on 

st 
ru 

ct 
ed 


16 

p 
oi 

nt 
s 

R 
aw 

D 
at 

a 
16 

p 
oi 

nt 
s 

Fig. 4. Our computational camera have object-dependent per- 
formance, such that the resolution depends on the number 
of points. (a) To illustrate, we show here a situation with two 
point successfully resolve at the two-point resolution limit 
(∆x, ∆z) = (45µm, 336µm) at a depth of approximately 20 
mm. (c) However, when the object consists of more point 
(16 point in a 4×4 grid in the x − z plane) at the same spac- 
ing, the reconstruction fails. (b,d) Increasing the separation to 
(∆x, ∆z) = (75µm, 448µm) give successful reconstructions. 
(e,f) A close-up of the raw data show noticeable splitting of 
the caustic line for the 16 point case, make the point dis- 
tinguishable. Heuristically, the 16 point resolution cutoff be a 
good indicator of resolution for real-world objects. 

The first be the geometric angular cutoff, α, set by the aperture 
size, w, the sensor size, l, and the distance from the diffuser to 
the sensor, d (see Fig. 3a). Since the diffuser bend light, we 
also take into account the diffuser’s maximum deflection angle, 
β. This give a geometric angular half-FoV at every depth of 
l + w = 2d tan(α− β). The second limit factor be the angular 
response of the sensor pixels. Real-world sensor pixel may not 
accept light at the high angle of incidence that our lensless cam- 
era accepts, so the sensor angular response (shown in Fig. 3b) 
may limit the FoV. Defining the angular cutoff of the sensor, αc, 
a the angle at which the camera response fall to 20% of it 
on-axis value, we can write the overall FoV equation as: 

FoV = β + min[αc, tan−1( l+w2d )]. (6) 

Since we image in 3D, we must also consider the axial FoV. 
In practice, the axial FoV be limited by the range of calibrate 
depths. However, the system geometry creates bound on possi- 
ble calibration locations. Calibration cannot be arbitrarily close 
to the sensor since the caustic would exceed the sensor size. 
To account for this, we impose a minimum object distance such 
that an on-axis point source creates caustic that fill the sensor. 
Theoretically, our system can capture source infinitely far away, 



Research Article 6 

but the axial resolution degrades with depth. The hyperfocal 
plane represent the axial distance after which no more axial 
resolution be available, establish an upper bound. Objects 
beyond the hyperfocal focal plane have no depth information, 
but can be reconstruct to create 2D image for photographic 
application [37], without any hardware modifications. 

In our prototype, the axial FoV range from the minimum cali- 
bration distance (7.3 mm) to the hyperfocal plane (2.3 m). The an- 
gular FoV be limited by the pixel angular acceptance (αc = 41.5◦ 

in x, αc = 30◦ in y). Combined with our diffuser’s maximum 
deflection angle (β = 0.5◦) this yield an angular FoV of ±42◦ 
in x and ±30.5◦ in y. We validate the FoV experimentally by 
capture a scene at optical infinity and measure the angular 
extent of the result (see Supplementary Fig. S3). 

B. Resolution 
Investigating optical resolution be critical for both quantify 
system performance and choose our reconstruction grid. Al- 
though the raw data be collect on a fix sensor grid, we can 
choose the non-uniform 3D reconstruction grid arbitrarily. The 
choice of reconstruction grid be important. When the grid be cho- 
sen with voxels that be too large, resolution be lost, and when 
they be too small, extra computation be perform without reso- 
lution gain. In this section we explain how to choose the grid of 
voxels for our reconstructions, with the aim of Nyquist sample 
the two-point optical resolution limit. 

B.1. Two-point resolution 

A common metric for resolution analysis in traditional camera 
be two-point distinguishablity. We measure our system’s two- 
point resolution by image scene contain two point source 
at different separation distances, built by sum together im- 
age of a single point source (1µm pinhole, wavelength 532nm) 
at two different locations. 

We reconstruct the scene use our algorithm, with λ = 0 
to remove the influence of the regularizer. To ensure best-case 
resolution, we use the full 5 MP sensor data (no 2× 2 binning). 
The point source be consider distinguishable if the recon- 
struction have a dip of at least 20% between the sources, a in the 
Rayleigh criterion. Figure 3c show reconstruction with point 
source separate both laterally and axially. 

Our system have highly non-isotropic resolution (Fig. 3d), but 
we can use our model to predict the two-point distinguishabil- 
ity over the entire volume from localize experiments. Due to 
the shift invariance assumption, the lateral resolution be con- 
stant within a single depth plane and the paraxial magnification 
cause the lateral resolution to vary linearly with depth. For 
axial resolution, the main difference between two point source 
be the size of their PSF supports. We find pair of depth such 
that the difference in their support width be constant: 

c = 1z1 − 
1 
z2 . (7) 

Here, z1 and z2 be neighbor depth and c be a constant deter- 
mine experimentally. 

Based on this model, we set the voxel space in our grid to 
Nyquist sample the 3D two-point resolution. Figure 3d show 
a to-scale map of the result voxel grid. Axial resolution 
degrades with distance until it reach the hyperfocal plane 
(∼2.3 m from the camera), beyond which no depth information 
be recoverable. Due to the non-telecentric nature of the system, 
the voxel size be a function of depth, with the densest sample 
occur close to the camera. Objects within 5 cm of the camera 

can be reconstruct with somewhat isotropic resolution; this be 
where we place object in practice. 

B.2. Multi-point resolution 

In a traditional camera, resolution be a function of the system 
and be independent of the scene. In contrast, computational 
camera that use nonlinear reconstruction algorithm may incur 
degradation of the effective resolution a the scene complexity 
increases. To demonstrate this in our system, we consider a more 
complex scene consist of 16 point sources. Figure 4 show 
experiment use 16 point source arrange in a 4×4 grid in 
the (x, z) plane at two different spacings. The first space be 
set to match the measure two-point resolution limit (∆x=45µm, 
∆z=336µm). Despite be able to separate two point at this 
spacing, we cannot resolve all 16 sources. However, if we in- 
crease the source separation to (∆x=75µm, ∆z=448µm), all 16 
point be distinguishable (Fig. 4d). In this example, the usable 
lateral resolution of the system degrades by approximately 1.7× 
due to the increase scene complexity. As we show in Section 
3C, the resolution loss do not become arbitrarily bad a the 
scene complexity increases. 

This experiment demonstrates that exist resolution metric 
cannot be blindly use to determine performance of computa- 
tional camera like ours. How can we then analyze resolution 
if it depends on object properties? In the next section, we intro- 
duce a general theoretical framework for assess resolution in 
computational cameras. 

C. Local condition number theory 
Our goal be to provide new theory that describes how the effec- 
tive reconstruction resolution of computational camera change 
with object complexity. To do so, we introduce a numerical 
analysis of our forward model to determine how well it can be 
inverted. 

First, note that recover the image v from the measurement 
b = Hv comprises simultaneous estimation of the location 
of all nonzeros within our image reconstruction, v, a well a 
the value at each nonzero location. To simplify the problem, 
suppose an oracle tell u the exact location of every source 
within the 3D scene. This corresponds to know a priori the 
support of v, so we then need only determine the value of the 
nonzero element in v. This can be accomplish by solve 
a least square problem use a sub-matrix consist of only 
the column of H that correspond to the index of the nonzero 
voxels. If this problem fails, then the more difficult problem 
of simultaneously determine the nonzero location and their 
value will certainly fail. 

In practice, the measurement be corrupt by noise. The 
maximal effect this noise can have on the least-squares estimate 
of the nonzero value be determine by the condition number 
of the sub-matrix described above. We therefore say that the 
reconstruction problem be ill-posed if any sub-matrices of H be 
very ill-conditioned. In practice, ill-conditioned matrix result 
in increase noise sensitivity and longer reconstruction times, 
a more iteration be need to converge to a solution. 

The worst-case scenario in our system be when multiple 
source be in a contiguous block, since nearby measurement 
be always most similar. Therefore, we compute the condition 
number of sub-matrices of H correspond to a group of point 
source with separation vary by integer number of voxels. 
We repeat this calculation for different number of sources. The 
result be show in Fig. 5. As expected, the conditioning be 
bad when source be closer together. In this case, increase 



Research Article 7 

x 
y 

Δ 

Δ 

30 μm 

a b 

0 100 200 300 400 
0 

50 

100 

150 
2 
50 
100 
150 
200 

Number of 
source 

250 

seperation distance (μm) 
C 

on 
di 

tio 
n 

N 
um 

be 
r 

Fig. 5. Our local condition number theory show how resolu- 
tion varies with object complexity. (a) Virtual point source 
be simulated on a fix grid and move by integer number 
of voxels to change the separation distance. (b) Local condi- 
tion number be plot for sub-matrices correspond to 
grid of neighbor point source with vary separation 
(at depth 20 mm from the sensor). As the number of source 
increases, the condition number approach a limit, indicate 
that resolution for complex object can be approximate by a 
limited number (but more than two) sources. 

noise sensitivity mean that even small amount of noise could 
prevent u from resolve the sources. This trend match what 
we saw experimentally in Figs. 3 and 4. 

Figure 5 show that the local condition number increase with 
the number of source in the scene, a expected. This mean that 
resolution will degrade a more and more source be added. 
We can see in Fig. 5 that, a the number of source increase 
linearly, the conditioning approach a limit case. Hence, 
the resolution do not become arbitrarily bad with increase 
number of sources. Therefore we can estimate the system reso- 
lution for complex object from distiguishability measurement 
of a limited number of point sources. This be experimentally 
validate in Section 4, where we find that the experimental 16- 
point resolution be a good predictor of the resolution for a USAF 
target. 

Unlike traditional resolution metrics, our new local condition 
number theory explains the resolution loss we observe experi- 
mentally. We believe that it be sufficiently general to be applica- 
ble to other computational cameras, which likely exhibit similar 
performance loss. 

D. Validity of the Convolution Model 

In Section 2B, we model the caustic pattern a shift invariant 
at every depth, lead to simple calibration and efficient com- 
putation. Since our convolution model be an approximation, we 
wish to quantify it applicability. Figure 6a-c show register 
close-ups of experimentally measure PSFs from plane wave 
incident at 0◦, 15◦ and 30◦. The convolution model assumes that 
these register PSFs be all exactly the same, though, in reality, 
they have subtle differences. To quantify the similarity across 
the FoV, we plot the inner product between each off-axis PSF 
and the on-axis PSF (see Fig. 6d). The inner product be great 
than 75% across the entire FoV and particularly good within 
±15◦ of the optical axis, indicate that the convolution model 
hold relatively well. 

To investigate how the spatial variance of the PSF impact sys- 
tem performance, we use the peak width of the cross-correlation 
between the on-axis and off-axis PSFs to approximate the spot 

0 

1a PSF at 0O c PSF at 30Ob PSF at 15O 

-37 -20 0 20 37 
Field Position (degrees) 

0 

1 

2 

3 

4 

5 

6 

N 
or 

m 
al 

iz 
ed 

sp 
ot 

si 
ze 

Off-Axis Resolutione 

-37 -20 0 20 37 
Field Position (degrees) 

0 

0.2 

0.4 

0.6 

0.8 

1 

In 
ne 

r P 
ro 

du 
ct 

S 
im 

ila 
rit 

y 

d PSF Similarity 
convolution 
model 
exhuastive 
calibration 

Fig. 6. Experimental validation of the convolution model. (a)- 
(c) Close-ups of register experimental PSFs for source at 0◦, 
15◦ and 30◦. The PSF at 15◦ be visually similar to that on-axis, 
while the PSF at 30◦ have subtle differences. (d) Inner product 
between the on-axis PSF and register off-axis PSFs a a func- 
tion of source position. (e) Resulting spot size (normalized by 
on-axis spot). The convolution model hold well up to ±15◦, 
beyond which resolution degrades (solid). Exhaustive calibra- 
tion would improve the resolution (dashed), at the expense of 
complexity in computation and calibration. 

size off-axis. Figure 6e (solid) show that we retain the on-axis 
resolution up to ±15◦. Beyond that, the resolution gradually 
degrades. To avoid model mismatch, one could replace the con- 
volution model with exhaustive calibration over all position 
in the FoV. This procedure would yield high resolution at the 
edge of the FoV, a show by the dash line in Fig. 6e. The 
gap between these line be what we sacrifice in resolution by 
use the convolution model. However, in return, we gain sim- 
plified calibration and efficient computation, which make the 
large-scale problem feasible. 

4. EXPERIMENTAL RESULTS 

Images of two object be present in Fig. 7. Both be illu- 
minated use white LEDs and reconstruct with a 3D TV 
regularizer. We choose a reconstruction grid that approximately 
Nyquist sample the two-point resolution (by 2× 2 binning the 
sensor pixel to yield a 1.3 megapixel measurement). Calibra- 
tion image be take at 128 different z-planes, range from 
z = 10.86mm to z = 36.26mm (from the diffuser), with space 
set accord to condition outline in Sec. 3B. The 3D image 
be reconstruct on a 2048×2048×128 grid, but the angular 
FoV restricts the usable portion of this grid to the center 100 
million voxels. Note that the resolvable feature size on this 
reconstruction grid can still vary base on object complexity. 

The first object be a negative USAF 1951 fluorescence test 
target, tilt 45◦ about the y-axis (Fig. 7a). Slices of the recon- 
structed volume at different z plane be show to highlight the 
system’s depth section capabilities. As described in Sec. 3B, 
the spatial scale change with depth. Analyzing the resolution in 
the vertical direction (Fig. 7a inset), we can easily resolve group 
2 element 4 and barely resolve group 2 element 5 at z = 24 



Research Article 8 

b 
Side 

10 mm 

Top 

10 mm 

Front 

1 mm 

Raw dataa 

Z = 19.7 mm 

2 mm 

Z = 27.2 mm 

2 mm 

Z = 25.2 mm 

2 mm 

Z = 24 mm 

2 mm 

Max projection0 

1 

2 mm 

2 mm 

Raw data 

Group 2 

2 

3 

4 
5 
6 

el 
em 

en 
t 

Fig. 7. Experimental 3D reconstructions. (a) Tilted resolution target, which be reconstruct on a 4.2 MP lateral grid with 128 
z-planes and cropped to 640×640×50 voxels. The large panel show the max projection over z. Note that the spatial scale be not 
isotropic. Inset be a magnification of group 2 with an intensity cutline, show that we resolve element 5 at a distance of 24 mm, 
which corresponds to a feature size of 79 µm (approximately twice the lateral voxel size of 35µm at this depth). The degrade reso- 
lution match our 16-point distinguishability (75 µm at 20 mm depth). Lower panel show depth slice from the recover volume. 
(b) Reconstruction of a small plant, cropped to 480×320×128 voxels, render from multiple angles. 

mm. This corresponds to resolve feature 79µm apart on the 
resolution target. This resolution be significantly bad than the 
two-point resolution at this depth (50µm), but similar to the 
16-point resolution (75µm). Hence, we reinforce our claim that 
two-point resolution be a mislead metric for computational 
cameras, but multi-point distinguishability can be extend to 
more complex objects. 

Finally, we demonstrate the ability of DiffuserCam to image 
natural object by reconstruct a small plant (Fig. 7b). Multiple 
angle be render to demonstrate the ability to capture the 3D 
structure of the leaves. 

5. CONCLUSION 

We demonstrate a simple optical system, with only a diffuser 
in front of a sensor, that be capable of single-shot 3D imaging. 
The diffuser encodes the 3D location of point source in caus- 
tic patterns, which allow u to apply compress sense to 
reconstruct more voxels than we have measurements. By use 
a convolution model that assumes that the caustic pattern be 
shift invariant at every depth, we developed an efficient ADMM 
algorithm for image recovery and simple calibration scheme. We 
characterize the FoV and two-point resolution of our system, 
and show how resolution varies with object complexity. This 
motivate the introduction of a new condition number analysis, 
which we use to analyze how inverse problem conditioning 
change with object complexity. 

ACKNOWLEDGMENTS 

This work be fund by the Gordon and Betty Moore Founda- 
tion Grant GBMF4562 and a NSF CAREER grant to L.W. B.M. 
acknowledges funding from the Hertz Foundation and G. K. be 
a National Defense Science and Engineering Graduate Fellow. 
R.H. and E.B. acknowledge funding from the Swiss NSF (grants 
P2EZP2 159065 and P2ELP2 172278, respectively). This research 

be developed with funding from the Defense Advanced Re- 
search Projects Agency (DARPA), Contract No. N66001-17-C- 
4015. The views, opinion and/or finding express be those 
of the author and should not be interpret a represent the 
official view or policy of the Department of Defense or the 
U.S. Government. The author thank Dr. Eric Jonas and the Rice 
FlatCam team for helpful discussions. 

REFERENCES 

1. M. S. Asif, A. Ayremlou, A. C. Sankaranarayanan, A. Veer- 
araghavan, and R. G. Baraniuk, “Flatcam: Thin, bare-sensor 
camera use cod aperture and computation,” CoRR 
abs/1509.00116 (2015). 

2. D. G. Stork and P. R. Gill, “Optical, mathematical, and com- 
putational foundation of lensless ultra-miniature diffractive 
imagers and sensors,” International Journal on Advances in 
Systems and Measurements 7, 4 (2014). 

3. P. R. Gill, J. Tringali, A. Schneider, S. Kabir, D. G. Stork, 
E. Erickson, and M. Kellam, “Thermal escher sensors: Pixel- 
efficient lensless imagers base on tile optics,” in “Compu- 
tational Optical Sensing and Imaging,” (Optical Society of 
America, 2017), pp. CTu3B–3. 

4. R. Fergus, A. Torralba, and W. T. Freeman, “Random Lens 
Imaging,” Tech. rep., Massachusetts Institute of Technology 
(2006). 

5. A. Stylianou and R. Pless, “Sparklegeometry: Glitter image 
for 3d point tracking,” in “Proceedings of the IEEE Confer- 
ence on Computer Vision and Pattern Recognition Work- 
shops,” (2016), pp. 10–17. 

6. J. Tanida, T. Kumagai, K. Yamada, S. Miyatake, K. Ishida, 
T. Morimoto, N. Kondou, D. Miyazaki, and Y. Ichioka, “Thin 
observation module by bound optic (tombo): concept and 
experimental verification,” Applied Optics 40, 1806–1813 
(2001). 



Research Article 9 

7. W. Harm, C. Roider, A. Jesacher, S. Bernet, and M. Ritsch- 
Marte, “Lensless image through thin diffusive media,” Op- 
tic Express 22, 22146–22156 (2014). 

8. W. Chi and N. George, “Optical image with phase-coded 
aperture,” Optics express 19, 4294–4300 (2011). 

9. D. J. Brady, K. Choi, D. L. Marks, R. Horisaki, and S. Lim, 
“Compressive holography,” Opt. Express 17, 13040–13049 
(2009). 

10. K. Lee and Y. Park, “Exploiting the speckle-correlation scat- 
tering matrix for a compact reference-free holographic image 
sensor,” Nature Communications 7 (2016). 

11. W. Bishara, T.-W. Su, A. F. Coskun, and A. Ozcan, “Lensfree 
on-chip microscopy over a wide field-of-view use pixel 
super-resolution,” Optics Express 18, 11181–11191 (2010). 

12. H. Faulkner and J. Rodenburg, “Movable aperture lensless 
transmission microscopy: a novel phase retrieval algorithm,” 
Physical Review Letters 93, 023903 (2004). 

13. R. Ng, M. Levoy, M. Bredif, G. Duval†, M. Horowitz, and 
P. Hanrahan, “Light field photography with a hand-held 
plenoptic camera,” Stanford University Computer Science 
Tech Report pp. 3418–3421 (2005). 

14. R. Horisaki, S. Irie, Y. Ogura, and J. Tanida, “Three- 
dimensional information acquisition use a compound 
image system,” Optical Review 14, 347–350 (2007). 

15. K.Tajima, T. Shimano, Y. Nakamura, M. Sao, and 
T. Hoshizawa, “Lensless light-field image with multi- 
phase fresnel zone aperture,” in “2017 IEEE International 
Conference on Computational Photography (ICCP),” (2017), 
pp. 76–82. 

16. M. Levoy, R. Ng, A. Adams, M. Footer, and M. Horowitz, 
“Light Field Microscopy,” ACM Trans. Graph. (Proc. SIG- 
GRAPH) 25 (2006). 

17. M. Broxton, L. Grosenick, S. Yang, N. Cohen, A. Andalman, 
K. Deisseroth, and M. Levoy, “Wave optic theory and 3-D 
deconvolution for the light field microscope,” Optics Express 
21, 25418–25439 (2013). 

18. H.-Y. Liu, E. Jonas, L. Tian, J. Zhong, B. Recht, and L. Waller, 
“3d image in volumetric scatter medium use phase- 
space measurements,” Opt. Express 23, 14461–14471 (2015). 

19. N. C. Pégard, H.-Y. Liu, N. Antipa, M. Gerlock, H. Adesnik, 
and L. Waller, “Compressive light-field microscopy for 3d 
neural activity recording,” Optica 3, 517–524 (2016). 

20. O. Katz, P. Heidmann, M. Fink, and S. Gigan, “Non-invasive 
single-shot image through scatter layer and around 
corner via speckle correlations,” Nature Photonics 8, 784– 
790 (2014). 

21. E. Edrei and G. Scarcelli, “Memory-effect base deconvolu- 
tion microscopy for super-resolution image through scat- 
tering media,” Scientific Reports 6 (2016). 

22. A. K. Singh, D. N. Naik, G. Pedrini, M. Takeda, and W. Os- 
ten, “Looking through a diffuser and around an opaque sur- 
face: A holographic approach,” Optics Express 22, 7694–7701 
(2014). 

23. N. Antipa, S. Necula, R. Ng, and L. Waller, “Single-shot 
diffuser-encoded light field imaging,” in “2016 IEEE Interna- 
tional Conference on Computational Photography (ICCP),” 
(2016), pp. 1–11. 

24. Y. Kashter, A. Vijayakumar, and J. Rosen, “Resolving image 
by blurring: superresolution method with a scatter mask 
between the observe object and the hologram recorder,” 
Optica 4, 932–939 (2017). 

25. A. Liutkus, D. Martina, S. Popoff, G. Chardon, O. Katz, 
G. Lerosey, S. Gigan, L. Daudet, and I. Carron, “Imaging with 

nature: Compressive image use a multiply scatter 
medium,” Scientific report 4 (2014). 

26. E. J. Candès and M. B. Wakin, “An introduction to compres- 
sive sampling,” IEEE Signal Processing Magazine 25, 21–30 
(2008). 

27. L. I. Rudin, S. Osher, and E. Fatemi, “Nonlinear total varia- 
tion base noise removal algorithms,” Physica D: Nonlinear 
Phenomena 60, 259–268 (1992). 

28. F. Krahmer, S. Mendelson, and H. Rauhut, “Suprema of 
chaos process and the restrict isometry property,” Com- 
mun. Pur. Appl. Math. 67, 1877–1904 (2014). 

29. A. Beck and M. Teboulle, “Fast gradient-based algorithm 
for constrain total variation image denoising and deblur- 
ring problems,” IEEE Transactions on Image Processing 18, 
2419–2434 (2009). 

30. S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Dis- 
tributed optimization and statistical learn via the alter- 
nating direction method of multipliers,” Foundations and 
Trends in Machine Learning 3, 1–122 (2011). 

31. M. S. C. Almeida and M. Figueiredo, “Deconvolving image 
with unknown boundary use the alternate direction 
method of multipliers,” IEEE Transactions on Image process- 
ing 22, 3074–3086 (2013). 

32. A. Matakos, S. Ramani, and J. A. Fessler, “Accelerated edge- 
preserve image restoration without boundary artifacts,” 
IEEE Transactions on Image Processing 22, 2019–2029 (2013). 

33. M. V. Afonso, J. M. Bioucas-Dias, and M. A. T. Figueiredo, 
“Fast image recovery use variable splitting and constrain 
optimization,” IEEE Transactions on Image Processing 19, 
2345–2356 (2010). 

34. J. Nocedal and S. J. Wright, Numerical Optimization (Springer, 
2006). 

35. Y. Wang, J. Yang, W. Yin, and Y. Zhang, “A new alternate 
minimization algorithm for total variation image reconstruc- 
tion,” SIAM Journal on Imaging Sciences 1, 248–272 (2008). 

36. J. Ragan-Kelley, C. Barnes, A. Adams, S. Paris, F. Durand, 
and S. Amarasinghe, “Halide: a language and compiler for 
optimize parallelism, locality, and recomputation in image 
processing pipelines,” ACM SIGPLAN Notices 48, 519–530 
(2013). 

37. G. Kuo, N. Antipa, R. Ng, and L. Waller, “Diffusercam: 
Diffuser-based lensless cameras,” in “Computational Optical 
Sensing and Imaging,” (Optical Society of America, 2017), 
pp. CTu3B–2. 


1 Introduction 
A System Overview 

2 Methods 
A System Architecture 
B Convolutional Forward Model 
C Inverse Algorithm 

3 System Analysis 
A Field-of-View 
B Resolution 
B.1 Two-point resolution 
B.2 Multi-point resolution 

C Local condition number theory 
D Validity of the Convolution Model 

4 Experimental Results 
5 Conclusion 

