




















































Neural Adaptive Video Streaming with Pensieve 


Neural Adaptive Video Streaming with Pensieve 
Hongzi Mao, Ravi Netravali, Mohammad Alizadeh 

MIT Computer Science and Artificial Intelligence Laboratory 
{hongzi,ravinet,alizadeh}@mit.edu 

ABSTRACT 
Client-side video player employ adaptive bitrate (ABR) algorithm 
to optimize user quality of experience (QoE). Despite the abundance 
of recently propose schemes, state-of-the-art ABR algorithm suffer 
from a key limitation: they use fix control rule base on simplify 
or inaccurate model of the deployment environment. As a result, 
exist scheme inevitably fail to achieve optimal performance 
across a broad set of network condition and QoE objectives. 

We propose Pensieve, a system that generates ABR algorithm 
use reinforcement learn (RL). Pensieve train a neural network 
model that selects bitrates for future video chunk base on obser- 
vations collect by client video players. Pensieve do not rely 
on pre-programmed model or assumption about the environment. 
Instead, it learns to make ABR decision solely through observation 
of the result performance of past decisions. As a result, Pensieve 
automatically learns ABR algorithm that adapt to a wide range of 
environment and QoE metrics. We compare Pensieve to state-of-the- 
art ABR algorithm use trace-driven and real world experiment 
span a wide variety of network conditions, QoE metrics, and 
video properties. In all consider scenarios, Pensieve outperforms 
the best state-of-the-art scheme, with improvement in average QoE 
of 12%–25%. Pensieve also generalizes well, outperform exist 
scheme even on network for which it be not explicitly trained. 

CCS Concepts: Information systems→Multimedia streaming; Networks 
→ Network resource allocation; Computing methodologies→ Reinforce- 
ment learn 

Keywords: bitrate adaptation, video streaming, reinforcement learn 
ACM Reference format: Hongzi Mao, Ravi Netravali, Mohammad Alizadeh 
MIT Computer Science and Artificial Intelligence Laboratory. 2017. Neural 
Adaptive Video Streaming with Pensieve. In Proceedings of SIGCOMM ’17, 
August 21-25, 2017, Los Angeles, CA, USA, 14 pags. 
DOI: http://dx.doi.org/10.1145/3098822.3098843 

1 INTRODUCTION 
Recent year have see a rapid increase in the volume of HTTP- 
base video stream traffic [7, 39]. Concurrent with this increase 
have be a steady rise in user demand on video quality. Many study 
have show that user will quickly abandon video session if the 
quality be not sufficient, lead to significant loss in revenue for 

Permission to make digital or hard copy of all or part of this work for personal or 
classroom use be grant without fee provide that copy be not make or distribute 
for profit or commercial advantage and that copy bear this notice and the full citation 
on the first page. Copyrights for component of this work own by others than the 
author(s) must be honored. Abstracting with credit be permitted. To copy otherwise, or 
republish, to post on server or to redistribute to lists, require prior specific permission 
and/or a fee. Request permission from permissions@acm.org. 
SIGCOMM ’17, Los Angeles, CA, USA 
© 2017 Copyright held by the owner/author(s). Publication right license to ACM. 
978-1-4503-4653-5/17/08. . . $15.00 
DOI: http://dx.doi.org/10.1145/3098822.3098843 

content provider [12, 25]. Nevertheless, content provider continue 
to struggle with deliver high-quality video to their viewers. 

Adaptive bitrate (ABR) algorithm be the primary tool that con- 
tent provider use to optimize video quality. These algorithm run 
on client-side video player and dynamically choose a bitrate for 
each video chunk (e.g., 4-second block). ABR algorithm make bi- 
trate decision base on various observation such a the estimate 
network throughput and playback buffer occupancy. Their goal be 
to maximize the user’s QoE by adapt the video bitrate to the 
underlie network conditions. However, select the right bitrate 
can be very challenge due to (1) the variability of network through- 
put [18, 42, 49, 52, 53]; (2) the conflict video QoE requirement 
(high bitrate, minimal rebuffering, smoothness, etc.); (3) the cascad- 
ing effect of bitrate decision (e.g., select a high bitrate may 
drain the playback buffer to a dangerous level and cause rebuffering 
in the future); and (4) the coarse-grained nature of ABR decisions. 
We elaborate on these challenge in §2. 

The majority of exist ABR algorithm (§7) develop fix con- 
trol rule for make bitrate decision base on estimate network 
throughput (“rate-based” algorithm [21, 42]), playback buffer size 
(“buffer-based” scheme [19, 41]), or a combination of the two 
signal [26]. These scheme require significant tune and do not 
generalize to different network condition and QoE objectives. The 
state-of-the-art approach, MPC [51], make bitrate decision by solv- 
ing a QoE maximization problem over a horizon of several future 
chunks. By optimize directly for the desire QoE objective, MPC 
can perform good than approach that use fix heuristics. How- 
ever, MPC’s performance relies on an accurate model of the system 
dynamics—particularly, a forecast of future network throughput. 
As our experiment show, this make MPC sensitive to throughput 
prediction error and the length of the optimization horizon (§3). 

In this paper, we propose Pensieve,1 a system that learns ABR 
algorithm automatically, without use any pre-programmed con- 
trol rule or explicit assumption about the operating environment. 
Pensieve us modern reinforcement learn (RL) technique [27, 
30, 43] to learn a control policy for bitrate adaptation purely through 
experience. During training, Pensieve start know nothing about 
the task at hand. It then gradually learns to make good ABR de- 
cisions through reinforcement, in the form of reward signal that 
reflect video QoE for past decisions. 

Pensieve’s learn technique mine information about the actual 
performance of past choice to optimize it control policy for the 
characteristic of the network. For example, Pensieve can learn how 
much playback buffer be necessary to mitigate the risk of rebuffering 
in a specific network, base on the network’s inherent throughput 
variability. Or it can learn how much to rely on throughput versus 
buffer occupancy signals, or how far into the future to plan it deci- 
sion automatically. By contrast, approach that use fix control 

1A pensieve be a device use in Harry Potter [38] to review memories. 



SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA H. Mao et al. 

rule or simplify network model be unable to optimize their bi- 
trate decision base on all available information about the operating 
environment. 

Pensieve represent it control policy a a neural network that 
map “raw” observation (e.g., throughput samples, playback buffer 
occupancy, video chunk sizes) to the bitrate decision for the next 
chunk. The neural network provide an expressive and scalable way 
to incorporate a rich variety of observation into the control policy.2 

Pensieve train this neural network use A3C [30], a state-of-the-art 
actor-critic RL algorithm. We describe the basic training algorithm 
and present extension that allow a single neural network model to 
generalize to video with different properties, e.g., the number of 
encoding and their bitrates (§4). 

To train it models, Pensieve us simulation over a large corpus 
of network traces. Pensieve us a fast and simple chunk-level simu- 
lator. While Pensieve could also train use packet-level simulations, 
emulations, or data collect from live video client (§6), the chunk- 
level simulator be much faster and allows Pensieve to “experience” 
100 hour of video downloads in only 10 minutes. We show that Pen- 
sieve’s simulator faithfully model video stream with live video 
players, provide that the transport stack be configure to achieve 
close to the true network capacity (§4.1). 

We evaluate Pensieve use a full system implementation (§4.4). 
Our implementation deploys Pensieve’s neural network model on an 
ABR server, which video client query to get the bitrate to use for the 
next chunk; client request include observation about throughput, 
buffer occupancy, and video properties. This design remove the 
burden of perform neural network computation on video clients, 
which may have limited computation power, e.g., TVs, mobile de- 
vices, etc. (§6). 

We compare Pensieve to state-of-the-art ABR algorithm use 
a broad set of network condition (both with trace-based emulation 
and in the wild) and QoE metric (§5.2). We find that in all con- 
sidered scenarios, Pensieve rival or outperforms the best exist 
scheme, with average QoE improvement range from 12%–25%. 
Additionally, our result show Pensieve’s ability to generalize to 
unseen network condition and video properties. For example, on 
both broadband and HSDPA networks, Pensieve be able to outper- 
form all exist ABR algorithm by training solely with a synthetic 
dataset. Finally, we present result which highlight Pensieve’s low 
overhead and lack of sensitivity to system parameters, e.g., in the 
neural network (§5.4). 

2 BACKGROUND 
HTTP-based adaptive stream (standardized a DASH [2]) be the 
predominant form of video delivery today. By transmit video 
use HTTP, content provider be able to leverage exist CDN 
infrastructure and maintain simplify (stateless) backends. Further, 
HTTP be compatible with a multitude of client-side application such 
a web browser and mobile applications. 

In DASH systems, video be store on server a multiple chunks, 
each of which represent a few second of the overall video playback. 
Each chunk be encode at several discrete bitrates, where a high 

2A few prior scheme [6, 8, 9, 47] have apply RL to video streaming. But these 
scheme use basic “tabular” RL approach [43]. As a result, they must rely on simplify 
network model and perform poorly in real network conditions. We discus these 
scheme further in §5.4 and §7. 

Video 
Server 

CDN 

Throughput 
Predictor 

Playback 
Buffer 

ABR 
Controller 

Rendered 
video chunk 

Chunk 
Info 

Throughput Estimate 

Buffer Occupancy 

Video Player 

Figure 1: An overview of HTTP adaptive video streaming. 

bitrate implies a high quality and thus a large chunk size. Chunks 
across bitrates be align to support seamless quality transitions, 
i.e., a video player can switch to a different bitrate at any chunk 
boundary without fetch redundant bit or skip part of the 
video. 

Figure 1 illustrates the end-to-end process of stream a video 
over HTTP today. As shown, a player embed in a client applica- 
tion first sends a token to a video service provider for authentication. 
The provider responds with a manifest file that directs the client 
to a CDN host the video and list the available bitrates for the 
video. The client then request video chunk one by one, use an 
adaptive bitrate (ABR) algorithm. These algorithm use a variety of 
different input (e.g., playback buffer occupancy, throughput mea- 
surements, etc.) to select the bitrate for future chunks. As chunk 
be downloaded, they be played back to the client; note that play- 
back of a give chunk cannot begin until the entire chunk have be 
downloaded. 

Challenges: The policy employ by ABR algorithm heavily 
influence video stream performance. However, these algorithm 
face four primary practical challenges: 
(1) Network condition can fluctuate over time and can vary signifi- 

cantly across environments. This complicates bitrate selection 
a different scenario may require different weight for input 
signals. For example, on time-varying cellular links, throughput 
prediction be often inaccurate and cannot account for sudden fluc- 
tuations in network bandwidth—inaccurate prediction can lead 
to underutilized network (lower video quality) or inflate down- 
load delay (rebuffering). To overcome this, ABR algorithm 
must prioritize more stable input signal like buffer occupancy 
in these scenarios. 

(2) ABR algorithm must balance a variety of QoE goal such a 
maximize video quality (i.e., high average bitrate), mini- 
mizing rebuffering event (i.e., scenario where the client’s play- 
back buffer be empty), and maintain video quality smoothness 
(i.e., avoid constant bitrate fluctuations). However, many of 
these goal be inherently conflict [3, 18, 21]. For example, 
on network with limited bandwidth, consistently request 
chunk encode at the high possible bitrate will maximize 
quality, but may increase rebuffer rates. Conversely, on vary- 
ing networks, choose the high bitrate that the network can 
support at any time could lead to substantial quality fluctuation, 
and hence degrade smoothness. To further complicate matters, 
preference among these QoE factor vary significantly across 
user [23, 31, 32, 34]. 

(3) Bitrate selection for a give chunk can have cascade effect 
on the state of the video player. For example, select a high 
bitrate may deplete the playback buffer and force subsequent 



Neural Adaptive Video Streaming with Pensieve SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA 

buffer 

ABR agent 

state 

Neural Network bitrates 
240P 
480P 
720P 
1080P 

QoE metricReward 

Client-side network and video player measurement 

bandwidth 

bit rate 

720P 

Figure 2: Applying reinforcement learn to bitrate adapta- 
tion. 

chunk to be download at low bitrates (to avoid rebuffering). 
Additionally, a give bitrate selection will directly influence the 
next decision when smoothness be considered—ABR algorithm 
will be less inclined to change bitrates. 

(4) The control decision available to ABR algorithm be coarse- 
grain a they be limited to the available bitrates for a give 
video. Thus, there exist scenario where the estimate through- 
put fall just below one bitrate, but well above the next available 
bitrate. In these cases, the ABR algorithm must decide whether 
to prioritize high quality or the risk of rebuffering. 

3 LEARNING ABR ALGORITHMS 
In this paper, we consider a learning-based approach to generate 
ABR algorithms. Unlike approach which use preset rule in the 
form of fine-tuned heuristics, our technique attempt to learn an 
ABR policy from observations. Specifically, our approach be base on 
reinforcement learn (RL). RL considers a general set in which 
an agent interacts with an environment. At each time step t , the agent 
observes some state st , and chooses an action at . After apply 
the action, the state of the environment transition to st+1 and the 
agent receives a reward rt . The goal of learn be to maximize 
the expect cumulative discount reward: E 

[∑∞ 
t=0 γ 

t rt 
] 
, where 

γ ∈ (0, 1] be a factor discounting future rewards. 
Figure 2 summarizes how RL can be apply to bitrate adaptation. 

As shown, the decision policy guide the ABR algorithm be not 
handcrafted. Instead, it be derive from training a neural network. The 
ABR agent observes a set of metric include the client playback 
buffer occupancy, past bitrate decisions, and several raw network 
signal (e.g., throughput measurements) and feed these value to the 
neural network, which output the action, i.e., the bitrate to use for 
the next chunk. The result QoE be then observe and pass back 
to the ABR agent a a reward. The agent us the reward information 
to train and improve it neural network model. More detail about 
the specific training algorithm we use be provide in §4.2. 

To motivate learning-based ABR algorithms, we now provide 
two example where exist technique that rely on fix heuristic 
can perform poorly. We choose these example for illustrative pur- 
poses. We do not claim that they be indicative of the performance 
gain with learn in realistic network scenarios. We perform thor- 
ough quantitative evaluation compare learning-generated ABR 
algorithm to exist scheme in §5.2. 

In these examples, we compare RL-generated ABR algorithm to 
MPC [51]. MPC us both throughput estimate and observation 
about buffer occupancy to select bitrates that maximize a give QoE 
metric across a future chunk horizon. Here we consider robustMPC, 
a version of MPC that be configure to use a horizon of 5 chunks, 

and a conservative throughput estimate which normalizes the de- 
fault throughput prediction with the max prediction error over the 
past 5 chunks. As the MPC paper shows, and our result validate, 
robustMPC’s conservative throughput prediction significantly im- 
prof performance over default MPC, and achieves a high level 
of performance in most case (§5.2). However, heuristic like ro- 
bustMPC’s throughput prediction require careful tune and can 
backfire when their design assumption be violated. 

Example 1: The first example considers a scenario in which the 
network throughput be highly variable. Figure 3a compare the net- 
work throughput specify by the input trace with the throughput 
estimate use by robustMPC. As shown, robustMPC’s estimate 
be overly cautious, hover around 2 Mbps instead of the average 
network throughput of roughly 4.5 Mbps. These inaccurate through- 
put prediction prevent robustMPC from reach high bitrates even 
though the occupancy of the playback buffer continually increases. 
In contrast, the RL-generated algorithm be able to properly ass 
the high average throughput (despite fluctuations) and switch to the 
high available bitrate once it have enough cushion in the playback 
buffer. The RL-generated algorithm consider here be train on 
a large corpus of real network trace (§5.1), not the synthetic trace 
in this experiment. Yet, it be able to make the appropriate decision. 

Example 2: In our second example, both robustMPC and the RL- 
generate ABR algorithm optimize for a new QoE metric which be 
gear towards user who strongly prefer HD video. This metric 
assigns high reward to HD bitrates and low reward to all other bitrates 
(details in Table 1), while still favor smoothness and penalize 
rebuffering. To optimize for this metric, an ABR algorithm should 
attempt to build the client’s playback buffer to a high enough level 
such that the player can switch up to and maintain an HD bitrate level. 
Using this approach, the video player can maximize the amount of 
time spent stream HD video, while minimize rebuffering time 
and bitrate transitions. However, perform well in this scenario 
require long term planning since at any give instant, the penalty of 
select a high bitrate (HD or not) may be incur many chunk 
in the future when the buffer cannot support multiple HD downloads. 

Figure 3b illustrates the bitrate selection make by each of these 
algorithms, and the effect that these decision have on the playback 
buffer. Note that robustMPC and the RL-generated algorithm be 
both configure to optimize for this new QoE metric. As shown, 
robustMPC be unable to apply the aforementioned policy. Instead, 
robustMPC maintains a medium-sized playback buffer and request 
chunk at bitrates that fall between the low level (300 kbps) 
and the low HD level (1850 kbps). The reason be that, despite 
be tune to consider a horizon of future chunk at every step, 
robustMPC fails to plan far enough into the future. In contrast, 
the RL-generated ABR algorithm be able to actively implement 
the policy outline above. It quickly grows the playback buffer by 
request chunk at 300 kbps, and then immediately jump to the 
HD quality of 1850 kbps; it be able to then maintain this level for 
nearly 80 seconds, thereby ensure quality smoothness. 

Summary: robustMPC have difficulty (1) factor throughput fluc- 
tuations and prediction error into it decisions, and (2) choose the 
appropriate optimization horizon. These deficiency exist because 
MPC lack an accurate model of network dynamics—thus it relies on 



SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA H. Mao et al. 

0 

1 

2 

3 

4 
Bi 
t r 
at 
e 
(M 

bp 
s) 

Pensieve robustMPC 

0 

20 

40 

0 3 0 6 0 9 0 1 2 0 

Bu 
ffe 

r s 
ize 

(s 
ec 
) Pensieve robustMPC 

1 

3 

5 

7 

9 

0 30 60 90 120 

Th 
ro 
ug 
hp 

ut 
(M 

bp 
s) 

Time (sec) 

True bandwidth robustMPC estimation 

(a) Synthetic network. 

0 
0.4 
0.8 
1.2 
1.6 
2 

Bi 
t r 
at 
e 
(M 

bp 
s) 

Pensieve robustMPC 

0 

20 

40 

0 4 0 8 0 1 2 0 1 6 0 

Bu 
ffe 

r s 
ize 

(s 
ec 
) Pensieve robustMPC 

0.3 

0.6 

0.9 

1.2 

1.5 

0 40 80 120 160 

Th 
ro 
ug 
hp 

ut 
(M 

bp 
s) 

Time (sec) 

True bandwidth robustMPC estimation 

(b) HSDPA network. 

Figure 3: Profiling bitrate selections, buffer occupancy, and throughput estimate with robustMPC [51] and Pensieve. 

simple and sub-optimal heuristic such a conservative throughput 
prediction and a small optimization horizon. More generally, any 
ABR algorithm that relies on fix heuristic or simplify system 
model suffers from these limitations. By contrast, RL-generated 
algorithm learn from actual performance result from different 
decisions. By incorporate this information into a flexible neural 
network policy, RL-generated ABR algorithm can automatically 
optimize for different network characteristic and QoE objectives. 

4 DESIGN 
In this section, we describe the design and implementation of Pen- 
sieve, a system that generates RL-based ABR algorithm and applies 
them to video stream sessions. We start by explain the training 
methodology (§4.1) and algorithm (§4.2) underlie Pensieve. We 
then describe an enhancement to the basic training algorithm, which 
enables Pensieve to support different video use a single model 
(§4.3). Finally, we explain the implementation detail of Pensieve 
and how it applies learn model to real stream session (§4.4). 

4.1 Training Methodology 
The first step of Pensieve be to generate an ABR algorithm use 
RL (§3). To do this, Pensieve run a training phase in which the 
learn agent explores a video stream environment. Ideally, 
training would occur use actual video stream clients. However, 
emulate the standard video stream environment entail use a 
web browser to continually download video chunks. This approach 
be slow, a the training algorithm must wait until all of the chunk in 
a video be completely download before update it model. 

To accelerate this process, Pensieve train ABR algorithm in a 
simple simulation environment that faithfully model the dynamic 
of video stream with real client applications. Pensieve’s simulator 
maintains an internal representation of the client’s playback buffer. 
For each chunk download, the simulator assigns a download time 
that be solely base on the chunk’s bitrate and the input network 

throughput traces. The simulator then drain the playback buffer by 
the current chunk’s download time, to represent video playback dur- 
ing the download, and add the playback duration of the download 
chunk to the buffer. The simulator carefully keep track of rebuffer- 
ing event that arise a the buffer occupancy changes, i.e., scenario 
where the chunk download time exceeds the buffer occupancy at 
the start of the download. In scenario where the playback buffer 
cannot accommodate video from an additional chunk download, 
Pensieve’s simulator pause request for 500 m before retrying.3 

After each chunk download, the simulator pass several state obser- 
vations to the RL agent for processing: the current buffer occupancy, 
rebuffering time, chunk download time, size of the next chunk (at 
all bitrates), and the number of remain chunk in the video. We 
describe how this input be use by the RL agent in more detail in 
§4.2. Using this chunk-level simulator, Pensieve can “experience” 
100 hour of video downloads in only 10 minutes. 

Though model the application layer semantics of client video 
player be straightforward, faithful simulation be complicate by 
intricacy at the transport layer. Specifically, video player may 
not request future chunk a soon a a chunk download completes, 
e.g., because the playback buffer be full. Such delay can trigger 
the underlie TCP connection to revert to slow start, a behavior 
know a slow-start-restart [4]. Slow start may in turn prevent the 
video player from fully use the available bandwidth, particularly 
for small chunk size (low bitrates). This behavior make simulation 
challenge a it inherently tie network throughput to the ABR 
algorithm be used, e.g., scheme that fill buffer quickly will 
experience more slow start phase and thus less network utilization. 

To verify this behavior, we load the test video described in §5.1 
over an emulate 6 Mbps link use four ABR algorithms, each of 
which continually request chunk at a single bitrate. We load the 
video with each scheme twice, both with slow-start-restart enable 

3This be the default request retry rate use by DASH player [2]. 



Neural Adaptive Video Streaming with Pensieve SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA 

3 

4 

5 

6 

0 30 60 90 

Th 
ro 
ug 
hp 

ut 
(M 

bp 
s) 

Time (sec) 

2.85 Mbps 1.2 Mbps 0.75 Mbps 0.3Mbps 

(a) TCP slow start restart enable 

5 

6 

0 30 60 90Th 
ro 
ug 
hp 

ut 
(M 

bp 
s) 

Time (sec) 

2.85 Mbps 1.2 Mbps 0.75 Mbps 0.3 Mbps 

(b) TCP slow start restart disabled 

Figure 4: Profiling the throughput usage per-chunk of commod- 
ity video player with and without TCP slow start restart. 

and disabled.4 Figure 4 show the throughput usage during chunk 
downloads for each bitrate in both scenarios. As shown, with slow- 
start-restart enabled, the throughput depends on the bitrate of the 
chunk; ABR algorithm use low bitrates (smaller chunk sizes) 
achieve less throughput per chunk. However, throughput be consistent 
and match the available bandwidth (6 Mbps) for different bitrates 
if we disable slow-start-restart. 

Pensieve’s simulator assumes that the throughput specify by 
the trace be entirely use by each chunk download. As the above 
result show, this can be achieve by disable slow-start-restart on 
the video server. Disabling slow-start-restart could increase traffic 
burstiness, but recent standard effort be tackle the same problem 
for video stream more gracefully by pace the initial burst from 
TCP follow an idle period [13, 17]. 

While it be possible to use a more accurate simulator (e.g., packet- 
level) to train Pensieve, in the end, no simulation can capture all 
real world system artifact with 100% accuracy. However, we find 
that Pensieve can learn very high quality ABR algorithm (§5.2) 
use imperfect simulations, a long a it experience a large enough 
variety of network condition during training. This be a consequence 
of Pensieve’s strong generalization ability (§5.3). 

4.2 Basic Training Algorithm 
We now describe our training algorithms. As show in Figure 5, 
Pensieve’s training algorithm us A3C [30], a state-of-the-art actor- 
critic method which involves training two neural networks. The 
detailed functionality of these network be explain below. 

Inputs: After the download of each chunk t , Pensieve’s learn 
agent take state input st = ( ~xt , ~τt , ~nt ,bt , ct , lt ) to it neural net- 
works. ~xt be the network throughput measurement for the past k 
video chunks; ~τt be the download time of the past k video chunks, 
which represent the time interval of the throughput measurements; 
~nt be a vector of m available size for the next video chunk; bt be 
the current buffer level; ct be the number of chunk remain in the 
video; and lt be the bitrate at which the last chunk be downloaded. 
4In Linux, the net.ipv4.tcp_slow_start_after_idle parameter can be 
use to set this configuration. 

xt xt-1 

n1 n2 nm 

bt 

ct 

lt 

Past chunk throughput 

Next chunk size 

Current buffer size 

Number of chunk left 

Last chunk bit rate 

State st 
Actor network 

Critic network 

policy 
πθ(st, at) 

value 
vπθ(st) 

1D-CNN 

1D-CNN 

1D-CNN 

1D-CNN 

τt τt-1 

Past chunk download time 

1D-CNN 

1D-CNN 

xt-k+1 

τt-k+1 

Figure 5: The Actor-Critic algorithm that Pensieve us to gen- 
erate ABR policy (described in §4.4). 

Policy: Upon receive st , Pensieve’s RL agent need to take an 
action at that corresponds to the bitrate for the next video chunk. 
The agent selects action base on a policy, define a a probability 
distribution over action π : π (st ,at ) → [0, 1]. π (st ,at ) be the 
probability that action at be take in state st . In practice, there be 
intractably many {state, action} pairs, e.g., throughput estimate and 
buffer occupancy be continuous real numbers. To overcome this, 
Pensieve us a neural network (NN) [15] to represent the policy 
with a manageable number of adjustable parameters, θ , which we 
refer to a policy parameters. Using θ , we can represent the policy 
a πθ (st ,at ). NNs have recently be apply successfully to solve 
large-scale RL task [27, 29, 40]. An advantage of NNs be that they 
do not need hand-crafted feature and can be apply directly to 
“raw” observation signals. The actor network in Figure 5 depicts how 
Pensieve us an NN to represent an ABR policy. We describe how 
we design the specific architecture of the NN in §5.3. 

Policy gradient training: After apply each action, the simulated 
environment provide the learn agent with a reward rt for that 
chunk. Recall from §3 that the primary goal of the RL agent be 
to maximize the expect cumulative (discounted) reward that it 
receives from the environment. Thus, the reward be set to reflect the 
performance of each chunk download accord to the specific QoE 
metric we wish to optimize. See §5 for example of QoE metrics. 

The actor-critic algorithm use by Pensieve to train it policy be 
a policy gradient method [44]. We highlight the key step of the 
algorithm, focus on the intuition. The key idea in policy gradient 
method be to estimate the gradient of the expect total reward 
by observe the trajectory of execution obtain by follow 
the policy. The gradient of the cumulative discount reward with 
respect to the policy parameters, θ , can be compute a [30]: 

∇θEπθ 
 

∞∑ 
t=0 

γ t rt 

 
= Eπθ 

[ 
∇θ logπθ (s,a)Aπθ (s,a) 

] 
. (1) 

Aπθ (s,a) be the advantage function, which represent the difference 
in the expect total reward when we deterministically pick action 
a in state s, compare with the expect reward for action drawn 
from policy πθ . The advantage function encodes how much good a 
specific action be compare to the “average action” take accord 
to the policy. 



SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA H. Mao et al. 

In practice, the agent sample a trajectory of bitrate decision and 
us the empirically compute advantage A(st ,at ), a an unbiased 
estimate of Aπθ (st ,at ). Each update of the actor network parameter 
θ follow the policy gradient, 

θ ← θ + α 
∑ 
t 
∇θ logπθ (st ,at )A(st ,at ), (2) 

where α be the learn rate. The intuition behind this update rule be a 
follows. The direction ∇θ logπθ (st ,at ) specifies how to change the 
policy parameter in order to increase πθ (st ,at ) (i.e., the probability 
of action at at state st ). Equation 2 take a step in this direction. The 
size of the step depends on the value of the advantage for action at 
in state st . Thus, the net effect be to reinforce action that empirically 
lead to good returns. 

To compute the advantage A(st ,at ) for a give experience, we 
need an estimate of the value function, vπθ (s )—the expect total 
reward start at state s and follow the policy πθ . The role of 
the critic network in Figure 5 be to learn an estimate of vπθ (s ) from 
empirically observe rewards. We follow the standard Temporal 
Difference method [43] to train the critic network parameter θv , 

θv ← θv − α ′ 
∑ 
t 
∇θv 

( 
rt + γV 

πθ (st+1;θv ) −V πθ (st ;θv ) 
)2 
, (3) 

where V πθ (·;θv ) be the estimate of vπθ (·), output by the critic net- 
work, and α ′ be the learn rate for the critic. For an experience 
(st ,at , rt , st+1) (i.e., take action at in state st , receive reward rt , and 
transition to st+1), the advantage A(st ,at ) can now be estimate a 
rt + γV 

πθ (st+1;θv ) −V πθ (st ;θv ). See [24] for more details. 
It be important to note that the critic network merely help to train 

the actor network. Post-training, only the actor network be require 
to execute the ABR algorithm and make bitrate decisions. 

Finally, we must ensure that the RL agent explores the action 
space adequately during training to discover good policies. One 
common practice to encourage exploration be to add an entropy 
regularization term to the actor’s update rule [30]; this can be crit- 
ical in help the learn agent converge to a good policy [50]. 
Concretely, we modify Equation 2 to be, 

θ ← θ + α 
∑ 
t 
∇θ logπθ (st ,at )A(st ,at ) + β∇θH (πθ (·|st )), (4) 

where H (·) be the entropy of the policy (the probability distribution 
over actions) at each time step. This term encourages exploration 
by push θ in the direction of high entropy. The parameter β be 
set to a large value at the start of training (to encourage exploration) 
and decrease over time to emphasize improve reward (§4.4). 

The detailed derivation and pseudocode can be found in [30] (§4 
and Algorithm S3). 

Parallel training: To further enhance and speed up training, Pen- 
sieve spawn multiple learn agent in parallel, a suggest by the 
A3C paper [30]. By default, Pensieve us 16 parallel agents. Each 
learn agent be configure to experience a different set of input 
parameter (e.g., network traces). However, the agent continually 
send their {state, action, reward} tuples to a central agent, which 
aggregate them to generate a single ABR algorithm model. For 
each sequence of tuples that it receives, the central agent us the 
actor-critic algorithm to compute a gradient and perform a gradient 
descent step (Equations (3) and (4)). The central agent then update 
the actor network and push out the new model to the agent which 

n1 
n2 
0 

n3 

Actor network 

p1 
p2 
0 

p3 
0 

Softm 
ax 

Mask 
(1 1 0 1 0) 

0N 
ex 

t c 
hu 

nk 
s 

iz 
e 

xtbtctlt τt 

Figure 6: Modification to the state input and the softmax output 
to support multiple videos. 

sent that tuple. Note that this can happen asynchronously among all 
agents, i.e., there be no lock between agent [36]. 

Choice of algorithm: A variety of different algorithm could be 
use to train the learn agent in the abstract RL framework de- 
scribed above (e.g., DQN [29], REINFORCE [44], etc.). In our 
design, we chose to use A3C [30] because (1) to the best of our 
knowledge, it be the state-of-art and it have be successfully apply 
to many other concrete learn problem [20, 48, 50]; and (2) in 
the video stream application, the asynchronous parallel training 
framework support online training in which many user concur- 
rently send their experience feedback to the agent. We also compare 
Pensieve with previous tabular Q-learning scheme [6] in §5.4. 

4.3 Enhancement for multiple video 
The basic algorithm described in §4.2 have some practical issues. The 
primary challenge be that video can be encode at different bitrate 
level and may have diverse chunk size due to variable bitrate en- 
cod [41], e.g., chunk size for 720p video be not identical across 
videos. Handling this variation would require each neural network 
to take a variable size set of input and produce a variable size set 
of outputs. The naive solution to support a broad range of video 
be to train a model for each possible set of video properties. Unfortu- 
nately, this solution be not scalable. To overcome this, we describe 
two enhancement to the basic algorithm that enable Pensieve to 
generate a single model to handle multiple video (Figure 6). 

First, we pick canonical input and output format that span the 
maximum number of bitrate level we expect to see in practice. For 
example, a range of 13 level cover the entire DASH reference 
client video list [11]. Then, to determine the input state for a specific 
video, we take the chunk size and map them to the index which have 
the closest bitrate. The remain input states, which pertain to the 
bitrates that the video do not support, be zeroed out. For example, 
in Figure 6, chunk size (n1,n2,n3) be mapped to the correspond 
indices, while the remain input value be fill with zeroes. 

The second change pertains to how the output of the actor net- 
work be interpreted. For a give video, we apply a mask to the output 
of the final softmax [5] layer in the actor network, such that the 
output probability distribution be only over the bitrates that the video 
actually supports. Formally, the mask be present by a 0-1 vec- 
tor [m1,m2, ...,mk ], and the modify softmax for the NN output 
[z1, z2, ..., zk ] will be 

pi = 
mie 

zi∑ 
jmje 

zj , (5) 

where pi be the normalize probability for action i. With this mod- 
ification, the output probability be still a continuous function of 



Neural Adaptive Video Streaming with Pensieve SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA 

the network parameters. The reason be that the mask value {mi } 
be independent of the network parameters, and be only a function 
of the input video. As a result, the standard back-propagation of 
the gradient in the NN still hold and the training technique estab- 
lished in §4.2 can be apply without modification. We evaluate the 
effectiveness of these modification in more detail in §5.4. 

4.4 Implementation 
To generate ABR algorithms, Pensieve pass k = 8 past bandwidth 
measurement to a 1D convolution layer (CNN) with 128 filters, 
each of size 4 with stride 1. Next chunk size be pass to another 
1D-CNN with the same shape. Results from these layer be then 
aggregate with other input in a hidden layer that us 128 neuron 
to apply the softmax function (Figure 5). The critic network us 
the same NN structure, but it final output be a linear neuron (with 
no activation function). During training, we use a discount factor 
γ = 0.99, which implies that current action will be influence by 
100 future steps. The learn rate for the actor and critic be con- 
figure to be 10−4 and 10−3, respectively. Additionally, the entropy 
factor β be control to decay from 1 to 0.1 over 105 iterations. We 
keep all these hyperparameters fix throughout our experiments. 
While some tune be useful, we found that Pensieve performs well 
for a wide range of hyperparameter values. Thus we do not use 
sophisticated hyperparameter tune method [14]. We implement 
this architecture use TensorFlow [1]. For compatibility, we lever- 
age the TFLearn deep learn library’s TensorFlow API [46] to 
declare the neural network during both training and testing. 

Once Pensieve have generate an ABR algorithm use it simula- 
tor, it must apply the model’s rule to real video stream sessions. 
To do this, Pensieve run on a standalone ABR server, implement 
use the Python BaseHTTPServer. Client request be modify 
to include additional information about the previous chunk down- 
load and the video be stream (§4.2). By collect information 
through client requests, Pensieve’s server and ABR algorithm can 
remain stateless while still benefitting from observation that can 
solely be collect in client video players. As client request for indi- 
vidual chunk arrive at the video server, Pensieve feed the provide 
observation through it actor NN model and responds to the video 
client with the bitrate level to use for the next chunk download; the 
client then contact the appropriate CDN to fetch the correspond 
chunk. It be important to note that Pensieve’s ABR algorithm could 
also operate directly inside video players. We evaluate the overhead 
that a server-side deployment have on video QoE in §5.4, and discus 
other deployment model in more detail in §6. 

5 EVALUATION 
In this section, we experimentally evaluate Pensieve. Our experi- 
ments cover a broad set of network condition (both trace-based 
and in the wild) and QoE metrics. Our result answer the follow 
questions: 
(1) How do Pensieve compare to state-of-the-art ABR algorithm 

in term of video QoE? We find that, in all of the consider 
scenarios, Pensieve be able to rival or outperform the best ex- 
isting scheme, with average QoE improvement range from 
12.1%–24.6% (§5.2); Figure 7 provide a summary. 

(2) Do model learn by Pensieve generalize to new network con- 
ditions and videos? We find that Pensieve’s ABR algorithm be 
able to maintain high level of performance both in the presence 
of new network condition and new video property (§5.3). 

(3) How sensitive be Pensieve to various parameter such a the neu- 
ral network architecture and the latency between the video client 
and ABR server? Our experiment suggest that performance 
be largely unaffected by these parameter (Tables 2 and 3). For 
example, apply 100 m RTT value between client and the 
Pensieve server reduces average QoE by only 3.5% (§5.4). 

5.1 Methodology 

Network traces: To evaluate Pensieve and state-of-the-art ABR 
algorithm on realistic network conditions, we create a corpus of 
network trace by combine several public datasets: a broadband 
dataset provide by the FCC [10] and a 3G/HSDPA mobile dataset 
collect in Norway [37]. The FCC dataset contains over 1 million 
throughput traces, each of which log the average throughput over 
2100 seconds, at a 5 second granularity. We generate 1000 trace 
for our corpus, each with a duration of 320 seconds, by concatenate 
randomly select trace from the “Web browsing” category in the 
August 2016 collection. The HSDPA dataset comprises 30 minute 
of throughput measurements, generate use mobile device that 
be stream video while in transit (e.g., via bus, train, etc.). To 
match the duration of the FCC trace include in our corpus, we 
generate 1000 trace (each span 320 seconds) use a slide 
window across the HSDPA dataset. To avoid scenario where bitrate 
selection be trivial, i.e., situation where pick the maximum bitrate 
be always the optimal solution, or where the network cannot support 
any available bitrate for an extend period, we only consider 
original trace whose average throughput be less than 6 Mbps, and 
whose minimum throughput be above 0.2 Mbps. We reformatted 
throughput trace from both datasets to be compatible with the 
Mahimahi [33] network emulation tool. Unless otherwise noted, 
we use a random sample of 80% of our corpus a a training set 
for Pensieve; we use the remain 20% a a test set for all ABR 
algorithms. All in all, our test set comprises of over 30 hour of 
network traces. 

Adaptation algorithms: We compare Pensieve to the follow 
algorithm which collectively represent the state-of-the-art in bitrate 
adaptation: 
(1) Buffer-Based (BB): mimic the buffer-based algorithm described 

by Huang et al. [19] which us a reservoir of 5 second and a 
cushion of 10 seconds, i.e., it selects bitrates with the goal of 
keep the buffer occupancy above 5 seconds, and automati- 
cally chooses the high available bitrate if the buffer occupancy 
exceeds 15 seconds. 

(2) Rate-Based (RB): predicts throughput use the harmonic mean 
of the experienced throughput for the past 5 chunk downloads. 
It then selects the high available bitrate that be below the 
predict throughput. 

(3) BOLA [41]: us Lyapunov optimization to select bitrates solely 
consider buffer occupancy observations. We use the BOLA 
implementation in dash.js [2]. 

(4) MPC [51]: us buffer occupancy observation and throughput 
prediction (computed in the same way a RB) to select the 



SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA H. Mao et al. 

bitrate which maximizes a give QoE metric over a horizon of 
5 future chunks. 

(5) robustMPC [51]: us the same approach a MPC, but account 
for error see between predict and observe throughput by 
normalize throughput estimate by the max error see in the 
past 5 chunks. 

Note: MPC involves solve an optimization problem for each bitrate 
decision which maximizes the QoE metric over the next 5 video 
chunks. The MPC [51] paper describes a method, fastMPC, which 
precomputes the solution to this optimization problem for a quan- 
tized set of input value (e.g., buffer size, throughput prediction, etc.). 
Because the implementation of fastMPC be not publicly available, 
we implement MPC use our ABR server a follows. For each 
bitrate decision, we solve the optimization problem exactly on the 
ABR server by enumerate all possibility for the next 5 chunks. 
We found that the computation take at most 27 m for 6 bitrate 
level and have negligible impact on QoE. 

Experimental setup: We modify dash.js (version 2.4) [2] to sup- 
port each of the aforementioned state-of-the-art ABR algorithms. 
For Pensieve and both variant of MPC, dash.js be configure 
to fetch bitrate selection decision from an ABR server that im- 
plemented the correspond algorithm. ABR server ran on the 
same machine a the client, and request to these server be 
make use XMLHttpRequests. All other algorithm ran directly 
in dash.js. The DASH player be configure to have a playback 
buffer capacity of 60 seconds. Our evaluation use the “Envivio- 
Dash3” video from the DASH-246 JavaScript reference client [11]. 
This video be encode by the H.264/MPEG-4 codec at bitrates 
in {300, 750, 1200, 1850, 2850, 4300} kbps (which pertain to video 
mode in {240, 360, 480, 720, 1080, 1440}p). Additionally, the video 
be divide into 48 chunk and have a total length of 193 seconds. 
Thus, each chunk represent approximately 4 second of video 
playback. In our setup, the client video player be a Google Chrome 
browser (version 53) and the video server (Apache version 2.4.7) 
ran on the same machine a the client. We use Mahimahi [33] to 
emulate the network condition from our corpus of network traces, 
along with an 80 m RTT, between the client and server. Unless 
otherwise noted, all experiment be perform on Amazon EC2 
t2.2xlarge instances. 

QoE metrics: There exists significant variance in user preference 
for video stream QoE [23, 31, 32, 34]. Thus, we consider a 
variety of QoE metrics. We start with the general QoE metric use 
by MPC [51], which be define a 

QoE = 
N∑ 
n=1 

q(Rn ) − µ 
N∑ 
n=1 

Tn − 
N−1∑ 
n=1 

����� 
q(Rn+1) − q(Rn ) 

����� 
(6) 

for a video with N chunks. Rn represent the bitrate of chunkn and 
q(Rn ) map that bitrate to the quality perceive by a user. Tn repre- 
sent the rebuffering time that result from download chunkn at 
bitrate Rn , while the final term penalizes change in video quality to 
favor smoothness. 

We consider three choice of q(Rn ): 
(1) QoEl in : q(Rn ) = Rn . This metric be use by MPC [51]. 
(2) QoEloд : q(Rn ) = log(R/Rmin ). This metric capture the notion 

that, for some users, the marginal improvement in perceive 
quality decrease at high bitrates and be use by BOLA [41]. 

Name bitrate utility (q (R )) rebuffer 
penalty (µ) 

QoEl in R 4.3 
QoEloд log (R/Rmin ) 2.66 

QoEhd 
0.3→1, 0.75→2, 1.2→3 81.85→12, 2.85→15, 4.3→20 

Table 1: The QoE metric we consider in our evaluation. Each 
metric be a variant of Equation 6. 

(3) QoEhd : This metric favor High Definition (HD) video. It as- 
sign a low quality score to non-HD bitrates and a high quality 
score to HD bitrates. 

The exact value of q(Rn ) for our baseline video be provide in 
Table 1. In this section, we report the average QoE per chunk, i.e., 
the total QoE metric divide by the number of chunk in the video. 

5.2 Pensieve vs. Existing ABR algorithm 
To evaluate Pensieve, we compare it with state-of-the-art ABR 
algorithm on each QoE metric list in Table 1. In each experiment, 
Pensieve’s ABR algorithm be train to optimize for the consider 
QoE metric, use the entire training corpus described in §5.1; both 
MPC variant be also modify to optimize for the consider 
QoE metric. For comparison, we also present result for the offline 
optimal scheme, which be compute use dynamic program with 
complete future throughput information. The offline optimal serf 
a an (unattainable) upper bound on the QoE that an omniscient 
policy with complete and perfect knowledge of the future network 
throughput could achieve. 

Figure 7 show the average QoE that each scheme achieves on 
our entire test corpus. Figures 8 and 9 provide more detailed result 
in the form of full CDFs for each network. There be three key take- 
aways from these results. First, we find that Pensieve either match 
or exceeds the performance of the best exist ABR algorithm 
on each QoE metric and network considered. The closest compet- 
ing scheme be robustMPC; this show the importance of tuning, a 
without robustMPC’s conservative throughput estimates, MPC can 
become too aggressive (relying on the playback buffer) and perform 
bad than even a naive rate-based scheme. For QoEl in , which be 
consider in the MPC paper [51], the average QoE for Pensieve be 
15.5% high than robustMPC on the FCC broadband network traces. 
The gap between Pensieve and robustMPC widens to 18.9% and 
24.6% for QoEloд and QoEhd . The result be qualitatively similar 
for the Norway HSDPA network traces. 

Second, we observe that the performance of exist ABR algo- 
rithms struggle to optimize for different QoE objectives. The reason 
be that these algorithm employ fix control laws, even though op- 
timizing for different QoE objective require inherently different 
ABR strategies. For example, for QoEloд , since the marginal im- 
provement in user-perceived quality diminishes at high bitrates, 
the optimal strategy be to avoid jumping to high bitrate level when 
the risk of rebuffering be high. However, to optimize for QoEl in , 
the ABR algorithm need to be more aggressive. Pensieve be able 
to automatically learn these policy and thus, performance with 
Pensieve remains consistently high a condition change. 

The result for QoEhd further illustrate this point. Recall that 
QoEhd favor HD video, assign the high utility to the top three 
bitrates available for our test video (see Table 1). As discuss in 
§3, optimize for QoEhd require longer term planning than the 



Neural Adaptive Video Streaming with Pensieve SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA 

0 

0.2 

0.4 

0.6 

0.8 

1 

QoE_lin QoE_log QoE_hd 

No 
rm 

al 
ize 

d 
av 
er 
ag 
e 
Q 
oE 

Buffer-based Rate-based BOLA MPC robustMPC Pensieve 

(a) FCC broadband dataset 

0 

0.2 

0.4 

0.6 

0.8 

1 

QoE_lin QoE_log QoE_hd 

No 
rm 

al 
ize 

d 
Av 
er 
ag 
e 
Q 
oE 

Buffer-based Rate-based BOLA MPC robustMPC Pensieve 

(b) Norway HSDPA dataset 

Figure 7: Comparing Pensieve with exist ABR algorithm on broadband and 3G/HSDPA networks. The QoE metric consider 
be present in Table 1. Results be normalize against the performance of Pensieve. Error bar span ± one standard deviation from 
the average. 

0 

0.5 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

Average QoE 

Buffer-based 
Rate-based 
BOLA 
MPC 
robustMPC 
Pensieve 
Offline optimal 

(a) QoEl in 

0 

0.5 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

Average QoE 

Buffer-based 
Rate-based 
BOLA 
MPC 
robustMPC 
Pensieve 
Offline optimal 

(b) QoEloд 

0 

0.5 

1 

-1 2 5 8 11 14 

CD 
F 

Average QoE 

Buffer-based 
Rate-based 
BOLA 
MPC 
robustMPC 
Pensieve 
Offline optimal 

(c) QoEhd 
Figure 8: Comparing Pensieve with exist ABR algorithm on the QoE metric list in Table 1. Results be collect on the FCC 
broadband dataset. Average QoE value be list for each ABR algorithm. 

0 

0.5 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

Average QoE 

Buffer-based 
Rate-based 
BOLA 
MPC 
robustMPC 
Pensieve 
Offline optimal 

(a) QoEl in 

0 

0.5 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

Average QoE 

Buffer-based 
Rate-based 
BOLA 
MPC 
robustMPC 
Pensieve 
Offline optimal 

(b) QoEloд 

0 

0.5 

1 

-1 2 5 8 11 14 

CD 
F 

Average QoE 

Buffer-based 
Rate-based 
BOLA 
MPC 
robustMPC 
Pensieve 
Offline optimal 

(c) QoEhd 
Figure 9: Comparing Pensieve with exist ABR algorithm on the QoE metric list in Table 1. Results be collect on the 
Norway HSDPA dataset. Average QoE value be list for each ABR algorithm. 

other two QoE metrics. When network bandwidth be inadequate, 
the ABR algorithm should build the playback buffer a quickly a 
possible use the low available bitrate. Once the buffer be large 
enough, it should then make a direct transition to the low HD 
quality (bypassing intermediate bitrates). However, building buffer 
to a level which circumvents rebuffering and maintains sufficient 
smoothness require a lot of foresight. As illustrate by the example 
in Figure 3b, Pensieve be able to learn such a policy with zero tune 
or designer involvement, while other scheme such a robustMPC 
have difficulty optimize such long term strategies. 

Finally, Pensieve’s performance be within 9.6%–14.3% of the 
offline optimal scheme across all network trace and QoE metrics. 
Recall that the offline optimal performance cannot be achieve in 
practice a it require complete knowledge of future throughput. 
This show that there be likely to be little room for any online algo- 
rithm (without future knowledge) to improve over Pensieve in these 
scenarios. We revisit the question of Pensieve’s optimality in §5.4. 

QoE breakdown: To good understand the QoE gain obtain 
by Pensieve, we analyze Pensieve’s performance on the individ- 
ual term in our general QoE definition (Equation 6). Specifically, 
Figure 10 compare Pensieve to state-of-the-art ABR algorithm in 
term of the utility from the average playback bitrate, the penalty 
from rebuffering, and the penalty from switch bitrates (i.e., the 
smoothness penalty). In other words, a give scheme’s QoE can be 
compute by subtract the rebuffering penalty and smoothness 
penalty from the bitrate utility. In the interest of space, Figure 10 
combine the result for the FCC broadband and HSDPA traces. 

As shown, a large portion of Pensieve’s performance gain come 
from it ability to limit rebuffering across the different network and 
QoE metric considered. Pensieve reduces rebuffering by 10.6%– 
32.8% across the three metric by building up sufficient buffer to 
handle the network’s throughput fluctuations. Additionally, Figure 6 
illustrates that Pensieve do not outperform all state-of-the-art 
scheme on every QoE factor. Instead, Pensieve be able to balance 



SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA H. Mao et al. 

0 
0.2 
0.4 
0.6 
0.8 
1 

1.2 
1.4 

Bitrate utility Rebuffering penalty Smoothness penalty 

Av 
er 
ag 
e 
va 
lu 
e 

Buffer-based Rate-based BOLA MPC robustMPC Pensieve 

(a) QoEl in 

0 
0.2 
0.4 
0.6 
0.8 
1 

1.2 
1.4 

Bitrate utility Rebuffering penalty Smoothness penalty 

Av 
er 
ag 
e 
va 
lu 
e 

Buffer-based Rate-based BOLA MPC robustMPC Pensieve 

(b) QoEloд 

0 
1 
2 
3 
4 
5 
6 
7 

Bitrate utility Rebuffering penalty Smoothness penalty 

Av 
er 
ag 
e 
va 
lu 
e 

Buffer-based Rate-based BOLA MPC robustMPC Pensieve 

(c) QoEhd 
Figure 10: Comparing Pensieve with exist ABR algorithm 
by analyze their performance on the individual component 
in the general QoE definition (Equation 6). Results consider 
both the broadband and HSDPA networks. Error bar span ± 
one standard deviation from the average. 

each factor in a way that optimizes the QoE metric. For example, to 
optimize QoEhd , Pensieve achieves the best bitrate utility by always 
try to download chunk at HD bitrates, while when optimize for 
QoEl in or QoEloд , Pensieve focus on achieve sufficiently high 
bitrates with the small amount of rebuffering and bitrate switches. 

5.3 Generalization 
In the experiment above, Pensieve be train with a set of trace 
collect on the same network that be use during testing; note 
that no test trace be directly include in the training set. How- 
ever, in practice, Pensieve’s ABR algorithm could encounter new 
networks, with different condition (and thus, with different optimal 
strategies). To evaluate Pensieve’s ability to generalize to new net- 
work conditions, we conduct two experiments. First, we evaluate 
Pensieve in the wild on two real networks. Second, we take general- 
ity to the extreme and show how Pensieve can be train to perform 
well across multiple environment use a purely synthetic dataset. 

Real world experiments: We evaluate Pensieve and several state- 
of-the-art ABR algorithm in the wild use three different networks: 
the Verizon LTE cellular network, a public WiFi network at a lo- 
cal coffee shop, and the wide area network between Shanghai and 
Boston. In these experiments, a client, run on a Macbook Pro 

0 

0.5 

1 

1.5 

2 

2.5 

LTE Public WiFi International link 

Av 
er 
ag 
e 
Q 
oE 

BOLA robustMPC Pensieve 

Figure 11: Comparing Pensieve with exist ABR algorithm 
in the wild. Results be for theQoEl in metric and be collect 
on the Verizon LTE cellular network, a public WiFi network, 
and the wide area network between Shanghai and Boston. Bars 
list average and error bar span ± one standard deviation from 
the average. 

laptop, contact a video server run on a desktop machine lo- 
cat in Boston. We consider a subset of the ABR algorithm list 
in §5.1: BOLA, robustMPC, and Pensieve. On each network, we 
load our test video ten time with each scheme, randomly select 
the order among them. The Pensieve ABR algorithm evaluate here 
be solely train use the broadband and HSDPA trace in our 
corpus. However, even on these new networks, Pensieve be able 
to outperform the other scheme on the QoEl in metric (Figure 11). 
Experiments with the other QoE metric show similar results. 

Training with a synthetic dataset: Can we train Pensieve without 
any real network data? Learning from synthetic data alone would 
of course be undesirable, but we use it a a challenge test of 
Pensieve’s ability to generalize. 

We design a data set to cover a relatively broad set of network 
conditions, with average throughput range from 0.2 Mbps to 4.3 
Mbps. Specifically, the dataset be generate use a Markovian 
model in which each state represent an average throughput in the 
aforementioned range. State transition be perform at a 1 second 
granularity and follow a geometric distribution (making it more 
likely to transition to a nearby average throughput). Each throughput 
value be then drawn from a Gaussian distribution center around 
the average throughput for the current state, with variance uniformly 
distribute between 0.05 and 0.5. 

We then use Pensieve to compare two ABR algorithm on the 
test dataset described above (i.e., a combination of the HSDPA and 
broadband datasets): one train solely use the synthetic dataset, 
and another train explicitly on broadband and HSDPA network 
traces. Figure 12 illustrates our result for all three QoE metric 
list in Table 1. As shown, Pensieve’s ABR algorithm that be 
train on the synthetic dataset be able to generalize across these new 
networks, outperform robustMPC and achieve average QoE 
value within 1.6%–10.8% of the ABR algorithm train directly 
on the test networks. These result suggest that, in practice, Pen- 
sieve will likely be able to generalize to a broad range of network 
condition encounter by it clients. 

Multiple videos: As a final test of generalization, we evaluate Pen- 
sieve’s ability to generalize across multiple video properties. To do 
this, we train a single ABR model on 1,000 synthetic video use 
the technique described in §4.3. The number of available bitrates 



Neural Adaptive Video Streaming with Pensieve SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA 

0 

0.2 

0.4 

0.6 

0.8 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

Average QoE 

robustMPC 
Pensieve (synthetic) 
Pensieve 

(a) QoEl in 

0 

0.2 

0.4 

0.6 

0.8 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

Average QoE 

robustMPC 
Pensieve (synthetic) 
Pensieve 

(b) QoEloд 

0 

0.2 

0.4 

0.6 

0.8 

1 

-1 2 5 8 11 14 

CD 
F 

Average QoE 

robustMPC 
Pensieve (synthetic) 
Pensieve 

(c) QoEhd 
Figure 12: Comparing two ABR algorithm with Pensieve on the broadband and HSDPA networks: one algorithm be train on 
synthetic network traces, while the other be train use a set of trace directly from the broadband and HSDPA networks. Results 
be aggregate across the two datasets. 

0 

0.2 

0.4 

0.6 

0.8 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

Average QoE 

Multi-video Pensieve 
Single-video Pensieve 

Figure 13: Comparing ABR algorithm train across multiple 
video with those train explicitly on the test video. The mea- 
suring metric be QoEl in . 

for each video be randomly select from [3, 10],5 and the value 
for each bitrate be then randomly chosen from {200, 300, 450, 750, 
1200, 1850, 2350, 2850, 3500, 4300} kbps. The number of video chunk 
for each video be randomly generate from [20, 100]; chunk size 
be compute by multiply the standard 4-second chunk size 
with Gaussian noise ∼ N (1, 0.1). Thus, these video diverge on nu- 
merous property include the bitrate option (both the number of 
option and value of each), number of chunks, chunk size and video 
duration. Importantly, we ensure that none of the generate training 
video have the exact same bitrate option a the test video. 

We compare this newly train model to the original model, which 
be train solely on the “EnvivioDash3” video described in §5.1 
(the test video). Our result measure QoEl in on broadband and HS- 
DPA network trace and be depict in Figure 13. As shown, the 
generalize ABR algorithm train across multiple video be able 
to achieve average QoEl in value within 3.2% of the model train 
explicitly on the test video. These result suggest that in practice, 
Pensieve server can be configure to use a small number of ABR 
algorithm to improve stream for a diverse set of videos. 

5.4 Pensieve Deep Dive 
In this section, we describe microbenchmarks that provide a deeper 
understand of Pensieve and shed light on some practical concern 
with use RL-generated ABR algorithms. We begin by compare 
Pensieve’s RL algorithm to tabular RL schemes, which be use by 
some previous proposal for apply RL to video streaming. We 
then analyze how robust Pensieve be to vary system parameter 
(e.g., neural network hyperparameters, client-to-ABR server latency) 
and evaluate it training time. Finally, we conduct experiment to 
understand how close Pensieve be to the optimal scheme. 

5This range represent the two end of the spectrum for the number of bitrates support 
by the video provide by the DASH reference client [11]. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 1 2 

CD 
F 

Average QoE 

Tabular Q-learning 
Pensieve 1 past chunk 
Pensieve 8 past chunk 
Pensieve 16 past chunk 

Figure 14: Comparing exist tabular RL scheme with vari- 
ant of Pensieve that consider different number of past 
throughout measurements. Results be evaluate with QoEl in 
for the HSDPA network. 

Comparison to tabular RL schemes: A few recent scheme [6, 
8, 9, 47] have apply “tabular” RL to video streaming. Tabular 
method represent the model to be learn a a table, with separate 
entry for all state (e.g., client observations) and action (e.g., 
bitrate decisions). Tabular method do not scale to large state/action 
spaces. As a result, such scheme be force to restrict the state 
space by make simplify (and unrealistic) assumption about 
network behavior. For example, the most recent tabular RL scheme 
for ABR [6] assumes network throughput be Markovian, i.e., the 
future bandwidth depends only on the throughput observe in the 
last chunk download. 

To compare these approach with Pensieve, we implement a 
tabular RL scheme with Q-learning [29]. Our implementation be mod- 
eled after the design in [6]. The state space be the same a described 
in §4.2 except that the past bandwidth measurement be restrict 
to only 1 sample (as in [6]). The past bandwidth measurement and 
buffer occupancy be quantize with 0.5 Mbps and 1 second granu- 
larity respectively. Our quantization be more fine-grained than that 
use in [6]; we found that this result in good performance in our 
experiments. (Note that simulation result in [6] use synthetically 
generate network trace with the Markov property.) 

Figure 14 show a significant performance gap (46.3%) between 
the tabular scheme and Pensieve. This result show that simple 
network model (e.g., Markovian dynamics) fail to capture the intri- 
cacies of real networks. Unlike tabular RL methods, Pensieve can 
incorporate a large amount of throughput history into it state space 
to optimize for actual network characteristics. 

To good understand the importance of throughput history, we 
try to answer: how many past chunk be necessary to include in the 
state space? To do this, we generate three ABR algorithm with Pen- 
sieve that consider different number of throughput measurements: 



SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA H. Mao et al. 

Number of neuron and filter (each) Average QoEhd 
4 3.850 ± 1.215 
16 4.681 ± 1.369 
32 5.106 ± 1.452 
64 5.496 ± 1.411 

128 5.489 ± 1.378 

Table 2: Sweeping the number of CNN filter and hidden neu- 
ron in Pensieve’s learn architecture. 

Number of hidden layer Average QoEhd 
1 5.489 ± 1.378 
2 5.396 ± 1.434 
5 4.253 ± 1.219 

Table 3: Sweeping the number of hidden layer in Pensieve’s 
learn architecture. 

1, 8, and 16 past video chunks. As show in Figure 14, consider 
only 1 past chunk do not provide enough information to infer 
future network characteristic and hurt performance. Considering 
the past 8 chunk allows Pensieve to extract more information and 
improve it policy. However, the benefit of additional throughput 
measurement eventually plateau. For example, provide Pensieve 
with measurement for the past 16 chunk only improves the aver- 
age QoE by 1% compare to use throughput measurement for 
8 chunks. This marginal improvement come at the cost of high 
burden during training. 

Neural network (NN) architecture: Starting with Pensieve’s de- 
fault learn architecture (Figure 5), we swept a range of NN pa- 
rameters to understand the impact that each have on QoEhd 6. First, 
use a single fix hidden layer, we varied the number of filter in 
the 1D-CNN and the number of neuron in the hidden merge layer. 
These parameter be swept in tandem, i.e., when 4 filter be 
used, 4 neuron be used. Results from this sweep be present in 
Table 2. As shown, performance begin to plateau once the number 
of filter and neuron each exceed 32. Additionally, notice that once 
these value reach 128 (Pensieve’s default configuration), variance 
level decrease while average QoE value remain stable. 

Next, after fix the number of filter and hidden neuron to 128, 
we varied the number of hidden layer in Pensieve’s architecture. 
The result QoEhd value be list in Table 3. Interestingly, we 
find that the shallowest network of 1 hidden layer yield the best per- 
formance; this represent the default value in Pensieve. Performance 
steadily degrades a we increase the number of hidden layers. How- 
ever, it be important to note that our sweep use a fix learn rate 
and number of training iterations. Tuning these parameter to cater 
to deeper network may improve performance, a these network 
generally take longer to train. 

Client-to-ABR server latency: Recall that Pensieve deploys the 
RL-generated ABR model on an ABR server (not the video stream- 
ing clients). Under this deployment model, client must first query 
the Pensieve’s ABR server to determine the bitrate to use for the 
next chunk, before download that chunk from a CDN server. To 
understand the overhead incur by this additional round trip, we 
perform a sweep of the RTT between the client player and ABR 

6QoEhd be use for the parameter sweep experiment a it highlight performance 
difference more clearly. 

RTT (ms) Average QoEhd 
0 5.407 ± 1.820 

20 5.356 ± 1.768 
40 5.309 ± 1.768 
60 5.271 ± 1.773 
80 5.217 ± 1.742 
100 5.219 ± 1.748 

Table 4: Average QoEhd value when different RTT value be 
impose between the client and Pensieve’s ABR server. 

0 

0.2 

0.4 

0.6 

0.8 

1 

-0.5 0.5 1.5 2.5 

CD 
F 

Average QoE 

Pensieve 
Online optimal 
Offline optimal 

Figure 15: Comparing Pensieve with online and offline optimal. 
The experiment us the QoEl in metric. 

server, consider value from 0 ms–100 ms. This experiment use 
the same setup described in §5.1, and measure the QoEhd metric. 
Table 4 list our results, highlight that the latency from this ad- 
ditional RTT have minimal impact on QoE: the average QoEhd with 
a 100 m latency be within 3.5% of that when the latency be 0 
ms. The reason be that the latency incur from the additional round 
trip to Pensieve’s ABR server be masked by the playback buffer 
occupancy and chunk download time [18, 21]. 

Training time: To measure the overhead of generate ABR algo- 
rithms use RL, we profile Pensieve’s training process. Training 
a single algorithm require approximately 50,000 iterations, where 
each iteration take 300 m and correspond to 16 agent update 
their parameter in parallel (using the training approach described in 
§4.2). Thus, in total, training take approximately 4 hours. We note 
that this cost be incur offline and can be perform infrequently 
depend on environment stability. 

Optimality: Our result illustrate that Pensieve be able to outperform 
exist ABR algorithms. However, Figures 8 and 9 show that there 
still exists a gap between Pensieve and the offline optimal. It be 
unclear to what extent this gap can be close since the offline optimal 
scheme make decision with perfect knowledge of future bandwidth 
(§5.1). A practical online algorithm would only know the underlie 
distribution of future network throughput (rather than the precise 
throughput values). Thus Pensieve may in fact be much closer to the 
optimal online scheme. 

Of course, we cannot compute the optimal online algorithm for 
real network traces, a we do not know the stochastic process 
underlie these traces. Thus, to understand how Pensieve compare 
to the best online algorithm, we conduct a control experiment 
where the download time for each chunk be generate accord to a 
know Markov process. Specifically, we simulate the download time 
Tn of chunk n asTn = Tn−1 (Rn/Rn−1)+ϵ, where Rn be the bitrate of 
chunk n and ϵ ∼ N 

( 
0, σ 2 

) 
. For this model, it be straightforward to 

compute the optimal online decision use dynamic programming. 
See [28] for details. 



Neural Adaptive Video Streaming with Pensieve SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA 

To compare the optimal online algorithm with Pensieve, we set the 
video chunk length δ to be 4 seconds, mimic the “EnvivioDash3” 
video described in §5.1. The initial download time T0 be set to 4 
second for bitrate R0 = 2 kbps, and the standard deviation σ of the 
Gaussian noise be set to 0.5. Both buffer occupancy and download 
time be quantize to 0.1 second to run dynamic programming. 

We use the same setup in §5.1 to train a Pensieve agent in this 
simulated environment, and compare Pensieve’s performance with 
the online and offline optimal schemes. Our experiment considers 
the QoEl in metric and the result be depict in Figure 15. As ex- 
pected, the offline optimal outperforms the online optimal by 9.1% 
on average. This be comparable to the performance gap between Pen- 
sieve and the offline optimal observe in §5.2. Indeed, the average 
QoE achieve by Pensieve be within 0.2% of the online optimal. 

6 DISCUSSION 

Deploying Pensieve in practice: In our current implementation, 
Pensieve’s ABR server run on the server-side of video stream 
applications. This approach offer several advantage over deploy- 
ment in client video players. First, a variety of client-side device 
be use for video stream today, range from multi-core desktop 
machine to mobile device to TVs. By use an ABR server to 
simply guide client bitrate selection, Pensieve can easily support 
this broad range of video client without modification that may sac- 
rifice performance. Additionally, ABR algorithm be traditionally 
deployed on client which can quickly react to change environ- 
ments [51]. However, a note in §4, Pensieve preserve this ability 
by have client include observation about the environment in 
each request sent to the ABR server. Further, our result suggest that 
the additional latency require to contact Pensieve’s ABR server 
have negligible impact on QoE (§5.4). If direct deployment in client 
video player be preferred, Pensieve could use compress neural 
network [16] or represent them in language support by many 
client applications, e.g., JavaScript [45]. 

Periodic and online training: In this paper, we primarily described 
RL-based ABR algorithm generation a an offline task. That is, with 
Pensieve, we assume that the ABR algorithm be generate a priori 
(during a training phase) and be then unmodified after deployment. 
However, Pensieve can naturally support an approach in which an 
ABR algorithm be generate or update periodically a new data 
arrives. This technique would enable ABR algorithm to further 
adapt to the exact condition that video client be experience at a 
give time. The extreme version of this approach be to train online 
directly on the video client. However, online training on video client 
raise two challenges. First, it increase the computational overhead 
for the client. Second, it require algorithm that can learn from 
small amount of data and converge to a good policy quickly. 

Retraining frequency depends on how quickly new network be- 
haviors emerge to which exist model do not generalize. While 
our generalization result (§5.3) suggest that retrain frequently 
may not be necessary, technique to determine when to retrain and 
investigate the tradeoff with online training be interest area 
for future work. 

7 RELATED WORK 
The early ABR algorithm can be primarily grouped into two 
classes: rate-based and buffer-based. Rate-based algorithm [21, 42] 
first estimate the available network bandwidth use past chunk 
downloads, and then request chunk at the high bitrate that the 
network be predict to support. For example, Festive [21] predicts 
throughput to be the harmonic mean of the experienced throughput 
for the past 5 chunk downloads. However, these method be hin- 
dered by the bias present when estimate available bandwidth 
on top of HTTP [22, 26]. Several system aim to correct these 
throughput estimate use smooth heuristic and data aggrega- 
tion technique [42], but accurate throughput prediction remains a 
challenge in practice [53]. 

In contrast, buffer-based approach [19, 41] solely consider the 
client’s playback buffer occupancy when decide the bitrates for 
future chunks. The goal of these algorithm be to keep the buffer 
occupancy at a pre-configured level which balance rebuffering and 
video quality. The most recent buffer-based approach, BOLA [41], 
optimizes for a specify QoE metric use a Lyapunov optimization 
formulation. BOLA also support chunk download abandonment, 
whereby a video player can restart a chunk download at a low 
bitrate level if it suspect that rebuffering be imminent. 

Each of these approach performs well in certain setting but 
not in others. Specifically, rate-based approach be best at startup 
time and when link rate be stable, while buffer-based approach 
be sufficient and more robust in steady state and in the presence of 
time-varying network [19]. Consequently, recently propose ABR 
algorithm have also investigate combine these two techniques. 
The state-of-the-art approach be MPC [51], which employ model 
predictive control algorithm that use both throughput estimate and 
buffer occupancy information to select bitrates that be expect to 
maximize QoE over a horizon of several future chunks. However, 
MPC still relies heavily on accurate throughput estimate which be 
not always available. When throughput prediction be incorrect, 
MPC’s performance can degrade significantly. Addressing this issue 
require heuristic that make throughput prediction more conser- 
vative. However, tune such heuristic to perform well in different 
environment be challenging. Further, a we observe in §3, MPC be 
often unable to plan far enough into the future to apply the policy 
that would maximize performance in give settings. 

A separate line of work have propose apply RL to adaptive 
video stream [6, 8, 9, 47]. All of these scheme apply RL in 
a “tabular form,” which store and learns the value function for 
all state and action explicitly, rather than use function approx- 
imators (e.g., neural networks). As a result, these scheme do not 
scale to the large state space necessary for good performance in 
real networks, and their evaluation have be limited to simulation 
with synthetic network models. For example, the most recent tabu- 
lar scheme [6] relies on the fundamental assumption that network 
bandwidth be Markovian, i.e., the future bandwidth depends only on 
the throughput observe in the last chunk download. This assump- 
tion confines the state space to consider only one past bandwidth 
measurement, make the tabular approach feasible to implement. 
As we saw in §5.4, the information contain in one past chunk be 
not sufficient to accurately infer the distribution of future bandwidth. 
Nevertheless, some of the technique use in the exist RL video 



SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA H. Mao et al. 

stream scheme (e.g., Post-Decision States [6, 35]) could be use 
to accelerate learn in Pensieve a well. 

8 CONCLUSION 
We present Pensieve, a system which generates ABR algorithm 
use reinforcement learning. Unlike ABR algorithm that use fix 
heuristic or inaccurate system models, Pensieve’s ABR algorithm 
be generate use observation of the result performance of 
past decision across a large number of video stream experiments. 
This allows Pensieve to optimize it policy for different network 
characteristic and QoE metric directly from experience. Over a 
broad set of network condition and QoE metrics, we found that 
Pensieve outperform exist ABR algorithm by 12%–25%. 

Acknowledgments. We thank our shepherd, John Byers, and the 
anonymous SIGCOMM reviewer for their valuable feedback. We 
also thank Te-Yuan Huang for her guidance regard video stream- 
ing in practice, and Jiaming Luo for fruitful discussion regard 
the learn aspect of the design. This work be fund in part by 
NSF grant CNS-1617702, CNS-1563826, and CNS-1407470, the 
MIT Center for Wireless Networks and Mobile Computing, and a 
Qualcomm Innovation Fellowship. 

REFERENCES 
[1] M. Abadi et al. 2016. TensorFlow: A System for Large-scale Machine Learning. 

In OSDI. USENIX Association. 
[2] Akamai. 2016. dash.js. https://github.com/Dash-Industry-Forum/dash.js/. (2016). 

[3] S. Akhshabi, A. C. Begen, and C. Dovrolis. 2011. An Experimental Evaluation of 
Rate-adaptation Algorithms in Adaptive Streaming over HTTP. In MMSys. 

[4] M. Allman, V. Paxson, and E. Blanton. 2009. TCP congestion control. RFC 5681. 

[5] C. M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer. 
[6] F. Chiariotti et al. 2016. Online learn adaptation strategy for DASH clients. In 

Proceedings of the 7th International Conference on Multimedia Systems. ACM, 8. 

[7] Cisco. 2016. Cisco Visual Networking Index: Forecast and Methodology, 
2015-2020. 

[8] M. Claeys et al. 2013. Design of a Q-learning-based client quality selection 
algorithm for HTTP adaptive video streaming. In Adaptive and Learning Agents 
Workshop. 

[9] M. Claeys et al. 2014. Design and optimisation of a (FA) Q-learning-based HTTP 
adaptive stream client. Connection Science (2014). 

[10] Federal Communications Commission. 2016. Raw Data - Measuring Broadband 
America. (2016). https://www.fcc.gov/reports-research/reports/ 
measuring-broadband-america/raw-data-measuring-broadband-america-2016 

[11] DASH Industry Form. 2016. Reference Client 2.4.0. http://mediapm.edgesuite. 
net/dash/public/nightly/samples/dash-if-reference-player/index.html. (2016). 

[12] F. Dobrian et al. 2011. Understanding the Impact of Video Quality on User 
Engagement. In SIGCOMM. ACM. 

[13] G. Fairhurst et al. 2015. Updating TCP to Support Rate-Limited Traffic. RFC 
7661 (2015). 

[14] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training 
deep feedforward neural networks.. In Aistats, Vol. 9. 249–256. 

[15] M. T. Hagan, H. B. Demuth, M. H. Beale, and O. De Jesús. 1996. Neural 
network design. PWS publishing company Boston. 

[16] S. Han, H. Mao, and W. J. Dally. 2015. Deep compression: Compressing deep 
neural network with pruning, train quantization and huffman coding. CoRR, 
abs/1510.00149 2 (2015). 

[17] M. Handley, J. Padhye, and S. Floyd. 2000. TCP Congestion Window Validation. 
RFC 2861 (2000). 

[18] T.Y. Huang et al. 2012. Confused, Timid, and Unstable: Picking a Video 
Streaming Rate be Hard. In Proceedings of the 2012 ACM Conference on Internet 
Measurement Conference (IMC). ACM. 

[19] T.Y. Huang et al. 2014. A Buffer-based Approach to Rate Adaptation: Evidence 
from a Large Video Streaming Service. In SIGCOMM. ACM. 

[20] M. Jaderberg et al. 2017. Reinforcement learn with unsupervised auxiliary 
tasks. In ICLR. 

[21] J. Jiang, V. Sekar, and H. Zhang. 2012. Improving Fairness, Efficiency, and 
Stability in HTTP-based Adaptive Video Streaming with FESTIVE. In CoNEXT. 

[22] J. Jiang et al. 2016. CFA: A Practical Prediction System for Video QoE 
Optimization. In NSDI. USENIX Association. 

[23] I. Ketykó et al. 2010. QoE Measurement of Mobile YouTube Video Streaming. In 
Proceedings of the 3rd Workshop on Mobile Video Delivery (MoViD). ACM. 

[24] V. R Konda and J. N. Tsitsiklis. 2000. Actor-critic algorithms. In Advances in 
neural information processing systems. 1008–1014. 

[25] S. S. Krishnan and R. K. Sitaraman. 2012. Video Stream Quality Impacts Viewer 
Behavior: Inferring Causality Using Quasi-experimental Designs. In Proceedings 
of the 2012 ACM Conference on Internet Measurement Conference (IMC). ACM. 

[26] Z. Li et al. 2014. Probe and Adapt: Rate Adaptation for HTTP Video Streaming 
At Scale. IEEE Journal on Selected Areas in Communications (2014). 

[27] H. Mao, M. Alizadeh, I. Menache, and S. Kandula. 2016. Resource Management 
with Deep Reinforcement Learning. In HotNets. ACM. 

[28] H. Mao, R. Netravali, and M. Alizadeh. 2017. Neural Adaptive Video Streaming 
with Pensieve. (2017). 
http://web.mit.edu/pensieve/content/pensieve-tech-report.pdf 

[29] V. Mnih et al. 2015. Human-level control through deep reinforcement learning. 
Nature 518 (2015), 529–533. 

[30] V. Mnih et al. 2016. Asynchronous method for deep reinforcement learning. In 
International Conference on Machine Learning. 1928–1937. 

[31] R. K.P. Mok, E. W. W. Chan, X. Luo, and R. K.C. Chang. 2011. Inferring the 
QoE of HTTP Video Streaming from User-viewing Activities. In Proceedings of 
the First ACM SIGCOMM Workshop on Measurements Up the Stack (W-MUST). 

[32] R. K. P. Mok, E. W. W. Chan, and R. K. C. Chang. 2011. Measuring the quality 
of experience of HTTP video streaming. In 12th IFIP/IEEE International 
Symposium on Integrated Network Management (IM 2011) and Workshops. 

[33] R. Netravali et al. 2015. Mahimahi: Accurate Record-and-Replay for HTTP. In 
Proceedings of USENIX ATC. 

[34] K. Piamrat, C. Viho, J. M. Bonnin, and A. Ksentini. 2009. Quality of Experience 
Measurements for Video Streaming over Wireless Networks. In Proceedings of 
the 2009 Sixth International Conference on Information Technology: New 
Generations (ITNG). IEEE Computer Society. 

[35] W. B. Powell. 2007. Approximate Dynamic Programming: Solving the curse of 
dimensionality. Vol. 703. John Wiley & Sons. 

[36] B. Recht, C. Re, S. Wright, and F. Niu. 2011. Hogwild: A lock-free approach to 
parallelize stochastic gradient descent. In Advances in Neural Information 
Processing Systems. 693–701. 

[37] H. Riiser et al. 2013. Commute Path Bandwidth Traces from 3G Networks: 
Analysis and Applications. In Proceedings of the 4th ACM Multimedia Systems 
Conference (MMSys). ACM. 

[38] J. K. Rowling. 2000. Harry Potter and the Goblet of Fire. London: Bloomsbury. 
[39] Sandvine. 2015. Global Internet Phenomena-Latin American & North America. 
[40] D. Silver et al. 2016. Mastering the game of Go with deep neural network and 

tree search. Nature 529 (2016), 484–503. 
[41] K. Spiteri, R. Urgaonkar, and R. K. Sitaraman. 2016. BOLA: Near-Optimal 

Bitrate Adaptation for Online Videos. CoRR abs/1601.06748 (2016). 
[42] Y. Sun et al. 2016. CS2P: Improving Video Bitrate Selection and Adaptation with 

Data-Driven Throughput Prediction. In SIGCOMM. ACM. 
[43] R. S. Sutton and A. G. Barto. 1998. Reinforcement Learning: An Introduction. 

MIT Press. 
[44] R. S. Sutton et al. 1999. Policy gradient method for reinforcement learn with 

function approximation.. In NIPS, Vol. 99. 1057–1063. 
[45] Synaptic. 2016. synaptic.js – The javascript architecture-free neural network 

library for node.js and the browser. https://synaptic.juancazala.com/. (2016). 
[46] TFLearn. 2017. TFLearn: Deep learn library feature a higher-level API for 

TensorFlow. http://tflearn.org/. (2017). 
[47] J. van der Hooft et al. A learning-based algorithm for improve 

bandwidth-awareness of adaptive stream clients. In 2015 IFIP/IEEE 
International Symposium on Integrated Network Management. IEEE. 

[48] A. S. Vezhnevets et al. 2017. FeUdal Networks for Hierarchical Reinforcement 
Learning. arXiv preprint arXiv:1703.01161 (2017). 

[49] K. Winstein, A. Sivaraman, and H. Balakrishnan. Stochastic Forecasts Achieve 
High Throughput and Low Delay over Cellular Networks. In NSDI. 

[50] Y. Wu and Y. Tian. 2017. Training agent for first-person shooter game with 
actor-critic curriculum learning. In ICLR. 

[51] X. Yin, A. Jindal, V. Sekar, and B. Sinopoli. 2015. A Control-Theoretic Approach 
for Dynamic Adaptive Video Streaming over HTTP. In SIGCOMM. ACM. 

[52] Y. Zaki et al. 2015. Adaptive congestion control for unpredictable cellular 
networks. In ACM SIGCOMM Computer Communication Review. ACM. 

[53] X. K. Zou. 2015. Can Accurate Predictions Improve Video Streaming in Cellular 
Networks?. In HotMobile. ACM. 

https://github.com/Dash-Industry-Forum/dash.js/ 
https://www.fcc.gov/reports-research/reports/measuring-broadband-america/raw-data-measuring-broadband-america-2016 
https://www.fcc.gov/reports-research/reports/measuring-broadband-america/raw-data-measuring-broadband-america-2016 
http://mediapm.edgesuite.net/dash/public/nightly/samples/dash-if-reference-player/index.html 
http://mediapm.edgesuite.net/dash/public/nightly/samples/dash-if-reference-player/index.html 
http://web.mit.edu/pensieve/content/pensieve-tech-report.pdf 
https://synaptic.juancazala.com/ 
http://tflearn.org/ 

Abstract 
1 Introduction 
2 Background 
3 Learning ABR Algorithms 
4 Design 
4.1 Training Methodology 
4.2 Basic Training Algorithm 
4.3 Enhancement for multiple video 
4.4 Implementation 

5 Evaluation 
5.1 Methodology 
5.2 Pensieve vs. Existing ABR algorithm 
5.3 Generalization 
5.4 Pensieve Deep Dive 

6 Discussion 
7 Related Work 
8 Conclusion 
References 

