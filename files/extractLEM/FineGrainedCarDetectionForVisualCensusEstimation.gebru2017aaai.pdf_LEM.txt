









































Insert Your Title Here 


Fine-Grained Car Detection for Visual Census Estimation 

Timnit Gebru and Jonathan Krause and Yilun Wang and Duyun Chen and Jia Deng and Li Fei-Fei 
Department of Computer Science, Stanford University 

{tgebru, jkrause, yilunw, duchen, feifeili}@cs.stanford.edu 
Department of Computer Science, University of Michigan 

jiadeng@umich.edu 

Abstract 

Targeted socio-economic policy require an accurate under- 
stand of a country’s demographic makeup. To that end, 
the United States spends more than 1 billion dollar a year 
gathering census data such a race, gender, education, oc- 
cupation and unemployment rates. Compared to the tradi- 
tional method of collect survey across many year which 
be costly and labor intensive, data-driven, machine learning- 
driven approach be cheaper and faster—with the potential 
ability to detect trend in close to real time. In this work, we 
leverage the ubiquity of Google Street View image and de- 
velop a computer vision pipeline to predict income, per caput 
carbon emission, crime rate and other city attribute from a 
single source of publicly available visual data. We first detect 
car in 50 million image across 200 of the large US city 
and train a model to predict demographic attribute use the 
detect cars. To facilitate our work, we have collect the 
large and most challenge fine-grained dataset report to 
date consist of over 2600 class of car comprise of im- 
age from Google Street View and other web sources, classi- 
fied by car expert to account for even the most subtle of vi- 
sual differences. We use this data to construct the large scale 
fine-grained detection system report to date. Our prediction 
result correlate well with ground truth income data (r=0.82), 
Massachusetts department of vehicle registration, and source 
investigate crime rates, income segregation, per caput car- 
bon emission, and other market research. Finally, we learn in- 
teresting relationship between car and neighbourhood al- 
low u to perform the first large scale sociological analysis 
of city use computer vision techniques. 

Introduction 

Many government and non-government project be ded- 
icated to study city and their inhabitants. For exam- 
ple, the American Community Survey (ACS) collect data 
related to the demographic makeup of the US, the En- 
vironmental Protection Agency gather data pertain to 
city pollution, and private organization such a car deal- 
erships gather information regard the relationship be- 
tween car and demographics. Traditionally, the most preva- 
lent method for obtain such personal, demographic and 
environmental information be through costly survey such 
a the US census, American community survey (ACS) and 

Copyright c� 2017, Association for the Advancement of Artificial 
Intelligence (www.aaai.org). All right reserved. 

other project conduct by disparate government entities. 
However, the emergence of large, diverse set of data gener- 
ated by people have enable computer scientist and compu- 
tational sociologist to gain interest insight by analyz- 
ing massive user text and social network (West et al. 2014; 
Cheng, Danescu-Niculescu-Mizil, and Leskovec 2014). For 
instance Michel et al analyze over one million book and 
present result related to the evolution of the English lan- 
guage a well a various cultural phenomenon (Michel et al. 
2011). And Blumenstock et al use mobile phone data to 
predict poverty rate in Rwanda (Blumenstock, Cadamuro, 
and On 2015). 

In contrast to textual data, the use of image for compu- 
tational social science have be largely unexplored. A snap- 
shot of a street or neighborhood tell a detailed story of it 
socioeconomic make up. However, while a few pioneer 
work have apply visual scene analysis technique to infer 
characteristic of neighborhood and cities, they have be 
limited in scope and scale. Jean et al use Satellite image 
feature to predict poverty rate in 5 African country (Jean 
et al. 2016). Similarly, work from Zhou et al, Ordonez et al, 
Naik et al and Arietta et al use global image feature from 
Google Street View image to learn various neighborhood 
characteristic (Zhou et al. 2014; Ordonez and Berg 2014; 
Naik et al. 2014; Arietta et al. 2014). 

Global image features, however, contain limited informa- 
tion about individuals, neighborhood and cities. In contrast, 
affluence (or lack thereof), culture, and even crime can be 
infer by observe houses, people, clothing style and 
type of car on the street. Ninety five percent of American 
household own automobile (Chase 2009), and a show 
by prior work (Choo and Mokhtarian 2004) car be a re- 
flection of their owners’ characteristic provide significant 
personal information. For instance, a large number of Teslas 
be a strong indicator of a wealthy neighborhood. 

Guided by this intuition we develop a novel computer vi- 
sion pipeline to predict demographic variable from Google 
Street View images. We recognize car in 50 million image 
from 200 of the large American city and use the detect 
car to predict income, segregation levels, per caput carbon 
emission and crime rates. In addition to accurately predict- 
ing ground truth data (e.g. Pearsons r=0.82 for income), we 
discover interest relationship between car and demo- 
graphic allow u to perform a sociological study of our 



Figure 1: Examples of car from our fine-grained car dataset. Left: example of car from edmunds.com, cars.com and 
craigslist.com. Right: example of car from streetview images. Green bound box indicate the ground truth location of 
car in Street View images. 

200 cities. For instance, we find that wealthy people drive 
foreign make car and that a large quantity of van be corre- 
lated with high crime. 

While specific vehicle attribute can be associate with 
certain type of people, classify different type of car 
be a difficult computer vision task call fine-grained ob- 
ject recognition. In fine-grained recognition we seek to dis- 
tinguish between similar object such a different type of 
cars, dog breed or clothing styles—a task that can even 
be challenge for humans. In this work, we train a fast, 
large scale fine-grained detection system able to detect car 
in 50 million Google Street View image in less than 2 
weeks. To train our model we gather a fine-grained car 
dataset of unprecedented scale. It have 2657 car class con- 
sisting of nearly all car type produce in the world after 
1990: with a total of 700,000 image from website such a 
edmunds.com, cars.com, craigslist.com and Google Street 
View (Fig. 1). We make our dataset publicly available and 
anticipate it use by computer vision researcher focus on 
fine-grained recognition. 

Finally, we use the detect car to train a model predict- 
ing socioeconomic attribute such a income and segregation 
levels. We show that from a single source of publicly avail- 
able visual data, Google Street View images, we be able to 
predict diverse set of important societal information typi- 
cally gather by different entities. We not only show good 
correlation with socioeconomic census data but can also pre- 
dict information absent from the census such a city pollu- 
tion levels, crime rates, income segregation levels, vehicle 
registration and other data related to cities. 

Attribute Training Validation Test 

Street View Images 199,666 39,933 159,732 
Product Shot Images 313,099 - - 
Total Images 512,765 39,933 159,732 
Street View BBoxes 34,712 6,915 27,865 
Product Shot BBoxes 313,099 - - 
Total BBoxes 347,811 6,915 27,865 

Table 1: Dataset statistic for our training, validation, and 
test splits. “BBox” be shorthand for Bounding Box. Prod- 
uct shot bound box and image be from craigslist.com, 
cars.com and edmunds.com. 

A Large Scale Fine-Grained Car Dataset 

In order to analyze society via the car visible in it, one 
must first create a dataset of all possible car we would like 
to study. However, this pose a challenge. What be all of 
the car in the world, and how can we possibly collect data 
for each one of them? We cannot use exist datasets such 
a (Deng et al. 2009) a they do not have an exhaustive set 
of annotate car images. To tackle this problem, we lever- 
age exist knowledgebases of cars, download image 
and data for roughly 18,000 different type of car from the 
website edmunds.com. We then grouped these car type into 
set of indistinguishable class and collect more image 
for each class from craigslist.com and cars.com. Both web- 
site contain ground truth car image from owner look to 
sell their cars. Using Amazon Mechanical Turk (AMT), we 
collect one bound box for each image yield 2,657 
visual group of car for our analysis. A bound box be a 



Figure 2: Left: A hierarchy of car class in our dataset. Classes become more difficult to distinguish low in the hierarchy, with 
difference extremely subtle at the year and trim level. Center: The number of image per class obtain from edmunds.com, 
cars.com, and craigslist.com vs. Right: The number of image per class from Street View. 

set of pixel coordinate describe the location of the image 
contain the object of interest—in our case a car. 

However, these image alone be not enough for an accu- 
rate study of society via car visible on the street. To perform 
our social analysis, we need to recognize car appear in 
Google Street View images. As show in (Fig. 1), product 
shot image typically focus on a single car, centrally placed, 
and from diverse viewpoints. Google Street View images, 
on the otherhand, may contain multiple car per image, each 
of which can be occlude and low resolution. This present 
a challenge for an automobile recognition system. Thus, a 
part of our fine-grained car dataset we also collect bound- 
ing box for nearly 400,000 car in Street View images. 
These be annotate by expert human labelers with one 
of the 2,657 fine-grained classes, provide u with data to 
train detector and classifier effective on real-world images. 
In Tab. 1 we present aggregate statistic of our fine-grained 
car dataset, which have a total of 712,430 image and 382,591 
bound boxes. 

What do this data look like? We analyze our car class 
in Fig. 2. At the most fine-grained level of car year and trim, 
difference between class become extremely subtle. A plot 
of the class distribution further show that product image 
have coverage over a wide variety of class while the Street 
View distribution be much more skewed. Finally, to perform 
our visual census, we collect 50 million Google Street 
View images, sample 8 million GPS point in 200 large 
US city every 25m. Images from 6 camera rotation be 
capture per GPS point. 

Fine-Grained Detection and Classification 

We train a model to distinguish car in Google Street View 
image in 2 steps. First, we localize part of the image hav- 
ing a high likelihood of contain a car; this be the generic 
car detection step. Then we take the region with the high 
likelihood and classify the type of car they contain use a 
custom convolutional neural network (CNN). 

Car Detection 

Our goal here be to train a model that can efficiently and ac- 
curately detect car in 50 million image with reasonable 

accuracy. To this end, we be willing to sacrifice a cou- 
ple of percent in accuracy to significantly speed up train 
and test times. Thus, although the current state-of-the-art 
for object detection be faster R-CNN (Girshick 2015) we 
choose the simplicity and efficiency of deformable part mod- 
els(DPM) (Felzenszwalb et al. 2010) for our task of large 
scale car detection. After extensive cross validation, we de- 
cided upon a single component DPM with 8 parts, achieve 
an average precision (AP) of 64.2% at 5 second per Street 
View image. Given an input image, our train DPM out- 
put bound box of vary size in the image with as- 
sociated score reflect the likelihood of contain a car. 
With this architecture, we detect car on our entire dataset 
in less than two week with 200 2.1 GHz CPU cores. 

To further improve detection performance, we leveraged 
knowledge of car distribution in Street View images. This 
be do by introduce a prior on the location and size of 
predict bound boxes. Concretely, we model the distri- 
bution of Street View car in image a a histogram over 
three variables: the x coordinate of the center of the bound- 
ing box, the y coordinate of the center, and log(area) where 
area be the area of the bound box. We divide each vari- 
able into 20 bin result in a total of 8,000 bin in the 
histogram, and estimate the probability of each bin use 
bound box statistic in the Street View training data. We 
add a pseudo count of 1 in each bin for regularization. With 
P (x, y, log (area)) denote this histogram, we augment 
each DPM detection score by: 

score 

DPM+LOC = scoreDPM+↵ log (P (x, y, log (area))) 
(1) 

where ↵ be a learn weight on the location prior. This im- 
prof detection AP by 1.92 at a negligible time cost. During 
analysis, DPM score be convert into estimate probabil- 
ities via isotonic regression (Barlow et al. 1972), learn on 
the validation set. The final output of the detection step pro- 
vides u with a set of bound box with vary size and 
location in an image, and the probability that each of these 
box contains a car. 



Car Classification 

To classify the detect car into one of 2,657 fine-grained 
classes, we use a convolutional neural network with an ar- 
chitecture follow (Krizhevsky, Sutskever, and Hinton 
2012). We choose this architecture for it efficiency in- 
stead of state-of-the-art fine-grained classification system 
like (Krause et al. 2015b; 2015a). Discriminative learn 
method include CNNs work best when train on data 
from a similar distribution a the test set (in our case, Street 
View images) (Ben-David et al. 2007). However, the pro- 
hibitive cost of label many Street View image for each of 
our 2,657 category prevents u from have enough train- 
ing data from this source. Instead, we train our CNN on a 
combination of Street View and the more plentiful and inex- 
pensively label product shot images. We make three mod- 
ifications to the traditional CNN training procedure to im- 
prove our classifier performance. 

First, we seek to prevent our classifier from overfitting on 
product shot image since they be a much large fraction of 
our training set. Inspired by domain adaptation works, we 
approximate the WEIGHTED method of Daume (Daumé 
III 2007) by duplicate each Street View example 10 time 
during training. This roughly equalizes the number of prod- 
uct shot and Street View training images. Next, we apply 
transformation to product shot image to make them sim- 
ilar to those from Street View. This decrease the distance 
between training and test data distributions, improve 
classifier performance. Cars in product shot image occupy a 
much large number of pixel in the image (Fig. 1) than those 
in Street View. To compensate, we first measure the dis- 
tribution of bound box resolution in Street View train- 
ing images. At training time, we dynamically downsize each 
product shot image accord to this distribution and rescale 
it to fit the input dimension of the CNN. Resolutions be 
parameterized by the geometric mean of the bound box 
width and height, and the probability distribution be give a 
a histogram over 35 different such resolutions. 

A further challenge in classify Street View image be 
that the input to our CNN consists of noisy bound box 
output by the previous detection step. This stand in con- 
trast to the default classifier input—ground truth bound 
box that be tight around each car. To tackle this challenge, 
we use our validation data to measure the distribution of in- 
tersection over union (IOU) overlap between ground truth 
bound box and those produce by our car detector. For 
each Street View bound box input to the CNN, we ran- 
domly sample it source image accord to this IOU distri- 
bution, simulate noisy detection during training. 

At test time, each detect bound box be input to the 
CNN, and we perform a single forward pas to get the soft- 
max probability specify the likelihood of contain a 
car belonging to each of the 2,657 categories. In practice, 
we only keep the top 20 predictions, since store a full 
2,657-dimensional float point vector for each bound 
box be prohibitively expensive. On average, these top 20 pre- 
diction account for 85.5% of the softmax layer probability 
mass. We also note that, after extensive code optimization to 
make this classification step a fast a possible, we be pri- 
marily limited by the time spent reading image from disk, 

Attribute Accuracy 

Make 66.38% 
Model 51.83% 
Submodel 77.74% 
Price 61.61% 
Domestic/Foreign 87.71% 
Country 84.21% 

Table 2: Classification accuracy on the test set for various 
car attributes. 

Figure 3: Confusion matrix for car body types. Each row 
represent the ground truth body type and each column 
show the body type predict by our classifier. 

especially when use multiple GPUs simultaneously. 

Analyzing Classification Performance 

Fine-grained classification across 2,657 class be a chal- 
lenging task even for expert humans. Our CNN achieves 
a remarkable classification accuracy of 31.27%, on ground 
truth bound boxes, and 33.27% on true positive DPM 
detections. Since our model will not always be accurate at 
this level of granularity, we closely examine it errors. Some 
type of mistake can undermine the accuracy of our visual 
census where a others do not matter. For example, mis- 
classify a 2001 Honda Accord LX a a 2001 Honda Ac- 
cord DX do not affect our social analysis. But incorrectly 
classify a 2012 BMW 3-Series a a 1996 Honda Accord 
could seriously impact the quality of our results. We list clas- 
sification accuracy for various car attribute in Tab. 2. We 
can classify these car property with much high accura- 
cies than the 2,657-way classification, indicate that error 
significantly impact the visual census be rare. We zoom 
in on the Body Type attribute in Fig. 3 and observe that most 
error be between highly similar body types; e.g. sedan and 
coupe, or extend and crew cab trucks. 



Figure 4: (A) Twenty two major American city ranked by 
segregation of car price. Our segregation index be Moran’s I 
statistic (Moran 1950). Insets show map of statistically sig- 
nificant cluster of car with high price (red), low price 
(yellow) a well a no statistically significant cluster 
(white) for the city of Chicago, IL, San Francisco, CA and 
Jacksonville, FL respectively use the Getis-Ord Gi statis- 
tic (Getis and Ord 1992). Chicago have large cluster of ex- 
pensive and cheap car whereas Jacksonville show almost 
no cluster of car by price. (B) Expensive/Cheap cluster 
of car in Chicago. (C) Zip code level median household 
income in Chicago. Large cluster of expensive car be in 
wealthy neighborhood whereas large cluster of cheap car 
be in unwealthy neighborhoods. 

Visual Census of Cities and Neighborhoods 

Using the output of our car classifier, we can answer diverse 
question range from which city have the most expensive 
car (New York, NY - $11,820.62), high percentage of 
Hummers (El Paso, TX - 0.39%), high percentage of for- 
eign car (San Francisco, CA - 60.02%) or low percent- 
age of foreign car (Casper Wyoming - 21%). We calculate 
these value by take the expectation of the various car at- 
tribute across all image in each city. The average number 
of instance of a particular car in a region be the sum of the 
expect number of instance of that car across all image in 
the region. Thus, compute the average number of car with 
a particular attribute in a city (or zip code) consists of calcu- 
lating this expectation for each image in the region and ag- 
gregating this value over all image in the city (or zip code). 

With I an image and c one of the 2,657 classes, we calcu- 
late the expect number of car of type c in a single image 

a 
E[#class c|I] = 

X 

bbox b 

P (car|b, I)P (class c|car, b, I) (2) 

where bbox b be the set of all detect car bound box 
in the city (or zip code), P (car|b, I) be the probability of a 
bound box containg a car (determined by our detection 
system) and P (class c|car, b, I) corresponds to the condi- 
tional probability output by the softmax layer of our CNN 
classifier. 

To obtain the expect value of a particular car attribute, 
e.g. the percentage of Hummers in a region, we aggregate 
category level expectation for all class whose make be 
Hummer. We follow this setup in all other experiments, and 
note that a similar procedure can be use to find expect 
value for any other car attribute, include car price, mile 
per gallon, and country of origin. 

To estimate the accuracy of our city-level car attribute 
predictions, we compare the distribution of car we de- 
tect in Street View image with the distribution of regis- 
tered car in Boston, Worcester and Springfield, MA (the 
three Massachusetts city in our dataset). We perform this 
comparison use record from Massachusetts DMV, the 
only state to publicly release extensive vehicle registration 
data (DMV 2014). We measure the Pearson correlation co- 
efficient between each detect and register make’s dis- 
tribution across zip codes. Twenty five of the top thirty 
make have a Pearson’s r correlation of r>0.5. Beyond Mas- 
sachusetts, we measure the correlation between our detect 
car make distribution and the 2011 national distribution of 
car make a r=0.97, verify the efficacy of our approach. 

Car attribute can reveal aspect of a city’s character that 
be not directly car-related. For example, our measurement 
show Burlington, Vermont to be the greenest city, with the 
high average mile per gallon (MPG) of any city in our 
dataset (average MPG=25.34). Burlington be also the first 
city in the United States to be power by 100% renew- 
able energy (PBS 2015). In contrast, we measure the low- 
est average MPG for Casper, WY (average MPG=21.28). 
Wyoming be the high coal-mining state in the US, produc- 
ing 39 percent of the nation’s coal in 2012 and emit the 
high rate of CO2 per person in the country (EIA 2015b). 
We aggregate these city-level statistic by state to discover 
pattern across the country. For example, by mapping our 
detect average MPG for each state in Fig. 5, we can see 
that coastal state be greener than inland ones, a find that 
agrees with publish carbon footprint (EIA 2015a) (r=- 
0.66 between our calculate MPG and carbon footprints). 

Income and Crime We can compare cities’ income- 
base segregation level use our car detections. After cal- 
culating the average car price for each GPS point, we mea- 
sure the level of cluster between similarly price car us- 
ing Moran’s I statistic (Moran 1950) define a 

I = 
N 

P 
i,j wi,j(xi � x̄)(xj � x̄)P 
i,j wi,j 

P 
i(xi � x̄)2 

(3) 

where each xi be a distinct GPS point, x̄ be the average of 
x and wi,j be a weight describe the similarity between 



Figure 5: (A) A map rank each state’s carbon footprint from the transportation sector in 2012 use data from (EIA 2015a). 
(B) A map rank each state’s average mile per gallon (MPG) calculate from car attribute detect from Google Street 
View. We measure a Pearson correlation coefficient of -0.66 between 2012 state level carbon footprint from the transportation 
sector and our calculate state average MPGs. Both map show that coastal state be greener than inland ones. 

point i and j. We use the square inverse Euclidean dis- 
tance a a similarity metric. Moran’s I of 1, -1 and 0 in- 
dicate total segregation, no segregation and a random pat- 
tern of segregation respectively. To gain further insight 
we visualize statistically significant cluster of expensive 
and cheap car use the Getis-Ord statistic, a more local 
measure of spatial autocorrelation (Getis and Ord 1992; 
Ord and Getis 1995). Fig. 4 show our result for 22 city 
with dense GPS sampling. 

Chicago be the most segregate city, with two large clus- 
ters of expensive and cheap car on the West and East 
side of the city respectively. Conversely, Jacksonville be the 
least segregate city with a Morans I only 33% a large 
a Chicago’s and exhibit little cluster of expensive and 
cheap cars. As show in Fig. 4B and Fig. 4C, Chicago’s 
cluster of expensive and cheap car fall in high and low 
income neighborhood respectively. Our result agree with 
finding from the Martin Prosperity Institute (Florida and 
Mellander 2015), rank Chicago, IL and Philadelphia, PA 
among the most segregate and Jacksonville, FL among the 
least segregate American cities. 

Our segregation analysis suggests that we can train a 
model to accurately predict a region’s income level from 
the property of it cars. To this end we first represent each 
zip code by an 88 dimensional vector comprise of car- 
related attribute such a the average MPG, the percentage 
of each body type, the average car price and the percent- 
age of each car make in the zip code. We then use 18% of 
our data to train a ridge regression model (Hoerl and Ken- 
nard 1970) predict median household income from car 
features. Remarkably, our model achieves a city level corre- 
lation of r=0.82 and a zip code level correlation of r=0.70 
with ground truth income data obtain from ACS (ACS 
2012) (p<1e-7). 

Investigating the relationship between income and in- 
dividual car attribute show a high correlation between 
median household income and the average car price in a 
zip code (r=0.44, p<< 0.001). As expected, wealthy peo- 
ple drive expensive cars. Perhaps surprisingly however, we 
found the most positively correlate car attribute with in- 

come to be the percentage of foreign manufacture car 
(r=0.47). In agreement with our results, Experian Automo- 
tive’s 2011 rank show that all of the top 10 car model 
prefer by wealthy individual be foreign, even when 
the car itself be comparatively cheap (e.g. Honda Accord 
or Toyota Camry) (Muller 2015). 

Following the same procedure, we predict burglary rate 
for city in our test set and achieve a pearson correlation of 
0.61 with ground truth data obtain from (Relocation Es- 
sentials 2015). While one of the best indicator of crime be 
the percentage of van (r=0.30 for total crime against peo- 
ple and properties), the single best predictor of unsafe zip 
code be the number of car per image (r=0.31 and r=0.36 for 
crime against people and property respectively). Accord- 
ing to study conduct by law enforcement, many crime 
be commit in area with a high density of car such a 
parking lot (Smith 1996), and some department be help- 
ing design neighborhood with a low number of park 
car on the street in order to reduce crime (Nevius 2011). 

Conclusion 

Through our analysis of 50 million image across 200 cities, 
we have show that car detect from Google Street View 
image contain predictive information about our neighbor- 
hoods, city and their demographic makeup. To facilitate 
this work, we have collect the large and most challeng- 
ing fine-grained dataset report to date and use it to train 
an ultra large scale car detection model. Using our system 
and a single source of visual data, we have predict in- 
come levels, crime rates, pollution level and gain insight 
into the relationship between car and people. In contrast to 
our automate method which quickly determines these vari- 
ables, this data be traditionally collect through costly and 
labor intensive survey conduct over multiple years. And 
while our method us a single source of publicly avail- 
able images, socioeconomic, crime, pollution, and car re- 
lated market research data be collect by disparate orga- 
nizations who keep the information for private use. Our ap- 
proach, couple with the increase proliferation of Street 



View and satellite imagery have the potential to enable close 
to real time census prediction in the future—augmenting or 
supplant survey base method of demographic data col- 
lection in the US. Our future work will investigate predict 
other demographic variable such a race, education level 
and voting pattern use the same methodology. 

Acknowledgments 

We thank Stefano Ermon, Neal Jean, Oliver Groth, Serena 
Yeung, Alexandre Alahi, Kevin Tang and everyone in the 
Stanford Vision Lab for valuable feedback. This research be 
partially support by an NSF grant (IIS-1115493), the Stan- 
ford DARE fellowship (to T.G.) and by NVIDIA (through 
donate GPUs). 

References 

ACS. 2012. American Community Survey 5 Year Data 
(2008-2012). http://www.census.gov/data/developers/data- 
sets/acs-survey-5-year-data.html. Accessed: 2014-9. 
Arietta, S. M.; Efros, A. A.; Ramamoorthi, R.; and 
Agrawala, M. 2014. City forensics: Using visual element 
to predict non-visual city attributes. IEEE transaction on 
visualization and computer graphic 20(12):2624–2633. 
Barlow, R. E.; Bartholomew, D. J.; Bremner, J.; and Brunk, 
H. D. 1972. Statistical inference under order restrictions: 
the theory and application of isotonic regression. Wiley 
New York. 
Ben-David, S.; Blitzer, J.; Crammer, K.; Pereira, F.; et al. 
2007. Analysis of representation for domain adaptation. 
Advances in neural information processing system 19:137. 
Blumenstock, J.; Cadamuro, G.; and On, R. 2015. Predicting 
poverty and wealth from mobile phone metadata. Science 
350(6264):1073–1076. 
Chase, R. 2009. Does everyone in america own a car? 
@ONLINE. 
Cheng, J.; Danescu-Niculescu-Mizil, C.; and Leskovec, J. 
2014. How community feedback shape user behavior. In 
Eighth International AAAI Conference on Weblogs and So- 

cial Media. 
Choo, S., and Mokhtarian, P. L. 2004. What type of vehicle 
do people drive? the role of attitude and lifestyle in influenc- 
ing vehicle type choice. Transportation Research Part A: 
Policy and Practice 38(3):201–222. 
Daumé III, H. 2007. Frustratingly easy domain adaptation. 
In Conference of the Association for Computational Linguis- 
tic (ACL). 
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei- 
Fei, L. 2009. Imagenet: A large-scale hierarchical im- 
age database. In Computer Vision and Pattern Recognition, 
2009. CVPR 2009. IEEE Conference on, 248–255. IEEE. 
DMV, M. 2014. 37 billion mile data challenge. 
http://www.37billionmilechallenge.org/. Accessed: 2014- 
10. 
EIA. 2015a. State CO 

2 

Emissions. 
http://www.eia.gov/environment/emissions/state/. Ac- 
cessed: 2015-2. 

EIA. 2015b. Wyoming State Profile and Energy Estimates. 
http://www.eia.gov/state/?sid=wy. Accessed: 2015-2. 
Felzenszwalb, P. F.; Girshick, R. B.; McAllester, D.; and Ra- 
manan, D. 2010. Object detection with discriminatively 
train part base models. IEEE Transactions on Pattern 
Analysis and Machine Intelligence 32(9):1627–1645. 
Florida, R., and Mellander, C. 2015. Segregated City: 
The Geography of Economic Segregation in Americas Met- 
ros. Technical report, Martin Prosperity Institute, Rotman 
School of Management, University of Toronto. Accessed: 
2015-3. 
Getis, A., and Ord, J. K. 1992. The analysis of spatial asso- 
ciation by use of distance statistics. Geographical analysis 
24(3):189–206. 
Girshick, R. 2015. Fast r-cnn. In Proceedings of the IEEE 
International Conference on Computer Vision, 1440–1448. 
Hoerl, A. E., and Kennard, R. W. 1970. Ridge regression: 
Biased estimation for nonorthogonal problems. Technomet- 
rics 12(1):55–67. 
Jean, N.; Burke, M.; Xie, M.; Davis, W. M.; Lobell, D. B.; 
and Ermon, S. 2016. Combining satellite imagery and ma- 
chine learn to predict poverty. Science 353(6301):790– 
794. 
Krause, J.; Jin, H.; Yang, J.; and Fei-Fei, L. 2015a. Fine- 
grain recognition without part annotations. In Proceed- 
ings of the IEEE Conference on Computer Vision and Pat- 

tern Recognition, 5546–5555. 
Krause, J.; Sapp, B.; Howard, A.; Zhou, H.; Toshev, A.; 
Duerig, T.; Philbin, J.; and Fei-Fei, L. 2015b. The unreason- 
able effectiveness of noisy data for fine-grained recognition. 
arXiv preprint arXiv:1511.06789. 
Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. 
Imagenet classification with deep convolutional neural net- 
works. In Advances in neural information processing sys- 
tems, 1097–1105. 
Michel, J.-B.; Shen, Y. K.; Aiden, A. P.; Veres, A.; Gray, 
M. K.; Pickett, J. P.; Hoiberg, D.; Clancy, D.; Norvig, P.; 
Orwant, J.; et al. 2011. Quantitative analysis of culture use 
million of digitize books. science 331(6014):176–182. 
Moran, P. A. 1950. Notes on continuous stochastic phenom- 
ena. Biometrika 17–23. 
Muller, J. 2015. What The Rich People Really Drive. 
http://www.forbes.com/sites/joannmuller/2011/12/30/what- 
the-rich-people-really-drive/. Accessed: 2015-2. 
Naik, N.; Philipoom, J.; Raskar, R.; and Hidalgo, C. 2014. 
Streetscore–predicting the perceive safety of one million 
streetscapes. In Computer Vision and Pattern Recognition 
Workshops (CVPRW), 2014 IEEE Conference on, 793–799. 
IEEE. 
Nevius, W. 2011. Banning parking on tenderloin block 
drive trouble away. SF Gate 1. Accessed: 2015-4. 
Ord, J. K., and Getis, A. 1995. Local spatial autocorrela- 
tion statistics: distributional issue and an application. Geo- 
graphical analysis 27(4):286–306. 



Ordonez, V., and Berg, T. L. 2014. Learning high-level 
judgment of urban perception. In Computer Vision–ECCV 
2014. Springer. 494–510. 
PBS. 2015. Running on renewable energy, Burling- 
ton, Vermont power green movement forward. 
http://www.pbs.org/newshour/bb/vermont-city-come- 
rely-100-percent-renewable-energy/. Accessed: 2015-2. 
Relocation Essentials. 2015. Relocation Essentials. 
http://www.relocationessentials.com/. Accessed: 2015-03. 
Smith, M. S. 1996. Crime prevention through environmen- 
tal design in parking facilities. US Department of Justice, 
Office of Justice Programs, National Institute of Justice. Ac- 
cessed: 2015-04. 
West, R.; Paskov, H. S.; Leskovec, J.; and Potts, C. 2014. 
Exploiting social network structure for person-to-person 
sentiment analysis. arXiv preprint arXiv:1409.2450. 
Zhou, B.; Liu, L.; Oliva, A.; and Torralba, A. 2014. Recog- 
nizing city identity via attribute analysis of geo-tagged im- 
ages. In Computer Vision–ECCV 2014. Springer. 519–534. 


