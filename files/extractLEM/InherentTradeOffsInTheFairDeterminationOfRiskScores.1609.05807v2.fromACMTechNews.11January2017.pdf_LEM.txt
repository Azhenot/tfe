














































ar 
X 

iv 
:1 

60 
9. 

05 
80 

7v 
2 

[ 
c 

.L 
G 

] 
1 

7 
N 

ov 
2 

01 
6 

Inherent Trade-Offs in the Fair Determination of Risk Scores 

Jon Kleinberg ∗ Sendhil Mullainathan † Manish Raghavan ‡ 

Abstract 

Recent discussion in the public sphere about algorithmic classification have involve tension between 
compete notion of what it mean for a probabilistic classification to be fair to different groups. We 
formalize three fairness condition that lie at the heart of these debates, and we prove that except in highly 
constrain special cases, there be no method that can satisfy these three condition simultaneously. 
Moreover, even satisfy all three condition approximately require that the data lie in an approximate 
version of one of the constrain special case identify by our theorem. These result suggest some 
of the way in which key notion of fairness be incompatible with each other, and hence provide a 
framework for think about the trade-off between them. 

1 Introduction 

There be many setting in which a sequence of people come before a decision-maker, who must make a 
judgment about each base on some observable set of features. Across a range of applications, these judg- 
ments be be carry out by an increasingly wide spectrum of approach range from human expertise 
to algorithmic and statistical frameworks, a well a various combination of these approaches. 

Along with these developments, a grow line of work have ask how we should reason about issue of bias 
and discrimination in setting where these algorithmic and statistical techniques, train on large datasets 
of past instances, play a significant role in the outcome. Let u consider three example where such issue 
arise, both to illustrate the range of relevant contexts, and to surface some of the challenges. 

A set of example domains. First, at various point in the criminal justice system, include decision 
about bail, sentencing, or parole, an officer of the court may use quantitative risk tool to ass a defendant’s 
probability of recidivism — future arrest — base on their past history and other attributes. Several recent 
analysis have ask whether such tool be mitigate or exacerbate the source of bias in the criminal 
justice system; in one widely-publicized report, Angwin et al. analyze a commonly use statistical method 
for assign risk score in the criminal justice system — the COMPAS risk tool — and argue that it be 
bias against African-American defendant [2, 23]. One of their main contention be that the tool’s error 
be asymmetric: African-American defendant be more likely to be incorrectly label a higher-risk 
than they actually were, while white defendant be more likely to be incorrectly label a lower-risk than 
they actually were. Subsequent analysis raise methodological objection to this report, and also observe 
that despite the COMPAS risk tool’s errors, it estimate of the probability of recidivism be equally well 
calibrate to the true outcome for both African-American and white defendant [1, 10, 13, 17]. 

∗Cornell University 
†Harvard University 
‡Cornell University 

1 

http://arxiv.org/abs/1609.05807v2 


Second, in a very different domain, researcher have begin to analyze the way in which different gender 
and racial group experience advertising and commercial content on the Internet differently [9, 26]. We 
could ask, for example: if a male user and female user be equally interested in a particular product, do 
it follow that they’re equally likely to be show an ad for it? Sometimes this concern may have broader 
implications, for example if woman in aggregate be show ad for lower-paying jobs. Other times, it may 
represent a clash with a user’s leisure interests: if a female user interact with an advertising platform be 
interested in an activity that tends to have a male-dominated viewership, like professional football, be the 
platform a likely to show her an ad for football a it be to show such an ad to an interested male user? 

A third domain, again quite different from the previous two, be medical test and diagnosis. Doctors 
make decision about a patient’s treatment may rely on test provide probability estimate for different 
disease and conditions. Here too we can ask whether such decision-making be be apply uniformly 
across different group of patient [16, 27], and in particular how medical test may play a differential role 
for condition that vary widely in frequency between these groups. 

Providing guarantee for decision procedures. One can raise analogous question in many other do- 
main of fundamental importance, include decision about hiring, lending, or school admission [24], but 
we will focus on the three example above for the purpose of this discussion. In these three example do- 
mains, a few structural commonality stand out. First, the algorithmic estimate be often be use a 
“input” to a large framework that make the overall decision — a risk score provide to a human expert in 
the legal and medical instances, and the output of a machine-learning algorithm provide to a large adver- 
tising platform in the case of Internet ads. Second, the underlie task be generally about classify whether 
people posse some relevant property: recidivism, a medical condition, or interest in a product. We will 
refer to people a be positive instance if they truly posse the property, and negative instance if they 
do not. Finally, the algorithmic estimate be provide for these question be generally not pure yes-no 
decisions, but instead probability estimate about whether people constitute positive or negative instances. 

Let u suppose that we be concerned about how our decision procedure might operate differentially between 
two group of interest (such a African-American and white defendants, or male and female user of an 
advertising system). What sort of guarantee should we ask for a protection against potential bias? 

A first basic goal in this literature be that the probability estimate provide by the algorithm should be 
well-calibrated: if the algorithm identifies a set of people a have a probability z of constitute positive 
instances, then approximately a z fraction of this set should indeed be positive instance [8, 14]. Moreover, 
this condition should hold when apply separately in each group a well [13]. For example, if we be 
think in term of potential difference between outcome for men and women, this mean require that a 
z fraction of men and a z fraction of woman assign a probability z should posse the property in question. 

A second goal focus on the people who constitute positive instance (even if the algorithm can only 
imperfectly recognize them): the average score receive by people constitute positive instance should 
be the same in each group. We could think of this a balance for the positive class, since a violation of it 
would mean that people constitute positive instance in one group receive consistently low probability 
estimate than people constitute positive instance in another group. In our initial criminal justice example, 
for instance, one of the concern raise be that white defendant who go on to commit future crime 
be assign risk score correspond to low probability estimate in aggregate; this be a violation of 
the condition here. There be a completely analogous property with respect to negative instances, which we 
could call balance for the negative class. These balance condition can be view a generalization of the 
notion that both group should have equal false negative and false positive rates. 

It be important to note that balance for the positive and negative classes, a define here, be distinct in 

2 



crucial way from the requirement that the average probability estimate globally over all member of the 
two group be equal. This latter global requirement be a version of statistical parity [12, 4, 21, 22]. In some 
case statistical parity be a central goal (and in some it be legally mandated), but the example consider 
so far suggest that classification and risk assessment be much broader activity where statistical parity be 
often neither feasible nor desirable. Balance for the positive and negative classes, however, be a goal that can 
be discuss independently of statistical parity, since these two balance condition simply ask that once we 
condition on the “correct” answer for a person, the chance of make a mistake on them should not depend 
on which group they belong to. 

The present work: Trade-offs among the guarantees. Despite their different formulations, the calibra- 
tion condition and the balance condition for the positive and negative class intuitively all seem to be 
ask for variant of the same general goal — that our probability estimate should have the same effec- 
tiveness regardless of group membership. One might therefore hope that it would be feasible to achieve all 
of them simultaneously. 

Our main result, however, be that these condition be in general incompatible with each other; they can only 
be simultaneously satisfied in certain highly constrain cases. Moreover, this incompatibility applies to 
approximate version of the condition a well. 

In the remainder of this section we formulate this main result precisely, a a theorem building on a model 
that make the discussion thus far more concrete. 

1.1 Formulating the Goal 

Let’s start with some basic definitions. As above, we have a collection of people each of whom constitutes 
either a positive instance or a negative instance of the classification problem. We’ll say that the positive 
class consists of the people who constitute positive instances, and the negative class consists of the people 
who constitute negative instances. For example, for criminal defendants, the positive class could consist of 
those defendant who will be arrest again within some fix time window, and the negative class could 
consist of those who will not. The positive and negative class thus represent the “correct” answer to the 
classification problem; our decision procedure do not know them, but be try to estimate them. 

Feature vectors. Each person have an associate feature vector σ, represent the data that we know 
about them. Let pσ denote the fraction of people with feature vector σ who belong to the positive class. 
Conceptually, we will picture that while there be variation within the set of people who have feature vector 
σ, this variation be invisible to whatever decision procedure we apply; all people with feature vector σ be 
indistinguishable to the procedure. Our model will assume that the value pσ for each σ be know to the 
procedure.1 

Groups. Each person also belongs to one of two groups, label 1 or 2, and we would like our decision 
to be unbiased with respect to the member of these two groups.2 In our examples, the two group could 
correspond to different race or genders, or other case where we want to look for the possibility of bias 
between them. The two group have different distribution over feature vectors: a person of group t have a 
probability atσ of exhibit the feature vector σ. However, people of each group have the same probability 

1Clearly the case in which the value of pσ be unknown be an important version of the problem a well; however, since our main 
result establish strong limitation on what be achievable, these limitation be only strong because they apply even to the case of 
know pσ. 

2We focus on the case of two group for simplicity of exposition, but it be straightforward to extend all of our definition to the 
case of more than two groups. 

3 



pσ of belonging to the positive class provide their feature vector be σ. In this respect, σ contains all the 
relevant information available to u about the person’s future behavior; once we know σ, we do not get any 
additional information from know their group a well.3 

Risk Assignments. We say that an instance of our problem be specify by the parameter above: a feature 
vector and a group for each person, with a value pσ for each feature vector, and distribution {atσ} give 
the frequency of the feature vector in each group. 

Informally, risk assessment be way of divide people up into set base on their feature vector σ (po- 
tentially use randomization), and then assign each set a probability estimate that the people in this set 
belong to the positive class. Thus, we define a risk assignment to consist of a set of “bins” (the sets), where 
each bin be label with a score vb that we intend to use a the probability for everyone assign to bin b. 
We then create a rule for assign people to bin base on their feature vector σ; we allow the rule to di- 
vide people with a fix feature vector σ across multiple bin (reflecting the possible use of randomization). 
Thus, the rule be specify by value Xσb: a fraction Xσb of all people with feature vector σ be assign 
to bin b. Note that the rule do not have access to the group t of the person be considered, only their 
feature vector σ. (As we will see, this do not mean that the rule be incapable of exhibit bias between 
the two groups.) In summary, a risk assignment be specify by a set of bins, a score for each bin, and value 
Xσb that define a mapping from people with feature vector to bins. 

Fairness Properties for Risk Assignments. Within the model, we now express the three condition dis- 
cuss at the outset, each reflect a potentially different notion of what it mean for the risk assignment to 
be “fair.” 

(A) Calibration within group require that for each group t, and each bin b with associate score vb, the 
expect number of people from group t in b who belong to the positive class should be a vb fraction 
of the expect number of people from group t assign to b. 

(B) Balance for the negative class require that the average score assign to people of group 1 who 
belong to the negative class should be the same a the average score assign to people of group 2 
who belong to the negative class. In other words, the assignment of score shouldn’t be systematically 
more inaccurate for negative instance in one group than the other. 

(C) Balance for the positive class symmetrically require that the average score assign to people of 
group 1 who belong to the positive class should be the same a the average score assign to people 
of group 2 who belong to the positive class. 

Why Do These Conditions Correspond to Notions of Fairness?. All of these be natural condition to 
impose on a risk assignment; and a indicate by the discussion above, all of them have be propose a 
version of fairness. The first one essentially asks that the score mean what they claim to mean, even when 
consider separately in each group. In particular, suppose a set of score lack the first property for some 
bin b, and these score be give to a decision-maker; then if people of two different group both belong to 
bin b, the decision-maker have a clear incentive to treat them differently, since the lack of calibration within 
group on bin b mean that these people have different aggregate probability of belonging to the positive 
class. Another way of state the property of calibration within group be to say that, condition on the 
bin to which an individual be assigned, the likelihood that the individual be a member of the positive class 
be independent of the group to which the individual belongs. This mean we be justified in treat people 

3As we will discus in more detail below, the assumption that the group provide no additional information beyond σ do not 
restrict the generality of the model, since we can always consider instance in which people of different group never have the same 
feature vector σ, and hence σ implicitly conveys perfect information about a person’s group. 

4 



with the same score comparably with respect to the outcome, rather than treat people with the same score 
differently base on the group they belong to. 

The second and third ask that if two individual in different group exhibit comparable future behavior 
(negative or positive), they should be treat comparably by the procedure. In other words, a violation of, 
say, the second condition would correspond to the member of the negative class in one group receive 
consistently high score than the member of the negative class in the other group, despite the fact that the 
member of the negative class in the higher-scoring group have do nothing to warrant these high scores. 

We can also interpret some of the prior work around our early example through the lens of these condi- 
tions. For example, in the analysis of the COMPAS risk tool for criminal defendants, the critique by Angwin 
et al. focus on the risk tool’s violation of condition (B) and (C); the counter-arguments establish that 
it satisfies condition (A). While it be clearly crucial for a risk tool to satisfy (A), it may still be important to 
know that it violates (B) and (C). Similarly, to think in term of the example of Internet advertising, with 
male and female user a the two groups, condition (A) a before require that our estimate of ad-click 
probability mean the same thing in aggregate for men and women. Conditions (B) and (C) be distinct; con- 
dition (C), for example, say that a female user who genuinely want to see a give ad should be assign 
the same probability a a male user who want to see the ad. 

1.2 Determining What be Achievable: A Characterization Theorem 

When can condition (A), (B), and (C) be simultaneously achieved? We begin with two simple case where 
it’s possible. 

• Perfect prediction. Suppose that for each feature vector σ, we have either pσ = 0 or pσ = 1. This 
mean that we can achieve perfect prediction, since we know each person’s class label (positive or 
negative) for certain. In this case, we can assign all feature vector σ with pσ = 0 to a bin b with score 
vb = 0, and all σ with pσ = 1 to a bin b′ with score vb′ = 1. It be easy to check that all three of the 
condition (A), (B), and (C) be satisfied by this risk assignment. 

• Equal base rates. Suppose, alternately, that the two group have the same fraction of member in the 
positive class; that is, the average value of pσ be the same for the member of group 1 and group 2. (We 
can refer to this a the base rate of the group with respect to the classification problem.) In this case, 
we can create a single bin b with score equal to this average value of pσ, and we can assign everyone 
to bin b. While this be not a particularly informative risk assignment, it be again easy to check that it 
satisfies fairness condition (A), (B), and (C). 

Our first main result establishes that these be in fact the only two case in which a risk assignment can 
achieve all three fairness guarantee simultaneously. 

Theorem 1.1 Consider an instance of the problem in which there be a risk assignment satisfy fairness 
condition (A), (B), and (C). Then the instance must either allow for perfect prediction (with pσ equal to 0 
or 1 for all σ) or have equal base rates. 

Thus, in every instance that be more complex than the two case note above, there will be some natural 
fairness condition that be violate by any risk assignment. Moreover, note that this result applies regardless 
of how the risk assignment be computed; since our framework considers risk assignment to be arbitrary 
function from feature vector to bin label with probability estimates, it applies independently of the 
method — algorithmic or otherwise — that be use to construct the risk assignment. 

5 

lphilippe 
Texte surligné 



The conclusion of the first theorem can be relaxed in a continuous fashion when the fairness condition be 
only approximate. In particular, for any ε > 0 we can define ε-approximate version of each of condition 
(A), (B), and (C) (specified precisely in the next section), each of which require that the correspond 
equality between group hold only to within an error of ε. For any δ > 0, we can also define a δ- 
approximate version of the equal base rate condition (requiring that the base rate of the two group be 
within an additive δ of each other) and a δ-approximate version of the perfect prediction condition (requiring 
that in each group, the average of the expect score assign to member of the positive class be at least 
1 − δ; by the calibration condition, this can be show to imply a complementary bound on the average of 
the expect score assign to member of the negative class). 

In these terms, our approximate version of Theorem 1.1 be the following. 

Theorem 1.2 There be a continuous function f , with f(x) go to 0 a x go to 0, so that the follow 
holds. For all ε > 0, and any instance of the problem with a risk assignment satisfy the ε-approximate 
version of fairness condition (A), (B), and (C), the instance must satisfy either the f(ε)-approximate 
version of perfect prediction or the f(ε)-approximate version of equal base rates. 

Thus, anything that approximately satisfies the fairness constraint must approximately look like one of the 
two simple case identify above. 

Finally, in connection to Theorem 1.1, we note that when the two group have equal base rates, then one 
can ask for the most accurate risk assignment that satisfies all three fairness condition (A), (B), and (C) 
simultaneously. Since the risk assignment that give the same score to everyone satisfies the three conditions, 
we know that at least one such risk assignment exists; hence, it be natural to seek to optimize over the set of 
all such assignments. We consider this algorithmic question in the final technical section of the paper. 

To reflect a bit further on our main theorem and what they suggest, we note that our intention in the present 
work isn’t to make a recommendation on how conflict between different definition of fairness should be 
handled. Nor be our intention to analyze which definition of fairness be violate in particular application 
or datasets. Rather, our point be to establish certain unavoidable trade-off between the definitions, regardless 
of the specific context and regardless of the method use to compute risk scores. Since each of the definition 
reflect (and have be propose as) natural notion of what it should mean for a risk score to be fair, these 
trade-off suggest a strike implication: that outside of narrowly delineate cases, any assignment of risk 
score can in principle be subject to natural criticism on the ground of bias. This be equally true whether 
the risk score be determine by an algorithm or by a system of human decision-makers. 

Special Cases of the Model. Our main results, which place strong restriction on when the three fairness 
condition can be simultaneously satisfied, have more power when the underlie model of the input be more 
general, since it mean that the restriction imply by the theorem apply in great generality. However, 
it be also useful to note certain special case of our model, obtain by limit the flexibility of certain 
parameter in intuitive ways. The point be that our result apply a fortiori to these more limited special 
cases. 

First, we have already observe one natural special case of our model: case in which, for each feature 
vector σ, only member of one group (but not the other) can exhibit σ. This mean that σ contains perfect 
information about group membership, and so it corresponds to instance in which risk assignment would 
have the potential to use knowledge of an individual’s group membership. Note that we can convert any 
instance of our problem into a new instance that belongs to this special case a follows. For each feature 
vector σ, we create two new feature vector σ(1) and σ(2); then, for each member of group 1 who have feature 
vector σ, we assign them σ(1), and for each member of group 2 who have feature vector σ, we assign them 

6 



σ(2). The result instance have the property that each feature vector be associate with member of only one 
group, but it preserve the essential aspect of the original instance in other respects. 

Second, we allow risk assignment in our model to split people with a give feature vector σ over several 
bins. Our result also therefore apply to the natural special case of the model with integral risk assignments, 
in which all people with a give feature σ must go to the same bin. 

Third, our model be a generalization of binary classification, which only allows for 2 bins. Note that although 
binary classification do not explicitly assign scores, we can consider the probability that an individual 
belongs to the positive class give that they be assign to a specific bin to be the score for that bin. Thus, 
our result hold in the traditional binary classification set a well. 

Data-Generating Processes. Finally, there be the question of where the data in an instance of our problem 
come from. Our result do not assume any particular process for generate the positive/negative class 
labels, feature vectors, and group memberships; we simply assume that we be give such a collection of 
value (regardless of where they come from), and then our result address the existence or non-existence of 
certain risk assignment for these values. 

This increase the generality of our results, since it mean that they apply to any process that produce data 
of the form described by our model. To give an example of a natural generative model that would produce 
instance with the structure that we need, one could assume that each individual start with a “hidden” class 
label (positive or negative), and a feature vector σ be then probabilistically generate for this individual from 
a distribution that can depend on their class label and their group membership. (If feature vector produce 
for the two group be disjoint from one another, then the requirement that the value of pσ be independent of 
group membership give σ necessarily holds.) Since a process with this structure produce instance from 
our model, our result apply to data that arises from such a generative process. 

It be also interest to note that the basic set-up of our model, with the population divide across a set 
of feature vector for which race provide no additional information, be in fact a very close match to the 
information one get from the output of a well-calibrated risk tool. In this sense, one set for our model 
would be the problem of apply post-processing to the output of such a risk tool to ensure additional 
fairness guarantees. Indeed, since much of the recent controversy about fair risk score have involve risk 
tool that be well-calibrated but lack the other fairness condition we consider, such an interpretation of the 
model could be a useful way to think about how one might work with these tool in the context of a broader 
system. 

1.3 Further Related Work 

Mounting concern over discrimination in machine learn have lead to a large body of new work seek 
to good understand and prevent it. Barocas and Selbst survey a range of way in which data-analysis 
algorithm can lead to discriminatory outcome [3], and review article by Romei and Ruggieri [25] and 
Zliobaite [30] survey data-analytic and algorithmic method for measure discrimination. 

Kamiran and Calders [21] and Hajian and Domingo-Ferrer [18] seek to modify datasets to remove any 
information that might permit discrimination. Similarly, Zemel et al. look to learn fair intermediate repre- 
sentations of data while preserve information need for classification [29]. Joseph et al. consider how 
fairness issue can arise during the process of learning, model this use a multi-armed bandit framework 
[20]. 

7 



One common notion of fairness be “statistical parity” – equal fraction of each group should be treat a 
belonging to the positive class [4, 21, 22]. Recent paper have also consider approximate relaxation of 
statistical parity, motivate by the formulation of disparate impact in the U.S. legal code [12, 28]. Work in 
these direction have developed learn algorithm that penalize violation of statistical parity [4, 22]. As 
note above, we consider definition other than statistical parity that take into account the class membership 
(positive or negative) of the people be classified. 

Dwork et al. propose a framework base on a task-specific externally define similarity metric between 
individuals, seek to achieve fairness through the goal that “similar people [be] treat similarly” [11]. 
They strive towards individual fairness, which be a strong notion of fairness than the definition we use; 
however, our approach share some of the underlie motivation (though not the specifics) in that our 
balance condition for the positive and negative class also reflect the notion that similar people should be 
treat similarly. 

Much of the apply work on risk scores, a note above, focus on calibration a a central goal [8, 10, 13]. 
In particular, respond to the criticism of their risk score a display asymmetric error for different 
groups, Dietrich et al. note that empirically, both in their domain and in similar settings, it be typically 
difficult to achieve symmetry in the error rate across group when base rate differ significantly. Our 
formulation of the balance condition for the positive and negative classes, and our result show the 
incompatibility of these condition with calibration, provide a theoretical basis for such observations. 

In recent work concurrent with ours, Hardt et al. consider the natural analogue of our condition (B) and (C), 
balance for the negative and positive classes, in the case of classifier that output binary “yes/no” prediction 
rather than real-valued score a in our case [19]. Since they do not require an analogue of calibration, it 
be possible to satisfy the two balance constraint simultaneously, and they provide method for optimize 
performance measure of the prediction rule subject to satisfy these two constraints. Also concurrent with 
our work and that of Hardt et al., Chouldechova [5] and Corbett-Davies et al. [6] (and see also [7]) consider 
binary prediction subject to these same analogue of the balance condition for the negative and positive 
classes, together with a form of calibration adapt to binary prediction (requiring that for all people give 
a positive label, the same fraction of people in each group should truly belong to the positive class). Among 
other results, they show that no classification rule satisfy the require constraint be possible. Finally, a 
recent paper of Friedler et al. [15] defines two axiomatic property of feature generation and show that no 
mechanism can be fair under these two properties. 

2 The Characterization Theorems 

Starting with the notation and definition from the previous section, we now give a proof of Theorem 1.1. 

Informal overview. Let u begin with a brief overview of the proof, before go into a more detailed 
version of it. For this discussion, let Nt denote the number of people in group t, and µt be the number of 
people in group t who belong to the positive class. 

Roughly speaking, the proof proceeds in two steps. First, consider a single bin b. By the calibration condi- 
tion, the expect total score give to the group-t people in bin b be equal to the expect number of group-t 
people in bin b who belong to the positive class. Summing over all bins, we find that the total score give 
to all people in group t (that is, the sum of the score receive by everyone in group t) be equal to the total 
number of people in the positive class in group t, which be µt. 

8 



Now, let x be the average score give to a member of the negative class, and let y be the average score give 
to a member of the positive class. By the balance condition for the negative and positive classes, these 
value of x and y be the same for both groups. 

Given the value of x and y, the total number of people in the positive class µt, and the total score give out 
to people in group t — which, a argue above, be also µt — we can write the total score a 

(N − µt)x+ µty = µt. 

This defines a line for each group t in the two variable x and y, and hence we obtain a system of two linear 
equation (one for each group) in the unknown x and y. 

If all three condition — calibration, and balance for the two class — be to be satisfied, then we must be 
at a set of parameter that represent a solution to the system of two equations. If the base rate be equal, 
then µ1 = µ2 and hence the two line be the same; in this case, the system of equation be satisfied by any 
choice of x and y. If the base rate be not equal, then the two line be distinct, and they intersect only at the 
point (x, y) = (0, 1), which implies perfect prediction — an average score of 0 for member of the negative 
class and 1 for member of the positive class. Thus, the three condition can be simultaneously satisfied if 
and only if we have equal base rate or perfect prediction. 

This concludes the overview of the proof; in the remainder of the section we describe the argument at a more 
detailed level. 

Definitions and notation. Recall from our notation in the previous section that an atσ fraction of the 
people in group t have feature vector σ; we thus write ntσ = atσNt for the number of people in group t with 
feature vector σ. Many of the component of the risk assignment and it evaluation can be write in term 
of operation on a set of underlie matrix and vectors, which we begin by specifying. 

• First, let |σ| denote the number of feature vector in the instance, and let p ∈ R|σ| be a vector indexed 
by the possible feature vectors, with the coordinate in position σ equal to pσ. For group t, let nt ∈ R|σ| 
also be a vector indexed by the possible feature vectors, with the coordinate in position σ equal to ntσ. 
Finally, it will be useful to have a representation of p a a diagonal matrix; thus, let P be a |σ| × |σ| 
diagonal matrix with Pσσ = pσ. 

• We now specify a risk assignment a follows. The risk assignment involves a set of B bin with 
associate scores; let v ∈ RB be a vector indexed by the bins, with the coordinate in position b equal 
to the score vb of bin b. Let V be a diagonal matrix version of v: it be a B × B matrix with Vbb = vb. 
Finally, let X be the |σ| × B matrix of Xσb values, specify the fraction of people with feature 
vector σ who get mapped to bin b under the assignment procedure. 

There be an important point to note about the Xσb values. If all of them be equal to 0 or 1, this corresponds 
to a procedure in which all people with the same feature vector σ get assign to the same bin. When some 
of the Xσb value be not equal to 0 or 1, the people with vector σ be be divide among multiple bins. 
In this case, there be an implicit randomization take place with respect to the positive and negative classes, 
and with respect to the two groups, which we can think of a follows. Since the procedure cannot distinguish 
among people with vector σ, in the case that it distributes these people across multiple bins, the subset of 
people with vector σ who belong to the positive and negative classes, and to the two groups, be divide 
up randomly across these bin in proportion correspond to Xσb. In particular, if there be ntσ group-t 
people with vector σ, the expect number of these people who belong to the positive class and be assign 
to bin b be ntσpσXσb. 

Let u now proceed with the proof of Theorem 1.1, start with the assumption that our risk assignment 
satisfies condition (A), (B), and (C). 

9 



Calibration within groups. We begin by work out some useful expression in term of the matrix 
and vector define above. We observe that n⊤t P be a vector in R 

|σ| whose coordinate correspond to 
feature vector σ equal the number of people in group t who have feature vector σ and belong to the positive 
class. n⊤t X be a vector in R 

B whose coordinate correspond to bin b equal the expect number of people 
in group t assign to bin b. 

By further multiply these vector on the right, we get additional useful quantities. Here be two in 
particular: 

• n⊤t XV be a vector in RB whose coordinate correspond to bin b equal the expect sum of the 
score assign to all group-t people in bin b. That is, use the subscript b to denote the coordinate 
correspond to bin b, we can write (n⊤t XV )b = vb(n 

⊤ 
t X)b by the definition of the diagonal matrix 

V . 

• n⊤t PX be a vector in RB whose coordinate correspond to bin b equal the expect number of 
group-t people in the positive class who be place in bin b. 

Now, condition (A), that the risk assignment be calibrate within groups, implies that the two vector above 
be equal coordinate-wise, and so we have the follow equation for all t: 

n⊤t PX = n 
⊤ 
t XV (1) 

Calibration condition (A) also have an implication for the total score receive by all people in group t. 
Suppose we multiply the two side of (1) on the right by the vector e ∈ RB whose coordinate be all 1, 
obtain 

n⊤t PXe = n 
⊤ 
t XV e. (2) 

The left-hand-side be the number of group-t people in the positive class. The right-hand-side, which we can 
also write a n⊤t Xv, be equal to the sum of the expect score receive by all group-t people. These two 
quantity be thus the same, and we write their common value a µt. 

Fairness to the positive and negative classes. We now want to write down vector equation correspond 
to the fairness condition (B) and (C) for the negative and positive classes. First, recall that for the B- 
dimensional vector n⊤t PX, the coordinate correspond to bin b equal the expect number of group-t 
people in the positive class who be place in bin b. Thus, to compute the sum of the expect score 
receive by all group-t people in the positive class, we simply need to take the inner product with the vector 
v, yield n⊤t PXv. Since µt be the total number of group-t people in the positive class, the average of the 

expect score receive by a group-t person in the positive class be the ratio 
1 

µt 
n⊤t PXv. Thus, condition 

(C), that member of the positive class should receive the same average score in each group, can be write 

1 

µ1 
n⊤1 PXv = 

1 

µ2 
n⊤2 PXv (3) 

Applying strictly analogous reason but to the fraction 1 − pσ of people in the negative class, we can 
write condition (B), that member of the negative class should receive the same average score in each group, 
a 

1 

N1 − µ1 
n⊤1 (I − P )Xv = 

1 

N2 − µ2 
n⊤2 (I − P )Xv (4) 

10 



Using (1), we can rewrite (3) to get 

1 

µ1 
n⊤1 XV v = 

1 

µ2 
n⊤2 XV v (5) 

Similarly, we can rewrite (4) a 

1 

N1 − µ1 
(µ1 − n⊤1 XV v) = 

1 

N2 − µ2 
(µ2 − n⊤2 XV v) (6) 

The portion of the score receive by the positive class. We think of the ratio on the two side of (3), 
and equivalently (5), a the average of the expect score receive by a member of the positive class in 
group t: the numerator be the sum of the expect score receive by the member of the positive class, and 
the denominator be the size of the positive class. Let u denote this fraction by γt; we note that this be the 
quantity y use in the informal overview of the proof at the start of the section. By (2), we can alternately 
think of the denominator a the sum of the expect score receive by all group-t people. Hence, the two 
side of (3) and (5) can be view a represent the ratio of the sum of the expect score in the positive 
class of group t to the sum of the expect score in group t a a whole. (3) require that γ1 = γ2; let u 
denote this common value by γ. 

Now, we observe that γ = 1 corresponds to a case in which the sum of the expect score in just the 
positive class of group t be equal to the sum of the expect score in all of group t. In this case, it must 
be that all member of the negative class be assign to bin of score 0. If any member of the positive 
class be assign to a bin of score 0, this would violate the calibration condition (A); hence all member 
of the positive class be assign to bin of positive score. Moreover, these bin of positive score contain 
no member of the negative class (since they’ve all be assign to bin of score 0), and so again by the 
calibration condition (A), the member of the positive class be all assign to bin of score 1. Finally, 
apply the calibration condition once more, it follow that the member of the negative class all have 
feature vector σ with pσ = 0 and the member of the positive class all have feature vector σ with pσ = 1. 
Hence, when γ = 1 we have perfect prediction. 

Finally, we use our definition of γt a 
1 

µt 
n⊤t XV v, and the fact that γ1 = γ2 = γ to write (6) a 

1 

N1 − µ1 
(µ1 − γµ1) = 

1 

N2 − µ2 
(µ2 − γµ2) 

1 

N1 − µ1 
µ1(1− γ) = 

1 

N2 − µ2 
µ2(1− γ) 

µ1/N1 
1− µ1/N1 

(1− γ) = µ2/N2 
1− µ2/N2 

(1− γ) 

Now, this last equality implies that one of two thing must be the case. Either 1 − γ = 0, in which case 
γ = 1 and we have perfect prediction; or 

µ1/N1 
1− µ1/N1 

= 
µ2/N2 

1− µ2/N2 
, 

in which case µ1/N1 = µ2/N2 and we have equal base rates. This completes the proof of Theorem 1.1. 

Some Comments on the Connection to Statistical Parity. Earlier we note that condition (B) and (C) 
— the balance condition for the positive and negative class — be quite different from the requirement of 
statistical parity, which asserts that the average of the score over all member of each group be the same. 

11 



When the two group have equal base rates, then the risk assignment that give the same score to everyone 
in the population achieves statistical parity along with condition (A), (B), and (C). But when the two 
group do not have equal base rates, it be immediate to show that statistical parity be inconsistent with both 
the calibration condition (A) and with the conjunction of the two balance condition (B) and (C). To see 
the inconsistency of statistical parity with the calibration condition, we take Equation (1) from the proof 
above, sum the coordinate of the vector on both sides, and divide by Nt, the number of people in group t. 
Statistical parity require that the right-hand side of the result equation be the same for t = 1, 2, while 
the assumption that the two group have unequal base rate implies that the left-hand side of the equation 
must be different for t = 1, 2. To see the inconsistency of statistical parity with the two balance condition 
(B) and (C), we simply observe that if the average score assign to the positive class and to the negative 
class be the same in the two groups, then the average of the score over all member of the two group 
cannot be the same provide they do not contain the same proportion of positive-class and negative-class 
members. 

3 The Approximate Theorem 

In this section we prove Theorem 1.2. First, we must first give a precise specification of the approximate 
fairness conditions: 

(1− ε)[n⊤t XV ]b ≤ [n⊤t PX]b ≤ (1− ε)[n⊤t XV ]b (A’) 

(1− ε) 
( 

1 

N2 − µ2 

) 

n⊤t (I − P )Xv ≤ 
( 

1 

N1 − µ1 

) 

n⊤t (I − P )Xv ≤ (1 + ε) 
( 

1 

N2 − µ2 

) 

n⊤t (I − P )Xv 

(B’) 

(1− ε) 
( 

1 

µ2 

) 

n⊤t PXv ≤ 
( 

1 

µ1 

) 

n⊤t PXv ≤ (1 + ε) 
( 

1 

µ2 

) 

n⊤t PXv (C’) 

For (B’) and (C’), we also require that these hold when µ1 and µ2 be interchanged. 

We also specify the approximate version of perfect prediction and equal base rate in term of f(ε), which 
be a function that go to 0 a ε go to 0. 

• Approximate perfect prediction. γ1 ≥ 1− f(ε) and γ2 ≥ 1− f(ε) 
• Approximately equal base rates. |µ1/N1 − µ2/N2| ≤ f(ε) 

A brief overview of the proof of Theorem 1.2 be a follows. It proceeds by first establish an approximate 
form of Equation (1) above, which implies that the total expect score assign in each group be approxi- 
mately equal to the total size of the positive class. This in turn make it possible to formulate approximate 
form of Equations (3) and (4). When the base rate be close together, the approximation be too loose to 
derive bound on the predictive power; but this be okay since in this case we have approximately equal base 
rates. Otherwise, when the base rate differ significantly, we show that most of the expect score must be 
assign to the positive class, give u approximately perfect prediction. 

The remainder of this section provide the full detail of the proof. 

Total score and the number of people in the positive class. First, we will show that the total score for 
each group be approximately µt, the number of people in the positive class. Define µ̂t = n⊤t Xv. Using (A’), 

12 



we have 

µ̂t = n 
⊤ 
t Xv 

= n⊤t XV e 

= 

B 
∑ 

b=1 

[n⊤t PX]b 

≤ (1 + ε) 
B 
∑ 

b=1 

[n⊤t PX]b 

= (1 + ε)n⊤t PXe 

= (1 + ε)µt 

Similarly, we can low bound µ̂t a 

µ̂t = 

B 
∑ 

b=1 

[n⊤t PX]b 

≥ (1− ε) 
B 
∑ 

b=1 

[n⊤t PX]b 

= (1− ε)µt 

Combining these, we have 
(1− ε)µt ≤ µ̂t ≤ (1 + ε)µt. (7) 

The portion of the score receive by the positive class. We can use (C’) to show that γ1 ≈ γ2. Recall 
that γt, the average of the expect score assign to member of the positive class in group t, be define a 
γt = 

1 
µt 
ntPXv. Then, it follow trivially from (C’) that 

(1− ε)γ2 ≤ γ1 ≤ (1 + ε)γ2. (8) 

The relationship between the base rates. We can apply this to (B’) to relate µ1 and µ2, use the obser- 
vation that the score not receive by people of the positive class must fall instead to people of the negative 
class. Examining the left inequality of (B’), we have 

(1− ε) 
( 

1 

N2 − µ2 

) 

n⊤t (I − P )Xv = (1− ε) 
( 

1 

N2 − µ2 

) 

(n⊤t Xv − n⊤t PXv) 

= (1− ε) 
( 

1 

N2 − µ2 

) 

(µ̂2 − γ2µ2) 

≥ (1− ε) 
( 

1 

N2 − µ2 

) 

((1− ε)µ2 − γ2µt) 

= (1− ε) 
( 

µ2 
N2 − µ2 

) 

(1− ε− γ2) 

≥ (1− ε) 
( 

µ2 
N2 − µ2 

)( 

1− ε− γ1 
1− ε 

) 

= (1− 2ε+ ε2 − γ1) 
( 

µ2 
N2 − µ2 

) 

13 



Thus, the left inequality of (B’) becomes 

(1− 2ε+ ε2 − γ1) 
( 

µ2 
N2 − µ2 

) 

≤ 
( 

1 

N1 − µ1 

) 

n⊤t (I − P )Xv (9) 

By definition, µ̂1 = n⊤t Xv and γtµt = n 
⊤ 
t PXv, so this becomes 

(1− 2ε+ ε2 − γ1) 
( 

µ2 
N2 − µ2 

) 

≤ 
( 

1 

N1 − µ1 

) 

(µ̂1 − γ1µ1) (10) 

If the base rate differ. Let ρ1 and ρ2 be the respective base rates, i.e. ρ1 = µ1/N1 and ρ2 = µ2/N2. 
Assume that ρ1 ≤ ρ2 (otherwise we can switch µ1 and µ2 in the above analysis), and assume towards 
contradiction that the base rate differ by at least 

√ 
ε, meaning ρ1 + 

√ 
ε < ρ2. Using (10), 

ρ1 + 
√ 
ε 

1− ρ1 = 
√ 
ε 
≤ ρ2 

1− ρ2 

≤ 
( 

1 + ε− γ1 
1− 2ε+ ε2 − γ1 

)( 

ρ1 
1− ρ1 

) 

(ρ1 + 
√ 
ε)(1− ρ1)(1− 2ε+ ε2 − γ1) ≤ ρ1(1− ρ1 − 

√ 
ε)(1 + ε− γ1) 

(ρ1 + 
√ 
ε)(1 − ρ1)(1− 2ε) − ρ1(1− ρ1 − 

√ 
ε)(1 + ε) ≤ γ1 

[ 

(ρ1 + 
√ 
ε)(1 − ρ1)− ρ1(1− ρ1 − 

√ 
ε) 
] 

ρ1[(1 − ρ1)(1− 2ε) − (1− ρ1 − 
√ 
ε)(1 + ε)] + 

√ 
ε(1− ρ1)(1− 2ε) ≤ γ1[ 

√ 
ε(1− ρ1) + 

√ 
ερ1] 

ρ1(−2ε+ 2ερ1 − ε+ ερ1 + 
√ 
ε+ ε 

√ 
ε) + 

√ 
ε(1− 2ε − ρ1 + 2ερ1) ≤ γ1 

√ 
ε 

ρ1(−3ε+ 3ερ1 + 
√ 
ε+ ε 

√ 
ε− 

√ 
ε+ 2ε 

√ 
ε) + 

√ 
ε(1− 2ε) ≤ γ1 

√ 
ε 

ερ1(−3 + 3ρ1 + 3 
√ 
ε) + 

√ 
ε(1− 2ε) ≤ γ1 

√ 
ε 

3ερ1(−1 + ρ1) + 
√ 
ε(1− 2ε) ≤ γ1 

√ 
ε 

1− 2ε− 3 
√ 
ερ1(1− ρ1) ≤ γ1 

1− 
√ 
ε 

( 

2 
√ 
ε+ 

3 

4 

) 

≤ γ1 

Recall that γ2 ≥ γ1(1− ε), so 

γ2 ≥ (1− ε)γ1 

≥ (1− ε) 
( 

1− 
√ 
ε 

( 

2 
√ 
ε+ 

3 

4 

)) 

≥ 1− ε− 
√ 
ε 

( 

2 
√ 
ε+ 

3 

4 

) 

= 1− 
√ 
ε 

( 

3 
√ 
ε+ 

3 

4 

) 

Let f(ε) = 
√ 
εmax(1, 3 

√ 
ε+ 3/4). Note that we assume that ρ1 and ρ2 differ by an additive 

√ 
ε ≤ f(ε). 

Therefore if the ε-fairness condition be met and the base rate be not within an additive f(ε), then γ1 ≥ 
1− f(ε) and γ2 ≥ 1− f(ε). This completes the proof of Theorem 1.2. 

4 Reducing Loss with Equal Base Rates 

In a risk assignment, we would like a much of the score a possible to be assign to member of the 
positive class. With this in mind, if an individual receives a score of v, we define their individual loss to 

14 



be v if they belong to the negative class, and 1 − v if they belong to the positive class. The loss of the risk 
assignment in group t be then the sum of the expect individual loss to each member of group t. In term 
of the matrix-vector product use in the proof of Theorem 1.1, one can show that the loss for group t may 
be write a 

ℓt(X) = n 
⊤ 
t (I − P )Xv + (µt − n⊤t PXv) 

= 2(µt − n⊤t PXv), 

and the total loss be just the weight sum of the loss for each group. 

Now, let u say that a fair assignment be one that satisfies our three condition (A), (B), and (C). As note 
above, when the base rate in the two group be equal, the set of fair assignment be non-empty, since the 
calibrate risk assignment that place everyone in a single bin be fair. We can therefore ask, in the case of 
equal base rates, whether there exists a fair assignment whose loss be strictly less than that of the trivial 
one-bin assignment. It be not hard to show that this be possible if and only if there be any assignment use 
more than one bin; we will call such an assignment a non-trivial assignment. 

Note that the assignment that minimizes loss be simply the one that assigns each σ to a separate bin with 
a score of pσ, meaning X be the identity matrix. While this assignment, which we refer to a the identity 
assignment I , be well-calibrated, it may violate fairness condition (B) and (C). It be not hard to show that 
the loss for any other assignment be strictly great than the loss for I . As a result, unless the identity 
assignment happens to be fair, every fair assignment must have large loss than that of I , force a tradeoff 
between performance and fairness. 

4.1 Characterization of Well-Calibrated Solutions 

To good understand the space of feasible solutions, suppose we drop the fairness condition (B) and (C) for 
now and study risk assignment that be simply well-calibrated, satisfy (A). As in the proof of Theorem 
1.1, we write γt for the average of the expect score assign to member of the positive class in group t, 
and we define the fairness difference to be γ1−γ2. If this be nonnegative, we say the risk assignment weakly 
favor group 1; if it be nonpositive, it weakly favor group 2. Since a risk assignment be fair if and only if 
γ1 = γ2, it be fair if and only if the fairness difference be 0. 

We wish to characterize when non-trivial fair risk assignment be possible. First, we observe that without 
the fairness requirements, the set of possible fairness difference under well-calibrated assignment be an 
interval. 

Lemma 4.1 If group 1 and group 2 have equal base rates, then for any two non-trivial well-calibrated 
risk assignment with fairness difference d1 and d2 and for any d3 ∈ [d1, d2], there exists a non-trivial 
well-calibrated risk assignment with fairness difference d3. 

Proof: The basic idea be that we can effectively take convex combination of well-calibrated assignment 
to produce any well-calibrated assignment “in between” them. We carry this out a follows. 

Let X(1) and X(2) be the allocation matrix for assignment with fairness difference d1 and d2 respectively, 
where d1 < d2. Choose λ such that λd1 + (1 − λ)d2 = d3, meaning λ = (d2 − d3)/(d2 − d1). Then, 
X(3) = [λX(1) (1− λ)X(2)] be a nontrivial well-calibrated assignment with fairness difference d3. 
First, we observe that X(3) be a valid assignment because each row sum to 1 (meaning everyone from every 
σ get assign to a bin), since each row of λX(1) sum to λ and each row of (1− λ)X(2) sum to (1− λ). 

15 



Moreover, it be nontrivial because every nonempty bin create by X(1) and X(2) be a nonempty bin under 
X(3). 

Let v(1) and v(2) be the respective bin label for assignment X(1) and X(2). Define v(3) = 

[ 

v(1) 

v(2) 

] 

. 

Finally, let V (3) = diag(v(3)). Define V (1) and V (2) analogously. Note that V (3) = 

[ 

V (1) 0 

0 V (2) 

] 

. 

We observe that X(3) be calibrate because 

n⊤t PX 
(3) = n⊤t P [λX 

(1) (1− λ)X(2)] 
= [λn⊤t PX 

(1) (1− λ)n⊤t PX(2)] 
= [λn⊤t X 

(1)V (1) (1− λ)n⊤t X(2)V (2)] 
= n⊤t [λX 

(1) (1− λ)X(2)]V (3) 

= n⊤t X 
(3)V (3) 

Finally, we show that the fairness difference be d3. Let γ 
(1) 
1 and γ 

(1) 
2 be the portion of the total expect 

score receive by the positive class from each group respectively. Define γ(2)1 , γ 
(2) 
2 , γ 

(3) 
1 , γ 

(3) 
2 similarly. 

γ 
(3) 
1 − γ 

(3) 
2 = 

1 

µ 
n⊤1 PX 

(3)v(3) − 1 
µ 
n⊤2 PX 

(3)v(3) 

= 
1 

µ 
(n⊤1 − n⊤2 )PX(3)v(3) 

= 
1 

µ 
(n⊤1 − n⊤2 )P [λX(1)v(1) (1− λ)X(2)v(2)] 

= 
1 

µ 
(λ(n⊤1 − n⊤2 )PX(1)v(1) + (1− λ)(n⊤1 − n⊤2 )X(2)v(2)]) 

= λ(γ 
(1) 
1 − γ 

(1) 
2 ) + (1− λ)(γ 

(2) 
1 − γ 

(2) 
2 ) 

= λd1 + (1− λ)d2 
= d3 

Corollary 4.2 There exists a non-trivial fair assignment if and only if there exist non-trivial well-calibrated 
assignment X(1) and X(2) such that X(1) weakly favor group 1 and X(2) weakly favor group 2. 

Proof: If there be a non-trivial fair assignment, then it weakly favor both group 1 and group 2, prove 
one direction. 

To prove the other direction, observe that the fairness difference d1 and d2 of X(1) and X(2) be nonnegative 
and nonpositive respectively. Since the set of fairness difference achievable by non-trivial well-calibrated 
assignment be an interval by Lemma 4.1, there exists a non-trivial well-calibrated assignment with fairness 
difference 0, meaning there exists a non-trivial fair assignment. 

It be an open question whether there be a polynomial-time algorithm to find a fair assignment of minimum 
loss, or even to determine whether a non-trivial fair solution exists. 

16 



4.2 NP-Completeness of Non-Trivial Integral Fair Risk Assignments 

As discuss in the introduction, risk assignment in our model be allow to split people with a give 
feature vector σ over several bins; however, it be also of interest to consider the special case of integral risk 
assignments, in which all people with a give feature σ must go to the same bin. For the case of equal base 
rates, we can show that determine whether there be a non-trivial integral fair assignment be NP-complete. 
The proof us a reduction from the Subset Sum problem and be give in the Appendix. 

The basic idea of the reduction be a follows. We have an instance of Subset Sum with number w1, . . . , wm 
and a target number T ; the question be whether there be a subset of the wi’s that sum to T . As before, γt 
denotes the average of the expect score receive by member of the positive class in group t. We first 
ensure that there be exactly one non-trivial way to allocate the people of group 1, allow u to control 
γ1. The fairness condition then require that γ2 = γ1, which we can use to encode the target value in the 
instance of Subset Sum. For every input number wi in the Subset Sum instance, we create pσ2i−1 and pσ2i , 
close to each other in value and far from all other pσ values, such that group σ2i−1 and σ2i together into 
a bin corresponds to choose wi for the subset, while not group them corresponds to not take wi. This 
ensures that group 2 can be assign with the correct value of γ2 if and only if there be a solution to the 
Subset Sum instance. 

5 Conclusion 

In this work we have formalize three fundamental condition for risk assignment to individuals, each of 
which have be propose a a basic measure of what it mean for the risk assignment to be fair. Our main 
result show that except in highly constrain special cases, it be not possible to satisfy these three constraint 
simultaneously; and moreover, a version of this fact hold in an approximate sense a well. 

Since these result hold regardless of the method use to compute the risk assignment, it can be phrase 
in fairly clean term in a number of domain where the trade-off among these condition do not appear to 
be well-understood. To take one simple example, suppose we want to determine the risk that a person be a 
carrier for a disease X, and suppose that a high fraction of woman than men be carriers. Then our result 
imply that in any test design to estimate the probability that someone be a carrier of X, at least one of the 
follow undesirable property must hold: (a) the test’s probability estimate be systematically skewed 
upward or downward for at least one gender; or (b) the test assigns a high average risk estimate to healthy 
people (non-carriers) in one gender than the other; or (c) the test assigns a high average risk estimate to 
carrier of the disease in one gender than the other. The point be that this trade-off among (a), (b), and (c) 
be not a fact about medicine; it be simply a fact about risk estimate when the base rate differ between two 
groups. 

Finally, we note that our result suggest a number of interest direction for further work. First, when 
the base rate between the two underlie group be equal, our result do not resolve the computational 
tractability of find the most accurate risk assignment, subject to our three fairness conditions, when the 
people with a give feature vector can be split across multiple bins. (Our NP-completeness result applies 
only to the case in which everyone with a give feature vector must be assign to the same bin.) Second, 
there may be a number of setting in which the cost (social or otherwise) of false positive may differ greatly 
from the cost of false negatives. In such cases, we could imagine search for risk assignment that satisfy 
the calibration condition together with only one of the two balance conditions, correspond to the class for 
whom error be more costly. Determining when two of our three condition can be simultaneously satisfied 
in this way be an interest open question. More broadly, determine how the trade-off discuss here can 

17 

lphilippe 
Texte surligné 



be incorporate into broader family of propose fairness condition suggests interest avenue for future 
research. 

References 

[1] Propublica analysis. https://docs.google.com/document/d/1pKtyl8XmJH7Z09lxkb70n6fa2Fiitd7ydbxgCT wCXs/edit?pref=2&pli=1. 

[2] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias: There’s software use 
across the country to predict future criminals. And it’s bias against blacks. ProPublica, May 23, 
2016. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing. 

[3] Solon Barocas and Andrew D Selbst. Big data’s disparate impact. California Law Review, 104, 2016. 

[4] Toon Calders and Sicco Verwer. Three naive bayes approach for discrimination-free classification. 
Data Mining and Knowledge Discovery, 21(2):277–292, 2010. 

[5] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism predic- 
tion instruments. arXiv preprint arXiv:1610.07524, October 2016. 

[6] Sam Corbett-Davies, Emma Pierson, Avi Feller, and Sharad Goel. A computer program use for bail 
and sentence decision be label bias against blacks. it actually not that clear. Washington Post, 
October 7, 2016. 

[7] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Fair algorithm and the 
equal treatment principle. Working paper, to appear. 

[8] Cynthia S. Crowson, Elizabeth J. Atkinson, and Terry M. Therneau. Assessing calibration of prognos- 
tic risk scores. Statistical Methods in Medical Research, 25(4):1692–1706, 2016. 

[9] Amit Datta, Michael Carl Tschantz, and Anupam Datta. Automated experiment on ad privacy settings. 
Proceedings on Privacy Enhancing Technologies, 2015(1):92–112, 2015. 

[10] William Dieterich, Christina Mendoza, and Tim Brennan. COMPAS risk scales: Demon- 
strating accuracy equity and predictive parity. Technical report, Northpointe, July 2016. 
http://www.northpointeinc.com/northpointe-analysis. 

[11] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard S. Zemel. Fairness 
through awareness. In Innovations in Theoretical Computer Science 2012, Cambridge, MA, USA, 
January 8-10, 2012, page 214–226, 2012. 

[12] Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubrama- 
nian. Certifying and remove disparate impact. In Proceedings of the 21th ACM SIGKDD Interna- 
tional Conference on Knowledge Discovery and Data Mining, KDD ’15, page 259–268, New York, 
NY, USA, 2015. ACM. 

[13] Anthony Flores, Christopher Lowenkamp, and Kristin Bechtel. False positives, false negatives, and 
false analyses: A rejoinder to “machine bias: Theres software use across the country to predict future 
criminals. and it bias against blacks.”. Technical report, Crime & Justice Institute, September 2016. 
http://www.crj.org/cji/entry/false-positives-false-negatives-and-false-analyses-a-rejoinder. 

[14] Dean P. Foster and Rakesh V. Vohra. Asymptotic calibration. Biometrika, 85(2):379–390, 1998. 

18 

https://docs.google.com/document/d/1pKtyl8XmJH7Z09lxkb70n6fa2Fiitd7ydbxgCT_wCXs/edit?pref=2&pli=1 


[15] Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im) possibility of 
fairness. arXiv preprint arXiv:1609.07236, September 2016. 

[16] Howard N. Garb. Race bias, social class bias, and gender bias in clinical judgment. Clinical Psychol- 
ogy: Science and Practice, 4(2):99–120, 1997. 

[17] Abe Gong. Ethics for powerful algorithm (1 of 4). Medium, July 12, 2016. 
https://medium.com/@AbeGong/ethics-for-powerful-algorithms-1-of-3-a060054efd84#.dhsd2ut3i. 

[18] Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination pre- 
vention in data mining. IEEE transaction on knowledge and data engineering, 25(7):1445–1459, 
2013. 

[19] Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervise learning. In 
Advances in Neural Information Processing Systems, 2016. 

[20] Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth. Fairness in learning: Classic 
and contextual bandits. In Advances in Neural Information Processing Systems, 2016. 

[21] Faisal Kamiran and Toon Calders. Classifying without discriminating. 2009 2nd International Con- 
ference on Computer, Control and Communication, IC4 2009, 2009. 

[22] Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learn through regulariza- 
tion approach. Proceedings - IEEE International Conference on Data Mining, ICDM, page 643–650, 
2011. 

[23] Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. How we analyze the COMPAS recidi- 
vism algorithm. ProPublica, May 23, 2016. https://www.propublica.org/article/how-we-analyzed-the- 
compas-recidivism-algorithm. 

[24] Executive Office of the President. Big data: A report on algorithmic systems, opportunity, and civil 
rights. Technical report, May 2016. 

[25] Andrea Romei and Salvatore Ruggieri. A multidisciplinary survey on discrimination analysis. The 
Knowledge Engineering Review, 29(05):582–638, 2014. 

[26] Latanya Sweeney. Discrimination in online ad delivery. Communications of the ACM, 56(5):44–54, 
2013. 

[27] David R. Williams and Selina A. Mohammed. Discrimination and racial disparity in health: Evidence 
and need research. J. Med. Behav., 32(1), 2009. 

[28] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna Gummadi. Learning 
fair classifiers. arXiv preprint arXiv:1507.05259, 2015. 

[29] Richard S Zemel, Yu Wu, Kevin Swersky, Toniann Pitassi, and Cynthia Dwork. Learning Fair Rep- 
resentations. Proceedings of the 30th International Conference on Machine Learning, 28:325–333, 
2013. 

[30] Indre Zliobaite. A survey on measure indirect discrimination in machine learning. arXiv preprint 
arXiv:1511.00148, 2015. 

19 



Appendix: NP-Completeness of Non-Trivial Integral Fair Risk Assignments 

We can reduce to the integral assignment problem, parameterized by a1σ, a2σ , and pσ, from subset sum a 
follows. 

Suppose we have an instance of the subset sum problem specify by m number w1, . . . , wm and a target 
T ; the goal be to determine whether a subset of the wi add up to T . We create an instance of the integral 
assignment problem with σ1, . . . , σ2m+2. a1,σi = 1/2 if i ∈ {2m + 1, 2m + 2} and 0 otherwise. a2,σi = 
1/(2m) if i ≤ 2m and 0 otherwise. We make the follow definitions: 

ŵi = wi/(Tm 
4) 

εi = 
√ 

ŵi/2 

pσ2i−1 = i/(m+ 1)− εi (1 ≤ i ≤ m) 
pσ2i = i/(m+ 1) + εi (1 ≤ i ≤ m) 

γ = 1/m 

2m 
∑ 

i=1 

p2σi − 1/m 
5 

pσ2m+1 = (1− 
√ 

2γ − 1)/2 
pσ2m+2 = (1 + 

√ 

2γ − 1)/2 

With this definition, the subset sum instance have a solution if and only if the integral assignment instance 
give by a1,σ, a2,σ, pσ1 , . . . , pσ2m+2 have a solution. 

Before we prove this, we need the follow lemma. 

Lemma 5.1 For any z1, . . . , zk ∈ R, 

k 
∑ 

i=1 

z2i − 
1 

k 

( 

m 
∑ 

i=1 

zi 

)2 

= 
1 

k 

k 
∑ 

i<j 

(zi − zj)2 

Proof: 

k 
∑ 

i=1 

z2i − 
1 

k 

( 

m 
∑ 

i=1 

zi 

)2 

= 

k 
∑ 

i=1 

z2i − 
1 

k 

 

 

k 
∑ 

i=1 

z2i + 2 

k 
∑ 

i<j 

zizj 

 

 

= 
k − 1 
k 

k 
∑ 

i=1 

z2i − 
2 

k 

k 
∑ 

i<j 

zizj 

= 
1 

k 

k 
∑ 

i<j 

(z2i + z 
2 
j )− 

2 

k 

k 
∑ 

i<j 

zizj 

= 
1 

k 

k 
∑ 

i<j 

z2i − 2zizj + z2j 

= 
1 

k 

k 
∑ 

i<j 

(zi − zj)2 

20 



Now, we can prove that the integral assignment problem be NP-hard. 

Proof: First, we observe that for any nontrivial solution to the integral assignment instance, there must 
be two bin b 6= b′ such that Xσ2m+1,b = 1 and Xσ2m+2,b′ = 1. In other words, the people with σ2m+1 
and σ2m+2 must be split up. If not, then all the people of group 1 would be in the same bin, meaning that 
bin must be label with the base rate ρ1 = 1/2. In order to maintain fairness, the same would have to 
be do for all the people of group 2, result in the trivial solution. Moreover, b and b′ must be label 
(1±√2γ − 1)/2 respectively because those be the fraction of people of group 1 in those bin who belong 
to the positive class. 

This mean that γ1 = 1/ρ · (a1,σ2m+1p2σ2m+1 + a1,σ2m+2p2σ2m+2) = p2σ2m+1 + p2σ2m+2 = γ a define above. 
We know that a well-calibrated assignment be fair only if γ1 = γ2, so we know γ2 = γ. 

Next, we observe that ρ2 = ρ1 = 1/2 because all of the positive a2,σ’s be 1/(2m), so ρ2 be just the average 
of {pσ1 , . . . , pσ2m}, which be 1/2 by symmetry. 
Let Q be the partition of [2m] correspond to the assignment, meaning that for a give q ∈ Q, there be a 
bin bq contain all people with σi such that i ∈ q. The label on that bin be 

vq = 

∑ 

i∈q a2,σipσi 
∑ 

i∈q a2,σi 

= 
1/(2m) 

∑ 

i∈q pσi 
|q|/(2m) 

= 
1 

|q| 
∑ 

i∈q 

pσi 

Furthermore, bin bq contains 
∑ 

i∈q a2,σipσi = 1/(2m) 
∑ 

i∈q pσi positive fraction. Using this, we can come 
up with an expression for γ2. 

γ2 = 
1 

ρ 

∑ 

q∈Q 

 

vb · 
1 

2m 

∑ 

i∈q 

pσi 

 

 

= 
1 

m 

∑ 

q∈Q 

1 

|q| 

 

 

∑ 

i∈q 

pσi 

 

 

2 

Setting this equal to γ, we have 

1 

m 

∑ 

q∈Q 

1 

|q| 

 

 

∑ 

i∈q 

pσi 

 

 

2 

= 
1 

m 

2m 
∑ 

i=1 

p2σi − 
1 

m5 

∑ 

q∈Q 

1 

|q| 

 

 

∑ 

i∈q 

pσi 

 

 

2 

= 

2m 
∑ 

i=1 

p2σi − 
1 

m4 

Subtracting both side from 
∑2m 

i=1 p 
2 
σi 

and use Lemma 5.1, we have 

∑ 

q∈Q 

1 

|q| 
∑ 

i<j∈q 

(pσi − pσj)2 = 
1 

m4 
(11) 

21 



Thus, Q be a fair nontrivial assignment if and only if (11) holds. 

Next, we show that there exists Q that satisfies (11) if and only if there there exists some S ⊆ [m] such that 
∑ 

i∈S ŵi = 1/m 
4. 

Assume Q satisfies (11). Then, we first observe that any q ∈ Q must either contain a single i, meaning 
it do not contribute to the left hand side of (11), or q = {2i − 1, 2i} for some i. To show this, observe 
that the closest two element of {pσ1 , . . . , pσ2m} not of the form {pσ2i−1 , pσ2i} must be some {pσ2i , pσ2i+1}. 
However, we find that 

(pσ2i+1 − pσ2i)2 = 
( 

i+ 1 

m+ 1 
− εi+1 − 

( 

i 

m+ 1 
+ εi 

))2 

= 

( 

1 

m+ 1 
− εi+1 − εi 

)2 

= 

( 

1 

m+ 1 
− 
√ 

ŵi+1 
2 

− 
√ 

ŵi 
2 

)2 

≥ 
( 

1 

m+ 1 
− 
√ 

2 

m4 

)2 

(ŵi ≤ 1/m4) 

= 

( 

1 

m+ 1 
− 

√ 
2 

m2 

)2 

≥ 
( 

1 

2m 
− 

√ 
2 

m2 

)2 

= 

( 

m− 2 
√ 
2 

2m2 

)2 

≥ 
( m 

4m2 

)2 

= 

( 

1 

4m 

)2 

= 
1 

16m2 

If any q contains any j, k not of the form 2i− 1, 2i, then (11) will have a term on the left hand side at least 
1/m · 1/(16m2) = 1/(16m3) > 1/m4 for large enough m, and since there can be no negative term on the 
left hand side, this immediately make it impossible for Q to satisfy (11). 

Consider every 2i − 1, 2i ∈ [2m]. Let qi = {2i − 1, 2i}. As show above, either qi ∈ Q or {2i − 1} ∈ Q 
and {2i} ∈ Q. In the latter case, neither pσ2i−1 nor pσ2i contributes to (11). If qi ∈ Q, then qi contributes 
1/2(pσ2i−pσ2i−1 ) 

2 = 1/2(2εi) 
2 = ŵi to the overall sum on the left hand side. Therefore, we can write the 

left hand side of (11) a 

∑ 

q∈Q 

1 

|q| 
∑ 

i<j∈q 

(pσi − pσj )2 = 
∑ 

qi∈Q 

1 

2 
(pσ2i−pσ2i−1 ) 

2 = 
∑ 

qi∈Q 

ŵi = 
1 

m4 

Then, we can build a solution to the original subset sum instance a S = {i : qi ∈ Q}, give u 
∑ 

i∈S ŵi = 
1 
m4 

. Multiplying both side by Tm4, we get 
∑ 

i∈S wi = T , meaning S be a solution for the subset sum 
instance. 

22 



To prove the other direction, assume we have a solution S ⊆ [m] such that ∑i∈S wi = T . Dividing both 
side by Tm4, we get 

∑ 

i∈S ŵi = 1/m 
4. We build a partition Q of 2m by start with the empty set and 

add qi = {2i− 1, 2i} to Q if i ∈ S and {2i− 1} and {2i} to Q otherwise. Clearly, each element of [2m] 
appear in Q at most once, make this a valid partition. Moreover, when check to see if (11) be satisfied 
(which be true if and only if Q be a fair assignment), we can ignore all q ∈ Q such that |q| = 1 because they 
don’t contribute to the left hand side. Since, we again have 

∑ 

q∈Q 

1 

|q| 
∑ 

i<j∈q 

(pσi − pσj )2 = 
∑ 

qi∈Q 

1 

2 
(pσ2i−pσ2i−1 ) 

2 = 
∑ 

qi∈Q 

ŵi = 
1 

m4 

meaning Q be a fair assignment. This completes the reduction. 

We have show that the integral assignment problem be NP-hard, and it be clearly in NP because give an 
integral assignment, we can verify in polynomial time whether such an assignment satisfies the condition 
(A), (B), and (C). Thus, the integral assignment problem be NP-complete. 

23 


1 Introduction 
1.1 Formulating the Goal 
1.2 Determining What be Achievable: A Characterization Theorem 
1.3 Further Related Work 

2 The Characterization Theorems 
3 The Approximate Theorem 
4 Reducing Loss with Equal Base Rates 
4.1 Characterization of Well-Calibrated Solutions 
4.2 NP-Completeness of Non-Trivial Integral Fair Risk Assignments 

5 Conclusion 

