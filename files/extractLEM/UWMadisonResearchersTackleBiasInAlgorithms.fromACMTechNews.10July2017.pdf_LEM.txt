






































UW-Madison researcher tackle bias in algorithm 


UW-Madison researcher tackle bias 
in algorithm 

If you’ve ever apply for a loan or checked your credit score, algorithm have 

played a role in your life. These mathematical model allow computer to use 

data to predict many thing — who be likely to pay back a loan, who may be a 

suitable employee, or whether a person who have broken the law be likely to 

reoffend, to name just a few examples. 

Yet while some may assume that computer remove human bias from decision- 

making, research have show that be not true. Biases on the part of those 

design algorithms, a well a bias in the data use by an algorithm, can 

introduce human prejudice into a situation. A seemingly neutral process 

becomes fraught with complications. 

For the past year, University of 

Wisconsin–Madison faculty in the 

Department of Computer Sciences 

have be work on tool to 

address unfairness in algorithms. 

Now, a $1 million grant from the 

National Science Foundation will 

accelerate their efforts. Their 

project, “Formal Methods for 

Program Fairness,” be fund 

through NSF’s Software and 

Hardware Foundations program. 

UW-Madison computer science 

professor Aws Albarghouthi 

(https://www.cs.wisc.edu/people/aws), Shuchi Chawla 

(http://www.cs.wisc.edu/people/shuchi), Loris D’Antoni 

(https://www.cs.wisc.edu/people/loris) and Jerry Zhu 

(http://www.cs.wisc.edu/people/jerryzhu) be lead the development of a 

tool call FairSquare. Computer science graduate student Samuel Drews 

(http://pages.cs.wisc.edu/~sdrews/) and David Merrell 

(https://dpmerrell.github.io/) be also involved. 

What set FairSquare apart be that it will not only detect bias, but also employ 

automate solutions. “Ultimately, we’d like this to be a regulatory tool when 

you’re deploy an algorithm make sensitive decisions. You can verify it’s 

indeed fair, and then fix it if it’s not,” say Albarghouthi. 

Decision-making algorithm can be mysterious even to those who use them, 

say the researchers, make a tool like FairSquare necessary. 

UW-Madison researcher tackle bias in algorithm http://news.wisc.edu/uw-madison-researchers-tackle-bias-in-algorithms/ 

1 sur 2 10/07/2017 20:17 



For example, consider a bank that us a third-party tool to evaluate who 

qualifies for a mortgage or small business loan, and at what interest rate. The 

bank may not know how the software be classify potential customers, how 

accurate it prediction truly are, or whether result reflect racial or other type 

of bias. 

“Many company use these algorithm don’t understand what (the 

algorithms) be doing,” say Albarghouthi. “An algorithm seem to work for 

them, so they use it, but usually there be no feedback or explainability” on how 

exactly it be working. That make these algorithm difficult to regulate in term 

of avoid illegal bias, he says. 

Companies design and sell these product be typically not eager to share 

their proprietary knowledge, make their algorithm what be know a 

“black box.” 

Says D’Antoni, “We’re try to give 

people the ability to ask about 

behavior of an algorithm. Does it 

prefer a certain gender, or certain 

behaviors, for example?” 

The stake behind these algorithm 

can be high, a journalist have 

noted. 

In a 2016 story by the investigative 

journalism organization ProPublica, 

a team of reporter examine a 

product use in law enforcement to 

predict offenders’ likelihood of reoffending. The reporter uncovered trouble 

racial bias, though the software company in question dispute their 

conclusions. 

According to ProPublica, “(B)lacks be almost twice a likely a white to be 

label a high risk but not actually reoffend.” With white offenders, the 

opposite mistake occurred. Whites be much more likely than black to be 

pegged a low-risk yet go on to commit additional crimes. 

The UW researcher be attack the problem by isolate fairness a a 

property of a software program that must be formally define and proven. 

This point to additional questions, say Drews. “Who decides what’s fair? How 

can you be certain you’re come up with a mathematical formula that mean 

the thing you want it to prove?” 

The FairSquare team be make connection with UW–Madison scholar in 

other field who can help illuminate certain aspect of this research, such a 

legal and ethical ramifications. 

“Computing be so much more involve in people’s life these days,” say Drews, 

make the development of FairSquare not only a significant compute 

challenge but also one with far-reaching social impact. 

Adds Merrell, “Machine learn algorithm have become very commonplace, 

but they aren’t always use in responsible ways. I hope our research will help 

engineer build safe, reliable and ethical systems.” 

UW-Madison researcher tackle bias in algorithm http://news.wisc.edu/uw-madison-researchers-tackle-bias-in-algorithms/ 

2 sur 2 10/07/2017 20:17 


