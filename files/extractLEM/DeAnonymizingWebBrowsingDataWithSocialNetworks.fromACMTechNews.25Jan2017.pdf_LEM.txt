









































de-anonymizing-web (70).pdf 


De-anonymizing Web Browsing Data with Social Networks 

Jessica Su 

Stanford University 

jtysu@stanford.edu 

Ansh Shukla 

Stanford University 

anshukla@stanford.edu 

Sharad Goel 

Stanford University 

scgoel@stanford.edu 

Arvind Narayanan 

Princeton University 

arvindn@cs.princeton.edu 

ABSTRACT 
Can online tracker and network adversary de-anonymize 
web browsing data readily available to them? We show— 
theoretically, via simulation, and through experiment on 
real user data—that de-identified web browsing history can 
be link to social medium profile use only publicly avail- 
able data. Our approach be base on a simple observation: 
each person have a distinctive social network, and thus the 
set of link appear in one’s feed be unique. Assuming 
user visit link in their feed with high probability than 
a random user, browsing history contain tell-tale mark of 
identity. We formalize this intuition by specify a model 
of web browsing behavior and then derive the maximum 
likelihood estimate of a user’s social profile. We evaluate 
this strategy on simulated browsing histories, and show that 
give a history with 30 link originate from Twitter, we 
can deduce the correspond Twitter profile more than 50% 
of the time. To gauge the real-world e↵ectiveness of this ap- 
proach, we recruit nearly 400 people to donate their web 
browsing histories, and we be able to correctly identify 
more than 70% of them. We further show that several on- 
line tracker be embed on su�ciently many website to 
carry out this attack with high accuracy. Our theoretical 
contribution applies to any type of transactional data and 
be robust to noisy observations, generalize a wide range 
of previous de-anonymization attacks. Finally, since our at- 
tack attempt to find the correct Twitter profile out of over 
300 million candidates, it is—to our knowledge—the largest- 
scale demonstrate de-anonymization to date. 

CCS Concepts 
•Security and privacy ! Pseudonymity, anonymity 
and untraceability; •Information system ! Online 
advertising; Social networks; 

Permission to make digital or hard copy of all or part of this work for personal or 
classroom use be grant without fee provide that copy be not make or distribute 
for profit or commercial advantage and that copy bear this notice and the full cita- 
tion on the first page. Copyrights for component of this work own by others than 
ACM must be honored. Abstracting with credit be permitted. To copy otherwise, or re- 
publish, to post on server or to redistribute to lists, require prior specific permission 
and/or a fee. Request permission from permissions@acm.org. 

c� 2017 ACM. ISBN TDB. 
DOI: TBD 

1. INTRODUCTION 
Online anonymity protects civil liberties. At an abstract 

level, it enables intellectual freedom: research show that 
user change their behavior when they know they be be- 
ing surveilled online [23], result in a chill e↵ect [32]. 
Concretely, user who have their anonymity compromise 
may su↵er harm range from persecution by government 
to target fraud that threaten public exposure of online 
activity [6]. 

The online advertising industry build browsing history 
of individual via third-party tracker embed on web 
pages. While a small number of company admit to attach- 
ing user identity to these browsing-history datasets, most 
company promise user that the history be pseudony- 
mous and not link to identity. Privacy advocate have 
argue that such data can be de-anonymized, but we lack 
conclusive evidence. It have remain unclear what type 
of identify auxiliary information could be use in a de- 
anonymization attack, whether an attack could work at the 
scale of million of users, and what the success rate of such 
an attack would be. 

In this paper we show that browsing history can be link 
to social medium profile such a Twitter, Facebook, or Reddit 
accounts. We begin by observe that most user subscribe 
to a distinctive set of other user on a service. Since user 
be more likely to click on link post by account that 
they follow, these distinctive pattern persist in their brows- 
ing history. An adversary can thus de-anonymize a give 
browsing history by find the social medium profile whose 
“feed” share the history’s idiosyncratic characteristics.1 

Such an attack be feasible for any adversary with access 
to browsing histories. This include third-party tracker 
and others with access to their data (either via intrusion or 
a lawful request). Network adversaries—including govern- 
ment surveillance agencies, Internet service providers, and 
co↵ee shop eavesdroppers—also see URLs of unencrypted 
web tra�c. The adversary may also be a cross-device track- 
ing company aim to link two di↵erent browsing history 
(e.g., history generate by the same user on di↵erent de- 
vices). For such an adversary, link to social medium profile 
be a step stone. 

We make three key contributions. First, we develop a 
general theoretical framework for de-anonymization. We as- 
sume there be a background probability of click on links, 
and that a link appear in a user’s feed increase it prob- 
ability of appear in their browsing history by a user- 

1A user’s feed or timeline contains the aggregate content 
post by all account to which the user subscribes. 



specific factor. We then derive a maximum likelihood es- 
timate, which let u identify the feed in the system most 
likely to have generate the observe history. This general 
frame applies to a variety of other de-anonymization at- 
tack (c.f. Section 8). 

Our second contribution be implement and evaluate 
this technique. We chose Twitter a the source of aux- 
iliary information for several reasons: it real-time API, 
which avoids the need for large-scale web-crawling; the fact 
that most activity be public; and finally, the fact that link 
be wrap in the t.co shortener, which simplifies detail 
of our attack. We assume that either due to the referer 
header or by exploit timing information, the adversary 
know which link in the user’s history result from click 
on Twitter. By employ a variety of cache and ap- 
proximation techniques, we built a system capable of de- 
anonymizing web browsing history in real-time, typically 
in under one minute. To test the performance of this sys- 
tem, we picked 60 active Twitter user at random, obtain 
their feeds, and simulated browsing history use a sim- 
ple behavioral model. Given a synthetic history contain 
30 Twitter links, we identify the correct Twitter profile— 
out of over 300 million active Twitter users—over 50% of 
the time. We show that our maximum likelihood estimate 
achieves good accuracy than intersection size and Jaccard 
similarity, two approach that have be previously study 
in the context of similar de-anonymization task [15, 35]. 

Finally, our third contribution be create an experiment 
to test this attack on real browsing histories.2 We built an 
online tool to allow user to donate their browsing history; 
upon which we execute our attack and show the result to 
the user so they could confirm or deny. The attack work 
correctly for 72% of the 374 user who complete the experi- 
ment. We present these result a a proof of concept, note 
that our sample of user be not representative. 

There be many way in which user may be de-anonymized 
when browsing the web (see Section 2). However, our attack 
be notable for it generality and for the variety of adver- 
saries who may employ it. Any social medium site can be 
use for such an attack, provide that a list of each user’s 
subscription can be inferred, the content be public, and the 
user visit su�ciently many link from the site. For ex- 
ample, on Facebook subscription can be infer base on 
“likes,” and on Reddit base on comments, albeit incom- 
pletely and with some error. Further, it be inherent in the 
web’s design and users’ behavior, and be not due to spe- 
cific, fixable vulnerability by browser or websites, unlike 
previous de-anonymization attacks. It simultaneously con- 
firm the fingerprintability of browsing profile and the easy 
availability of auxiliary information. Application-layer de- 
anonymization have long be consider the Achilles’ heel 
of Tor and other anonymity systems, and our work provide 
another reason why that be the case. 

The increase adoption of HTTPS on the web diminishes 
the strength of an attack by network adversaries, but not by 
third-party trackers. However, network adversary still see 
the domain of encrypt requests, even if the URL be hidden. 
We hypothesize that the attack will still work in this scenario 
but will require a great number of link per user. Users 
can mitigate attack by instal tracker-blocking tool such 
a Ghostery, uBlock Origin, or Privacy Badger, a well a 

2This experiment be approve by Stanford University’s In- 
stitutional Review Board (Protocol No. 34095). 

HTTPS everywhere to increase the use of encryption. Of 
course, not reveal one’s real-world identity on social me- 
dia profile also make it harder for the adversary to identify 
the user, even if the link be successful. Nascent project 
such a Contextual Identity container for Firefox help user 
more easily manage their identity online [5]. None of these 
solution be perfect; ultimately, protect anonymity online 
require vigilance and awareness of potential attacks. 

2. RELATED WORK 
The de-anonymization literature be vast, but linkage at- 

tack (and demonstration of uniqueness) base on behavior 
be especially relevant to our work. These include transac- 
tional record of movie view [28], location trace [7, 22], 
credit-card metadata [8], and write style [27]. Attacks on 
anonymous communication system such a long-term in- 
tersection attack and statistical disclosure attack employ 
similar principle [24]. 

To our knowledge, the only previous work that study 
the uniqueness of browsing history be by Olejnik et al. [31]. 
Based on a large online experiment, they report that test 
50 link be su�cient to uniquely fingerprint 42% of user in 
a sample of about 370,000 users, and that these behavioral 
fingerprint be stable over time. Their result be not di- 
rectly comparable with ours: the browsing history in their 
experiment be obtain via “history sni�ng” (which us 
a browser bug that have long be fixed). As a result, they 
be only able to test for the presence or absence of URLs 
from a select list, rather than analyze the entire brows- 
ing history of participate users. Further, the work leaf 
open the question of whether these behavioral fingerprint 
can actually be link to auxiliary information. 

Wondracek et al. [38] present an online de-anonymization 
attack that be conceptually similar to ours, although it again 
involves history sni�ng. Further, it relies on additional pri- 
vacy vulnerability on social medium sites: specifically, tak- 
ing an action (such a commenting) result in a request to a 
URL specific to that comment or other action. From a scien- 
tific perspective, the paper show that group membership 
in social medium site tend to be unique, but do not shed 
light on the uniqueness or de-anonymizability of browsing 
history in general (in the absence of the now-fixed privacy 
vulnerabilities). 

Our work also directly relates to third-party online track- 
ing. Such track have grown tremendously in prevalence 
and complexity over the past two decade [20, 25, 34, 4]. 
Today Google can track user across nearly 80% of site 
through it various third-party domain [21]. Web track 
have expand from simple HTTP cooky to include more 
persistent track techniques, such a the use of flash cook- 
y to“respawn”or re-instantiate HTTP cooky [36], the use 
of cache E-Tags and HTML5 localStorage for the same pur- 
pose [3], and“cookie syncing”between di↵erent third party 
[11, 1]. Device fingerprint attempt to identify user by 
a combination of the device’s property [9, 19]. New finger- 
printing technique be continually discover [26, 30, 13], 
and be subsequently use for track [29, 2, 1, 10]. These 
technique allow tracker to more e↵ectively compile unique 
browsing histories, but they do not by themselves link his- 
tory to identity. 

Leaks of PII from first party to third party be rampant, 
and this be one way in which an identity may be attach to 
pseudonymous browsing history [18, 17]. Further, the NSA 



be know to piggyback on advertising cooky for surveil- 
lance; Englehardt et al. [11] show that this technique can be 
e↵ective and that such a network eavesdropper may also be 
able to learn users’ identity due to usernames and other 
PII transmit by website in the clear. Our work present 
a new way in which eavesdropper may connect web tra�c 
to identities, and it will work even if the PII leak be fixed. 

3. DE-ANONYMIZATION STRATEGY 
Our de-anonymization strategy proceeds in three steps. 

First, we posit a simple model of web browsing behavior 
in which a user’s likelihood of visit a URL be govern 
by the URL’s overall popularity and whether the URL ap- 
peared in the user’s Twitter feed. Next, for each user, we 
compute their likelihood (under the model) of generate a 
give anonymous browsing history. Finally, we identify the 
user most likely to have generate that history. A similar 
likelihood-based approach be use by Ma et al. [22] to 
de-anonymize location traces. 

We construct a stylize model of web browsing by first 
assume that a user’s web history be generate by a se- 
quence of independent, identically distribute random vari- 
ables H1, . . . , Hn, where Ht corresponds to the t-th URL 
visit by the user. We further assume that each user i have 
a personalize set of recommend link Ri. For example, 
on a social medium site like Twitter, we can take this recom- 
mendation set to be those link that appear in the user’s 
feed (i.e., link post by the user’s friend on the network). 
Finally, we assume that a user be more likely to visit a link 
if it appear in the user’s recommendation set, where there 
be a user-specific multiplicative factor ri that describes each 
user’s responsiveness to the recommendation set. A user’s 
web browsing behavior be thus control by two parameters: 
the recommendation set Ri (which be a set of links), and the 
recommendation factor ri. 

Theorem 1 below formalizes this generative model of web 
browsing behavior. Further, give a browsing history and 
a set of candidate user with recommendation set C = 
{R1, . . . , Rk}, it derives the maximum likelihood estimate 
R̂ and r̂. In particular, R̂ be the recommendation set (and 
hence user) most likely associate with a given, de-identified 
browsing history. 

Theorem 1. Let ⌦ = {!1, . . . ,!N} be a universe of items, 
and suppose {pj} give a probability distribution on ⌦ (i.e., 
pj � 0 and 

PN 
j=1 pj = 1). Let C = {R1, . . . , Rk} be a collec- 

tion of recommendation sets, where Ri ✓ ⌦. For any R 2 C 
and r > 0, define a random variable Ht(R, r) take value 
in ⌦ such that 

Pr(Ht = !j) = 

( 
rpj/z if !j 2 R 
pj/z if !j 62 R 

where z be a normalize factor: z = r 
P 

!j2R pj+ 
P 

!j 62R pj . 

Then, give i.i.d. draw H1(R, r), . . . , Hn(R, r), the maxi- 
mum likelihood estimate (R̂, r̂) of the underlie parameter 
be 

R̂ = argmax 
R2C 

 
qR log 

✓ 
qR 
pR 

◆ 
+ (1� qR) log 

✓ 
1� qR 
1� pR 

◆� 
(1) 

and 

r̂ = 

✓ 
qR̂ 

1� qR̂ 

◆,✓ 
pR̂ 

1� pR̂ 

◆ 
(2) 

where qR = |{t | Ht 2 R}|/n and pR = 
P 

!j2R pj . 

In the theorem above, qR be the fraction of link in the 
observe browsing history that be in the recommendation 
set R (e.g., the fraction of link appear in the associate 
user’s Twitter feed). Similarly, pR be the generalize size of 
the recommendation set, where it account both for the total 
number of item in the set and the popularity of those items. 
Intuitively, R̂ be a recommendation set for which qR̂ be large 
and pR̂ be small; that is, many of the link in the observe 

history appear in R̂ and R̂ be not too big. The theorem allows 
for ri < 1, in which case Ri be an “anti-recommendation” set 
(e.g., a list of malware link one should not visit). However, 
in the case we consider here, ri > 1. 

Proof. Let XR(!) = 1 if ! 2 R, and XR(!) = 0 oth- 
erwise. Furthermore, suppose Ht = !at . Then the log- 
likelihood L(R, r) of H1, . . . , Hn be 

L(R, r) = 
nX 

t=1 

h 
XR(!at) log 

⇣rpat 
z 

⌘ 
+ (1�XR(!at)) log 

⇣pat 
z 

⌘i 

= 
nX 

t=1 

[XR(!at) log r + log pat � log z] . 

Now, note that 

z = r 
X 

!j2R 

pj + 
X 

!j 62R 

pj 

= (r � 1) 
X 

!j2R 

pj + 
X 

!j2⌦ 

pj 

= (r � 1)pR + 1. 

Consequently, 

L(R, r) = 
nX 

t=1 

[XR(!at) log r + log pat � log((r � 1)pR + 1)] 

= nqR log r � n log ((r � 1)pR + 1) + 
nX 

t=1 

log pat . 

Di↵erentiating L with respect to r, we have 

@ 
@r 

L(R, r) = nqR 
r 

� npR 
(r � 1)pR + 1 

and so @@rL(R, r) = 0 when 

r = 
qR 
pR 

· 1� pR 
1� qR 

. 

At these critical points, 

z = (r � 1)pR + 1 

= 
1� pR 
1� qR 

. 

Substituting these value into the original expression, we 
find that the value of R at which L(R, r) attains it maxi- 
mum must also maximize the function 



100 

300 

500 

700 

900 

10% 30% 50% 70% 90% 
Fraction of history link in recommendation set 

Si 
ze 

o 
f r 

ec 
om 

m 
en 

da 
tio 

n 
se 

t 

MLE Intersection size Jaccard 

Figure 1: Contour plot that show how each rank- 

ing method score candidates, a a function of the 

proportion of history link that appear in the candi- 

date’s recommendation set and the size of the rec- 

ommendation set. Contours of the MLE method be 

steeper than Jaccard but not a steep a intersection 

size. 

qR log r � log z 

= qR log 

 
qR 
pR 

· 1� pR 
1� qR 

� 
� log 

 
1� pR 
1� qR 

� 

= qR log 
qR 
pR 

� (1� qR) log 
 
1� pR 
1� qR 

� 
. 

Therefore, we find that 

R̂ = argmax 
R2C 

 
qR log 

✓ 
qR 
pR 

◆ 
+ (1� qR) log 

✓ 
1� qR 
1� pR 

◆� 

and 

r̂ = 

✓ 
qR̂ 

1� qR̂ 

◆,✓ 
pR̂ 

1� pR̂ 

◆ 
. 

To help provide intuition about our MLE approach, we 
compare it to two natural alternative strategies. In the 
first—which we term the intersection size method [33, 38]— 
one associate a give history H with the recommendation 
set R that contains the most URLs from the history (i.e., R 
that maximizes |R \ H|). In contrast to the MLE, such a 
strategy do not explicitly adjust for the size of the recom- 
mendation set, and one worry be that the intersection size 
method be bias toward large recommendation sets. To 
account for the size of a recommendation set, a second al- 
ternative be to associate a history with the recommendation 
set that have the great Jaccard similarity with the history: 
|H \R|/|H [R|. In many cases, the recommendation set R 
be much large than the history H (e.g., the number of link 
appear in one’s Twitter feed be often much large than 
the number of link one visits), and so maximize Jaccard 
similarity approximately amount to find the candidate 
set R that maximizes |H \R|/|R|. 

Figure 1 show that our MLE approach penalizes the size 
of a candidate recommendation set more than the intersec- 
tion size method and less than Jaccard similarity. For each 
of the three de-anonymization methods, the line in the con- 
tour plot indicate level curves, along which candidate rec- 
ommendation set receive equal scores. In compute the 
MLE, for simplicity we assume each item in the recommen- 
dation set have constant probability pj = 1/10 

6. Because the 
intersection size method be independent of the size of the 
recommendation set, it level curve be vertical. Jaccard, 
in contrast, have linear contour (where we approximate Jac- 
card by |H \ R|/|R|, so that we do not need to consider 
dependence on history size). Finally, the MLE have contour 
that be close to those of intersection size, but not quite 
vertical. 

4. SYSTEM DESIGN 
The MLE estimate described above require calculate 

qR, the number of history link appear in a candidate’s 
recommendation set, and pR, the generalize size of the rec- 
ommendation set. Furthermore, it require maximize an 
expression of these quantity across all user in the network. 
For a typical network with hundred of million of users, ac- 
quiring and calculate these quantity be computationally 
challenging. Here, we outline a system that address these 
hurdle for the Twitter social network and enables real-time 
link of web browsing history to user profiles. 

4.1 Candidate Ranking 
We begin by express the MLE estimate concretely in 

term of the Twitter network: recommendation sets, Ri, be 
the link post by friend of a particular user i,3 and the 
anonymous history H = {H1, . . . , Hn} be a set of link some 
user have clicked on from Twitter.4 In this formulation, qRi 
be the number of link from H post by user i’s friends, and 
pRi be a measure of the user’s feed size. 

An immediate simplification be to reduce our candidate set 
C to only feed that have at least one link from the history, 
Ĉ = {Ri | Ri \ H 6= ;}, since the MLE be approximately 
zero outside of this set. Because the complete list of poster 
for a link be obtainable through a search query, we can find 
all recommendation set in Ĉ. Specifically, we first search 
Twitter to determine all the user who post a link in the 
history. Then, for every poster, we add their followers’ feed 
to our reduce set of candidate Ĉ. The search result also 
let u calculate qRi score for every candidate by counting 
the number of distinct link in H post by their friends. 

The score pRi be the total background probability of click- 
ing on link in Ri. In practice, it be impossible to deter- 
mine the exact distribution of these probabilities. We ap- 
proximate pRi by assume that each of user i’s friend 
tweet link with a fix total probability and estimate p̂Ri = 
� · (# of friend of user i), where � = e�15. The parame- 
ter � be loosely estimate base on the volume of Twitter 
links; it be fix prior to any empirical evaluation. 

3Twitter defines the friend of a user a the set of people the 
user follows. 
4The exact method use to restrict an entire browsing his- 
tory to link clicked on from Twitter depends on the mode 
of the attack and be described late in more detail. 



Network Data 
Realtime 

De-anonymization 

Network Cache 

1. Search 

2. Crawl 

3. Rank 

Realtime 
Crawler 

Background 
Crawler 

Twitter 

Listener 

Figure 2: System architecture for real-time de- 

anonymization of web browsing histories. 

4.2 System for Real-Time De-Anonymization 
Our strategy to generate candidate query Twitter data 

source for search result and network information (i.e., fol- 
low lists). There is, however, a limit to how many query 
we can make in real-time. To address this issue, we first ob- 
serve that link appear in a large number of candidate set 
provide little signal, and so one can calculate the MLE on a 
reduce history Ĥ ✓ H of informative link whose network 
information be obtainable within some set amount of time. 
If any part of a link’s network be too expensive to obtain, 
we disregard the link entirely. This ensures we calculate qRi 
exactly for a give history Ĥ. Our approximation Ĥ can be 
thought of a strategically throw away some signal about 
link clicked by the user in order to exactly and tractably 
calculate qR. 

To e�ciently assemble network information, we use an 
eager cache strategy: in the background, we run a service 
that listens to the stream of tweet and find user with 
between 10,000 and 100,000 followers; these user be then 
add to a cache and crawl by a di↵erent process. Our 
de-anonymization strategy thus relies on both pre-fetched 
data and information obtain in real-time. 

Figure 2 outline the system architecture use for real- 
time de-anonymization. Our de-anonymization service start 
by receive an anonymous historyH. It then search Twit- 
ter for all tweet contain link found in the history. The 
search result be pass on to the crawl system which 
attempt to crawl the follower of every poster in the search 
results. If a poster be not cached and be too expensive to 
crawl in real-time, we omit the link they post from our 
history set to produce Ĥ. At the end of the crawl stage, 
we have a list of recommendation set and qRi scores. The 
final step of the de-anonymization calculates an MLE use 
Theorem 1 with the approximation Ĉ, Ĥ, and p̂Ri described 
in this section. 

5. SIMULATIONS 

5.1 Constructing synthetic history 
To evaluate our de-anonymization strategy, we start by 

examine performance on a set of synthetically generate 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 
● 

0% 

20% 

40% 

60% 

80% 

100% 

10 20 30 40 50 
Number of link 

Ac 
cu 

ra 
cy 

MLE 
Intersection size 
Jaccard 

Figure 3: De-anonymization accuracy on synthetic 

browsing history for three candidate rank meth- 

ods: the MLE of Theorem 1, the number of history 

link appear in the candidate’s feed (intersection 

size), and Jaccard similarity. 

histories. The simulated history be construct to follow a 
simple behavioral model: a user click on link mostly from 
their own feed and sometimes click on link post by a 
friend of a friend. A user might be encourage to visit such 
friend-of-friend link by Twitter’s algorithmic recommenda- 
tion system [37] or simply visit these link due to organic 
exploration. In either case, these friend-of-friend URLs test 
a de-anonymization model’s resilience to noise. 

Based on the behavioral model, we construct these histo- 
ries in three steps. First, we monitor the real-time stream 
of Twitter activity and randomly select a user who post 
a tweet, with two caveats: we exclude user with few than 
20 friend or follower and those with more than 300 friends. 
The former restriction ensures that our sample include rea- 
sonably active users, and the latter limit be chosen so that 
we can e�ciently construct history within rate limits. We 
note that this sample of user be not representative of the 
overall population of Twitter users, in part because those 
who appear in the real-time stream of tweet tend to be 
significantly more active than average. 

Next, for each of the select users, we generate friend 
link and friend-of-friend links. Friend links—posted by a 
friend of the user and thus appear in the user’s feed—are 
generate by randomly select one of the user’s friend and 
then randomly select a URL post by that friend in the 
last 30 days. We sample link post by friends-of-friends 
by first sample a friend of the user uniformly at random, 
then sample a friend of that friend at random, and finally 
sample a link post by that friend-of-friend. 

In total we generate 50 friend URLs and 10 friend-of-friend 
URLs for 60 users.5 We blend the friend and friend-of-friend 
URLs to create a collection of synthetic history for each 
user with various size and link compositions. 

5We select 90 user from the stream, but 30 have too little 
obtainable feed activity to simulate 50 URLs. 



● 

● 
● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

0% 

20% 

40% 

60% 

80% 

100% 

10 20 30 40 50 
Number of link 

Ac 
cu 

ra 
cy 

Friend−of−friend link 
0% 
10% 
20% 

Figure 4: De-anonymization accuracy on synthetic 

browsing history generate with vary level of 

noise. 

5.2 Analysis 
Figure 3 compare the accuracy of the MLE of Theorem 1 

to that of the intersection and Jaccard method for pure 
friend histories. The plot illustrates two points. First, even 
with a relatively small history of links, the MLE method suc- 
cessfully de-anonymizes a substantial proportion of users. 
For example, give 30 URLs, the MLE correctly identi- 
fied 52% of user in our sample. Second, at each history 
size, the MLE match or outperforms the other two de- 
anonymization methods. In contrast to the MLE’s 52% de- 
anonymization rate with 30 URLs, the intersection method 
correctly identify 42% of users, and Jaccard identify just 
13% of users. 

We next examine the robustness of our approach to histo- 
ries contain friend-of-friend URLs. Figure 4 show these 
result for blend history contain 10% and 20% friend- 
of-friend link at various history sizes. As expected, history 
with friend-of-friend link be harder to de-anonymize. In 
particular, with 30 URLs, 20% of which be friend-of-friend 
links, we successfully identify 40% of user in our sample, 
compare to 52% for history of size 30 contain only 
friend links. Nevertheless, de-anonymization accuracy be still 
relatively high. 

6. REAL-WORLD EVALUATION 

6.1 Collecting web browsing data 
The above result on synthetic browsing history point to 

the potential for our de-anonymization approach. We next 
evaluate this method on real, user-contributed web browsing 
histories, which we collect via an online experiment. 

The experiment be open to user run the Google 
Chrome web browser on a desktop computer. As show in 
Figure 5, when user first visit our site, they be pro- 
vided with a brief description of the experiment and then 
ask to install a Chrome extension to send u their re- 
cent web browsing history. The extension extract up to 
100 Twitter links—marked with domain name t.co—visited 
within the past 30 day to generate the Twitter history H 

for de-anonymization. If few than five link be found 
in their history, we told user that we would not be able to 
de-anonymize them and sent no data to our servers. Users 
with at least five t.co link be give an opportunity to 
verify their data and confirm that they want to share it. 

The uploaded history be process by the real-time de- 
anonymization system described in Section 4.2. The system 
construct the reduce history Ĥ of link by define infor- 
mative link a those which were: (1) tweet or retweeted at 
most 100 times; and (2) have only be tweet or retweeted 
by people with at most 100,000 followers. If a user do not 
have at least four informative link (i.e., if |Ĥ| < 4), we again 
told the user that we do not have enough information to suc- 
cessfully run the de-anonymization procedure. Overall, 84% 
of user who submit their browsing history pass this 
filter; among those with at least 10 links, 92% passed; and 
among those with at least 20 links, 97% pass the filter. 

The de-anonymization procedure produce a list of can- 
didates ranked by MLE score. Users be show the top 15 
candidate and prompt to inform u which, if any, corre- 
sponded to their Twitter profile. After respond to this 
question, user be o↵ered an optional opportunity to dis- 
close their identity by signing into Twitter, in which case 
we would know their identity even if none of our top 15 
candidate be correct. 

We recruit participant by advertising the experiment 
on a variety of websites, include Twitter, Facebook, Quora, 
Hacker News, and Freedom to Tinker. In total, 649 people 
submit web browsing histories. In 119 case (18%), our 
application encounter a fatal error (e.g., because the Twit- 
ter API be temporarily unavailable), and we be unable to 
run the de-anonymization algorithm. Of the 530 remain 
cases, 87 user (16%) have few than four informative links, 
and so we do not attempt to de-anonymize them; we thus 
attempt to de-anonymize 443 users. Of these, 374 user 
(84%) confirm whether or not our de-anonymization at- 
tempt be successful. And of these 374 users, 77 (21%) 
additionally disclose their identity by signing into Twitter. 

We note that the user who participate in our experiment 
be not representative of the Twitter population. In partic- 
ular, they be quite active: the user who report their 
identity have a median number of 378 follower and post a 
median number of 2,041 total tweets. 

6.2 Analysis 
Of the 374 people who confirm the accuracy of our de- 

anonymization attempt, 268 (72%) be the top candidate 
generate by the MLE, and 303 participant (81%) be 
among the top 15 candidates.6 Consistent with our simu- 
lation results, we be able to successfully de-anonymize a 
substantial proportion of user who contribute their web 
browsing histories. 

Figure 6 add detail to this result, show accuracy a a 
function of the size of a participant’s submit history. As 
expected, performance be strongly related to history size. We 
correctly identify 86% of user with 50–75 URLs whereas 
our accuracy fall to 71% for participant with 25–50 URLs. 

We also compare the performance of our de-anonymization 
approach to the intersection method and Jaccard similarity. 

6In part we achieve this performance because t.co link be 
uniquely generate every time a link be post to Twitter. 
However, even if we consider only the original, unshortened 
links, we still achieve 49% de-anonymization accuracy. 



Figure 5: Screenshots of the online experiment. 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

0% 

20% 

40% 

60% 

80% 

100% 

25 50 75 100 
Number of link 

Ac 
cu 

ra 
cy 

MLE 
Intersection size 
Jaccard 

Figure 6: De-anonymization accuracy for three can- 

didate rank method on user-contributed web 

browsing histories. Accuracy for intersection size 

and Jaccard ranking be approximate, a ground- 

truth answer be typically only available for user 

who be ranked in the top 15 by the MLE. 

Unfortunately, because of the experiment’s design, we typi- 
cally only know ground truth for the individual who ranked 
in the top 15 by our approach, and it be possible in theory 
that the other two method succeed precisely where the MLE 
fails. To ass this possibility, we consider the 11 case in 
which an individual do not appear in our list of top 15 can- 
didates but disclose their identity by signing into Twitter. 
In all of these 11 cases, both the intersection method and 
Jaccard fail to successfully identify the user. Thus, while 
base on a small sample, it seem reasonable to assume that 
if a participant be not ranked in the top 15 by the MLE 
method, then other de-anonymization method would also 
have failed. Based on this assumption, Figure 6 compare 
the performance of all three de-anonymization method on 
the full set of 374 users. As on the simulated data, we find 
that our method outperforms Jaccard similarity and inter- 
section size, often by a substantial margin. 

We can further use the MLE score to estimate the con- 
fidence of our predictions. Given order candidate score 
s1 � s2 � · · · � sn for an anonymous browsing history H, 
the eccentricity [28] of H be (s1�s2)/std-dev({si}). Figure 7 

70% 

75% 

80% 

85% 

90% 

95% 

100% 

0% 20% 40% 60% 80% 100% 
Coverage 

Ac 
cu 

ra 
cy 

Eccentricity 
History length 

Figure 7: De-anonymization accuracy on the top-k 
history ranked by eccentricity and history length. 

show prediction accuracy on the top-k instance ranked by 
eccentricity. The right-most point on the plot corresponds 
to accuracy on the full set of 374 history (72%); if we limit 
to the 50% of history with the high eccentricity, accu- 
racy increase to 96%. For comparison, the plot also show 
accuracy a a function of history length, and indicates that 
eccentricity be the good predictor of accuracy. 

7. THREAT MODELS 
Our de-anonymization strategy assumes access to an in- 

dividual’s Twitter browsing history. Such data be available 
to a variety of organization with commercial or strategic in- 
centives to de-anonymize users. In this section, we describe 
two such possible attacker and evaluate the e�cacy of our 
approach on data available to them. 

Third-party tracker be entity embed into some web- 
site for the purpose of collect individual user browsing 
habits. Trackers can determine whether a user arrive from 
Twitter to a site where they be embed by examine 
the page’s document.referrer property. We estimate the 
de-anonymization capability of four common third-party 
trackers: Google, Facebook, ComScore, and AppNexus. For 
each user-contributed history, and for each organization, we 
first determine which URLs in the history they be likely 
able to track by check if the organization have a tracker 



● 

● 

● 

● 

● 

● 

● 

● ● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

0% 

20% 

40% 

60% 

80% 

100% 

25 50 75 100 
Number of link 

Ac 
cu 

ra 
cy 

● 

● 

● 

● 

Google 
Facebook 
AppNexus 
ComScore 

Figure 8: De-anonymization accuracy on the subset 

of link visible to various organization that track 

online behavior, base on user-submitted histories. 

instal on the top-level domain of the URL [10]. We then 
attempt to de-anonymize each history base only on the sub- 
set of URLs visible to each track organization. Figure 8 
show the result of this analysis and illustrates that all four 
organization be pervasive enough to successfully carry out 
de-anonymization use our method. 

Finally, we examine the de-anonymization capability of a 
network eavesdropper. These be attacker capable of sni↵- 
ing network tra�c (e.g., state actor with access to back- 
bone servers) and monitoring server request from anony- 
mous users. Due to security feature of the http protocol, 
such attack can only determine the full URL of request 
make over http. Therefore, to simulate the data available 
to them, we run our de-anonymization strategy use only 
http link submit in our real-world experiment. We find 
that network attacker can be fairly successful: 31% of par- 
ticipants in our experiment be identify use only their 
http links. 

8. DISCUSSION AND CONCLUSION 
We have show theoretically and empirically that web 

browsing history can be link to social medium profile us- 
ing only public auxiliary information. The form of Theo- 
rem 1 applies to any bound set of item from which an 
anonymous actor make selection influence by some a�n- 
ity mechanism. For example, paper citation be likely se- 
lected from a universe of relevant work where author have 
an a�nity for their own work or past citations, a show 
by [14]. With this framing, our MLE may be use to de- 
anonymize paper with author name strip for double- 
blind review. Similarly, our model be applicable to the prob- 
lem of de-anonymizing a movie rental record base on re- 
view post on the web [28], a well a a long-term inter- 
section attack against an anonymity system base on, say, 
the timing of a user’s tweet or blog post [24]. All of these 
can be see a behavioral fingerprint of a user, and our anal- 
ysis help explain why such fingerprint tend to be unique 
and linkable. Exploring other problem where our model 
applies be one direction for future work. 

Our statistical approach yield a useful algorithm that 
we be able to validate in simulation and with real brows- 
ing histories. Our quantitative estimate of accuracy might 
overestimate the e↵ectiveness of a real-life attack in some 
way but underestimate it in other ways. For example, 
a third-party tracker may not always be able to learn if 
the current page visit originate from the social medium site 
in question. On the other hand, the adversary may fruit- 
fully make use of other fingerprint information available 
through URLs, such a UTM codes. Thus, the main lesson 
of our paper be qualitative: we present multiple line of evi- 
dence that browsing history may be link to social medium 
profiles, even at a scale of hundred of million of potential 
users. Furthermore, our attack have no universal mitigation 
outside of disable public access to social medium sites, an 
act that would undermine the value of these sites. 

There be many way in which browsing history may be 
de-anonymized online. Most straightforwardly, both Face- 
book and Google—exploiting the fact that they be promi- 
nent first party a well a third parties—track user under 
their real identities. However, our attack be significant for it 
broad applicability. The technique be available to all track- 
ers, include those with whom the user have no first-party 
relationship. Our finding be relevant in various other set- 
tings. One example be the Federal Communications Com- 
mission’s recently adopt privacy rule for Internet service 
providers: the FCC require that to store and use customer 
information, ISPs ensure that the information be “not rea- 
sonably linkable” to individuals. Our result suggest that 
pseudonymous browsing history fail this test, and call for 
more research into privacy-preserving data mining of brows- 
ing history [16, 12]. 

Acknowledgments 
We thank Twitter for support this research by provide 
free access to the Gnip search API. We also thank Henri 
Stern for his help building the online experiment. Finally, 
we thank Jonathan Mayer and the anonymous reviewer 
for their helpful feedback. Narayanan be support by NSF 
award CNS 1526353. 

9. REFERENCES 
[1] G. Acar, C. Eubank, S. Englehardt, M. Juarez, 

A. Narayanan, and C. Diaz. The web never forgets: 
Persistent track mechanism in the wild. In 
Proceedings of ACM CCS, page 674–689. ACM, 2014. 

[2] G. Acar, M. Juarez, N. Nikiforakis, C. Diaz, S. Gürses, 
F. Piessens, and B. Preneel. Fpdetective: dust the 
web for fingerprinters. In Proceedings of the 2013 ACM 
SIGSAC conference on Computer & communication 
security, page 1129–1140. ACM, 2013. 

[3] M. D. Ayenson, D. J. Wambach, A. Soltani, N. Good, 
and C. J. Hoofnagle. Flash cooky and privacy II: 
Now with html5 and etag respawning. 2011. 

[4] C. Budak, S. Goel, J. Rao, and G. Zervas. 
Understanding emerge threat to online advertising. 
In Proceedings of the ACM Conference on Economics 
and Computation, 2016. 

[5] M. Chew and S. Stamm. Contextual identity: 
Freedom to be all your selves. In Proceedings of the 
Workshop on Web, volume 2. Citeseer, 2013. 



[6] N. Christin, S. S. Yanagihara, and K. Kamataki. 
Dissecting one click frauds. In Proceedings of the 17th 
ACM conference on Computer and communication 
security, page 15–26. ACM, 2010. 

[7] Y.-A. De Montjoye, C. A. Hidalgo, M. Verleysen, and 
V. D. Blondel. Unique in the crowd: The privacy 
bound of human mobility. Scientific reports, 3, 2013. 

[8] Y.-A. De Montjoye, L. Radaelli, V. K. Singh, et al. 
Unique in the shopping mall: On the reidentifiability 
of credit card metadata. Science, 347(6221), 2015. 

[9] P. Eckersley. How unique be your web browser? In 
International Symposium on Privacy Enhancing 
Technologies Symposium, page 1–18. Springer, 2010. 

[10] S. Englehardt and A. Narayanan. Online tracking: A 
1-million-site measurement and analysis. In ACM 
Conference on Computer and Communications 
Security, 2016. 

[11] S. Englehardt, D. Reisman, C. Eubank, 
P. Zimmerman, J. Mayer, A. Narayanan, and E. W. 
Felten. Cookies that give you away: The surveillance 
implication of web tracking. In Proceedings of the 
24th Conference on World Wide Web, 2015. 

[12] Ú. Erlingsson, V. Pihur, and A. Korolova. Rappor: 
Randomized aggregatable privacy-preserving ordinal 
response. In Proceedings of the Conference on 
Computer and Communications Security, 2014. 

[13] D. Fifield and S. Egelman. Fingerprinting web user 
through font metrics. In International Conference on 
Financial Cryptography and Data Security, 2015. 

[14] S. Hill and F. Provost. The myth of the double-blind 
review?: Author identification use only citations. 
SIGKDD Explor. Newsl., 5(2):179–184, Dec. 2003. 

[15] M. Korayem and D. J. Crandall. De-anonymizing 
user across heterogeneous social compute 
platforms. In ICWSM, 2013. 

[16] A. Korolova, K. Kenthapadi, N. Mishra, and 
A. Ntoulas. Releasing search query and click 
privately. In Proceedings of the 18th international 
conference on World wide web. ACM, 2009. 

[17] B. Krishnamurthy, K. Naryshkin, and C. Wills. 
Privacy leakage vs. protection measures: the grow 
disconnect. In Proceedings of the Web, 2011. 

[18] B. Krishnamurthy and C. E. Wills. On the leakage of 
personally identifiable information via online social 
networks. In Proceedings of the 2nd ACM workshop on 
Online social networks, page 7–12. ACM, 2009. 

[19] P. Laperdrix, W. Rudametkin, and B. Baudry. Beauty 
and the beast: Diverting modern web browser to 
build unique browser fingerprints. In 37th IEEE 
Symposium on Security and Privacy, 2016. 

[20] A. Lerner, A. K. Simpson, T. Kohno, and F. Roesner. 
Internet jones and the raider of the lose trackers: An 
archaeological study of web track from 1996 to 
2016. In 25th USENIX Security Symposium, 2016. 

[21] T. Libert. Exposing the invisible web: An analysis of 
third-party http request on 1 million websites. 
International Journal of Communication, 9:18, 2015. 

[22] C. Y. Ma, D. K. Yau, N. K. Yip, and N. S. Rao. 
Privacy vulnerability of publish anonymous mobility 

traces. IEEE/ACM Transactions on Networking, 
21(3):720–733, 2013. 

[23] A. Marthews and C. Tucker. Government surveillance 
and internet search behavior. Available at SSRN 
2412564, 2015. 

[24] N. Mathewson and R. Dingledine. Practical tra�c 
analysis: Extending and resist statistical disclosure. 
In International Workshop on Privacy Enhancing 
Technologies, page 17–34. Springer, 2004. 

[25] J. R. Mayer and J. C. Mitchell. Third-party web 
tracking: Policy and technology. In 2012 IEEE 
Symposium on Security and Privacy. IEEE, 2012. 

[26] K. Mowery and H. Shacham. Pixel perfect: 
Fingerprinting canvas in HTML5. W2SP, 2012. 

[27] A. Narayanan, H. Paskov, N. Z. Gong, J. Bethencourt, 
E. Stefanov, E. C. R. Shin, and D. Song. On the 
feasibility of internet-scale author identification. In 
IEEE Symposium on Security and Privacy, 2012. 

[28] A. Narayanan and V. Shmatikov. Robust 
de-anonymization of large sparse datasets. In 2008 
IEEE Symposium on Security and Privacy (sp 2008), 
page 111–125. IEEE, 2008. 

[29] N. Nikiforakis, A. Kapravelos, W. Joosen, C. Kruegel, 
F. Piessens, and G. Vigna. Cookieless monster: 
Exploring the ecosystem of web-based device 
fingerprinting. In Security and privacy (SP), 2013 
IEEE symposium on, page 541–555. IEEE, 2013. 

[30] L. Olejnik, G. Acar, C. Castelluccia, and C. Diaz. The 
leak battery A privacy analysis of the HTML5 
Battery Status API. Technical report, 2015. 

[31] L. Olejnik, C. Castelluccia, and A. Janc. Why Johnny 
can’t browse in peace: On the uniqueness of web 
browsing history patterns. In 5th Workshop on Hot 
Topics in Privacy Enhancing Technologies, 2012. 

[32] J. Penney. Chilling e↵ects: Online surveillance and 
wikipedia use. Berkeley Technology Law Journal, 2016. 

[33] A. Ramachandran, Y. Kim, and A. Chaintreau. “I 
knew they clicked when I saw them with their 
friends”. In Proceedings of the 2nd Conference on 
Online Social Networks, 2014. 

[34] F. Roesner, T. Kohno, and D. Wetherall. Detecting 
and defend against third-party track on the web. 
In Proceedings of the 9th USENIX conference on 
Networked Systems Design and Implementation, page 
12–12. USENIX Association, 2012. 

[35] K. Sharad and G. Danezis. An automate social graph 
de-anonymization technique. In Proceedings of the 
13th Workshop on Privacy in the Electronic Society, 
page 47–58. ACM, 2014. 

[36] A. Soltani, S. Canty, Q. Mayo, L. Thomas, and C. J. 
Hoofnagle. Flash cooky and privacy. In AAAI spring 
symposium: intelligent information privacy 
management, volume 2010, page 158–163, 2010. 

[37] J. Su, A. Sharma, and S. Goel. The e↵ect of 
recommendation on network structure. In Proceedings 
of the 25th Conference on World Wide Web, 2016. 

[38] G. Wondracek, T. Holz, E. Kirda, and C. Kruegel. A 
practical attack to de-anonymize social network users. 
In IEEE Symposium on Security and Privacy, 2010. 


