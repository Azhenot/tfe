


















































Accurate Intelligible Models with Pairwise Interactions 

Yin Lou 
Dept. of Computer Science 

Cornell University 
yinlou@cs.cornell.edu 

Rich Caruana 
Microsoft Research 

Microsoft Corporation 
rcaruana@microsoft.com 

Johannes Gehrke 
Dept. of Computer Science 

Cornell University 
johannes@cs.cornell.edu 

Giles Hooker 
Dept. of Statistical Science 

Cornell University 
giles.hooker@cornell.edu 

ABSTRACT 
Standard generalize additive model (GAMs) usually model 
the dependent variable a a sum of univariate models. Al- 
though previous study have show that standard GAMs 
can be interpret by users, their accuracy be significantly 
less than more complex model that permit interactions. 

In this paper, we suggest add select term of inter- 
act pair of feature to standard GAMs. The result 
models, which we call GA2M-models, for Generalized Addi- 
tive Models plus Interactions, consist of univariate term and 
a small number of pairwise interaction terms. Since these 
model only include one- and two-dimensional components, 
the component of GA2M-models can be visualize and in- 
terpreted by users. To explore the huge (quadratic) number 
of pair of features, we develop a novel, computationally ef- 
ficient method call FAST for rank all possible pair of 
feature a candidate for inclusion into the model. 

In a large-scale empirical study, we show the effectiveness 
of FAST in rank candidate pair of features. In addition, 
we show the surprising result that GA2M-models have al- 
most the same performance a the best full-complexity mod- 
el on a number of real datasets. Thus this paper postulate 
that for many problems, GA2M-models can yield model 
that be both intelligible and accurate. 

Categories and Subject Descriptors 
I.2.6 [Computing Methodologies]: Learning—Induction 

Keywords 
classification, regression, interaction detection 

1. INTRODUCTION 
Many machine learn technique such a boost or 

bag trees, SVMs with RBF kernels, or deep neural net 

Permission to make digital or hard copy of all or part of this work for personal or 
classroom use be grant without fee provide that copy be not make or distribute 
for profit or commercial advantage and that copy bear this notice and the full citation 
on the first page. Copyrights for component of this work own by others than the 
author(s) must be honored. Abstracting with credit be permitted. To copy otherwise, or 
republish, to post on server or to redistribute to lists, require prior specific permission 
and/or a fee. Request permission from permissions@acm.org. 
KDD’13, August 11–14, 2013, Chicago, Illinois, USA. 
Copyright be held by the owner/author(s). Publication right license to ACM. 
ACM 978-1-4503-2174-7/13/08 ...$15.00. 

be powerful classification and regression model for high- 
dimensional prediction problems. However, due to their 
complexity, the result model be hard to interpret for 
the user. But in many applications, intelligibility be a im- 
portant a accuracy [19], and thus building model that user 
can understand be a crucial requirement. 

Generalized additive model (GAMs) be the gold stan- 
dard for intelligibility when only univariate term be con- 
sidered [13, 19]. Standard GAMs have the form 

g(E[y]) = 
∑ 

fi(xi), (1) 

where g be the link function. Standard GAMs be easy to 
interpret since user can visualize the relationship between 
the univariate term of the GAM and the dependent vari- 
able through a plot fi(xi) vs. xi. However there be unfor- 
tunately a significant gap between the performance of the 
best standard GAMs and full complexity model [19]. In 
particular, Equation 1 do not model any interaction be- 
tween features, and it be this limitation that lie at the core 
of the lack of accuracy of standard GAMs a compare to 
full complexity models. 

Example 1. Consider the function F (x) = log(x21x3) + 
x2x3. F have a pairwise interaction (x2, x3), but no in- 
teractions between (x1, x2) or (x1, x3), since log(x 

2 
1x3) = 

2 log(x1) + log(x3), which be additive. 

Our first contribution in this paper be to build model 
that be more powerful than GAMs, but be still intelligible. 
We observe that two-dimensional interaction can still be 
render a heatmaps of fij(xi, xj) on the two-dimensional 
xi, xj-plane, and thus a model that include only one- and 
two-dimensional component be still intelligible. Therefore 
in this paper, we propose building model of the form 

g(E[y]) = 
∑ 

fi(xi) + 
∑ 

fij(xi, xj); (2) 

we call the result model class Generalized Additive Models 
plus Interactions, or short GA2Ms. 

The main challenge in building GA2Ms be the large num- 
ber of pair of feature to consider. We thus only want to 
include “true” interaction that pas some statistical test. 
To this end, we focus on problem with up to thousand of 
feature since for truly high dimensional problem (e.g., mil- 
lion of features), it be almost intractable to test all possible 
pairwise interaction (e.g., trillion of feature pairs). 

Existing approach for detect statistical interaction 
can be divide into two classes. One class of method di- 



rectly model and compare the interaction effect and ad- 
ditive effect [10, 11, 18, 25]. One drawback of these meth- 
od be that spurious interaction may be report over low- 
density region [15]. The second class of method measure 
the performance drop in the model if certain interaction be 
not included; they compare the performance between re- 
stricted and unrestricted models, where restrict model 
be not allow to model an interaction in question [22]. 
Although this class of method do not suffer from the 
problem of low-density regions, they be computationally 
extremely expensive even for pairwise interaction detection. 

Our second contribution in this paper be to scale the con- 
struction of GA2Ms by propose a novel, extremely efficient 
method call FAST to measure and rank the strength of the 
interaction of all pair of variables. Our experiment show 
that FAST can efficiently rank all pairwise interaction close 
to a ground truth ranking. 

Our third contribution be an extensive empirical evalua- 
tion of GA2M-models. Surprisingly, on many of the datasets 
include in our study, the performance of GA2M-models be 
close and sometimes good than the performance of full- 
complexity models. These result indicate that GA2M-models 
not only make a significant step in improve accuracy over 
standard GAMs, but in some case they actually come all the 
way to the performance of full-complexity models. The per- 
formance may be due to the difficulty of estimate intrin- 
sically high dimensional function from limited data, sug- 
gesting that the bias associate with the GA2M structure be 
outweigh by a drop in variance. We also demonstrate that 
the result model be intelligible through a case study. 

In this paper we make the follow contributions: 

• We introduce the model class GA2M. 

• We introduce our new method FAST for efficient in- 
teraction detection. (Section 4) 

• We show through an extensive experimental evalua- 
tion that (1) GA2Ms have accuracy comparable to full- 
complexity models; (2) FAST accurately rank inter- 
action a compare to a gold standard; and (3) FAST 
be computationally efficient. (Section 5) 

We start with a problem definition and a survey of related 
work in Sections 2 and 3. 

2. PROBLEM DEFINITION 
Let D = {(xi, yi)}N1 denote a dataset of size N , where 

xi = (xi1, ..., xin) be a feature vector with n feature and yi 
be the response. Let x = (x1, ..., xn) denote the variable or 
feature in the dataset. For u ⊆ {1, ..., n}, we denote by xu 
the subset of variable whose index be in u. Similarly x−u 
will indicate the variable with index not in u. To simplify 
notation, we denote U1 = {{i}|1 ≤ i ≤ n}, U2 = {{i, j}|1 ≤ 
i < j ≤ n}, and U = U1 ∪ U2, i.e., U contains all index for 
all feature and pair of features. 

For any u ∈ U , letHu denote the Hilbert space of Lebesgue 
measurable function fu(xu), such that E[fu] = 0 and E[f 

2 
u] 

< ∞, equip with the inner product 〈fu, f ′u〉 = E[fuf ′u]. 
Let H1 = 

∑ 
u∈U1 Hu denote the Hilbert space of func- 

tions that have additive form F (x) = 
∑ 

u∈U1 fu(xu) on 
univariate compnents; we call those component shape func- 
tions [19]. Similarly let H = 

∑ 
u∈U Hu denote the Hilbert 

space of function of x = (x1, ..., xn) that have additive form 

F (x) = 
∑ 

u∈U fu(xu) on both one- and two-dimensional 
shape functions. Models described by sum of low-order 
component be call generalize additive model (GAMs), 
and in the remainder of the paper, we use GAMs to denote 
model that only consist of univariate terms. 

We want to find the best model F ∈ H that minimizes 
the follow objective function: 

min 
F∈H 

E[L(y, F (x))], (3) 

where L(·, ·) be a non-negative convex loss function. When L 
be the square loss, our problem becomes a regression prob- 
lem, and if L be logistic loss function, we be deal with a 
classification problem. 

3. EXISTING APPROACHES 

3.1 Fitting Generalized Additive Models 
Terms in GAMs can be represent by a variety of func- 

tions, include spline [24], regression trees, or tree ensem- 
bles [9]. There be two popular method of fitting GAMs: 
Backfitting [13] and gradient boost [10]. When the shape 
function be spline, fitting GAMs reduces to fitting general- 
ized linear model with different bases, which can be solve 
by least square or iteratively reweighted least square [25]. 

Spline-based method become inefficient when model 
high order interaction because the number of parame- 
ters to estimate grows exponentially; tree-based method 
be more suitable in this case. Standard additive model- 
ing only involves model individual feature (also call 
feature shaping). Previous research show that gradient 
boost with ensemble of shallow regression tree be the 
most accurate method among a number of alternative [19]. 

3.2 Interaction Detection 
In this section, we briefly review exist approach to 

interaction detection. 
ANOVA. An additive model be fit with all pairwise inter- 

action term [13] and the significance of interaction term be 
measure through an analysis of variance (ANOVA) test [25]. 
The correspond p-value for each pair can then be com- 
puted; however, this require the computation of the full 
model, which be prohibitively expensive. 

Partial Dependence Function. Friedman and Popescu 
propose the follow statistic to measure the strength of 
pairwise interactions, 

H2ij = 

∑N 
k=1[F̂ij(xki, xkj)− F̂i(xki)− F̂j(xkj))] 

2∑N 
k=1 F̂ 

2 
ij(xki, xkj) 

(4) 

where F̂u(xu) = Ex−u [F (xu, x−u)] be the partial dependence 
function (PDF) [10, 11] and F be a complex multi-dimensional 

function learn on the dataset. Computing F̂u(xu) on the 
whole dataset be expensive, thus one often specifies a subset 
of size m on which to compute F̂u(xu). The complexity be 
then O(m2). However, since partial dependence function 
be compute base on uniform sampling, they may detect 
spurious interaction over low-density region [15]. 

GUIDE. GUIDE test pairwise interaction base on the 
χ2 test [18]. An additive model F be fit in H1 and residual 
be obtained. To detect interaction for (xi, xj), GUIDE 
divide the (xi, xj)-space into four quadrant by splitting the 
range of each variable into two half at the sample median. 



Then GUIDE construct a 2×4 contingency table use the 
residual sign a row and the quadrant a columns. The 
cell value in the table be the number of “+”s and “-”s in 
each quadrant. These count permit the computation of a 
p-value to measure the interaction strength of a pair. While 
this might be more robust to outliers, in practice it be less 
powerful than the method we propose. 

Grove. Sorokina et al. propose a grove-based method to 
detect statistical interaction [22]. To measure the strength 
of a pair (xi, xj), they build both the restrict model Rij(x) 
and unrestricted model F (x), where Rij(x) be prevent 
from model an interaction (xi, xj): 

Rij(x) =f\i(x1, ..., xi−1, xi+1, ..., xn) 

+ f\j(x1, ..., xj−1, xj+1, ..., xn). (5) 

To correctly estimate interaction strength, such method re- 
quire a model to be highly predictive when certain interac- 
tion be not allow to appear, and therefore many learn 
algorithm be not applicable (e.g., bag decision trees). 
To this end, they choose to use Additive Groves [21]. 

They measure the performance a standardize root mean 
square error (RMSE) and quantify the interaction strength 
Iij by the difference between Rij(x) and F (x), 

stRMSE(F (x)) = 
RMSE(F (x)) 

StD(F ∗(x)) 
(6) 

Iij = stRMSE(Rij(x))− stRMSE(F (x)) (7) 

where Std(F ∗(x)) be calculate a standard deviation of the 
response value in the training set. The rank of all pair 
can be generate base on the strength Iij . 

To handle correlation among features, they use a vari- 
ant of backward elimination [12] to do feature selection. 
Although Grove be accurate in practice, building restrict 
and unrestricted model be computationally expensive and 
therefore this method be almost infeasible for large high di- 
mensional datasets. 

4. OUR APPROACH 
For simplicity and without loss of generality, we focus 

in this exposition on regression problems. Since there be 
O(n2) pairwise interactions, it be very hard to detect pair- 
wise interaction when n be large. Therefore we propose a 
framework use greedy forward stagewise selection strategy 
to build the most accurate model in H. 

Algorithm 1 summarizes our approach call GA2M. We 
maintain two set S and Z, where S contains the select 
pair so far and Z be the set of the remain pair (Line 1- 
2). We start with the best additive model F so far in Hilbert 
space H1+ 

∑ 
u∈S Hu (Line 4) and detect interaction on the 

residual R (Line 5). Then for each pair in Z, we build an 
interaction model on the residual R (Line 6-7). We select 
the best interaction pair and include it in S (Line 9-10). We 
then repeat this process until there be no gain in accuracy. 

Note that Algorithm 1 will find an overcomplete set S by 
the greedy nature of the forward selection strategy. When 
feature be correlated, it be also possible that the algorithm 
include false pairs. For example, consider the function in 
Example 1. If x1 be highly correlate with x3, then (x1, x2) 
may look like an interaction pair, and it may be include in 
S before we select (x2, x3). But since we will refit the model 
every time we include a new pair, it be expect that F will 

Algorithm 1 GA2M Framework 

1: S ← ∅ 
2: Z ← U2 
3: while not converge do 
4: F ← arg minF∈H1+∑u∈S Hu 12E[(y − F (x))2] 
5: R← y − F (x) 
6: for all u ∈ Z do 
7: Fu ← E[R|xu] 
8: u∗ ← arg minu∈Z 12E[(R− Fu(xu)) 

2] 
9: S ← S ∪ {u∗} 

10: Z ← Z − {u∗} 

xi 

xj 

cj 

ci ci 

cj cj 

Figure 1: Illustration for search cut on input 
space of xi and xj. On the left we show a heat map 
on the target for different value of xi and xj. ci and 
cj be cut for xi and xj, respectively. On the right 
we show an extremely simple predictor of model 
pairwise interaction. 

perfectly model (x2, x3) and therefore (x1, x2) will become 
a less important term in F . 

For large high-dimensional datasets, however, Algorithm 
1 be very expensive for two reasons. First, fitting interaction 
model for O(n2) pair in Z can be very expensive if the 
model be non-trivial. Second, every time we add a pair, we 
need to refit the whole model, which be also very expensive 
for large datasets. As we will see in Section 4.1 and Sec- 
tion 4.2, we will relax some of the constraint in Algorithm 1 
to achieve good scalability while still stay accurate. 

4.1 Fast Interaction Detection 
Consider the conceptual additive model in Equation 2, 

give a pair of variable (xi, xj) we wish to measure how 
much benefit we can get if we model fij(xi, xj) instead of 
fi(xi) + fj(xj). Since we start with shape individual fea- 
tures and always detect interaction on the residual, fi(xi)+ 
fj(xj) be presumably model and therefore we only need 
to look at the residual sum of square (RSS) for the inter- 
action model fij . The intuition be that when (xi, xj) be a 
strong interaction, model fij can significantly reduce the 
RSS. However, we do not wish to fully build fij since this 
be a very expensive operation; instead we be look for a 
cheap substitute. 

4.1.1 Overview 
Our idea be to build an extremely simple model for fij 

use cut on the input space of xi and xj , a illustrate 
in Figure 1. The simplest model we can build be to place 
one cut on each variable, i.e., we place one ci and one cut 



xi 

xj 

CHtj(cj) CH 
t 
j(cj) 

C 
H 

ti (c 
i ) 

C 
H 

ti (c 
i ) 

a b 

c d 

a = pre-computed 

b = CHti (ci)− a 

c = CHtj(cj)− a 

d = CHti (ci)− c 

Figure 2: Illustration for compute sum of target 
for each quadrant. Given that the value of red quad- 
rant be known, we can easily recover value in other 
quadrant use marginal cumulative histograms. 

cj on xi and xj , respectively. Those cut be parallel to 
the axes. The interaction predictor Tij be construct by 
take the mean of all point in each quadrant. We search 
for all possible (ci, cj) and pick the best Tij with the low 
RSS, which be assign a weight for (xi, xj) to measure the 
strength of interaction. 

4.1.2 Constructing Predictors 
Näıve implementation of FAST be straightforward, but 

careless implementation have very high complexity since we 
need to repeatedly build a lot of Tij for different cuts. The 
key insight for faster version of FAST be that we do not 
need to scan through the dataset each time to compute Tij 
and compute it RSS. We show that by use very sim- 
ple bookkeeping data structures, we can greatly reduce the 
complexity. 

Let dom(xi) = {v1i , ..., vdii } be a sort set of possible 
value for variable xi, where di = |dom(xi)|. Define Hti (v) 
a the sum of target when xi = v, and define H 

w 
i (v) a 

the sum of weight (or counts) when xi = v. Intuitively, 
these be the standard histogram when construct re- 
gression trees. Similarly, we define CHti (v) and CH 

w 
i (v) 

a the cumulative histogram for sum of target and sum 
of weights, respectively, i.e., CHti (v) = 

∑ 
u≤vH 

t 
i (u) and 

CHwi (v) = 
∑ 

u≤vH 
w 
i (u). Accordingly, define CH 

t 
i (v) =∑ 

u>vH 
t 
i (u) = CH 

t 
i (v 

di 
i ) − CH 

t 
i (v) and define CH 

w 
i (v) =∑ 

u>vH 
w 
i (u) = CH 

w 
i (v 

di 
i ) − CH 

w 
i (v). Furthermore, define 

Htij(u, v) and H 
w 
ij(u, v) a the sum of target and the sum 

of weights, respectively, when (xi, xj) = (u, v). 
Consider again the input space for (xi, xj), we need a 

quick way to compute the sum of target and sum of weight 
for each quadrant. Figure 2 show an example for compute 
sum of target on each quadrant. Given the above notations, 
we already know the marginal cumulative histogram for xi 
and xj , but unfortunately use these marginal value only 
can not recover value on four quadrants. Thus, we have to 
compute value for one quadrant. 

We show that it be very easy and efficient to compute all 
possible value for the red quadrant give any cut (ci, cj) 
use dynamic programming. Once that quadrant be known, 
we can easily recover value in other quadrant use marginal 
cumulative histograms. We store those value into lookup 
tables. Let Lt(ci, cj) = [a, b, c, d] be the lookup table for sum 

Algorithm 2 ConstructLookupTable 

1: sum← 0 
2: for q = 1 to dj do 
3: sum← sum+Htij(v1i , vqj ) 
4: a[1][q]← sum 
5: L(v1i , v 

q 
j )← ComputeV alues(CH 

t 
i , CH 

t 
j , a[1][q]) 

6: for p = 2 to di do 
7: sum← 0 
8: for q = 1 to dj do 
9: sum← sum+Htij(vpi , v 

q 
j ) 

10: a[p][q]← sum+ a[p− 1][q] 
11: L(vpi , v 

q 
j )← ComputeV alues(CH 

t 
i , CH 

t 
j , a[p][q]) 

of target on cut (ci, cj), and denote L 
w(ci, cj) = [a, b, c, d] 

a the lookup table for sum of weight on cut (ci, cj). 
Algorithm 2 describes how to compute the lookup table 

Lt. We focus on compute quadrant a and other quad- 
rant can be easily computed, which be handle by subrou- 
tine ComputeV alues. Given Htij , we first compute a for 
the first row of Lt (Line 3-5). Let a[p][q] denote the value 
for cut (p, q). Note a[p][q] = a[p− 1][q] + 

∑ 
k≤qH 

t 
ij(v 

p 
i , v 

k 
j ). 

Thus we can efficiently compute the rest of the lookup table 
row by row (Line 6-11). 

Once we have Lt and Lw, give any cut (ci, cj), we can 
easily construct Tij . For example, we can set the leftmost 
leaf value in Tij a L 

t(ci, cj).a/L 
w(ci, cj).a. It be easy to see 

that with those bookkeeping data structures, we can reduce 
the complexity of building predictor to O(1). 

4.1.3 Calculating RSS 
In this section, we show that calculate RSS for Tij 

can be very efficient. Consider the definition of RSS. Let 
Tij .r denote the prediction value on region r, where r ∈ 
{a, b, c, d}. 

RSS = 

N∑ 
k=1 

(yk − Tij(xk))2 (8) 

= 

( 
N∑ 

k=1 

y2k − 2 
∑ 
r 

Tij .rL 
t.r + 

∑ 
r 

(Tij .r) 
2Lw.r 

) 
(9) 

In practical implementation, we only need to care about∑ 
r(Tij .r) 

2Lw.r−2 
∑ 

r Tij .rL 
t.r since we be only interested 

in relative order of RSS, and it be easy to see the com- 
plexity of compute RSS for Tij be O(1). 

4.1.4 Complexity Analysis 
For each pair (xi, xj), compute the histogram and cu- 

mulative histogram need to scan through the data and 
therefore it complexity be O(N). Constructing the lookup 
table take O(didj + N) time. Thus, the time complexity 
of FAST be O(didj +N) for one pair (xi, xj). Besides, Since 
we need to store di-by-dj matrix for each pair, the space 
complexity be O(didj). 

For continuous features, didj can be quite large. However, 
we can discretize the feature into b equi-frequency bins. 
Such feature discretizing usually do not hurt the perfor- 
mance of regression tree [17]. As we will see in Section 5, 
FAST be not sensitive to a wide range of bs. Therefore, the 
complexity can be reduce to O(b2 + N) per pair when we 



discretize feature into b bins. For small b (b ≤ 256), we 
can quickly process each pair. 

4.2 Two-stage Construction 
With FAST, we can quickly rank of all pair in Z, the re- 

maining pair set, and add the best interaction to the model. 
However, refit the whole model after each pair be add 
can be very expensive for large high-dimensional datasets. 
Therefore, we propose a two-stage construction approach. 

1. In Stage 1, build the best additive model F inH1 use 
only one-dimensional components. 

2. In Stage 2, fix the one-dimensional functions, and build 
model for pairwise interaction on residuals. 

4.2.1 Implementation Details 
To scale up to large datasets and many features, we dis- 

cretize the feature into 256 equi-frequency bin for contin- 
uous features.1 We find such feature discretization rarely 
hurt the performance but substantially reduces the run- 
ning time and memory footprint since we can use one byte 
to store a feature value. Besides, discretizing the feature re- 
move the sort requirement for continuous feature when 
search for the best cut in the space. 

Previous research show that feature shape use gra- 
dient boost [10] with shallow regression tree ensemble 
can achieve the best accuracy [19]. We follow similar ap- 
proach (i.e., gradient boost with shallow tree-like ensem- 
bles) in this work. However, a regression tree be not the ideal 
learn method for each component for two reasons. First, 
while regression tree be good a a generic shape function 
for any xu, shape a single feature be equivalent to cut 
on a line, but line cut can be make more efficient than 
regression tree. Second, use regression tree to shape pair- 
wise function can be problematic. Recall that in Stage 1, 
we obtain the best additive model after gradient boost 
converges. This mean add more cut to any one feature 
do not reduce the error, and equivalently, any cut on a sin- 
gle feature be random. Therefore, when we begin to shape 
pairwise interactions, the root test in a regression tree that 
be construct greedily top-down be random. 

Similar to [19], to effectively shape pairwise interactions, 
we build shallow tree-like model on the residual a illus- 
trated in Figure 3. We enumerate all possible cut ci on 
xi. Given this cut, we greedily search the best cut c 

1 
j in the 

region above ci and similarly greedily search the best cut c 
2 
j 

in the region below ci. Note we can reuse the lookup table 
Lt and Lw we developed for FAST for fast search of those 
three cuts. Figure 3 show an example of compute the leaf 
value give ci, c 

1 
j and c 

2 
j . Similarly, we can quickly compute 

the RSS give any combination of 3 cut once the leaf value 
be available, just a we do in Section 4.1.4, and therefore 
it be very fast to search for the best combination of cut in 
this space. Similarly, we search for the best combination of 
3 cut with 1 cut on xj and 2 cut on xi and pick the good 
model with low RSS. It be easy to see the complexity be 
O(N + b2), where b be the number of bin for each feature 
and b = 256 in our case. 

1Note that this be not the number of bin use in FAST, 
the interaction detection process. Here we use 256 bin for 
feature/pair shaping. 

xi 

xj 

c1j 

c2j 

ci 
a b 

c d 

a = Lt(ci, c 
1 
j ).a/L 

w(ci, c 
1 
j ).a 

b = Lt(ci, c 
1 
j ).b/L 

w(ci, c 
1 
j ).b 

c = Lt(ci, c 
2 
j ).c/L 

w(ci, c 
2 
j ).c 

d = Lt(ci, c 
2 
j ).d/L 

w(ci, c 
2 
j ).d 

Figure 3: Illustration for compute shape function 
for pairwise interaction. 

Dataset Size Attributes %Pos 

Delta 7192 6 - 
CompAct 8192 22 - 

Pole 15000 49 - 
CalHousing 20640 9 - 
MSLR10k 1200192 137 - 
Spambase 4601 58 39.40 

Gisette 6000 5001 50.00 
Magic 19020 11 64.84 
Letter 20000 17 49.70 

Physics 50000 79 49.72 

Table 1: Datasets. 

4.2.2 Further Relaxation 
For large datasets, even refit the model on select 

pair can be very expensive. Therefore, we propose to use 
the rank of FAST right after Stage 1, to select the top-K 
pair to S, and fit a model use the pair in S on the residual 
R, where K be chosen accord to compute power. 

4.2.3 Diagnostics 
Models that combine both accuracy and intelligibility be 

important. Usually S will still be an overcomplete set. For 
intelligibility, once we have learn the best model in H, 
we would like to rank all term (one- and two-dimensional 
components) so that we can focus on the most important 
features, or pairwise interactions. Therefore, we need to 
assign weight for each term. We use 

√ 
E[f2u], the standard 

deviation of fu (since E[fu] = 0), a the weight for term 
u. Note this be a natural generalization of the weight in 
the linear models; this be easy to see since fi(xi) = wixi,√ 
E[f2i ] be equivalent to |wi| if feature be normalize so 

that E[x2i ] = 1. 

5. EXPERIMENTS 
In this section we report experimental result on both syn- 

thetic and real datasets. The result in Section 5.1 show 
GA2M learns model that be nearly a accurate a full- 
complexity random forest model while use term that de- 
pend only on single feature and pairwise interaction and 
thus be intelligible. The result in Section 5.2 demonstrate 
that FAST find the most important interaction of O(n2) 
feature pair to include in the model. Section 5.3 compare 
the computational cost of FAST and GA2M to compete 
methods. Section 5.4 briefly discus several important de- 



Model Delta CompAct Pole CalHousing MSLR10k Mean 

Linear Regression 0.58±0.01 7.92±0.47 30.41±0.24 7.28±0.80 0.76±0.00 1.52±0.79 
GAM 0.57±0.02 2.74±0.04 21.62±0.38 5.76±0.55 0.75±0.00 1.00±0.00 

GA2M Rand - - 11.37±0.38 - 0.73±0.00 - 
GA2M Coef - - 11.61±0.43 - 0.73±0.00 - 

GA2M Order - - 10.81±0.29 - 0.74±0.00 - 
GA2M FAST 0.55±0.02 2.53±0.02 10.59±0.35 5.00±0.91 0.73±0.00 0.84±0.20 

Random Forests 0.53±0.19 2.45±0.08 11.38±1.03 4.90±0.81 0.71±0.00 0.83±0.17 

Table 2: RMSE for regression datasets. Each cell contains the mean RMSE ± one standard deviation. 
Average normalize score be show in the last column, calculate a relative improvement over GAM. 

Model Spambase Gisette Magic Letter Physics Mean 

Logistic Regression 6.22±0.93 15.78±3.28 17.11±0.08 27.54±0.27 30.02±0.37 1.79±1.25 
GAM 5.09±0.64 3.95±0.65 14.85±0.28 17.84±0.20 28.83±0.24 1.00±0.00 

GA2M Rand 5.04±0.52 3.53±0.61 - - 28.82±0.25 - 
GA2M Coef 4.89±0.54 3.43±0.55 - - 28.74±0.37 - 

GA2M Order 4.93±0.65 3.08±0.55 - - 28.76±0.34 - 
GA2M FAST 4.78±0.70 2.91±0.38 13.88±0.32 8.62±0.31 28.20±0.18 0.81±0.21 

Random Forests 4.76±0.70 3.25±0.47 12.45±0.64 6.16±0.22 28.48±0.40 0.79±0.26 

Table 3: Error rate for classification datasets. Each cell contains the error rate ± one standard deviation. 
Average normalize score be show in the last column, calculate a relative improvement over GAM. 

sign choice make for FAST and GA2M. Finally, Section 5.5 
concludes with a case study. 

5.1 Model Accuracy on Real Datasets 
We run experiment on ten real datasets to show the accu- 

racy that GA2M can achieve with model that depend only 
on 1-d feature and pairwise feature interactions. 

5.1.1 Datasets 
Table 1 summarizes the 10 datasets. Five be regression 

problems: “Delta” be the task of control the aileron of 
an F16 aircraft [1]. “CompAct” be from the Delve repository 
and describes the state of multiuser computer [2]. “Pole” 
describes a telecommunication problem [23]. “CalHousing” 
describes how housing price depend on census variable [16]. 
“MSLR10k” be a learning-to-rank dataset but we treat rele- 
vance a regression target [3]. The other five datasets be bi- 
nary classification problems: The “Spambase”, “Magic” and 
“Letter” datasets be from the UCI repository [4]. “Gisette” 
be from the NIPS feature selection challenge [5]. “Physics” be 
from the KDD Cup 2004 [6]. 

The feature in all datasets be discretized into 256 equi- 
frequency bins. For each model we include at most 1000 
feature pairs; we include all feature pair in the six problem 
with least dimension, and the top 1000 feature pair found 
by FAST on the “Pole”, “MSLR10k”, “Spambase”, “Gisette”, 
and “Physics” datasets. Although it be possible that high 
accuracy might be obtain by include more or few fea- 
ture pairs, search for the optimal number of pair be expen- 
sive and GA2M be reasonably robust to excess feature pairs. 
However, it be too expensive to include all feature pair on 
problem with many features. We use 8 bin for FAST in 
all experiments. 

5.1.2 Results 
We compare GA2M to linear/logistic regression, feature 

shape (GAMs) without interactions, and full-complexity 

random forests. For regression problem we report root 
mean square error (RMSE) and for classification problem 
we report 0/1 loss. To compare result across different datasets, 
we normalize result by the error of GAMs on each dataset. 
For all experiments, we train on 80% of the data and hold 
aside 20% of the data a test sets. 

In addition to FAST, we also consider three baseline meth- 
od on five high dimensional datasets, i.e., GA2M Rand, 
GA2M Coef and GA2M Order. GA2M Rand mean we add 
same number of random pair to GAM. GA2M Order and 
GA2M Coef use the weight of 1-d feature in GAM to pro- 
pose pairs; GA2M Order generates pair by the order of 1-d 
feature and GA2M Coef generates pair by the product of 
weight of 1-d features. 

The regression and classification result be present in 
Table 2 and Table 3. As expected, the improvement over 
linear model from shape individual feature (GAMs) be 
substantial: on average feature shape reduces RMSE 34% 
on the regression problems, and reduces 0/1 loss 44% on 
the classification problems. What be surprising, however, be 
that by add shape pairwise interaction to the models, 
GA2M FAST substantially close the accuracy gap between 
unintelligible full-complexity model such a random forest 
and GAMs. On some datasets, GA2M FAST even outper- 
form the best random forest model. Also, none of the base- 
line method perform comparably GA2M FAST. 

5.2 Detecting Feature Interactions with FAST 
In this section we evaluate how accurately FAST detects 

feature interaction on synthetic problems. 

5.2.1 Sensitivity to the Number of Bins 
To evaluate sensitivity of FAST we use the synthetic func- 

tion generator in [10] to generate random functions. Because 
these be synthetic function, we know the ground truth in- 
teracting pair and use average precision (area under the 
precision-recall curve evaluate at true points) a the eval- 



0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

1.1 

2 4 8 16 32 64 128 256 

Av 
er 

ag 
e 

Pr 
ec 

be 
io 

n 

Number of Bins 

10^2 10^3 10^4 10^5 10^6 

(a) 10 features. 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

2 4 8 16 32 64 128 256 

Av 
er 

ag 
e 

Pr 
ec 

be 
io 

n 

Number of Bins 

10^2 10^3 10^4 10^5 10^6 

(b) 100 features. 

Figure 4: Sensitivity of FAST to the number of bins. 

0.75 

0.8 

0.85 

0.9 

0.95 

1 

Grove 
ANOVA 

FAST 
GUIDE 

PDF 100 

PDF 200 

PDF 400 

PDF 800 

Average Precision 

(a) 

10 

100 

1000 

10000 

100000 

1e+06 

Grove 
ANOVA 

PDF 800 

PDF 400 

PDF 200 

PDF 100 

GUIDE 

FAST 

Time (s) 

(b) 

Figure 5: Precision/Cost on synthetic function. 

uation metric. We vary b = 2, 4, ..., 256 and the dataset 
size N = 102, 103, ..., 106. For each fix N , we generate 
datasets with n feature and k high order interaction xu, 
where |u| = b1.5 + rc and r be drawn from an exponential 
distribution with mean λ = 1. We experiment with two 
cases: 10 feature with 25 high order interaction and 100 
feature with 1000 high order interactions. 

Figure 4 show the mean average precision and variance 
for 100 trial at each setting. As expected, average pre- 
cision increase a dataset size increases, and decrease a 
the number of feature increase from 10 (left graph) to 100 
(right graph). When there be only 10 feature and a many 
a 106 samples, FAST rank all true interaction above all 
non-interacting pair (average precision = 1) in most cases, 
but a the sample size decrease or the problem difficulty 
increase average precision drop below 1. In the graph on 
the right with 100 feature there be 4950 feature pairs, and 
FAST need large sample size (106 or greater) to achieve av- 
erage precision above 0.7, and a expect performs poorly 
when there be few sample than pair of features. 

On these test problem the optimal number of bin ap- 
pear to be about b = 8, with average precision fall 
slightly for number of bin large and small than 8. This be 
a classic bias-variance tradeoff: small b reduces the chance 
of overfitting but at the risk of fail to model some kind 
of interactions, while large b allows more complex interac- 
tions to be model but at the risk of allow some false 
interaction to be confuse with weak true interactions. 

5.2.2 Accuracy 
The previous section show that FAST accurately de- 

tects feature interaction when the number of sample be 
much large than the number of feature pairs, but that ac- 
curacy drop a the number of feature pair grows compa- 
rable to and then large than the number of samples. In 
this section we compare the accuracy of FAST to the in- 
teraction detection method discuss in Section 3.2. For 
ANOVA, we use R package mgcv to compute p-values un- 
der a Wald test [25]. For PDF, we use RuleFit package and 
we choose m = 100, 200, 400, 800, where m be the sample size 
that trade off efficiency and accuracy [7]. Grove be available 
in TreeExtra package [8]. 

Here we conduct experiment on synthetic data generate 
by the follow function [14, 22]. 

F (x) = πx1x2 
√ 

2x3 − sin−1(x4) + log(x3 + x5)− 
x9 
x10 

√ 
x7 
x8 
− x2x7 (10) 

Variables x4, x5, x8, x10 be uniformly distribute in [0.6, 1] 
and the other variable be uniformly distribute in [0, 1]. 

We generate 10, 000 point for these experiments. Figure 5(a) 
show the average precision of the methods. On this prob- 
lem, the Grove and ANOVA method be accurate and rank 
all 11 true pair in the top of the list. FAST be almost a 
good and correctly rank the top ten pairs. The other meth- 
od be significantly less accurate than Grove, ANOVA, and 
FAST. 

To understand why FAST do not pick up the 11th pair, 
we plot heat map of the residual of select pair in Fig- 
ure 6. (x1, x2) and (x2, x7) be two of the correctly ranked 
true pairs, (x1, x7) be a false pair ranked below the true pair 
FAST detects correctly but above the true pair it misses, and 
(x8, x10) be the true pair FAST miss and rank below this 
false pair. The heat map show strong interaction be easy 
to distinguish, but some false interaction such a (x1, x7) 
can have signal a strong a that of weak true interaction 
such a (x8, x10). In fact, Sorokina et al. found that x8 
be a weak feature, and do not consider pair that use x8 a 
interaction on 5, 000 sample [22], so we be near the thresh- 
old of detectability of (x8, x10) go from 5, 000 to 10, 000 
samples. 

5.2.3 Feature Correlation and Spurious Pairs 
If feature be correlated, spurious interaction may be 

detect because it be difficult to tell the difference between 
a true interaction between x1 and x2 and the spurious in- 
teraction between x1 and x3 when x3 be strongly correlate 
with x2; any interaction detection method such a FAST 
that examines pair in isolation will have this problem. With 
GA2M, however, it be fine to include some false positive pair 
because GA2M be able to post-filter false positive pair by 
look at the term weight of shape interaction in the 
final model. 

To demonstrate this, we use the synthetic function in 
Equation 10, but make x6 correlate to x1. We generate 
2 datasets, one with ρ(x1, x6) = 0.5 and the other with 
ρ(x1, x6) = 0.95, where ρ be the correlation coefficient. We 
run FAST on residual after feature shaping. We give the 
top 20 pair found by FAST to GA2M, which then us gra- 
dient boost to shape those pairwise interactions. Figure 7 
illustrates how the weight of select pairwise interaction 
evolve after each step of gradient boosting. Although the 
pair (x2, x6) can be incorrectly introduce by FAST because 
of the high correlation between x1 and x6, the weight on this 
false pair decrease quickly a boost proceeds, indicate 
that this pair be spurious. This not only allows the model 
train on the pair to remain accurate in the face of spu- 
rious pairs, but also reduces the weight (and ranking) give 
to this shape term so that intelligibility be not be hurt by 
the spurious term. 



0 

5 

10 

15 

20 

25 

30 

0 5 10 15 20 25 30 

x2 

x1 

"hm/0.1.txt" u 1:2:3 

-0.6 
-0.4 
-0.2 
0 
0.2 
0.4 
0.6 
0.8 

0 

5 

10 

15 

20 

25 

30 

0 5 10 15 20 25 30 

x7 

x2 

"hm/1.6.txt" u 1:2:3 

-0.6 
-0.4 
-0.2 
0 
0.2 
0.4 
0.6 
0.8 

(x1, x2) (x2, x7) 

0 

5 

10 

15 

20 

25 

30 

0 5 10 15 20 25 30 

x7 

x1 

"hm/0.6.txt" u 1:2:3 

-0.4 
-0.3 
-0.2 
-0.1 
0 
0.1 
0.2 
0.3 
0.4 

0 

5 

10 

15 

20 

25 

30 

0 5 10 15 20 25 30 

x1 
0 

x8 

"hm/7.9.txt" u 1:2:3 

-0.4 
-0.3 
-0.2 
-0.1 
0 
0.1 
0.2 
0.3 
0.4 

(x1, x7) (x8, x10) 

Figure 6: True/Spurious heat maps. Features be 
discretized into 32 bin for visualization. 

0 
0.002 
0.004 
0.006 
0.008 

0.01 
0.012 
0.014 
0.016 
0.018 

0.02 

0 1000 2000 3500 

W 
ei 

gh 
t 

Iteration 

(x1, x2) 
(x2, x6) 
(x1, x3) 

(x7, x9) 
(x3, x6) 
(x2, x3) 

(x9, x10) 
(x3, x5) 
(x8, x9) 

(a) ρ(x1, x6) = 0.5 

0 
0.002 
0.004 
0.006 
0.008 

0.01 
0.012 
0.014 
0.016 

0 1000 2000 3500 

W 
ei 

gh 
t 

Iteration 

(x1, x2) 
(x2, x6) 
(x1, x3) 

(x7, x9) 
(x3, x6) 
(x2, x3) 

(x9, x10) 
(x3, x5) 
(x8, x9) 

(b) ρ(x1, x6) = 0.95 

Figure 7: Weights for pairwise interaction term in 
the model. 

5.3 Scalability 
Figure 5(b) illustrates the run time of different meth- 

od on 10, 000 sample from Equation 10. Model building 
time be included. FAST take about 10 second to rank 
all possible pair while the two other accurate methods, 
ANOVA and Grove, be 3-4 order of magnitude slower. 
Grove, which be probably the most accurate interaction de- 
tection method currently available, take almost a week to 
run once on this data. This show the advantage of FAST; 
it be very fast with high accuracy. On this problem FAST 
take less than 1 second to rank all pair and the majority 
of time be devote to building the additive model. 

Figure 8 show the run time of FAST per pair on real 
datasets. It be clear that on real datasets, FAST be both 
accurate and efficient. 

5.4 Design Choices 
An alternate to interaction detection that we consider 

be to build ensemble of tree on residual after shape the 
individual feature and then look at tree statistic to find 
combination of feature that co-occur in path more often 
than their independent rate warrants. By use 1-step look- 
ahead at the root we also hop to partially mitigate the 
myopia of greedy feature installation to make interaction 
more likely to be detected. Unfortunately, feature with 
high “co-occurence counts” do not correlate well with true 
interaction on synthetic test problems, and the best tree- 
base method we could devise do not detect interaction 
a well a FAST, and be considerably more expensive. 

● 

● 

● 

● 

● 
●●● 

● 

● 

5e+03 2e+04 1e+05 5e+05 

2e 
− 

04 
5e 

− 
03 

1e 
− 

01 

Size of dataset 

T 
im 

e 
(s 

) 
pe 

r 
pa 

ir 

Spambase 
Gissette 

Delta 

CompAct 
Pole 

Magic Letter 

CalHousing 
Physics 

MSLR10k 

Figure 8: Computational cost on real datasets. 

5.5 Case Study: Learning to Rank 
Learning-to-rank be an important research topic in the 

data mining, machine learn and information retrieval com- 
munities. In this section, we train intelligible model with 
shape one-dimensional feature and pairwise interaction 
on the “MSLR10k” dataset. A complete description of fea- 
tures can be found in [3]. We show the top 10 most im- 
portant individual feature and their shape function in first 
two row of Figure 9. The number above each plot be the 
weight for the correspond term in the model. Interest- 
ingly, we found BM25 [20], usually consider a a powerful 
feature for ranking, ranked 70th (BM25 url) in the list af- 
ter shaping. Other feature such a IDF (inverse document 
frequency) enjoy much high weight in the learn model. 

The last two row of Figure 9 show the 10 most important 
pairwise interaction and their term strengths. Each of them 
show a clear interaction that could not be model by addi- 
tive terms. The non-linear shape of the individual feature 
in the top plot and the pairwise interaction in the bottom 
plot be intelligible to expert and feature engineers, but 
would be well hidden in full-complexity models. 

6. CONCLUSIONS 
We present a framework call GA2M for building intel- 

ligible model with pairwise interactions. Adding pairwise 
interaction to traditional GAMs retains intelligibility, while 
substantially increase model accuracy. To scale up pair- 
wise interaction detection, we propose a novel method call 
FAST that efficiently measure the strength of all potential 
pairwise interactions. 

Acknowledgements. We thank the anonymous review- 
er for their valuable comments, and we thank Nick Craswell 
of Microsoft Bing for insightful discussions. This research 
have be support by the NSF under Grants IIS-0911036 
and IIS-1012593. Any opinions, finding and conclusion or 
recommendation express in this material be those of the 
author and do not necessarily reflect the view of the NSF. 

7. REFERENCES 
[1] http://www.liaad.up.pt/~ltorgo/Regression/ 

DataSets.html. 

[2] http: 
//www.cs.toronto.edu/~delve/data/datasets.html. 

[3] http: 
//research.microsoft.com/en-us/projects/mslr/. 

http://www.liaad.up.pt/~ltorgo/Regression/DataSets.html 
http://www.liaad.up.pt/~ltorgo/Regression/DataSets.html 
http://www.cs.toronto.edu/~delve/data/datasets.html 
http://www.cs.toronto.edu/~delve/data/datasets.html 
http://research.microsoft.com/en-us/projects/mslr/ 
http://research.microsoft.com/en-us/projects/mslr/ 


0.0093 0.0084 0.0061 0.0058 0.0057 

-0.2 

-0.1 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0 50 100 150 200 250 
-0.2 

0 

0.2 

0.4 

0.6 

0.8 

1 

1.2 

1.4 

0 50 100 150 200 250 
-0.2 

0 
0.2 
0.4 
0.6 
0.8 

1 
1.2 
1.4 
1.6 
1.8 

0 2 4 6 8 10 12 14 16 18 
-0.15 
-0.1 

-0.05 
0 

0.05 
0.1 

0.15 
0.2 

0.25 
0.3 

0.35 
0.4 

0 50 100 150 200 250 
-0.25 

-0.2 
-0.15 

-0.1 
-0.05 

0 
0.05 

0.1 
0.15 

0.2 
0.25 

0 50 100 150 200 250 

stream length body query-url click count cover query term number sum of term frequency body LAIR.ABS body 
title 

0.0055 0.0052 0.0050 0.0040 0.037 

-0.6 
-0.4 
-0.2 

0 
0.2 
0.4 
0.6 
0.8 

1 
1.2 

0 50 100 150 200 250 
-0.8 

-0.6 

-0.4 

-0.2 

0 

0.2 

0.4 

0 10 20 30 40 50 60 
-1 

-0.5 

0 

0.5 

1 

1.5 

2 

2.5 

3 

0 10 20 30 40 50 60 70 80 90 
-0.2 

-0.15 
-0.1 

-0.05 
0 

0.05 
0.1 

0.15 
0.2 

0.25 
0.3 

0 50 100 150 200 250 
-0.8 

-0.6 

-0.4 

-0.2 

0 

0.2 

0.4 

0.6 

0.8 

0 20 40 60 80 100 120 

min of term frequency cover query term ratio stream length url IDF title outlink number 
whole document title 

6.6934e-4 6.6726e-4 5.5579e-4 3.4585e-4 3.0110e-4 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 
-0.3 

-0.2 

-0.1 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0 

50 

100 

150 

200 

250 

0 5 10 15 20 25 
-0.2 

-0.1 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0 

5 

10 

15 

20 

25 

0 10 20 30 40 50 60 70 80 90 
-0.2 
-0.1 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 250 
-0.6 

-0.4 

-0.2 

0 

0.2 

0.4 

0.6 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 250 
-0.15 

-0.1 

-0.05 

0 

0.05 

0.1 

0.15 

0.2 

0.25 

qualityscore2 vs. inlink number vs. number of slash in url vs. url click count vs. pagerank vs. min of stream 
siterank number of slash in url stream length url query-url click count length normalize term 

frequency whole document 

2.4755e-4 2.4104e-4 2.3218e-4 2.2952e-4 2.1643e-4 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 250 
-0.5 

-0.4 

-0.3 

-0.2 

-0.1 

0 

0.1 

0.2 

0.3 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 250 
-0.8 

-0.6 

-0.4 

-0.2 

0 

0.2 

0.4 

0 

50 

100 

150 

200 

0 20 40 60 80 100 120 
-0.15 
-0.1 
-0.05 
0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 250 
-0.2 

0 

0.2 

0.4 

0.6 

0.8 

1 

1.2 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 250 
-0.6 
-0.5 
-0.4 
-0.3 
-0.2 
-0.1 
0 
0.1 
0.2 
0.3 
0.4 
0.5 

url dwell time vs. qualityscore2 vs. siterank vs. pagerank vs. query-url click count vs. 
query-url click count min of tf*idf url outlink number BM25 url length of url 

Figure 9: Shapes of feature and pairwise interaction for the “MSLR10k” dataset with weights. Top two 
row show top 10 strong features. Next two row show top 10 strong interactions. 

[4] http://archive.ics.uci.edu/ml/. 

[5] http://www.nipsfsc.ecs.soton.ac.uk/. 

[6] http://osmot.cs.cornell.edu/kddcup/. 

[7] http: 
//www-stat.stanford.edu/~jhf/R-RuleFit.html. 

[8] http://additivegroves.net. 

[9] E. Bauer and R. Kohavi. An empirical comparison of 
voting classification algorithms: Bagging, boosting, 
and variants. Machine learning, 36(1):105–139, 1999. 

[10] J. Friedman. Greedy function approximation: a 
gradient boost machine. Annals of Statistics, 
29:1189–1232, 2001. 

[11] J. Friedman and B. Popescu. Predictive learn via 
rule ensembles. The Annals of Applied Statistics, 
page 916–954, 2008. 

[12] I. Guyon and A. Elisseeff. An introduction to variable 
and feature selection. The Journal of Machine 
Learning Research, 3:1157–1182, 2003. 

[13] T. Hastie and R. Tibshirani. Generalized additive 
models. Chapman & Hall/CRC, 1990. 

[14] G. Hooker. Discovering additive structure in black box 
functions. In KDD, 2004. 

[15] G. Hooker. Generalized functional anova diagnostics 
for high-dimensional function of dependent variables. 
Journal of Computational and Graphical Statistics, 
16(3):709–732, 2007. 

[16] R. Kelley Pace and R. Barry. Sparse spatial 

autoregressions. Statistics & Probability Letters, 
33(3):291–297, 1997. 

[17] P. Li, C. Burges, and Q. Wu. Mcrank: Learning to 
rank use multiple classification and gradient 
boosting. In NIPS, 2007. 

[18] W. Loh. Regression tree with unbiased variable 
selection and interaction detection. Statistica Sinica, 
12(2):361–386, 2002. 

[19] Y. Lou, R. Caruana, and J. Gehrke. Intelligible 
model for classification and regression. In KDD, 2012. 

[20] C. D. Manning, P. Raghavan, and H. Schütze. 
Introduction to information retrieval. Cambridge 
University Press Cambridge, 2008. 

[21] D. Sorokina, R. Caruana, and M. Riedewald. Additive 
grove of regression trees. In ECML, 2007. 

[22] D. Sorokina, R. Caruana, M. Riedewald, and D. Fink. 
Detecting statistical interaction with additive grove 
of trees. In ICML, 2008. 

[23] S. M. Weiss and N. Indurkhya. Rule-based machine 
learn method for functional prediction. Journal of 
Artificial Intelligence Research, 3:383–403, 1995. 

[24] S. Wood. Thin plate regression splines. Journal of the 
Royal Statistical Society: Series B (Statistical 
Methodology), 65(1):95–114, 2003. 

[25] S. Wood. Generalized additive models: an introduction 
with R. CRC Press, 2006. 

http://archive.ics.uci.edu/ml/ 
http://www.nipsfsc.ecs.soton.ac.uk/ 
http://osmot.cs.cornell.edu/kddcup/ 
http://www-stat.stanford.edu/~jhf/R-RuleFit.html 
http://www-stat.stanford.edu/~jhf/R-RuleFit.html 
http://additivegroves.net 

Introduction 
Problem Definition 
Existing Approaches 
Fitting Generalized Additive Models 
Interaction Detection 

Our Approach 
Fast Interaction Detection 
Overview 
Constructing Predictors 
Calculating RSS 
Complexity Analysis 

Two-stage Construction 
Implementation Details 
Further Relaxation 
Diagnostics 


Experiments 
Model Accuracy on Real Datasets 
Datasets 
Results 

Detecting Feature Interactions with FAST 
Sensitivity to the Number of Bins 
Accuracy 
Feature Correlation and Spurious Pairs 

Scalability 
Design Choices 
Case Study: Learning to Rank 

Conclusions 
References 

