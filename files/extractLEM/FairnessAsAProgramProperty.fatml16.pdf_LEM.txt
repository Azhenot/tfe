









































Fairness a a Program Property 

Aws Albarghouthi, Loris D’Antoni, 
Samuel Drews 

University of Wisconsin–Madison 

Aditya Nori 

Microsoft Research 

Abstract We explore the follow question: Is a 
decision-making program fair, for some useful defi- 
nition of fairness? First, we describe how several al- 
gorithmic fairness question can be phrase a pro- 
gram verification problems. Second, we discus an au- 
tomated verification technique for prove or disprov- 
ing fairness of decision-making program with respect 
to a model of the population. 

1. Introduction 
Algorithms have become powerful arbitrator of a 
range of significant decision with far-reaching soci- 
etal impact—hiring [21, 22], welfare allocation [15], 
prison sentence [2], police [5, 25], amongst many 
others. With the range and sensitivity of algorithmic de- 
cisions expand by the day, the question of whether 
an algorithm be fair be a press one. Indeed, the notion 
of algorithmic fairness have capture the attention of a 
broad spectrum of experts: machine learn and the- 
ory researcher [6, 13, 16, 29]; privacy researcher and 
investigative journalist [2, 10, 26, 28]; law scholar 
and social scientist [1, 3, 27]; governmental agency 
and NGOs [24]. 

Ultimately, algorithmic fairness be a question about 
program and their properties: Is a give program P 
fair, under some definition of fairness? Or, how fair 
be P? In this paper, we describe a line of work that 
approach the question of algorithmic fairness from 
a program-analytic perspective, in which our goal be 
to analyze a give decision-making program and con- 
struct a proof of it fairness or unfairness—just a a 
traditional static program verifier would prove correct- 
ness of a program with respect to, for example, lack of 
division by zero, integer overflows, null-pointer dere- 
frences, etc. 

We start by analyze what be the challenge and 
research question in check algorithmic fairness for 
decision make program (Section 2). We then present 

Decision-making program P 
(e.g., a learn classifier) 

Population model M 
(a probabilistic program) 

Fairness verifier 
a probabilistic 

program verifier 

Program P be fair wrt M 
(proof of fairness) 

Program P be unfair wrt M 
(proof of unfairness) 

Fairness 
property 

Figure 1. Overview 

a simple case study and show how technique for ver- 
ifying probabilistic program can be use to automat- 
ically prove or disprove global fairness for a class of 
program that subsume a range of machine learn 
classifier (Section 3). Finally, we lay a list of many 
challenge and interest question that the algo- 
rithms and program language community need 
to answer to achieve the ultimate goal of building a 
fully automate system for verify and guarantee 
algorithmic fairness in real-world application (Sec- 
tion 4). 

2. Proving Programs Fair 
In this section, we describe the component of the fair- 
ness verification problem. Intuitively, our goal be to 
prove whether a certain program be fair with respect to 
the set of possible input over which it operates. Tack- 
ling the fairness-verification problem require answer- 
ing a number of challenge questions: 
– What class of decision-making program should our 

program model capture? 

1 



– How can we define the set of possible input to the 
program and capture complex probability distribu- 
tions that be useful and amenable to verification? 

– How can we describe what it mean for the program 
to be fair? 

– How can we fully automate the verification process? 
Figure 1 provide a high-level picture of our pro- 

pose framework. As shown, the fairness verifier take 
a (white-box) decision-making program P and a pop- 
ulation model M . The verifier then proceeds to prove 
or disprove that P be fair for the give population de- 
fin by the model M . Here, the model M defines a 
joint probability distribution on the input of P . Exist- 
ing definition of fairness define program a fair or un- 
fair with respect to a give concrete dataset. While us- 
ing a concrete dataset simplifies the verification prob- 
lem, it also raise question of whether the dataset be 
representative for the population for which we be try- 
ing to prove fairness. Our technique move away from 
concrete datasets and replaces them with a probabilistic 
population model. We envision a future in which fair- 
ness verification be regulated.1 For instance, a govern- 
mental agency can publish a probabilistic population 
model (e.g., generate from census data). Any orga- 
nization employ a decision-making algorithm with 
potentially significant consequence (e.g., hiring) must 
quantify fairness of their algorithmic process against 
the current picture of the population, a specify by 
the population model. 

Decision-making program In the context of algorith- 
mic fairness, a program P take a input a vector of 
argument v represent a set of input attribute (fea- 
tures), where one (or more) of the argument v in the 
vector v be sensitive—e.g., gender or race. Evaluating 
P(v) may return a Boolean value indicating—e.g., hire 
or not hire—if the program be a binary or a numerical 
value—e.g., a mortgage rate. The set of combinators, 
operations, and type use by the program can vastly 
affect the complexity of the verification procedures. For 
example, loop be the hardest type of program 
construct to reason about, but most machine learn 
classifier do not contain loops. Similarly, since classi- 
fiers typically operate over real values, we can limit the 
set of possible type allow in our program to only 
be real or other type that can be desugared into 

1 The European Union (EU), for instance, have already begin regu- 
lating algorithmic decision-making [17]. 

reals. All these decision be crucial in the design of a 
verification procedure. 
Population model To be able to reason about the out- 
come of the program we need to specify what kind of 
input the program will operate on. For example, al- 
though a program that allocates mortgage might be 
“fair” with respect a certain set of applicants, it may be- 
come unfair when consider a different pool of peo- 
ple. In program verification, the “kind of inputs” over 
which the program operates be call the precondition 
and be typically state a a formal logical property with 
the program input a free variables. An example of 
program precondition be 

vgender = f → vjob 6= priest 

which indicates that none of the program input be both 
a woman and a priest. Of course, there be many pos- 
sible choice for what language we can use to describe 
the program’s precondition. In particular, if we want to 
capture a certain probability distribution over the input 
of the program, our language will be a logic that can 
describe probability and random variables. For exam- 
ple, we might want to be able to specify that half of 
the input be female, Pr[vgender = f ] = 0.5, or that 
the age of the process input have a particular distri- 
bution, vage ∼ gauss(18, 5). Again, the choice of the 
language allow in the precondition be crucial in the 
design of a verification procedure. From now on, we 
refer to the program precondition, Dpop, a the popula- 
tion model. 
Fairness property The next step be to define a prop- 
erty state that the program’s outcome be fair with re- 
spect to the program’s precondition. In program verifi- 
cation, this be call the postcondition of the program. 
As observe in the fairness literature, there be many 
way to define when and why a program be fair or un- 
fair. 

For example, if we want to prove group fairness— 
i.e., that the algorithm be just a likely to hire a mi- 
nority applicant (m) a it be for other, non-minority 
applicants—our postcondition will be an expression of 
the form 

Pr[P(v) = true | v = m] 
Pr[P(v) = true | v 6= m] 

> 1− � 

where true be the desire return value of the program, 
e.g., indicate hiring. On the other hand, if we want to 
prove individual fairness—i.e., similar input should 

2 



have similar outcomes—our postcondition will be an 
expression of the form 

Pr[P(v) 6= P(v′) | v ∼ v′] < � 

Notice that the last postcondition relates the outcome 
of the program on different input values. As the two 
type of property we described be radically differ- 
ent, they will also require different verification mecha- 
nisms. 
Proofs of (un)fairness The task of prove whether 
a program be fair boil down to statically check 
whether, on input satisfy the precondition, the out- 
come of the program satisfies the post-condition. For 
simple definitions, such a group fairness, the verifica- 
tion problem reduces to compute the probability of 
a number of event with respect to the program and 
the population model. For more complex definitions, 
such a individual fairness, prove fairness require 
more complex reason involve multiple run of the 
program (i.e., a hyperproperty [9]), a notoriously hard 
problem. In the case of a negative result, the verifier 
should provide the user with a proof of unfairness. De- 
pending on the fairness definition, produce a human- 
readable proof might be challenge a the argument 
might involve multiple and potentially infinite inputs. 
For example, in the case of group fairness it might be 
challenge to explain why the program output true on 
40% of the minority input and on 70% of the majority 
inputs. 

3. Case Study 
We now describe a simplify case study demonstrat- 
ing how our fairness verification methodology can be 
use to prove or disprove fairness of a give decision- 
make program. 
A program and a population model Consider the fol- 
low program dec, which be a decision-making pro- 
gram that take a job applicant’s college rank and 
year of experience and decides whether they get hire 
or not (the fairness target). The program implement 
a decision tree, perhaps one generate by a machine- 
learn algorithm. A person be hire if they attend 
a top-5 college (colRank <= 5) or have lot of expe- 
rience compare to their college’s rank (expRank > 
-5). Observe that dec do not access ethnicity. 

define dec(colRank, yExp) 
expRank ← yExp - colRank 
if (colRank <= 5) 

hire ← true 
elif (expRank > -5) 
hire ← true 

else 
hire ← false 

return hire 

Now, consider the program popModel, which be a 
probabilistic program describe a simple model of the 
population. Here, a member of the population have three 
attributes, all of which be real-valued: (i) ethnicity; 
(ii) colRank, the rank of the college the person at- 
tend (lower be better); and (iii) yExp, the year of 
work experience a person has. We consider a person be 
a member of a protect group if ethnicity > 10; we 
call this the sensitive condition. The population model 
can be view a a generative model of record of 
individuals—the more likely a combination be to occur 
in the population, the more likely it will be generated. 
For instance, the year of experience an individual have 
(line 4) follow a Gaussian distribution with mean 10 
and standard deviation 5. 

define popModel() 
ethnicity ~ gauss(0,10) 
colRank ~ gauss(25,10) 
yExp ~ gauss(10,5) 
if (ethnicity > 10) 
colRank ← colRank + 5 

return colRank, yExp 

A note on the program model Note that our pro- 
gram model, while admit arbitrary programs, be 
rich enough to capture program (classifiers) generate 
by standard machine learn algorithms. For exam- 
ple, linear support vector machines, decision trees, and 
neural networks, can be represent in our language 
simply use assignment with arithmetic expression 
and conditionals. Similarly, the population model be a 
probabilistic program, where assignment can be make 
by draw value from predefined distributions. Like 
other probabilistic program languages, our pro- 
gramming model be rich enough to subsume graphical 
model like Bayesian network [19]. 

Group fairness Suppose that our goal be to prove 
group fairness, follow the definition of Feldman et 
al. [16]: 

Pr[hire | min] 
Pr[hire | ¬min] 

> 1− � 

where min be shorthand for the sensitive condition eth- 
nicity > 10. 

3 



Probabilistic inference a volume computation To 
prove (un)fairness of the decision-making model with 
respect to the population, we need to compute the prob- 
ability appear in the group fairness ratio. For il- 
lustration, suppose we be compute the probability 
Pr[hire ∧ ¬min]. We need to reason about the com- 
position of the two programs, dec ◦ popModel. That 
is, we want to compute the probability that (i) pop- 
Model generates a non-minority applicant, and (ii) dec 
hire that applicant. To do so, we observe that every 
possible execution of the composition dec ◦ popModel 
be uniquely characterize by the set of the three prob- 
abilistic choice make by popModel. In other words, 
every execution be characterize by a vector v ∈ R3. 

Thus, our goal be to compute the probability that we 
draw a vector v that result in a minority applicant be- 
ing hired. Probabilistic program languages, e.g., 
Church [18], R2 [23], and Stan [7], employ approxi- 
mate inference techniques, like MCMC, which converge 
in the limit but offer no guarantee on how far we be 
from the exact result. In our work, we consider exact 
inference, which have primarily receive attention in the 
Bayesian network setting, and boil down to solve a 
#SAT instance [8]. In our setting, however, we be deal- 
ing with real-valued variables. 

Using standard technique from program analysis 
and verification, we can characterize the set of all 
such vector a a formula ϕ, which be comprise of 
Boolean combination (conjunctions/disjunctions) of 
linear inequalities—since our program only have lin- 
ear expressions. Geometrically, the formula ϕ be a set 
of convex polyhedron in Rn. Therefore, the probabil- 
ity Pr[hire ∧ ¬min] be the same a the probability of 
draw a vector v that lie inside of ϕ. In other words, 
we be interested in the volume of ϕ, weight by the 
probabilistic choices. Formally: 

Pr[hire ∧ ¬min] = 
∫ 
ϕ 
pepypc dedydc 

where, e.g., pe be the probability density function of the 
distribution gauss(0,10)—the distribution from which 
the value of ethnicity be drawn in line 2 of popModel. 

The volume computation problem be a well-studied 
and hard problem [14, 20]. Indeed, even for a convex 
polytope, compute it volume be #P-hard. Leverag- 
ing the great development in satisfiabiltiy modulo the- 
ories (SMT) solver [4], we developed a procedure that 
reduces the volume compuation problem to a series of 

colRank 

ethnicity yExp 

Underapproximation of ' 
a a union of hyperrectangles 

Formula ' in R3 
(blue face be unbounded) 

Figure 2. Underapproximation of ϕ a hyperrectangles 

call to the SMT solver, view completely a an or- 
acle. Specifically, our procedure us the SMT solver 
to sample subregions of ϕ that be hyperrectangular. 
Intuitively, for hyperrectangular region in Rn, evalu- 
ating the above integral be a matter of evaluate the 
CDFs of the various distributions. Thus, by systemat- 
ically sample more and more non-overlapping hy- 
perrectangles in ϕ, we maintain a low bound on the 
probability of interest. Figure 2 pictorially illustrates 
ϕ and an under-approximation with 4 hyperrectangles. 
Similarly, to compute an upper bound on the probabil- 
ity, we can simply invoke our procedure on ¬ϕ. 
Fairness certificate The fairness verification tool ter- 
minates when it have compute lower/upper bound that 
prove or disprove the desire fairness criteria. The hy- 
perrectangles sample in the process of compute vol- 
umes can serve a proof certificates. That is, an external 
entity can take the hyperrectangles, compute their vol- 
umes, and ensure that they indeed lie in the expect 
region in Rn. 

4. Experience and future Outlook 
Experience We have built a fairness-verification tool, 
call FairSquare, that take a decision-making pro- 
gram, a population model, and verifies fairness of the 
program with respect to the model. So far, we have fo- 
cused on group fairness. The tool us the popular Z3 
SMT solver [12] for manipulate first-order formula 
over arithmetic theories. 

We have use FairSquare to prove or disprove fair- 
ness of a suite of population model and program rep- 
resent machine-learning classifier that be auto- 
matically generate from real-world datasets use in 
other work on algorithmic fairness [11, 16, 29]. Specif- 
ically, we have consider linear SVMs, simple neural 
network with rectify linear units, and decision trees. 
Future outlook Looking forward, we see a wide range 
of avenue for improvement and exploration. For in- 

4 



stance, we be currently work on the problem of 
make an unfair program fair. That is, give a pro- 
gram P that be consider unfair, what be the small 
tweak that would make it fair. Our goal be to repair the 
program, make it fair, while ensure that it be seman- 
tically close to the original program. 

References 
[1] Ifeoma Ajunwa, Sorelle Friedler, Carlos E Scheidegger, 

and Suresh Venkatasubramanian. Hiring by algorithm: 
predict and prevent disparate impact. Available 
at SSRN 2746078, 2016. 

[2] Julia Angwin, Jeff Larson, Surya Mattu, and 
Lauren Kirchner. Machine bias: There’s soft- 
ware use across the country to predict fu- 
ture criminals. and it’s bias against blacks. 
https://www.propublica.org/article/machine- 
bias-risk-assessments-in-criminal-sentencing, 
May 2016. (Accessed on 06/18/2016). 

[3] Solon Barocas and Andrew D Selbst. Big data’s dis- 
parate impact. Available at SSRN 2477899, 2014. 

[4] Clark W. Barrett, Roberto Sebastiani, Sanjit A. Seshia, 
and Cesare Tinelli. Satisfiability modulo theories. In 
Handbook of Satisfiability, page 825–885. 2009. 

[5] Nate Berg. Predicting crime, lapd-style. 
https://www.theguardian.com/cities/2014/jun/ 
25/predicting-crime-lapd-los-angeles-police- 
data-analysis-algorithm-minority-report, June 
2014. (Accessed on 06/18/2016). 

[6] Toon Calders and Sicco Verwer. Three naive bayes ap- 
proaches for discrimination-free classification. Data 
Mining and Knowledge Discovery, 21(2):277–292, 
2010. 

[7] Bob Carpenter, Andrew Gelman, Matt Hoffman, Daniel 
Lee, Ben Goodrich, Michael Betancourt, Marcus A 
Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 
Stan: a probabilistic program language. Journal 
of Statistical Software, 2015. 

[8] Mark Chavira and Adnan Darwiche. On probabilistic 
inference by weight model counting. Artificial Intel- 
ligence, 172(6):772–799, 2008. 

[9] Michael R Clarkson and Fred B Schneider. Hyper- 
properties. Journal of Computer Security, 18(6):1157– 
1210, 2010. 

[10] Amit Datta, Michael Carl Tschantz, and Anupam 
Datta. Automated experiment on ad privacy set- 
tings. Proceedings on Privacy Enhancing Technolo- 
gies, 2015(1):92–112, 2015. 

[11] Anupam Datta, Shayak Sen, and Yair Zick. Algorith- 
mic transparency via quantitative input influence. In 
Proceedings of 37th IEEE Symposium on Security and 
Privacy, 2016. 

[12] Leonardo De Moura and Nikolaj Bjørner. Z3: An 
efficient smt solver. In International conference on 
Tools and Algorithms for the Construction and Analysis 
of Systems, page 337–340. Springer, 2008. 

[13] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer 
Reingold, and Richard S. Zemel. Fairness through 
awareness. In Innovations in Theoretical Computer 
Science 2012, Cambridge, MA, USA, January 8-10, 
2012, page 214–226, 2012. 

[14] Martin E. Dyer and Alan M. Frieze. On the complex- 
ity of compute the volume of a polyhedron. SIAM 
Journal on Computing, 17(5):967–974, 1988. 

[15] Virginia Eubanks. The danger of let algorithm 
enforce policy. http://www.slate.com/articles/ 
technology/future_tense/2015/04/the_dangers_ 
of_letting_algorithms_enforce_policy.html, 
April 2015. (Accessed on 06/18/2016). 

[16] Michael Feldman, Sorelle A. Friedler, John Moeller, 
Carlos Scheidegger, and Suresh Venkatasubramanian. 
Certifying and remove disparate impact. In Proceed- 
ings of the 21th ACM SIGKDD International Confer- 
ence on Knowledge Discovery and Data Mining, Syd- 
ney, NSW, Australia, August 10-13, 2015, page 259– 
268, 2015. 

[17] B. Goodman and S. Flaxman. EU regulation on algo- 
rithmic decision-making and a “right to explanation”. 
ArXiv e-prints, June 2016. 

[18] Noah Goodman, Vikash Mansinghka, Daniel M Roy, 
Keith Bonawitz, and Joshua B Tenenbaum. Church: 
a language for generative models. arXiv preprint 
arXiv:1206.3255, 2012. 

[19] Andrew D Gordon, Thomas A Henzinger, Aditya V 
Nori, and Sriram K Rajamani. Probabilistic program- 
ming. In Proceedings of the on Future of Software En- 
gineering, page 167–181. ACM, 2014. 

[20] Leonid Khachiyan. Complexity of polytope volume 
computation. Springer, 1993. 

[21] Nicole Kobie. Who do you blame when an algorithm 
get you fired? http://www.wired.co.uk/article/ 
make-algorithms-accountable, January 2016. (Ac- 
cessed on 06/18/2016). 

[22] Claire Cain Miller. Can an algorithm hire good than 
a human? http://www.nytimes.com/2015/06/26/ 
upshot/can-an-algorithm-hire-better-than-a- 
human.html, June 2015. (Accessed on 06/18/2016). 

[23] Aditya V Nori, Chung-Kil Hur, Sriram K Rajamani, 
and Selva Samuel. R2: An efficient mcmc sampler 
for probabilistic programs. In AAAI, page 2476–2482, 
2014. 

[24] Executive Office of the President. Big data: Seiz- 
ing opportunities, preserve values. https: 
//www.whitehouse.gov/sites/default/files/ 

5 



docs/big_data_privacy_report_may_1_2014.pdf, 
May 2014. (Accessed on 06/18/2016). 

[25] Walt L Perry. Predictive policing: The role of crime 
forecasting in law enforcement operations. Rand Cor- 
poration, 2013. 

[26] Latanya Sweeney. Discrimination in online ad delivery. 
Queue, 11(3):10, 2013. 

[27] Andrew Tutt. An fda for algorithms. Available at SSRN 
2747994, 2016. 

[28] Jennifer Valentino-Devries, Jeremy Singer-Vine, and 
Ashkan Soltani. Websites vary prices, deal base on 
users’ information. http://www.wsj.com/articles/ 
SB10001424127887323777204578189391813881534, 
December 2012. (Accessed on 06/18/2016). 

[29] Richard S. Zemel, Yu Wu, Kevin Swersky, Toniann 
Pitassi, and Cynthia Dwork. Learning fair representa- 
tions. In Proceedings of the 30th International Confer- 
ence on Machine Learning, ICML 2013, Atlanta, GA, 
USA, 16-21 June 2013, page 325–333, 2013. 

6 


