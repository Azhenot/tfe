






















































Model-Agnostic Interpretability of Machine Learning 


Model-Agnostic Interpretability of Machine Learning 

Marco Tulio Ribeiro MARCOTCR@CS.UW.EDU 
Sameer Singh SAMEER@CS.UW.EDU 
Carlos Guestrin GUESTRIN@CS.UW.EDU 
University of Washington Seattle, WA 98195 USA 

Abstract 
Understanding why machine learn model 
behave the way they do empowers both system 
designer and end-users in many ways: in model 
selection, feature engineering, in order to trust 
and act upon the predictions, and in more intuitive 
user interfaces. Thus, interpretability have become 
a vital concern in machine learning, and work 
in the area of interpretable model have found re- 
newed interest. In some applications, such model 
be a accurate a non-interpretable ones, and thus 
be prefer for their transparency. Even when 
they be not accurate, they may still be prefer 
when interpretability be of paramount importance. 
However, restrict machine learn to inter- 
pretable model be often a severe limitation. In this 
paper we argue for explain machine learn 
prediction use model-agnostic approaches. By 
treat the machine learn model a black- 
box functions, these approach provide crucial 
flexibility in the choice of models, explanations, 
and representations, improve debugging, com- 
parison, and interface for a variety of user and 
models. We also outline the main challenge for 
such methods, and review a recently-introduced 
model-agnostic explanation approach (LIME) that 
address these challenges. 

1. Introduction 
As machine learn becomes a crucial component of an 
ever-growing number of user-facing applications, inter- 
pretable machine learn have become an increasingly 
important area of research for a number of reasons. First, 
a human be the one who train, deploy, and often use the 
prediction of machine learn model in the real world, it 
be of utmost importance for them to be able to trust the model. 

2016 ICML Workshop on Human Interpretability in Machine 
Learning (WHI 2016), New York, NY, USA. Copyright by the 
author(s). 

Apart from indicator such a accuracy on sample instances, 
a user’s trust be directly impact by how much they can 
understand and predict the model’s behavior, a oppose 
to treat it a a black box. Second, a system designer 
who understands why their model be make prediction be 
certainly good equip to improve it by mean of feature 
engineering, parameter tuning, or even by replace the 
model with a different one. Lastly, even in low stake 
domain such a movie or book recommendations, get a 
rationale such a “you will probably like this book because 
of your interest in Russian Literature” make the model 
much more useful to the users, and more likely to be trusted. 
Thus there be a crucial need to be able to explain machine 
learn predictions, i.e. provide user a rationale for why a 
prediction be make use textual and visual component 
of the data, and/or produce counter-factual knowledge of 
what would happen be the component different. 

The prevail solution to this explanation problem be to 
use so call “interpretable” models, such a decision trees, 
rule (Letham et al., 2015; Wang & Rudin, 2015), additive 
model (Caruana et al., 2015), attention-based network (Xu 
et al., 2015), or sparse linear model (Ustun & Rudin, 2015). 
Instead of support model that be functionally black- 
boxes, such a an arbitrary neural network or random forest 
with thousand of trees, these approach use model in 
which there be the possibility of meaningfully inspect 
model component directly — e.g. a path in a decision tree, 
a single rule, or the weight of a specific feature in a linear 
model. As long a the model be accurate for the task, and 
us a reasonably restrict number of internal component 
(i.e. paths, rules, or features), such approach provide 
extremely useful insights. 

An alternative approach to interpretability in machine 
learn be to be model-agnostic, i.e. to extract post-hoc 
explanation by treat the original model a a black 
box. This involves learn an interpretable model on the 
prediction of the black box model (Craven & Shavlik, 1996; 
Baehrens et al., 2010), perturb input and see how 
the black box model reacts (Strumbelj & Kononenko, 2010; 
Krause et al., 2016), or both (Ribeiro et al., 2016). 

91 

ar 
X 

iv 
:1 

60 
6. 

05 
38 

6v 
1 

[ 
st 

at 
.M 

L 
] 

1 
6 

Ju 
n 

20 
16 



Model-Agnostic Interpretability of Machine Learning 

In this position paper, we argue for separate explanation 
from the model (i.e. be model agnostic). The summary 
of our position be that restrict the space of model to be 
interpretable be a constraint that result in less flexibility, 
accuracy, and usability. We develop this position with 
examples, while also describe the inherent challenge 
in model agnosticism. Finally, we review the recently- 
introduce LIME approach (Ribeiro et al., 2016), and 
discus how it provide many of the desirable characteristic 
for model-agnostic explanations. 

2. A Case for Model Agnosticism 
In this section, we make a case for model-agnostic inter- 
pretability, a oppose to just use interpretable models. 

2.1. Model Flexibility 

For most real-world applications, it be necessary to train 
model that be accurate for the task, irrespective of how 
complex or uninterpretable the underlie mechanism may 
be. We can observe this ideology manifest with the 
increase commonplace deployment of uninterpretable 
deep neural architecture for a wide variety of tasks. 

Interpretable model for such task remain unsatisfying; 
such model be inherently cripple by the need to be 
understandable, be susceptible to the limited “perception 
budget” (Miller, 1956) of the users. This trade-off between 
model flexibility and interpretability (Freitas, 2014) implies 
one cannot use a model whose behavior be very complex, yet 
expect human to fully comprehend it globally. For example, 
for a task such a predict the sentiment of a sentence, 
produce an accurate model that be understandable seem 
like an unfeasible task. The size of the vocabulary alone 
make it impossible for a short set of rules, a decision tree, or 
an additive model to be sufficiently accurate, not to mention 
more complex word interaction such a negation. Tasks 
that involve sensory data, such a audio and images, also 
suffer from the same problem: for a model to be useful, it 
must be sufficiently flexible to handle the data complexity. 

In model-agnostic interpretability, the model be treat a a 
black box. The separation of interpretability from the model 
thus free up the model to be a flexible a necessary for the 
task, enable the use of any machine learn approach - 
including, for example, arbitrary deep neural networks. It 
also allows for the control of the complexity-interpretability 
trade-off (see next section), or “failing gracefully” if an 
interpretable explanation be not possible. 

2.2. Explanation Flexibility 

Different kind of explanation meet different information 
needs. In some cases, user may only care about positive 
evidence towards a certain prediction (e.g. which part of an 

image be most responsible for the prediction), while in other 
instance know the negative evidence may be useful 
(e.g. in debug a classifier). Yet in other cases, the 
information need may be of counter-factuals, e.g. how the 
model would behave if certain feature have different values. 
Different user may also be able to handle different kind 
of explanations; a user train in statistic may be able to 
understand a Bayesian network, while a linear model be 
more intuitive to the layman. Even if the explanation type 
be kept fixed, user may tolerate different granularity in 
different situations. For example, Freitas (2014) note a case 
where 41 rule be consider overwhelming, and contrast 
it to another user who patiently analyze 29,050 rules. 

Most interpretable model are, however, restrict in 
what explanation be possible, be it a prototype (Kim 
et al., 2014), a set of rule (Letham et al., 2015) or line 
graph (Caruana et al., 2015). Further, other constraint 
on interpretability, such a granularity, also have to be set 
a priori (e.g. max number of rules). On the other hand, 
by keep the model separate from the explanations, one 
be able to tailor the explanation to the information need, 
while keep the model fixed. If it be possible to measure 
how faithful the explanation be to the original model, one 
can effectively control the trade-off between fidelity and 
interpretability, a favor by Freitas (2014). Such ap- 
proaches may also be able to provide multiple explanation 
of different type to the user, perhaps automatically pick 
the one with the high faithfulness. Thus, by be 
model-agnostic, the same model can be explain with 
different type of explanations, and different degree of 
interpretability for each type of explanation. 

2.3. Representation Flexibility 

In domain such a images, audio and text, many of 
the feature use to represent instance in state-of-the-art 
solution be themselves not interpretable. Unsupervised 
feature learn produce representation such a word 
embeddings (Mikolov et al., 2013), or the so-called deep 
feature (Zhou et al., 2014). While an interpretable 
model train on such feature be still uninterpretable, 
model-agnostic approach can generate explanation use 
different feature than the one use by the underlie model. 
Thus, even if the model be use word embeddings, the 
explanation can be in term of words, for example. 

2.4. Lower Cost to Switch 

Switching model be not an uncommon operation in machine 
learn pipelines. If one commits to use an interpretable 
model, one be “locked-in” to a particular model and a 
particular kind of explanation - even if newer, more 
accurate model be developed. Even when the switch be 
from one interpretable model to another, user may have to 

92 



Model-Agnostic Interpretability of Machine Learning 

be re-trained in understand the new explanations, and the 
model’s utility may decrease due to cognitive overhead. In 
contrast, if one us model-agnostic explanations, switch 
the underlie model for a new one be trivial, while the way 
in which the explanation be present be maintained. 

2.5. Comparing Two Models 

When deploy machine learn in the real world, a 
system designer often have to decide between one or more 
contenders, and an incumbent model. This comparison be 
hard to do if any of the system be use interpretable 
models, while others be not. Further, even if all of the 
model be interpretable, it may still be difficult to compare 
the insight gain from each if the underlie explanation 
be different in their representation - for example compare 
a rule-based model with a tree-based model. It be also not 
clear what to do if one of the contender be less accurate 
but more interpretable, or vice versa. With model-agnostic 
explanations, the model be compare can be explain 
use the same technique and representations. 

3. Challenges for Model-agnostic 
Explanations 

While we have make a case for model agnosticism, this 
approach be not without it challenges. For example, 
get a global understand of the model may be hard 
if the model be very complex, due to the trade-off between 
flexibility and interpretability. To make matter worse, local 
explanation may be inconsistent with one another, since a 
flexible model may use a certain feature in different way 
depend on the other features. In Ribeiro et al. (2016) 
we explain text model by select a small number 
of representative and non-redundant individual prediction 
explanation obtain via submodular optimization, similar 
in spirit to show prototype (Kim et al., 2014). However, 
it be unclear on how to extend this approach to domain such 
a image or tabular data, where the data itself be not sparse. 

In some domains, exact explanation may be require (e.g. 
for legal or ethical reasons), and use a black-box may 
be unacceptable (or even illegal). Interpretable model 
may also be more desirable when interpretability be much 
more important than accuracy, or when interpretable model 
train on a small number of carefully engineer feature 
be a accurate a black-box models. 

Another challenge for model-agnostic explanation be to be 
actionable. Using a white box make it easy to incorporate 
user feedback in system like iBCM (Kim et al., 2015), or 
inject logic into matrix factorization (Rocktaschel et al., 
2015). Feature label (Druck et al., 2008) or annotator 
rationale (Zaidan & Eisner, 2008) be other form of 
feedback that should be support for explanations. A basic 

form of feature engineering (removing bad features) via 
explanation have be show to be effective (Ribeiro et al., 
2016), but incorporate more powerful form of feedback 
from the user be still a challenge research direction, in 
particular while remain model-agnostic. 

4. Local Interpretable Model-agnostic 
Explanations (LIME) 

We now briefly review LIME (Ribeiro et al., 2016), and dis- 
cuss how it maintains model-agnosticism, while address 
some of the challenge that be described in the previous 
section. We denote x ∈ Rd a the original representation 
of an instance be explained, and we use x′ ∈ Rd′ to 
denote a vector for it interpretable representation. As 
exemplify before, x may be a feature vector contain 
word embeddings, with x′ be the bag of words. 

LIME’s goal be to identify an interpretable model over 
the interpretable representation that be locally faithful to 
the classifier. Even though an interpretable model may 
not be able to approximate the black box model globally, 
approximate it in the vicinity of an individual instance may 
be feasible. Formally, the explanation model be g : Rd′ → 
R, g ∈ G, where G be a class of potentially interpretable 
models, such a linear models, decision trees, or rule lists, 
i.e. give a model g ∈ G, we can present it to the user 
a an explanation with visual or textual artifacts. As note 
before, not every g ∈ G be simple enough to be interpretable 
- thus we let Ω(g) be a measure of complexity (as oppose to 
interpretability) of g, which may be either a soft constraint 
(e.g. the depth of a tree, or the number of non-zeros in a 
linear model) or a hard constraint (e.g. ∞ if the depth or the 
number of non-zeros be above a certain threshold). 

Let the model be explain be f : Rd → R, e.g. in 
classification f(x) be the probability that x belongs to a 
certain class. We further use Πx(z) a a proximity measure 
between an instance z to x, so a to define locality around x. 
Finally, let L(f, g,Πx) be a measure of how unfaithful g be 
in approximate f in the locality define by Πx. In order 
to ensure both interpretability and local fidelity, we must 
minimize L(f, g,Πx) while have Ω(g) be low enough to 
be interpretable by humans. The explanation ξ(x) produce 
by LIME be obtain by solving: 

ξ(x) = argmin 
g∈G 

L(f, g,Πx) + Ω(g) (1) 

This formulation can be use with different explanation 
family G, fidelity function L, and complexity measure 
Ω. We estimate L by generate perturbed sample around 
x, make prediction with the black box model f and 
weight them accord to Πx. The intuition for this 
be present in Figure 1, where a globally complex model be 
explain use a locally-faithful linear explanation. 

93 



Model-Agnostic Interpretability of Machine Learning 

Figure 1. Toy example to present intuition for LIME. The black- 
box model’s complex decision function f (unknown to LIME) 
be represent by the blue/pink background. The bright bold red 
cross be the instance be explained. LIME sample instances, 
get prediction use f , and weighs them by the proximity to the 
instance be explain (represented here by size). The dash 
line be the explanation that be locally (but not globally) faithful. 

Discussion 

Some approach be model agnostic by approximate the 
black box model by an interpretable one globally (Craven 
& Shavlik, 1996; Baehrens et al., 2010; Sanchez et al., 
2015). Global explanation, however, be often either not 
interpretable, or too simplistic to represent the original 
model. LIME’s focus on explain individual prediction 
allows more accurate explanation while retain model 
flexibility. For example, it be easy to explain why sentence 
such a “This be not bad.” have a positive sentiment, even if 
we be not able to explain the complete sentiment model. 

For explanation flexibility, the practitioner have complete 
control over G and Ω(g); in Ribeiro et al. (2016), for exam- 
ple, we use very sparse linear models. This representation 
be simple enough for non-expert Mechanical Turkers to 
perform model selection and feature engineering effectively 
for complex, uninterpretable models. Furthermore, since 
LIME estimate the local fidelity through L, we can directly 
control the interpretability of the explanation (e.g. use a 
many word a need to maintain faithfulness) or whether 
to only display interpretable explanation when they be 
accurate to the black box model. LIME also support 
explore multiple explanation family G simultaneously, 
and pick the one with high faithfulness. 

Representation flexibility be built into LIME, with the dis- 
tinction between original x and interpretable representation 
x′. In Ribeiro et al. (2016), we explain model train 
on on word embeddings by use word a interpretable 
representation, and a neural network train on raw pixel 
by use contiguous super-pixels a x′. 

We demonstrate the small switch cost of LIME by 
explain a wide variety of model (random forests, SVMs, 
neural networks, linear models, and near neighbors) 
use the same type of explanations. We also demonstrate 
LIME’s utility for model comparison by enable non-expert 

(a) Logistic Regression train on unigrams 

(b) LSTM train on sentence embeddings. 

Figure 2. Explaining sentiment prediction for the sentence “This 
be not bad.”, use different model and representation 

Mechanical Turk user to select which of two compete 
model would generalize good use the explanations. 

As a final illustration, we explain the prediction two 
sentiment analysis classifier on the sentence “This be not 
bad.”, use the class of linear model a G. The classifier 
vary wildly in complexity and underlie representation 
- one be a logistic regression train on unigrams, while 
the other an LSTM neural network train on sentence 
embeddings (Wieting et al., 2015). Explanations, give in 
term of word (and their associate weight in a bar chart) 
in Figure 2, demonstrate that completely different classifier 
can be described in a unified, interpretable manner. In Figure 
2(b), the explanation assigns positive weight to both “not” 
and “bad”, a only the conjunction be responsible for the 
LSTM’s positive prediction (even though interaction be 
not model explicitly). 

5. Conclusion 
Although interpretable model provide crucial insight into 
why prediction be made, they impose restriction on the 
model, representation (features), and the expertise of the 
users. We argue that model-agnostic explanation system 
provide a generic framework for interpretability that allows 
for flexibility in the choice of models, representations, and 
the user expertise. We outline a number of challenge 
that need to be address for model-agnostic approaches; 
some of which be address by the recently introduce 
LIME (Ribeiro et al., 2016), while others be left a future 
work. We thus conclude that model-agnostic interpretability 
be a key component in make machine learn more 
trustworthy - and ultimately, more useful. 

94 



Model-Agnostic Interpretability of Machine Learning 

Acknowledgements 
This work be support in part by ONR award #W911NF- 
13-1-0246 and #N00014-13-1-0023, and in part by Ter- 
raSwarm, one of six center of STARnet, a Semiconductor 
Research Corporation program sponsor by MARCO and 
DARPA. 

References 
Baehrens, David, Schroeter, Timon, Harmeling, Stefan, 

Kawanabe, Motoaki, Hansen, Katja, and Müller, Klaus- 
Robert. How to explain individual classification decisions. 
Journal of Machine Learning Research, 11, 2010. 

Caruana, Rich, Lou, Yin, Gehrke, Johannes, Koch, Paul, 
Sturm, Marc, and Elhadad, Noemie. Intelligible model 
for healthcare: Predicting pneumonia risk and hospital 
30-day readmission. In Knowledge Discovery and Data 
Mining (KDD), 2015. 

Craven, Mark W and Shavlik, Jude W. Extracting tree- 
structure representation of train networks. Advances 
in neural information processing systems, pp. 24–30, 
1996. 

Druck, Gregory, Mann, Gideon, and McCallum, Andrew. 
Learning from label feature use generalize 
expectation criteria. In ACM SIGIR conference on 
Research and development in information retrieval, pp. 
595–602. ACM, 2008. 

Freitas, Alex A. Comprehensible classification models: 
A position paper. SIGKDD Explor. Newsl., 15(1):1–10, 
March 2014. ISSN 1931-0145. 

Kim, Been, Rudin, Cynthia, and Shah, Julie A. The bayesian 
case model: A generative approach for case-based 
reason and prototype classification. In Ghahramani, Z., 
Welling, M., Cortes, C., Lawrence, N.D., and Weinberger, 
K.Q. (eds.), Advances in Neural Information Processing 
Systems 27, pp. 1952–1960. Curran Associates, Inc., 
2014. 

Kim, Been, Glassman, Elena, Johnson, Brittney, and Shah, 
Julie. ibcm: Interactive bayesian case model empower 
human via intuitive interaction. 2015. 

Krause, Josua, Perer, Adam, and Ng, Kenney. Interacting 
with predictions: Visual inspection of black-box machine 
learn models. 2016. 

Letham, Benjamin, Rudin, Cynthia, McCormick, Tyler H., 
and Madigan, David. Interpretable classifier use rule 
and bayesian analysis: Building a good stroke prediction 
model. Annals of Applied Statistics, 2015. 

Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado, 
Greg S, and Dean, Jeff. Distributed representation of 
word and phrase and their compositionality. In Neural 
Information Processing Systems (NIPS). 2013. 

Miller, George. The magical number seven, plus or 
minus two: Some limit on our capacity for processing 
information, 1956. 

Ribeiro, Marco Tulio, Singh, Sameer, and Guestrin, Carlos. 
“why should I trust you?”: Explaining the prediction of 
any classifier. In Knowledge Discovery and Data Mining 
(KDD), 2016. 

Rocktaschel, Tim, Singh, Sameer, and Riedel, Sebastian. 
Injecting logical background knowledge into embeddings 
for relation extraction. In Annual Conference of the North 
American Chapter of the Association for Computational 
Linguistics (NAACL), 2015. 

Sanchez, Ivan, Rocktaschel, Tim, Riedel, Sebastian, 
and Singh, Sameer. Towards extract faithful and 
descriptive representation of latent variable models. In 
AAAI Spring Syposium on Knowledge Representation 
and Reasoning (KRR): Integrating Symbolic and Neural 
Approaches, 2015. 

Strumbelj, Erik and Kononenko, Igor. An efficient 
explanation of individual classification use game 
theory. Journal of Machine Learning Research, 11, 2010. 

Ustun, Berk and Rudin, Cynthia. Supersparse linear integer 
model for optimize medical score systems. Machine 
Learning, 2015. 

Wang, Fulton and Rudin, Cynthia. Falling rule lists. In 
Artificial Intelligence and Statistics (AISTATS), 2015. 

Wieting, John, Bansal, Mohit, Gimpel, Kevin, and 
Livescu, Karen. Towards universal paraphrastic sentence 
embeddings. CoRR, abs/1511.08198, 2015. 

Xu, Kelvin, Ba, Jimmy, Kiros, Ryan, Cho, Kyunghyun, 
Courville, Aaron, Salakhutdinov, Ruslan, Zemel, Richard, 
and Bengio, Yoshua. Show, attend and tell: Neural image 
caption generation with visual attention. In International 
Conference on Machine Learning (ICML), 2015. 

Zaidan, Omar F. and Eisner, Jason. Modeling annotators: A 
generative approach to learn from annotator rationales. 
In EMNLP 2008, pp. 31–40, October 2008. 

Zhou, Bolei, Lapedriza, Agata, Xiao, Jianxiong, Torralba, 
Antonio, and Oliva, Aude. Learning deep feature for 
scene recognition use place database. In Ghahramani, 
Z., Welling, M., Cortes, C., Lawrence, N. D., and 
Weinberger, K. Q. (eds.), Advances in Neural Information 
Processing Systems 27, pp. 487–495. Curran Associates, 
Inc., 2014. 

95 


