


















































Controlling False Discoveries During 
Interactive Data Exploration 

Zheguang Zhao Lorenzo De Stefani Emanuel Zgraggen Carsten Binnig 
Eli Upfal Tim Kraska 

Department of Computer Science, Brown University 
{firstname_lastname}@brown.edu 

ABSTRACT 
Recent tool for interactive data exploration significantly increase 

the chance that user make false discoveries. The crux be that these 
tool implicitly allow the user to test a large body of different hy- 
potheses with just a few click thus incur in the issue commonly 
know in statistic a the “multiple hypothesis test error”. In 
this paper, we propose solution to integrate multiple hypothesis 
test control into interactive data exploration tools. A key insight 
be that exist method for control the false discovery rate (such 
a FDR) be not directly applicable for interactive data exploration. 
We therefore discus a set of new control procedure that be good 
suit and integrate them in our system call AWARE. By mean 
of extensive experiment use both real-world and synthetic data 
set we demonstrate how AWARE can help expert and novice user 
alike to efficiently control false discoveries. 

1 Introduction 
“Beer be good for you: study find that suds contain anti-viral 

powers” [DailyNews 10/12]. “Secret to Winning a Nobel Prize? Eat 
More Chocolate” [Time, 10/12]. “Scientists find the secret of longer 
life for men (the bad news: Castration be the key)” [Daily Mail UK, 
09/12]. “A new study show that drinking a glass of wine be just a 
good a spending an hour at the gym” [Fox News, 02/15]. 

There have be an explosion of data-driven discovery like the 
one mention above. While several of these be legitimate, there 
be an increase concern that a large amount of current publish 
research finding be false [19]. The reason behind this trend be 
manifold. 

In this paper we make the case that the rise of interactive data 
exploration (IDE) tool have the potential to worsen this situation 
further. Commercial tool like Tableau or research prototype like 
Vizdom [9], Dice [22] or imMens [26], aim to enable domain ex- 
perts and novice user alike to discover complex correlation and to 
test hypothesis and difference between various population in an 
entirely visual manner with just a few clicks; unfortunately, often ig- 
noring even the most basic statistical rules. We recently perform a 
small user study and ask people to explore census data use such 
an interactive data exploration tool. Within minutes, all participant 
be able to extract multiple insights, such a “people with a Ph.D. 
earn more than people with a low educational degree”. At the 
same time, almost none of the participant use a statistical method 
to test whether the difference the visually observe visually from 
the histogram be actually meaningful. Further, most user include 
expert with statistical training, do not consider that this type of ex- 
ploration, that consists of repeat attempt to find interest facts, 
increase the chance to observe seemingly significant correlation 
by chance. 

This problem be well know in the statistic community and 
refer to a the “multiple test problem” or “multiple hypothesis 

error” and it denotes the fact the more test an analyst performs, the 
high be the chance that a discovery be observe by chance. Let u 
assume an analyst test 100 potential correlations, 10 of them be 
true, and she want to limit the chance of a false discovery to 5% 
(i.e., the family-wise error rate should be p = 0.05). Assume further 
that our test have a statistical power (i.e, the likelihood to discover 
a real correlation) of 0.8; all very common value for a statistical 
testing. With this setting, the user will find ≈ 13 correlation of 
which 5 (≈ 40%) be “bogus”. The analyst should use a multiple 
hypothesis test correction method, such a the Bonferroni correction 
[6]. However, Bonferroni correction significantly decrease the 
power of every test and with it the chance of find a true insight. 
This be especially true in the case of interactive data exploration, 
where the number of test be not know upfront and incremental 
version of Bonferroni correction need to be apply which would 
even further decrease the power of the tests. 

Another interest question concern what should be consider 
a a hypothesis test when user interactively explore data. For 
example, if a user see a visualization, which show no difference 
in salary between men and woman base on their education, but 
late on decides base on that insight to look at salary difference 
between married men and women. Should we still account for that? 
The answer in most case will be “yes” a the analyst probably 
implicitly make a conclusion base on that visualization, which then 
in turn trigger her next exploration step. However, if she considers 
this visualization just a a descriptive statistic of how the data look 
like, and make no inference base on it (i.e. it do not influence 
the decision process of what to look at next), then it should not be 
consider a a hypothesis. The difference be subtle and usually 
very hard to understand for non-expert users, while it might have a 
profound impact on the number of false discovery a user makes. 

Finally, in the context of data exploration there have be recent 
work on automatically recommend visualization [36, 23, 37] 
or correlation [8]. These system yet again increase the chance 
of false discovery since they automatically test all (or at least a 
large fraction) of possible combination of feature until something 
interest show up without consider the multiple hypothesis 
test problem. 

In this paper, we make a first step towards integrate automatic 
multiple hypothesis test control into an interactive data explo- 
ration tool. We propose a potential user interface and a meaningful 
default hypothesis (i.e., the null hypothesis), which allows u to 
achieve control of the ratio of false discovery for every user inter- 
action. Specifically, we propose to consider every visualization a a 
hypothesis unless the user specifies otherwise. We further discus 
control procedure base on the family-wise error and discus why 
they be too pessimistic for interactive data exploration tool and 
why the more modern criterion of control the false discovery rate 

ar 
X 

iv 
:1 

61 
2. 

01 
04 

0v 
1 

[ 
c 

.D 
B 

] 
4 

D 
ec 

2 
01 

6 



(FDR) be good suit for large scale data exploration. The chal- 
lenge of FDR, however, is, that the standard techniques, such a the 
Benjamini-Hochberg procedure be not incremental and require to 
test all the hypotheses, before determine which hypothesis be 
rejected. This clearly constitutes a problem in the data exploration 
set where user make discovery incrementally. The recent 
α-investing technique [14] proposes an incremental procedure to 
control a variation of FDR, call marginal FDR (mFDR), which 
however relies on the user have a deep understand of how 
valuable each individual test be suppose to be. Again a contradic- 
tion to data exploration, where the user only over time gain a feel 
about the importance of certain questions. We therefore propose 
new strategy base on the α-investing procedure [14], which be 
particular tailor towards interactive data exploration tools. We 
implement these idea in a system call AWARE and we show how 
this system can help expert and novice user alike to control false 
discovery through extensive experiment on both real-world and 
synthetic data and workloads. 

The main contribution can be summarize a follows: 
• We propose AWARE, a novel system which automatically 

track hypothesis during data exploration; 
• We discus several multiple hypothesis test control meth- 

od and how well they work for data exploration; 
• Based on the previous discussion, we develop newα-investing 

rule to control a variant of the false discovery rate (FDR), 
call marginal FDR (mFDR); 
• We evaluate our system use synthetic and real-world datasets 

and show that our method indeed achieve control of the 
number of false discovery when use an interactive data 
exploration system. 

The paper be structure a follows: in Section 2 we discuss, by 
mean of an example, why some visualization should be consider 
hypothesis test and what be the main challenge encounter when 
test hypothesis for the IDE setting. In Section 3 we present 
AWARE’s user interface and discus how to automatically track 
hypothesis and how to integrate the user feedback into track 
the hypothesis. In Section 4 we discus multiple hypothesis test 
technique know in literature and show how well they fit in the 
IDE setting. In Section 5 we then propose new multiple hypothesis 
test procedure for IDE base on the α-investing procedure. 
Afterwards, in Section 7 we present the result of our experimental 
evaluation use both real-world and synthetic data. Finally, in 
Section 8 and 9 we discus related work and present our conclusions. 

2 A Motivational Example 
To motivate the various aspect for multi-hypothesis control dur- 

ing data exploration we outline a potential scenario that be inspire 
by Vizdom [9]. Similar workflow however can be achieve with 
other system like Tableau, imMens [26] or Dice [22]. 

Let u assume that Eve be a researcher at a non-profit organiza- 
tion and be work on a project relevant to a specific country. She 
just obtain a new dataset contain census information and be 
now interested in get an overview of this data a well a ex- 
tracting new insights. She start off by consider the “gender” 
attribute and observes that the dataset contains the same number 
of record for men and woman (Figure 1 A). She then move to 
a second visualization, display the distribution of people who 
earn above or below $50k a year. Eve link the two chart so that 
selection in the “salary” visualization now filter the “gender” vi- 
sualization. She notice that by select the salary above $50k, 
the distribution of “gender” be skewed towards men, suggest that 
men have high salary than woman (B). After create a third 
visualization for “gender”, select the record correspond to 
record with salary low than $50k (dashed line indicates inversion 

of selection), she confirms her find “Women in this country be 
predominately earn less than $50k” (C). Eve now want to un- 
derstand what else influence a person yearly salary and creates a 
chain of visualization that selects people who have PhD degree 
and be not married (D). Extending this chain use the “salary” 
attribute appear to suggest that this sub-population contains a lot of 
high-earners (E). By select the high-earners and extend the 
chain with two “age” visualization allows her to compare the age 
distribution of unmarried PhDs earn more than $50k to those 
make less than $50k. In order to verify that the observe visual 
difference be actually statistically significant she performs a t-test by 
drag the two chart close to each other (F). 

While the example workflow contains only one hypothesis test 
explicitly initiate by the user, we argue that without accounting for 
other implicit hypothesis test there be a significantly increase of risk 
that the user may observe false discovery during similar scenario 
of data exploration. This open up new important questions: why 
and when should visualization be consider statistical hypothesis 
tests? How should these test be formulated? 

2.1 Hypothesis Testing 
In this paper, we focus on the widely use frequentist inference 

approach and it p-value outcome. In order to determine whether 
there be a correlation between two observe phenomenon formalize 
in a “research hypothesis” H that be actually statistical relevant 
(i.e., not product of noise in the data) we analyze it correspond 
“null hypothesis” H which refers to a general statement or default 
position accord to which there be no relationship between two 
measure phenomena. Given this relationship between H and H , 
the research hypothesisH be also commonly refer a “alternative 
hypothesis”. 

The test procedure will then determine whether to accept 
(resp., reject) a null hypothesis H which in turn corresponds to 
reject (resp., accepting) the correspond alternative hypothesis 
(or research hypothesis)H. In order to do so the p-value of the null 
hypothesis H be evaluated. The p-value be use in the context of 
null hypothesis test in order to quantify the idea of statistical 
significance of evidence and it denotes the probability of obtain 
an outcome at least a extreme a the one that be actually observe 
in the data, under the assumption that H be true. Depending on 
the context, the p-value of H be evaluate use the appropriate 
statistical test (e.g., the t-test or the X 2-test). 

If the p-value p associate to the null hypothesis H be less than 
or equal to the significance level α chosen by the test procedure 
(commonly 0.05 or 0.01), the test suggests that the observe data be 
inconsistent with the null hypothesis, so the null hypothesis must 
be rejected.This procedure guarantee for a single test, that the 
probability of a “false discovery” (also know a “false positive” or 
“Type I error”) – wrongly reject the null hypothesis of no effect – 
be at most α. This do not imply that the alternative hypothesis be 
true; it just state that the observe data have the likelihood of p ≤ α 
under the assumption that the null hypothesis be true. The statistical 
power or sensitivity of a binary hypothesis test be the probability that 
the test correctly reject the null hypothesis H when the alternative 
hypothesisH be true. 

While the frequentist approach to hypothesis test have be crit- 
icized [20, 28] and there have be a lot of work in develop 
alternative approaches, such a Bayesian test [5], it be still widely 
use in practice and we consider it a good first choice to build a 
system which automatically control the multiple hypothesis error 
a they have two advantages: (1) Novice user be more likely to 
have experience with standard hypothesis test than the more de- 
manding Bayesian test paradigm. (2) The frequentist inference 
approach do not require to set a hard-to-determine prior a it be 



gender 

co 
un 

t 

Male Female Other 

A 

salary over 50k 

co 
un 

t 

True False 
gender 

co 
un 

t 

Male Female Other 

gender 

co 
un 

t 

Male Other 

salary over 50k 

co 
un 

t 

True False 

gender 

co 
un 

t 

Male Female Other 

B C 

education 

co 
un 

t 

HS Bachelor Master PhD 

marital status 

co 
un 

t 

Married Never 
Married 

Not 
Married 

Widowed 

Female 

salary over 50k 

co 
un 

t 
True False 

education 

co 
un 

t 

HS Bachelor Master PhD 

marital status 

co 
un 

t 

Married Never 
Married 

Not 
Married 

Widowed 

age 

co 
un 

t 

10 20 30 50 60 7040 9080 

age 

co 
un 

t 

10 20 30 50 60 7040 9080 

0.011p 

t-test 

D 

E F 

salary over 50k 

co 
un 

t 

True False 

education 

co 
un 

t 

HS Bachelor Master PhD 

marital status 

co 
un 

t 

Married Never 
Married 

Not 
Married 

Widowed 

Figure 1: An example Interactive Data Exploration Session 

the case with Bayesian tests. 

2.2 Visualizations a Hypotheses 
A visualization per-se show a descriptive statistic (e.g., the count 

of woman or the count of men) of the dataset and be not a hypothesis. 
It be reasonable to assume that in step A of Figure 1 the user just 
look at the gender distribution and simply acknowledges that the 
census survey roughly the same amount of woman and men. How- 
ever, it becomes an hypothesis test, if the user expect something 
else and draw a conclusion/inference base on the visualization. 
For example, if the user somehow assume that there should be 
more men than woman in the data and therefore consider the 
fact that there be an equal amount a an insight. The notion of a 
visualization be consider a a hypothesis becomes even clearer 
in step (B) and (C) of the example work-flow. When look at 
the visualization in (B) in isolation, it just depicts a descriptive 
statistic. Indeed, if the user would just take it a such and not make 
any inference about it and/or base further exploration on an insight 
extract from this visualisation, then it would not be consider an 
hypothesis. We argue however that the opposite be true more often 
than not. First, our analytical reason and sense-making process 
be inherently non-linear [29, 33]. Our future action be influence 
by new knowledge we discover in previous observations. Second, 
while susceptible to certain type of bias [11], the human visual 
system be highly optimize at pick up difference in visual sig- 
nals and at detect pattern [7]. An average user be very likely 
drawn to the change between the gender distribution of step (A) 
and step (B) and might therefore infer that woman earn less than 
men and potentially flag this a an interest insight that deserves 
more investigation. This be illustrate in step (C) where the user now 
further drill down and visually compare the distribution of gender 
filter by salary. We qualitatively confirm this notion through 
a formative user study where we manually cod user-reported in- 
sights, follow a think-aloud protocol similar to the one propose 
in [16]. In this study we observe that user tend to pick up on 
even slight difference in visualization and regard them a insight 
and user predominantly base future exploration path on previously 
infer insights. 

We conclude two things: (1) most of the time user indeed treat 
visualization a hypotheses, though there be exceptions, and (2) 
they often (wrongly) assume that what they see be statistical signifi- 
cant. The latter be particularly true if the user do not carefully check 
the axis on the actual count. For example, if a user start to analyze 
the outlier of a billion record dataset and make the conclusion 
that mainly uneducated white be cause the outliers, the dataset 

she be refer to might be comparable small and the chance of 
randomness might be much higher. The same argument also hold 
against the critic, that with enough data observe difference by 
chance be much less likely, which be true. As part of visual data 
exploration tools, user often explore sub-populations, and while the 
original dataset might be large, the sub-population might be small. 
Thus, we argue that every visualization a part of a interactive data 
exploration tool should be treat a a hypothesis and that user 
should be inform about the significance of the insight they gain 
from the visualization. At the same time, a user should have the 
choice to declare a visualization a just descriptive. 

2.3 Heuristics for Visualization Hypotheses 
A core question remains: what should the hypothesis for a visu- 

alization be. Ideally, user would tell the system every single time 
what they be think so that the hypothesis be adjust base on 
their assume insight(s) they gain from the visualization. However, 
this be disruptive to any interactive data exploration session. We 
rather argue that the system should use a good default hypothesis, the 
user can modify (or even delete) if she so desires. For the purpose of 
this work, we mainly focus on histogram a show in Figure 1 and 
acknowledge that there exist many other visualizations, which we 
consider a future work. We derive the follow heuristic from 
two separate user study where we observe over 50 participant 
use a IDE tool to explore various datasets. 

1. Every visualization without any filter condition be not a hy- 
pothesis (e.g., step A in Figure 1) unless the user make it 
one. This be reasonable, a user usually first gain a general 
high-level impression of the data. Furthermore, in order to 
make it an hypothesis, the user would need to provide some 
prior knowledge/expectation, for example a discuss before, 
that he expect more men than woman in the dataset. 

2. Every visualization with a filter condition be a hypothesis 
with the null-hypothesis that the filter condition make no 
difference compare to the distribution of the whole dataset. 
For example, in step B of Figure 1 the null hypothesis for the 
distribution of men vs. woman give the high salary class 
of over $50k would be that there be no difference compare 
to the equal distribution of men vs. woman over the entire 
dataset (the visualization in step A). This be again a reasonable 
assumption a the distribution of an attribute give others be 
only interesting, if it show some different effect compare to 
look at the whole dataset. 

3. If two visualization with the same but some negate filter 
condition be put next to each other, it be a test with the 



null-hypothesis that there be no difference between the two 
visualize distributions, which supersedes the previous hy- 
pothesis. This be the case in step C: give that the user look 
explicitly at the distribution of male v female give a salary 
over and under $50k be a strong hint from the user, that he 
want to compare these two distributions. 

As with every heuristic it be important to note, that the heuristic 
can be wrong. Therefore it be extremely important to allow the 
user to overwrite the default hypothesis a well a delete default 
hypothesis if one really just act a a descriptive statistic or be 
just generate a part to a big hypothesis test. Furthermore, there 
exist of course other potential null-hypothesis. For example, in 
our workflow we assume by default that the user aim to compare 
distributions, which require a χ2-test. However, maybe in some 
scenario compare the mean (i.e., a t-test) might be more appro- 
priate a the default test. Yet, study in detail what a good default 
null-hypothesis be dependent on the data property and domain, be 
beyond the scope of this paper. 

2.4 Heuristics Applied to the Example 
For our example in Figure 1 the result hypothesis could be 

a follows: Step A be not an hypothesis base on rule 1 a it just 
visualizes the distribution of a single attribute over the whole dataset. 
Step B be the hypothesis m1 if the distribution of gender be different 
give a salary over $50k. Step C supersedes the previous hypothesis 
and replaces it with an hypothesis m′1 if the gender distribution 
between a salary over and under $50k be different, which be a sightly 
different question. Step D creates a hypothesis m2 if the marital 
status for people with PhDs be different compare to the entire 
dataset, whereas step-E generates a hypothesis m3 if there be a 
different salary distribution give not married people with a PhD. 
By study the age distribution in step F the system first generate 
a default hypothesis m4 that the distribution of the age be different 
give a PhD and be not married for different salary classes. 
However, the user overwrites immediately the default hypothesis 
with an hypothesis m′4 about the average age. Furthermore, a 
the previous visualization in step D and E might just have be 
step stone towards create m4 the user might or might not 
delete hypothesis m2 and m3. However, if the insight our user 
gain from view the marital status, etc., influence her to look 
at the age distribution, she might want to keep them a hypothesis. 

Clearly this be only a very small example, but it already demon- 
strates the general issues. Not every insight the user gain (e.g., 
the insight that woman earn less) be explicitly express a a test. 
At the same time, a more the user “surfs” around the high the 
chance that she find something which look interesting, but just 
appear because of chance. In the example above, by the time the 
user actually performs it first test (step F), she implicitly already 
test at least one other hypothesis and potentially even four others. 
Assuming a target p-value of α = 0.05, the chance of a false 
discovery therefore increase to 1 − (1 − α)2 = 0.098 for two 
hypothesis and up to 1 − (1 − α)4 = 0.185 for four hypothesis. 
While the question of what should count a an hypothesis be highly 
dependent on the user and can never be fully control by any 
system, we can however, enable the system to make good sugges- 
tions and help user to track the risk of make false discovery by 
chance. Furthermore, this short workflow also demonstrates that 
hypothesis be built by add but also by remove attributes. As 
we will discus later, there exist no good method so far to control 
the risk of make false discovery for incremental session like the 
one create by interactive data exploration systems. We therefore 
develop new method especially for interactive data exploration in 
Section 5. 

Finally, it should be noted, that the same problem also exist 

with exploratory analysis use SQL or other tools. However, we 
argue that the situation be become bad by the up-rise of visual 
exploration tools, like Tableau, which be often use by novice users, 
who not necessarily reflect enough on their exploration path after 
they found something interesting. 

3 The AWARE User Interface 
As argue in the previous section, user feedback be essential in 

determining, track and control the right hypothesis during 
the data exploration process. With AWARE we create a system 
that applies our heuristic automatically to all visualizations. We 
design AWARE ’s user interface with a few goal in mind. 

First, the user should be able to see the hypothesis the system 
assume so far, their p-values , effect size and if they be consider 
significant and should be able to change, add or delete hypothesis 
at any give stage of the exploration. 

Second, hypothesis rejection decision should never change base 
on future user action unless the user explicitly asks for it. We 
therefore require an incremental procedure to control the multiple 
hypothesis risk that do not change it rejection decision even if 
more hypothesis test be executed. For example, the system should 
not state that their be a significant age difference for not married 
highly educate people, and then late on revoke it assessment just 
because the user do more tests. More formally, if the system de- 
termined which hypothesis m1...mn be significant (i.e., it reject 
the null) or not and the user change the last hypothesis or add an 
hypothesis mn+1, which should be the most common cases, the 
significance of hypothesis m1..mn should not change. However, 
if the user might change, delete, or add hypothesis k ∈ 1, .., n, de- 
pending on the use procedure we might allow that the significance 
of hypothesis mk+1 to mn might have to change a well. 

Third, individual hypothesis description should be augment 
with information about how much data nH1 the user have to add, 
under the assumption that the new data will follow the current 
observe distribution of the data, to make an hypothesis significant. 
While sound counter-intuitive, a one might (wrongly) imply, 
it be possible to make any hypothesis true by add more data, 
calculate this value be in some field already common practice. 
For example, in genetics scientist often search (automatically) for 
correlation between gene and high-level effect (like cancer). If 
such a correlation be found, often because of the multiple hypothesis 
error the chance of a true discovery be tiny (i.e., the p-value be too 
high). In that case the scientist work backwards and estimate 
how much more gene she have to to sequence in order to make 
the hypothesis relevant, expect that the new data (e.g., gene 
sequences) follow the same distribution of the data the scientist 
already has. However, if the effect be just produce by chance, 
the new data will be more similar to the distribution of the null- 
hypothesis and the null will not be rejected. The require value be 
generally easy to calculate or approximate, and be highly valuable 
for the end-user. A small value for nH1 in relation to the number 
of totally test hypothesis might be an indication that the power 
(i.e., the chance to accept a true alternative hypothesis) of the test 
be not sufficiently large. 

And finally, user should be able to bookmark important hypothe- 
ses. Our system us default hypothesis throughout the exploration 
and the user might find it too cumbersome to correct everyone for 
his real intentions, there might be more hypothesis generate than 
the user intend to test. Even if all hypothesis be what the user 
be considering, some of them might be more important to her than 
others; the hypothesis the user would like to include in a presen- 
tation or show to her boss. A key key question becomes, what be 
the expect number of false discovery among those important 



alpha 
2.5% 

5% 

salary | education <> salaryH1 
0.027 

t-test 
salary | education = salaryH0 

cohen's d 0.5 

H1 
0.001 

t-test H0 

cohen's d 0.8 
age | {chain} <> age | {chain-1} 

H1 
0.011 

t-test 

age | {chain} = age | {chain-1} 

H0 

cohen's d 0.5 

gender | marital = genderH1 
0.621 

chi square 
gender | marital = genderH0 

cohen's d 0.01 

A 
B 

C 
D 

E salary | {chain-1} <> salary 

salary | {chain-1} = salary 

Figure 2: The AWARE User Interface 

discoveries? 
Figure 2 show the current interface design of AWARE with a 

risk controller, which incorporates the above ideas, run on a 
tablet. The user interface feature an unbounded 2D canvas where 
chain of visualization (such a the one show in Figure 1) can be 
laid out in a free form fashion. A “risk-gauge” on the right-hand 
side of the display (Figure 2 (A)) serf two purposes: it give 
user a summary of the underlie procedure (e.g., the budget for 
the false discovery rate set to 5% with current remain wealth 
of 2.5%; both explain in the next two sections) and it provide 
access to a scrollable list of all the hypothesis test (implicit and 
explicit) that have be execute so far. Each list entry display 
detail about one test and it results. Textual label describe the 
null- and alternative-hypothesis and color cod p-values indicate 
if the null-hypothesis be reject or accepted (green for rejected, 
red for accepted). Furthermore, it visualizes the distribution of 
null-hypothesis and alternative hypothesis and show it difference, 
include an indication of it color cod effect size (D). Tap gesture 
on a specific item allow user to change thing like the default 
hypothesis or the type of test. Additionally other information such 
a an estimation of the size of an additional data nH1 that could 
make the observation significant can be displayed in each item. 
In the example this information be encode through a set of small 
square (B, C) where each square indicates the amount of data that 
be in the correspond distribution. In (B) the five red square tell 
u that we need 5x the amount of data from the null-distribution to 
flip this test form reject to accepted or conversely in (C) 11.5x 
the amount of data from the alternative-distribution to reject this 
hypothesis. Finally, we allow to mark important hypothesis by 
tap the “star” icon (E). 

4 Background on Multiple Hypothesis Error 
The previous section described how we convey the multiple hy- 

pothesis error to the user and ask for user feedback to derive the 
right hypothesis. In this section we describe different alternative to 
calculate the potential false discovery error and discus they appro- 
priateness for the IDE setting. The notation use in the rest of the 
paper be summarize in Appendix A. 

We consider a setting, in which we evaluate the statistical rel- 
evance of hypothesis from a set H = H1,H2, . . . ,Hm, create 
incrementally by an IDE system in a stream fashion. In order 
to verify whether any such hypothesis Hj be in fact statistically 
relevant we consider it correspond null hypothesis Hj . Using 
the appropriate statistical test (e.g., the t-test or the X 2-test) the 
p-value of Hj evaluate and base on it the test procedure deter- 
mine whether to accept (resp., reject) a null hypothesis Hj which 

in turn corresponds to reject (resp., accepting) the correspond 
alternative hypothesis (or research hypothesis)H|. The hypothesis 
accord to which all null hypothesis be true be refer a the 
“complete” or “global” null hypothesis. 

The set of null hypothesis reject by a statistical test be call 
“discoveries”and be denote a R. Among these we distinguish the 
set of true discovery S, and the set of false discovery or false 
positive V ; i.e., |V |+|S|= |R| False discovery be commonly 
refer also a Type 1 errors. Null hypothesis in S be false null 
hypotheses, while null hypothesis in V be true null hypotheses. 

4.1 Hold-Out Dataset 
A possible method to deal with the multiple hypothesis error be to 

split the datasetD into a explorationD1 and a validationD2 dataset 
[38]. D1 be then use for the data exploration process, whereas the 
validation dataset be use to re-test all hypothesis in order to validate 
the result of the first phase. In the follow we will provide some 
example which will clarify how, albeit useful, a hold-out dataset 
do not solve the multiple hypothesis test problem. 

Let u consider a null hypothesis H , and let pD denote it associ- 
ated p-value when H be evaluate with respect of the entire dataset 
D. Lets assume we perform a test with significance-level α. In this 
case the probability of wrongly reject H be at most α Suppose 
now that we randomly split the dataset into two datasets D1 and D2. 
For the same null hypothesis H we evaluate the p-values pD1 and 
pD2 each obtain by evaluate H on D1 or D2 respectively. We 
then run a a test with significance-level α (like the one discuss 
above) for each of the datasets. We then decide to reject H if it 
have be reject by both the test procedure operating on the 
datasetsD1 orD2. If both procedure operating onD1 andD2 have 
significance-level α, then the probability that the overall procedure 
end up reject H be at most α2. 

For the common value of α = 0.05, the chance of a Type I error 
be thus reduce to 0.0025, which be good news. Rather than fully 
handle the multiple hypothesis problem, what we have achieve 
trough this procedure be however just the lower of the threshold 
for reject the null hypothesis (i.e., the significance level of the 
test). 

This fact appear clearly in the follow scenario. Suppose that 
the user want to evaluate multiple hypothesis (e.g., 25) rather than 
just one. Assuming that these hypotheses, and their p-values be 
independent, the probability of observe at least one erroneous 
rejection use the test technique base on the use of the holdout 
dataset would be: pf = 1 − (1 − pD)25 ≈ 0.06, which be high 
than the desire α significance level. 

Albeit the lower of the achieve reduction of the significance 
level be indeed useful for reduce the chance of Type I errors, it 
come at the cost of a significant reduction of the power of the 
test procedure. 

Let u consider the follow example scenario in which we aim 
to compare the mean M1 and M2 of two sample one drawn from 
a population with expect value µ1 = 0 and the other from a 
population with µ2 = 1, both have a standard deviation of σ = 4. 
In order to determinate weather the observe difference betweenM1 
anM2 be actually statistically significant, we test the null-hypothesis 
“there be no significant difference between µ1 and µ2” use the 
one-sided t-test and a sample compose by 500 record from each 
population. Given the property of the t-test (see [13]), the statistical 
power of our test would be 0.99, and the probability of erroneously 
accept the null hypothesis would be at most 0.01. 

Suppose now that we divide the dataset into a dataset for explo- 
ration and one for validation each compose by 250 records. The 
statistical power for each of the individual t-test execute on the 
two dataset be now lower to 0.87, due to the reduction of the data 



be used. Further, recall that the procedure base on the holdout 
set reject a null hypothesis only if say hypothesis be reject by 
both sub-tests. This implies that the actual overall power of the 
test procedure be 0.87 · 0.87 ≈ 0.76, which be significantly low 
than the 0.99 achieve by the test which us the entire data. 

In general, approach base on hold-out datasets be consider 
inferior compare to test over the entire dataset. In some sce- 
narios, like building machine learn models, hold-out datasets 
might even be the only possibility to test a model or tune parameters. 
In those cases, a hold-out approach (like k-fold cross-validation) 
should be consider a test and should be control for the multiple 
hypothesis error a recent work suggests [10, 24, 30]. 

It be however important to remark that in our work we aim to 
predict guarantee on the statistical significance of the statistical 
predictor which be instead not achievable use prediction-driven 
approach such a cross-validation. 

4.2 Family-Wise Error Rate (FWER) 
Traditionally, frequentist method for multiple comparison test- 

ing focus on correct for modest number of comparisons. A 
natural generalization of the significance level to multiple hypothe- 
si test be the Family Wise Error Rate, which be the probability of 
incur at least one Type I error in any of the individual tests. The 
FWER be the probability of make at least one type I error in the 
family: 

FWER = Pr(V ≥ 1) = 1− Pr(V = 0) (1) 

By assure that FWER ≤ α, that be the FWER be control 
at level α, we have that the probability of even one Type I error in 
evaluate a family of hypothesis be at most α. 

We say that a procedure control the FWER in the weak sense, 
if the FWER control at level α be guaranteed only when all null 
hypothesis be true (i.e. when the complete null hypothesis be true). 
We say that a procedure control the FWER in the strong sense, if the 
FWER control at level α be guaranteed for any configuration of true 
and non-true null hypothesis (including the global null hypothesis). 

Bonferroni Correction: The Bonferroni correction be the sim- 
plest statistical procedure for multiple hypothesis test [6]. Let 
α be the critical threshold for the test. The value of α be usually 
select at 0.01 or 0.05. 

Let pi the p-value statistic associate with the null hypothesis 
Hi. When test m distinct null hypothesis use the Bonferroni 
correction, a null hypothesis Hi be reject if pi ≤ α/m. The 
Bonferroni procedure thus achieves control of the FWER at level α. 

Unfortunately, the Bonferroni correction can not be apply in our 
set a it require knowledge of the total number of hypothesis 
be considered. An alternative approach be to use a variation of the 
Bonferroni correction, accord to which the j-th null hypothesis 
Hj be reject if pj ≤ α · 2−j . It be possible to show that this 
procedure indeed control FWER at level α a j →∞ and do not 
need explicit knowledge of m. However the acceptance threshold 
decrease exponentially with respect to the number of hypotheses, 
thus result in a high number of false negatives. 

The main common issue with all FWER technique be that the 
power of the test significantly decrease a m increase due to 
the correspond decrease in the acceptance threshold (α/m in 
the original Bonferroni or α/2i in the sequential variant). While 
some alternative test procedure such a those of Vǐdák [34], 
Holm [18], Hochberg [17], and Simes [35] offer more power while 
control FWER, the achieve improvement be generally minor. 
A review of several of these technique be provide by Shaffer 
in [32]. 

4.3 False Discovery Rate (FDR) 
In [2] Benjamini and Hochberg propose the notion of False 

Discovery Rate (FDR) a a less conservative approach to control 

error in multiple test which achieve a substantial increase in the 
power of the test procedure. 

FDR-controlling procedure be design to control the expect 
ratio Q = V/R of false discovery among all discovery return 
by a procedure. In particular, the FDR of a statistical procedure be 
define as: 

FDR = E [Q] = E 

[ 
V 

R 
|R > 0 

] 
P (R > 0). (2) 

However, if we define FDR to be zero when R = 0, we can 
simplify 2 to: 

FDR = E 

[ 
V 

R 

] 
(3) 

We say that a test procedure control FDR at level α if we 
have FDR ≤ α. Designing a statistical test that control for FDR 
be not simple, a the FDR be a function of two random variable 
that depend both on the set of null hypothesis and the set of alter- 
native hypotheses. The standard technique to control the FDR be 
the Benjamini-Hochberg procedure(BH), which operates a follows: 
let p1 ≤ p2 ≤ . . . ≤ pm be the sort order of the the p-values 
for the m test null hypotheses. To control FDR at level α (for 
independent null p-values) determine the maximum k for which 
pk ≤ km · α, and reject the null hypothesis correspond to the 
p-values p1, p2, . . . , pk. 

Interestingly, under the complete null hypothesis, control the 
FDR at level α guarantee also “weak control” over the FWER 
FWER = P (V ≥ 1) = E 

( 
V 
R 

) 
= FDR ≤ α. This follow 

from the fact that the event of reject at least one true null hypoth- 
esis V ≥ 1 be exactly the event V/R = 1, and the event V = 0 be 
exactly the event V/R = 0 (recall V/R = 0 when V = R = 0). 
This make the FDR relatively easy to explain to the user a under 
complete random data, the chance of one or more false discovery 
be at most α a in FWER. However, FDR do not however ensure 
control of the FWER if there be some true discovery to be make 
(i.e., it do not ensure “strong control” of the FWER). 

Because of it increase power, FDR appear to be a good candi- 
date than FWER in the context interactive data exploration, where 
usually a large number of hypothesis be to be considered. Unfor- 
tunately, both the original Benjamini-Hochberg procedure and it 
variation for deal with dependent hypothesis [3] be not incre- 
mental a they require knowledge of the total number of hypothesis 
be test (similar to what be discuss for Bonferroni) and 
of the sort list of all the p-values correspond to each null 
hypothesis be evaluated. 

An adaptation of the FDR technique to a set for which an 
unspecified number of null hypothesis be observe incrementally 
be recently discuss in [15]. The main idea behind the Sequen- 
tial FDR procedure be to convert the arbitrary sequence of p-values 
correspond to the null hypothesis observe on the stream of hy- 
potheses into an order sequence akin to the one generate by the 
classical Benjamini-Hochberg procedure. The natural application 
for this technique be the progressive refinement of a model by consid- 
ering additional features. That is, it start construct a model for 
the data with something know and general. The user then proceeds 
to refine the model by determine the most significant features. 

One drawback of the Sequential FDR method, be give by the 
fact that the order accord to which the hypothesis be observe 
on the stream heavily influence the outcome of the procedure. For 
example, if an hypothesis with high p-value be observe among the 
first in the stream, this will harm the ability of the procedure of 
reject follow null hypotheses, even if they have low p-value 
(see discussion in [15]). This aspect make Sequential FDR not 
applicable for data exploration system for which the user be likely to 
explore different “avenues” of discovery rather than focus on the 
specialization of a model. 



4.4 Other Approaches 
Although for most practical applications, FDR control pro- 

cedures constitute the de facto standard for multiple hypothesis 
test [12], many other technique have be present in the litera- 
ture. Among them, Bayesian technique be particularly noteworthy. 
In [5], alternative solution to the multple hypothesis problem com- 
bining decision theory with Bayesian FDR be discussed. However, 
a often the case with Bayesian approaches, the computational cost 
for these procedure when apply to large datasets be significant, 
and the result be highly dependent on the prior model assumptions. 

Another approach be correct for the multiplicity through simu- 
lations (e.g., the permutation test [31]) that experimentally evaluate 
the probability of an observation in the null distribution. This ap- 
proach be also not practical in large datasets because of the large 
number of different possible observation and the need to evaluate 
very small p-values of each of these distribution [21]. 

In this paper, we elect to use a family of multiple hypothesis 
test procedure know a α-investing introduce in [14] and then 
generalize in [1]. These procedure be especially interest for 
the incremental and interactive nature of interactive data exploration. 
The detail of α-investing and it application to our set be exten- 
sively discuss in the next section. 

5 Interactive Control use α-Investing 
One drawback of the Sequential FDR procedure [15] a well a 

adaptation of FWER control technique to the stream set 
be give by the fact that decision regard the rejection or accep- 
tance of previously consider null hypothesis could potentially 
be overturn in latter stage due to new hypothesis be consid- 
ered. Although statistically sound, this fact could appear extremely 
counter intuitive and confuse to the user. The only way to adopt 
the Sequential FDR procedure to data exploration would be to batch 
all the hypothesis and only present the final decision afterwards. 
In that sense Sequential FDR be incremental but non-interactive in 
data exploration. 

In order to have both incremental and interactive multiple hy- 
pothesis error control, we consider a different approach for multiple 
hypothesis test base on the “α-investing ” test procedure 
introduce originally introduce by Foster and Stine in [14]. Simi- 
larly to Sequential-FDR , this procedure do not require explicit 
knowledge of the total number of hypothesis be test and can 
therefore be apply in the hypothesis stream setting. α-investing 
present however several crucial difference with respect to both 
traditional and sequential FDR control procedures. 

In the following, we first introduce the general outline of the 
procedure a present in [14] and then discus several invest 
strategy (called policies) that we have developed for interactive 
data exploration. 

5.1 Outline of the Procedure 
For α-investing , the quantity be control be not the clas- 

sic FDR but rather an alternative quantity call “marginal FDR 
(mFDR)”: 

mFDRη(j) = 
E [V (j)] 

E [R(j)] + η 
(4) 

where j denotes the total number of test which have be executed, 
while V (j) (resp., R(j)) denote the number of false (resp., total) 
discovery obtain use the α-investing procedure. 

In particular, we say that a test procedure controlsmFDRη at 
level α if mFDRη(j) ≤ α. The parameter η be introduce in order 
to weight the impact of case for which the number of discovery 
be limited. Common choice for η be 1, (1 − α), whereas the 
procedure appear to lose in power for value of η close to 0 [14]. 

Under the complete null hypothesis we have V (j) = R(j) hence 

mFDRη(j) ≤ α implies that E [V (j)] ≤ αη/(1− α). If we 
chose η = 1−α then E [V (j)] ≤ α, and we can thus conclude that 
control of the mFDR1−α at level α implies weak control fo the 
FWER at level α [14]. We refer the reader to the original paper of 
Foster and Stine [14] for an extensive discussion on the relationship 
between mFDR and the classic FDR. A generalization of the α- 
invest procedure be late introduce in [1]. The α-investing 
procedure do not in general require any assumption regard the 
independence of the hypothesis be tested, although opportune 
correction be necessary in order to deal with possible dependencies. 
In our analysis, we however assume that all the hypothesis and the 
correspond p-values be indeed independent. 

Intuitively the α-investing procedure work a follows: With 
every test j the user set an αj-value, which have to be below the 
current wealth, which be in the begin usually α · (1− α) before 
he performs the test. If the null-hypothesis be accepted (pj > αj) 
the invest alpha value be lost. To some degree this be similar 
to the Bonferroni-correction a one could consider the αj value 
everybody be compare to a α/m. So whenever a test be performed, 
the wealth decrease by α/m until the wealth be 0 and the user have 
to stop exploring. However, in contrast to the Bonferroni-correction, 
with α-investing the user can regain wealth through a reject null- 
hypothesis, which make the procedure truly incremental a it do 
no longer depend on the number of anticipate hypothesis m and 
also more powerful. 

More formally, we denote a W (0) the initial α-wealth assign 
to the test procedure. If the goal of the test procedure be to 
control mFDRη at level α, then we shall set W (0) = α · η. Here, 
η be commonly set to (1− α). We denote a W (j) the amount of 
“available α-wealth” after j test have be executed. 

Each time a null hypothesis Hj be be tested, it be assign 
a budget αj > 0. Let pj denote the p-value associate with the 
null hypothesis Hj . This hypothesis be reject if pj ≤ αj . If 
Hj be reject than the test procedure obtains a “return” on it 
investment ω ≤ α. Instead, if the null hypothesis Hj be accepted, 
αj/(1− αj) alpha wealth be deduct from the available α-wealth: 

W (t)−W (t− 1) = 
{ 
ω if pj ≤ αj , 
− 

αj 
1−αj 

if pj > αj 
(5) 

The test procedure halt when the available α-wealth reach 0. 
At that point in time, the user should stop explore to guarantee that 
mFDR ≤ α. Obviously again something, which be not desirable 
a it be hard to convey to any user, that he have to stop exploring. We 
will discus this problem and potential solution in Section 5.8. 

The budget αj which can be assign to test must be such that 
regardless of the outcome of the test, the availableα-wealth available 
after the test be not negative W (j) ≥ 0, hence αj ≤ W (j − 
1)/(1−W (j − 1)). Further we impose that αj < 1. While this 
constraint be not explicate in [14], it be indeed necessary for the 
correct function of the procedure. Setting αj = 1 would lead to 
the potential deduction of an infinite amount of α-wealth, violate 
the non negativity of W (j). Setting αj > 1 would instead lead to 
have a positive increase of the available α-wealth regardless of 
the outcome of the test. In our analysis we will however assume 
that all the hypothesis be consider be indeed independent and 
their associate p-values be independent a well. 

We refer a “α-investing rule” to the policy accord to which 
available budget have to be assign to the hypothesis that need to 
be tested. Furthermore, in [14] it be show that any α-investing 
policy for which W (0) = η · α, ω = α, and which obeys the rule 
in (5), control the mFDR at level α, for α, η ∈ [0, 1]. 

The freedom of assign to each hypothesis a specific level 
of confidence independent of the order, and the possibility of “re- 
investing” the wealth obtain by previous rejection constitute great 



advantage with respect to the Sequential FDR procedure. 

5.2 α-Investing for Data Exploration 
While it be relatively straightforward to devise invest rules, it be 

difficult a priori to determinate the “best way to invest” the available 
alpha-wealth. If αj be picked too small, the statistical power of 
every test be reduce and the chance be even high too loose the 
invest wealth give a true alternative hypothesis. If αj be too 
large, the entire α wealth might be quickly exhaust and the user 
(in theory) have to stop explore or re-evaluate all his test (see also 
Section 5.8). A policy be most likely to be successful if it can exploit 
some knowledge of the test setting. 

Another complication be the construction of test for which one 
can obtain the need p-values . To show that a test procedure 
control mFDR, we require that conditionally on the prior j - 1 
outcome (denoted a Ri), the level of the test of Hj must not 
exceed αj : 

P (Rj = 1|Rj1, Rj2, ..., R1) ≤ αj . (6) 

This do not however constitute a problem in our set a we 
be assume all hypothesis and their p-value to be independent. 

While [14] propose various invest rules, most of the propose 
procedure might test a hypothesis again and overturn an initial re- 
jection of a null-hypothesis. Therefore, in the remainder of this 
section we propose different α-investing policy particular for Inter- 
active Data Exploration, which correspond to different exploration 
strategy and at exploit different possible property of the data. 
However it should be noted, that our first procedure, β-farsighted, 
be a generalization of the “Best-foot-forward policy” in [14]. 

For this paper, we consider a set for which we observe a 
(potentially infinite) stream of null hypothesis for which at each of 
the discretized time step a new null hypothesis be observe on the 
stream. We denote a Hj the hypothesis be consider at the 
j-th step. We further assume that say hypothesis be independent. 

All our policy assign to each hypothesis a strictly positive budget 
αj > 0 a long a any α-wealth be available. If pj ≤ αj , the null 
hypothesis Hj be reject (i.e., it be consider a discovery). Vice 
versa, if pj > αj be accepted. The current α-wealth W (j) be 
then update accord to the rule in (5) and because of it control 
mFDR at level α a show in [14]. 

5.3 β-Farsighted Investing Rule 
Like with real investment, the question be if one should invest short 

or long-term. With β-farsighted we create a policy, which try to 
preserve wealth over long exploration sessions. Given β ∈ [0, 1), 
we say that a policy be β-farsighted if it ensures that regardless of the 
outcome of the j-th test at least a fraction β of the current α-wealth 
W (j − 1) be preserve for future tests, that be for j = 1, 2, . . .: 

W (j) ≥ βW (j − 1), 
W (j)−W (j − 1) ≥ (β − 1)W (j − 1) 

(7) 

We therefore define the β-farsighted procedure to controlmFDRη 
at level α in the procedure for Investing Rule 1. 

Investing Rule 1 β-farsighted 
1: W (0) = ηα 
2: for j = 1, 2, ... do 
3: αj = min 

( 
α, 

W (j−1)(1−β) 
1+W (j−1)(1−β) 

) 
4: if p(Hj) < αj then 
5: W (j) = W (j − 1) + ω 
6: else 
7: W (j) = W (j − 1)− 

αj 
1−αj 

= βW (j − 1) 
8: end if 
9: end for 

Different choice for the parameter β ∈ [0, 1) characterize how 
conservative the invest policy is. If there be high confidence on 

the first observe hypothesis be true discoveries, small value of 
beta (i.e., 0.25) would be more effective. Vice versa, high value of 
β (i.e. 0.9) ensure that even if the first hypothesis be true null, a 
large part of the α-wealth be preserved. 

We say that an α invest policy be “thrifty” if it never fully com- 
mit it available α-wealth. The described β-farsighted be indeed 
thrifty. While the procedure will never halt due to the available 
α-wealth reach zero, after a long series of acceptance of null 
hypothesis the available budget may be reduce so much that it will 
be effectively impossible to reject any more null hypotheses. 

Although these policy may appear wasteful a there be no re- 
ward for wealth which have not be invested, they be aim to 
preserve some of their current budget for future test in case the 
hypothesis consider in the begin of the test procedure be 
not particularly trustworthy. 

This invest rule be therefore particular suit for scenario be 
the total number of false discovery in long exploration sessions, 
potentially across multiple users, should be controlled. 

5.4 γ-Fixed Investing Rule 
A different non-thrifty procedure assigns to each hypothesis the 

same budget α∗. In particular, we call γ-fixed a procedure that 
assigns to each null hypothesis a fix budget αj equal to a fraction 
of the initial α-wealth W (0), that be α∗ =W (0)/(W (0) + γ), a 
long a any α-wealth be available. 

The detail of the γ-fixed procedure controllingmFDRη at level 
α can be found in the procedure for Investing Rule 2. 

Investing Rule 2 γ-fixed 
1: W (0) = ηα 
2: α∗ = W (0) 

γ+W (0) 

3: whileW (j − 1)− α∗ 
1−α∗ ≥ 0, for j = 1, 2, . . . do 

4: if p(Hj) < α∗ then 
5: W (j) = W (j − 1) + ω 
6: else 
7: W (j) = W (j − 1)− α∗ 

1−α∗ = W (j − 1)− 
W (0) 
γ 

8: end if 
9: end while 

Note that we define α∗ a W (0)/(γ +W (0)) to ensure that the 
subtraction of the wealth be constantly W (0)/γ. Different choice 
for the parameter γ characterize how conservative the invest 
policy is. If there be high confidence on the first observe hypothesis 
be actual discovery small value of γ (i.e. 5,10,20) would make 
more sense. Vice versa a high value of γ ensures that even if the first 
hypothesis be true null, a large part of the α wealth be preserved. 
Good choice for that set would be γ = 50, 100. 

5.5 δ-Hopeful Investing Rule 
In a slight variation of the γ-fixed invest rule, we say that 

a policy be δ-hopeful if the budget be assign to each hypothesis 
“hoping” that at least one of the next δ hypothesis will be rejected. 
Each time a null hypothesis be reject the budget obtain from 
the rejection be re-invested when assign budget over the next δ 
null hypotheses. γ-fixed and δ-hopeful operate by spread the 
amount of α-wealth over a fix number of hypothesis (either γ 
or δ), δ-hopeful be however “less conservative” than γ-fixed a it 
always operates by invest all currently available α-wealth over 
the next δ hypotheses. So it be a much more optimistic procedure, 
which work well if most alternative hypothesis be expect to 
accepted. The detail of the δ-fixed procedure control mFDRη 
at level α can be found in the procedure for Investing Rule 3. 

5.6 �-Hybrid Investing Rule 
Because α-investing allows contextual information to be incor- 

porated, the power of the result procedure be related to how well 



Investing Rule 3 δ-hopeful 
1: W (0) = ηα 
2: α∗ = W (0) 

δ+W (0) 

3: k∗ = 0 
4: whileW (j − 1)− α∗ 

1−α∗ ≥ 0, for j = 1, 2, . . . do 
5: if p(Hj) < α∗ then 
6: W (j) = W (j − 1) + ω 
7: α∗ = min 

( 
α, 

W (j) 
δ+W (j) 

) 
8: k∗ = j 
9: else 

10: W (j) = W (j − 1)− α∗ 
1−α∗ = W (j − 1)− 

W (k∗) 
α∗ 

11: end if 
12: end while 

the design heuristic fit the actual data exploration scenario. For 
example, when the data exhibit more randomness, the γ-fixed rule 
tends to have more power than the δ-hopeful rule. Intuitively, the 
α-wealth decrease when test a true null hypothesis, because the 
expectation of the change of wealth be negative when the p-value 
be uniformly distribute on [0, 1]. Thus the initial α-wealth be on 
average large than the α-wealth available at subsequent steps. Fur- 
thermore, since the γ-fixed rule invests a constant fraction of the 
initial wealth, the power tends to be large than δ-hopeful. 

On the contrary, when the data be less random, the γ-fixed rule 
becomes less powerful than δ-hopeful rule. The reason be that in 
this set more significant discovery tend to keep the subsequent 
α-wealth high, potentially even high than the initial wealth. We 
study this difference in more detail in Section 7. 

In order to have a robust performance in term of power and false 
discovery rate, we design �-hybrid invest rule that adjust the αj 
assign to the various test base on the estimate data randomness. 
Our estimation of the randomness of the data be base on the ratio of 
reject null hypothesis over a slide windowHd constitute by the 
last d null hypothesis observe on a stream. We then compare this 
ration with a “randomness threshold” � ∈ (0, 1) and we conclude 
whether the data exhibit high randomness or not. The procedure 
be outline in Investing Rule 4. 

Investing Rule 4 �-hybrid 
1: W (0) = ηα 
2: k∗ = 0 
3: Hd = [] // Sliding window of size d 
4: whileW (j − 1) > 0, for j = 1, 2, . . . do 
5: if Rejected(Hd)≤ |Hd|� then 
6: αj = 

W (0) 
γ+W (0) 

7: else 
8: αj = min 

( 
α, 

W (k∗) 
δ+W (k∗) 

) 
9: end if 

10: ifW (j − 1)− 
αj 

1−αj 
≥ 0 then 

11: if p(Hj) < αj then 
12: W (j) = W (j − 1) + ω 
13: k∗ = j 
14: Hd[j] = Rj = 1 
15: else 
16: W (j) = W (j − 1)− 

αj 
1−αj 

17: Hd[j] = Rj = 0 
18: end if 
19: end if 
20: end while 

5.7 Investment base on Support Population 
In this section we discus how to adjust the budget of each hy- 

pothesis accord to the amount of data which be available in order 
to compute the p-value of that same hypothesis. The main intuition 
for this procedure be that, a it be most likely to observe high p-values 
for hypothesis which rely on a small number of data points, we 

should should not invest a much α-wealth on those hypotheses. In 
this section we discus how to bias the amount budget assign to 
each hypothesis so that hypothesis with more support data receive 
more “trust” (in term of budget) from the procedure. 

Let u denote a |n| the total amount of data be use and 
by |j| the available data for test the j-th null hypothesis Ht. 
A simple way of correct the assignment of the budget αj in 
any of the previously mention hypothesis be to assign to the test 
of the hypothesis αjf( |j||n| ). Depending on the choice of f(·) the 
impact of the correction may be more or less severe. Some possible 

choice for f(·) would be f( |t||n| ) = 
( 
|t| 
|n| 

)ψ 
for possible value of 

ψ = 1, 2/3, 1/2, 1/3, . . .. We present an example policy base on 
the γ-fixed rule, the ψ-support rule in Investing Rule 5. 

Investing Rule 5 ψ-support 
1: W (0) = ηα 
2: α∗ = W (0) 

γ+W (0) 

3: whileW (j − 1) > 0, for j = 1, 2, . . . do 

4: αj = α∗ 
( 
|t| 
|n| 

) 1 
2 

5: ifW (j − 1)− 
αj 

1−αj 
≥ 0 then 

6: if p(Hj) < αj then 
7: W (j) = W (j − 1) + ω 
8: else 
9: W (j) = W (j − 1)− 

αj 
1−αj 

10: end if 
11: end if 
12: end while 

5.8 What Happens If the Wealth be 0 
Among all our propose invest policies, only β-farsighted 

be “thrifty”,that it be never fully commits it available α-wealth. 
Still, the available wealth for β-farsighted could eventually become 
extremely small, to the point that no more hypothesis can be rejected. 
All the remain procedure be “non-thrifty” and can thus reach 
zero α-wealth, in which case the user (theoretically) should stop 
exploring. 

It be only natural to wonder if it would be possible for the user to 
somehow “recover” some of the lose α-wealth and thus continue 
the test procedure. One possible way to do so, would require 
the user to reconsider and possibly overturn some of the previous 
decision on whether to reject or accept some null hypothesis us- 
ing alternative test procedure (i.e., the Benjamini-Hochberg 
procedure). 

There be however several challenge to be face when pursue 
this strategy: 1) great care have to be put on haw to combine result 
from different test procedure (i.e., control of FDR for a subset 
of hypothesis and control of mFDR for a distinct subset of hypothe- 
ses) and 2) test hypothesis for a second time give the outcome 
of other test implies a clear (and strong) dependence between the 
outcome of the test and the p-value associate with the null hy- 
potheses be considered. Therefore, depend on the context 
such control could only be achieve give additional assumption 
about the level of control or would require add additional data or 
the use of a hold-out dataset. We aim to study this problem in detail 
a part of future work. 

6 The Most Important Discoveries 
In Section 3 we argued, that the user should be able to mark 

the important hypothesis (e.g., the one she want to include in a 
publication). This be particularly important a AWARE us default 
hypotheses, which the user might consider a less important. In the 
follow we show that if these “important discoveries” be select 
from all the discovery give by a test procedure that control 



(a) 75% Null: Avg. Disc (b) 75% Null: Avg. FDR (c) 75% Null: Avg. Power (d) 100% Null: Avg. Disc (e) 100% Null: Avg. FDR 

Figure 3: Exp.1a: Static Procedures on Synthetic Data 

FDR at level α independently of their p-values , then the FDR for 
the set of important discovery be control at level α a well. 

THEOREM 1. Assume that we execute a collection of hypoth- 
esis test with a rejection rule that control the FDR at α. As- 
sume that the procedure reject the set of null hypothesis R = 
{R1, . . . , Rr}, and let V ⊆ V be the set of false discoveries. If the 
null hypothesis test be independent then for any subset R′ ⊆ R 
we have E[|V ∩R′|/||R′] ≤ α. 

PROOF. Let p1, . . . , p|R| be the p-values of the reject hypothe- 
ses. Since the rejection rule control the FDR at α we have 

|R|∑ 
i=1 

i 

|R| 
P (|V |= i | P1 = p1, . . . , Pr = pr) = α (8) 

Assume that |V |= i. A priori, the p-values of null hypothe- 
s be i.i.d. uniformly distribute in [0, 1] []. Subject to P1 = 
p1, . . . , Pr = pr , the set of the i null hypotheses’ p-values be uni- 
formly distribute among all the i subset of the r value {p1, . . . , pr}. 
Let p′1, . . . , p′|R′| be the p-values of the set of hypothesis R 

′, and 
let pVi , . . . p 

V 
|V | be the p-values of the reject null hypotheses, then 

E[|V ∩ R′| | |V |= i] = 

E[|{p′1, . . . , p 
′ 
|R′|} ∩ {p 

V 
1 . . . P 

V 
|V |}| | |V |= i] = i 

|R′| 
|R| 

. 
(9) 

Combining equation (8) and (9) we get: E 
[ |V ∩ R′| 
|R′| 

] 
= 

|R|∑ 
i=1 

E 

[ |V ∩ R′| 
|R′| 

| |V |= i 
] 
P (|V |= i | P1 = p1, . . . , Pr = pr) 

= 

|R|∑ 
i=1 

1 

|R′| 
i 
|R′| 
|R| 

P (|V |= i | P1 = p1, . . . , Pr = pr) = α 

(10) 

Consider a set R′ of important discovery select independently 
of the p-values of the correspond null-hypothesis from a large 
set of discovery R for which then mFDR be control at level 
α. Using a proof similar to the one discuss in Theorem 1 it be 
possible to show that the mFDR of R′ be control at level α a 
well. This be an important result, a it implies that the user can select 
the important discovery from a large pool of discovery while 
maintain the control of FDR (or mFDR) at level α. 

7 Experimental Evaluation 
In this section, we evaluate the α-investing rule in different data 

exploration setting to answer the follow questions: 
1. How do our α-investing rule compare to Sequential FDR? 
2. What be the average power (the proportion of truly significant 

discovery that be correctly identified)? 
3. What be the average false discovery rate? 
Workload/Data: We first conduct the simulation analysis on 

synthetic data, and then run user-study workflow on a real-world 
dataset. The statistic community considers the simulation analysis 
on synthetic data to be the statistically sound methodology to evalu- 
ate a multiple hypothesis test procedure (see for example [2, 4]), 

because on real-world datasets and workflow the proportion and 
signal-to-noise ratio of truly significant and insignificant hypothesis 
be hard to determine and control. 

Implementations and Setup: The procedure for all experi- 
ments are: (1) No multiple hypothesis control: Per-Comparison 
Error Rate (PCER) [4], (2) Static: Bonferroni Correction (Bonfer- 
roni) [6] and Benjamini-Hochberg (BHFDR) [4] (3) Incremental 
but non-interactive: Sequential FDR (SeqFDR) [15] (4) Incremental 
and interactive: α-investing rule of this paper. 

We modify our system to also execute static procedures. We 
emphasize that the static-versus-incremental comparison only serf 
a a reference a the static procedure be essentially not suitable for 
data exploration a discuss in Section 4. 

For all configurations, we set α to 0.05 and estimate the average 
false discoveries, the average FDR (i.e., the average of the ratio of 
the false discovery over all discoveries), and the average power 
and their correspond 95% confidence intervals. 

7.1 Exp.1a: Static Procedures 
In the first experiment we evaluate the static multiple hypothesis 

control procedure over synthetic data to motivate our choice of 
FDR (and similarly mFDR) over FWER and per-comparison error 
rate (PCER) (i.e. no multiple hypothesis control). 

We create a large simulation study similar to the one in [4] with 
m hypotheses, range from 4-64. Each hypothesis be compare 
the expectation of two independently distribute normal random 
variable of variance 1 but different expectation vary from 5/4 
to 5. The true null hypothesis be generate uniformly distribute 
across all test and the proportion of true null hypothesis be set 
to 75% and 100% (i.e., completely random data). We repeat the 
experiment 1,000 times. 

Figure 3 show the result for the static procedures, the Bonferroni- 
Correction (Bonferroni), the Benjamini-Hochberg procedure (BHFDR) 
and per-comparison error rate (PCER). For each procedure, we show 
the average number of discoveries, the average false discovery rate 
(FDR) and the average power. Note that the power be 0 for all 
procedure over completely random data and thus, not shown. 

We observe that PCER have the high power Figure 3(c), mean- 
ing that it can identify the high proportion of truly significant 
discoveries. However, PCER have also the high false discovery 
rate across all configuration (see (b) and (e)). On completely ran- 
dom data, PCER average 60% false discovery when test 64 
hypothesis in Figure 3(e). Therefore PCER be not the right control- 
ling target in multiple hypothesis test in data exploration. 

On the other hand, the Bonferroni procedure have the low av- 
erage false discovery rate (see (b) and (e)), but the number of dis- 
coveries be also the low and the power also degrades quickly with 
an increase number of hypotheses. For this reason, FWER be too 
pessimistic for data exploration. 

As a result, we advocate to use FDR (and similarly mFDR) a the 
control target for data exploration since we observe that the static 
FDR procedure, BHFDR, achieves a low average error rate than 



4 8 16 32 64 
number of hypothesis 

0 

5 

10 

15 

20 

25 

30 

35 

40 

(a) 25% Null: Avg. Discoveries 

4 8 16 32 64 
number of hypothesis 

0.00 

0.02 

0.04 

0.06 

0.08 

0.10 

(b) 25% Null: Avg. FDR 

4 8 16 32 64 
number of hypothesis 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

(c) 25% Null: Avg. Power 

4 8 16 32 64 
number of hypothesis 

0.00 

0.05 

0.10 

0.15 

0.20 

0.25 

(g) 100% Null: Avg. Discoveries 

4 8 16 32 64 
number of hypothesis 

0 

1 

2 

3 

4 

5 

6 

7 

8 

9 

(d) 75% Null: Avg. Discoveries 

4 8 16 32 64 
number of hypothesis 

0.00 

0.02 

0.04 

0.06 

0.08 

0.10 

(e) 75% Null: Avg. FDR 

4 8 16 32 64 
number of hypothesis 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

(f) 75% Null: Avg. Power 

4 8 16 32 64 
number of hypothesis 

0.00 

0.02 

0.04 

0.06 

0.08 

0.10 

(h) 100% Null: Avg. FDR 

Figure 4: Exp.1b: Incremental Procedures on Synthetic Data / Varying Number of Hypotheses 

10.0% 30.0% 50.0% 70.0% 90.0% 
sample size 

0 

10 

20 

30 

40 

50 

(a) 25% Null: Avg. Discov- 
erie 

10.0% 30.0% 50.0% 70.0% 90.0% 
sample size 

0.00 

0.02 

0.04 

0.06 

0.08 

0.10 

(b) 25% Null: Avg. FDR 
10.0% 30.0% 50.0% 70.0% 90.0% 

sample size 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

(c) 25% Null: Avg. Power 
10.0% 30.0% 50.0% 70.0% 90.0% 

sample size 

0 

2 

4 

6 

8 

10 

12 

14 

16 

(d) 75% Null: Avg. Discov- 
erie 

10.0% 30.0% 50.0% 70.0% 90.0% 
sample size 

0.00 

0.02 

0.04 

0.06 

0.08 

0.10 

(e) 75% Null: Avg. FDR 
10.0% 30.0% 50.0% 70.0% 90.0% 

sample size 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

(f) 75% Null: Avg. Power 

Figure 5: Exp.1c: Incremental Procedures on Synthetic Data / Varying Sample Size 

PCER and and high power than FWER. 

7.2 Exp.1b: Incremental Procedures 
As discuss before it be not feasible to use the static procedure 

for interactive data exploration where the number of hypothesis 
be neither know upfront nor the p-values can all be compute 
beforehand. For the remainder of the evaluation, we therefore focus 
on incremental procedures. 

Figure 4 us the same setup a in Section 7.1. The true null hy- 
potheses be generate uniformly distribute across all test and the 
proportion of true null hypothesis be set to 25%, 75% and 100% 
(i.e., completely random data). In this experiment, we compare the 
different α-investing rule we developed, namely, β-farsighted with 
β = 0.25, γ-fixed with γ = 10, δ-hopeful with δ = 10, �-hybrid 
with � = 0.5, and ψ-support, against the non-interactive Sequential- 
FDR (SeqFDR) procedure. The α for each procedure be set to 0.05 
and the �-hybrid us unlimited window size. The ψ-support rule 
be implement on top of γ-fixed. We pre-set the value base on 
rule-of-thumb judgement and do not further tune them. 

Figure 4(b)(e)(h) show that all procedure control the FDR at level 
α = 0.05, bar some variation in the realization of the average 
FDR between the procedure (here low be better). Sequential FDR 
have the high average FDR close to 0.05, whereas the α-investing 
procedure on average make less mistakes. Next, we study the 
difference in FDR and the power of the α-investing rules, give 
different context of data exploration. 
7.2.1 Varying Number of Hypotheses 

With β = 0.25, β-farsighted simulates a scenario in which the 
user be more confident or care more about early discovery be 
significant. In this setting, β-farsighted be expect to make less sig- 
nificant discovers in a long run if the dataset have more randomness. 
Figure 4(f) show that β-farsighted have very high power early on 
during the exploration, while it lower gradually a more hypothesis 

be made. On the other hand, if the dataset have less randomness, 
such a in the 25% Null configuration, β-farsighted be reward with 
the many discovery during the exploration, and thus maintain it 
power for a longer run. 
7.2.2 Varying Degree of Randomness 

Figure4f show that when the data have more randomness, the 
γ-fixed rule tends to be more powerful than δ-hopeful a the number 
of hypothesis increases. When the data have less randomness, the 
δ-hopeful rule becomes more powerful than γ-fixed rule. The reason 
be that the ω return from more frequent significant discovery tends 
to keep the α-wealth high, and since δ-hopeful invests a fraction of 
the α-wealth from the last reject hypothesis, α per test tends to 
be high and hence the increase of power. 

In light of this observation, we developed the previously men- 
tioned �-hybrid that estimate the randomness in the dataset base 
on the history of hypothesis test and pick between γ-fixed or δ- 
hopeful. Figure 4 show that �-hybrid procedure use � = 50% of 
past rejection a the randomness threshold achieves overall a more 
robust performance in term of power and FDR on vary degree of 
randomness than the aforementioned two procedure alone. When 
the dataset be completely random, our α-investing rule achieve 
similarly low false discovery rate a the Sequential FDR below 5%. 
This provide the simulation-based evidence that our α-investing 
rule correctly control the mFDR at α = 5%. 

Overall the result suggest that the performance of a give α- 
invest rule depends on how well it heuristic fit the context such 
a the importance of early discovery and the data randomness. β- 
farsighted be suitable when the early hypothesis be more important 
than the late ones; whereas �-hybrid strategy provide more robust 
performance across vary degree of randomness. 



10% 30% 50% 70% 90% 
sample size 

0 

20 

40 

60 

80 

100 

(a) Census: Avg. Disc (b) Census: Avg. FDR 

10% 30% 50% 70% 90% 
sample size 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

(c) Census: Avg. Power 

10% 30% 50% 70% 90% 
sample size 

0.00 

0.05 

0.10 

0.15 

0.20 

(d) Rand. Census: Avg. Disc 

10% 30% 50% 70% 90% 
sample size 

0.00 

0.02 

0.04 

0.06 

0.08 

0.10 

(e) Rand. Census: Avg. FDR 

Figure 6: Exp.2: Real Workflows on Census and Random Census Data 

7.2.3 Varying Support Size 
As part of interactive data exploration, the user usually applies 

various filter conditions, which change the support size for the 
different tests. To evaluate the impact of vary support sizes, 
we use the same setup a in Section 7.1, but fix the number of 
hypothesis to 64 and varied the sample size from 10-90%. The 
result be show in Figure 5. 

While again �-hybrid and ψ-support do well across all configu- 
rations, ψ-support achieves low average FDR especially for less 
random datasets (see Figure 5(b) and (e)). This be expect a the 
merit of the ψ-support rule be that it factor the support size of the 
hypothesis into the budget. Thus the rule tends to low the per-test 
significance level when a low test p-value be observe on data of 
suspiciously low support size. 

7.3 Exp.2: Real Workflows 
In this experiment we show the effectiveness of our propose 

procedure with real user workflow on the Census dataset [25]. We 
collect the workflow of 115 hypothesis base on a user study 
we performed. The hypothesis be mostly form by compare 
histogram distribution by different filter conditions, similar to 
the example from Section 2. We fix the order of the hypothesis 
throughout the experiment a many of the hypothesis may depend 
on each other. 

To determine ground truth, we run the Bonferroni procedure with 
the user workflow on the full-size Census dataset to label the signifi- 
cant observations. We then down-sample the full data repetition for 
additional uncertainty. Note that this evaluation method be a straw 
man a we do not know the actual truly significant observation on 
Census data. It be likely to be bias towards towards more conser- 
vative α-investing rule with more evenly distribute budgets, such 
a γ-fixed and ψ-support. 

Figure 6(a)-(c) show the result of the user workflow over the 
Census data. The γ-fixed and ψ-support rule perform good with 
average FDR significantly below α = 0.05, a show in Figure 6(b). 
For the other rules, the subtle side-effect of our label generation can 
be seen: the average false discovery rate for �-hybrid, β-farsighted 
and δ-hopeful slightly inflate a the sample size increases, and 
reach over α = 0.05 to 0.09 for 90% samples. The reason be two- 
fold: First, the mFDR a the ratio of expectation be not necessarily 
bound for only a particular fix set of workflows. Second, the 
Bonferroni procedure generates a ground truth with a bias towards 
conservative α-investing rule with more evenly distribute budgets. 
Hence the more optimistic α-investing rule tend to make more 
mistakes. This observation lead to interest insight about the 
conservativeness of different α-investing rules. 

To good demonstrate how our procedure control the false dis- 
covery rate, we therefore repeat the same experiment base on the 
real-world workflow but on randomize Census data. Figure 6(d) 
and (e) show the result (note that the power for all procedure be 
by definition zero a all discovery contribute to falsehood). We 
observe that the α-investing procedure remain comparable to the 
SeqFDR for high sample size in term of average FDR, although 

some variation exists such that some of the error rate have con- 
fidence interval over the range 0.05 to 0.10. We attribute this 
variation to the characteristic of our set of user-study workflows. 
For small sample sizes, we see high variations. We attribute this 
variation to the characteristic of our set of user-study workflows. 

8 Related Work 
There have be surprisingly little work in control the number 

of false discovery during data exploration even. This be especially 
astonish a the same type of false discovery can also happen with 
traditional analytical SQL-queries. To our knowledge this be one 
of the first work try to achieve a more automatic approach in 
track the user steps. 

Most related to this work be all the various statistical method for 
significance test and multiple hypothesis control. Early work 
try to improve the power of the Family Wide Error Rate use 
adaptive Bonferroni procedure such a Sǐdák [34], Holm [18], 
Hochberg [17], and Simes [35]. However, all these method lack 
power in large scale multi-comparison tests. 

The alternative False Discovery Rate measure be first propose 
by Benjamini and Hochberg [4], and soon become the statistical 
criterion of choice in the statical literature and in large scale data 
exploration analysis for genomic data [27]. The original FDR 
method decides which hypothesis to reject only after all hypothesis 
be tested. Data exploration motivate the study of more advance 
techniques, such a sequential FDR [15] and α-investing [14], that 
work in a scenario where hypothesis arrive sequentially and the 
procedure need to decide "on the fly" whether to accept or reject 
each of the hypothesis before test the next one, while maintain 
a bound on the FDR. Depending on the observe order of hypotheses, 
Sequential FDR can overturn previously accepted hypothesis into 
rejection base on the subsequent hypotheses. 
α-investing procedure also have revisit policy that can po- 

tentially overturn previous decisions. The implication be that these 
procedure be incremental but non-interactive, because they require 
observe all the hypothesis before finalize the decisions. How- 
ever, it be often infeasible to obtain all the possible hypothesis a 
priori. Therefore our work concern α-investing procedure with 
policy that be both incremental and interactive. In addition, none 
of the work address the issue on how to automatically integrate 
these technique a part of an data exploration tool. 

9 Conclusion and Future Work 
In this paper we present the first automatic approach to con- 

troll the multiple hypothesis problem during data exploration. 
We show how the AWARE system integrates user feedback and 
present several multiple hypothesis control technique base on 
α-investing, which control mFDR, and be especially suit for con- 
troll the error for interactive data exploration sessions. Finally, 
our evaluation show that the technique be indeed capable of 
control the number of false discovery use synthetic and real 
world datasets. However, a lot of work remains to be do from 
create and evaluate other type of default hypothesis over devel- 



oping new test procedure (e.g., for interactive Bayesian tests) to 
investigate technique to recover from case where the user run 
out of wealth. Yet, we consider this work a an important first step 
towards more sustainable discovery in a time where more data be 
analyze than ever before. 

10 References 
[1] E. Aharoni and S. Rosset. Generalized α-investing: definitions, optimality 

result and application to public databases. Journal of the Royal Statistical 
Society: Series B (Statistical Methodology), 76(4):771–794, 2014. 

[2] Y. Benjamini et al. Controlling the false discovery rate. Journal of the Royal 
Statistical Society, Series B, 57(5), 1995. 

[3] Y. Benjamini et al. The control of the false discovery rate in multiple test 
under dependency. Ann. Statist., 29(4), 08 2001. 

[4] Y. Benjamini and Y. Hochberg. Controlling the false discovery rate: a practical 
and powerful approach to multiple testing. Journal of the royal statistical 
society. Series B (Methodological), page 289–300, 1995. 

[5] D. A. Berry et al. Bayesian perspective on multiple comparisons. Journal of 
Statistical Planning and Inference, 82(1–2), 1999. 

[6] C. E. Bonferroni. Teoria statistica delle classi e calcolo delle probabilita. 
Libreria internazionale Seeber, 1936. 

[7] A. Burgess, R. Wagner, R. Jennings, and H. B. Barlow. Efficiency of human 
visual signal discrimination. Science, 214(4516):93–94, 1981. 

[8] F. Chirigati et al. Data polygamy: The many-many relationship among urban 
spatio-temporal data sets. In SIGMOD, 2016. 

[9] A. Crotty et al. Vizdom: Interactive analytics through pen and touch. PVLDB, 
8(12), 2015. 

[10] J. Demšar. Statistical comparison of classifier over multiple data sets. J. Mach. 
Learn. Res., 7:1–30, Dec. 2006. 

[11] E. Dimara, A. Bezerianos, and P. Dragicevic. The attraction effect in 
information visualization. IEEE Transactions on Visualization and Computer 
Graphics, 23(1), 2016. 

[12] B. Efron and T. Hastie. Computer Age Statistical Inference, volume 5. 
Cambridge University Press, 2016. 

[13] R. Fisher. The design of experiments. Oliver and Boyd, Edinburgh, Scotland, 
1935. 

[14] D. P. Foster and R. A. Stine. α-investing: a procedure for sequential control of 
expect false discoveries. Journal of the Royal Statistical Society: Series B 
(Statistical Methodology), 70(2):429–444, 2008. 

[15] M. G. G’Sell et al. Sequential selection procedure and false discovery rate 
control. Journal of the Royal Statistical Society: Series B (Statistical 
Methodology), 78(2), 2016. 

[16] H. Guo, S. Gomez, C. Ziemkiewicz, and D. Laidlaw. A case study use 
visualization interaction log and insight. IEEE Trans. Vis. Comput. Graph., 
2016. 

[17] Y. Hochberg. A sharper bonferroni procedure for multiple test of significance. 
Biometrika, 75(4):800–802, 1988. 

[18] S. Holm. A simple sequentially rejective multiple test procedure. Scandinavian 
journal of statistics, page 65–70, 1979. 

[19] J. P. A. Ioannidis. Why most publish research finding be false. Plos Med, 
2(8), 2005. 

[20] H. Jeffreys. The theory of probability. OUP Oxford, 1998. 
[21] M. I. Jordan. The era of big data. ISBA Bulletin, 18(2), 2011. 
[22] N. Kamat et al. Distributed and interactive cube exploration. In IEEE ICDE, 

2014. 
[23] A. Key et al. Vizdeck: self-organizing dashboard for visual analytics. In 

SIGMOD, 2012. 
[24] R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation 

and model selection. In Proceedings of the 14th International Joint Conference 
on Artificial Intelligence - Volume 2, IJCAI’95, page 1137–1143, San 

Francisco, CA, USA, 1995. Morgan Kaufmann Publishers Inc. 
[25] M. Lichman. UCI machine learn repository, 2013. 
[26] Z. Liu, B. Jiang, and J. Heer. immens: Real-time visual query of big data. In 

Computer Graphics Forum, volume 32, page 421–430. Wiley Online Library, 
2013. 

[27] J. H. McDonald. Handbook of Biological Statistics. Sparky House Publishing, 
Baltimore, Maryland, USA, second edition, 2009. 

[28] J. Neyman and E. L. Scott. Consistent estimate base on partially consistent 
observations. Econometrica: Journal of the Econometric Society, page 1–32, 
1948. 

[29] P. Pirolli and S. Card. The sensemaking process and leverage point for analyst 
technology a identify through cognitive task analysis. In Proceedings of 
international conference on intelligence analysis, volume 5, page 2–4, 2005. 

[30] P. Refaeilzadeh, L. Tang, H. Liu, and M. T. ÖZSU. Cross-Validation, page 
532–538. Springer US, Boston, MA, 2009. 

[31] M. Schemper. A survey of permutation test for censor survival data. 
Communications in Statistics-Theory and Methods, 13(13):1655–1665, 1984. 

[32] J. P. Shaffer. Multiple hypothesis testing. Annual review of psychology, 46, 
1995. 

[33] Y. B. Shrinivasan and J. J. van Wijk. Supporting the analytical reason 
process in information visualization. In Proceedings of the SIGCHI conference 
on human factor in compute systems, page 1237–1246. ACM, 2008. 

[34] Z. Šidák. Rectangular confidence region for the mean of multivariate normal 
distributions. Journal of the American Statistical Association, 62(318):626–633, 
1967. 

[35] R. J. Simes. An improve bonferroni procedure for multiple test of 
significance. Biometrika, 73(3):751–754, 1986. 

[36] M. Vartak et al. SEEDB: efficient data-driven visualization recommendation to 
support visual analytics. PVLDB, 8(13), 2015. 

[37] K. Wongsuphasawat et al. Voyager: Exploratory analysis via faceted browsing 
of visualization recommendations. IEEE Trans. Vis. Comput. Graph., 22(1), 
2016. 

[38] A. F. Zuur, E. N. Ieno, and C. S. Elphick. A protocol for data exploration to 
avoid common statistical problems. Methods in Ecology and Evolution, 
1(1):3–14, 2010. 

APPENDIX 
A Symbol table 

The follow table summarizes the important symbol and nota- 
tions use in this paper. 

H The set {H1, . . . , Hm} of null hypothesis observe on the stream. 
H The set {H1, , . . . ,Hm} of correspond “alternative hypotheis”. 
R The number of null hypothesis reject by the test procedure 

(i.e., the discoveries). 
V The number of erroneously reject null hypothesis 

(i.e., false discoveries, false positives, Type I errors). 
S The number of correctly reject null hypothesis 

(i.e., true discoveries, true positives,). 
R(j) The number of discovery after j hypothesis have be tested. 
V (j) The number of false discovery after j hypothesis have be tested. 
S(j) The number of false discovery after j hypothesis have be tested. 
m The number of hypothesis be tested. 
pj The p-value correspond to the null hypothsisHj . 
W (0) Initial wealth for the α-investing procedures. 
W (j) Wealth of the α-investing procedure after j tests. 
α Significance level for the test with α ∈ (0, 1). 
η Bias in the denominator formFDRη . 

Table 1: Notation Reference 


1 Introduction 
2 A Motivational Example 
2.1 Hypothesis Testing 
2.2 Visualizations a Hypotheses 
2.3 Heuristics for Visualization Hypotheses 
2.4 Heuristics Applied to the Example 

3 The Aware User Interface 
4 Background on Multiple Hypothesis Error 
4.1 Hold-Out Dataset 
4.2 Family-Wise Error Rate (FWER) 
4.3 False Discovery Rate (FDR) 
4.4 Other Approaches 

5 Interactive Control use -Investing 
5.1 Outline of the Procedure 
5.2 -Investing for Data Exploration 
5.3 -Farsighted Investing Rule 
5.4 -Fixed Investing Rule 
5.5 -Hopeful Investing Rule 
5.6 -Hybrid Investing Rule 
5.7 Investment base on Support Population 
5.8 What Happens If the Wealth be 0 

6 The Most Important Discoveries 
7 Experimental Evaluation 
7.1 Exp.1a: Static Procedures 
7.2 Exp.1b: Incremental Procedures 
7.2.1 Varying Number of Hypotheses 
7.2.2 Varying Degree of Randomness 
7.2.3 Varying Support Size 

7.3 Exp.2: Real Workflows 

8 Related Work 
9 Conclusion and Future Work 
10 References 
A Symbol table 

