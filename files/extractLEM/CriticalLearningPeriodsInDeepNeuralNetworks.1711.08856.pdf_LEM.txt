


















































Critical Learning Periods in Deep Neural Networks 
UCLA-TR-170017 

Alessandro Achillea,1,2, Matteo Rovereb,1, and Stefano Soattoa 

aDepartment of Computer Science, University of California, Los Angeles, 405 Hilgard Ave, Los Angeles, 90095, CA, USA; bAnn Romney Center for Neurologic Diseases, 
Brigham and Women’s Hospital and Harvard Medical School, Boston, MA, USA 

This manuscript be compile on May 16, 2018 

Critical period be phase in the early development of human and 
animal during which experience can irreversibly affect the architec- 
ture of neuronal networks. In this work, we study the effect of visual 
stimulus deficit on the training of artificial neural network (ANNs). 
Introducing well-characterized visual deficits, such a cataract-like 
blurring, in the early training phase of a standard deep neural net- 
work cause a permanent performance loss that closely mimic crit- 
ical period behavior in human and animal models. Deficits that do 
not affect low-level image statistics, such a vertical flip of the 
images, have no last effect on the ANNs’ performance and can be 
rapidly overcome with further training. In addition, the deeper the 
ANN is, the more pronounce the critical period. To good under- 
stand this phenomenon, we use Fisher Information a a measure of 
the strength of the network’s connection during the training. Our 
information-theoretic analysis suggests that the first few epoch be 
critical for the creation of strong connection across different layers, 
optimal for processing the input data distribution. Once such strong 
connection be created, they do not appear to change during addi- 
tional training. These finding suggest that the initial rapid learn 
phase of ANN training, under-scrutinized compare to it asymptotic 
behavior, play a key role in define the final performance of net- 
works. Our result also show how critical period be not restrict 
to biological systems, but can emerge naturally in learn systems, 
whether biological or artificial, due to fundamental constrains arise 
from learn dynamic and information processing. 

Critical Period | Deep Learning | Information Theory | Artificial Neuro- 
science | Information Plasticity 

The term “critical period” refers to a phase in brain devel-opment during which the effect of experience lead to deep 
and irreversible remodel of neural circuit (1). Similarly, 
the expression “sensitive period” be use to describe a time of 
heightened, yet reversible, plasticity in response to experience 
(2). The concept be introduce by Hubel and Wiesel in the 
1960s, a part of their seminal work on the architecture and 
development of the visual system (3). In their classical experi- 
ments use monocularly deprive kittens, they study the 
effect of the deficit on ocular dominance in the primary visual 
cortex (V1). They found that kitten monocularly blind by 
lid suture in the first 3 month after birth remain blind in 
the deprive eye, while no effect be observe in adult cat 
subject to analogous or more severe deficit (4, 5). Critical 
period be late report for a variety of animal species, 
include human (6, 7), and be not limited to visual or, for 
that matter, to sensory systems. Either “critical” or “sensitive” 
period have be observed, among others, in auditory space 
processing (8), filial imprint (9), song learn in songbird 
(10), language proficiency (11), and behavioral development 
(12). The evolutionary advantage of all these phenomenon have 
be attribute to the delicate balance between adaptivity 
and robustness that have to be struck in order to maximize 

fitness in a mutable environment, use early experience a a 
guide (2, 13). 

The biological mechanism underlie the regulation of 
critical period be manifold and depend on the specie be 
studied, the brain region involve and the nature of the expe- 
rience (14). In this work we show that, by introduce visual 
deficit during the initial training epoch (i.e., iteration over 
the training data) of common artificial deep neural networks, 
we observe trend that closely mimic those of human and ani- 
mal models. This may come a a surprise, since contemporary 
artificial neural network (ANNs) be only loosely inspire by 
biological system (15). 

Most study to date have focus either on the behavior 
of network at convergence (Representation Learning) or on 
the asymptotic property of the numerical scheme use to get 
there (Optimization). The role of the initial transient, espe- 
cially it effect in biasing the network towards “good” region 
of the complex and high-dimensional optimization problem, be 
rarely addressed. To study this initial learn phase of ANNs, 
we replicate experiment perform in animal model and find 
that the response to early deficit be remarkably similar, de- 
spite the large underlie difference between the two systems. 
In particular, we show that the quality of the solution depends 
only minimally on the final, relatively well-understood, phase 
of the training process or on it very first epochs; instead, it 
depends critically on the period prior to initial convergence. 

In animals, sensory deficit introduce during critical peri- 
od induce change in the architecture of the correspond 
area (3, 7, 16). To determine whether a similar phenomenon 
exists in ANNs, we compute the Fisher Information of the 
weight of the network a a proxy to measure it “effective 
connectivity”, that is, the density of connection that be ef- 
fectively use by the network in order to solve the task. Like 

Significance Statement 

Similar to human and animals, we find that artificial neural 
network (ANNs) exhibit critical period during which a tempo- 
rary stimulus deficit can impair the development of a skill. The 
extent of the impairment depends on the inception and length 
of the deficit window, a in animal models, and on the size of 
the neural network. Our work show how layer-wise change 
in the effective connectivity, measure use the Fisher Infor- 
mation of the connections’ weights, underpin critical period 
in ANNs, and could be one of the mechanism underlie bio- 
logical critical period a well. We refer to this phenomenon in 
deep network a “Information Plasticity”. 

1Alessandro Achille and Matteo Rovere contribute equally to this work and be list in alphabet- 
ical order. 

2To whom correspondence should be addressed. E-mail: achille@cs.ucla.edu 

ar 
X 

iv 
:1 

71 
1. 

08 
85 

6v 
2 

[ 
c 

.L 
G 

] 
1 

5 
M 

ay 
2 

01 
8 



others before u (17), we observe two distinct phase during 
the training, first a “learning phase” in which the Fisher Infor- 
mation of the weight increase a the network learns from the 
data, follow by a “consolidation” or “compression” phase in 
which the Fisher Information decrease and stabilizes. Sensi- 
tivity to critical-period-inducing deficit be maximal exactly 
when the Fisher Information peaks. 

A layer-wise analysis of the network’s effective connectivity 
show that, in the task and deficit we consider, the hierarchy 
of low-level and high-level feature in the training data be a 
key aspect behind the observe phenomena. In particular, our 
experiment suggest that the existence of critical period in 
deep neural network depends on the inability of the network 
to change it effective connectivity pattern in order to process 
different information (in response to deficit removal). We 
call this phenomenon, which be not mediate by any external 
factors, a loss of the “Information Plasticity” of the network. 

Results 

Deep Artificial Neural Networks exhibit critical periods. Stud- 
y of the critical period for monocular deprivation in human 
rely on cohort of patient affected by stimulus-deprivation 
amblyopia (reduced visual acuity in one eye), either in their 
infancy or childhood, cause by spontaneous or traumatic 
unilateral cataract (18, 19). After surgical correction of the 
cataracts, the ability of the patient to regain normal acuity 
in the affected eye depends both on the duration of the deficit 
and on it age of onset. Earlier and longer deficits, a in animal 
study of monocular deprivation (5, 20), lead to increasingly 
severe effects. 

In order to replicate this experimental setup in ANNs, we 
train a standard convolutional network (CNN) to classify ob- 
jects in small 32× 32 RGB image from the CIFAR-10 dataset 
(21) in 10 classes. Recognition performance in the image 
classification task be akin to optotype symbol identification, 
employ in optometric acuity testing. To simulate the effect 
of cataracts, for the first t0 epoch the image in the dataset 
be downsampled to 8× 8 and then upsampled back to 32× 32 
use bilinear interpolation, in practice blurring the image 
and destroy small-scale details.∗ After that, the training 
continue for 300 more epochs, give the network enough time 
to converge and ensure it be expose to the same number of 
uncorrupted image a in the control (t0 = 0) experiment. 

In Figure 1, we graph the final performance of the network 
(described in Materials and Methods) a a function of the 
epoch at which the deficit be correct (t0). We clearly observe 
the existence of a critical period for this deficit in the ANN: 
if the blur be not remove within the first 60 epochs, the 
final performance be severely decrease when compare to the 
baseline (from a test error of ∼ 6.4%, in the absence of a 
deficit, to more than 18% when the blur be present over 140 
epochs, a ∼ 300% increase). Once rescale to account for the 
arbitrary time unit (training epoch vs. days), the profile 
of the curve be also strikingly similar to the one obtain in 
kitten monocularly deprive from near birth and whose visual 
acuity upon eye-opening be test and plot against the 
length of the deficit window (23). Just like in human and 
animal model (where critical period be characteristic of 
early development), the critical period in the DNN also arises 

∗We employ this method, instead of a simpler Gaussian blur, since it have a very similar effect and 
make the quantification of information loss clearer. 

0 20 40 60 80 100 120 140 
Deficit removal (epoch) 

82% 

84% 

86% 

88% 

90% 

T 
e 

t a 
cc 

ur 
ac 

y 

Convolutional Network 

0 100 200 
Age (days) 

2 

4 

6 

A 
cu 

ity 

Kitten 

0 20 40 60 80 100 120 140 
Window onset (epoch) 

0% 

1% 

2% 

D 
ec 

re 
a 

e 
in 

T 
e 

t A 
cc 

ur 
ac 

y 

Sensitivity of learn phase 

25 50 75 100 
Window onset (days) 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

S 
en 

si 
tiv 

ity 

Sensitivity of learn phase (kitten) 

Fig. 1. (Top Left) Final test accuracy (solid line) achieve by a CNN train in the 
presence of a deficit (image blur) plot against the epoch at which the deficit be 
removed. The performance of the network be impaired if the blur be not remove 
early enough in the training. The critical period be center in the early rapid learn 
phase of the network (the dash line show the rapid increase of test accuracy a 
a function of the training epoch, when training the CNN in the absence of deficit). 
(Top Right) Development of visual acuity in the cat a a function of age (dashed 
line), compare with the critical period for monocular deprivation (solid line; the visual 
acuity value plot be those of kitten monocularly deprive since birth and test 
at the time of eye-opening, while the green triangle mark the acuity of non-deprived 
kittens). Adapted from (22) and (23). Despite the profound difference between the 
two systems, the trend observe in animal model be remarkably similar to the one 
obtain from a CNN. (Bottom Left) Here the deficit be introduce in a short window 
of 40 epochs, vary the epoch of onset, and the final test accuracy of the network be 
plot a a function of the onset. The decrease in the CNN’s performance can be 
use to probe the sensitivity of each epoch to the deficit: the most affected epoch 
be in the middle of the early rapid learn phase, before the test error (dashed line) 
begin to plateau. After the test error plateaus, the network be largely unaffected by 
the deficit. (Bottom Right) Degree of functional disconnection (normalized number 
of V1 monocular cell disconnect from the contralateral eye, 0 in a non-deprived 
kitten) plot against the kittens’ age at the onset of the monocular deprivation 
deficit window of 10-12 day (solid line). Adapted from (20). Overlaid on the critical 
period profile be the the plot of the percentage miss visual acuity at each age, 
normalize to the value attain by a mature cat (dashed line, data from (22)). Again, 
a similar experiment perform on animal model show result analogous to the 
ANN’s performance. 

during the initial rapid learn phase. At this stage, the 
network be quickly learn a solution before the test error 
plateau and the longer asymptotic convergence phase begins. 

To quantify more accurately the sensitivity of the ANN 
to image blurring throughout it early learn phase, we 
introduce the deficit in a short constant window (40 epochs), 
start at different epochs, and then measure the decrease 
in the ANN’s final performance compare to the baseline. In 
Figure 1, we plot the final test error of the network against 
the epoch of onset of the deficit. We observe that the network’s 
sensitivity to blurring peak in the central part of the early 
rapid learn phase (around 30 epochs), while late deficit 
produce little or no effect. A similar experiment be also 
perform on kitten by Olson and Freeman, use a window 
of 10-12 day during which the animal be monocularly 
deprive and use it to “scan” the first 4 month after birth 
to obtain a sensitivity profile (20). The result obtain on the 
DNN are, again, in accord with the trend observe in animal 



0 20 40 60 80 100 120 140 
Deficit removal (epoch) 

82% 

84% 

86% 

88% 

90% 

92% 

T 
e 

t a 
cc 

ur 
ac 

y 
Vertical flip 

0 20 40 60 80 100 120 140 
Deficit removal (epoch) 

82% 

84% 

86% 

88% 

90% 

92% 

Label permutation 

0 20 40 60 80 100 120 140 160 
Deficit removal (epoch) 

82% 

84% 

86% 

88% 

90% 

92% 

T 
e 

t a 
cc 

ur 
ac 

y 

Noise a input 

Fig. 2. High-level perturbation do not induce a critical period. (Left) Final perfor- 
mance of a CNN train on vertically flip image for the first t0 epochs, plot 
against t0 and (Right) the same experiment perform when use permutation of 
the label a the deficit. The solid blue curve be the test error at the end of the 
training a a function of the epoch of deficit removal, whereas the green dash 
curve be the test error during training of a network in the absence of deficit. When 
the deficit only affect high-level feature (as in the vertical flip) or the last layer of 
the CNN (label permutation), the network do not present a critical period (test 
accuracy remains largely flat). High-level feature have also be show to remain 
plastic well into adulthood in human and animal model (24, 25). These finding 
suggest that the critical period in DNNs be closely related to the depth of the network 
and the hierarchy of image features. (Bottom) Sensory deprivation cause a less 
severe deficit response during the critical period. We simulate sensory deprivation 
by replace the input image with Gaussian noise; remarkably, a critical period still 
exists, but it be longer and less pronounce than the one obtain in the case of image 
blur (Figure 1). A similar phenomenon have be report in human and animal 
models, where early sensory deprivation prolongs the plasticity of neuronal network 
(26, 27). 

model (Figure 1).† 

High-level deficit be not associate with a critical period. 
Critical period of neuronal network deal with high-level 
aspect of sensory processing end late than their lower-level 
counterparts, with some nucleus remain plastic even in adult- 
hood (7). Although the inversion of the visual field have not 
be systematically test on young animal (or humans), it 
be well-reported that adult human can quickly adapt to it 
(24, 30), suggest the absence of a critical period. In addition 
to the behavioral recovery from the deficit, the anatomical plas- 
ticity in the adult have also be confirm in animal model 
(25). 

We observe a similar behavior in ANNs: in Figure 2 we 
perform the same experiment a before, this time use vertical 
flip of the training image a the deficit, instead of blurring. 
We observe that the ANN be largely unaffected by vertical 
flip and that the network quickly recovers it baseline 
performance after deficit correction. An analogous result be 
also obtain when the high-level deficit be a permutation 
of the task label (Figure 2). These finding suggest that 

†Olson and Freeman here employ a neuroanatomical readout to measure the impact of monocular 
deprivation on visual development. While excellent correlation between anatomy, physiology and 
behavior be generally observe (7, 16, 28, 29), direct comparison with the ANN’s performance 
in the image classification task be less warrant in this case. We late return to this aspect and 
characterize the “connectivity” of ANNs during training in Information, synaptic strength, critical 
periods. 

0 20 40 60 80 100120140160180 
Deficit removal (epoch) 

82% 

85% 

88% 

90% 

92% 

95% 

T 
e 

t a 
cc 

ur 
ac 

y 

ResNet 

0 80 160 240 320 400 480 
Deficit removal (epoch) 

98.6% 

98.8% 

99.0% 

99.2% 

Fully Connected 

0 80 160 240 320 400 
Deficit removal (epoch) 

86% 

87% 

88% 

89% 

T 
e 

t a 
cc 

ur 
ac 

y 

Fixed learn rate 

0 80 160 240 
Deficit removal (epoch) 

­6% 

­4% 

­2% 

0% 

R 
el 

at 
iv 

e 
te 

st 
a 

cc 
ur 

ac 
y 

Dependency on deepness 

n=1 
n=2 
n=3 

Fig. 3. Critical period for an image blur deficit (Top Left) in a ResNet architecture 
train on CIFAR-10 with learn rate anneal and (Top Right) in a deep fully- 
connect network train on MNIST with a fix learn rate. Different architectures, 
use different optimization method and train on vastly different datasets, still 
exhibit similar critical period behavior. (Bottom Left) Same experiment a in Figure 1, 
but in this case the network be train with a fix learn rate instead of use 
an anneal scheme. Although the time scale of the critical period be longer, the 
qualitative trend be the same. This find support the notion that have a critical 
period be an intrinsic property of deep network and cannot be explain solely in 
term of the loss landscape and energy functions, without take into account the 
network’s architecture. (Bottom Right) Dependence of the critical period profile on 
the network’s depth: add more convolutional block to the network increase the 
effect of the deficit during it critical period. 

the existence of a critical period depends on the structure 
of the input data and the nature of the deficit applied, and, 
a such, cannot be interpret solely a an artifact of the 
optimization process or the network’s architecture. We have 
further explore these aspect in late experiments. 

Sensory deprivation cause a less severe critical period. It 
have be observe in animal models, and recently indirectly 
confirm in human (26), that a period of early sensory 
deprivation (dark-rearing) can lengthen the critical period and 
thus cause less severe effect (at the same age) than those 
document in light-reared animal (27). Similarly, both in 
human and animal models, the effect of binocular cataract 
have be characterize a being, somewhat counter-intuitively, 
less severe than those report in patient and animal suffer 
from monocular deficit (31, 32). These findings, though, 
can be explain consider that, since visual experience be 
necessary to shape neuronal circuits, lack of experience can be, 
under certain circumstances, less damage to the development 
of visual skill than defective experience (2). 

Simulating complete visual deprivation in a neural network 
be not a simple a feed a constant stimulus: a network 
present with a constant blank input will rapidly become 
trivial and thus unable to train on new data. This be to be 
expected, since a blank input be a perfectly predictable stimulus 
and thus the network can quickly learn the (trivial) solution to 
the task. We instead want to model an uninformative stimulus, 
akin to noise. Moreover, even when the eye be suture or 
maintain in the darkness, there will be background excitation 
of photoreceptors that be best model a noise. To account for 



this, we simulate sensory deprivation by replace the input 
image with a dataset compose of (uninformative) random 
Gaussian noise. This way the network be train on solve the 
highly non-trivial task of memorize the association between 
the finitely-many noise pattern and their correspond labels. 

Figure 2 show that this extreme form of deficit have a 
much less severe effect, during it critical period, than the 
one obtain by only blurring images. Using Gaussian noise 
during the early training phase of the ANN do not provide 
any information on the natural images. Yet it effect be 
milder than those cause by a deficit (e.g., image blur), which 
instead conveys some information, but lead the network to 
(incorrectly) learn that no fine structure be present in the 
images. It be also interest to note how a network train 
to memorize noise patterns, while training on a completely 
different array of feature than those need to classify natural 
images, can still learn to identify image correctly with relative 
ease (Figure 2). 

Dependence on the optimization algorithm and architecture. 
Deep network be commonly train use stochastic gradient 
descent (SGD) with an anneal scheme decrease the learn- 
ing rate over the duration of the training, in order to obtain 
convergence despite the noise introduce by the stochastic 
gradients. One possible explanation for the existence of critical 
period in DNNs would thus be that, once the network enters 
a sub-optimal local minimum because of the deficit introduced, 
it cannot then (when the deficit be removed) escape due to the 
decrease learn rate. In Figure 2 we have already observe 
that in the case of several deficit the network manages to es- 
cape such a hypothetical local extremum despite the decrease 
learn rate. In Figure 3, we confirm that a critical period 
exists even when the learn rate, which can be thought of a 
regulate the temperature of the system undergo gradient 
descent (34), be kept constant. 

Figure 3 also show that deep fully-connected networks, 
train on the MNIST digit classification dataset, present a 
critical period for the image blur deficit; therefore the convo- 
lutional structure be not necessary, nor be the use of natural 
images. Similarly, a ResNet-18 train on CIFAR-10 also have 
a critical period, sharper than the one found in a standard 
convolutional network (Figure 1). This be especially interesting, 
since ResNets allow for easy backpropagation of gradient to 
the low layers, thus suggest that the critical period be not 
cause by vanish gradients. 

Information, synaptic strength, critical period 

Deficits that result in critical period reflect in change of 
the brain architecture in the associate area (7). This be 
inevitably different in artificial networks, a their connectivity 
be formally fix at all time during training. However, not 
all the connection have the same importance for the final 
output of the network: This fact be captured, for example, 
by the Fisher Information Matrix (FIM) (35), which may be 
consider the artificial network equivalent of population-level 
measure of synaptic strength and connectivity in neuronal 
network (36) (but see also (37) for alternative metrics). In 
this section, we briefly recall some property of the FIM, 
and apply it to the study of the connections’ strength of an 
artificial neural network train in the presence and absence 
of deficits. Using this approach we aim to determine whether 

the (global) response of the ANN to deficit correlate with 
(local) change in connectivity, similarly to how behavioral and 
physiological feature correlate in experience-deprived animal 
model (3, 16, 28, 29). 

The Fisher Information Matrix. Generally, an artificial neural 
network encodes a distribution pw(y|x) — parametrized by 
the weight w — of the task variable y (e.g., an image label 
in CIFAR-10), give an input image x. Intuitively, the impor- 
tance of a specific connection can be estimate by perturb 
the correspond weight and look at the magnitude of 
the change in the final distribution. Let u consider then a 
perturbation w′ = w + δw of the weights, which result in a 
perturbed distribution pw′ (y|x). The discrepancy between the 
original distribution and the perturbed one can be measure 
by their Kullback-Leibler divergence, which, to second-order 
approximation, be give by: 

Ex KL( pw′ (y|x) ‖ pw(y|x) ) = δw · Fδw + o(δw2), 

where the expectation over x be compute use the em- 
pirical data distribution Q̂(x) give by the dataset, and 
F := Ex∼Q̂(x)Ey∼pw(y|x)[∇w log pw(y|x)∇w log pw(y|x) 

T ] be 
the Fisher Information Matrix (FIM). The FIM can thus 
be consider a local metric measure how much the pertur- 
bation of a single weight (or a combination of weights) affect 
the output of the the network (38). In particular, weight 
with low Fisher information can be remove (pruned) without 
affect the network’s performance, which suggests that the 
Fisher Information can be used, in an artificial network, a a 
measure of the effective connectivity or, more generally, of the 
strength of a connection (36). For these reasons, especially 
when draw comparison with neuronal networks, we will 
sometimes use “connection strength” and Fisher Information 
interchangeably. As it name suggests, the FIM can also be 
consider a a measure of the quantity of information that 
the model contains about the training data (35). Indeed, we 
expect the strength of the connection to increase a we learn 
more information from experience. Finally, the FIM be also a 
semi-definite approximation of the Hessian of the loss function 
(39) and hence of the curvature of the loss landscape at a par- 
ticular point w during training, which provide a connection 
between the FIM and the optimization procedure (38). 

We will employ the trace of the Fisher Information Matrix 
to measure the global or layer-wise connection strength, since 
it can be calculate efficiently despite the large size of our 
networks. We also consider compute the log-determinant 
of the matrix use the Kronecker-Factorized approximation 
(40, 41) in order to capture the behavior of off-diagonal terms, 
but we observe the same qualitative trend a the trace. We 
use ResNets to compute the FIM because, be a local mea- 
sure, it be very sensitive to the irregularity of the loss land- 
scape and ResNets have a relatively smooth landscape (42). 
For other network architecture we use a more stable estimator 
of the FIM base on the injection of noise in the weight (43) 
(also see Materials and Methods). 

Information change during training. In Figure 4 we estimate 
the global change to the connections’ strength in a ResNet by 
plot the trace of the FIM during the network training. We 
observe that during an initial phase the network acquires infor- 
mation about the data, which result in a large increase in the 
strength of the connections. However, once the performance 



0 50 100 150 200 
Epoch 

100 

200 

300 

400 

500 

600 
F 

be 
he 

r 
In 

fo 
rm 

at 
io 

n 
tr 

ac 
e 

Fisher Information vs. deficit sensitivity 

tr(F) 
exp( S40) 
exp( S60) 

102 103 104 

Conceptional age (days) 

15 

20 

25 

30 

35 

S 
yn 

ap 
tic 

d 
en 

si 
ty 

Birth 

Synaptic density in monkey V1 vs. age 

0 50 100 150 200 
Epoch 

0 

500 

1000 

1500 

F 
be 

he 
r 

In 
fo 

rm 
at 

io 
n 

tr 
ac 

e 

Fisher Information vs. deficit end 

No deficit 
Deficit until 220 
Deficit until 30 
Deficit until 50 
Deficit until 70 
Deficit until 100 
Deficit until 220 

Fig. 4. (Left) Plot of the trace of the Fisher Information Matrix (FIM) of the network weight a a function of the training epoch (solid blue line). We observe two distinct 
phase during the training: First, the network employ increasingly more connection in order to solve the task (and consequently information grows). Once a good solution be 
found, the network start to consolidate the connection (and information decreases), so that few resource (information, or connection if the weak one be pruned) be 
need to solve the task. This follow the same trend a the network’s sensitivity to critical periods, a compute in Figure 1 and fit to the Fisher Information use a simple 
exponential (see Materials and Methods), suggest that the two quantity be linked. We show the result use both a window size of 40 and 60 to compute the sensitivity 
curve. (Center) The FIM can be interpret a a measure of the density of the effective network connections. Synaptic density during development (here show for the visual 
cortex of macaques; the green dash line mark birth and the green stripe be the average synaptic density in a healthy adult, adapt from (33)) follow a similar trend where, 
after a sharp increase and a peak during early development, progressive elimination of synapsis (pruning) occurs and continue throughout life. ANNs, though, appear to 
have both a different timescale and a more abrupt decrease in the effective connectivity after peaking. (Right) Effects of deficit (image blur) on the FIM. In the presence of a 
critical-period-inducing deficit, the ANN employ a much large set of connection to solve the task. This can be explain by the fact that the network, unable to find a general 
rule to classify the corrupt images, be force to memorize them in order to solve the task. This reflect in increase use of the network’s resources, correctly capture by the 
FIM, it be a measure of the ANN’s information content. 

in the task begin to plateau, the network start reduce 
the overall strength of it connection while keep it per- 
formance unchanged. This can be see a a “consolidation” 
or “compression” phase, during which redundant connection 
be eliminate and non-relevant memorize information be dis- 
carded. It must be note how a related observation on the 
training procedure consist of two phase have be suggest 
by (17), who, however, focus on the compression of the ac- 
tivations of the network, rather than on the information in 
the weight and their connectivity. There be a fundamental 
difference between these two analyses: The relation between 
the information in the weight and the activation be non- 
trivial and articulate in (43). In particular, compression of 
the weight can imply compression of the activations, under 
suitable non-generic assumptions. 

Interestingly, this change in the connection strength be 
closely related to the sensitivity to critical-period-inducing 
deficit a image blur, compute use the slide window 
method a in Figure 1. In Figure 4 we see that the sensitiv- 
ity be maximal precisely when the Fisher Information peaks. 
Moreover, we observe that the exponential of the sensitivity 
closely fit the Fisher Information, which be remarkable since 
the FIM be a local quantity compute at a single point during 
the training of a network without deficit, while the sensitivity 
during a critical period be compute use the test data at the 
end of the training of a network in the presence of a deficit. 
It be also interest to see the effect of deficit introduction 
on the Fisher Information. From Figure 4 (Right), we see 
that the FIM trace grows much more in the presence of the 
deficit (image blur) and remains substantially high even 
after it be removed. When the data be so corrupt a to be 
classify incorrectly, the network be force to memorize the 
labels, therefore increase the quantity of information need 
to perform the same task. 

Notice that a similar phenomenon be observe in the devel- 
opment of neuronal network (Figure 4, Center), where the 
density of synaptic connection rapidly peak during early 
development and slowly decrease afterwards a a result of 
synaptic prune (44, 45). However, while deficit have be 

show to affect the synaptic density in animal model (45), an 
increase in the overall number of connection be not observed. 
This difference can be partly attribute to the fact that in our 
experiment ANNs be train use a finite dataset, make 
pure memorization a viable strategy to solve the task and 
therefore encourage the storage of unnecessary information. 

Layer-wise effect of deficits. We can gain additional insight 
on what be happen in an ANN affected by a deficit by 
look at the layer-wise change of the Fisher Information. 
In order to do so, we use the All-CNN architecture, which 
have a clearer subdivision in layer than a ResNet. When 
the network be train without deficit (Figure 5, Left), the 
strong (most important) connection be in the intermediate 
layers. This reflect the fact that most of the information in 
CIFAR-10 image be present at an “intermediate” scale, with 
low level information (e.g., edges) alone be insufficient 
for identification, and global/contextual information (e.g., 
background, position of objects) provide only limited benefit 
to the task performance. Therefore, we expect the network 
to focus it resource on the intermediate layers, since their 
receptive field be optimal for this task. 

On the other hand, if the network be initially train on 
blur data (Figure 5, Top Center and Top Right), the 
strength of the connection be dominate by the top layer 
(Layer 6). This be to be expected, since the low-level and 
mid-level structure of the image be destroyed and all of the 
remain information be contain in the global feature of 
the image, which can be process by Layer 6, but not by 
the low-level layer with their small receptive fields. When 
the deficit be remove early in the training (Figure 5, Top 
Center), the network manages to “re-organize”, reduce the 
information contain in the last layer, and, at the same time, 
increase the Fisher Information in the intermediate layers, in 
order to process the new feature available. We refer to these 
change in the Fisher Information a “Information Plasticity”. 
However, if the deficit be remove after the begin of the 
“consolidation” phase, the quantity of information contain 
in the mid-level layer do not change significantly because 
of the network’s loss of Information Plasticity. 



0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

Im 
a 

g 
e 

B 
lu 

r 

N 
or 

m 
al 

iz 
ed 

In 
fo 

. 
in 

W 
ei 

gh 
t 

No deficit Deficit until epoch 40 Deficit until epoch 100 

Layer 0 

Layer 1 

Layer 2 

Layer 3 

Layer 4 

Layer 5 

Layer 6 

Layer 7 

0 50 100 150 

Epoch 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

V 
er 

ti 
ca 

l 
F 

lip 

N 
or 

m 
al 

iz 
ed 

In 
fo 

. 
in 

W 
ei 

gh 
t 

0 25 50 75 100 125 150 

Epoch 

0 25 50 75 100 125 150 

Epoch 

Layer 0 

Layer 1 

Layer 2 

Layer 3 

Layer 4 

Layer 5 

Layer 6 

Layer 7 

Fig. 5. Normalized quantity of information contain in the weight of each layer a a function of the training epoch. (Top Left) In the absence of deficit, the network initially 
acquires a large amount of information, store in the connection (weights), and us it to quickly learn the new task (the training error also decrease rapidly, with possible 
overfitting). As training progresses, the network explores more efficient solution to the task, require less resource (the “compression phase” to optimize the information 
regularizer described in (43)). It must be note that most of the resource of the network (i.e., information in the weights) be allocate to the middle layer (layers 3-4-5). (Top 
Right) In the presence of an image blur deficit until epoch 100, the task be significantly more difficult, and the network allocates more resource in order to solve it (see also 
Figure 4). Most importantly, more resource be allocate to the high layer (layer 6) rather than to the middle layer (as it be the case in the absence of deficit). This can be 
explain by the fact that the deficit destroys the low- and mid-level feature that could be process by those layer and leaf only the global feature of the image, which can 
only be process by the high layers. When the deficit be removed, after a small transient, the layer maintain the same content of information instead of allocate more 
resource to the middle layers. (Top Center) On the other hand, when the deficit be remove at an early epoch, the layer are, at least partially, reconfigured (layer 6 rapidly 
loses information, relative to other layers), without long-term consequences. The most sensitive period for learn a task appear to coincide with the start of a “consolidation 
phase”, after the initial fast-changing transient. (Bottom Left, Center, Right) The layer-wise normalize quantity of information be show for an ANN train in the presence of 
a vertical flip deficit, which do not induce a critical period (from Left to Right: no deficit, vertical flip until epoch 40, vertical flip until epoch 100). As expected, the relative 
quantity of information in the layer be not affected. 

Discussion 

ANNs and neuronal network be vastly different systems, 
nonetheless both exhibit critical period during their early 
learn phase and these share a number of common features. 
In particular, we have show that while cataract-like deficits, 
such a severe image blurring, present a critical period, high- 
level impairments, like image flipping, have little or no effect on 
the DNNs’ performance. These finding be in agreement with 
the characterization of critical period response to deficit in 
human and animal models. In addition, we use the Fisher 
Information to study the development of the DNNs’ effective 
connection during training and recognize an early phase where 
the Fisher Information increase sharply, follow by a decrease. 
The maximum sensitivity to critical-period-inducing deficit 
corresponds to the maximum of the Fisher Information. 

It must be note that critical period depend on the task 
explored, the deficit applied, and the readout used: Due to the 
fundamental difference between neuronal network and ANNs, 
it be challenge to perfectly match experiment to animal 
models. Since visual pathway be extensively study in the 
neurobiology of critical period and DNNs be particularly 
suit for image classification, we chose to focus our analysis 
on this task. 

“Behavioral” readout upon deficit removal, both in ANNs 
and neuronal networks, can potentially be confound by 
deficit-coping change at different level of the visual pathway 
(2, 7). Since deficit in deprive animal be mirror by ab- 
normality in the circuitry of the visual pathways, we chose to 
also characterize their effect on network connections. While 
the connectivity of an ANNs be fix by design, it “effective 
connectivity”, i.e., the connection that be actually employ 

by the network to solve the task, be not: We quantify it from the 
Fisher Information Matrix of the network’s weights. Sensitiv- 
ity to critical period and the trace of the Fisher Information 
peak at the same epoch, in accord with the evidence that 
skill development and critical period in neuronal network 
be modulate by change (generally experience-dependent) 
in synaptic plasticity (2, 14). Our layer-wise analysis of the 
Fisher Information (Figure 5) also show that, while in a non- 
deprive network connection be strong in the intermediate 
layers, upon introduction of an image blurring deficit, the 
ANN reinforces high layer to the detriment of intermediate 
layers, leave low-level layer virtually untouched. If the 
deficit be remove after the ANN’s critical period ends, the 
network be not able to reverse these effect. Although the two 
system be radically different, it be worth note how simi- 
larities to such response to the introduction of visual deficit 
can be found in the visual pathway of animal model (and, in 
part, in humans). Lower level (e.g., retina, lateral geniculate 
nucleus) be little (or not at all) affected by deficit (4, 16, 46). 
Higher-level visual area e.g., V2 and post-V2) also show little 
remodel upon deprivation (47–49), possibly because their 
plasticity be maintain into adulthood (7) and thus most of 
the neuroanatomical and physiological change due to depriva- 
tion cluster, with vary severity, in the different layer of V1 
(which we can compare to an intermediate-processing level) 
(3, 16). 

In ANNs, the Fisher Information can be interpret a an 
approximation of the local curvature of the loss landscape (39). 
According to this view, Figure 4 (Left) suggests that SGD 
encounter two different phase during the network training: 
At first, the network move towards high curvature region of 



the loss landscape, while in the second phase the curvature 
decrease and the network eventually converges to a flat mini- 
mum (as also observe in (50)). We could interpret these a 
the network cross a narrow bottleneck during it training 
before enter a flat region of the loss surface. When combin- 
ing this assumption with our deficit sensitivity analysis, we 
can hypothesize that the critical period occurs precisely upon 
cross of this bottleneck. A different two-phase interpreta- 
tion of SGD have also be propose in (17): In the authors’ 
analysis, the network first learns to encode the task in the 
network’s activations, while in a second phase it minimizes the 
quantity of information, while keep enough of it to maintain 
good task performance. The relationship between the bipha- 
sic behavior we observe in the weights, and those observe 
by (17) in the activations, be non-trivial, a described in (43) 
and even define information quantity for the activation 
present a number of technical challenges. 

There be evidence that convergence to flat minimum (minima 
with low curvature) in a DNN correlate with a good gener- 
alization performance (42, 50–52). Indeed, by recall the 
link between the Fisher Information and curvature, we can 
deduce from Figure 4 (Right) that network impaired by the 
deficit converge to sharper minimum than unaffected networks. 
However, we have found that the performance of the network be 
determine by an early sensitive phase, during which the net- 
work cross high-curvature region of the loss landscape. This 
suggests that the final sharpness at convergence may be an 
epiphenomenon rather than the cause of good generalization. 

Applying a deficit at the begin of the training may be 
compare to the common practice of pre-training and fine- 
tune an ANN, whereby the network be initially train on a 
similar task, for which data be plentiful, and then fine-tuned 
to solve a specific task. This be generally found to improve the 
performance of the network. In (53) a similar practice be ana- 
lyzed, but in the different set of layer-wise unsupervised 
pretraining, seldom use in current practice, and suggests that 
the pre-training may act a a regularizer by move the weight 
of the network towards an area of the loss landscape closer to 
the attractor for good solutions. The author observe that 
the initial example have a strong effect in steer the net- 
work towards particular solutions. Here, we have show that 
pre-training on blur data can have the opposite effect; i.e., 
it can severely decrease the final performance of the network. 
Following the loss landscape interpretation, this would suggest 
that, surprisingly, a configuration of parameter able to classify 
blur image be farther apart from a configuration able to 
correctly classify real images. On the other hand, a config- 
uration able to memorize noise pattern can access a good 
solution to the original task. However, it must be remember 
that the interpretation of the deficit’s effect a move the 
network close to a bad attractor be difficult to reconcile with 
the smooth transition observe in the critical periods, since 
the network would either converge to this attractor, and thus 
have low accuracy, or not. A richer theoretical framework may 
therefore be need to study critical period purely in term 
of the geometry of the loss landscape. 

A peculiar interpretation of critical period in human and 
animal model be propose in (2). Knudsen claim that the 
initial connectivity profile of neuronal network be unstable and 
easily change by early experience (highly plastic). As more 
“samples” be observed, the connection change and reach a 

more stable configuration, which be not a easily modify by 
additional experience. Learning can, however, still happen 
within the newly create connectivity pattern. This be largely 
compatible with our findings: Sensitivity to critical-period- 
induce deficit peak when the remodel of connection 
be at it maximum (Figure 4, Left) and different connectivity 
profile be observe in network train with and without a 
deficit (Figure 5). This also agrees with the fact that high-level 
deficit such a image-flipping and label permutation, which 
do not require restructuring of the network’s connection in 
order to be corrected, do not exhibit a critical period. 

Finally, we want to emphasize how our goal here be not 
so much to investigate the human (or animal) brain through 
ANNs, a to understand fundamental information processing 
phenomena, both in their biological or artificial implementa- 
tions. It be also not our goal to suggest that, since they both 
exhibit critical periods, ANNs be necessarily a valid model of 
neurobiological information processing, although recent work 
have emphasize this aspect (15, 54, 55). We engage in an “Arti- 
ficial Neuroscience” exercise in part to address a technological 
need to develop “explainable” artificial intelligence system 
whose behavior can be understood and predicted. While tra- 
ditionally well-understood mathematical model be use 
by neuroscientist to study biological phenomena, informa- 
tion processing in modern artificial network be just a poorly 
understood a in biology, so we chose to exploit well-known bi- 
ological phenomenon a probe to study information processing 
in artificial networks. We have show how our approach can 
help unveil how information be represented, acquire and trans- 
form in complex network of relatively simple components, 
whether biological or artificial. 

Materials and Methods 

In all of the experiments, unless otherwise stated, we use the 
follow architecture, adapt from (56): 

conv 96 - conv 96 - conv 192 s2 - conv 192 - conv 192 - 
conv 192 s2 - conv 192 - conv1 192 - conv1 10 - 

avg. pool - softmax 

where each conv block consists of a 3× 3 convolution, batch normal- 
ization and ReLU activations. conv1 denotes a 1× 1 convolution. 
The network be train with SGD, with a batch size of 128, learn 
rate start from 0.05 and decay smoothly by a factor of .97 
at each epoch. We also use weight decay with coefficient 0.001. 
In the experiment with a fix learn rate, we fix the learn 
rate to 0.001, which we find to allow convergence without excessive 
overfitting. For the ResNet experiments, we use the ResNet-18 
architecture from (57) with initial learn rate 0.1, learn rate 
decay .97 per epoch, and weight decay 0.0005. When experiment 
with vary network depths, we use the follow architecture: 
conv 96 - [conv 96 · 2i−1 - conv 96 · 2i s2]ni=1 - conv 96 · 2 

n 

- conv1 96 · 2n - conv1 10 
In order to avoid interference between the anneal scheme and the 
architecture, in these experiment we fix the learn rate to 0.001. 
The Fully Connected network use for the MNIST experiment have 
hidden layer of size [2500, 2000, 1500, 1000, 500]. All hidden layer 
use batch normalization follow by ReLU activations. We fix the 
learn rate to 0.005. Weight decay be not used. We use data 
augmentation with random translation up to 4 pixel and random 
horizontal flipping. For MNIST, we pad the image with zero to 
bring them to size 32× 32. 

To compute the trace of the Fisher Information Matrix, we use 
the follow expression derive directly from the definition: 
tr(F ) = Ex∼Q̂(x)Ey∼pw(y|x)[tr(∇w log pw(y|x)∇w log pw(y|x) 

T )] 

= Ex∼Q̂(x)Ey∼pw(y|x)[‖∇w log pw(y|x)‖ 
2], 



where the input image x be sample from the dataset, while the 
label y be sample from the output posterior. Expectations be 
approximate by Monte-Carlo sampling. Notice, however, that 
this expression depends only on the local gradient of the loss with 
respect to the weight at a point w = w0, so it can be noisy when the 
loss landscape be highly irregular. This be not a problem for ResNets 
(42), but for other architecture we use instead a different technique, 
propose in (43). More in detail, let L(w) be the standard cross- 
entropy loss. Given the current weight w0 of the network, we find 
the diagonal matrix Σ that minimizes: 

L′ = Ew∼N(w0,Σ)[L(w)]− β log |Σ|, 

where β be a parameter that control the smoothness of the ap- 
proximation. Notice that L′ can be minimize efficiently use the 
method in (58). To see how this relates to the Fisher Information 
Matrix, assume that L(w) can be approximate locally in w0 a 
L(w) = L0 + a · w + w ·Hw. We can then rewrite L′ a 

L′ = L0 + tr(ΣH)− β log |Σ|. 

Taking the derivative with respect to Σ, and set it to zero, we 
obtain Σii = β/Hii. We can then use Σ to estimate the trace of 
the Hessian, and hence of the Fisher information. 

Fitting of sensitivity curve and synaptic density profile from 
the literature be perform use GraphPad Prism 7 (GraphPad 
Software, La Jolla, CA), using: 

f(t) = e−(t−d)/τ1 − ke−(t−d)/τ2 

a the fitting equation. t be the age at the time of sample and τ1, 
τ2, k and d be unconstrained parameter (59). 

The exponential fit of the sensitivity to the Fisher Information 
trace us the expression F (t) = a exp(cSk(t)) + b, where a, b and c 
be unconstrained parameters, F (t) be the Fisher Information trace 
at epoch t of the training of a network without deficit and Sk be 
the sensitivity compute use a window of size k. That is, Sk(t) be 
the increase in the final test error over a baseline when the network 
be train in the presence of a deficit between epoch t and t+ k. 

ACKNOWLEDGMENTS. Supported by ONR N00014-17-1-2072, 
ARO W911NF-17-1-0304, AFOSR FA9550-15-1-0229 and FA8650- 
11-1-7156. 

1. Kandel ER, Schwartz JH, Jessell TM, Siegelbaum SA, Hudspeth AJ (2013) Principles of 
Neural Science. (McGraw-Hill, New York, NY), 5th edition. 

2. Knudsen EI (2004) Sensitive period in the development of the brain and behavior. Journal 
of cognitive neuroscience 16(8):1412–25. 

3. Wiesel TN, Hubel DH (1963) Single-Cell response in striate cortex of kitten deprive of 
vision in one eye. Journal of neurophysiology 26:1003–17. 

4. Wiesel TN, Hubel DH (1963) Effects of visual deprivation on morphology and physiology of 
cell in the cat’s lateral geniculate body. Journal of Neurophysiology 26(6):978–993. 

5. Wiesel TN (1982) Postnatal development of the visual cortex and the influence of environ- 
ment. Nature 299(5884):583–91. 

6. Berardi N, Pizzorusso T, Maffei L (2000) Critical period during sensory development. Current 
opinion in neurobiology 10(1):138–145. 

7. Daw NW (2014) Visual Development. (Springer, New York, NY), 3rd edition. 
8. Knudsen EI (2002) Instructed learn in the auditory localization pathway of the barn owl. 

Nature 417(6886):322–8. 
9. Hess E (1973) Imprinting: early experience and the developmental psychobiology of attach- 

ment, Behavioral Science Series. (Van Nostrand Reinhold Co., New York, NY). 
10. Konishi M (1985) Birdsong: from behavior to neuron. Annual review of neuroscience 

8(1):125–70. 
11. Newport EL, Bavelier D, Neville HJ (2001) Critical think about critical periods: Perspectives 

on a critical period for language acquisition. in Language, brain and cognitive development: 
Essays in honor of Jacques Mehler, ed. Doupoux E. (MIT Press, Cambridge, MA), pp. 481– 
502. 

12. Fox SE, Levitt P, Nelson CA (2010) How the timing and quality of early experience influence 
the development of brain architecture. Child development 81(1):28–40. 

13. Bateson P, Gluckman P (2011) Plasticity, Robustness, Development and Evolution. (Cam- 
bridge University Press, Cambridge, UK). 

14. Hensch TK (2004) Critical period regulation. Annual review of neuroscience 27(1):549–79. 
15. Hassabis D, Kumaran D, Summerfield C, Botvinick M (2017) Neuroscience-inspired artificial 

intelligence. Neuron 95(2):245–258. 
16. Hendrickson AE, et al. (1987) Effects of early unilateral blur on the macaque’s visual system. 

II. Anatomical observations. The Journal of neuroscience 7(5):1327–39. 
17. Shwartz-Ziv R, Tishby N (2017) Opening the black box of deep neural network via informa- 

tion. arXiv preprint arXiv:1703.00810. 
18. Vaegan, Taylor D (1979) Critical period for deprivation amblyopia in children. Transactions of 

the ophthalmological society of the United Kingdom 99(3):432–9. 
19. von Noorden GK (1981) New clinical aspect of stimulus deprivation amblyopia. American 

journal of ophthalmology 92(3):416–21. 

20. Olson CR, Freeman RD (1980) Profile of the sensitive period for monocular deprivation in 
kittens. Experimental Brain Research 39(1):17–21. 

21. Krizhevsky A, Hinton G (2009) Learning multiple layer of feature from tiny images, (Univer- 
sity of Toronto), Technical report. 

22. Giffin F, Mitchell DE (1978) The rate of recovery of vision after early monocular deprivation in 
kittens. The Journal of physiology 274(1978):511–37. 

23. Mitchell DE (1988) The extent of visual recovery from early monocular or binocular visual 
deprivation in kittens. The Journal of physiology 395:639–60. 

24. Kohler I (1964) The formation and transformation of the perceptual world, Psychological Is- 
sue Monographs. (International Universities Press, New York, NY). 

25. Sugita Y (1996) Global plasticity in adult visual cortex follow reversal of visual input. Nature 
380(6574):523–6. 

26. Kalia A, et al. (2014) Development of pattern vision follow early and extend blindness. 
Proceedings of the National Academy of Sciences 111(5):2035–2039. 

27. Mower GD (1991) The effect of dark rear on the time course of the critical period in cat 
visual cortex. Brain research. Developmental Brain Research 58(2):151–8. 

28. Kiorpes L, et al. (1987) Effects of early unilateral blur on the macaque’s visual system. I. 
Behavioral observations. The Journal of neuroscience 7(5):1318–26. 

29. Movshon JA, et al. (1987) Effects of early unilateral blur on the macaque’s visual system. III. 
Physiological observations. The Journal of neuroscience 7(5):1340–51. 

30. Stratton GM (1896) Some preliminary experiment on vision without inversion of the retinal 
image. Psychological Review 3(6):611–617. 

31. Maurer D, Lewis TL (1993) Visual outcome after infantile cataract. in Early visual develop- 
ment, normal and abnormal., ed. Simons K. (Oxford University Press, New York, NY), p. 
454–484. 

32. Wiesel TN, Hubel DH (1965) Comparison of the effect of unilateral and bilateral eye closure 
on cortical unit response in kittens. Journal of neurophysiology 28(6):1029–40. 

33. Rakic P, Bourgeois JP, Eckenhoff MF, Zecevic N, Goldman-Rakic PS (1986) Concurrent 
overproduction of synapsis in diverse region of the primate cerebral cortex. Science 
232(4747):232–5. 

34. Welling M, Teh YW (2011) Bayesian learn via stochastic gradient langevin dynamic in 
Proceedings of the 28th International Conference on Machine Learning (ICML-11). pp. 681– 
688. 

35. Fisher RA (1925) Theory of statistical estimation in Mathematical Proceedings of the Cam- 
bridge Philosophical Society. (Cambridge University Press), Vol. 22, pp. 700–725. 

36. Kirkpatrick J, et al. (2017) Overcoming catastrophic forget in neural networks. Proceed- 
ings of the National Academy of Sciences 114(13):3521–3526. 

37. Zenke F, Poole B, Ganguli S (2017) Continual learn through synaptic intelligence in Inter- 
national Conference on Machine Learning. pp. 3987–3995. 

38. Amari Si, Nagaoka H (2007) Methods of information geometry. (American Mathematical 
Soc.) Vol. 191. 

39. Martens J (2014) New insight and perspective on the natural gradient method. arXiv 
preprint arXiv:1412.1193. 

40. Martens J, Grosse R (2015) Optimizing neural network with kronecker-factored approximate 
curvature in International conference on machine learning. pp. 2408–2417. 

41. Grosse R, Martens J (2016) A kronecker-factored approximate fisher matrix for convolution 
layer in International Conference on Machine Learning. pp. 573–582. 

42. Li H, Xu Z, Taylor G, Goldstein T (2017) Visualizing the loss landscape of neural nets. arXiv 
preprint arXiv:1712.09913. 

43. Achille A, Soatto S (2017) Emergence of Invariance and Disentangling in Deep Representa- 
tions. ArXiv e-prints. 

44. Huttenlocher PR, de Courten C, Garey LJ, Van der Loos H (1982) Synaptogenesis in human 
visual cortex – evidence for synapse elimination during normal development. Neuroscience 
letter 33(3):247–52. 

45. Huttenlocher PR (2002) Neural Plasticity: The Effects of Environment on the Development of 
the Cerebral Cortex. (Harvard University Press, Cambridge, MA). 

46. Sherman SM, Stone J (1973) Physiological normality of the retinal in visually deprive cats. 
(1):224–30. 

47. Sincich LC, Jocson CM, Horton JC (2012) Neuronal projection from V1 to V2 in amblyopia. 
The Journal of neuroscience : the official journal of the Society for Neuroscience 32(8):2648– 
56. 

48. Gandhi T, Kalia A, Ganesh S, Sinha P (2015) Immediate susceptibility to visual illusion after 
sight onset. Current Biology 25(9):R358–R359. 

49. Gandhi TK, Singh AK, Swami P, Ganesh S, Sinha P (2017) Emergence of categorical face 
perception after extend early-onset blindness. Proceedings of the National Academy of 
Sciences of the United States of America 114(23):6139–6143. 

50. Keskar NS, Mudigere D, Nocedal J, Smelyanskiy M, Tang PTP (2017) On large-batch training 
for deep learning: Generalization gap and sharp minima. 

51. Hochreiter S, Schmidhuber J (1997) Flat minima. Neural Computation 9(1):1–42. 
52. Chaudhari P, Choromanska A, Soatto S, LeCun Y (2016) Entropy-sgd: Biasing gradient de- 

scent into wide valleys. arXiv preprint arXiv:1611.01838. 
53. Erhan D, et al. (2010) Why do unsupervised pre-training help deep learning? Journal of 

Machine Learning Research 11(Feb):625–660. 
54. Yamins DL, DiCarlo JJ (2016) Using goal-driven deep learn model to understand sensory 

cortex. Nature neuroscience 19(3):356. 
55. Kriegeskorte N (2015) Deep neural networks: a new framework for model biological vision 

and brain information processing. Annual review of vision science 1:417–446. 
56. Springenberg JT, Dosovitskiy A, Brox T, Riedmiller M (2014) Striving for simplicity: The all 

convolutional net. arXiv preprint arXiv:1412.6806. 
57. He K, Zhang X, Ren S, Sun J (2016) Deep residual learn for image recognition in Pro- 

ceedings of the IEEE conference on computer vision and pattern recognition. pp. 770–778. 
58. Kingma DP, Salimans T, Welling M (2015) Variational dropout and the local reparameteriza- 

tion trick in Proceedings of the 28th International Conference on Neural Information Process- 
ing Systems, NIPS’15. pp. 2575–2583. 



59. Banks MS, Aslin RN, Letson RD (1975) Sensitive period for the development of human binoc- ular vision. Science 190(4215):675–7. 


1 Materials and Methods 

