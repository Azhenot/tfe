


















































deep learn a critical appraisal.formatted.pages 


Deep Learning: 
A Critical Appraisal 

Gary Marcus 1 
New York University 

Abstract 

Although deep learn have historical root go back decades, neither the term “deep 
learning” nor the approach be popular just over five year ago, when the field be 
reignite by paper such a Krizhevsky, Sutskever and Hinton’s now classic 2012 
(Krizhevsky, Sutskever, & Hinton, 2012)deep net model of Imagenet. 

What have the field discover in the five subsequent years? Against a background of 
considerable progress in area such a speech recognition, image recognition, and game 
playing, and considerable enthusiasm in the popular press, I present ten concern for deep 
learning, and suggest that deep learn must be supplement by other technique if we 
be to reach artificial general intelligence. 

! Departments of Psychology and Neural Science, New York University, gary.marcus at nyu.edu. I thank Christina 1 
Chen, François Chollet, Ernie Davis, Zack Lipton, Stefano Pacifico, Suchi Saria, and Athena Vouloumanos for 
sharp-eyed comments, all generously supply on short notice during the holiday at the close of 2017. 

Page ! of !1 27 

http://nyu.edu 


For most problem where deep learn have enable 
transformationally good solution (vision, speech), we've 
enter diminish return territory in 2016-2017. 

François Chollet, Google, author of Keras 
neural network library 
December 18, 2017 

‘Science progress one funeral at a time.' The future 
depends on some graduate student who be deeply suspicious 
of everything I have said. 

Geoff Hinton, grandfather of deep learn 
September 15, 2017 

1. Is deep learn approach a wall? 

Although deep learn have historical root go back decades(Schmidhuber, 2015), it 
attract relatively little notice until just over five year ago. Virtually everything 
change in 2012, with the publication of a series of highly influential paper such a 
Krizhevsky, Sutskever and Hinton’s 2012 ImageNet Classification with Deep 
Convolutional Neural Networks (Krizhevsky, Sutskever, & Hinton, 2012), which 
achieve state-of-the-art result on the object recognition challenge know a ImageNet 
(Deng et al., ). Other lab be already work on similar work (Cireşan, Meier, Masci, 
& Schmidhuber, 2012). Before the year be out, deep learn make the front page of 
The New York Times , and it rapidly become the best know technique in artificial 2 
intelligence, by a wide margin. If the general idea of training neural network with 
multiple layer be not new, it was, in part because of increase in computational power 
and data, the first time that deep learn truly become practical. 

Deep learn have since yield numerous state of the art results, in domain such a 
speech recognition, image recognition , and language translation and play a role in a 
wide swath of current AI applications. Corporations have invest billion of dollar 
fight for deep learn talent. One prominent deep learn advocate, Andrew Ng, have 
go so far to suggest that “If a typical person can do a mental task with less than one 
second of thought, we can probably automate it use AI either now or in the near 

http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-2 
intelligence.html 

Page ! of !2 27 

http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html 
http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html 
http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html 
https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf 
https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf 


future.” (A, 2016). A recent New York Times Sunday Magazine article , largely about 3 
deep learning, imply that the technique be “poised to reinvent compute itself.” 

Yet deep learn may well be approach a wall, much a I anticipate earlier, at 
begin of the resurgence (Marcus, 2012), and a lead figure like Hinton (Sabour, 
Frosst, & Hinton, 2017) and Chollet (2017) have begin to imply in recent months. 

What exactly be deep learning, and what have it show about the nature of intelligence? 
What can we expect it to do, and where might we expect it to break down? How close or 
far be we from “artificial general intelligence”, and a point at which machine show a 
human-like flexibility in solve unfamiliar problems? The purpose of this paper be both 
to temper some irrational exuberance and also to consider what we a a field might need 
to move forward. 

This paper be write simultaneously for researcher in the field, and for a grow set of 
AI consumer with less technical background who may wish to understand where the 
field be headed. As such I will begin with a very brief, nontechnical introduction aim at 4 
elucidate what deep learn system do well and why (Section 2), before turn to an 
assessment of deep learning’s weakness (Section 3) and some fear that arise from 
misunderstanding about deep learning’s capability (Section 4), and closing with 
perspective on go forward (Section 5). 

Deep learn be not likely to disappear, nor should it. But five year into the field’s 
resurgence seem like a good moment for a critical reflection, on what deep learn have 
and have not be able to achieve. 

2. What deep learn is, and what it do well 

Deep learning, a it be primarily used, be essentially a statistical technique for classify 
patterns, base on sample data, use neural network with multiple layers. 5 

https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html3 

For more technical introduction, there be many excellent recent tutorial on deep learn include (Chollet, 4 
2017) and (Goodfellow, Bengio, & Courville, 2016), a well a insightful blog and online resource from Zachary 
Lipton, Chris Olah, and many others. 

Other application of deep learn beyond classification be possible, too, though currently less popular, and 5 
outside of the scope of the current article. These include use deep learn a an alternative to regression, a a 
component in generative model that create (e.g.,) synthetic images, a a tool for compress images, a a tool for 
learn probability distributions, and (relatedly) a an important technique for approximation know a variational 
inference. 

Page ! of !3 27 



Neural network in the deep learn literature typically consist of a set of input unit that 
stand for thing like pixel or words, multiple hidden layer (the more such layers, the 
deeper a network be say to be) contain hidden unit (also know a node or neurons), 
and a set output units, with connection run between those nodes. In a typical 
application such a network might be train on a large set of handwritten digit (these 
be the inputs, represent a images) and label (these be the outputs) that identify the 
category to which those input belong (this image be a 2, that one be a 3, and so forth). 


Over time, an algorithm call back-propagation allows a process call gradient descent 
to adjust the connection between unit use a process, such that any give input tends to 
produce the correspond output. 

Collectively, one can think of the relation between input and output that a neural 
network learns a a mapping. Neural networks, particularly those with multiple hidden 
layer (hence the term deep) be remarkably good at learn input-output mappings, 

Such system be commonly described a neural network because the input nodes, 
hidden nodes, and output node can be thought of a loosely analogous to biological 
neurons, albeit greatly simplified, and the connection between node can be thought of 
a in some way reflect connection between neurons. A longstanding question, outside 
the scope of the current paper, concern the degree to which artificial neural network be 
biologically plausible. 

Most deep learn network make heavy use of a technique call convolution (LeCun, 
1989), which constrains the neural connection in the network such that they innately 
capture a property know a translational invariance. This be essentially the idea that an 
object can slide around an image while maintain it identity; a circle in the top left can 
be presumed, even absent direct experience) to be the same a a circle in the bottom right. 

Page ! of !4 27 

Input layer Output layer 
Hidden layer 

... 
... ... 

... 



Deep learn be also know for it ability to self-generate intermediate representations, 
such a internal unit that may respond to thing like horizontal lines, or more complex 
element of pictorial structure. 

In principle, give infinite data, deep learn system be powerful enough to represent 
any finite deterministic “mapping” between any give set of input and a set of 
correspond outputs, though in practice whether they can learn such a mapping 
depends on many factors. One common concern be get caught in local minima, in 
which a system get stuck on a suboptimal solution, with no good solution nearby in the 
space of solution be searched. (Experts use a variety of technique to avoid such 
problems, to reasonably good effect). In practice, result with large data set be often 
quite good, on a wide range of potential mappings. 

In speech recognition, for example, a neural network learns a mapping between a set of 
speech sounds, and set of label (such a word or phonemes). In object recognition, a 
neural network learns a mapping between a set of image and a set of label (such that, 
for example, picture of car be label a cars). In DeepMind’s Atari game system 
(Mnih et al., 2015), neural network learn mapping between pixel and joystick 
positions. 

Deep learn system be most often use a classification system in the sense that the 
mission of a typical network be to decide which of a set of category (defined by the 
output unit on the neural network) a give input belongs to. With enough imagination, 
the power of classification be immense; output can represent words, place on a Go 
board, or virtually anything else. 

In a world with infinite data, and infinite computational resources, there might be little 
need for any other technique. 

3. Limits on the scope of deep learn 

Deep learning’s limitation begin with the contrapositive: we live in a world in which 
data be never infinite. Instead, system that rely on deep learn frequently have to 
generalize beyond the specific data that they have seen, whether to a new pronunciation 
of a word or to an image that differs from one that the system have see before, and where 
data be less than infinite, the ability of formal proof to guarantee high-quality 
performance be more limited. 

Page ! of !5 27 



As discuss late in this article, generalization can be thought of a come in two 
flavors, interpolation between know examples, and extrapolation, which require go 
beyond a space of know training example (Marcus, 1998a). 

For neural network to generalize well, there generally must be a large amount of data, 
and the test data must be similar to the training data, allow new answer to be 
interpolate in between old ones. In Krizhevsky et al’s paper (Krizhevsky, Sutskever, & 
Hinton, 2012), a nine layer convolutional neural network with 60 million parameter and 
650,000 node be train on roughly a million distinct example drawn from 
approximately one thousand categories. 6 

This sort of brute force approach work well in the very finite world of ImageNet, into 
which all stimulus can be classify into a comparatively small set of categories. It also 
work well in stable domain like speech recognition in which exemplar be mapped in 
constant way onto a limited set of speech sound categories, but for many reason deep 
learn cannot be consider (as it sometimes be in the popular press) a a general 
solution to artificial intelligence. 

Here be ten challenge face by current deep learn systems: 

3.1. Deep learn thus far be data hungry 

Human being can learn abstract relationship in a few trials. If I told you that a schmister 
be a sister over the age of 10 but under the age of 21, perhaps give you a single 
example, you could immediately infer whether you have any schmisters, whether your best 
friend have a schmister, whether your child or parent have any schmisters, and so forth. 
(Odds are, your parent no longer do, if they ever did, and you could rapidly draw that 
inference, too.) 

In learn what a schmister is, in this case through explicit definition, you rely not on 
hundred or thousand or million of training examples, but on a capacity to represent 
abstract relationship between algebra-like variables. 

Humans can learn such abstractions, both through explicit definition and more implicit 
mean (Marcus, 2001). Indeed even 7-month old infant can do so, acquire learn 
abstract language-like rule from a small number of unlabeled examples, in just two 

Using a common technique know a data augmentation, each example be actually present along with it label 6 
in a many different locations, both in it original form and in mirror reverse form. A second type of data 
augmentation varied the brightness of the images, yield still more example for training, in order to train the 
network to recognize image with different intensities. Part of the art of machine learn involves know what 
form of data augmentation will and won’t help within a give system. 

Page ! of !6 27 



minute (Marcus, Vijayan, Bandi Rao, & Vishton, 1999). Subsequent work by Gervain 
and colleague (2012) suggests that newborn be capable of similar computations. 

Deep learn currently lack a mechanism for learn abstraction through explicit, 
verbal definition, and work best when there be thousands, million or even billion of 
training examples, a in DeepMind’s work on board game and Atari. As Brenden Lake 
and his colleague have recently emphasize in a series of papers, human be far more 
efficient in learn complex rule than deep learn system be (Lake, Salakhutdinov, 
& Tenenbaum, 2015; Lake, Ullman, Tenenbaum, & Gershman, 2016). (See also related 
work by George et al (2017), and my own work with Steven Pinker on children’s 
overregularization error in comparison to neural network (Marcus et al., 1992).) 

Geoff Hinton have also worried about deep learning’s reliance on large number of label 
examples, and express this concern in his recent work on capsule network with his 
coauthor (Sabour et al., 2017) note that convolutional neural network (the most 
common deep learn architecture) may face “exponential inefficiency that may lead to 
their demise. A good candidate be the difficulty that convolutional net have in 
generalize to novel viewpoint [ie perspective on object in visual recognition tasks]. 
The ability to deal with translation[al invariance] be built in, but for the other ... [common 
type of] transformation we have to chose between replicate feature detector on a grid 
that grows exponentially ... or increase the size of the label training set in a similarly 
exponential way.” 

In problem where data be limited, deep learn often be not an ideal solution. 

3.2.Deep learn thus far be shallow and have limited capacity for 
transfer 

Although deep learn be capable of some amaze things, it be important to realize that 
the word “deep” in deep learn refers to a technical, architectural property (the large 
number of hidden layer use in a modern neural networks, where there predecessor 
use only one) rather than a conceptual one (the representation acquire by such 
network don’t, for example, naturally apply to abstract concept like “justice”, 
“democracy” or “meddling”). 

Even more down-to-earth concept like “ball” or “opponent” can lie out of reach. 
Consider for example DeepMind’s Atari game work (Mnih et al., 2015) on deep 
reinforcement learning, which combine deep learn with reinforcement learn (in 
which a learner try to maximize reward). Ostensibly, the result be fantastic: the system 
meet or beat human expert on a large sample of game use a single set of 
“hyperparameters” that govern property such a the rate at which a network alters it 
weights, and no advance knowledge about specific games, or even their rules. But it be 

Page ! of !7 27 



easy to wildly overinterpret what the result show. To take one example, accord to a 
widely-circulated video of the system learn to play the brick-breaking Atari game 
Breakout, “after 240 minute of training, [the system] realizes that dig a tunnel 
thought the wall be the most effective technique to beat the game”. 

But the system have learn no such thing; it doesn’t really understand what a tunnel, or 
what a wall is; it have just learn specific contingency for particular scenarios. Transfer 
test — in which the deep reinforcement learn system be confront with scenario 
that differ in minor way from the one one on which the system be train show that 
deep reinforcement learning’s solution be often extremely superficial. For example, a 
team of researcher at Vicarious show that a more efficient successor technique, 
DeepMind’s Atari system [Asynchronous Advantage Actor-Critic; also know a A3C], 
fail on a variety of minor perturbation to Breakout (Kansky et al., 2017) from the 
training set, such a move the Y coordinate (height) of the paddle, or insert a wall 
midscreen. These demonstration make clear that it be mislead to credit deep 
reinforcement learn with induce concept like wall or paddle; rather, such remark 
be what comparative (animal) psychology sometimes call overattributions. It’s not that 
the Atari system genuinely learn a concept of wall that be robust but rather the system 
superficially approximate break through wall within a narrow set of highly train 
circumstances. 7 

My own team of researcher at a startup company call Geometric Intelligence (later 
acquire by Uber) found similar result a well, in the context of a slalom game, In 2017, 
a team of researcher at Berkeley and OpenAI have show that it be not difficult to 
construct comparable adversarial example in a variety of games, undermine not only 
DQN (the original DeepMind algorithm) but also A3C and several other related 
technique (Huang, Papernot, Goodfellow, Duan, & Abbeel, 2017). 

Recent experiment by Robin Jia and Percy Liang (2017) make a similar point, in a 
different domain: language. Various neural network be train on a question 
answer task know a SQuAD (derived from the Stanford Question Answering 
Database), in which the goal be to highlight the word in a particular passage that 
correspond to a give question. In one sample, for instance, a train system correctly, 
and impressively, identify the quarterback on the win of Super Bowl XXXIII a 
John Elway, base on a short paragraph. But Jia and Liang show the mere insertion of 
distractor sentence (such a a fictional one about the allege victory of Google’s Jeff 

In the same paper, Vicarious propose an alternative to deep learn call schema network (Kansky et al., 2017) 7 
that can handle a number of variation in the Atari game Breakout, albeit apparently without the multi-game 
generality of DeepMind’s Atari system. 

Page ! of !8 27 



Dean in another Bowl game ) cause performance to drop precipitously. Across sixteen 8 
models, accuracy drop from a mean of 75% to a mean of 36%. 

As be so often the case, the pattern extract by deep learn be more superficial than 
they initially appear. 

3.3.Deep learn thus far have no natural way to deal with 
hierarchical structure 

To a linguist like Noam Chomsky, the trouble Jia and Liang document would be 
unsurprising. Fundamentally, most current deep-learning base language model 
represent sentence a mere sequence of words, whereas Chomsky have long argue that 
language have a hierarchical structure, in which large structure be recursively 
construct out of small components. (For example, in the sentence the teenager who 
previously cross the Atlantic set a record for fly around the world, the main clause be 
the teenager set a record for fly around the world, while the embed clause who 
previously cross the Atlantic be an embed clause that specifies which teenager.) 

In the 80’s Fodor and Pylyshyn (1988)expressed similar concerns, with respect to an 
early breed of neural networks. Likewise, in (Marcus, 2001), I conjecture that single 
recurrent neural network (SRNs; a forerunner to today’s more sophisticated deep 
learn base recurrent neural networks, know a RNNs; Elman, 1990) would have 
trouble systematically represent and extend recursive structure to various kind of 
unfamiliar sentence (see the cite article for more specific claim about which types). 

Earlier this year, Brenden Lake and Marco Baroni (2017) test whether such pessimistic 
conjecture continued to hold true. As they put it in their title, contemporary neural net 
be “Still not systematic after all these years”. RNNs could “generalize well when the 
difference between training and test ... be small [but] when generalization require 
systematic compositional skills, RNNs fail spectacularly”. 

Similar issue be likely to emerge in other domains, such a planning and motor control, 
in which complex hierarchical structure be needed, particular when a system be likely to 
encounter novel situations. One can see indirect evidence for this in the struggle with 
transfer in Atari game mention above, and more generally in the field of robotics, in 
which system generally fail to generalize abstract plan well in novel environments. 

Here’s the full Super Bowl passage; Jia and Liang’s distractor sentence that confuse the model be at the end. 8 
Peyton Manning become the first quarterback ever to lead two different team to multiple Super Bowls. He be also 
the old quarterback ever to play in a Super Bowl at age 39. The past record be held by John Elway, who lead the 
Broncos to victory in Super Bowl XXXIII at age 38 and be currently Denver’s Executive Vice President of Football 
Operations and General Manager. Quarterback Jeff Dean have jersey number 37 in Champ Bowl XXXIV. 

Page ! of !9 27 



The core problem, at least at present, be that deep learn learns correlation between 
set of feature that be themselves “flat” or nonhierachical, a if in a simple, unstructured 
list, with every feature on equal footing. Hierarchical structure (e.g., syntactic tree that 
distinguish between main clause and embed clause in a sentence) be not inherently 
or directly represent in such systems, and a a result deep learn system be force 
to use a variety of proxy that be ultimately inadequate, such a the sequential position 
of a word present in a sequences. 

Systems like Word2Vec (Mikolov, Chen, Corrado, & Dean, 2013) that represent 
individual word a vector have be modestly successful; a number of system that 
have use clever trick 
try to represent complete sentence in deep-learning compatible vector space (Socher, 
Huval, Manning, & Ng, 2012). But, a Lake and Baroni’s experiment make clear. 
recurrent network continue limited in their capacity to represent and generalize rich 
structure in a faithful manner. 

3.4.Deep learn thus far have struggle with open-ended inference 
If you can’t represent nuance like the difference between “John promise Mary to leave” 
and “John promise to leave Mary”, you can’t draw inference about who be leave 
whom, or what be likely to happen next. Current machine reading system have achieve 
some degree of success in task like SQuAD, in which the answer to a give 
question be explicitly contain within a text, but far less success in task in which 
inference go beyond what be explicit in a text, either by combine multiple sentence 
(so call multi-hop inference) or by combine explicit sentence with background 
knowledge that be not state in a specific text selection. Humans, a they read texts, 
frequently derive wide-ranging inference that be both novel and only implicitly 
licensed, a when they, for example, infer the intention of a character base only on 
indirect dialog. 

Altough Bowman and colleague (Bowman, Angeli, Potts, & Manning, 2015; Williams, 
Nangia, & Bowman, 2017) have take some important step in this direction, there is, at 
present, no deep learn system that can draw open-ended inference base on real- 
world knowledge with anything like human-level accuracy. 

3.5.Deep learn thus far be not sufficiently transparent 
The relative opacity of “black box” neural network have be a major focus of discussion 
in the last few year (Samek, Wiegand, & Müller, 2017; Ribeiro, Singh, & Guestrin, 
2016). In their current incarnation, deep learn system have million or even billion 
of parameters, identifiable to their developer not in term of the sort of human 

Page ! of !10 27 



interpretable label that canonical programmer use (“last_character_typed”) but only in 
term of their geography within a complex network (e.g., the activity value of the ith node 
in layer j in network module k). Although some stride have be in visualize the 
contribution of individual node in complex network (Nguyen, Clune, Bengio, 
Dosovitskiy, & Yosinski, 2016), most observer would acknowledge that neural network 
a a whole remain something of a black box. 

How much that matter in the long run remains unclear (Lipton, 2016). If system be 
robust and self-contained enough it might not matter; if it be important to use them in the 
context of large systems, it could be crucial for debuggability. 

The transparency issue, a yet unsolved, be a potential liability when use deep learn 
for problem domain like financial trade or medical diagnosis, in which human user 
might like to understand how a give system make a give decision. As Catherine 
O’Neill (2016) have point out, such opacity can also lead to serious issue of bias. 


3.6.Deep learn thus far have not be well integrate with prior 
knowledge 

The dominant approach in deep learn be hermeneutic, in the sense of be self- 
contain and isolated from other, potentially usefully knowledge. Work in deep learn 
typically consists of find a training database, set of input associate with respective 
outputs, and learn all that be require for the problem by learn the relation between 
those input and outputs, use whatever clever architectural variant one might devise, 
along with technique for cleaning and augment the data set. With just a handful of 
exceptions, such a LeCun’s convolutional constraint on how neural network be 
wired(LeCun, 1989), prior knowledge be often deliberately minimized. 

Thus, for example, in a system like Lerer et al’s (2016) effort to learn about the physic 
of fall towers, there be no prior knowledge of physic (beyond what be imply in 
convolution). Newton’s laws, for example, be not explicitly encoded; the system instead 
(to some limited degree) approximates them by learn contingency from raw, pixel 
level data. As I note in a forthcoming paper in innate (Marcus, in prep) researcher in 
deep learn appear to have a very strong bias against include prior knowledge even 
when (as in the case of physics) that prior knowledge be well known. 

It also not straightforward in general how to integrate prior knowledge into a deep 
learn system:, in part because the knowledge represent in deep learn system 
pertains mainly to (largely opaque) correlation between features, rather than to 
abstraction like quantify statement (e.g. all men be mortal), see discussion of 
universally-quantified one-to-one-mappings in Marcus (2001), or generic (violable 

Page ! of !11 27 



statement like dog have four leg or mosquito carry West Nile virus (Gelman, Leslie, 
Was, & Koch, 2015)). 

A related problem stem from a culture in machine learn that emphasizes competition 
on problem that be inherently self-contained, without little need for broad general 
knowledge. This tendency be well exemplify by the machine learn contest platform 
know a Kaggle, in which contestant vie for the best result on a give data set. 
Everything they need for a give problem be neatly packaged, with all the relevant input 
and output files. Great progress have be make in this way; speech recognition and some 
aspect of image recognition can be largely solve in the Kaggle paradigm. 

The trouble, however, be that life be not a Kaggle competition; child don’t get all the 
data they need neatly package in a single directory. Real-world learn offer data 
much more sporadically, and problem aren’t so neatly encapsulated. Deep learn 
work great on problem like speech recognition in which there be lot of label 
examples, but scarcely any even know how to apply it to more open-ended problems. 
What’s the best way to fix a bicycle that have a rope caught in it spokes? Should I major 
in math or neuroscience? No training set will tell u that. 

Problems that have less to do with categorization and more to do with commonsense 
reason essentially lie outside the scope of what deep learn be appropriate for, and so 
far a I can tell, deep learn have little to offer such problems. In a recent review of 
commonsense reasoning, Ernie Davis and I (2015) begin with a set of easily-drawn 
inference that people can readily answer without anything like direct training, such a 
Who be taller, Prince William or his baby son Prince George? Can you make a salad out 
of a polyester shirt? If you stick a pin into a carrot, do it make a hole in the carrot or in the 
pin? 

As far a I know, nobody have even try to tackle this sort of thing with deep learning. 

Such apparently simple problem require human to integrate knowledge across vastly disparate 
sources, and a such be a long way from the sweet spot of deep learning-style perceptual 
classification. Instead, they be perhaps best thought of a a sign that entirely different 
sort of tool be needed, along with deep learning, if we be to reach human-level 
cognitive flexibility. 

3.7.Deep learn thus far cannot inherently distinguish causation 
from correlation 

If it be a truism that causation do not equal correlation, the distinction between the two 
be also a serious concern for deep learning. Roughly speaking, deep learn learns 
complex correlation between input and output features, but with no inherent 

Page ! of !12 27 



representation of causality. A deep learn system can easily learn that height and 
vocabulary are, across the population a a whole, correlated, but less easily represent the 
way in which that correlation derives from growth and development (kids get big a 
they learn more words, but that doesn’t mean that grow tall cause them to learn more 
words, nor that learn new word cause them to grow). Causality have be central 
strand in some other approach to AI (Pearl, 2000) but, perhaps because deep learn be 
not gear towards such challenges, relatively little work within the deep learn 
tradition have try to address it. 9 

3.8.Deep learn presumes a largely stable world, in way that may 
be problematic 

The logic of deep learn be such that it be likely to work best in highly stable worlds, 
like the board game Go, which have unvarying rules, and less well in system such a 
politics and economics that be constantly changing. To the extent that deep learn be 
apply in task such a stock prediction, there be a good chance that it will eventually 
face the fate of Google Flu Trends, which initially do a great job of predict 
epidemological data on search trends, only to complete miss thing like the peak of the 
2013 flu season (Lazer, Kennedy, King, & Vespignani, 2014). 

3.9. Deep learn thus far work well a an approximation, but it 
answer often cannot be fully trust 

In part a a consequence of the other issue raise in this section, deep learn system 
be quite good at some large fraction of a give domain, yet easily fooled. 

An ever-growing array of paper have show this vulnerability, from the linguistic 
example of Jia and Liang mention above to a wide range of demonstration in the 
domain of vision, where deep learn system have mistaken yellow-and-black pattern 
of stripe for school bus (Nguyen, Yosinski, & Clune, 2014) and sticker-clad parking 
sign for well-stocked refrigerator (Vinyals, Toshev, Bengio, & Erhan, 2014) in the 
context of a caption system that otherwise seem impressive. 

More recently, there have be real-world stop signs, lightly defaced, that have be 
mistaken for speed limit sign (Evtimov et al., 2017) and 3d-printed turtle that have be 
mistake for rifle (Athalye, Engstrom, Ilyas, & Kwok, 2017). A recent news story 

One example of interest recent work be (Lopez-Paz, Nishihara, Chintala, Schölkopf, & Bottou, 2017), albeit 9 
focus specifically on an rather unusual sense of the term causation a it relates to the presence or absence of 
object (e.g., “the presence of car cause the presence of wheel[s]). This strike me a quite different from the sort of 
causation one find in the relation between a disease and the symptom it causes. 

Page ! of !13 27 



recount the trouble a British police system have have in distinguish nude from sand 
dunes. 10 

The “spoofability” of deep learn system be perhaps first note by Szegedy et 
al(2013). Four year later, despite much active research, no robust solution have be 
found. 11 

3.10. Deep learn thus far be difficult to engineer with 
Another fact that follow from all the issue raise above be that be simply hard to do 
robust engineering with deep learning. As a team of author at Google put it in 2014, in 
the title of an important, and a yet unanswered essay (Sculley, Phillips, Ebner, 
Chaudhary, & Young, 2014), machine learn be “the high-interest credit card of 
technical debt”, meaning that be comparatively easy to make system that work in some 
limited set of circumstance (short term gain), but quite difficult to guarantee that they 
will work in alternative circumstance with novel data that may not resemble previous 
training data (long term debt, particularly if one system be use a an element in another 
large system). 

In an important talk at ICML, Leon Bottou (2015) compare machine learn to the 
development of an airplane engine, and note that while the airplane design relies on 
building complex system out of simpler system for which it be possible to create 
sound guarantee about performance, machine learn lack the capacity to produce 
comparable guarantees. As Google’s Peter Norvig (2016) have noted, machine learn a 
yet lack the incrementality, transparency and debuggability of classical programming, 
trading off a kind of simplicity for deep challenge in achieve robustness. 

Henderson and colleague have recently extend these points, with a focus on deep 
reinforcement learning, note some serious issue in the field related to robustness and 
replicability (Henderson et al., 2017). 

Although there have be some progress in automate the process of develop machine 
learn system (Zoph, Vasudevan, Shlens, & Le, 2017), there be a long way to go. 

https://gizmodo.com/british-cops-want-to-use-ai-to-spot-porn-but-it-keeps-m-1821384511/amp10 

Deep learning’s predecessor be vulnerable to similar problems, a Pinker and Prince (1988)pointed out, in a 11 
discussion of neural network that produce bizarre past tense form for a subset of it inputs. The verb to mail, for 
example, be inflect in the past tense a membled, the verb tour a toureder. Children rarely if ever make mistake 
like these. 

Page ! of !14 27 



3.11. Discussion 

Of course, deep learning, be by itself, just mathematics; none of the problem identify 
above be because the underlie mathematics of deep learn be somehow flawed. In 
general, deep learn be a perfectly fine way of optimize a complex system for 
represent a mapping between input and outputs, give a sufficiently large data set. 

The real problem lie in misunderstand what deep learn is, and be not, good for. The 
technique excels at solve closed-end classification problems, in which a wide range of 
potential signal must be mapped onto a limited number of categories, give that there be 
enough data available and the test set closely resembles the training set. 

But deviation from these assumption can cause problems; deep learn be just a 
statistical technique, and all statistical technique suffer from deviation from their 
assumptions. 

Deep learn system work less well when there be limited amount of training data 
available, or when the test set differs importantly from the training set, or when the space 
of example be broad and fill with novelty. And some problem cannot, give real- 
world limitations, be thought of a classification problem at all. Open-ended natural 
language understanding, for example, should not be thought of a a classifier mapping 
between a large finite set of sentence and large, finite set of sentences, but rather a 
mapping between a potentially infinite range of input sentence and an equally vast array 
of meanings, many never previously encountered. In a problem like that, deep learn 
becomes a square peg slam into a round hole, a crude approximation when there 
must be a solution elsewhere. 

One clear way to get an intuitive sense of why something be amiss to consider a set of 
experiment I do long ago, in 1997, when I test some simplify aspect of language 
development on a class of neural network that be then popular in cognitive science. 
The 1997-vintage network were, to be sure, simpler than current model — they use no 
more than three layer (inputs node connect to hidden node connect to output 
node), and lack Lecun’s powerful convolution technique. But they be driven by 
backpropagation just a today’s system are, and just a beholden to their training data. 

In language, the name of the game be generalization — once I hear a sentence like John 
pilked a football to Mary, I can infer that be also grammatical to say John pilked Mary the 
football, and Eliza pilked the ball to Alec; equally if I can infer what the word pilk means, 
I can infer what the latter sentence would mean, even if I have not hear them before. 

Page ! of !15 27 



Distilling the broad-ranging problem of language down to a simple example that I 
believe still have resonance now, I ran a series of experiment in which I train three- 
layer perceptrons (fully connect in today’s technical parlance, with no convolution) on 
the identity function, f(x) = x, e.g, f(12)=12. 

Training example be represent by a set of input node (and correspond output 
nodes) that represent number in term of binary digits. The number 7 for example, 
would be represent by turn on the input (and output) node represent 4, 2, and 1. 
As a test of generalization, I train the network on various set of even numbers, and 
test it all possible inputs, both odd and even. 

Every time I ran the experiment, use a wide variety of parameters, the result be the 
same: the network would (unless it get stuck in local minimum) correctly apply the 
identity function to the even number that it have see before (say 2, 4, 8 and 12), and to 
some other even number (say 6 and 14) but fail on all the odds numbers, yielding, for 
example f(15) = 14. 

In general, the neural net I test could learn their training examples, and interpolate to a 
set of test example that be in a cloud of point around those example in n- 
dimensional space (which I dubbed the training space), but they could not extrapolate 
beyond that training space. 

Odd number be outside the training space, and the network could not generalize 
identity outside that space. Adding more hidden unit didn’t help, and nor do add 12 
more hidden layers. Simple multilayer perceptrons simply couldn’t generalize outside 
their training space (Marcus, 1998a; Marcus, 1998b; Marcus, 2001). (Chollet make quite 
similar point in the closing chapter of his his (Chollet, 2017) text.) 

What we have see in this paper be that challenge in generalize beyond a space of 
training example persist in current deep learn networks, nearly two decade later. 
Many of the problem review in this paper — the data hungriness, the vulnerability to 
fooling, the problem in deal with open-ended inference and transfer — can be see 
a extension of this fundamental problem. Contemporary neural network do well on 
challenge that remain close to their core training data, but start to break down on case 
further out in the periphery. 

Of course, the network have never see an odd number before, but pretraining the network on odd number in a 12 
different context didn’t help. And of course people, in contrast, readily generalize to novel word immediately upon 
hearing them. Likewise, the experiment I do with seven-month-olds consist entirely of novel words. 

Page ! of !16 27 



The widely-adopted addition of convolution guarantee that one particular class of 
problem that be akin to my identity problem can be solved: so-called translational 
invariances, in which an object retains it identity when it be shift to a location. But the 
solution be not general, a for example Lake’s recent demonstration show. (Data 
augmentation offer another way of deal with deep learning’s challenge in 
extrapolation, by try to broaden the space of training example itself, but such 
technique be more useful in 2d vision than in language). 

As yet there be no general solution within deep learn to the problem of generalize 
outside the training space. And it be for that reason, more than any other, that we need to 
look to different kind of solution if we want to reach artificial general intelligence. 

4. Potential risk of excessive hype 

One of the big risk in the current overhyping of AI be another AI winter, such a the 
one that devastate the field in the 1970’s, after the Lighthill report (Lighthill, 1973), 
suggest that AI be too brittle, too narrow and too superficial to be use in practice. 
Although there be vastly more practical application of AI now than there be in the 
1970s, hype be still a major concern. When a high-profile figure like Andrew Ng writes in 
the Harvard Business Review promising a degree of imminent automation that be out of 
step with reality, there be fresh risk for seriously dash expectations. Machines cannot in 
fact do many thing that ordinary human can do in a second, range from reliably 
comprehend the world to understand sentences. No healthy human be would 
ever mistake a turtle for a rifle or parking sign for a refrigerator. 

Executives invest massively in AI may turn out to be disappointed, especially give 
the poor state of the art in natural language understanding. Already, some major project 
have be largely abandoned, like Facebook’s M project, which be launch in August 
2015 with much publicity a a general purpose personal assistant, and then late 13 
downgrade to a significantly small role, help user with a vastly small range of 
well-defined task such a calendar entry. 

It be probably fair to say that chatbots in general have not live up to the hype they 
receive a couple year ago. If, for example, driverless car should also, disappoint, 
relative to their early hype, by prove unsafe when roll out at scale, or simply not 
achieve full autonomy after many promises, the whole field of AI could be in for a 
sharp downturn, both in popularity and funding. We already may be see hint of this, 

https://www.wired.com/2015/08/how-facebook-m-works/13 

Page ! of !17 27 



a in a just publish Wired article that be entitle “After peak hype, self-driving car 14 
enter the trough of disillusionment.” 

There be other serious fears, too, and not just of the apocalyptic variety (which for now 
to still seem to be stuff of science fiction). My own large fear be that the field of AI 
could get trap in a local minimum, dwell too heavily in the wrong part of 
intellectual space, focus too much on the detailed exploration of a particular class of 
accessible but limited model that be gear around capture low-hanging fruit — 
potentially neglect riskier excursion that might ultimately lead to a more robust path. 

I be remind of Peter Thiel’s famous (if now slightly outdated) damn of an often 
too-narrowly focus tech industry: “We want fly cars, instead we get 140 
characters”. I still dream of Rosie the Robost, a full-service domestic robot that take of 
my home; but for now, six decade into the history of AI, our bot do little more than play 
music, sweep floors, and bid on advertisements. 

If didn’t make more progress, it would be a shame. AI come with risk, but also great 
potential rewards. AI’s great contribution to society, I believe, could and should 
ultimately come in domain like automate scientific discovery, lead among other 
thing towards vastly more sophisticated version of medicine than be currently possible. 
But to get there we need to make sure that the field a whole doesn’t first get stuck in a 
local minimum. 

5. What would be better? 

Despite all of the problem I have sketched, I don’t think that we need to abandon deep 
learning. 

Rather, we need to reconceptualize it: not a a universal solvent, but simply a one tool 
among many, a power screwdriver in a world in which we also need hammers, wrenches, 
and pliers, not to mention chisel and drills, voltmeters, logic probes, and oscilloscopes. 

In perceptual classification, where vast amount of data be available, deep learn be a 
valuable tool; in other, richer cognitive domains, it be often far less satisfactory. 

The question is, where else should we look? Here be four possibilities. 

https://www.wired.com/story/self-driving-cars-challenges/14 

Page ! of !18 27 

https://www.wired.com/story/self-driving-cars-challenges/ 


5.1.Unsupervised learn 

In interviews, deep learn pioneer Geoff Hinton and Yann LeCun have both recently 
point to unsupervised learn a one key way in which to go beyond supervised, data- 
hungry version of deep learning. 

To be clear, deep learn and unsupervised learn be not in logical opposition. Deep 
learn have mostly be use in a supervise context with label data, but there be 
way of use deep learn in an unsupervised fashion. But there be certainly reason in 
many domain to move away from the massive demand on data that supervise deep 
learn typically requires. 

Unsupervised learning, a the term be commonly used, tends to refer to several kind of 
systems. One common type of system “clusters” together input that share properties, 
even without have them explicitly labeled. Google’s cat detector model (Le et al., 2012) 
be perhaps the most publicly prominent example of this sort of approach. 

Another approach, advocate researcher such a Yann LeCun (Luc, Neverova, Couprie, 
Verbeek, & LeCun, 2017), and not mutually exclusive with the first, be to replace label 
data set with thing like movie that change over time. The intuition be that system 
train on video can use each pair of successive frame a a kind of ersatz teach 
signal, in which the goal be to predict the next frame; frame t becomes a predictor for 
frame t1, without the need for any human labeling. 

My view be that both of these approach be useful (and so be some others not discuss 
here), but that neither inherently solve the sort of problem outline in section 3. One be 
still left with data hungry system that lack explicit variables, and I see no advance there 
towards open-ended inference, interpretability or debuggability. 

That said, there be a different notion of unsupervised learning, less discussed, which I find 
deeply interesting: the kind of unsupervised learn that human child do. Children 
often y set themselves a novel task, like create a tower of Lego brick or climb 
through a small aperture, a my daughter recently do in climb through a chair, in the 
space between the seat and the chair back . Often, this sort of exploratory problem 
solve involves (or at least appear to involve) a good deal of autonomous goal set 
(what should I do?) and high level problem solve (how do I get my arm through the 
chair, now that the rest of my body have pass through?), a well the integration of 
abstract knowledge (how body work, what sort of aperture and affordances various 
object have, and so forth). If we could build system that could set their own goal and 
do reason and problem-solving at this more abstract level, major progress might 
quickly follow. 

Page ! of !19 27 



5.2.Symbol-manipulation, and the need for hybrid model 

Another place that we should look be towards classic, “symbolic” AI, sometimes refer 
to a GOFAI (Good Old-Fashioned AI). Symbolic AI take it name from the idea, central 
to mathematics, logic, and computer science, that abstraction can be represent by 
symbols. Equations like f = ma allow u to calculate output for a wide range of inputs, 
irrespective of whether we have see any particular value before; line in computer 
program do the same thing (if the value of variable x be great than the value of variable 
y, perform action a). 

By themselves, symbolic system have often proven to be brittle, but they be largely 
developed in era with vastly less data and computational power than we have now. The 
right move today may be to integrate deep learning, which excels at perceptual 
classification, with symbolic systems, which excel at inference and abstraction. One 
might think such a potential merger on analogy to the brain; perceptual input systems, 
like primary sensory cortex, seem to do something like what deep learn does, but there 
be other areas, like Broca’s area and prefrontal cortex, that seem to operate at much 
high level of abstraction. The power and flexibility of the brain come in part from it 
capacity to dynamically integrate many different computation in real-time. The process 
of scene perception, for instance, seamlessly integrates direct sensory information with 
complex abstraction about object and their properties, light sources, and so forth. 

Some tentative step towards integration already exist, include neurosymbolic 
model (Besold et al., 2017) and recent trend towards system such a differentiable 
neural computer (Graves et al., 2016), program with differentiable interpreter 
(Bošnjak, Rocktäschel, Naradowsky, & Riedel, 2016), and neural program with 
discrete operation (Neelakantan, Le, Abadi, McCallum, & Amodei, 2016). While none 
of this work have yet fully scale towards anything like full-service artificial general 
intelligence, I have long argue (Marcus, 2001) that more on integrate microprocessor- 
like operation into neural network could be extremely valuable. 

To the extent that the brain might be see a consist of “a broad array of reusable 
computational primitives—elementary unit of processing akin to set of basic 
instruction in a microprocessor—perhaps wire together in parallel, a in the 
reconfigurable integrate circuit type know a the field-programmable gate array”, a I 
have argue elsewhere(Marcus, Marblestone, & Dean, 2014), step towards enrich the 
instruction set out of which our computational system be built can only be a good thing. 

Page ! of !20 27 



5.3.More insight from cognitive and developmental psychology 

Another potential valuable place to look be human cognition (Davis & Marcus, 2015; 
Lake et al., 2016; Marcus, 2001; Pinker & Prince, 1988). There be no need for machine 
to literally replicate the human mind, which is, after all, deeply error prone, and far from 
perfect. But there remain many areas, from natural language understand to 
commonsense reasoning, in which human still retain a clear advantage; learn the 
mechanism underlie those human strength could lead to advance in AI, even the 
goal be not, and should not be, an exact replica of human brain. 

For many people, learn from human mean neuroscience; in my view, that may be 
premature. We don’t yet know enough about neuroscience to literally reverse engineer the 
brain, per se, and may not for several decades, possibly until AI itself get better. AI can 
help u to decipher the brain, rather than the other way around. 

Either way, in the meantime, it should certainly be possible to use technique and insight 
drawn from cognitive and developmental and psychology, now, in order to build more 
robust and comprehensive artificial intelligence, building model that be motivate not 
just by mathematics but also by clue from the strength of human psychology. 

A good start point might be to first to try understand the innate machinery in human 
minds, a a source of hypothesis into mechanism that might be valuable in develop 
artificial intelligences; in companion article to this one (Marcus, in prep) I summarize a 
number of possibilities, some drawn from my own early work (Marcus, 2001) and 
others from Elizabeth Spelke’s (Spelke & Kinzler, 2007). Those drawn from my own 
work focus on how information might be represent and manipulated, such a by 
symbolic mechanism for represent variable and distinction between kind and 
individual from a class; those drawn from Spelke focus on how infant might represent 
notion such a space, time, and object. 

A second focal point might be on common sense knowledge, both in how it develops 
(some might be part of our innate endowment, much of it be learned), how it be 
represented, and how it be integrate on line in the process of our interaction with the 
real world (Davis & Marcus, 2015). Recent work by Lerer et al (2016), Watters and 
colleague (2017), Tenenbaum and colleagues(Wu, Lu, Kohli, Freeman, & Tenenbaum, 
2017) and Davis and myself (Davis, Marcus, & Frazier-Logue, 2017) suggest some 
compete approach to how to think about this, within the domain of everyday physical 
reasoning. 

Page ! of !21 27 



A third focus might be on human understand of narrative, a notion long ago suggest 
by Roger Schank and Abelson (1977) and due for a refresh (Marcus, 2014; Kočiský et al., 
2017). 

5.4.Bolder challenge 
Whether deep learn persists in current form, morphs into something new, or get 
replace altogether, one might consider a variety of challenge problem that push system 
to move beyond what can be learn in supervise learn paradigm with large 
datasets. Drawing in part of from a recent special issue of AI Magazine devote to 
move beyond the Turing Test that I edit with Francesca Rossi, Manuelo Veloso 
(Marcus, Rossi, Veloso - AI Magazine, & 2016, 2016), here be a few suggestions: 

• A comprehension challenge (Paritosh & Marcus, 2016; Kočiský et al., 2017)] which 
would require a system to watch an arbitrary video (or read a text, or listen to a 
podcast) and answer open-ended question about what be contain therein. (Who be the 
protagonist? What be their motivation? What will happen if the antagonist succeed in 
her mission?) No specific supervise training set can cover all the possible 
contingencies; infererence and real-world knowledge integration be necessities. 

• Scientific reason and understanding, a in the Allen AI institute’s 8th grade science 
challenge (Schoenick, Clark, Tafjord, P, & Etzioni, 2017; Davis, 2016). While the 
answer to many basic science question can simply be retrieve from web searches, 
others require inference beyond what be explicitly stated, and the integration of general 
knowledge. 

• General game play (Genesereth, Love, & Pell, 2005), with transfer between game 
(Kansky et al., 2017), such that, for example, learn about one first-person shooter 
enhances performance on another with entirely different images, equipment and so 
forth. (A system that can learn many games, separately, without transfer between them, 
such a DeepMind’s Atari game system, would not qualify; the point be to acquire 
cumulative, transferrable knowledge). 

• A physically embody test an AI-driven robot that could build thing (Ortiz Jr, 2016), 
range from tent to IKEA shelves, base on instruction and real-world physical 
interaction with the object parts, rather than vast amount trial-and-error. 

No one challenge be likely to be sufficient. Natural intelligence be multi-dimensional 
(Gardner, 2011), and give the complexity of the world, generalize artificial intelligence 
will necessarily be multi-dimensional a well. 

By push beyond perceptual classification and into a broader integration of inference 
and knowledge, artificial intelligence will advance, greatly. 

Page ! of !22 27 



6. Conclusions 

As a measure of progress, it be worth consider a somewhat pessimistic piece I write 
for The New Yorker five year ago , conjecture that “deep learn be only part of the 15 
large challenge of building intelligent machines” because “such technique lack way of 
represent causal relationship (such a between disease and their symptoms), and be 
likely to face challenge in acquire abstract idea like “sibling” or “identical to.” They 
have no obvious way of perform logical inferences, and they be also still a long way 
from integrate abstract knowledge, such a information about what object are, what 
they be for, and how they be typically used.” 

As we have seen, many of these concern remain valid, despite major advance in 
specific domain like speech recognition, machine translation, and board games, and 
despite equally impressive advance in infrastructure and the amount of data and compute 
available. 

Intriguingly, in the last year, a grow array of other scholars, come from an 
impressive range of perspectives, have begin to emphasize similar limits. A partial list 
include Brenden Lake and Marco Baroni (2017), François Chollet (2017), Robin Jia and 
Percy Liang (2017), Dileep George and others at Vicarious (Kansky et al., 2017) and 
Pieter Abbeel and colleague at Berkeley (Stoica et al., 2017). 

Perhaps most notably of all, Geoff Hinton have be courageous enough to reconsider have 
own beliefs, reveal in an August interview with the news site Axios that he be 16 
“deeply suspicious” of back-propagation, a key enabler of deep learn that he help 
pioneer, because of his concern about it dependence on label data sets. 

Instead, he suggest (in Axios’ paraphrase) that “entirely new method will probably 
have to be invented.” 

I share Hinton’s excitement in see what come next. 

https://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence15 

https://www.axios.com/ai-pioneer-advocates-starting-over-2485537027.html16 

Page ! of !23 27 

https://www.axios.com/ai-pioneer-advocates-starting-over-2485537027.html 


References 
Athalye, A., Engstrom, L., Ilyas, A., & Kwok, K. (2017). Synthesizing Robust Adversarial 

Examples. arXiv, cs.CV. 
Besold, T. R., Garcez, A. D., Bader, S., Bowman, H., Domingos, P., Hitzler, P. et al. (2017). 

Neural-Symbolic Learning and Reasoning: A Survey and Interpretation. arXiv, cs.AI. 
Bošnjak, M., Rocktäschel, T., Naradowsky, J., & Riedel, S. (2016). Programming with a 

Differentiable Forth Interpreter. arXiv. 
Bottou, L. (2015). Two big challenge in machine learning. Proceedings from 32nd International 

Conference on Machine Learning. 
Bowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotate corpus for 

learn natural language inference. arXiv, cs.CL. 
Chollet, F. (2017). Deep Learning with Python. Manning Publications. 
Cireşan, D., Meier, U., Masci, J., & Schmidhuber, J. (2012). Multi-column deep neural network 

for traffic sign classification. Neural networks. 
Davis, E., & Marcus, G. (2015). Commonsense reason and commonsense knowledge in 

artificial intelligence. Communications of the ACM, 58(9)(9), 92-103. 
Davis, E. (2016). How to Write Science Questions that Are Easy for People and Hard for 

Computers. AI magazine, 37(1)(1), 13-22. 
Davis, E., Marcus, G., & Frazier-Logue, N. (2017). Commonsense reason about container 

use radically incomplete information. Artificial Intelligence, 248, 46-84. 
Deng, J., Dong, W., Socher, R., Li, L. J., Li - Computer Vision and, K., & 2009 Imagenet: A 

large-scale hierarchical image database. Proceedings from Computer Vision and Pattern 
Recognition, 2009. CVPR 2009. IEEE Conference on. 

Elman, J. L. (1990). Finding structure in time. Cognitive science, 14(2)(2), 179-211. 
Evtimov, I., Eykholt, K., Fernandes, E., Kohno, T., Li, B., Prakash, A. et al. (2017). Robust 

Physical-World Attacks on Deep Learning Models. arXiv, cs.CR. 
Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: a critical 

analysis. Cognition, 28(1-2)(1-2), 3-71. 
Gardner, H. (2011). Frames of mind: The theory of multiple intelligences. Basic books. 
Gelman, S. A., Leslie, S. J., Was, A. M., & Koch, C. M. (2015). Children’s interpretation of 

general quantifiers, specific quantifiers, and generics. Lang Cogn Neurosci, 30(4)(4), 
448-461. 

Genesereth, M., Love, N., & Pell, B. (2005). General game playing: Overview of the AAAI 
competition. AI magazine, 26(2)(2), 62. 

George, D., Lehrach, W., Kansky, K., Lázaro-Gredilla, M., Laan, C., Marthi, B. et al. (2017). A 
generative vision model that train with high data efficiency and break text-based 
CAPTCHAs. Science, 358(6368)(6368). 

Gervain, J., Berent, I., & Werker, J. F. (2012). Binding at birth: the newborn brain detects identity 
relation and sequential position in speech. J Cogn Neurosci, 24(3)(3), 564-574. 

Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press. 

Page ! of !24 27 



Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A. et al. 
(2016). Hybrid compute use a neural network with dynamic external memory. Nature, 
538(7626)(7626), 471-476. 

Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017). Deep 
Reinforcement Learning that Matters. arXiv, cs.LG. 

Huang, S., Papernot, N., Goodfellow, I., Duan, Y., & Abbeel, P. (2017). Adversarial Attacks on 
Neural Network Policies. arXiv, cs.LG. 

Jia, R., & Liang, P. (2017). Adversarial Examples for Evaluating Reading Comprehension 
Systems. arXiv. 

Kahneman, D. (2013). Thinking, fast and slow (1st pbk. ed. ed.). New York: Farrar, Straus and 
Giroux. 

Kansky, K., Silver, T., Mély, D. A., Eldawy, M., Lázaro-Gredilla, M., Lou, X. et al. (2017). 
Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive 
Physics. arXIv, cs.AI. 

Kočiský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G. et al. (2017). The 
NarrativeQA Reading Comprehension Challenge. arXiv, cs.CL. 

Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep 
convolutional neural networks. In (pp. 1097-1105). 

Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015). Human-level concept learn 
through probabilistic program induction. Science, 350(6266)(6266), 1332-1338. 

Lake, B. M., Ullman, T. D., Tenenbaum, J. B., & Gershman, S. J. (2016). Building Machines 
That Learn and Think Like People. Behav Brain Sci, 1-101. 

Lake, B. M., & Baroni, M. (2017). Still not systematic after all these years: On the compositional 
skill of sequence-to-sequence recurrent networks. arXiv. 

Lazer, D., Kennedy, R., King, G., & Vespignani, A. (2014). Big data. The parable of Google Flu: 
trap in big data analysis. Science, 343(6176)(6176), 1203-1205. 

Le, Q. V., Ranzato, M.-A., Monga, R., Devin, M., Chen, K., Corrado, G. et al. (2012). Building 
high-level feature use large scale unsupervised learning. Proceedings from International 
Conference on Machine Learning. 

LeCun, Y. (1989). Generalization and network design strategies. Technical Report CRG-TR-89-4. 
Lerer, A., Gross, S., & Fergus, R. (2016). Learning Physical Intuition of Block Towers by 

Example. arXiv, cs.AI. 
Lighthill, J. (1973). Artificial Intelligence: A General Survey. Artificial Intelligence: a paper 

symposium. 
Lipton, Z. C. (2016). The Mythos of Model Interpretability. arXiv, cs.LG. 
Lopez-Paz, D., Nishihara, R., Chintala, S., Schölkopf, B., & Bottou, L. (2017). Discovering 

causal signal in images. Proceedings from Proceedings of Computer Vision and Pattern 
Recognition (CVPR). 

Luc, P., Neverova, N., Couprie, C., Verbeek, J., & LeCun, Y. (2017). Predicting Deeper into the 
Future of Semantic Segmentation. International Conference on Computer Vision (ICCV 
2017). 

Page ! of !25 27 



Marcus, G., Rossi, F., Veloso - AI Magazine, M., & 2016. (2016). Beyond the Turing Test. AI 
Magazine, Whole issue. 

Marcus, G., Marblestone, A., & Dean, T. (2014). The atom of neural computation. Science, 
346(6209)(6209), 551-552. 

Marcus, G. (in prep). Innateness, AlphaZero, and Artificial Intelligence. 
Marcus, G. (2014). What Comes After the Turing Test? The New Yorker. 
Marcus, G. (2012). Is “Deep Learning” a Revolution in Artificial Intelligence? The New Yorker. 
Marcus, G. F. (2008). Kluge : the haphazard construction of the human mind. Boston: Houghton 

Mifflin. 
Marcus, G. F. G. F. (2001). The Algebraic Mind: Integrating Connectionism and cognitive 

science. Cambridge, Mass.: MIT Press. 
Marcus, G. F. (1998a). Rethinking eliminative connectionism. Cogn Psychol, 37(3)(3), 243-282. 
Marcus, G. F. (1998b). Can connectionism save constructivism? Cognition, 66(2)(2), 153-182. 
Marcus, G. F., Pinker, S., Ullman, M., Hollander, M., Rosen, T. J., & Xu, F. (1992). 

Overregularization in language acquisition. Monogr Soc Res Child Dev, 57(4)(4), 1-182. 
Marcus, G. F., Vijayan, S., Bandi Rao, S., & Vishton, P. M. (1999). Rule learn by seven- 

month-old infants. Science, 283(5398)(5398), 77-80. 
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word 

Representations in Vector Space. arXiv. 
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G. et al. (2015). 

Human-level control through deep reinforcement learning. Nature, 518(7540)(7540), 
529-533. 

Neelakantan, A., Le, Q. V., Abadi, M., McCallum, A., & Amodei, D. (2016). Learning a Natural 
Language Interface with Neural Programmer. arXiv. 

Ng, A. (2016). What Artificial Intelligence Can and Can’t Do Right Now. Harvard Business 
Review. 

Nguyen, A., Clune, J., Bengio, Y., Dosovitskiy, A., & Yosinski, J. (2016). Plug & Play 
Generative Networks: Conditional Iterative Generation of Images in Latent Space. arXiv, 
cs.CV. 

Nguyen, A., Yosinski, J., & Clune, J. (2014). Deep Neural Networks be Easily Fooled: High 
Confidence Predictions for Unrecognizable Images. arXiv, cs.CV. 

Norvig, P. (2016). State-of-the-Art AI: Building Tomorrow’s Intelligent Systems. Proceedings 
from EmTech Digital, San Francisco. 

O’Neil, C. (2016). Weapons of math destruction : how big data increase inequality and threatens 
democracy. 

Ortiz Jr, C. L. (2016). Why we need a physically embody Turing test and what it might look 
like. AI magazine, 37(1)(1), 55-63. 

Paritosh, P., & Marcus, G. (2016). Toward a comprehension challenge, use crowdsourcing a a 
tool. AI Magazine, 37(1)(1), 23-31. 

Pearl, J. (2000). Causality : models, reasoning, and inference /. Cambridge, U.K.; New York : 
Cambridge University Press. 

Page ! of !26 27 



Pinker, S., & Prince, A. (1988). On language and connectionism: analysis of a parallel distribute 
processing model of language acquisition. Cognition, 28(1-2)(1-2), 73-193. 

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why Should I Trust You?”: Explaining the 
Predictions of Any Classifier. arXiv, cs.LG. 

Sabour, S., Frosst, N., & Hinton, G. E. (2017). Dynamic Routing Between Capsules. arXiv, 
cs.CV. 

Samek, W., Wiegand, T., & Müller, K.-R. (2017). Explainable Artificial Intelligence: 
Understanding, Visualizing and Interpreting Deep Learning Models. arXiv, cs.AI. 

Schank, R. C., & Abelson, R. P. (1977). Scripts, Plans, Goals and Understanding: an Inquiry into 
Human Knowledge Structures. Hillsdale, NJ: L. Erlbaum. 

Schmidhuber, J. (2015). Deep learn in neural networks: An overview. Neural networks. 
Schoenick, C., Clark, P., Tafjord, O., P, T., & Etzioni, O. (2017). Moving beyond the Turing Test 

with the Allen AI Science Challenge. Communications of the ACM, 60 (9)(9), 60-64. 
Sculley, D., Phillips, T., Ebner, D., Chaudhary, V., & Young, M. (2014). Machine learning: The 

high-interest credit card of technical debt. Proceedings from SE4ML: Software 
Engineering for Machine Learning (NIPS 2014 Workshop). 

Socher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Semantic compositionality through 
recursive matrix-vector spaces. Proceedings from Proceedings of the 2012 joint conference 
on empirical method in natural language processing and computational natural language 
learning. 

Spelke, E. S., & Kinzler, K. D. (2007). Core knowledge. Dev Sci, 10(1)(1), 89-96. 
Stoica, I., Song, D., Popa, R. A., Patterson, D., Mahoney, M. W., Katz, R. et al. (2017). A 

Berkeley View of Systems Challenges for AI. arXiv, cs.AI. 
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. et al. (2013). 

Intriguing property of neural networks. arXiv, cs.CV. 
Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2014). Show and Tell: A Neural Image Caption 

Generator. arXiv, cs.CV. 
Watters, N., Tacchetti, A., Weber, T., Pascanu, R., Battaglia, P., & Zoran, D. (2017). Visual 

Interaction Networks. arXiv. 
Williams, A., Nangia, N., & Bowman, S. R. (2017). A Broad-Coverage Challenge Corpus for 

Sentence Understanding through Inference. arXiv, cs.CL. 
Wu, J., Lu, E., Kohli, P., Freeman, B., & Tenenbaum, J. (2017). Learning to See Physics via 

Visual De-animation. Proceedings from Advances in Neural Information Processing 
Systems. 

Zoph, B., Vasudevan, V., Shlens, J., & Le, Q. V. (2017). Learning Transferable Architectures for 
Scalable Image Recognition. arXiv, cs.CV. 

Page ! of !27 27 


