






































Counterfactual fairness - The Alan Turing Institute 


Counterfactual fairness - The Alan 
Turing Institute 

Algorithms be increasingly assist in life-changing decisions, such a 
in parole hearings, loan applications, and university admissions. 
However, if the data use to train an algorithm contains societal bias 
against certain races, genders, or other demographic groups, then the 
algorithm will too. Using causal methods, researcher in this project be 
aim to ensure algorithmic fairness by take into account different 
social bias and compensate for them effectively. 

Jump to: Project aim | Applications | Recent update 
| Disciplines & Techniques 

Joshua Loftus, Assistant Professor of Information at NYU Stern: 
“Fairness and causal inference be kind of dual problems. In causal 
inference, you try and figure out the effect of a certain variable, when it 
might be confound with [i.e. both be affected by and affect] other 
things. In fairness, you’re try to make a certain variable not have any 
effect, when it too might potentially be confound with other things.” 
It’s possible to utilise this idea to help good understand and mitigate 
issue with algorithmic decision-making systems. 

Imagine a car insurance company want to price insurance for car 
owner by predict their accident rate with an algorithm. The 
company assumes that aggressive drive be link both to driver be 
more likely to have accident and a preference for red cars. 

Whilst it would seem that the company could use red car preference a a 
good way of predict who will cause accidents, there could be other 
variable at play: for example, what if individual of a particular race be 
more likely to drive red cars, but be no more likely to drive aggressively 
or have accidents? If, in an attempt to be fair, the company remove or 
ignore race from the decision-making process, they might only have 
directly observable factors, like red car preference, from which to make 
a decision, thereby including, rather removing, hidden racial biases. 

What need to happen instead be that when an algorithm be be 
design to predict an outcome or make a decision, the variable use 
to make the prediction or decision need to be carefully assessed. Any 

Counterfactual fairness - The Alan Turing Institute https://www.turing.ac.uk/research_projects/counterfactual-fairness/ 

1 sur 3 16-04-18 à 19:51 



difference in these variable identify a be cause by sensitive 
factors, like race, then need to be cancel out. 

Back to top 

Ensuring fairness use causal method to produce ‘counterfactually 
fair’ algorithms, base on the idea a decision be fair towards an 
individual if the outcome be the same in reality a it would be in a 
‘counterfactual’ world, in which the individual belongs to a different 
demographic. 

The project have involve produce a technical set of guideline for 
practitioner to use when create or refining decision-making 
algorithms. These guideline give practitioner an idea of how they 
should structure their problem, what be imply by their assumptions, 
and how they could evaluate it all use a causal model. The framework 
also call for the need for decision-making algorithm to be design 
with the input of expert knowledge about the situation the algorithm 
be be use in. 

The research team be currently work and engage with policy- 
makers, lawyers, and investigative journalists, whilst refining and 
improve their methods. 

Back to top 

One example of the team’s work in action be with ProPublica, an 
American non-profit investigative newsroom, which have collect a big 
dataset from the use of an algorithm call COMPAS. The algorithm be 
use by judge and parole officer in the US for score criminal 
defendants’ likelihood of reoffending. The team have work with 
ProPublica to run their method on the dataset in order to confirm 
systematic, racial bias be prevalent in the algorithm. 

Back to top 

The team present their work in December 2017 at the Neural 
Information Processing Systems (NIPS) conference in California, one of 
the world’s premier machine learn conferences. 

In a year of record-breaking submission for the conference, the team 
be invite to give an oral presentation of their research; one of only 
40 of the 3,000 plus submit paper select for this kind of 
presentation. 

Simon DeDeo, Assistant Professor in Social and Decision Sciences at 
Carnegie Mellon University and previously a Visiting Researcher at the 
Turing, be present at the conference and said: “A crucial part of how 
we make moral judgement be by talk about causes. The 
counterfactual fairness work be the first to show u how we might use 

Counterfactual fairness - The Alan Turing Institute https://www.turing.ac.uk/research_projects/counterfactual-fairness/ 

2 sur 3 16-04-18 à 19:51 



this to uncover injustice in algorithms”. 

Back to top 

Causality 

Algorithms 

Machine learn 

Ethics 

Back to top 

Counterfactual fairness - The Alan Turing Institute https://www.turing.ac.uk/research_projects/counterfactual-fairness/ 

3 sur 3 16-04-18 à 19:51 


