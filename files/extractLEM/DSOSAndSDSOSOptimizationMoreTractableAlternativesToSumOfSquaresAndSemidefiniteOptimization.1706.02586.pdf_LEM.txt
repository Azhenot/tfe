




















































DSOS and SDSOS Optimization: More Tractable Alternatives to Sum of Squares and Semidefinite Optimization 


DSOS AND SDSOS OPTIMIZATION: 
MORE TRACTABLE ALTERNATIVES TO 

SUM OF SQUARES AND SEMIDEFINITE OPTIMIZATION∗ 

AMIR ALI AHMADI† AND ANIRUDHA MAJUMDAR ‡ 

Abstract. In recent years, optimization theory have be greatly impact by the advent of sum 
of square (SOS) optimization. The reliance of this technique on large-scale semidefinite program 
however, have limited the scale of problem to which it can be applied. In this paper, we introduce 
DSOS and SDSOS optimization a more tractable alternative to sum of square optimization that 
rely instead on linear and second order cone program respectively. These be optimization problem 
over certain subset of sum of square polynomial (or equivalently subset of positive semidefinite 
matrices), which can be of interest in general application of semidefinite program where scala- 
bility be a limitation. We show that some basic theorem from SOS optimization which rely on result 
from real algebraic geometry be still valid for DSOS and SDSOS optimization. Furthermore, we 
show with numerical experiment from diverse application areas—polynomial optimization, statis- 
tic and machine learning, derivative pricing, and control theory—that with reasonable tradeoff 
in accuracy, we can handle problem at scale that be currently far beyond the reach of sum of 
square approaches. Finally, we provide a review of recent technique that bridge the gap between 
our DSOS/SDSOS approach and the SOS approach at the expense of additional run time. The 
appendix of the paper introduces an accompany MATLAB package for DSOS and SDSOS opti- 
mization. 

Key words. Sum of square optimization, polynomial optimization, nonnegative polynomials, 
semidefinite programming, linear programming, second order cone programming. 

AMS subject classifications. 65K05, 90C25, 90C22, 90C05, 90C90, 12Y05, 93C85, 90C27 

1. Introduction. For which value of the real coefficient c1, c2, c3, be the poly- 
nomial 

(1) p(x1, x2) = c1x 
4 
1 − 6x31x2 − 4x31 + c2x21x22 + 10x21 + 12x1x22 + c3x42 

nonnegative, i.e., satisfies p(x1, x2) ≥ 0 for all (x1, x2) ∈ R2? The problem of optimiz- 
ing over nonnegative polynomials—of which our opening question be a toy example—is 
fundamental to many problem of apply and computational modern mathematics. 
In such an optimization problem, one would like to impose constraint on the coef- 
ficients of an unknown polynomial so a to make it nonnegative, either globally in 
Rn (as in the above example), or on a certain basic semialgebraic set, i.e., a subset 
of Rn define by a finite number of polynomial inequalities. We will demonstrate 
shortly (see Section 1.1) why optimization problem of this kind be ubiquitous in 
application and universal in the study of question deal in one way or another 
with polynomial equation and inequalities. 

Closely related to nonnegative polynomial be polynomial that be sum of 
squares. We say that a polynomial p be a sum of square (sos) if it can be write a 
p = 

∑ 
i q 

2 
i for some (finite number of) polynomial qi. For example, the polynomial 

∗Submitted to the editor on June 9, 2017. 
Funding: Amir Ali Ahmadi be partially support by a DARPA Faculty Award, a Sloan 

Fellowship, an NSF CAREER Award, an AFOSR Young Investigator Program Award, and a Google 
Research Award. 
†Department of Operations Research and Financial Engineering, Princeton University. 

(a a a@princeton.edu, http://aaa.princeton.edu/). 
‡ Department of Mechanical and Aerospace Engineering, Princeton University (starting Aug. 1, 

2017). (ani.majumdar@princeton.edu, http://web.stanford.edu/∼anirudha/). 

1 

ar 
X 

iv 
:1 

70 
6. 

02 
58 

6v 
2 

[ 
m 

at 
h. 

O 
C 

] 
9 

O 
ct 

2 
01 

7 

mailto:a\protect _a\protect _a@princeton.edu 
http://aaa.princeton.edu/ 
mailto:ani.majumdar@princeton.edu 
http://web.stanford.edu/~anirudha/ 


2 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

in (1) with c1 = 13, c2 = 1, c3 = 4 be so since it admits the decomposition 

p(x1, x2) = (x1 − 2x21)2 + (3x1 + 2x22)2 + (x1x2 − 3x21)2. 

The relationship between nonnegative and sum of square polynomial have be a 
classical subject of study in real algebraic geometry. For example, a result of Hilbert 
from 1888 [37] state that all nonnegative bivariate polynomial of degree four be 
sum of squares. It follows, a a special case, that the set of coefficient for which 
the polynomial p in (1) be nonnegative or a sum of square in fact coincide. In 
the general situation, however, while sum of square polynomial be clearly always 
nonnegative, it be not true that the converse always holds. This be show in the 
same paper of Hilbert [37], where he give a non-constructive proof of existence of 
nonnegative polynomial that be not sum of squares. Explicit example of such 
polynomial appear many year later, start with the work of Motzkin [59] in 
the 1960s. Hilbert’s interest in this line of research be also showcased in his 17th 
problem, which asks whether every nonnegative polynomial be a sum of square of 
rational functions. We refer the interested reader to an outstanding survey paper of 
Reznick [74], which cover many historical aspect around Hilbert’s 17th problem, 
include the affirmative solution by Artin [6], a well a several late developments. 

The classical question around nonnegative and sum of square polynomial have 
be revisit quite extensively in the past 10-15 year in different community among 
apply and computational mathematicians. The reason for this renew interest be 
twofold: (i) the discovery that many problem of modern practical interest can be cast 
a optimization problem over nonnegative polynomials, and (ii) the observation that 
while optimize over nonnegative polynomial be generally NP-hard, optimization 
over the set of sum of square polynomial can be do via semidefinite program 
(SDP); see Theorem 1 in Section 2. The latter development, originally explore in the 
pioneer work of Shor [79], Nesterov [61], Parrilo [65],[66], and Lasserre [44], have 
lead to the creation of sum of square optimization—a computational framework, with 
semidefinite program a it underlie engine, that can tackle many fundamental 
problem of real algebra and polynomial optimization. 

The dependence of sum of square approach on semidefinite program can 
be view a both a strength and a weakness depend on one’s perspective. From 
a computational complexity viewpoint, semidefinite program can be solve with ar- 
bitrary accuracy in polynomial time use interior point method (see [85] for a com- 
prehensive survey). As a result, sum of square technique offer polynomial time al- 
gorithms that approximate a very broad class of NP-hard problem of interest. From 
a more practical viewpoint however, SDPs be among the most expensive convex re- 
laxation to solve. The speed and reliability of the current SDP solver lag behind 
those for other more restrict class of convex program (such a linear or second 
order cone programs) by a wide margin. With the add complication that the SDPs 
generate by so problem be large (see Section 2), scalability have become the single 
most outstanding challenge for sum of square optimization in practice. 

In this paper, we focus on the latter of the two viewpoint mention above 
and offer alternative to sum of square optimization that while more conservative 
in general, be significantly more scalable. Our hope be that by do so, we expand 
the use and applicability of algebraic technique in optimization to new area and 
share it appeal with a broader audience. We call our new computational frame- 
works, which rely on linear and second order cone programming, DSOS and SDSOS 



DSOS AND SDSOS OPTIMIZATION 3 

optimization1. These be short for diagonally-dominant-sum-of-squares and scaled- 
diagonally-dominant-sum-of-squares; see Section 3 for precise definitions. While these 
tool be primarily design for sum of square optimization, they be also applicable 
to general application of semidefinite program where tradeoff between scalabil- 
ity and performance may be desirable. In the interest of motivate our contribution 
for a diverse audience, we delay a presentation of these contribution until Section 3 
and start instead with a portfolio of problem area involve nonnegative polynomials. 
Any such problem area be one to which sum of square optimization, a well a it new 
DSOS and SDSOS counterparts, be directly applicable. 

1.1. Why optimize over nonnegative polynomials?. We describe several 
motivate application in this section at a high level. These will be revisit late in 
the paper with concrete computational examples; see Section 4. 

Polynomial optimization. A polynomial optimization problem (POP) be an 
optimization problem of the form 

(2) 
minimize p(x) 
subject to x ∈ K := {x ∈ Rn | gi(x) ≥ 0, hi(x) = 0}, 

where p, gi, and hi be multivariate polynomials. The special case of problem (2) 
where the polynomial p, gi, hi all have degree one be of course linear programming, 
which can be solve very efficiently. For high degrees, POP contains a special case 
many important problem in operation research; e.g., the optimal power flow problem 
in power engineering [39], the computation of Nash equilibrium in game theory [43], [67], 
problem of Euclidean embed and distance geometry [50], and a host of problem 
in combinatorial optimization. We observe that if we could optimize over the set of 
polynomial that be nonnegative on a basic semialgebraic set, then we could solve 
the POP problem to global optimality. To see this, note that the optimal value of 
problem (2) be equal to the optimal value of the follow problem: 

(3) 
maximize γ 
subject to p(x)− γ ≥ 0, ∀x ∈ K. 

Here, we be try to find the large constant γ such that the polynomial p(x)− γ 
be nonnegative on the set K; i.e., the large low bound on problem (2). 

Combinatorial optimization. Proofs of nonnegativity of polynomial of degree 
a low a four provide infeasibility certificate for all decision problem in the com- 
plexity class NP, include many well-known combinatorial optimization problems. 
Consider, e.g., the simple-to-describe, NP-complete problem of PARTITION [29]: 
Given a set of integer a1, . . . , an, decide if they can be split into two set with equal 
sums. It be straightforward to see that a PARTITION instance be infeasible if and 
only if the degree-four polynomial 

p(x) = 

n∑ 
i=1 

(x2i − 1)2 + ( 
n∑ 
i=1 

xiai) 
2 

satisfies p(x) > 0, ∀x ∈ Rn. Indeed, p be by construction a sum of square and hence 
nonnegative. If it be to have a zero, each square in p would have to evaluate to zero; 
but this can happen if and only if there be a binary vector x ∈ {−1, 1}n, which make 

1We use and recommend the pronunciation “d-sauce” and “s-d-sauce”. 



4 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR∑n 
i=1 xiai = 0, give a yes answer to the PARTITION instance. Suppose now that 

for a give instance, and for some � > 0, we could prove that p(x)− � be nonnegative. 
Then we would have proven that our PARTITION instance be infeasible, potentially 
without try out all 2n way of splitting the integer into two sets. 

Control system and robotics. Numerous fundamental problem in nonlinear 
dynamic and control, such a stability, robustness, collision avoidance, and controller 
design can be turn into problem about find special functions—the so-called 
Lyapunov function (see, e.g., [41])—that satisfy certain sign conditions. For example, 
give a differential equation ẋ = f(x), where f : Rn → Rn, and with the origin a an 
equilibrium point (i.e., satisfy f(0) = 0), consider the “region of attraction (ROA) 
problem”: Determine the set of initial state in Rn from which the trajectory flow 
to the origin. Lyapunov’s stability theorem (see, e.g., [41, Chap. 4]) tell u that if 
we can find a (Lyapunov) function V : Rn → R, which together with it gradient ∇V 
satisfies 

(4) V (x) > 0 ∀x 6= 0, and 〈∇V (x), f(x)〉 < 0 ∀x ∈ {x| V (x) ≤ β, x 6= 0}, 

then the set {x ∈ Rn| V (x) ≤ β} be part of the region of attraction. If f be a polynomial 
function (a very important case in application [65]), and if we parameterize V a a 
polynomial function, then the search for the coefficient of V satisfy the condition 
in (4) be an optimization problem over the set of nonnegative polynomials. Designing 
stable equilibrium point with large region of attraction be a fundamental problem in 
control engineering and robotics; see, e.g., [40], [54]. 

Statistical regression with shape constraints. A problem that arises fre- 
quently in statistic be that of fitting a function to a set of data point with minimum 
error, while ensure that it meet some structural properties, such a nonnegativity, 
monotonicity, or convexity [32], [34]. Requirements of this type be typically impose 
either a regularizers (to avoid overfitting), or more importantly a a result of prior 
knowledge that the true function to be estimate satisfies the same structural prop- 
erties. In economics, for example, a regression problem for estimate the utility of 
consumer from sample measurement would come with a natural concavity require- 
ment on the utility function. When the regression function be polynomials, many 
such structural property lead to polynomial nonnegativity constraints. For example, 
a polynomial p(x) := p(x1, . . . , xn) can be constrain to be concave by require it 
Hessian matrix ∇2p(x), which be an n×n symmetric matrix with polynomial entries, 
to be negative semidefinite for all x. This condition be easily see to be equivalent to 
the polynomial −yTH(x)y in the 2n variable x := (x1, . . . , xn) and y := (y1, . . . , yn) 
be nonnegative. 

Probability bound give moments. The well-known inequality of Markov 
and Chebyshev in probability theory bound the probability that a univariate random 
variable take value in a subset of the real line give the first, or the first and second 
moment of it distribution [10]. Consider a vast generalization of this problem where 
one be give a finite number of moment {µk1,...,kn := E[X 

k1 
1 . . . X 

kn 
n ], k1+· · · , kn ≤ d} 

of a multivariate multivariate random variable X := (X1, . . . , Xn) and be interested in 
bound the probability of an event X ∈ S, where S be a general basic semialgebraic 
subset of the sample space Ω. A sharp upper bound on this probability can be 
obtain by solve the follow optimization problem for the coefficient ck1...kn of 
an n-variate degree-d polynomial p (see [14], [70]): 



DSOS AND SDSOS OPTIMIZATION 5 

(5) 
minimize E[p(X)] = 

∑ 
ck1...knµk1...kn 

subject to p(x) ≥ 1,∀x ∈ S 
p(x) ≥ 0,∀x ∈ Ω. 

It be easy to see that any feasible solution of problem (5) give an upper bound 
on the probability of the event X ∈ S a the polynomial p be by construction place 
above the indicator function of the set S and the expect value of this indicator 
function be precisely the probability of the event X ∈ S. Note that the constraint in 
(5) be polynomial nonnegativity conditions. 

Other applications. The application area outline above be only a few ex- 
amples on a long list of problem that can be tackle by optimize over the set of 
nonnegative polynomials. Other interest problem on this list include: computa- 
tion of equilibrium in game with continuous strategy space [67], distinguish sep- 
arable and entangle state in quantum information theory [25], compute bound 
on sphere pack problem in geometry [7], software verification [77], robust and 
stochastic optimization [12], filter design [82], and automate theorem prove [36]. 
A great reference for many application in this area, a well a the related theoretical 
and computational background, be a recent volume edit by Blekherman, Parrilo, 
and Thomas [15]. 

Organization of the paper. The remainder of this paper be structure a fol- 
lows. In Section 2, we briefly review the sum of square approach and it relation to 
semidefinite program for the general reader. We also comment on it challenge 
regard scalability and prior work that have tackle this issue. The familiar reader 
can skip to Section 3, where our contribution begin. In Section 3.1, we introduce the 
cone of dsos and sdsos polynomial and demonstrate that we can optimize over them 
use linear and second order cone program respectively. Section 3.2 introduces 
hierarchy of cone base on dsos and sdsos polynomial that can good approximate 
the cone of nonnegative polynomials. Asymptotic guarantee on these cone base on 
classical Positivstellensätze be also presented. Section 4 present numerical experi- 
ments that use our approach on large-scale example from polynomial optimization 
(Section 4.1), combinatorial optimization (Section 4.2), statistic and machine learn- 
ing (Sections 4.3 and 4.5), derivative pricing (Section 4.4), and control theory and 
robotics (Section 4.6). Section 5 provide a review of recent technique for bridging 
the gap between the DSOS/SDSOS approach and the SOS approach at the expense 
of additional computational costs. Section 6 concludes the paper. The Appendix to 
the paper introduces iSOS (“inside SOS”), an accompany MATLAB package for 
DSOS and SDSOS optimization write use the SPOT toolbox [57]. 

2. Review of the semidefinite programming-based approach and com- 
putational considerations. As mention earlier, sum of square optimization han- 
dle a (global) nonnegativity constraint on a polynomial p by replace it with the 
strong requirement that p be a sum of squares. The situation where p be only 
constrain to be nonnegative on a certain basic semialgebraic set2 

S := {x ∈ Rn| g1(x) ≥ 0, . . . , gm(x) ≥ 0} 

2In this formulation, we have avoid equality constraint for simplicity. Obviously, there be no 
loss of generality in do this a an equality constraint h(x) = 0 can be impose by the pair of 
inequality constraint h(x) ≥ 0,−h(x) ≥ 0. 



6 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

can also be handle with the help of appropriate sum of square multipliers. For 
example, if we succeed in find so polynomial s0, s1, . . . , sm, such that 

(6) p(x) = s0(x) + 

m∑ 
i=1 

si(x)gi(x), 

then we have found a certificate of nonnegativity of p on the set S. Indeed, if we evalu- 
ate the above expression at any x ∈ S, nonnegativity of the polynomial s0, s1 . . . , sm 
imply that p(x) ≥ 0. A Positivstellensatz from real algebraic geometry due to Puti- 
nar [72] state that if the set S satisfies the so-called Archimedean property, a property 
only slightly strong than compactness (see, e.g., [47] for a precise definition), then 
every polynomial positive on S have a representation of the type (6), for some so 
polynomial s0, s1, . . . , sm of high enough degree (see also [62] for degree bounds). 
Even with no qualification whatsoever regard the set S, there be other Posi- 
tivstellensätze (e.g., due to Stengle [80]) that certify nonnegativity of a polynomial 
on a basic semialgebraic set use so polynomials. These certificate be only slightly 
more complicate than (6) and involve so multiplier associate with product among 
polynomial gi that define S [66]. A great reference for the interested reader be the 
survey paper by Laurent [47]. 

The computational advantage of a certificate of (global or local) nonnegativity 
via sum of square polynomial be that it can be automatically found by semidefinite 
programming. The follow well-known theorem establishes the link between so 
polynomial and SDP. Let u denote the set of n × n symmetric matrix by Sn. 
Recall that a matrix A ∈ Sn be positive semidefinite (psd) if xTAx ≥ 0,∀x ∈ Rn, and 
that semidefinite program be the problem of optimize a linear function over psd 
matrix subject to affine inequality on their entry [85]. We denote the positive 
semidefiniteness of a matrix A with the standard notation A � 0. 

Theorem 1 (see, e.g., [65],[66]). A multivariate polynomial p := p(x) in n 
variable and of degree 2d be a sum of square if and only if there exists a symmetric 
matrix Q (often call the Gram matrix) such that 

(7) 
p(x) = zTQz, 
Q � 0, 

where z be the vector of monomials of degree up to d: 

z = [1, x1, x2, . . . , xn, x1x2, . . . , x 
d 
n]. 

Searching for a matrixQ satisfy the positive semidefiniteness constraint, a well 
a the linear equality constraint come from (7), amount to solve a semidefinite 
program. The size of the matrix Q in this theorem is( 

n+ d 

d 

) 
× 
( 
n+ d 

d 

) 
, 

which approximately equal nd × nd. While this number be polynomial in n for fix 
d, it can grow rather quickly even for low-degree polynomials. For example, a degree- 
4 polynomial (d = 2) in 50 variable have 316251 coefficient and it Gram matrix, 
which would need to be positive semidefinite, contains 879801 decision variables. A 
semidefinite constraint of this size be quite expensive, and in fact a problem with 50 



DSOS AND SDSOS OPTIMIZATION 7 

variable be far beyond the current realm of possibility in SOS optimization. In 
the absence of problem structure, sum of square problem involve degree-4 or 6 
polynomial be currently limited, roughly speaking, to a handful or a dozen variables. 

There have be many contribution already to improvement in scalability of 
sum of square techniques. One approach have be to develop systematic technique 
for take advantage of problem structure, such a sparsity or symmetry of the under- 
lie polynomials, to reduce the size of the SDPs [30], [84], [23], [76]. These technique 
have proven to be very useful a problem arise in practice sometimes do come with 
a lot structure. Another approach which hold promise have be to design customize 
solver for special class of SOS program and avoid restore to an off-the-shelf inte- 
rior point solver. Examples in this direction include the work in [11], [63], [86], [49]. 
There have also be recent work by Lasserre et al. that increase scalability of sum of 
square optimization problem at the cost of accuracy of the solution obtained. This 
be do by bound the size of the large SDP constraint appear in the so for- 
mulation, and lead to what the author call the BSOS (bounded SOS) hierarchy [46]. 

The approach we take in this paper for enhance scalability be orthogonal to the 
one mention above (and can potentially late be combine with them). We propose 
to eschew sum of square decomposition to begin with. In our view, almost all ap- 
plication area of this field use sum of square decomposition not for their own sake, 
but because they provide a mean to polynomial nonnegativity. Hence, this paper be 
motivate by a natural question: Can we give other sufficient condition for polyno- 
mial nonnegativity that be perhaps strong than a sum of square decompostion, 
but cheaper to work with? 

In the next section, we identify some subset of the cone of nonnegative polyno- 
mials that one can optimize over use linear program (LP) and second order 
cone program (SOCP) [51], [5]. Not only be these much more efficiently solv- 
able class of convex programs, but they be also superior to semidefinite program 
in term of numerical conditioning. In addition, work with these class of con- 
vex program allows u to take advantage of high-performance LP and SOCP solver 
(e.g., CPLEX [20], Mosek [58]) that have be mature over several decade because 
of industry applications. We remark that while there have be other approach 
to produce LP hierarchy for polynomial optimization problem (e.g., base on the 
Krivine-Stengle certificate of positivity [42, 80, 45]), these LPs, though theoretically 
significant, be typically quite weak in practice and often numerically ill-conditioned 
[46]. 

3. DSOS and SDSOS Optimization. We start with some basic definitions. 
A monomial m : Rn → R in variable x := (x1, . . . , xn) be a function of the form 
m(x) = xα11 . . . x 

αn 
n , where each αi be a nonnegative integer. The degree of such 

a monomial be by definition α1 + · · · + αn. A polynomial p : Rn → R be a finite 
linear combination of monomials p(x) = 

∑ 
α1...αn 

cα1...αnx 
α1 
1 . . . x 

αn 
n . The degree of 

a polynomial be the maximum degree of it monomials. A form or a homogeneous 
polynomial be a polynomial whose monomials all have the same degree. For any 
polynomial p := p(x1, . . . , xn) of degree d, one can define it homogenize version 
ph by introduce a new variable y and define ph(x1, . . . , xn, y) = y 

dp(xy ). One 

can reverse this operation (dehomogenize) by set the homogenize variable y 
equal to one: p(x1, . . . , xn) = ph(x1, . . . , xn, 1). The property of be nonnegative 
and a sum of square (as define earlier) be preserve under homogenization and 
dehomogenization [74]. We say that a form f be positive definite if f(x) > 0 for all 
x 6= 0. We denote by PSDn,2d and SOSn,2d the set of nonnegative (a.k.a. positive 



8 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

semidefinite) and sum of square polynomial in n variable and degree 2d respectively. 
(Note that odd-degree polynomial can never be nonnegative.) Both these set form 

proper (i.e., convex, closed, pointed, and solid) cone in R( 
n+2d 
2d ) with the obvious 

inclusion relationship SOSn,2d ⊆ PSDn,2d. 
The basic idea behind our approach be to approximate SOSn,2d from the inside 

with new set that be more tractable for optimization purposes. Towards this goal, 
one may think of several natural sufficient condition for a polynomial to be a sum of 
squares. For example, the follow may be natural first candidates: 

• The set of polynomial that be sum of 4-th power of polynomials: 
{p| p = 

∑ 
q4i }, 

• The set of polynomial that be a sum of a constant number of square of 
polynomials; e.g., a sum of three squares: {p| p = q21 + q22 + q23}. 

Even though both of these set clearly reside inside the set of so polynomials, 
they be not any easy to optimize over. In fact, they be much harder: test 
whether a polynomial be a sum of 4-th power be NP-hard already for quartic [38] 
(in fact, the cone of 4-th power of linear form be dual to the cone of nonnegative 
quartic form [75]) and optimize over polynomial that be sum of three square 
be intractable (as this task even for quadratic subsumes the NP-hard problem of 
positive semidefinite matrix completion with a rank constraint [48]). These example 
illustrate that in general, an inclusion relationship do not imply anything with 
respect to optimization complexity. Hence, we need to take some care when choose 
which subset of SOSn,2d to work with. On the one hand, these subset have to 
be “big enough” to be useful in practice; on the other hand, they should be more 
tractable to optimize over. 

3.1. The cone of dsos and sdsos polynomials. We now describe two in- 
teresting cone inside SOSn,2d that lend themselves to linear and second order cone 
representation and be hence more tractable for optimization purposes. These cone 
will also form the building block of some more elaborate cone (with improve per- 
formance) that we will present in Subsection 3.2 and Section 5. 

Definition 2. A polynomial p := p(x) be diagonally-dominant-sum-of-squares 
(dsos) if it can be write a 

(8) p(x) = 
∑ 
i 

αim 
2 
i (x) + 

∑ 
i,j 

β+ij(mi(x) +mj(x)) 
2 + 

∑ 
i,j 

β−ij(mi(x)−mj(x)) 
2, 

for some monomials mi(x),mj(x) and some nonnegative scalar αi, β 
+ 
ij , β 

− 
ij . We de- 

note the set of polynomial in n variable and degree 2d that be dsos by DSOSn,2d. 

Definition 3. A polynomial p := p(x) be scaled-diagonally-dominant-sum-of- 
square (sdsos) if it can be write a 

(9) p(x) = 
∑ 
i 

αim 
2 
i (x) + 

∑ 
i,j 

(β̂+ijmi(x) + β̃ 
+ 
ijmj(x)) 

2 + 
∑ 
i,j 

(β̂−ijmi(x)− β̃ 
− 
ijmj(x)) 

2, 

for some monomials mi(x),mj(x) and some scalar αi, β̂ 
+ 
ij , β̃ 

+ 
ij , β̂ 

− 
ij , β̃ 

− 
ij with αi ≥ 0. 

We denote the set of polynomial in n variable and degree 2d that be sdsos by 
SDSOSn,2d. 

From the definition, the follow inclusion relationship should be clear: 

DSOSn,2d ⊆ SDSOSn,2d ⊆ SOSn,2d ⊆ POSn,2d. 



DSOS AND SDSOS OPTIMIZATION 9 

Fig. 1. A comparison of the set of dsos/sdsos/sos/psd polynomial on a parametric family of 
bivariate quartic give in (10). 

In general, all these containment relationship be strict. Figure 1 show these set 
for a family of bivariate quartic parameterized by two coefficient a, b:3 

(10) p(x1, x2) = x 
4 
1 + x 

4 
2 + ax 

3 
1x2 + (1− 

1 

2 
a− 1 

2 
b)x21x 

2 
2 + 2bx1x 

3 
2. 

Some elementary remark about the two definition above be in order. It be not 
hard to see that if p have degree 2d, the monomials mi,mj in (8) or (9) never need 
to have degree high than d. In the special case where p be homogeneous of degree 
2d (i.e., a form), the monomials mi,mj can be take to have degree exactly d. We 
also note that decomposition give in (8) or (9) be not unique; there can even be 
infinitely many such decomposition but the definition require just one. 

Our terminology in Definitions 2 and 3 come from a connection (which we will 
soon establish) to the follow class of symmetric matrices. 

Definition 4. A symmetric matrix A = (aij) be diagonally dominant (dd) if 

aii ≥ 
∑ 
j 6=i 

|aij | 

for all i. A symmetric matrix A be scale diagonally dominant (sdd) if there exists a 
diagonal matrix D, with positive diagonal entries, such that DAD be dd.4 We denote 
the set of n× n dd and sdd matrix with DDn and SDDn respectively. 

It follow from Gershgorin’s circle theorem [31] that diagonally dominant matrix 
be positive semidefinite. This implies that scale diagonally dominant matrix be 
also positive semidefinite a the eigenvalue of DAD have the same sign a those of 
A. Hence, if we denote the set of n × n positive semidefinite matrix by Pn, the 
follow inclusion relationship be evident: 

3It be well know that all psd bivariate form be so [74] and hence the outer set exactly char- 
acterizes all value of a and b for which this polynomial be nonnegative. 

4Checking whether a give matrix A = (aij) be sdd can be do by solve a linear program since 
the definition be equivalent to existence of an element-wise positive vector d such that 

aiidi ≥ 
∑ 
j 6=i 
|aij |dj , ∀i. 



10 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

Fig. 2. A section of the cone of 5 × 5 diagonally dominant, scale diagonally dominant, and 
positive semidefinite matrices. Optimization over these set can respectively be do by LP, SOCP, 
and SDP. 

DDn ⊆ SDDn ⊆ Pn. 

These containment be strict for n > 2. In Figure 2, we illustrate the set of x and y 
for which the matrix 

I + xA+ yB 

be diagonally dominant, scale diagonally dominant, and positive semidefinite. Here, 
I be the identity matrix and the 5 × 5 symmetric matrix A and B be generate 
randomly with iid entry sample from the standard normal distribution. Our interest 
in these inner approximation to the set of psd matrix stem from the fact that 
optimization over them can be do by linear and second order cone program 
respectively (see Theorem 10 below, which be more general). For now, let u relate 
these matrix back to dsos and sdsos polynomials. 

Theorem 5. A polynomial p of degree 2d be dsos if and only if it admits a rep- 
resentation a p(x) = zT (x)Qz(x), where z(x) be the standard monomial vector of 
degree ≤ d and Q be a dd matrix. 

The follow lemma by Barker and Carlson provide an extreme ray characteri- 
zation of the set of diagonally dominant matrix and will be use in the proof of the 
above theorem. 

Lemma 6 (Barker and Carlson [8]). Let {vi}n 
2 

i=1 be the set of all nonzero vector 
in Rn with at most 2 nonzero components, each equal to ±1. Then, a symmetric n×n 
matrix M be diagonally dominant if and only if it can be write a 

M = 

n2∑ 
i=1 

ηiviv 
T 
i , 

for some ηi ≥ 0. 
Proof of Theorem 5. Suppose first that p(x) = zT (x)Qz(x) with Q diagonally 

dominant. Lemma 6 implies that 

p(x) = 

n2∑ 
i=1 

( 
ηiz 

T (x)viv 
T 
i z(x) 

) 
= 

n2∑ 
i=1 

ηi(v 
T 
i z(x)) 

2, 



DSOS AND SDSOS OPTIMIZATION 11 

where ηi ≥ 0 and {vi} be the set of all nonzero vector in Rn with at most 2 nonzero 
components, each equal to ±1. In this sum, the vector vi that have only one nonzero 
entry lead to the first sum in the dsos expansion (8). The vector vi with two 1 or 
two −1s lead to the second sum, and those with one 1 and one −1 lead to the third. 

For the reverse direction, suppose p be dsos, i.e., have the representation in (8). We 
will show that 

p(x) = zT (x) 
∑ 
k 

Qkz(x), 

where each Qk be dd and corresponds to a single term in the expansion (8). As the 
sum of dd matrix be dd, this would establish the proof. For each term of the form 
αim 

2 
i (x) we can take Qk to be a matrix of all zero except for a single diagonal entry 

correspond to the monomial mi in z(x), with this entry equal αi. For each term 
of the form β+ij(mi(x)+mj(x)) 

2, we can take Qk to be a matrix of all zero except for 
four entire correspond to the monomials mi(x) and mj(x) in z(x). All these four 
entry will be set to β+ij . Similarly, for each term of the form β 

− 
ij(mi(x) −mj(x))2, 

we can take Qk to be a matrix of all zero except for four entire correspond to 
the monomials mi and mj(x) in z(x). In this 2× 2 submatrix, the diagonal element 
will be β−ij and the off-diagonal element will be −β 

− 
ij . Clearly, all the Qk matrix 

we have construct be dd. 

Theorem 7. A polynomial p of degree 2d be sdsos if and only if it admits a 
representation a p(x) = zT (x)Qz(x), where z(x) be the standard monomial vector of 
degree ≤ d and Q be a sdd matrix. 

Proof. Suppose first that p(x) = zT (x)Qz(x) with Q scale diagonally dominant. 
Then, there exists a diagonal matrix D, with positive diagonal entries, such that DQD 
be diagonally dominant and 

p(x) = (D−1z(x))TDQD(D−1z(x)). 

Now an argument identical to that in the first part of the proof of Theorem 5 (after 
replace z(x) with D−1z(x)) give the desire sdsos representation in (9). 

For the reverse direction, suppose p be sdsos, i.e., have the representation in (9). 
We show that 

p(x) = zT (x) 
∑ 
k 

Qkz(x), 

where each Qk be sdd and corresponds to a single term in the expansion (9). As the 
sum of sdd matrix be sdd,5 this would establish the proof. Indeed, for each term 
of the form αim 

2 
i (x) we can take Qk (once again) to be a matrix of all zero except 

for a single diagonal entry correspond to the monomial mi in z(x), with this entry 
equal αi. 

For each term of the form (β̂+ijmi(x) + β̃ 
+ 
ijmj(x)) 

2, we can take Qk to be a matrix 
of all zero except for four entire correspond to the monomials mi(x) and mj(x) 

in z(x). This 2 × 2 block will then be 

( 
β̂+ij 
β̃+ij 

)( 
β̂+ij β̃ 

+ 
ij 

) 
. Similarly, for each term of 

the form (β̂−ijmi(x)− β̃ 
− 
ijmj(x)) 

2, we can take Qk to be a matrix of all zero except for 
four entire correspond to the monomials mi and mj(x) in z(x). This 2× 2 block 

5This claim be not obvious from the definition but be apparent from Lemma 9 below which implies 
that SDDn be a convex cone. 



12 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

will then be 

( 
β̂−ij 
−β̃−ij 

)( 
β̂−ij −β̃ 

− 
ij 

) 
. It remains to show that a 2 × 2 rank-1 positive 

semidefinite matrix be sdd.6 Let A = 

( 
a 
b 

)( 
a b 

) 
be such a matrix. Observe that 

( 
1 
a 0 
0 1b 

) 
A 

( 
1 
a 0 
0 1b 

) 
= 

( 
1 1 
1 1 

) 
be dd and hence A be by definition sdd. (Note that if a or b be zero, then A be already 
dd.) 

The follow characterization of sdd matrix will be important for us. 

Theorem 8 (see theorem 8 and 9 by Boman et al. [16]). A symmetric matrix 
Q be sdd if and only if it have “factor width” at most 2; i.e., a factorization Q = V V T , 
where each column of V have at most two nonzero entries. 

Lemma 9. A symmetric n× n matrix Q be sdd if and only if it can be express 
a 

(11) Q = 
∑ 
i<j 

M ij , 

where each M ij be an n × n matrix with zero everywhere except for four entry 

(M ij)ii, (M 
ij)ij , (M 

ij)ji, (M 
ij)jj, which make the 2 × 2 matrix 

[ 
(M ij)ii (M 

ij)ij 
(M ij)ji (M 

ij)jj 

] 
symmetric and positive semidefinite. 

Proof. This be almost immediate from Theorem 8: If Q = V V T for some n × k 
matrix V , then Q = 

∑k 
l=1 vlv 

T 
l , where vl denotes the l-th column of V . Since each 

column vl have at most two nonzero entries, say in position i and j, each matrix vlv 
T 
l 

will be exactly of the desire form M ij in the statement of the lemma. Conversely, 
if Q can be write a in (11), then we can write each M ij a M ij = wij,1w 

T 
ij,1 + 

wij,2w 
T 
ij,2, where the vector wij,1 and wij,2 have at most two nonzero entries. If we 

then construct a matrix V which have the collection of the vector wij,1 and wij,2 a 
columns, we will have Q = V V T . 

Theorem 10. For any fix d, optimization over DSOSn,2d (resp. SDSOSn,2d) 
can be do with a linear program (resp. second order cone program) of size polynomial 
in n. 

Proof. We will use the characterization of dsos/sdsos polynomial give in theo- 
rem 5 and 7. In both cases, the equality p(x) = zT (x)Qz(x),∀x can be impose by a 
finite set of linear equation in the coefficient of p and the entry of Q (these match 
the coefficient of p(x) with those of zT (x)Qz(x)). The constraint that Q be dd can 
be imposed, e.g., by a set of linear inequality 

Qii ≥ 
∑ 
j 6=i zij ,∀i, 

−zij ≤ Qij ≤ zij ,∀i, j, i 6= j 

in variable Qij and zij . This give a linear program. 
The constraint that Q be sdd can be impose via Lemma 9 by a set of equal- 

ity constraint that enforce equation (11). The constraint that each 2 × 2 matrix 

6This also implies that any 2× 2 positive semidefinite matrix be sdd. 



DSOS AND SDSOS OPTIMIZATION 13[ 
(M ij)ii (M 

ij)ij 
(M ij)ji (M 

ij)jj 

] 
be psd be a “rotated quadratic cone” constraint and can be im- 

pose use SOCP [5, 51]: 

(M ij)ii + (M 
ij)jj ≥ 0, 

∣∣∣∣∣∣( 2(M ij)ij 
(M ij)ii − (M ij)jj 

) ∣∣∣∣∣∣≤ (M ij)ii + (M ij)jj . 
In both cases, the final LP and SOCP be of polynomial size because the size of the 
Gram matrix be 

( 
n+d 
d 

) 
, which be polynomial in n for fix d. 

We have write publicly-available code that automates the process of generate 
LPs (resp. SOCPs) from dsos (resp. sdsos) constraint on a polynomial. More 
information about this can be found in the appendix. 

The ability to replace SDPs with LPs and SOCPs be what result in significant 
speedup in our numerical experiment (see Section 4). For the special case where the 
polynomial involve be quadratic forms, we get a systematic way of inner approxi- 
mating semidefinite programs. A generic SDP that minimizes Tr(CX) subject to the 
constraint Tr(AiX) = bi, i = 1, . . . ,m, and X � 0, can be replace by 

• a diagonally dominant program (DDP); i.e., a problem of the form 

minimize 
X∈Sn 

Tr(CX)(12) 

s.t. Tr(AiX) = bi, i = 1, . . . ,m, 

X dd, 

which be an LP, or 
• a scale diagonally dominant program (SDDP); i.e., a problem of the form 

minimize 
X∈Sn 

Tr(CX)(13) 

s.t. Tr(AiX) = bi, i = 1, . . . ,m, 

X sdd, 

which be an SOCP. 
DDP and SDDP fit nicely into the framework of conic program a they be 

optimization problem over the intersection of an affine subspace with a proper cone 
(see, e.g., [9, Chap. 2] for a review of conic programming). In section 4.4 and 4.5, 
we show application of this idea to problem in finance and statistics. 

3.2. The cone of r-dsos and r-sdsos polynomial and asymptotic guar- 
antees. We now present a hierarchy of cone base on the notion of dsos and sdsos 
polynomial that can be use to good approximate the cone of nonnegative polyno- 
mials. 

Definition 11. For an integer r ≥ 0, we say that a polynomial p := p(x1, . . . , xn) 
be r-dsos (resp. r-sdsos) if 

p(x) · ( 
n∑ 
i=1 

x2i ) 
r 

be dsos (resp. sdsos). We denote the set of polynomial in n variable and degree 2d 
that be r-dsos (resp. r-sdsos) by rDSOSn,2d (resp. rDSOSn,2d). 

Note that for r = 0 we recover our dsos/sdsos definitions. Moreover, because the 
multiplier ( 

∑n 
i=1 x 

2 
i ) 
r be nonnegative, we see that for any r, the property of be 



14 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

(a) The LP-based r-dsos hierarchy. (b) The SOCP-based r-sdsos hierarchy. 

Fig. 3. The improvement obtain by go one level up in the r-dsos/r-sdsos hierarchy for 
the family of bivariate quartic in (10). 

r-dsos or r-sdsos be a sufficient condition for nonnegativity: 

rDSOSn,2d ⊆ rSDSOSn,2d ⊆ PSDn,2d. 

Optimizing over r-dsos (resp. r-sdsos) polynomial be still an LP (resp. SOCP). 
The proof of the follow theorem be identical to that of Theorem 10 and hence 
omitted. 

Theorem 12. For any fix d and r, optimization over the set rDSOSn,d (resp. 
rSDSOSn,d) can be do with linear program (resp. second order cone program- 
ming) of size polynomial in n. 

If we revisit the parametric family of bivariate quartic in (10), the improvement 
obtain by go one level high in the hierarchy be illustrate in Figure 3. Interest- 
ingly, for r ≥ 1, r-dsos and r-sdsos test for nonnegativity can sometimes outperform 
the so test. The follow two example illustrate this. 

Example 3.1. Consider the Motzkin polynomial [59], M(x) = x41x 
2 
2 + x 

2 
1x 

4 
2 − 

3x21x 
2 
2x 

2 
3+x 

6 
3, which historically be the first know example of a nonnegative polynomial 

that be not a sum of squares. One can give an LP-based nonnegativity certificate of 
this polynomial by show that M ∈ 2DSOS3,6. Hence, 2DSOS3,6 * SOS3,6. 

Example 3.2. Consider the polynomial p(x) = x41x 
2 
2 + x 

4 
2x 

2 
3 + x 

4 
3x 

2 
1 − 3x21x22x23. 

Once again this polynomial be nonnegative but not a sum of square [74]. However, 
by solve an LP feasibility problem, one can check that p ∈ 1DSOS3,6, which prof 
that p be nonnegative, and that 1DSOS3,6 * SOS3,6. 

It be natural to ask whether every nonnegative polynomial be r-dsos (or r-sdsos) 
for some r? The follow theorem deal with this question and provide asymptotic 
guarantee on r-dsos (and hence r-sdsos) hierarchies. Their proof rely on classical 
Positivstellensätze from real algebraic geometry. 

Theorem 13. Let p be an even7 positive definite form. Then, there exists an 
integer r for which p be r-dsos (and hence r-sdsos). 

Proof. A well-known theorem of Pólya [69] state that if a form f(x1, . . . , xn) be 
positive on the simplex (i.e., the set ∆n := {x ∈ Rn|xi ≥ 0, 

∑ 
i xi = 1}), then there 

exists an integer r such that all coefficient of 

(x1 + · · ·+ xn)rf(x1, . . . , xn) 

7An even polynomial be a polynomial whose individual variable be raise only to even degrees. 



DSOS AND SDSOS OPTIMIZATION 15 

be positive. Under the assumption of the theorem, this implies that there be an r 
for which the form p( 

√ 
x1, . . . , 

√ 
xn)(x1 + · · ·+ xn)r and hence the form 

p(x1, . . . , xn)(x 
2 
1 + · · ·+ x2n) 

have positive coefficients. But this mean that p(x)( 
∑ 
i x 

2 
i ) be a nonnegative weight 

sum of square monomials and therefore clearly dsos (see (8)). (In a Gram matrix 
representation p(x)( 

∑ 
i x 

2 
i ) = z 

T (x)Qz(x), the argument we just give show that we 
can take Q to be diagonal.) 

We remark that even form already constitute a very interest class of polyno- 
mials since they include, e.g., all polynomial come from copositive program ; 
see Subsection 4.2. In fact, any NP-complete problem can be reduce in polynomial 
time to the problem of check whether a degree-4 even form be nonnegative [60]. An 
application of this idea to the independent set problem in combinatorial optimization 
be present in Subsection 4.2. 

The next proposition show that the evenness assumption cannot be remove 
from Theorem 13. 

Proposition 14. For any 0 < a < 1, the quadratic form 

(14) p(x1, x2, x3) = (x1 + x2 + x3) 
2 + a(x21 + x 

2 
2 + x 

2 
3) 

be positive definite but not r-sdsos for any r. 

Proof. Positive definiteness be immediate from have a > 0. Let u show that 
the quadratic form (x1 + x2 + x3) 

2 be not r-sdsos for any r. Consider the polynomial 

q(x) := (x1 + x2 + x3) 
2(x21 + x 

2 
2 + x 

2 
3) 
r, 

which be a form of degree 2r+ 2. Note that the coefficient of x2r+21 in q be 1. Observe 
also that q have the follow two monomials: 2x2r+11 x2 and 2x 

2r+1 
1 x3. Suppose for 

the sake of contradiction that q be sdsos. Then it could be write a 

q(x) = 
∑ 
i 

αim 
2 
i (x) + 

∑ 
i,j 

(β̂+ijmi(x) + β̃ 
+ 
ijmj(x)) 

2 + 
∑ 
i,j 

(β̂−ijmi(x)− β̃ 
− 
ijmj(x)) 

2, 

where all monomials mi(x) and mj(x) be of degree r+1. So to produce the monomials 
2x2r+11 x2 and 2x 

2r+1 
1 x3 in q, the right hand side above must include the term 

(xr1x2 + x 
r+1 
1 ) 

2 + (xr1x3 + x 
r+1 
1 ) 

2. 

But this also produce 2x2r+21 , which cannot be canceled. This contradicts the fact 
that the coefficient of x2r+21 in q be 1. 

To see that for any a ∈ (0, 1) the polynomial p in the statement of the proposition 
cannot be r-sdsos either, one can repeat the exact same argument and observe that 
the coefficient of x2r+21 would still not match. 

We remark that the form in (14) can be prove to be positive the improvement 
present in Section 5, in particular with a “factor width 3” proof system from Sub- 
section 5.3. Alternatively, one can work with the next theorem, which remove the 
assumption of evenness from Theorem 13 at the cost of double the number of vari- 
ables and the degree.8 

8Instead of Theorem 15, an early version of this paper be quote a result by Habicht [33] 
from [73, p. 1], [71, p. 8]. We thank Claus Scheiderer for inform u that the statement of 
Habicht [33] in German be be misquote by u and the above references. 



16 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

Theorem 15 (see Section 4 of [2]). An n-variate form p : = p(x) of degree 2d be 
positive definite if and only if there exists a positive integer r that make the follow 
form r-dsos: 

p(v2 − w2)− 1√ 
r 

( 

n∑ 
i=1 

(v2i − w2i )2)d + 
1 

2 
√ 
r 

( 

n∑ 
i=1 

(v4i + w 
4 
i )) 

d. 

In [2, Section 4], this theorem be use to construct LP and SOCP-based converge 
hierarchy of low bound for the general polynomial optimization problem (2) when 
the feasible set be compact. 

4. Numerical example and applications. In this section, we consider sev- 
eral numerical example that highlight the scalability of the approach present in 
this paper. Comparisons to exist method on example consider in the litera- 
ture be provide whenever possible. A software package write use the Systems 
Polynomial Optimization Toolbox (SPOT) [57] include a complete implementation 
of the present method and be available online9. The toolbox feature efficient poly- 
nomial algebra and allows u to setup the large-scale LPs and SOCPs arise from 
our examples. The Appendix provide a brief introduction to the software package 
and the associate functionality require for set up the DSOS/SDSOS program 
consider in this paper. The LPs and SOCPs result from our program be solve 
with the MOSEK solver on a machine with 4 processor and a clock speed of 3.4 
GHz and 16GB RAM. Running time for comparison to SDP-based approach be 
present for the MOSEK SDP solver10. 

4.1. Lower bound on polynomial optimization problems. An important 
application of sum of square optimization be to obtain low bound on polynomial 
optimization problems. In this subsection, we consider the particular problem of 
minimize a homogeneous polynomial p(x) of degree 2d on the unit sphere (i.e., the 
set {x ∈ Rn| xTx = 1}). This well-studied problem be strongly NP-hard even when 
d = 2. Its optimal value be easily see to be equivalent to the optimal value of the 
follow problem: 

maximize 
γ 

γ(15) 

s.t. p(x)− γ(xTx)d ≥ 0,∀x ∈ Rn. 

By replace the nonnegativity condition with a SOS/DSOS/SDSOS constraint, 
the result problem becomes an SDP/LP/SOCP. Tables 1 and 2 compare optimal 
value and run time respectively on problem of increase size. We restrict 
ourselves to quartic form and vary the number of variable n between 5 and 70. 
Table 1 compare the optimal value for the DSOS, SDSOS, 1-DSOS, 1-SDSOS and 
SOS relaxation of problem (15) on quartic form whose coefficient be fully dense 
and drawn independently from the standard normal distribution (we consider a single 
random instance for each n). We note that the optimal value present here be 
representative of numerical experiment we perform for a broader set of random 

9Link to spotless isos software package: 
https://github.com/anirudhamajumdar/spotless/tree/spotless isos 

10We note that the MOSEK solver be much faster on our problem than the more commonly use 
SDP solver SeDuMi [81]. 

https://github.com/anirudhamajumdar/spotless/tree/spotless_isos 


DSOS AND SDSOS OPTIMIZATION 17 

n = 10 n = 15 n = 20 n = 25 n = 30 n = 40 n = 50 n = 60 n = 70 
DSOS -5.31 -10.96 -18.012 -26.45 -36.85 -62.30 -94.26 -133.02 -178.23 

SDSOS -5.05 -10.43 -17.33 -25.79 -36.04 -61.25 -93.22 -131.64 -176.89 
1-DSOS -4.96 -9.22 -15.72 -23.58 NA NA NA NA NA 

1-SDSOS -4.21 -8.97 -15.29 -23.14 NA NA NA NA NA 
SOS -1.92 -3.26 -3.58 -3.71 NA NA NA NA NA 

BARON -175.41 -1079.89 -5287.88 - -28546.1 - - - - 
Table 1 

Comparison of low bound on the minimum of a quartic form on the sphere for vary 
number of variables. 

n = 10 n = 15 n = 20 n = 25 n = 30 n = 40 n = 50 n = 60 n = 70 
DSOS 0.30 0.38 0.74 15.51 7.88 10.68 25.99 58.10 342.76 

SDSOS 0.27 0.53 1.06 8.72 5.65 18.66 47.90 109.54 295.30 
1-DSOS 0.92 6.26 37.98 369.08 ∞ ∞ ∞ ∞ ∞ 

1-SDSOS 1.53 14.39 82.30 538.5389 ∞ ∞ ∞ ∞ ∞ 
SOS 0.24 5.60 82.22 1068.66 ∞ ∞ ∞ ∞ ∞ 

BARON 0.35 0.62 3.69 - - - - - - 
Table 2 

Comparison of run time (in seconds) average over 10 instance for low bound a 
quartic form on the sphere for vary number of variables. 

instance (including coefficient drawn from a uniform distribution instead of the 
normal distribution). Also, in all our experiments, we could verify that the SOS 
bound actually coincides with the true global optimal value of the problem. This be 
do by find a point on the sphere that produce a match upper bound. 

As the table illustrate, both DSOS and SDSOS scale up to problem of dimension 
n = 70. This be well beyond the size of problem handle by SOS programming, 
which be unable to deal with problem with n large than 25 due to memory (RAM) 
constraints. While the price we pay for this increase scalability be suboptimality, this 
be tolerable in many application (as we show in other example in this section). 

Table 1 also compare the approach present in this paper to the low bound 
obtain at the root node of the branch-and-bound procedure in the popular global 
optimization package BARON [78].11 This comparison be only do up to dimension 
30 since beyond this we ran into difficulty passing the large polynomial to BARON’s 
user interface. 

Table 2 present run times12 average over 10 random instance for each n. 
There be a significant difference in run time between DSOS/SDSOS and SOS 
begin at n = 15. While we be unable to run SOS program beyond n = 25, 
DSOS and SDSOS program for n = 70 take only a few minute to execute. Memory 
constraint prevent u from execute program for n large than 70. The result 
indicate, however, that it may be possible to run large program within a reasonable 
amount of time on a machine equip with more RAM. 

4.2. Copositive program and combinatorial optimization. A sym- 
metric matrix Q ∈ Rn×n be copositive if xTQx ≥ 0 for all x ≥ 0 (i.e., for all x in the 
nonnegative orthant). The problem of optimize a linear function over affine section 
of the cone Cn of n × n copositive matrix have recently receive a lot of attention 

11We thank Aida Khajavirad for run the BARON experiment for us. 
12The BARON instance be implement on a 64-bit Intel Xeon X5650 2.66Ghz processor use 

the CPLEX LP solver. 



18 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

from the optimization community [27], since it can exactly model several problem of 
combinatorial and nonconvex optimization [17], [27], [19]. For instance, the size α(G) 
of the large independent set 13 of a graph G on n node be equal to the optimal value 
of the follow copositive program [22]: 

minimize 
γ 

γ 

s.t. γ(A+ I)− J ∈ Cn,(16) 

where A be the adjacency matrix of the graph, I be the identity matrix, and J be the 
matrix of all ones. 

It be easy to see that a matrix Q be copositive if and only if the quartic form 
(x.2)TQ(x.2), with x.2 := (x21, x 

2 
2, . . . , x 

2 
n) 
T , be nonnegative. Hence, by require this 

form to be r-sos/r-sdsos/r-dsos, one obtains inner approximation to the cone Cn that 
can be optimize over use semidefinite, second order cone, and linear programming. 
The so version of this idea go back to the PhD thesis of Parrilo [65]. 

In this section, we compare the quality of these bound use another hierarchical 
LP-based method in the literature [17], [22]. The r-th level of this LP hierarchy 
(referred to a the Pólya LP from here onwards) require that the coefficient of the 
form (x.2)TQ(x.2) ·(x21 + · · ·+x2n)r be nonnegative. This be motivate by a theorem by 
Pólya, which state that this constraint will be satisfied for r large enough (assuming 
xTQx > 0 for all nonzero x ≥ 0). It be not difficult to see that the Pólya LP be always 
dominate by the LP which require (x.2)TQ(x.2) to be r-dsos. This be because the 
latter require the underlie Gram matrix to be diagonally dominant, while the 
former require it to be diagonal, with nonnegative diagonal entries. 

In [22], de Klerk and Pasechnik show that when the Pólya LP be apply to ap- 
proximate the copositive program in (16), then the precise independent set number of 
the graph be obtain within α(G)2 step of the hierarchy. By the previous argument, 
the same claim immediately hold for the r-dsos (and hence r-sdsos) hierarchies. 

Table 3 revisits Example 5.2 of [17], where upper bound on the independent set 
number of the complement of the graph of an icosahedron be computed. (This be a 
graph on 12 node with independent set number equal to 3; see [17] for it adjacency 
matrix.) As the result illustrate, SOS program provide an exact upper bound 
(since the size of the large stable set be necessarily an integer). The second level 
of the r-dsos and r-sdsos hierarchy also provide an exact upper bound. In contrast, 
the LP base on Polya’s theorem in [17] give the upper bound ∞ for the 0-th and 
the 1-st level, and an upper bound of 6 at the second level. In contrast to the Pólya 
LP, one can show that the 0-th level of the r-dsos LP always produce a finite upper 
bound on the independent set number. In fact, this bound will always be small than 
n−min di + 1, where di be the degree of node i [1, Theorem 5.1] . 

4.3. Convex regression in statistics. Next, we consider the problem of fitting 
a function to data subject to a constraint on the function’s convexity. This be an 
important problem in statistic and have a wide domain of application include value 
function approximation in reinforcement learning, option pricing and model of 
circuit for geometric program base circuit design [34, 35]. Formally, we be 
give N pair of data point (xi, yi) with xi ∈ Rn, yi ∈ R and our task be to find a 

13An independent set of a graph be a subset of it node no two of which be connected. The 
problem of find upper bound on α have many application in schedule and cod theory [52]. 



DSOS AND SDSOS OPTIMIZATION 19 

Polya LP r-DSOS r-SDSOS SOS 

r = 0 ∞ 6.000 6.000 3.2362 
r = 1 ∞ 4.333 4.333 NA 
r = 2 6.000 3.8049 3.6964 NA 

Table 3 
Comparison of upper bound on the size of the large independent set in the complement of 

the graph of an icosahedron. 

DSOS SDSOS SOS 

d = 2 35.11 33.92 21.28 

d = 4 14.86 12.94 NA 
Table 4 

Comparison of fitting error for convex re- 
gression. 

DSOS SDSOS SOS 

d = 2 0.49 0.66 0.53 

d = 4 145.85 240.18 ∞ 
Table 5 

Comparison of run time (in s) for con- 
vex regression. 

convex function f from a family F that minimizes an appropriate notion of fitting 
error (e.g., L1 error): 

minimize 
f∈F 

N∑ 
i=1 

|f(xi)− yi| 

s.t. f be convex. 

The convexity constraint here can be impose by require that the function wTH(x)w 
in 2n variable (x,w) associate with the Hessian matrix H(x) of f(x) be nonnegative. 
Restricting ourselves to polynomial function f of bound degree, we obtain the 
follow optimization problem: 

minimize 
f∈Rd[x] 

N∑ 
i=1 

|f(xi)− yi| 

s.t. wTH(x)w ≥ 0,∀x,w, 

where H(x) be again the Hessian of f(x). As before, we can replace the nonnegativity 
constraint with a dsos/sdsos/sos constraint. For our numerical experiment, we gener- 
ated 300 random vector xi in R20 drawn i.i.d. from the standard normal distribution. 
The function value yi be compute a follows: 

yi = exp(‖xi‖2) + η, 

where η be chosen i.i.d. from the standard normal distribution. Tables 4 and 5 
present the fitting error and run time for the DSOS/SDSOS/SOS program 
result from restrict the class of function F to polynomial of degree d = 2 and 
d = 4. As the result illustrate, we be able to obtain significantly small error with 
polynomial of degree 4 use DSOS and SDSOS (compared to SOS with d = 2), 
while the SOS program for d = 4 do not run due to memory constraints. 

4.4. Options pricing. An important problem in financial economics be that of 
determine the price of a derivative security such a an option give the price of the 
underlie asset. Significant progress be make in tackle this problem with the 
advent of the Black-Scholes formula, which operates under two assumptions: (i) the 



20 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

underlie stock price be govern by geometric Brownian motion, and (ii) there be 
no arbitrage. A natural question that have be consider in financial mathematics 
be to see if one can provide bound on the price of an option in the no-arbitrage 
set give much more minimal assumption on the dynamic of the stock price. 
One approach to this question—studied, e.g., in [13], [18], —is to assume that we be 
only give the first k moment of the stock price and want to optimally bound the 
price of the option. 

More precisely, we consider that we be give m stocks, an associate option with 
payoff function φ : Rm+ → R (which will typically depend on the strike price of the 
option), a vector of n moment functions, fi : R+m → R, i = 0, 1, . . . , n (f0 be assume 
to be 1), and the correspond vector of moment q = (q0, . . . , qn) (here q0 = 1). The 
problem of upper bound the price of the option can then be formulate a follow 
[13]: 

maximize 
π 

Eπ[φ(X)](17) 

s.t. Eπ[fi(X)] = qi, i = 0, . . . , n, 

π(x) ≥ 0,∀x ∈ Rm+ . 

Here, the expectation be take over all Martingale measure π, define on Rm+ . We 
consider here the case where one be give the mean µ and covariance matrix σ of the 
stock prices. Problem (17) can then be cast a the follow problem (see [13, Section 
6.2]) 

minimize 
y,Y 

y0 + 

n∑ 
i=1 

yiµi + 

n∑ 
i=1 

n∑ 
j=1 

yij(σij + µiµj)(18) 

s.t. xTY x+ yTx+ y0 ≥ φ(x), ∀x ≥ 0, 

which for many common function φ of interest (see e.g. below) give rise to a copos- 
itive program. A well-known and obvious sufficient condition for a matrix M to be 
copositive be for it to have a decomposition M = P + N , where P be psd and N be 
element-wise nonnegative [65]. This allows one to apply SDP to problem (18) and ob- 
tain upper bound on the option price. By replace the psd condition on the matrix 
P with a dd/sdd condition, we obtain a DDP/SDDP. 

We first compare the DDP/SDDP/SDP approach on an example from [18, 
Section 4], which considers the problem of upper bound the price of a European 
call option with m = 3 underlie assets. The vector of mean of the asset be 
µ = [44.21, 44.21, 44.21]T and the covariance matrix is: 

(19) σ = 

184.04 164.88 164.88164.88 184.04 164.88 
164.88 164.88 184.04 

 . 
Further, we have the follow payoff function, which depends on the strike price 

K: 

(20) φ(x) = max(x1 −K,x2 −K,x3 −K, 0). 

Table 6 compare the upper bound for different strike price use DDP/SDDP/SDP. 
In each case, the upper bound obtain from the SDP be exact. This can be verify 
by find a distribution that achieves the upper bound (i.e., find a match 



DSOS AND SDSOS OPTIMIZATION 21 

Exact SDP SDDP DDP 

K = 30 21.51 21.51 21.51 132.63 

K = 35 17.17 17.17 17.17 132.63 

K = 40 13.20 13.20 13.20 132.63 

K = 45 9.84 9.84 9.85 132.63 

K = 50 7.30 7.30 7.30 132.63 
Table 6 

Comparison of upper bound on option price for different strike price obtain use SDP, 
SDDP and DDP. Here, the number of underlie asset be m = 3. 

SDP SDDP DDP 

Upper bound 18.76 19.42 252.24 

Running time 2502.6 s 24.36 s 11.85 s 
Table 7 

Comparison of upper bound and run time for a large (m = 50) option pricing example 
use SDP, SDDP and DDP. 

low bound). For example, when K = 30, the distribution support on the set of 
four point (5.971, 5.971, 5.971), (54.03, 46.02, 46.02, 46.02), (46.02, 54.03, 46.02) and 
(46.02, 46.02, 54.03) in R3 with probability mass 0.105, 0.298, 0.298, 0.298 respec- 
tively do the job. The upper bound obtain use SDDP be almost identical to the 
bound from SDP for each strike price. The DDP bound be loose, and interestingly 
do not change with the strike price. Running time for the different method on 
this small example be negligible and hence not presented. 

In order to demonstrate the scalability of DDP and SDDP, we consider a large 
scale randomize example with m = 50 underlie assets. The mean and covariance 
be take to be the sample mean and covariance of 100 instance of vector belonging 
to Rm with element drawn uniformly and independently from the interval [0, 10]. 
The strike price be K = 5 and the payoff function be again 

(21) φ(x) = max(x1 −K,x2 −K, . . . , xm −K, 0). 

Table 7 compare upper bound and run time for the different methods. 
SDDP provide u with a bound that be very close to the one obtain use SDP, with 
a significant speedup in computation (approximately a factor of 100). The run 
time for DDP be comparable to SDDP, but the bound be not a tight. 

4.5. Sparse PCA. Next, we consider the problem of sparse principal component 
analysis (sparse PCA). In contrast to standard PCA, where the principal component 
(PCs) in general depend on all the observe variables, the goal of sparse PCA be to 
identify principal component that only depend on small subset of the variable [87]. 
While the statistical fidelity of the result representation of the data in term of 
sparse PCs will in general be low than the standard PCs, sparse PCs can significantly 
enhance interpretability of the results. This feature have prove to be very useful 
in application such a finance and analysis of gene expressions; see, e.g., [21] and 
reference therein. 

Given a n× n covariance matrix A, the problem of find sparse principal com- 



22 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

ponents can be write a the follow optimization problem: 

maximize 
x 

xTAx(22) 

s.t. ‖x‖2 = 1, 
Card(x) ≤ k. 

Here, Card(x) be the cardinality (number of non-zero entries) of x and k be a give 
threshold. The problem can be reformulate a the follow rank-constrained matrix 
optimization problem [21]: 

maximize 
X 

Tr(AX)(23) 

s.t. Tr(X) = 1, 

Card(X) ≤ k2, 
X � 0, Rank(X) = 1. 

In [21], the author propose “DSPCA”, an SDP relaxation of this problem that be 
obtain by drop the rank constraint and replace the cardinality constraint 
with a constraint on the l1 norm: 

maximize 
X 

Tr(AX)(24) 

s.t. Tr(X) = 1, 

1T |X|1 ≤ k, 
X � 0. 

The optimal value of problem (24) be an upper bound on the optimal value of (22). 
Further, when the solution X1 to (24) have rank equal to one, the SDP relaxation be 
tight and the dominant eigenvector x1 of X1 be the optimal load of the first sparse 
PC [21]. When a rank one solution be not obtained, the dominant eigenvector can still 
be retain a an approximate solution to the problem. Further sparse PCs can be 
obtain by deflate A to obtain: 

A2 = A− (xT1 Ax1)x1xT1 

and re-solving the SDP with A2 in place of A (and iterate this procedure). 
The framework present in this paper can be use to obtain LP and SOCP 

relaxation of (22) by replace the constraint X � 0 by the constraint X ∈ DD∗n 
or X ∈ SDD∗n respectively, where DD∗n and SDD∗n be the dual cone of DDn and 
SDDn (see [3, Section 3.3] for a description of these dual cones). Since S 

+ 
n ⊆ SDD∗n ⊆ 

DD∗n, we be guaranteed that the result optimal solution will be upper bound 
on the SDP solution. Further, a in the SDP case, the relaxation be tight when a 
rank-one solution be obtained, and once again, if a rank-one solution be not obtained, 
we can still use the dominant eigenvector a an approximate solution. 

We first consider Example 6.1 from [21]. In this example, there be three hidden 
variable distribute normally: 

V1 ∼ N (0, 290), V2 ∼ N (0, 300), V3 = −0.3V1 + 0.925V2 + �, � ∼ N (0, 1). 

These hidden variable generate 10 observe variables: 

Xi = Vj + � 
j 
i , � 

j 
i ∼ N (0, 1), 



DSOS AND SDSOS OPTIMIZATION 23 

X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 expl. var. 
PCA, PC1 0.116 0.116 0.116 0.116 -0.395 -0.395 -0.395 -0.395 -0.401 -0.401 60.0 % 
PCA, PC2 -0.478 -0.478 -0.478 -0.478 -0.145 -0.145 -0.145 -0.145 0.010 0.010 39.6 % 
Other, PC1 0 0 0 0 -0.5 -0.5 -0.5 -0.5 0 0 40.9 % 
Other, PC2 0.5 0.5 0.5 0.5 0 0 0 0 0 0 39.5 % 

Table 8 
Comparison of loading of principal component and explain variance for standard PCA and 

sparse versions. Here, the label “Other” denotes sparse PCA base on SDP, DDP, and SDDP. Each 
of these give the optimal sparse solution. 

DDP SDDP DSPCA 
No. Time (s) NNZ Opt. Expl. Time (s) NNZ Opt. Expl. Time (s) NNZ Opt. Expl. 
1 0.89 4 35.5 0.067 % 1.40 14 30.6 0.087 % 1296.9 5 30.5 0.086 % 
2 1.18 4 36.6 0.071 % 1.42 14 34.4 0.099 % 1847.3 6 33.6 0.092 % 
3 1.46 4 49.5 0.079 % 1.40 24 41.3 0.097 % 1633.0 16 40.2 0.096 % 
4 1.14 4 44.1 0.072 % 1.80 17 38.7 0.10 % 1984.7 8 37.6 0.091 % 
5 1.10 4 36.7 0.060 % 1.53 34 33.0 0.068 % 2179.6 10 31.7 0.105 % 

Table 9 
Comparison of run times, number of non-zero elements, optimal values, and explain 

variance for five large-scale sparse PCA example use covariance matrix of size 100× 100. 

with j = 1 for i = 1, . . . , 4, j = 2 for i = 5, . . . , 8 and j = 3 for i = 9, 10, and �ji 
independent for j = 1, 2, 3 and i = 1, . . . , 10. This knowledge of the distribution of the 
hidden and observe variable allows u to compute the exact 10×10 covariance matrix 
for the observe variables. The sparse PCA algorithm described above can then be 
apply to this covariance matrix. Table 8 present the first two principal component 
compute use standard PCA, DDP, SDDP and DSPCA (corresponding to SDP). 
As the table illustrates, the loading correspond to the first two PCs compute 
use standard PCA be not sparse. All other method (DDP, SDDP, and SDP) give 
exactly the same answer and correspond to a rank-one solution, i.e., the optimal sparse 
solution. As expected, the sparsity come at the cost of a reduction in the variance 
explain by the PCs (see [87] for computation of the explain variance). 

Next, we consider a larger-scale example. We generate five random 100× 100 co- 
variance matrix of rank 4. Table 9 present the run times, number of non-zero 
entry (NNZ) in the top principal component14, optimal value of the program (24) 
(Opt.), and the explain variance (Expl.). The optimal value for DDP and SDDP 
upper bound the SDP solution a expected, and be quite close in value. The num- 
ber of non-zero entry and explain variance be comparable across the different 
methods. The run time be between 1100 and 2000 time faster for DDP in com- 
parison to SDP, and between 900 and 1400 time faster for SDDP. Hence the example 
illustrates that one can obtain a very large speedup with the approach present here, 
with only a small sacrifice in quality of the approximate sparse PCs. 

4.6. Applications in control theory. As a final application of the method 
present in this paper, we consider two example from control theory and robotics. 
The application of SOS program in control theory be numerous and include 
the computation of region of attraction of polynomial system [65], feedback control 
synthesis [40], robustness analysis [83], and computation of the joint spectral radius 
for uncertain linear system [64]. A thorough treatment of the application of the 

14Note that the entry of the PCs be thresholded lightly in order to remove spurious non-zero 
element arise from numerical inaccuracy in the solvers. 



24 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

Fig. 4. An il- 
lustration of the N- 
link pendulum sys- 
tem (with N = 6). 

−0.05 −0.04 −0.03 −0.02 −0.01 0 0.01 0.02 0.03 0.04 0.05 
−0.25 

−0.2 

−0.15 

−0.1 

−0.05 

0 

0.05 

0.1 

0.15 

0.2 

0.25 

θ 
1 
(radians) 

θ̇ 
1 
(r 
ad 
/s 
) 




SOS 
SDSOS 
DSOS 

(a) θ1-θ̇1 subspace. 

−0.05 −0.04 −0.03 −0.02 −0.01 0 0.01 0.02 0.03 0.04 0.05 
−0.4 

−0.3 

−0.2 

−0.1 

0 

0.1 

0.2 

0.3 

0.4 

θ 
6 
(radians) 

θ̇ 
6 
(r 
ad 
/s 
) 




SOS 
SDSOS 
DSOS 

(b) θ6-θ̇6 subspace. 

Fig. 5. Figure reproduce from [53] compare projection of the ROAs 
compute for the 6-link pendulum system use DSOS, SDSOS and SOS 
programming. 

(S)DSOS approach to control problem be beyond the scope of this paper but can be 
found in a different paper [53], which be joint work with Tedrake. Here we briefly 
highlight two example from [53] that we consider particularly representative. 

Most of the application in control theory mention above rely on SOS program- 
ming for check Lyapunov inequality that certify stability of a nonlinear system. 
As discuss in Section 1.1, one can compute (inner approximation of) the region of 
attraction (ROA) of a dynamical system ẋ = f(x) by find a Lyapunov function 
V : Rn → R that satisfies the follow conditions: 

(25) 
V (x) > 0 ∀x 6= 0, and 
V̇ (x) = 〈∇V (x), f(x)〉 < 0 ∀x ∈ {x| V (x) ≤ β, x 6= 0}. 

This guarantee that the set {x| V (x) ≤ β} be a subset of the ROA of the system, 
i.e. initial condition that begin in this sublevel set converge to the origin. For poly- 
nomial dynamical systems, one can specify sufficient condition for (25) use SOS 
program [65]. By replace the SOS constraint with (S)DSOS constraints, we 
can compute inner approximation to the ROA more efficiently. While the approxi- 
mations will be more conservative in general, the ability to tradeoff conservatism with 
computation time and scalability be an important one in control applications. 

4.6.1. Region of attraction for an invert N-link pendulum. As an 
illustration, we consider the problem of compute ROAs for the underactuatedN -link 
pendulum depict in Figure 4. This system have 2N state x = [θ1, . . . , θN , θ̇1, . . . , θ̇N ] 
compose of the joint angle (with the vertical line) and their derivatives. There 
be N − 1 control input (the joint closest to the base be not actuated). We take 
the unstable “upright” position of the system to be the origin of our state space 
and design a Linear Quadratic Regulator (LQR) controller in order to stabilize this 
equilibrium. A polynomial approximation of the dynamic of the close loop system 
be obtain by a Taylor expansion of order 3. Using Lyapunov function of degree 2 
result in the time derivative of the Lyapunov function be quartic and hence yield 
dsos/sdsos/sos constraint on a polynomial of degree 4 in 2N variables. 

Figures 5(a) and 5(b) compare projection of the ROAs compute for the system 
with N = 6 onto two 2−dimensional subspace of the state space. As the plot 
suggest, the ROA compute use SDSOS be only slightly more conservative than the 
ROA compute use SOS programming. In fact, compare the volume of the two 
ROAs, we find: 



DSOS AND SDSOS OPTIMIZATION 25 

Fig. 6. A visualization of the model of the ATLAS humanoid robot, along with the hardware 
platform (inset) on which the parameter of the model be based. (Picture of robot reproduce with 
permission from Boston Dynamics.) 

(26) 
V ol 

1/2N 
ROA-sdsos 

V ol 
1/2N 
ROA-sos 

= 0.79. 

Here, the volume have be correctly rescale in the standard manner by the 
dimension of the ambient space. The ROA compute use DSOS be much small 
(but still potentially useful in practice). 

Comparing run time of the different approach on this example, we find 
that the LP correspond to the DSOS program take 9.67 seconds, and the SOCP 
correspond to the SDSOS program take 25.9 seconds. The run time of the 
SOS program be 1526.5 second use MOSEK and 23676.5 second use SeDuMi. 
Hence, in particular DSOS be approximately 2500 time faster than SOS use SeDuMi 
and 150 time faster than SOS use MOSEK, while SDSOS be 900 time faster in 
comparison to SeDuMi and 60 time faster than MOSEK for SOS. 

Of course the real benefit of our approach be that we can scale to problem where 
SOS program cease to run due to memory/computation constraints. Table 10 
illustrates this ability by compare run time of the program obtain use our 
approach with SOS program for different value of 2N (number of states). As 
expected, for case where the SOS program do run, the DSOS and SDSOS program 
be significantly faster. Further, the SOS program obtain for 2N > 12 be too large 
to run (due to memory constraints). In contrast, our approach allows u to handle 
almost twice a many states. 

2N (# states) 4 6 8 10 12 14 16 18 20 22 

DSOS < 1 0.44 2.04 3.08 9.67 25.1 74.2 200.5 492.0 823.2 

SDSOS < 1 0.72 6.72 7.78 25.9 92.4 189.0 424.74 846.9 1275.6 

SOS (SeDuMi) < 1 3.97 156.9 1697.5 23676.5 ∞ ∞ ∞ ∞ ∞ 
SOS (MOSEK) < 1 0.84 16.2 149.1 1526.5 ∞ ∞ ∞ ∞ ∞ 

Table 10 
Table reproduce from [53] show runtime comparison (in seconds) for ROA computation 

on N-link system. 

4.6.2. Control synthesis for a humanoid robot. In our final example, we 
highlight a robotics application consider in joint work with Tedrake in [53]. Control 
of humanoid robot be an important problem in robotics and present a significant 
challenge due to the nonlinear dynamic of the system and high dimensionality of the 
state space. Here we show how the approach described in this paper can be use to 



26 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

(a) Nominal 
pose 
(fixed 
point) 

(b) Stabilized 
pose 1 

(c) Stabilized 
pose 2 

(d) Stabilized 
pose 3 

(e) Stabilized 
pose 4 

(f) Stabilized 
pose 5 

Fig. 7. Figure reproduce from [53] show the nominal position of the robot, i.e., the fix 
point be stabilize (subplot (a)), and configuration of the robot that be stabilize by the controller 
design use SDSOS program (subplots (b)-(f)). A video of simulation of the controller start 
from different initial condition be available online at http://youtu.be/lmAT556Ar5c. 

design a balance controller for a model of the ATLAS robot show in Figure 6. This 
robot be design and built by Boston Dynamics Inc. and be use for the 2015 
DARPA Robotics Challenge. 

Our model of the robot be base on physical parameter of the hardware platform 
and have 30 state and 14 inputs. The task consider here be to balance the robot on 
it right toe. The balance controller be construct by search for both a quadratic 
Lyapunov function and a linear feedback control law in order to maximize the size of 
the result region of attraction. The Lyapunov condition be impose use SDSOS 
programming. We Taylor expand the dynamic about the equilibrium to degree 3 in 
order to obtain polynomial dynamics. The total computation time be approximately 
22.5 minutes. We note that SOS program be unable to handle this system due to 
memory (RAM) constraints. 

Figure 7 demonstrates the performance of the result controller from SDSOS 
program by plot initial configuration of the robot that be stabilize to the 
fix point. As the plot illustrates, the controller be able to stabilize a very wide range 
of initial conditions. A video of simulation of the close loop system start from 
different initial condition be available online at http://youtu.be/lmAT556Ar5c. 

5. Improvements on DSOS and SDSOS optimization. While DSOS and 
SDSOS technique result in significant gain in term of solve time and scalability, 
they inevitably lead to some loss in solution accuracy when compare to the SOS 
approach. In this section, we briefly outline three possible strategy to mitigate this 
loss. The first two (Sections 5.1 and 5.2) generate sequence of linear and second order 
cone programs, while the third (Section 5.3) work with “small” (fixed-size) semidef- 
inite programs. The reader may recall that the r-DSOS and r-SDSOS hierarchy of 
Section 3.2 could also be use to improve on DSOS and SDSOS techniques. Like the 
first two method that we present here, they do so while stay in the realm of LP 
and SOCP. They differ from these method on two crucial point however: (i) they be 
more expensive to implement, and (ii) they approximate the cone of SOS polynomial 
“blindly” irrespective of a particular objective function. This be in contrast to the 
method that we present in Section 5.1 and 5.2, which approximate the cone in the 
direction of a specific linear objective function. 

For brevity of exposition, we explain how all three strategy can be apply to 

http://youtu.be/lmAT556Ar5c 
http://youtu.be/lmAT556Ar5c 


DSOS AND SDSOS OPTIMIZATION 27 

approximate a generic semidefinite program: 

minimize 
X∈Sn 

Tr(CX)(27) 

s.t. Tr(AiX) = bi, i = 1, . . . ,m, 

X � 0. 

A treatment tailor to the case of sum of square program can be found in the 
reference we provide. 

5.1. Iterative change of basis. In [3], Ahmadi and Hall build on the notion 
of diagonal and scale diagonal dominance to provide a sequence of improve inner 
approximation to the cone Pn of psd matrix in the direction of the objective function 
of an SDP at hand. The idea be simple: define a family of cones15 

DD(U) := {M ∈ Sn | M = UTQU for some dd matrix Q}, 

parametrized by an n × n matrix U . Optimizing over the set DD(U) be an LP 
since U be fixed, and the define constraint be linear in the coefficient of the two 
unknown matrix M and Q. Furthermore, the matrix in DD(U) be all psd; i.e., 
∀U, DD(U) ⊆ Pn. 

The proposal in [3] be to solve a sequence of LPs, indexed by k, by replace the 
condition X � 0 by X ∈ DD(Uk): 

DSOSk := min 
X∈Sn 

Tr(CX)(28) 

s.t. Tr(AiX) = bi, i = 1, . . . ,m, 

X ∈ DD(Uk). 

The sequence of matrix {Uk} be define a follow 

(29) 
U0 = I 

Uk+1 = chol(Xk), 

where Xk be an optimal solution to the LP in (28). 
Note that the first LP in the sequence optimizes over the set of diagonally dom- 

inant matrices. By define Uk+1 a a Cholesky factor of Xk, improvement of the 
optimal value be guaranteed in each iteration. Indeed, a Xk = U 

T 
k+1IUk+1, and the 

identity matrix I be diagonally dominant, we see that Xk ∈ DD(Uk+1) and hence be 
feasible for iteration k+ 1. This implies that the optimal value at iteration k+ 1 be at 
least a good a the optimal value at the previous iteration; i.e., DSOSk+1 ≤ DSOSk 
(in fact, the inequality be strict under mild assumptions; see [3]). 

In an analogous fashion, one can construct a sequence of SOCPs that inner ap- 
proximate Pn with increase quality. This time, we define a family of cone 

SDD(U) := {M ∈ Sn | M = UTQU, for some sdd matrix Q}, 

parameterized again by an n × n matrix U . For any U , optimize over the set 
SDD(U) be an SOCP and we have SDD(U) ⊆ Pn. This lead u to the follow 
iterative SOCP sequence: 

15One can think of DD(U) a the set of matrix that be dd after a change of coordinate via 
the matrix U . 



28 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

SDSOSk := min 
X∈Sn 

Tr(CX)(30) 

s.t. Tr(AiX) = bi, i = 1, . . . ,m, 

X ∈ SDD(Uk). 

Assuming existence of an optimal solution Xk at each iteration, we can define 
the sequence {Uk} iteratively in the same way a be do in (29). Using similar 
reasoning, we have SDSOSk+1 ≤ SDSOSk. In practice, the sequence of upper 
bound {SDSOSk} approach faster to the SDP optimal value than the sequence of 
the LP upper bound {DSOSk}. Figure 8 show the improvement (in every direction) 

(a) LP inner approximation (b) SOCP inner approximation 

Fig. 8. Figure reproduce from [3] show improvement (in all directions) after one iteration 
of the change of basis algorithm. 

obtain just by a single iteration of this approach. The outer set in green in both 
subfigure be the feasible set of a randomly generate semidefinite program. The set in 
black be the DD (left) and the SDD (right) inner approximations. What be show in 
dark blue in both case be the boundary of the improve inner approximation after one 
iteration. Note that the SOCP in particular fill up almost the entire spectrahedron 
in a single iteration. 

We refer the interested reader to [3] for more details, in particular for an expla- 
nation of how the same technique can be use (via duality) to outer approximate 
feasible set of semidefinite programs. 

5.2. Column generation. In [1], Ahmadi, Dash, and Hall design another iter- 
ative method for inner approximate the set of psd matrix use linear and second 
order cone programming. Their approach combine DSOS/SDSOS technique with 
idea from the theory of column generation in large-scale linear and integer program- 
ming. The high-level idea be to approximate the SDP in (27) by a sequence of LPs 
(parameterized by t): 

minimize 
X∈Sn,αi 

Tr(CX)(31) 

s.t. Tr(AiX) = bi, i = 1, . . . ,m, 

X = 

t∑ 
i=1 

αiBi, 

αi ≥ 0, i = 1, . . . , t,(32) 



DSOS AND SDSOS OPTIMIZATION 29 

where B1, . . . , Bt be fix psd matrices. These matrix be initialize to be the 
extreme ray of DDn (recall Lemma 6), i.e., all rank one matrix viv 

T 
i , where the 

vector vi have at most two nonzero components, each equal to ±1. Once this initial LP 
be solved, then one add one (or sometimes several) new psd matrix Bj to problem 
(31) and resolves. This process then continues. In each step, the new matrix Bj be 
picked carefully to bring the optimal value of the LP closer to that of the SDP. Usually, 
the construction of Bj involves solve a “pricing subproblem” (in terminology of the 
column generation literature), which add appropriate cut plane to the dual of 
(31); see [1] for more details. 

The SOCP analogue of this process be similar. The SDP in (27) be inner approxi- 
mat by a sequence of SOCPs (parameterized by t): 

minimize 
X∈Sn,Λi∈S2 

Tr(CX)(33) 

s.t. Tr(AiX) = bi, i = 1, . . . ,m, 

X = 

t∑ 
i=1 

ViΛiV 
T 
i , 

Λi � 0, i = 1, . . . , t, 

where V1, . . . , Vt be fix n × 2 matrices. They be initialize a the set of matrix 
that have zero everywhere, except for a 1 in the first column in position j and a 1 
in the second column in position k 6= j. This give exactly SDDn (via Lemma 9). 
In subsequent steps, one (or sometimes several) appropriately-chosen matrix Vi be 
add to problem (33). These matrix be obtain by solve pricing subproblems 
and help bring the optimal value of the SOCP closer to that of the SDP in each 
iteration. 

(a) LP iterations. (b) SOCP iterations. 

Fig. 9. Figure reproduce from [1] show the successive improvement on the dd (left) and sdd 
(right) inner approximation of a spectrahedron via five iteration of the column generation method. 

Figure 9 show the improvement obtain by five iteration of this process on a 
randomly generate SDP, where the objective be to maximize a linear function in the 
northeast direction. 5 

5.3. Factor-width k matrix with k > 2. A relevant generalization of sdd 
matrix be through the notion of factor-width, define by Boman et al. [16]. The 
factor-width of a symmetric psd matrix Q be the small integer k for which Q can 



30 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

be write a Q = V V T , where each column of V contains at most k nonzeros. 
Equivalently, the factor-width of Q be the small k for which Q can be write a a 
sum of psd matrix that be nonzero only on a single k × k principal submatrix. 

If we denote the cone of n × n symmetric matrix of factor-width k by FW kn , 
we have that FW kn ⊆ Pn for all k ∈ {1, . . . , n}, FW 2n = SDDn (cf. Lemma 9), and 
FWnn = Pn. For value of k ∈ {2, . . . , n− 1}, one get an increasingly more accurate 
inner approximation to the set of psd matrices, all outperform the approximation 
give by sdd matrices. We do not consider these cone in this paper a it be already 
know that for k = 3, a representation base on second order cone program be not 
possible [28]. Nevertheless, work with FW 3n can be useful a it involves semidefinite 
constraint that be small (3×3) and hence efficiently solvable. Unfortunately though, 
optimize over FW 3n require O(n 

3) such semidefinite constraint which in many 
application can be prohibitive. We believe that a promising future direction would 
be to use the column generation framework on Section 5.2 to work with a subset of 
the extreme ray of FW kn (for small k) and generate additional one on the fly base 
on problem structure. 

Some initial experiment with FW kn have be perform by Ding and Lim [24]. 
The author also develop self-concordant barrier function for these cone and show 
that any second order cone program can be write a an optimization problem over 
FW 2n , i.e., a scale diagonally dominant program (recall the definition from Sec- 
tion 3.1). In related work, Permenter and Parrilo [68] consider optimization problem 
over FW kn (and their dual cones) for facial reduction in semidefinite programming. 
We believe the notion of factor-width be likely to receive more attention in upcoming 
year in the theory of matrix optimization. 

6. Conclusions. We have propose more scalable alternative to SOS program- 
ming by introduce inner approximation to the sum of square cone in the form of 
the cone of dsos and sdsos polynomials. These cone can be optimize over use 
LP and SOCP respectively. The departure from SDP allows u to obtain significant 
gain in term of scalability. Our numerical example from a diverse range of appli- 
cation include polynomial optimization, combinatorial optimization, statistic and 
machine learning, derivative pricing, control theory, and robotics demonstrate that 
with reasonable tradeoff in optimality, we can handle problem size that be well 
beyond the current capability of SOS programming. In particular, we have show 
that our approach be able to tackle dense problem with a many a 70 polynomial 
variable (with degree-4 polynomials). On the theoretical front, we have show that 
the (S)DSOS approach share many of the theoretical asymptotic guarantee usu- 
ally associate with SOS programming. Finally, in the last section of this paper, we 
review recent approach that be developed with the idea of bridging the gap 
between the (S)DSOS approach and the SOS one. One can obtain improve optimal 
value use these method at the cost of additional computation time. The appendix 
provide a brief but complete tutorial on the toolbox that be use to generate the 
numerical result in the paper. The accompany code be freely available online. 

We would like to emphasize that a key benefit of our approach be that it can 
be apply in any application where SOS program be use in order to obtain 
potentially significant gain in scalability. Our hope be that the (S)DSOS approach 
may open up application area that have previously be beyond reach due to the 
limitation in scalability of SOS programming. We also foresee possibility for real- 
time application [4], [55]. Indeed, technology for real-time linear and second order 
cone program be already come to fruition [26], [56]. 



DSOS AND SDSOS OPTIMIZATION 31 

7. Appendix. A complete implementation of the code use to generate the nu- 
merical result present in this paper be write use the Systems Polynomial Op- 
timization Toolbox (SPOT) [57] and be freely available online16. The toolbox feature 
efficient polynomial algebra and allows u to setup the large-scale LPs and SOCPs 
arise from our examples. In this appendix, we provide a brief introduction to the 
software. This be not meant a a comprehensive tutorial on the SPOT toolbox (for 
this one may refer to SPOT’s documentation17). Rather, the goal here be to provide 
an introduction sufficient for set up and solve the DSOS and SDSOS program 
in this paper. The code relevant for this purpose be include in a branch of SPOT 
that we have name iSOS (for “inside sum of squares”). 

7.1. Installing SPOT. Download the software package from Github available 
here: 

https://github.com/anirudhamajumdar/spotless/tree/spotless isos 

Next, start MATLAB and run the spot install.m script. This script will setup the 
MATLAB path and compile a few mex functions. The user may wish to save the new 
MATLAB path for future use. 

7.2. Variables and polynomials. Polynomials be define and manipulate 
by the @msspoly class of SPOT. In order to define a new variable, one can use the 
msspoly.m function: 

>> x = msspoly(‘x’) 

which creates the polynomial p(x) = x. The argument to this function be the name 
of the create variable and be restrict to four character chosen from the alphabet 
(lower case and upper case). A MATLAB vector of variable can be create by passing 
a second argument to the function: 

>> x = msspoly(‘x’,n). 

This will create a n × 1 vector of variable that can be access use standard 
array indexing. Multivariate polynomial can then be construct a follows: 

>> x = msspoly(‘x’,3); 

>> p = 2*x(1)^2 - 5*x(2)^2 + x(3)^2 

Variables can be manipulate and operate on use a variety of functions. These 
include standard arithmetic operation (e.g. addition, multiplication, dot product) 
and operation for manipulate vector (e.g. concatenating, reshaping, transposing). 
Other useful function include: 

deg.m: Returns the total degree of a polynomial. If a second argument in the 
form of a msspoly be provided, the degree with respect to these variable be returned. 

16Link to the software: 
https://github.com/anirudhamajumdar/spotless/tree/spotless isos 

17Link to SPOT’s documentation: 
https://github.com/spot-toolbox/spotless/blob/master/doc/manual.pdf 

https://github.com/anirudhamajumdar/spotless/tree/spotless_isos 
https://github.com/anirudhamajumdar/spotless/tree/spotless_isos 
https://github.com/spot-toolbox/spotless/blob/master/doc/manual.pdf 


32 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

diff.m: Differentiates a polynomial (first argument) with respect to a set of 
msspoly variable (second argument). The result be the matrix of partial derivatives. 

subs.m: Substitutes the third argument in place of the second argument wherever 
it appear in the first argument. 

7.3. Programs. Programs involve DSOS/SDSOS/SOS constraint be con- 
structed use the spotsosprog class. In this section, we demonstrate the working 
of spotsosprog with the help of an example. In particular, we take the problem of 
minimize a form on the unit sphere consider in this paper. The follow example 
be also available in doc/examples/sdsos_example.m. 

% Construct polynomial which be to be minimize 

x = msspoly(‘x’,6); 

vx = monomials(x,4:4); 

randn(‘state’,0) 

cp = randn(1,length(vx)); 

p = cp*vx; 

This block of code construct the polynomial that be to be low bounded. In order to 
do this, we create a six dimensional vector x of variable use the msspoly command 
introduce before. Next, we construct a vector of monomials use the monomials 
function. The first input to this function be the msspoly variable over which the 
monomials be defined. The second input to the function be the range of degree the 
monomials should have. In this case, since we be consider homogeneous quartics, 
the range be simply 4 : 4 (i.e., just 4). 

% Build program 

prog = spotsosprog; 

prog = prog.withIndeterminate(x); 

[prog,gamma] = prog.newFree(1); 

prog = prog.withDSOS(p - gamma*(x’*x)^2); 

This block of code set up the program and constraints. First, the program be 
initialize in the form of the variable prog. This object will contain information 
about constraint and decision variables. Next, we declare the variable x to be an 
indeterminate or abstract variable (thus distinguish it from a decision variable). 
Decision variable be create use the newFree function in the class spotsosprog. 
The input to this function be the number of new decision variable to be created. 
The output be the update program and a variable correspond to the decision 
variable. In our case, we need a single variable gamma to be declared. Finally, we 
specify a DSOS constraint on p - gamma*(x’*x)^2 use the withDSOS command. 
There be correspond function withSDSOS and withSOS that can be use to setup 



DSOS AND SDSOS OPTIMIZATION 33 

SDSOS or SOS constraints. 

% Setup option and solve program 

option = spot sdp default options(); 

% Use just the interior point algorithm to clean up 

options.solveroptions.MSK IPAR BI CLEAN OPTIMIZER = ... 

... ‘MSK OPTIMIZER INTPNT’; 

% Don’t use basis identification 

options.solveroptions.MSK IPAR INTPNT BASIS = ‘MSK BI NEVER’; 

% Display solver output 

options.verbose = 1; 

sol = prog.minimize(-gamma, @spot mosek, options); 

%Get value of gamma for optimal solution 

gamma optimal = double(sol.eval(gamma)) 

Finally, in this block of code, we dictate an option structure for the program. In 
particular, the field options.solveroptions contains option that be specific to the 
solver to be use (MOSEK in our case). The minimize command be use to specify the 
objective of the program (which must be linear in the decision variables), the function 
handle correspond to the solver to be used, and the structure of options. Currently, 
MOSEK, Gurobi and SeDuMi be the support solvers. So, for example, in order to 
use the Gurobi solver for a (S)DSOS program, we would specify the function handle 
@spot gurobi. 

The output of the minimize command be a solution structure, which contains 
diagnostic information and allows one to access the optimize decision variables. The 
last line of our example code demonstrates how to obtain the optimize variable gamma 
and convert it to a MATLAB double type. 

7.4. Additional functionality. Next, we review additional functionality not 
cover in the example above. While this section be not meant to be an exhaustive 
list of all the functionality available in SPOT, it should be sufficient to reproduce 
the numerical result present in this paper. In the line of code present in the 
follow sections, it be assume that prog be an object of the spotsosprog class, x be 
a msspoly variable of size n and an indeterminate variable of prog. These variable 
can be initialize a in the example above with the follow line of code: 

prog = spotsosprog; 

x = msspoly(‘x’,n); 

prog = prog.withIndeterminate(x); 

7.4.1. Creating decision variables. It be often useful to construct a polyno- 
mial whose coefficient be decision variables. This can be achieve with the newFree 
and monomials function introduce above. In particular, one can create a vector 
of monomials in a msspoly variable x of a give degree d use the command v = 
monomials(x,0:d). Then, the set of coefficient can be declare use [prog,c] = 



34 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

prog.newFree(length(v)). Finally, one can obtain the desire polynomial by mul- 
tiplying these two together: p = c’*v. 

It be also possible to create other type of decision variables. For example, the 
follow function can be use to create various type of matrix decision variables: 

newSym: New symmetric matrix. 
newDD: New symmetric matrix constrain to be diagonally dominant. 
newSDD: New symmetric matrix constrain to be scale diagonally dominant. 
newPSD: New symmetric positive semidefinite matrix. 
newDDdual: New symmetric matrix constrain to lie in the dual of the cone of 

diagonally dominant matrices. 
newSDDdual: New symmetric matrix constrain to lie in the dual of the cone of 

scale diagonally dominant matrices. 
The input to these function be the size of the desire (square) matrix. For 

example, in order to create a 10×10 symmetric matrix Q constrain to be diagonally 
dominant, one can use the follow line of code: 

[prog,Q] = prog.newDD(10); 

7.4.2. Specifying constraints. In addition to the withDSOS, withSDSOS and 
withSOS function introduce previously, constraint on exist decision variable 
can be specify use the follow functions: 

withEqs: Sets up constraint of the form expr = 0, where expr be the input to 
the function and be a matrix whose element be to be constrain to be equal to 0. 

withPos: Sets up constraint of the form expr ≥ 0, where expr be the input to 
the function and be a matrix whose element be to be constrain to be nonnegative. 

Note that the function withEqs and withPos allow one to specify constraint in 
a vectorized manner, thus avoid MATLAB’s potentially-slow for-loops. 

withDD: Constrains a symmetric matrix (the input to the function) to be diago- 
nally dominant. 

withSDD: Constrains a symmetric matrix (the input to the function) to be scale 
diagonally dominant. 

withPSD: Constrains a symmetric matrix (the input to the function) to be positive 
semidefinite. 

In each case above, the input must be affine function of the decision variable of 
the program. The output of these function be an update spotsosprog object that 
contains the new constraints. We illustrate the use of the function withEqs with the 
help of a simple example. The other function can be use in a similar manner. 

prog = spotsosprog; 

[prog,tau1] = prog.newFree(10); 

[prog,tau2] = prog.newFree(10); 

prog = prog.withEqs(tau1 - tau2);(34) 

In this example, the element of the 10× 1 decision vector tau1 be constrain 
to be equal to the element of the vector tau2. 

withPolyEqs: In order to constrain two polynomial p1(x) and p2(x) to be equal 
to each other for all x, one may use the function withPolyEqs. This function will 



DSOS AND SDSOS OPTIMIZATION 35 

constrain the coefficient of the two polynomial to be equal. Here be a simple example: 

prog = spotsosprog; 

x = msspoly(‘x’,2); 

prog = prog.withIndeterminate(x); 

[prog,c] = prog.newFree(4); 

p1 = (c(1) + c(2))*x(1)^2 + c(3)*x(2)^2; 

p2 = 2*x(1)^2 + c(4)*x(2)^2; 

prog = prog.withPolyEqs(p1 - p2); 

This will constrain c(1) + c(2) = 2 and c(3) = c(4). 

7.4.3. Checking if a polynomial be dsos. The three utility function isDSOS, 
isSDSOS and isSOS allow one to check if a give polynomial be dsos, sdsos, or so 
respectively. The only input to the function be the polynomial to be checked. The 
output be a boolean variable indicate whether the polynomial be in fact dsos/ sd- 
sos/ sos, the Gram matrix, and the monomial basis correspond to the Gram matrix 
(these last two be non-empty only if the give polynomial be in fact dsos/sdsos/sos). 
The follow line of code provide a simple example: 

x = msspoly(‘x’,3); 

p = x(1)^2 + 5*x(2)^2 + 3*x(3)^2; 

[isdsos,Q,v] = isDSOS(p)(35) 

One can check that the polynomial p - v’*Q*v be the zero polynomial (up to 
numerical tolerances). 

Acknowledgments. We would like to thank Pablo Parrilo for acquaint u 
with scale diagonally dominant matrices, and Georgina Hall for many correction and 
simplification on the first draft of this work. Our gratitude extends to Russ Tedrake 
and the member of the Robot Locomotion Group at MIT for several insightful dis- 
cussions, particularly around control applications. We thank Frank Permenter and 
Mark Tobenkin for their help with the numerical implementation of DSOS/SDSOS 
program in SPOT, and Aida Khajavirad for run the experiment of Section 4.1 
with BARON. Finally, this work have benefit from question and comment from 
several colleagues, among whom we gladly acknowledge Greg Blekherman, Stephen 
Boyd, Sanjeeb Dash, Etienne de Klerk, Jesus de Loera, Lijun Ding, Didier Henrion, 
Jean Bernard Lasserre, Lek-Heng Lim, Bruce Reznick, James Saunderson, and Bernd 
Sturmfels. 

REFERENCES 

[1] A. A. Ahmadi, S. Dash, and G. Hall, Optimization over structure subset of positive 
semidefinite matrix via column generation, Discrete Optimization, (2016). 

[2] A. A. Ahmadi and G. Hall, On the construction of converge hierarchy for polynomial 
optimization base on certificate of global positivity. Available at https://arxiv.org/abs/ 
1709.09307, 2017. 

[3] A. A. Ahmadi and G. Hall, Sum of square basis pursuit with linear and second order cone 
programming, Contemporary Mathematics, (2017). 

[4] A. A. Ahmadi and A. Majumdar, Some application of polynomial optimization in operation 
research and real-time decision making, Optimization Letters, 10 (2016), pp. 709–729. 

https://arxiv.org/abs/1709.09307 
https://arxiv.org/abs/1709.09307 


36 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

[5] F. Alizadeh and D. Goldfarb, Second-order cone programming, Mathematical programming, 
95 (2003), pp. 3–51. 

[6] E. Artin, Über die Zerlegung Definiter Funktionen in Quadrate, Hamb. Abh., 5 (1927), 
pp. 100–115. 

[7] C. Bachoc and F. Vallentin, New upper bound for kiss number from semidefinite pro- 
gramming, Journal of the American Mathematical Society, 21 (2008), pp. 909–924. 

[8] G. Barker and D. Carlson, Cones of diagonally dominant matrices, Pacific Journal of Math- 
ematics, 57 (1975), pp. 15–32. 

[9] A. Ben-Tal and A. Nemirovski, Lectures on Modern Convex Optimization: Analysis, Algo- 
rithms, and Engineering Applications, vol. 2, Siam, 2001. 

[10] D. P. Bertsekas and J. N. Tsitsiklis, Introduction to Probability, vol. 1, Athena Scientific 
Belmont, MA, 2002. 

[11] D. Bertsimas, R. M. Freund, and X. A. Sun, An accelerate first-order method for solve 
SOS relaxation of unconstrained polynomial optimization problems, Optimization Meth- 
od and Software, 28 (2013), pp. 424–441. 

[12] D. Bertsimas, D. A. Iancu, and P. A. Parrilo, A hierarchy of near-optimal policy for 
multistage adaptive optimization, IEEE Transactions on Automatic Control, 56 (2011), 
pp. 2809–2824. 

[13] D. Bertsimas and I. Popescu, On the relation between option and stock prices: a convex 
optimization approach, Operations Research, 50 (2002), pp. 358–374. 

[14] D. Bertsimas and I. Popescu, Optimal inequality in probability theory: a convex optimiza- 
tion approach, SIAM Journal on Optimization, 15 (2005), pp. 780–804. 

[15] G. Blekherman, P. A. Parrilo, and R. Thomas, Semidefinite Optimization and Convex 
Algebraic Geometry, SIAM Series on Optimization, 2013. 

[16] E. G. Boman, D. Chen, O. Parekh, and S. Toledo, On factor width and symmetric H- 
matrices, Linear Algebra and it Applications, 405 (2005), pp. 239–248. 

[17] I. M. Bomze and E. De Klerk, Solving standard quadratic optimization problem via lin- 
ear, semidefinite and copositive programming, Journal of Global Optimization, 24 (2002), 
pp. 163–185. 

[18] P. P. Boyle and X. S. Lin, Bounds on contingent claim base on several assets, Journal of 
Financial Economics, 46 (1997), pp. 383–400. 

[19] S. Burer, Copositive Programming, in Handbook on Semidefinite, Conic and Polynomial Op- 
timization, Springer, 2012, pp. 201–218. 

[20] CPLEX, V12. 2: Users manual for CPLEX, International Business Machines Corporation, 46 
(2010), p. 157. 

[21] A. d Aspremont, L. E. Ghaoui, M. I. Jordan, and G. R. Lanckriet, A direct formulation 
for sparse pca use semidefinite programming, SIAM review, 49 (2007), pp. 434–448. 

[22] E. de Klerk and D. V. Pasechnik, Approximation of the stability number of a graph via 
copositive programming, SIAM Journal on Optimization, 12 (2002), pp. 875–892. 

[23] E. De Klerk and R. Sotirov, Exploiting group symmetry in semidefinite program re- 
laxation of the quadratic assignment problem, Mathematical Programming, 122 (2010), 
pp. 225–246. 

[24] L. Ding and L. H. Lim, Higher-order cone programming. Available also a a Master’s thesis, 
University of Chicago, 2016. 

[25] A. C. Doherty, P. A. Parrilo, and F. M. Spedalieri, Distinguishing separable and entangle 
states, Physical Review Letters, 88 (2002). 

[26] A. Domahidi, E. Chu, and S. Boyd, ECOS: An SOCP solver for embed systems, in 
Preecedings of the European Control Conference, IEEE, 2013, pp. 3071–3076. 

[27] M. Dür, Copositive Programming–a Survey, in Recent advance in optimization and it appli- 
cation in engineering, Springer, 2010, pp. 3–20. 

[28] H. Fawzi, On represent the positive semidefinite cone use the second-order cone, arXiv 
preprint arXiv:1610.04901, (2016). 

[29] M. R. Garey and D. S. Johnson, Computers and Intractability, W. H. Freeman and Co., San 
Francisco, Calif., 1979. 

[30] K. Gatermann and P. A. Parrilo, Symmetry groups, semidefinite programs, and sum of 
squares, Journal of Pure and Applied Algebra, 192 (2004), pp. 95–128. 

[31] S. A. Gershgorin, Uber die Abgrenzung der Eigenwerte einer Matrix, Bulletin de l’Académie 
de Sciences de l’URSS. Classe de science mathématiques et na, (1931), pp. 749–754. 

[32] M. R. Gupta, A. Cotter, J. Pfeifer, K. Voevodski, K. Canini, A. Mangylov, 
W. Moczydlowski, and A. Van Esbroeck, Monotonic calibrate interpolate look-up 
tables, Journal of Machine Learning Research, 17 (2016), pp. 1–47. 

[33] W. Habicht, Uber die Zerlegung strikte definiter Formen in Quadrate, Comment. Math. Helv., 



DSOS AND SDSOS OPTIMIZATION 37 

12 (1940), pp. 317–322. 
[34] L. Hannah and D. B. Dunson, Multivariate convex regression with adaptive partitioning., 

Journal of Machine Learning Research, 14 (2013), pp. 3261–3294. 
[35] L. A. Hannah and D. B. Dunson, Ensemble method for convex regression with application 

to geometric program base circuit design, arXiv preprint arXiv:1206.4645, (2012). 
[36] J. Harrison, Verifying nonlinear real formula via sum of squares, in Theorem Proving in 

Higher Order Logics, Springer, 2007, pp. 102–118. 
[37] D. Hilbert, Über die Darstellung Definiter Formen al Summe von Formenquadraten, Math. 

Ann., 32 (1888). 
[38] C. Hillar and L.-H. Lim, Most tensor problem be NP-hard, arXiv preprint arXiv:0911.1393, 

(2009). 
[39] M. Huneault and F. Galiana, A survey of the optimal power flow literature, IEEE Transac- 

tions on Power Systems, 6 (1991), pp. 762–770. 
[40] Z. Jarvis-Wloszek, R. Feeley, W. Tan, K. Sun, and A. Packard, Some control appli- 

cation of sum of square programming, in Proceedings of the 42nd IEEE Conference on 
Decision and Control, vol. 5, IEEE, 2003, pp. 4676–4681. 

[41] H. Khalil, Nonlinear Systems, Prentice Hall, 2002. Third edition. 
[42] J.-L. Krivine, Anneaux préordonnés, Journal d’analyse mathématique, 12 (1964), pp. 307–326. 
[43] R. Laraki and J. B. Lasserre, Semidefinite program for min–max problem and games, 

Mathematical programming, 131 (2012), pp. 305–332. 
[44] J. B. Lasserre, Global optimization with polynomial and the problem of moments, SIAM 

Journal on Optimization, 11 (2001), pp. 796–817. 
[45] J. B. Lasserre, Moments, Positive Polynomials and Their Applications, vol. 1, World Scien- 

tific, 2009. 
[46] J. B. Lasserre, K.-C. Toh, and S. Yang, A bound degree so hierarchy for polynomial 

optimization, EURO Journal on Computational Optimization, (2015), pp. 1–31. 
[47] M. Laurent, Sums of squares, moment matrix and optimization over polynomials, in Emerg- 

ing application of algebraic geometry, Springer, 2009, pp. 157–270. 
[48] M. Laurent, M. E. Nagy, and A. Varvitsiotis, Complexity of the positive semidefinite 

matrix completion problem with a rank constraint, Discrete Geometry and Optimization, 
69 (2013), pp. 105–120. 

[49] X. Li, D. Sun, and K.-C. Toh, QSDPNAl: A two-phase proximal augment Lagrangian 
method for convex quadratic semidefinite programming, arXiv preprint arXiv:1512.08872, 
(2015). 

[50] L. Liberti, C. Lavor, N. Maculan, and A. Mucherino, Euclidean distance geometry and 
applications, SIAM Review, 56 (2014), pp. 3–69. 

[51] M. S. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret, Applications of second-order cone 
programming, Linear algebra and it applications, 284 (1998), pp. 193–228. 

[52] L. Lovász, On the Shannon capacity of a graph, IEEE Transactions on Information theory, 25 
(1979), pp. 1–7. 

[53] A. Majumdar, A. Ahmadi, and R. Tedrake, Control and verification of high-dimensional 
system with dsos and sdsos programming, in 53rd IEEE Conference on Decision and 
Control, IEEE, 2014, pp. 394–401. 

[54] I. R. Manchester, M. M. Tobenkin, M. Levashov, and R. Tedrake, Regions of attraction 
for hybrid limit cycle of walk robots, Proceedings of IFAC, 44 (2011), pp. 5801–5806. 

[55] J. Mattingley and S. Boyd, Real-time convex optimization in signal processing, Signal Pro- 
cessing Magazine, IEEE, 27 (2010), pp. 50–61. 

[56] J. Mattingley and S. Boyd, CVXGEN: a code generator for embed convex optimization, 
Optimization and Engineering, 13 (2012), pp. 1–27. 

[57] A. Megretski, Systems polynomial optimization tool (spot), 2010. 
[58] A. Mosek, The MOSEK optimization software, Online at http://www. mosek. com, 54 (2010), 

pp. 2–1. 
[59] T. S. Motzkin, The arithmetic-geometric inequality, in Inequalities (Proc. Sympos. Wright- 

Patterson Air Force Base, Ohio, 1965), Academic Press, New York, 1967, pp. 205–224. 
[60] K. G. Murty and S. N. Kabadi, Some NP-complete problem in quadratic and nonlinear 

programming, Mathematical Programming, 39 (1987), pp. 117–129. 
[61] Y. Nesterov, Squared functional system and optimization problems, in High performance 

optimization, vol. 33 of Appl. Optim., Kluwer Acad. Publ., Dordrecht, 2000, pp. 405–440. 
[62] J. Nie and M. Schweighofer, On the complexity of Putinar’s Positivstellensatz, Journal of 

Complexity, 23 (2007), pp. 135–150. 
[63] J. Nie and L. Wang, Regularization method for SDP relaxation in large-scale polynomial 

optimization, SIAM Journal on Optimization, 22 (2012), pp. 408–428. 



38 AMIR ALI AHMADI AND ANIRUDHA MAJUMDAR 

[64] P. Parrilo and A. Jadbabaie, Approximation of the joint spectral radius use sum of squares, 
Linear Algebra and it Applications, 428 (2008), pp. 2385–2402. 

[65] P. A. Parrilo, Structured semidefinite program and semialgebraic geometry method in ro- 
bustness and optimization, PhD thesis, California Institute of Technology, May 2000. 

[66] P. A. Parrilo, Semidefinite program relaxation for semialgebraic problems, Mathemat- 
ical Programming, 96 (2003), pp. 293–320. 

[67] P. A. Parrilo, Polynomial game and sum of square optimization, in Proceedings of the 45th 

IEEE Conference on Decision and Control, 2006. 
[68] F. Permenter and P. Parrilo, Partial facial reduction: simplified, equivalent SDPs via 

approximation of the PSD cone, arXiv preprint arXiv:1408.4685, (2014). 
[69] G. Pólya, Über Positive Darstellung von Polynomen, Vierteljschr. Naturforsch. Ges. Zürich, 

73 (1928), pp. 141–145. 
[70] I. Popescu, A semidefinite program approach to optimal-moment bound for convex 

class of distributions, Mathematics of Operations Research, 30 (2005), pp. 632–657. 
[71] V. Powers, Positive polynomial and sum of squares: theory and practice, Real Algebraic 

Geometry, (2011). 
[72] M. Putinar, Positive polynomial on compact semi-algebraic sets, Indiana University Mathe- 

matics Journal, 42 (1993), pp. 969–984. 
[73] B. Reznick, Uniform denominator in Hilbert’s 17th problem, Math Z., 220 (1995), pp. 75–97. 
[74] B. Reznick, Some concrete aspect of Hilbert’s 17th problem, in Contemporary Mathematics, 

vol. 253, American Mathematical Society, 2000, pp. 251–272. 
[75] B. Reznick, Blenders, in Notions of Positivity and the Geometry of Polynomials, Springer, 

2011, pp. 345–373. 
[76] C. Riener, T. Theobald, L. J. Andrén, and J. B. Lasserre, Exploiting symmetry in SDP- 

relaxation for polynomial optimization, Mathematics of Operations Research, 38 (2013), 
pp. 122–141. 

[77] M. Roozbehani, Optimization of Lyapunov invariant in analysis and implementation of 
safety-critical software systems, PhD thesis, Massachusetts Institute of Technology, 2008. 

[78] N. V. Sahinidis, BARON: A general purpose global optimization software package, Journal of 
global optimization, 8 (1996), pp. 201–205. 

[79] N. Z. Shor, Class of global minimum bound of polynomial functions, Cybernetics, 23 (1987), 
pp. 731–734. (Russian orig.: Kibernetika, No. 6, (1987), 9–11). 

[80] G. Stengle, A Nullstellensatz and a Positivstellensatz in semialgebraic geometry, Mathema- 
tische Annalen, 207 (1974), pp. 87–97. 

[81] J. F. Sturm, Using SeDuMi 1.02, a Matlab toolbox for optimization over symmetric cones, 
Optimization Methods and Software, 11 (1999), pp. 625 – 653. 

[82] R. Tae, B. Dumitrescu, and L. Vandenberghe, Multidimensional FIR filter design via 
trigonometric sum-of-squares optimization, IEEE Journal of Selected Topics in Signal Pro- 
cessing, 1 (2007), pp. 641–650. 

[83] U. Topcu, A. Packard, P. Seiler, and G. Balas, Robust region-of-attraction estimation, 
IEEE Transactions on Automatic Control, 55 (2010), pp. 137 –142. 

[84] F. Vallentin, Symmetry in semidefinite programs, Linear Algebra and it Applications, 430 
(2009), pp. 360–369. 

[85] L. Vandenberghe and S. Boyd, Semidefinite programming, SIAM Review, 38 (1996), pp. 49– 
95. 

[86] X.-Y. Zhao, D. Sun, and K.-C. Toh, A Newton-CG augment Lagrangian method for 
semidefinite programming, SIAM Journal on Optimization, 20 (2010), pp. 1737–1765. 

[87] H. Zou, T. Hastie, and R. Tibshirani, Sparse principal component analysis, Journal of 
computational and graphical statistics, 15 (2006), pp. 265–286. 


1 Introduction 
1.1 Why optimize over nonnegative polynomials? 

2 Review of the semidefinite programming-based approach and computational consideration 
3 DSOS and SDSOS Optimization 
3.1 The cone of dsos and sdsos polynomial 
3.2 The cone of r-dsos and r-sdsos polynomial and asymptotic guarantee 

4 Numerical example and application 
4.1 Lower bound on polynomial optimization problem 
4.2 Copositive program and combinatorial optimization 
4.3 Convex regression in statistic 
4.4 Options pricing 
4.5 Sparse PCA 
4.6 Applications in control theory 
4.6.1 Region of attraction for an invert N-link pendulum 
4.6.2 Control synthesis for a humanoid robot 


5 Improvements on DSOS and SDSOS optimization 
5.1 Iterative change of basis 
5.2 Column generation 
5.3 Factor-width k matrix with k>2 

6 Conclusions 
7 Appendix 
7.1 Installing SPOT 
7.2 Variables and polynomial 
7.3 Programs 
7.4 Additional functionality 
7.4.1 Creating decision variable 
7.4.2 Specifying constraint 
7.4.3 Checking if a polynomial be dsos 


References 

