









































Journal of Artificial Intelligence Research 60 (2017) 179-219 Submitted 12/16; publish 09/17 

Combining Lexical and Syntactic Features for 
Detecting Content-Dense Texts in News 

Yinfei Yang yangyin7@gmail.com 
1600 Amphitheatre Pkwy 
Mountain View, CA 94043 

Ani Nenkova nenkova@seas.upenn.edu 
University of Pennsylvania 

3330 Walnut Street 

Philadelphia, PA, 19103 USA 

Abstract 

Content-dense news report important factual information about an event in direct, suc- 
cinct manner. Information seek application such a information extraction, question 
answer and summarization normally assume all text they deal with be content-dense. 
Here we empirically test this assumption on news article from the business, U.S. inter- 
national relations, sport and science journalism domains. Our finding clearly indicate 
that about half of the news text in our study be in fact not content-dense and motivate 
the development of a supervise content-density detector. We heuristically label a large 
training corpus for the task and train a two-layer classify model base on lexical and un- 
lexicalize syntactic features. On manually annotate data, we compare the performance 
of domain-specific classifiers, train on data only from a give news domain and a general 
classifier in which data from all four domain be pool together. Our annotation and pre- 
diction experiment demonstrate that the concept of content density varies depend on 
the domain and that naive annotator provide judgement bias toward the stereotypical 
domain label. Domain-specific classifier be more accurate for domain in which content- 
dense text be typically fewer. Domain independent classifier reproduce good naive 
crowdsourced judgements. Classification prediction be high across all conditions, around 
80%. 

1. Introduction 

News article be write with different goal in mind. Some aim to inform the reader 
about an important event, focus on specific detail such a who do what to whom where 
and when. Others aim to provide background information, fact related to an event and 
necessary to understand an event but not newsworthy by themselves. Yet others seek to 
entertain the reader, or to showcase the brilliant mastery of language and the wit of the 
author. 

In this paper we introduce the task of detect if a text be content-dense or not. Content- 
dense news report important factual information about an event, in direct and succinct 
manner. Prototypical example of content-dense text be newswire articles, which be 
usually perfect answer to a “What happened?” question, ground in a specific event. In 
general news, however, newswire-like, content-dense text be not the norm. 

c©2017 AI Access Foundation. All right reserved. 



Yang & Nenkova 

We base our analysis on the opening paragraph, call the lead or lede, of news article 
drawn from the New York Times. News report often adhere to the invert pyramid struc- 
ture, in which the lead conveys what happened, when and where, follow by more detail 
in the body. Information that be not essential be include in the final tail. When writer 
adhere to this style of writing, the lead be informative and provide positive example of 
content-dense texts. Alternatively, the lead may be creative, provocative or entertain 
rather than informative, provide example of non content-dense texts. 

Consider the lead below, from the politics and sport section of the New York Times. 
The first two be content-dense leads. The other two be non content-dense lead that do 
not focus on events; and which be much richer stylistically. 

Content-dense: 

[Politics] Evo Morales, a candidate for president who have pledge to reverse a campaign 
finance by the United States to wipe out coca growing, score a decisive victory in general 
election in Bolivia on Sunday. 

Mr. Morales, 46, an Aymara Indian and former coca farmer who also promise to 
roll back American-prescribed economic changes, have garner up to 51 percent of the vote, 
accord to televise quick-count polls, which tally a sample of vote at polling place and 
be consider highly accurate. 

[Sports] North Carolina (29-1) and Duke (26-3) of the Atlantic Coast Conference re- 
ceived No. 1 seedings yesterday in the 64-team women’s N.C.A.A. tournament, along with 
Ohio State (28-2) and Louisiana State (27-3). 

The top-ranked Tar Heels receive the No. 1 overall seeding, but be place in what 
appear to be the most difficult regional. 

Non content-dense: 

[Politics] When the definitive history of the Iraq war be written, future historian will 
surely want to ask Saddam Hussein and George W. Bush each one big question. To Saddam, 
the question would be: What be you thinking? If you have no weapon of mass destruction, 
why do you keep act a though you did? For Mr. Bush, the question would be: What 
be you thinking? If you bet your whole presidency on succeed in Iraq, why do you 
let Donald Rumsfeld run the war with just enough troop to lose? Why didn’t you establish 
security inside Iraq and along it borders? How could you ever have thought this would be 
easy? 

The answer to these question can be found in what be America’s great intelligence 
failure in Iraq – and that be not about W.M.D. 

[Sports] With his silver pant and dark blue jersey cover by a mottle mix of grass 
stains, paint and mud, New England Patriots run back Corey Dillon sat on an aluminum 
bench on the sideline at Gillette Field on Sunday, look exhaust and frozen. 

Only a few minute remain in the Patriots’ 20-3 victory over the Indianapolis Colts, 
and Dillon be resting. He star at the field, snowflake swirl around his head a the 
realization of his first playoff victory swirl inside it. 

Below we propose an approach for label short news text a content-dense or not. Our 
analysis of manual annotation reveals that uninformative article lead be common. We 
investigate several type of lexical and non-lexicalized syntactic feature for distinguish 

180 



Detecting Content-Dense News Texts 

content-dense text from other more general or creatively write texts. We present a 
two-layer classifier model which significantly outperforms a baseline assume that all news 
lead be content-dense. We also study the robustness of the definition of content density 
across domains, a well a the performance of domain-dependent and domain-independent 
(general) classifiers. 

2. Motivation 

Traditionally, natural language processing practitioner work under the assumption that 
the direct goal of text analysis be to ultimately derive a semantic interpretation of text. Our 
work deviate from this tradition and instead focus on detect style difference first, 
defer or entirely forego semantic interpretation. This “style, then semantics if need 
be” approach to understand reflect typical human behavior (Kahneman, 2011). 

Under style we hope to capture how content be conveyed rather than exactly what fact 
be be communicate (Queneau, 1947) or what truth value one ought to assign to the 
express statements. This definition be remarkably close to decades-old attempt to define 
style a part of text typology: 

Style be use here to mean the way text be internally differentiate other than by topic; 
mainly by the choice of the presence or absence of some of a large range of structural 
and lexical features. 

(Sinclair & Ball, 1996) 

In the article we also investigate how broad topic (our news domains) interact with 
style, both in the way domain information influence people’s style judgement and in the 
change of the structural and lexical indicator of style across domains. 

In spirit, our work belongs to a grow body of research concerned with develop 
method for deduce how information in longer text1 be conveyed and how information will 
be perceive by reader (Yu & Hatzivassiloglou, 2003; Danescu-Niculescu-Mizil, Kossinets, 
Kleinberg, & Lee, 2009; Jurafsky, Ranganath, & McFarland, 2009; Ashok, Feng, & Choi, 
2013; Cook & Hirst, 2013; Louis & Nenkova, 2014). Our effort be complementary, and 
cannot be compare directly, to work concerned with propositional meaning directly, such 
a event detection (Peng, Song, & Roth, 2016; Feng, Huang, Tang, Ji, Qin, & Liu, 2016; 
Nguyen & Grishman, 2016), veridicality (Sauŕı & Pustejovsky, 2009; de Marneffe, Manning, 
& Potts, 2012) or fact-checking (Vlachos & Riedel, 2014). 

3. Corpus 

The data for our experiment come from the New York Times (NYT) annotate corpus 
(LDC Catalog No. LDC2008T19). The corpus contains 20 year worthy of NYT editions, 
along with rich meta-data about the newspaper section in which the article appear and 
summary produce by information scientist for many of the articles. The lead of article 
be explicitly marked in the corpus, so extract the relevant text for further analysis be 
straightforward. 

1. rather than individual word and sentence 

181 



Yang & Nenkova 

In our previous proof-of-concept work (Yang & Nenkova, 2014), we select a subcorpus 
of article publish in 2005 or 2006 from four different genre (business, U.S. international 
relations, science and sports). Given the selection criteria, the data in that prior work 
contain considerably few article from the science and the sport domain compare to 
the other two domains. Moreover, the performance of the content-dense classifier in the 
science and sport domain be notably bad than the other two domains, which could 
be explain either by the fact that these classifier be train on small datasets or by 
the intrinsic difficult of predict content density in these two domains. To definitively 
resolve this question, and to benefit from the large training dataset possible, we extend 
the corpus to the full NYT corpus in the experiment report in this manuscript. 

We also expect that the degree to which a text would be judged to be content-dense, 
reporting on important event in a direct manner, be influence by the domain of the article. 
It be reasonable to expect that typical event in science or sport would not be consider of 
the same importance a international political or business events. To study the cross-domain 
differences, we analyze four news domains: Business, Sports, Science2 and US International 
Relations (or Politics for short). 

3.1 Training Set Heuristic 

To automatically label lead a content-dense or not, we make use of the manual summary 
which accompany many article in the NYT corpus. For the article with content-dense 
leads, the manual summary will be very similar to the lead itself, a this type of lead by 
definition provide a fact-focused summary of the article. For lead that simply seek to 
engage the reader via more creative devices, the manual summary will differ considerably 
from the lead. Overall, the similarity between the lead and the manual summary provide 
a strong indication of the importance and factual, event-oriented, nature of the information 
express in the lead. 

For article with manual summary of at least 25 words, we calculate a content-dense 
score. For each word in the summary, a tuple t(w, pos) be create contain the word and 
it part of speech. The score be compute as: 

Score = 
# of t(w, pos) also in lead 

# of t(w, pos) in sum 
(1) 

3.2 Label Analysis 

Table 1 show detail about the number of all NYT article from each of the four domains. 
The first column show the number of article in the NYT from the give domain. The 
second column show the number of article use for training domain-dependent classifier 
(we explain the selection procedure below). Overall only about one third of article have 
associate manual summaries. 

The distribution of content-dense score assign a a function of the overlap with the 
human summary be show in Figure 1. In the business domain the distribution of score 

2. The science article be from the CATS corpus (Louis & Nenkova, 2013), which only contains article 
publish after 1999. 

182 



Detecting Content-Dense News Texts 

Table 1: Number of article in the corpus. 

Total number of article Articles use in training (Percentage) 

Business 149,113 21,224 (14.2%) 

Science 23,240 7,737 (33.%) 

Sports 134,925 10,670 (7.9%) 

Politics 45,926 10,503 (22.8%) 

Overall 353,204 50,134 (14.2%) 

be almost uniform, reflect the fact that in that section there be article about impor- 
tant events—company mergers, unexpected stock price changes, product announcement 
and lawsuits—but also non-event specific analysis of current trends, minor event such a 
auction and people-centered piece about prominent business men and women. 

In sport and science, the distribution of content-dense score be clearly skewed towards 
the non content-dense end of the spectrum. In these domain writer more often resort to 
the use of creative and indirect language meant to provoke readers’ interest. 

The content-dense score in politics be almost normally distributed, with mean roughly 
in the middle of the possible range, and much high than any of the other domains. The 
non content-dense lead in this domain usually provide a commentary on an ongoing event 
rather than report of a specific new development. 

In the rest of the paper, we focus on the binary classification task of predict if a 
lead be content-dense or not. However, it be reasonable to expect that our indirect label 
score be noisy. To obtain cleaner data for training our model, we label only the lead with 
most extreme scores: we assign the label non content-dense to the lead with score that 
fall below the 20th percentile and label content-dense to lead that score above the 80th 
percentile for their domain. The 20th/80th percentile set be color red in Figure 1. In 
the general (domain-independent) model, the data be pool together and again the lead 
with low score be assign to the non content-dense class and the lead with high 
score be consider content-dense. 

4. Methodology 

In this section, we introduce the feature and model we use in our experiments. In our 
prior experiment (Yang & Nenkova, 2014), we found that lexical feature be well-suited 
for the task, particularly lexical representation determine independently of the training 
data. Along with these, unlexicalized syntactic representation also lead to remarkably 
good results. A number of other representation we experiment with do not appear to 
be that beneficial for the task. Motivated by these findings, here we study in depth the 
lexical representation and the unlexicalized syntactic representation, and explore way to 
combine the prediction of these model to achieve even good accuracy. 

4.1 Features 

We compare and combine two lexical and one syntactic representation. For the lexical 
representation, we use the vocabulary from the MRC Database(MRC), which be inde- 

183 



Yang & Nenkova 

Figure 1: Score histogram for the four genres: [Top Left] Business, [Top Right] Science, 
[Bottom Left] Sports, [Bottom Right] Politics. 20th and 80th percentile be color 
red. Red star indicates the average content-dense score for each genre. 

pendent of our training set and a vocabulary derive from the training set and weight by 
Mutual Information(MI). The syntactic representation be simply the list of Production 
Rules(PR) from the constituency parse of the sentence in the lead. 

4.1.1 MRC Database(MRC) 

The MRC Psycholinguistic Database (Wilson, 1988) be an electronic dictionary contain 
150,837 words, different subset of which be annotate for 26 linguistic and psycholinguis- 
tic attributes. We select a subset of 4,923 word normed for age of acquisition, imagery, 
concreteness, familiarity and ambiguity. In (Wilson, 1988), the word be chosen among 
those with medium frequency in a large corpus and experiment subject be ask to rate 
on a scale the degree to which each word have one of these properties. The MRC dictionary 
be a compilation of result from different studies, run by different research groups, with 
different criterion for select the list of word for which to solicit norms. We use the list 
of word which have at least one of above ratings. The value of each feature be equal to the 
number of time it appear in the lead, divide by the number of word in the lead. 

184 



Detecting Content-Dense News Texts 

About 90% of the MRC vocabulary (4,647 words) appear at least once in the training 
data. About 4,300 appear more than five times.3 

4.1.2 Mutual Information(MI) 

The lexical representation described above be domain independent, determine without any 
knowledge about the data which will be use for training and test of our classification 
models. We also introduce a domain-dependent lexical representation, derive from the 
training data for the classifier and use mutual information to measure the association 
between particular word and the content-dense and non content-dense write styles. For 
each genre, we compute the mutual information between word and lead type in the training 
data as: 

MIc = log 
p(word, c) 

p(word)p(c) 
(2) 

Here c be either the content-dense or the non-content dense class. We only compute the 
MI score for word that appear at least 5 time in the training set. We select the top 500 
word with high association with each of the write styles, for a total of 1,000 features. 
The value of the feature be 1 if the word occurs in the lead and 0 otherwise. 

The word with high mutual information4 with the content-dense class and non 
content-dense class be list in Table 2. 

The word with high mutual information with the content-dense class be distinctly 
domain specific. Content-dense lead in the business domain be more likely to talk about 
company and their executives, deals, agreement and offers. Content-dense lead in science 
be more likely to discus a specific study or drug, since they be overwhelm bias 
towards health-related topics. Sports content-dense lead be associate with specific sport 
event or deals. In politics, content-dense lead discus American involvement and attacks. 

In addition, the word “yesterday” and “today” also appear among those associate with 
content-dense leads, provide a strong indicator that the news be focus on a specific recent 
event rather than a general discussion or personal aspect story. The word associate with 
the non content-dense class in contrast tend to be related to non-specific activity (find, 
feel, hear, smile, remember, sit, wait) and focus on personal aspect () rather than on the 
professional role (man, people, friend, husband, guy, kid, child, friend). 

Only about half of the word in the mutual information representation also appear in 
the MRC. 

3. As we mention in the opening of this section, in our early work (Yang & Nenkova, 2014) we also 
experiment with other dictionaries, include LIWC and the General Inquirer. Results consistently 
confirm that the MRC lead to best prediction results. This be also the large resource, guarantee 
the best coverage of feature for new texts. For these reasons, we include only MRC feature in the work 
present here, to focus the presentation on the cross-domain difference and classifier combination, 
which be novel with respect to our prior work. 

4. We ran 10 fold cross validation in the experiments. The mutual information be compute separately base 
on the training set of each fold. The word list in Table 2 be from fold 0. High mutual information 
word from other fold be very similar. 

185 



Yang & Nenkova 

Table 2: Top 30 select word for each domain and overall data 

Content-dense Non content-dense 

Business 

company, yesterday, million, billion, 
today, percent, group, announce, 

executive, plan, share, corporation, 
york, part, deal, agree, largest, unit, 

court, agency, inc., commission, bank, 
include, firm, chief, agreement, 

chairman, offer, service 

day, stock, ago, work, thing, good, 
investor, year, find, turn, long, man, 
economy, job, people, home, street, 

room, time, rate, lot, index, city, sit, 
mr., market, wall, money, ms., life 

Science 

study, health, today, yesterday, report, 
drug, official, research, federal, state, 

scientist, administration, disease, 
researcher, company, government, 
accord, human, virus, university, 

group, million, expert, announce, cell, 
include, cancer, united, agency, issue 

day, mr., ms., ago, feel, hear, room, 
sit, walk, home, eye, life, friend, thing, 

run, talk, live, game, stand, back, 
family, hand, foot, good, morning, 
husband, hour, night, town, son 

Sports 

yesterday, today, league, team, 
national, million, season, association, 
official, die, cup, contract, race, year, 

deal, tonight, game, conference, major, 
president, round, lead, charge, 

announce, committee, victory, win, 
woman, world, series 

fan, ago, watch, back, stand, ball, day, 
question, good, turn, moment, room, 
smile, feel, hand, time, wear, people, 
knicks, hear, remember, n.b.a., net, 

guy, sit, thing, stadium, shot, kid, walk 

Politics 

official, united, today, american, 
states, administration, mr., clinton, 

military, government, weapon, 
international, effort, attack, security, 

nuclear, force, report, intelligence, 
group, court, defense, nations, 

program, include, china, agency, 
secretary, nato, plan 

man, day, world, war, people, time, 
u.s., ago, back, sit, thing, front, live, 

city, child, street, room, stand, 
saddam, morning, america, word, 

year, wait, car, kerry, young, friend, 
watch, hour 

Overall 

yesterday, today, company, million, 
official, billion, group, united, percent, 
announce, states, plan, administration, 

york, include, american, agency, 
government, federal, report, accord, 

court, executive, national, drug, part, 
state, international, corporation, deal 

day, ago, thing, man, good, stock, 
time, room, sit, back, stand, turn, 

watch, street, hear, home, feel, people, 
long, life, lot, ms., walk, town, wall, 

word, friend, live, moment, eye 

4.1.3 Production Rules(PR) 

Finally, we use production rule a the syntactic representation (Louis & Nenkova, 2012; 
Ganjigunte Ashok, Feng, & Choi, 2013; Post & Bergsma, 2013; Malmasi & Dras, 2014). 

We view each sentence a the set of grammatical productions, LHS → RHS, which 
appear in the syntactic parse tree of the sentence. We keep only non-terminal nodes, 
exclude all lexical information, so the lexical and syntactic representation capture non- 
overlap aspect of write style. All production rule from the training set be use in 

186 



Detecting Content-Dense News Texts 

the representation. The number of production rule vary for the four domains, from 16,000 
rule (Science) to 32,000 rule (Business).5 

4.2 Classifier Combination 

The three feature representation we introduce capture domain independent lexical clue 
for content-density, domain-dependent indicator for important event and general style of 
write capture by the structure of sentence in the text. We train a logistic regression 
classifier with each class of feature individually. Furthermore in this section, we examine 
two approach for combine the prediction from the three class of features. 

4.2.1 Feature-Level Combination(C1) 

First we examine the performance of feature-level combination to develop a system that 
make use of all three type of indicator of content density. We concatenate the three 
feature representation together in a feature vector. The number of entry in the feature 
vector be equal to the sum of the number of feature of the MRC, mutual information and 
production rule representations. Then we train a logistic regression model base on the 
concatenate feature representation. This way of combine evidence lead to overall im- 
provements in our early work. However much work on ensemble learn have demonstrate 
that for variety of task this method of combination be not a powerful a decision-level 
combination (for example see Raaijmakers, Truong, & Wilson, 2008; van Halteren, Zavrel, 
& Daelemans, 1998; Metallinou, Lee, & Narayanan, 2010; Bertolami & Bunke, 2006). We 
treat the feature-level combination a the baseline for our experiments. Figure 2 (a) show 
the structure of feature-level combination classifier. 

4.2.2 Decision-Level Combination(C2) 

Classifier combination have be show to outperform feature combination in a single classi- 
fier (Tulyakov, Jaeger, Govindaraju, & Doermann, 2008). There be multiple reason why 
this may be the case, especially for a linear classifier like the one we use. Concatenating all 
feature in a single representation make the system prone to over-fitting, a the number 
of feature becomes closer to the number of training examples. If the number of feature 
of a give type be considerably small (for example there be many more feature in the 
production rule representation compare to the mutual information representation), the 
signal contribute to the final decision may be dominate by the large class, defeat the 
purpose of evidence combination. It could also lead to the presence of correlate features, 
for example in the combination of the two type of lexical features. 

We propose a two layer classifier combination system. We first train a logistic regression 
classifier with each of the three feature representation individually. Then another model 
be trained, in which the feature be the probability of the content-dense class from the 
first layer classifiers. In the experiment, the corpus be split into training set, development 
set and test set. The first layer classifier be train on the training set, and the second 

5. Stanford CoreNLP package (Manning, Surdeanu, Bauer, Finkel, Bethard, & McClosky, 2014) be use to 
extract production rules. 

187 



Yang & Nenkova 

layer classifier be train on development set. Figure 2 (b) illustrates the structure of the 
decision-level combination system. 

(a) Feature-level combination (b) Decision-level combination 

Figure 2: Illustration of feature-level combination and decision-level combination 

5. Evaluation on Automatic Annotations 

In this section we will evaluate the effectiveness of each of the feature a well a the two 
combination systems. 

5.1 Classifier Evaluation 

In the feature-level combination system, we train the binary classifier use Liblinear (R.- 
E. Fan & Lin, 2008) with L2-regularized logistic regression model setting. In the decision- 
level combination experiments, we first train binary classifier base on each feature rep- 
resentation use LibLinear with the same settings. Using the probability output (for the 
content-dense class) of the first stage classifier a features, we then train a final binary 
classifier use LibSVM (Chang & Lin, 2011) with linear kernel. Grid search be use on 
training and development set to find the best hyper-parameters in all models. 

We perform 10-fold cross-validation experiment on the entire heuristically label data. 
The entire dataset be split into 10 partitions. At each run, five partition be use for training 
first-stage classifier and the feature-level combination classifier. Four partition be use 
for training the second-stage combination classifier, which us only the probability of the 
content-dense class from the first stage classifiers. One partition be use for test the 
classifiers. We evaluate the two combination model on the automatically label data but 
also analyze the performance when only a single class of feature be used. 6 

The result be present in Table 3. Because of the way the data be labeled, the two 
class be of equal size, with 50% accuracy a the random baseline. The top three row 
in the table corresponds to a system train with only one class of features. The last two 
row show the result for the two combination systems. The column correspond to the 
domain we study—business, science, sport and politics. The domain-specific model be 

6. We train the first- and second-stage classifier on different portion of the training data in the fold in 
order to obtain realistic prediction from the first-stage classifier. If we be to use all nine partition in 
the current fold, the second-stage classifier would be train on the prediction of the first-stage classifier 
on it own training data which would be unrealistically accurate. The training protocol we adopt reflect 
good realistic usage of the combine classifier. 

188 



Detecting Content-Dense News Texts 

train and test only on the data from the give domain and the result be show in the 
first four columns. The general, domain-independent model be train and test on the 
combine dataset and the last column show it performance. 

Precision, recall, F-score and accuracy be show in the table. Depending on the 
domain, accuracy be high, range between 87.2% for business and 83.6% for politics. 
The precision and recall be very balance accord the numbers, which lead F-score very 
close to accuracy in all experiments. Here we mostly focus our discussion on accuracy. 

Of the individual feature classes, the production rule representation lead to the best 
overall accuracy. Combining the representation at the feature level lead to improvement 
over the production rule classifier for the business and politics domain, a well a in the 
general domain-independent classifier but not for science and sport where performance 
use all feature be in fact bad than use production rule alone. 

In line with our previous work, all single feature classifier have very good performances. 
The production rule (PR) syntactic representation lead to the best performance for all 
domains, with accuracy over or close to 80% for all domains. The most important rule 
be quite different in each genre, but the discover pattern be mostly align with our 
intuition. For example, VP ->VB NP PRT ADVP be often associate with content-dense 
lead in Business, the example text like VP ->VB[push] NP[the Czech currency] PRT[up] 
ADVP[sharply]. The rule NP ->JJ CD NNS, however, be usually associate with non 
content-dense leads, e.g. NP ->JJ[pre-April] CD[15] NNS[blues]. The production rule 
with high weight be list in Appendix B. 

Of the lexical representations, the MRC representation lead to good results, with 
accuracy vary from 82.7% for the business domain to 79.4% for the sport domain. 
The corpus-dependent lexical representation base on mutual information have a slightly 
low performance: the accuracy range between 81.9% for business and 78.1% for politics. 

The result for the general classifier—which be train and test on data from the four 
domain pool together—are similar. For this classifier lead may change their labels, for 
example a sport article whose content-dense score be in the 80th percentile of score for 
sport may fall below the 20th percentile when all data be combined. 

The fact that the representation design independently of the training data can lead 
to such good result be a positive finding, indicate that the result be likely to be robust. 

For all the domain and general domain-independent data, decision-level combination 
considerably improves the performance compare to classifier train with only one of the 
representations. It be the most accurate among the five classifier that we compare, with up 
to 3.8% performance gain in politics compare to the best single feature classifier. 

The baseline combination system, feature-level combination, performs bad than the 
decision-level combination. One of the possible reason be that give the increase number 
of features, this model may require more training data to reach it performance potential. 
We study this aspect of model development in section 5.3. 

5.2 Combining Classifiers with Different Representations 

Here we evaluate different possible combination of feature types. We compare these pos- 
sibilities for decision-level combination, which we already establish work good than 
feature-level combination. 

189 



Yang & Nenkova 

Table 3: Binary classification result of 10-fold cross validation on the automatically label 
set for different class of feature and two fusion model for all domains: [P]recision / 
[R]ecall / [F]score / [A]ccuracy (%) 

Business Science 
P R F A P R F A 

MRC 82.0 84.4 83.1 82.7 79.4 84.1 81.6 81.1 
MI 80.0 85.1 82.5 81.9 76.9 85.0 80.7 79.8 
PR 83.8 83.9 83.8 83.8 83.2 84.7 83.9 83.8 

C1 85.8 85.3 85.5 85.5 80.4 83.7 82.0 81.4 
C2 87.9 86.4 87.1 87.2 87.5 87.0 87.2 87.3 

Sports Politics 
P R F A P R F A 

MRC 78.6 80.9 79.7 79.4 76.8 82.3 79.4 78.5 
MI 76.0 84.4 80.0 78.8 74.7 84.8 79.4 78.1 
PR 82.1 83.3 82.7 82.6 79.4 80.4 79.9 79.8 

C1 80.8 83.0 81.9 81.5 77.0 83.6 80.1 80.8 
C2 86.0 85.0 85.5 85.6 83.1 84.2 83.6 83.6 

Table 4: Binary classification result of 10-fold cross validation on the automatically label 
set for different class of feature and two fusion model for general: [P]recision / [R]ecall 
/ [F]score / [A]ccuracy (%) 

General 
P R F A 

MRC 81.2 83.4 82.3 82.0 
MI 79.4 82.2 80.8 80.6 
PR 83.5 83.5 83.5 83.5 

C1 84.4 85.8 85.1 85.0 
C2 86.8 86.4 86.6 86.7 

The motivation to examine combination of feature be that not all feature be available 
in all applications. Moreover concern about run time may make syntactic feature unde- 
sirable in certain settings, where syntactic parse may not be feasible. Mutual information 
representation also require large training data for each domain of interest, to compute 
the mutual weight for each feature. So we examine the effectiveness of combine different 
feature classes. The multilayer structure make the decision-level fusion easy to add or 
remove features. Developers can simply train a classifier base on new features, then add 
them to the second layer without affect exist single feature classifiers. 

We show the result from evaluate three different classifier combinations: MRC+MI 
(lexical feature only), MRC+PR (domain independent feature only) and MRC+MI+PR 
(all feature together). 

The result be show in Table 5. The top row in the table corresponds to the baseline, 
feature-level combination model with all three class of features. Rows 2-4 correspond to 

190 



Detecting Content-Dense News Texts 

Table 5: Binary classification result of 10-fold cross validation on the automatically label 
set for different combination of feature for all domains: [P]recision / [R]ecall / [F]1 / 
[A]ccuracy (%) 

Business Science 
P R F A P R F A 

C1 85.8 85.3 85.5 85.5 80.4 83.7 82.0 81.4 

MRC+MI 84.8 84.8 84.8 84.8 83.7 82.7 83.1 83.2 
MRC+PR 87.2 86.1 86.6 86.7 86.5 86.6 86.6 86.6 

MRC+MI+PR 87.9 86.4 87.1 87.2 87.5 87.0 87.2 87.3 

Sports Politics 
P R F A P R F A 

C1 80.8 83.0 81.9 81.5 77.0 83.6 80.1 80.8 

MRC+MI 82.2 81.8 82.0 82.0 79.5 82.3 80.9 80.6 
MRC+PR 84.6 84.7 84.6 84.7 82.2 83.5 82.8 82.7 

MRC+MI+PR 86.0 85.0 85.5 85.6 83.1 84.2 83.6 83.6 

Table 6: Binary classification result of 10-fold cross validation on the automatically label 
set for different combination of feature for general: [P]recision / [R]ecall / [F]1 / [A]ccuracy 
(%) 

General 
P R F A 

C1 84.4 85.8 85.1 85.0 

MRC+MI 83.7 83.3 83.4 83.5 
MRC+PR 86.5 85.4 85.9 86.1 

MRC+MI+PR 86.8 86.4 86.6 86.7 

decision-level model with the three different classifier combinations. As in previous tables, 
the first four column correspond to domain-specific models, and the last column show 
the result for the general, domain-independent model. Combination classifier base on 
all three feature in decision-level combination still have the high accuracy, show that 
each of the three representation contributes to the improve performance of the classifier. 
The domain independent features, MRC+PR with decision-level combination show a com- 
petitive result too, suggest that the mutual information representation be the one that 
could be remove with least degradation in performance. The accuracy be just slightly 
low than the best, 0.5% low for the business domain for example. 

The decision-level combination of lexical representation have low performance then 
the other two decision-level combination models. The accuracy range between 84.5% 
for the business domain and 80.3% for the politics domain. The combination of the two 
lexical representation lead to good performance than use either of the individual feature 
classes, suggest that MRC+MI combination at the decision-level be a good alternative 
when syntactic feature be not available. 

191 



Yang & Nenkova 

5.3 Is the Training Data Enough? 

We now discus the impact of the training set size on classifier performance. We evaluate 
the relationship between classifier accuracy and the increase of the number of training 
instance for each domain. We start with a training set of 100 articles, grow to 6,500 
instance in the training data, increase the training set with 100 randomly select article 
in each step. Accuracy be compute on the same test set for each domain. As in 
our previous experiments, 10-fold cross validation be performed. For each fold, there be 
a dedicate test set, which mean all cross-validation iteration use the same test set. The 
report result be an average of the accuracy on the fix test set in each fold. 

Figure 3 show the accuracy/size curve for each domain. Among the four genres, 
decision-level combination of all three feature have the high accuracy. The accuracy 
increase rapidly with the increase of training data when the number of training article be 
less than 2,000. When the size be large than 2,000, it continue to increase, but very slowly. 
The decision-level combination of MRC+PR features, which be the second best model for 
all domains, behaves similarly. The accuracy of the MRC+MI decision-level combination be 
the bad of the combination system and exhibit the slowest increase. 

The accuracy of decision-level combination with 6,500 training article be already very 
close to the final number with full training set (shown in table 5). Increasing the number 
of training instance barely change the performance after this point. 

The baseline, feature-level combination, have the low accuracies. Yet we still see 
increase in accuracy a the training set size increases. For three of the domains, it perfor- 
mance becomes the same a that of the MRC+MI combination with a large enough training 
set. 

The result also indicate that decision-level combination be able to achieve good per- 
formance with less training data. 

The graph suggest that the difference in performance of the content-density predictor 
in the four domain likely reflect the difficulty of the domain rather than the difference in 
training data size. 

6. Evaluation on Human Annotations 

So far we have establish that recognition of content-dense text can be do very ac- 
curately when the label for the lead be determine by intuitive heuristic on the available 
article/summary resources. We would like however to test the model on manually anno- 
tat data a well, in order to verify that the prediction indeed conform to reader perception 
of the style of the article. 

6.1 Human Annotated Dataset 

We select a total of 1,000 article and split them into two sets. For the first set of 400 
articles, the author of the paper annotate the content-dense label and provide a real- 
value score for the domain-dependent content-density of each text. Then a second set of 
600 article be select and annotate on Amazon Mechanic Turk (AMT). All annotate 
article be randomly picked from the NYT data and do not appear in the training data 
for the classifier that we evaluate here. 

192 



Detecting Content-Dense News Texts 

Figure 3: Accuracy by change size of training set for the four genres: [Top Left] Business, 
[Top Right] Science, [Bottom Left] Sports, [Bottom Right] Politics 

6.1.1 Basic Set 

In the basic human annotation set, the author of the paper annotate 400 NYT articles, 100 
from each domain, with judgement of their perceive informativeness. Similar to prior work 
on grammatically judgement (Bard, Robertson, & Sorace, 1996), the annotation be do 
with respect to a reference lead that fell around the middle of the content-dense spectrum. 
Leads be label by domain: the question be if a specific article from domain D be 
content-dense compare to the reference lead for that domain. All 100 lead from the same 
domain be grouped together and displayed in random order, with the annotator see 
lead only from the same domain until they complete the annotation for that domain. The 
reference lead in each case be drawn from the respective domain. The annotator give both 
a categorical label for the lead (less content-dense or more content-dense than the reference) 
and a real value score (ranging between 0 to 100) via a slide bar. The categorical label 
be use to test the binary classifiers. The real-valued annotation be use to compute 
correlation with classification score produce by the classifier. 

Inter-Annotator Agreement All 400 test lead be annotate a be content-dense 
or not and with a real-value indicator of the extent to which they be content-dense. Table 
7 show the percent agreement between the two annotator on the binary level task, a well 
a the correlation of the real-value annotation. For the binary annotation we also report 
the Kappa statistic. 

193 



Yang & Nenkova 

Table 7: Inter-annotator agreement on manual annotations. Percent agreement be compute 
on the binary annotation, correlation be compute on the real-value degree of content-density 
of the leads. All correlation be highly significant, with p < 0.001. 

Agreement Kappa Correlation 

Business 0.70 0.405 0.608 

Science 0.74 0.455 0.523 

Sports 0.73 0.460 0.522 

Politics 0.78 0.550 0.711 

As table 7 shows, the agreement for all domain be considerably high but not perfect. 
Agreement be highest—almost 80%— for the politics domain. The agreement be low 
in the business domain, 70%. The correlation of content-density score exceed 0.5 and 
be highly significant (p < 0.001) for all domains. The high correlation of real-valued 
scores, especially for the politics and business domains, suggest that the task may be more 
amenable to annotation and automation a a real-value prediction task rather than a a 
binary distinction. 

Kappa however be relatively low, indicate that the annotation task be rather difficult. 
To refine our instruction for annotation, we adjudicate all lead for which there be no 
initial agreement on the label. Both author sat together, reading the reference lead and 
each of the lead to be annotated, discuss the reason why the lead should be label 
content-dense or not. In many cases, the final decision be make by take into account 
the domain from which the lead be drawn (i.e. “there isn’t much important information 
in a sport lead, but it could be consider content-dense in the context of sport news 
reporting”), a well a the reference lead for the specific genre (i.e. “the lead be not that 
content-dense but appear to contain more important fact or report the news in a more 
direct style than the reference lead”). We study further the way domain and perception of 
content density interact in the next section, where independent annotator rat content- 
density both in in-domain and in domain-independent general settings.7 

Below be an example on whose label the author initially disagreed. In this lead, the 
first paragraph be non-informative and the second paragraph be informative, provide partial 
justification for either overall label. 

[Example of label disagreement] Many elderly people be already distressed by 
the increase number of drug they be taking, include painkiller and heart medication. 
Now, those who be also battling depression may be wonder where it all will end. 

Last week, researcher at the University of Pittsburgh present finding from a large 
government-financed study suggest that antidepressant be more effective in ward 
off a recurrence of late-life depression than periodic session of interpersonal therapy, a 
standardize form of talk treatment. 

7. As we will shortly see, the classifier be impressively accurate on instance in which the annotator agree 
in their initial annotation and quite poor on the lead that require adjudication. These finding suggest 
that in future work in may be beneficial to develop a classifier for sentence-level prediction (Yang, Bao, & 
Nenkova, 2017) of content-density, which would be helpful for characterize lead that mix informative 
and entertain sentences. Another clear alternative be to develop a classifier to predict that a text be 
ambiguous in term of it content-density status. 

194 



Detecting Content-Dense News Texts 

6.1.2 AMT Annotation Set 

We also compile a second set of 600 NYT articles, 150 for each domain. In an attempt 
to provide more guidance to the annotators, we give four reference lead for each domain, 
two a example of prototypical content-dense lead and two a example of lead that be 
clearly not content dense. The reference lead for each domain be show in Appendix A. 
The annotator saw the four prototypical leads, a well a a group of lead that they have 
to annotate. They provide both a categorical label for each target lead (content-dense or 
not) and a real value score for the degree to which it can be consider content-dense (range 
between 0 and 100). 

The annotation be partition into group of five leads—an annotator have to label at 
least five lead and then request more data for annotation, in group of five. To embed some 
quality control, one of the five lead in each group be a lead from the dataset annotate by 
the authors, for which they agree in independent annotation before the adjudication step. 
This data allow u to ass the quality of annotation after problematic annotator be 
filter out. 

Here we also study the difference in how the content-density of a text would be perceive 
in-domain and in general setting. For each lead text, two task be publish separately 
for label content-density in-domain or in general. For the in-domain task, annotator 
be give domain information (i.e. “Here be article drawn from the Sports section of a 
newspaper...”) and the reference lead be select from that domain. In the general task, 
worker be not told the domain of the lead and the reference lead be select without 
regard to domain.8 

Ten annotator annotate each lead in each of the two conditions. 

We use two rule to filter out unqualified annotators. We filter out all annotation by 
annotator who annotate too quickly or be inconsistent. The first rule be that annotator’s 
average annotation time per task should be longer than 40 seconds. For reference, the 
average annotation time per task among all annotator be around two minutes. The second 
rule be that label category and score should be consistent for each lead text. If an annotator 
label a lead a content-dense but give a very low content-dense score or vice versa, we 
know something in their understand of the task be amiss. 

There be on average 8 annotator for each item after filter out unqualified words. 
For each lead, we use the majority category a the final category label and the average score 
a the final score label. If there be a tie for a lead, we label it content-dense. 

Table 8 show the agreement and kappa between the majority label from AMT worker 
and the authors’ agree labels. We compute these only for the in-domain label because 
our initial annotation be domain dependent. 

Agreement for the business and sport category be high but only moderate for science 
and politics. We be unsure about the exact reason why this be the case. 

AMT worker annotate lead in two conditions: in-domain, where the judgement be 
specific to the domain from which the lead be drawn and general (domain-independent), 
where a domain be not specify and text from all four domain be randomly mixed in the 
annotation tasks. Table 9 show the number of content-dense lead for each domain for both 

8. The content-dense example be from Business and Science, and the non content-dense from Business 
and Politics. 

195 



Yang & Nenkova 

Table 8: Agreement of embed baseline lead between AMT worker and author of the 
paper. 

Agreement(%) Kappa 

Business 92.1 0.841 

Science 86.8 0.622 

Sports 97.3 0.947 

Politics 79.0 0.574 

Table 9: Number (and percentage) of content-dense lead annotate by AMT worker for 
each domain. The same data be annotate with respect to in-domain and general criterion 
and the statistic for each condition be show in the first and last column respectively. The 
two middle column show the number of lead that change label from content-dense(CD) 
to non content-dense(Non-CD) or vice versa between the in-domain and general condition, 
broken down accord to the direction of the change. 

In-Domain 
Label Changes 

General 
CD → Non-CD Non-CD → CD 

Business 93 (62.0%) 8 11 96 (66.0%) 

Science 64 (42.7%) 16 25 73 (48.7%) 

Sports 76 (51.1%) 38 2 40 (26.7%) 

Politics 72 (48.0%) 2 53 123 (82.0%) 

Overall 305 (50.8%) 64 91 332 (55.3%) 

conditions, along with the number of lead whose label change across conditions. The first 
and the forth column correspond respectively to the number (percentage) of content-dense 
lead among all in-domain and general label for the same data. The second and third 
column show the number of label that change their label from content-dense (CD) to 
non content-dense (Non-CD) or vice versa, between the domain-dependent and the domain- 
independent labelling. 

Clearly, the domain context play a large role in the perception of content density. 
The change be most clear for the politics and sport domain: in the domain-independent 
label a large number of sport leads, which appear content-dense for their domain, 
be consider non content-dense in general. For sports, during in-domain annotation we 
have about half of the lead marked a content-dense, while just under 30% of the same 
lead be marked a content-dense in domain independent annotation. Similarly many of 
the politics lead consider non content-dense for the standard of the politics domain 
be consider a such in the domain-independent setting. There be virtually no change 
in label in the opposite direction, which conforms to our expectation and provide an 
additional confirmation of the reasonable quality of the crowdsourced annotations. 

The politics domain appear most stable, with very similar percentage of lead judged 
a content-dense in in-domain and general annotation. We also get some additional evi- 
dence that this domain be harder to annotate, possibly because lead there often mix both 
direct fact and non-literal content. We discuss this trend in our analysis of the author 

196 



Detecting Content-Dense News Texts 

annotation of the domain. In the business domain, the ratio of lead that change label 
between the in-domain and general set be closest to 1, show least bias in perception. 
This be in stark contrast with politics for example, which be consider more content dense 
in general, attest by both the number of lead that change label and the percentage of 
lead in the general set (82%). 

Overall the in-domain annotator have a more balance number of content-dense and 
non content dense labels. 

6.2 Are Leads Informative? 

In automatic summarization research, the article lead be generally consider to be infor- 
mative, or content-dense. The begin of the article be know to be a strong summary 
baseline (Mani, Klein, House, Hirschman, Firmin, & Sundheim, 2002; Nenkova, 2005) and 
many feature for identify important content in article be base on overlap with the 
opening paragraph. Our annotation allow u to directly examine to what extent this 
general intuition hold across domain of journalistic write in the New York Times. 

Table 9 show the number of lead in each domain label a content-dense in the 
manually annotate dataset described above. It be clear that the prevail assumption 
that the lead of the article be always content-dense be not support in the data we analyze 
here. 

The majority of article in the politics domain, which be representative of the data 
on which large-scale evaluation of summarization system tend to be perform and which 
focus on specific current events, be indeed content-dense. More than 60% of lead in this 
domain be label a content-dense in the authors’ annotation. The trend be similar in the 
AMT annotations. 

Conforming to intuition, the second large proportion of content-dense lead be in the 
business domain. There the article be often trigger by current event but here be more 
analysis, humor and creativity. In these lead important information can often be infer 
but be not directly state in factual form. Business lead also tend to have the same labels, 
regardless of whether they be annotate with respect to the domain standard or in general. 
For the business domain, only 19 out of 150 label change across condition (cf. first line 
in Table 9), which corresponds to at least half the rate of label change for any of the other 
domains. 

In sport the factual information in the lead that have to be conveyed be not much and 
it be embellish and present in a verbose and entertain manner. Particularly AMT 
annotator consider less than a third of the sport lead to be content-dense across domains. 
In the science journalism section many lead only establish a general topic or an issue, or 
include a human interest story. Overall there be only a small partition of science lead 
label a content-dense. 

The perception of content density be certainly influence by the context of the domain. 
There be 55 politics lead that change label from in-domain to the general condition, and 
53 of them be change from non content-dense to content-dense, indicate that in that 
set annotator follow their domain bias in decide the label. Similarly 38 sport in- 
domain-content-dense lead be non content-dense across domains, but only 2 lead change 
in the opposite direction. 

197 



Yang & Nenkova 

These finding have two important implication for language processing application and 
summarization in particular. 

It be unrealistic to expect that all newspaper text have high informational value. Find- 
ing valuable content have be address a a standalone problem in social medium (Becker, 
Naaman, & Gravano, 2011) and user generate data (Agichtein, Castillo, Donato, Gionis, 
& Mishne, 2008) but generally have be ignore in news analysis. 

In addition, our analysis cast doubt on the practicality of require summarization 
system to produce summary of fix length. Many of the article with lead that be not 
content-dense do not discus even in the body of the article an event reader would consider 
important. An appropriate summary should simply indicate this, or a summary should not 
be even attempted. Automatic system be anyhow not particularly good at summarize 
article that deal with opinion or discussion rather than a specific event (Nenkova & Louis, 
2008). In information access applications, tag the genre of the article a event-centered 
or not (similar to early work in distinguish opinion piece from factual reporting, see Yu 
& Hatzivassiloglou, 2003) may be most helpful, with preview snippet summary produce 
only for the event-centered articles. 

6.3 Classifier Evaluation 

Here we evaluate the combine two-layer classifier train on heuristically label data on 
the manual annotations. Note that the manual annotate lead be use for evaluation 
only, no additional training be perform at this stage. 

Following the assumption prevail in summarization research that the lead of the 
article be always content-dense, the first baseline (Baseline-1) always considers the lead of 
the article content-dense. 

The second baseline (Baseline-2) be establish base on the length of the entire news 
article, not only the lead. The intuition be that longer article may have uninformative lead 
design to draw the reader into the subject while short article need to start out with a 
more focus presentation of the event so be likely to have an content-dense lead. We train 
a L2-regularized logistic regression model base on this single feature. As table 10 shows, 
the single feature classifier achieve reasonable accuracy of 68% for the science domain. 

6.3.1 Classification Results on the Basic Set 

Table 10 show the result from apply the domain-dependent and the general domain- 
independent model on the basic human annotation set. Accuracies compute against each 
of the two individual annotator be show in the last two columns. Sports and politics 
domain have high prediction accuracy on the data label by the first annotator, and 
business and science domain have high prediction accuracy for the second annotator’s 
labels. Also the prediction accuracy have small variance on the data label by the first 
annotator, between 78% for the politics domain and 74% for science the domain, compare 
with the accuracy on data label by the second annotator, between 87% for business 
and 71% for sports. Overall however the prediction accuracy on the final combine data, 
after disagreement have be adjudicated, be highest, demonstrate that the adjudication 
procedure do lead to more internally consistent labels. As in the heuristically label data, 

198 



Detecting Content-Dense News Texts 

recognition accuracy be high for the business and science domain (83%) and low for 
the sport and politics domain (around 80%). 

We also evaluate the prediction accuracy separately on the subset of the data for 
which the two annotator agree on the label in the first stage of independent annotation, 
correspond to the presumably clear-cut cases, and those for which adjudication be 
needed. Clearly, the classifier capture characteristic of content-dense lead quite well. 
The accuracy on the subset of the data for which the annotator agree be much high 
than that for individual annotators, indicate that when the text have mixed characteristic 
lead to disagreement in annotation, it be more likely that the classifier make more error 
a well. 

On the agree subset—marked with the same label by both annotator during indepen- 
dent annotation—accuracies be around 90% for the business and science domains, 80% for 
sport and politics domains. 

The classifier accuracy be much high than the baseline for all domains. 

We also calculate the precision, recall and F-score for the content-dense lead class for 
the combine dataset. The result be show in Table 11. The domain model performs 
best in three of four genres, while the overall general model lead in politics. This find 
be again align with what we observe on accuracy. Although both model can achieve 
good accuracy on sport leads, the F score in that domain be not a good a in the other 
domains. Here both the domain model and overall model can achieve a very high precision 
but a relatively low recall. 

Table 12 show the correlation between the classification score from the final classifier 
and the real-value score of content-dense by the two annotators. All correlation be highly 
statistically significant. In line with what we have see in the analysis of other results, the 
correlation be the high for the business domain. 

Similarly we compute the prediction accuracy stratify accord to the classifier con- 
fidence in that prediction. Figure 4 show the plot on all four genres. The accuracy of 
high confidence prediction be much high than the overall accuracy. The ”article length” 
baseline, however, have low accuracy in it high confidence predictions. 

6.3.2 Classification Results on the AMT Annotations 

Table 13 show the accuracy and F-score of the domain-dependent and the general domain- 
independent model on the AMT annotations. As in previous tables, row 1 and 2 represent 
the result from domain model and the domain-independent model respectively. Rows 
3 to 4 show result for the two baselines. Our classifier outperform the baseline by a 
large margin except for politics in the domain-independent labels, where the baseline that 
considers all lead to be content dense work best. Overall however, the result show that 
the baseline of assume all lead be content-dense performs poorly and the propose 
approach significantly improve the accuracies. 

Comparing the accuracy of prediction for data drawn from the same newspaper section, 
it be evident that business and science have the most stable prediction and the accuracy 
of the domain-dependent and the domain-independent classifier do not differ much on 
these subset of the test data. The classifier train on domain-independent label achieves 
78.0% accuracy on the domain-specific labels, in which the annotator be explicitly told 

199 



Yang & Nenkova 

Table 10: Binary classification accuracies(%) on basic human annotate datasets for model 
train on heuristically label data. 

Business Combined Agreed Adjudicated Anno 1 Anno 2 

Domain model 83 94.3 56.7 75 87 
Overall model 79 91.4 50.0 75 83 

Baseline-1 53 52.8 53.3 47 57 
Baseline-2 60 65.7 46.7 58 64 

Science Combined Agreed Adjudicated Anno 1 Anno 2 

Domain model 83 89.2 65.4 77 80 
Overall model 81 89.2 57.7 71 86 

Baseline-1 37 31.1 53.8 45 27 
Baseline-2 68 69 65.4 62 65 

Sports Combined Agreed Adjudicated Anno 1 Anno 2 

Domain model 78 80.8 70.3 74 71 
Overall model 75 75.3 74.1 69 68 

Baseline-1 49 46.5 55.6 45 50 
Baseline-2 65 70 51.9 63 66 

Politics Combined Agreed Adjudicated Anno 1 Anno 2 

Domain model 78 83.3 59.1 78 74 
Overall model 80 83.3 68.2 76 76 

Baseline-1 61 60.3 63.6 55 61 
Baseline-2 51 55 36.4 55 53 

Table 11: [P]recision, [R]ecall and [F]score (%) on basic human annotate datasets for mod- 
el train on heuristically label data. [D]omain model, [O]verall model, and [B]aseline-2. 

Business Science Sports Politics 
P R F P R F P R F P R F 

D 81 88.7 84.7 95.7 57.9 72.1 90.9 50 64.5 95.4 67.7 79.2 
O 76.8 84.3 80.4 91.3 55.2 68.8 86.3 50 63.3 87.3 78.7 82.7 

B-2 61 67.9 64.3 61.5 42.1 50 66.7 57.1 61.5 63 47.5 54.2 

Table 12: Correlation between predict probability and human annotate scores. All 
correlation be highly significant with p < 0.001. 

Annotator 1 Annotator 2 
Domain Models Overall Models Domain Models Overall Models 

Business 0.621 0.647 0.797 0.810 
Science 0.575 0.546 0.711 0.758 
Sports 0.590 0.575 0.588 0.582 
Politics 0.658 0.629 0.609 0.592 

the news section from which the article be drawn and use this information in judging if 
the lead be content-dense or not. This accuracy be less than 2% low than the prediction on 

200 



Detecting Content-Dense News Texts 

Figure 4: Prediction accuracy base on probability rank on basic human annotation 
set. [Top Left] Business, [Top Right] Science, [Bottom Left] Sports, [Bottom Right] 
Politics 

domain-independent labels. Similarly in the science domain the difference in performance 
for the two type of label be a low a 2.0%. In stark contrast, there be large difference 
in performance for the in-domain and domain-independent label for sport and politics, 
where the difference between the two reach 10%. 

The crowdsourced annotation be perform both in-domain (judging the content 
density with respect to the expectation for the give domain, politics for example) and in 
domain independent setting. Here domain model be on average bad than the domain 
independent models. For the sport domain, training a domain-specific classifier help 
most in improve the detection of content-dense sport lead but for the other domain 
the advantage be less clear. This result be reassuring. If the domain model be clearly 
superior, one would have need accurate domain predictor for practical applications. The 
analysis present here demonstrates that a domain-independent classifier may be sufficient 
for many applications. 

The accuracy of the domain model drop considerably compare to their respective 
accuracy on the author-annotated set. For example, there be around 8.0% drop in the 
politics domain. There be several possible reason for this difference. The article in the 
initial set that the author annotate be select only from the article publish in 2005 
and 2006 while the AMT set be select from the entire NYT dataset from 1987 to 2007. The 
annotation instruction also differ for the two sets. The AMT annotator be present 
with prototypical content-dense and non content-dense lead a references, while the author 

201 



Yang & Nenkova 

have only one lead in the middle of the range of content-density a reference. Finally, the 
general domain-independent classifier on average work best, predict both the in-domain 
and general label in the test set good than the domain-dependent classifiers. This trend 
indicates that AMT worker be likely more influence by general domain expectation 
when label the data. It be plausible that domain-dependent annotation require more 
detailed instruction that be not a readily pass on in the crowdsourced setting. 

Table 13: Binary classification result on AMT annotate datasets for model train on 
heuristically label data: [A]ccuracies(%) and [F]scores(%) 

In-domain Business Science Sports Politics Average 
A F A F A F A F A F 

Domain model 76.0 78.3 72.7 70.9 73.3 69.5 70.7 73.5 73.2 73.5 
General model 78.0 80.4 76.7 74.8 70.0 63.1 70.0 73.3 73.7 73.9 

Baseline-1 62.0 – 42.7 – 51.1 – 48.0 – 55.0 – 
Baseline-2 58.0 62.7 64.7 40.0 62.7 51.2 52.7 48.5 59.5 61.4 

Domain indep. Business Science Sports Politics Average 
A F A F A F A F A F 

Domain model 79.3 81.4 73.3 74.0 80.0 62.8 76.7 83.8 77.3 75.5 
General model 77.3 80.4 78.7 78.4 82.0 67.6 77.3 84.5 78.8 77.7 

Baseline-1 66.0 – 48.7 – 26.7 – 82.0 – 55.9 – 
Baseline-2 62.7 68.6 66.7 62.8 68.0 53.3 50.7 61.1 62.0 61.7 

We further compute the correlation coefficient between predict probability and av- 
erage score annotate by AMT workers. The result be show in Table 14. Domain model 
have good correlation than general model in three of domain for domain dependent (in- 
domain) labels, but with small absolute difference in correlation. The domain-independent 
model be much good in predict content-dense in the general, domain-independent 
condition. All correlation be highly significant, range from 0.577 to 0.661 against in- 
domain label and from 0.602 to 0.730 against domain-independent labels. As in the binary 
prediction task, the domain-independent label appear to be easy for the system to predict. 
The correlation coefficient be in line with our intuition and much closer to the number we 
have see base on the basic author-annotated set (shown in Table 12). This trend implies 
that predict content-density in term of real-value score may be more suitable for this 
task. 

Table 14: Correlation between predict probability and average score annotate by AMT 
worker in the domain specific and general condition. All correlation be highly significant 
with p < 0.001. 

In-Domain Labels Domain Independent Labels 
Domain Models General Model Domain Models General Model 

Business 0.602 0.614 0.713 0.730 
Science 0.661 0.646 0.652 0.690 
Sports 0.600 0.577 0.619 0.602 
Politics 0.616 0.615 0.652 0.668 

202 



Detecting Content-Dense News Texts 

For the AMT annotate test set, we also compute the prediction accuracy stratify 
accord to percentile of data ranked by the classifier confidence in that prediction. Fig- 
ures 5 and 6 show the plot on all four domain for the two type of annotate label 
(domain-specific or domain-independent). Again, the accuracy of high confidence predic- 
tions be much high than the overall accuracy. The article length baseline, however, have 
much low accuracies. 

Figure 5: Prediction accuracy base on probability rank on AMT annotate data. The 
x axis represent the percentile of data use to calculate the accuracy accord to the 
predication confidence. (In-domain): [Top Left] Business, [Top Right] Science, [Bottom 
Left] Sports, [Bottom Right] Politics 

7. Recognizing Better Summaries 

So far we have demonstrate that detector of content-density can be developed use heuris- 
tically label data and that it can achieve respectable accuracy in intrinsic evaluation on 
human-labeled leads. Ultimately however the goal would be to integare the content-density 
prediction in information seek application such a summarization and news browsing. 
Testing the impact of the content-density prediction in such extrinsic evaluation will be 
the main focus of future work. 

Here, however, we show a feasibility study to verify the potential for development of more 
inform summarization method that exploit the concept of content density. Specifically, 
we demonstrate that the content-density detector be able to recognize when an automatic 
summary of a single news article be good than the lead of the article. This be an important 
open problem in summarization, where the lead paragraph baseline be very strong and few 

203 



Yang & Nenkova 

Figure 6: Prediction accuracy base on probability rank on AMT annotate data. The 
x axis represent the percentile of data use to calculate the accuracy accord to the 
predication confidence. (Domain independent): [Top Left] Business, [Top Right] Science, 
[Bottom Left] Sports, [Bottom Right] Politics 

system outperform it (Over, Dang, & Harman, 2007). Moreover, all of the proof-of-concept 
experiment from the previous section be perform on data drawn from the NYT. We 
would like to verify that the usefulness of the prediction remains when data from other 
source be considered. 

Motivated by these goals, we perform our experiment on detect if a machine sum- 
mary be more informative than the lead on two datasets: NYT and on data from the 
Document Understanding Conference, which have data from a variety of sources. 

We randomly select 400 article with manual summary from the NYT, 100 for each 
genre. We generate automatic summary for the article use two systems. The first 
system, LeadSumm, be the strong lead baseline which pick the first 100 word a the 
summary. The second system be IcsiSumm (Gillick & Favre, 2009), which be one of the 
state-of-the-art multi-document summarization system (Gillick, Riedhammer, Favre, & 
Hakkani-Tur, 2009; Berg-Kirkpatrick, Gillick, & Klein, 2011). 

We perform human evaluation to determine which of the two summary in the pair 
(LeadSumm and IcsiSumm) be better. We ask annotator to first read the manual sum- 
mary from NYT, then read the two summary generate by the systems. Then we ask 
the annotator to indicate which of the two system summary cover good the information 
express in the NYT goldstandrd. They be also provide with an option to indicate that 
the two system summary cover the information express in the goldstandard equally well. 

204 



Detecting Content-Dense News Texts 

The flow between the sentence in the LeadSumm summary be good than those in the 
automatic summary because this be a snippet of professionally write discourse. Here our 
goal be to study the content-density of the two summaries, independently of the linguistic 
quality of the summary which we know favor the lead system. For this reason, we random- 
ized the order of the sentence in both of the LeadSumm and IcsiSumm summaries. The 
order of present the LeadSumm and IcsiSumm to annotator be also randomize during 
judgement collection. 

The task be publish on Amazon Mechanical Turk (AMT) and each task be assign 
to 10 annotators, with one summary per task/HIT. IcsiSumm generate empty summary 
for 77 out of the 400 randomly select article and two of it summary be identical to 
the lead baseline. We remove those task so there be total 323 task be published. The 
majority vote of the 10 annotator be use a the final label. The human annotation be 
use a ground truth in the follow steps. 

Next we apply the content-dense detector on the generate IcsiSumm and LeadSumm 
to get content-dense probability score for each. The summary with high content-dense 
score be predict to be the good summary. As expect from prior manual share task 
evaluations, LeadSumm be good then IcsiSumm for most of the articles. 

The confidence in the prediction that one summary be good than another be control 
by the content-dense score difference. The large the difference between the content-density 
score of the IcsiSumm and LeadSumm is, the more confident we can be that the summary 
be indeed better. We track how the summarization performance varies with the difference in 
content-density scores. In case when the difference between the two content-density score 
be low than a set value, we consider that the lead summary be better. By define the 
score difference = scoreIcsiSumm − scoreLeadSumm, we cutoff the evaluation sample by 
score difference compute the metric for different cutoff levels. 

Table 15 show the result of detect when IcsiSumm produce good summary than 
the lead baseline on the NYT articles. This be equivalent to a combination system which 
us lead summary unless it be confident that the automatic summary be better, in which 
case it us the IcsiSumm summary. The first column represent the cutoff value and the 
second column show the number of total sample within this cutoff. Column 3 to 5 be 
the statistic of human judgements. The last column show the number (percentage) of 
correctly predict sample of the combination system. Each row show the result of 
the system with a cutoff value. The last row show the statistic for the entire dataset 
and two baselines, one that pick ICSISumm summary only and another that pick lead 
summary only. The lead be good for 59% of the test articles; the IcsiSumm produce 
the more informative summary for 34% of the test articles. The two summary be 
consider equally informative in the rest of the cases. Clearly, a expect from past DUC 
evaluations, the lead baseline summary be good than the automatic summarizer. However 
even an extractive summarizer can significantly outperform the lead baseline if we have a 
reliable way in which to predict when an alternative summary would be more informative; 
this could improve one out of each three summary produce by the summarizer. 

Last column show the performance of a combination system use content-density score 
to decide which of the two available summary be superior. Whenever at least some threshold 
be use to decide when the automatic summary be better, the combination system’s output 
be prefer by the assessor considerably more often than the output for the lead baseline. 

205 



Yang & Nenkova 

Particularly for threshold between 0.1 and 0.4, the output of the combination system be 
prefer between 64 and 66% of the time, compare to the 58.5% for the lead baseline. 
These improvement be statistically significant (with p < 0.05) accord to a binomial 
test with expect probability of produce good summary of 0.585, correspond to the 
human preference for the baseline lead summaries. 

Table 15: Performance of combination system with different cutoff on NYT articles. The 
last column show the number (percentage) of correct predict sample of the combination 
system. 

cutoff # of sample 
Human Judgement 

Icsisumm Tie Leadsumm Combination System 

0.5 18 14 0 4 199 (61.6%) 
0.4 29 23 1 5 207 (64.1%) 
0.3 42 32 2 8 213 (65.9%) 

NYT 0.2 54 39 2 13 215 (66.6%) 
0.1 79 47 4 28 208 (64.4%) 
0 179 78 7 94 173 (53.6%) 

All 323 109 (33.7%) 25 189 (58.5%) N/A 

Next we verify that the propose model work with reasonable accuracy on source 
other than the NYT. We run the same LeadSumm and IcsiSumm system on the DUC 
dataset (Over et al., 2007). We only perform the experiment on the data from DUC2002, 
which be the last year NIST provide single-document human summaries. 

There be total of 533 article from various source in DUC2002, include Associated 
Press (AP), Wall Street Journal (WSJ), Los Angeles Magazine (LA), FT Magazine (FT), 
San Jose Mercury News (SJMN) and Foreign Broadcast Information Service Daily Reports 
(FBIS). AP be a newswirse service, provide high-quality news reporting use by many 
medium outlets. By the nature of newswire services, the AP article (and leads) be expect 
to be contain a large propartition of content-dense texts. The other article source be 
drawn from newspapers, so be much more likely to include lead that be not content- 
dense. 

Again, we filter out article for which IcsiSumm summary could not be generate or 
for which the automatic and lead summary be identical. Again, we exclude from consid- 
erations article for which IcsiSumm do not generate a summary or for which IcsiSumm 
produce a summary consist of the lead of the article. After filter these, we obtain 
493 article for the evaluation. 

As with the NYT experiment, we use AMT to obtain judgement about which of the 
two summary of the article be better. All the annotation setting be exactly the same 
with the NYT annotation described above. 

Again, we then apply content-dense detector on the generate IcsiSumm and LeadSumm 
to detect which summary be good base on their content-dense scores. Table 16 show the 
result of detect good IcsiSumm on the DUC2002 articles. 

The judgement on data drawn from source different from the NYT allows u to get a 
sense about the extent to which the content-density detector we developed to the news genre 
in general rather than specific to the NYT. The observation that bear special mention be 

206 



Detecting Content-Dense News Texts 

Table 16: Performance of combination system with different cutoff on DUC2002 articles. 
The last column show the number (percentage) of correct predict sample of the combi- 
nation system. 

cutoff # of sample 
Human Judgement 

Icsisumm Tie Leadsumm Combination System 

0.5 5 4 0 1 246 (78.6%) 
0.4 13 9 0 4 248 (79.2%) 

DUC 0.3 22 11 1 10 244 (78.0%) 
AP 0.2 38 15 1 22 236 (75.4%) 

0.1 72 21 2 49 215 (68.7%) 
0 117 30 5 82 191 (61.0%) 

All 313 51 (16.3%) 19 243 (77.6%) N/A 

0.5 4 3 0 1 134 (74.4%) 
0.4 6 5 0 1 135 (75.0%) 

DUC 0.3 8 6 1 1 136 (75.6%) 
Other 0.2 9 6 2 1 136 (75.6%) 

0.1 18 9 4 5 135 (75.0%) 
0 48 19 5 27 123 (68.3%) 

All 180 41 (22.7%) 8 131 (72.8%) N/A 

that here, the percentage of article for which the IcsiSumm be able to produce more infor- 
mative summary than the lead summarizer be considerably small than in the randomly 
select sample of NYT articles. For the AP articles, the automatic summary be judged 
a good than the lead baseline for 16% of the test articles. This conforms with expectation 
that the AP article and lead will be overall more content-dense than regular newspaper 
sources. For the other source (newspaper), the percentage of automatic summary that 
be good than the lead be 23%. For comparison, in the NYT sample the system produce 
a more informative summary in 34% of the cases. This large percentage may reflect the 
style of the New York Times or the fact that the article from NYT be randomly drawn, 
so cover a broader range of domain than the DUC data. 

The number indicate that the style of AP be the most typically informational while the 
NYT be the most stylistically rich, with lead that be often not content-dense. If this be the 
case, the room for expedite news search and browsing via automatic summarization have 
be underestimated in DUC evaluations. 

The last column show the performance of the combination system. The performance 
on DUC Other be still similar to the performance on NYT. Just pick any cutoff ≥ 0.1 
will lead a performance improvement. However, the combination system be only good than 
lead baseline for AP when the cutoff ≥ 0.3 and reach the best performance when set 
cutoff to 0.4, which be align with previous find that AP be typically informational. 

Overall, the combination system performs good than baseline system when we pick 
the right cutoff. The choice of cutoff depends on the source type and the reason be the 
write style can be very different in different source a discuss above. For NYT and 
DUC newspaper source (excluding AP), the combination system be able the achieve good 

207 



Yang & Nenkova 

performance when set cutoff a 0.1 and achieve high accuracy when set cutoff 
a 0.2. For the AP, however, it be much harder to find a cut off in which the combination 
system would outperform the lead baseline. The cutoff have to be set to 0.4 to get the best 
performance, so the general ability to produce a good summary with AP data be dubious. 

This analysis of cut-offs at which prediction of content density would improve summa- 
rization be only a pilot analysis. Ideally, we would need sufficient data to have a dedicate 
development set on which to determine the cut-off and an independent test set on which 
to verify it utility. Such in-depth study be left for future work. Here however we note that 
there be a clear difference in the performance for the stylistically different newswire and 
newspaper and that there be sufficient evidence for potential of use the content-density 
stylistic distinction for improve single document summarization. 

8. Conclusion and Future Work 

In this paper we introduce the task of detect content-dense news article leads. We use 
article/summary pair from the NYT corpus to heuristically label a large set of article a 
content-dense when the lead of the article overlap highly with the human summary and a 
non content-dense when the overlap be low. 

We present experiment with two lexical representation and one syntactic representa- 
tion. The production rule syntactic representation be the best predictor of lead content- 
density among the three. The corpus-independent lexical representation from a vocabulary 
define by the MRC lexicon prove to be the more useful lexical representation. We com- 
par a feature-level combination model and a two-layer decision-level combination model. 
The latter performs best in all our experiments. 

Our analysis reveals that there be a large variation across news domain in the fraction 
of content-dense lead and in the prediction accuracy that can be achieved. Contrary to 
popular assumption in news summarization, we find that a large fraction of lead be in 
fact not content-dense and thus do not provide a satisfactory summary. 

Overall domain-specific model be more accurate than in-domain label from train 
annotators. The general model train on all data pool together achieves good perfor- 
mance on crowdsourced annotation in both domain-dependent and domain independent 
annotation conditions. Our experiment indicate that predict content-dense in term of 
real-value score may be more accurate and beneficial for application than simply classify- 
ing a lead a content-dense or not. 

In this work, we have establish the feasibility of the task of detect content-dense 
texts. We have confirm that the automatic annotation of data capture distinction in 
informativeness a perceive by people. We also show proof-of-concept experiment that 
show how the approach can be use to improve single-document summarization of news 
and the generation of summary snippet in news-browsing applications. In future work the 
task can be extend to more fine-grained levels, with prediction on sentence level and the 
predictor will be intergared in a fully function summarization system. 

All data for the work present in this paper and the domain-dependent and general 
classifier will be make publicly with the publication of this article. 

208 



Detecting Content-Dense News Texts 

Acknowledgments 

We report our initial work on detection of information-dense text in a paper publish at 
AAAI 2014 (Yang & Nenkova, 2014) In this manuscript we have further extend the work 
by use much large sample of New York Times article for training and by analyze 
the performance of a large number of two-layer classifiers. Here we also introduce a new 
collection of manually annotate test data, for both domain-dependent and general content- 
density of texts. We make use of this collection for detailed evaluation of the content-density 
classifiers. In our AAAI 2014 paper, we use the term information-dense to describe the 
type of text we wish to detect. Here we switch the terminology to content-dense, to avoid 
confusion with work in the intersection of cognitive science and computational linguistics 
that us the term information density in information-theoretic sense to describe the change 
in surprise in the linear processing of sentence (Jaeger, 2010; Pate & Goldwater, 2015). 

Appendix A. Reference Leads Used in AMT Annotations 

Here we present all the reference lead annotator saw in AMT human intelligent task 
(HITs). 

A.1 In-Domain Reference Leads 

In in-domain annotations, annotator label a group of five lead from same domain in 
each HIT. Two content-dense lead and two non content-dense lead from the same domain 
be displayed at the beginning. 

A.1.1 Reference Leads for Business Domain 

[Content-dense Ref 1] Securities regulator charge one of the richest men in Mexico, 
Ricardo B. Salinas Pliego, with fraud yesterday, in a lawsuit that seek to have him bar 
a a director or officer of any company whose share trade on an American exchange. 

The Securities and Exchange Commission also sought to have Mr. Salinas Pliego, the 
chairman of TV Azteca, the second-biggest Spanish-language broadcaster, give up more 
than $110 million he make from trading in the company’s stock and debt. 

[Content-dense Ref 2] In a rare move, Microsoft say yesterday that it have agree 
to pay a percentage of the sale of it new portable medium player to the Universal Music 
Group. 

Universal Music, a unit of Vivendi, will receive a royalty on the Zune player in exchange 
for licensing it recording for Microsoft’s new digital music service, the company said. 

[Non content-dense Ref 1] LOOKING for some thong underwear or perhaps a leather 
jacket and don’t know where to find them? Try log on to a restaurant Web site. 

Small restaurateur be increasingly use the Internet to sell good that go far beyond 
the usual array of brand T-shirts and hats, in hope of not just building the bottom line, 
but also cultivate possible new market for expansion. 

209 



Yang & Nenkova 

[Non content-dense Ref 2] ”WHAT stress me most,” the chief executive of Novartis, 
Daniel L. Vasella, said, ”is that we be get new regulation from abroad without any 
consultation.“ 

This have be the World Economic Forum that the United States government largely 
pass by. In a world that both respect and fear American power, there be worry that the 
United States do not care what others think. 

A.1.2 Reference Leads for Science Domain 

[Content-dense Ref 1] Scientists have decode the chimp genome and compare it 
with that of humans, a major step toward define what make people human and develop 
a deep insight into the evolution of human sexual behavior. 

The comparison pinpoint the genetic difference that have arisen in the two specie 
since they split from a common ancestor some six million year ago. 

[Content-dense Ref 2] A popular class of drug for high blood pressure, ACE in- 
hibitors, may cause birth defect if take during the first three month of pregnancy, doc- 
tor be reporting. Pregnant woman and those who be planning to become pregnant should 
avoid the drugs, the researcher and official at the Food and Drug Administration warn. 

ACE inhibitor have long be know to cause birth defect if take late in pregnancy, 
but until now be consider safe if take in the first trimester. 

[Non content-dense Ref 1] To gauge the potential consumer impact of the consoli- 
dation sweep the telephone industry, look no further than the silver-toned plastic phone 
gathering dust on the desk in Justin Martikovic’s studio apartment. 

Mr. Martikovic, 30, a junior architect who relies on a cellphone for his normal calling, 
say he never us the desk phone – but he pay $360 a year to keep it hooked up. 

[Non content-dense Ref 2] As the horror of the South Asian tsunami spread and 
people gather online to discus the disaster on site know a Web logs, or blogs, those 
of a political bent naturally turn the discussion to their favorite topics. 

To some in the blogosphere, it simply have to be the government’s fault. 

A.1.3 Reference Leads for Sports Domain 

[Content-dense Ref 1] Ivor G. Balding, one of three British brother who gain 
international fame a polo star in the 1930’s, when the sport attract large crowd and 
wide press coverage, die on Thursday at his home in Camden, S.C. He be 96. 

His death be announce by his family. 

[Content-dense Ref 2] Finally, the deal be done. 

Laveranues Coles, the wide receiver from the Washington Redskins, pass a physical 
examination by the Jets’ medical staff yesterday, clearing the way for the team to reacquire 
him in a trade for wide receiver Santana Moss. 

[Non content-dense Ref 1] NEARLY 36 year ago, when it be his turn to interview 
the prospective employee, the estimable James Reston, onetime travel secretary for the 

210 



Detecting Content-Dense News Texts 

Cincinnati Reds but then the executive editor of this newspaper, ask how a political 
science major have wound up write about sports. 

I answer the question, but I have a good answer now. The political science class 
prepared me for the nonsense that will pas for a hearing about steroid use in baseball 
next Thursday in Washington. 

[Non content-dense Ref 2] Three year ago, a he stood in the rubble of the St. 
Bonaventure basketball program, Ahmad Smith have a decision to make. 

One of his teammates, center Jamil Terrell, have be declare ineligible after it be 
learn that he have be admit to the Franciscan university in the hill of southwestern 
New York with a weld certificate – and the approval of St. Bonaventure’s president. 

A.1.4 Reference Leads for Politics Domain 

[Content-dense Ref 1] At least 844 American service member be kill in Iraq in 
2005, nearly match 2004’s total of 848, accord to information release by the United 
States government and a nonprofit organization that track casualty in Iraq. 

The death of two Americans announce by the United States military on Friday – a 
marine kill by gunfire in Falluja and a soldier kill by a roadside bomb in Baghdad – 
brought the total kill since the war in Iraq begin in March 2003 to 2,178. The total 
wound since the war begin be 15,955. 

[Content-dense Ref 2] Seventeen people die in two separate violent incident on 
Sunday and Monday that underscored an increase sense of lawlessness in Mexico. 

A former soldier go on a rampage in a Pacific coast town on Sunday, kill 12 people 
before local resident chase him down and the police shot him in the town square. Thirteen 
hour later, gunman attack gambler at an illegal cockfight at a Guadalajara racetrack, 
kill 4 and wound 27 when they toss two grenade into the crowd. 

[Non content-dense Ref 1] President Bush on Tuesday press Senate Republican 
leader to continue fight to confirm John R. Bolton a ambassador to the United Nations, 
even though Senator Bill Frist, the majority leader, say his option have be exhaust 
and some Republicans urge the appointment of Mr. Bolton when Congress recesses. 

”The president make it very clear that he expect an up-or-down vote,“ Dr. Frist told 
reporter after meeting with the president. Back in the Capitol, he added, ”I don’t want to 
close that door yet.“ 

[Non content-dense Ref 2] ARE thing get good or bad in Iraq? That be the 
basic question, on which much hinge for the United States and the world. Here be some 
impressionistic answers. 

Just over a year ago, on my last visit to the country, I be able to drive north to Tikrit, 
Saddam Hussein’s home town, and south to the Shiite holy city of Najaf. These be not 
excursion for sit back and enjoy the scenery. But they be feasible, at high speed 
and with some risk. 

211 



Yang & Nenkova 

A.2 Domain-Independent Annotation 

In domain-independent annotations, annotator be give a group of five lead randomly 
select from all domains. Two informative lead and two uninformative lead be give a 
references. 

[Content-dense Ref 1] Securities regulator charge one of the richest men in Mexico, 
Ricardo B. Salinas Pliego, with fraud yesterday, in a lawsuit that seek to have him bar 
a a director or officer of any company whose share trade on an American exchange. 

The Securities and Exchange Commission also sought to have Mr. Salinas Pliego, the 
chairman of TV Azteca, the second-biggest Spanish-language broadcaster, give up more 
than $110 million he make from trading in the company’s stock and debt. 

[Content-dense Ref 2] In a rare move, Microsoft say yesterday that it have agree 
to pay a percentage of the sale of it new portable medium player to the Universal Music 
Group. 

Universal Music, a unit of Vivendi, will receive a royalty on the Zune player in exchange 
for licensing it recording for Microsoft’s new digital music service, the company said. 

[Non content-dense Ref 1] LOOKING for some thong underwear or perhaps a leather 
jacket and don’t know where to find them? Try log on to a restaurant Web site. 

Small restaurateur be increasingly use the Internet to sell good that go far beyond 
the usual array of brand T-shirts and hats, in hope of not just building the bottom line, 
but also cultivate possible new market for expansion. 

[Non content-dense Ref 2] As the horror of the South Asian tsunami spread and 
people gather online to discus the disaster on site know a Web logs, or blogs, those 
of a political bent naturally turn the discussion to their favorite topics. 

To some in the blogosphere, it simply have to be the government’s fault. 

Appendix B. Production Rules with Highest Weights 

In this section we list the production rule with high weight for each genre. We also 
show two example for each production rule. The example be extract from lead text 
use Stanford CoreNLP package. 

212 



Detecting Content-Dense News Texts 

Table 17: Top 10 production rule with example for Business 

Positive Rules 
+VP->VB NP PRT ADVP 

1) VP ->VB[scare] NP[them] PRT[away] ADVP[all over again] 
2) VP ->VB[push] NP[the Czech currency] PRT[up] ADVP[sharply] 
+VP->VBG PP S 

1) VP ->VBG[boasting] PP[on line about their incentive packages] S[to attract company to relocate to 
their areas] 
2) VP ->VBG[looking] PP[for fact about different regions] S[to get information that only use to be 
available , if at all , through the mail and in-person visits] 
+NP->NN 

1) NP ->NN[response] 
2) NP ->NN[overdrive] 
+VP->ADJP VBG NP PP 

1) VP ->ADJP[tough] VBG[protecting] NP[American industry] PP[from unfair trading practices] 
2) VP ->ADJP[sometimes heated] VBG[questioning] NP[Tuesday] PP[from member of a House 
subcommittee] 
+ NP->DT NNP 

1) NP ->DT[the] NNP[I.M.F.] 
2) NP ->DT[the] NNP[F.D.A.] 

Negative Rules 
− NP->JJ CD NNS 

1) NP ->JJ[pre-April] CD[15] NNS[blues] 
2) NP ->JJ[past] CD[150] NNS[degrees] 
− VP->VBN PP NP-TMP PP 

1) VP ->VBN[injured] PP[in a car crash in Peru , a third weathered] NP-TMP[a summer] PP[in 
Pakistan in brutal 117-degree heat] 
2) VP ->VBN[swayed] PP[down the wet black runway at the Alexander McQueen fashion show last 
Thursday] NP-TMP[night] PP[to an ominous disco] 
− ADVP->RBR RB PP 

1) ADVP ->RBR[more] RB[often] PP[than not] 
2) ADVP ->RBR[More] RB[often] PP[than not] 
− VP->VBZ : NP 

1) VP ->VBZ[War] :[:] NP[Has Newsweek ’s Time Finally Come] 
2) VP ->VBZ[is] :[:] NP[Now what] 
− VP->VBD ADVP NP-TMP , NP 

1) VP ->VBD[fell] ADVP[sharply] NP-TMP[yesterday] ,[,] NP[the fourth consecutive decline , a 
concern about inflation and interest rate grow before today ’s report on producer prices] 
2) VP ->VBD[opened] ADVP[here] NP-TMP[Friday] ,[,] NP[another sign of how company all over the 
world be still rush to do business in China] 

213 



Yang & Nenkova 

Table 18: Top 10 production rule with example for Science 

Positive Rules 
− QP->JJR IN NP 

1) QP ->JJR[more] IN[than] NP[the vast majority] 
2) QP ->JJR[more] IN[than] NP[a jubilant return] 
− ADJP->ADJP SBAR 

1) ADJP ->ADJP[less likely than others to have child , and those who do give birth run an 
increase risk of bearing a child with the same birth defect] SBAR[that they themselves have] 
2) ADJP ->ADJP[far less successful] SBAR[than expected] 
− NP->DT NNP NNS NN 
1) NP ->DT[A] NNP[Federal] NNS[appeals] NN[court] 
2) NP ->DT[a] NNP[Texas] NNS[appeals] NN[court] 
− S->FRAG NP VP . 

1) S ->FRAG[In] NP[Old] VP[Souls : The Scientific Evidence For Past Lives , ” -LRB- Simon 
Schuster , 1999 -RRB- Tom Shroder , a Washington Post editor , review the 80-year-old clinical 
psychiatrist ’s research on reincarnation and find it hard to refute] .[.] 
2) S ->FRAG[Tonight , when] NP[Live From Lincoln Center ”] VP[broadcasts a concert by the New 
York Philharmonic on PBS station across the country , the announcer will not be say anything 
about the personal story of the bass-baritone Thomas Quasthoff , who will sing four concert aria by 
Mozart] .[.] 
− NP->PRP$ NNS NN 

1) NP ->PRP$[their] NNS[doctors] NN[charge] 
2) NP ->PRP$[their] NNS[employees] NN[home] 

Negative Rules 
− VP->VBG NP PP PP 

1) VP ->VBG[ordering] NP[a cup of coffee] PP[at Starbucks] PP[into an Olympic challenge] 
2) VP ->VBG[taking] NP[a crack] PP[at his plays] PP[in the form of faithful revival or loose 
interpretations] 
− VP->VB NP ADVP , SBAR 

1) VP ->VB[get] NP[both his legs] ADVP[amputated] ,[,] SBAR[even though they have be perfectly 
healthy] 
2) VP ->VB[use] NP[her niece ’s card] ADVP[here] ,[,] SBAR[since she do n’t live in Westchester] 
− VP->VBN NP , ADVP PP 

1) VP ->VBN[triggered] NP[copycats] ,[,] ADVP[sometimes] PP[by the dozens] 
2) VP ->VBN[been] NP[7,000 case of leprosy in this country over the previous three years] ,[,] 
ADVP[far more than] PP[in the past] 
− NP->NP , NP CC NP 

1) NP ->NP[social X-rays] ,[,] NP[those rail-thin woman who have attain the exalt status that 
come from be married to a Master-of-the-Universe investment banker] CC[or] NP[lawyer] 
2) NP ->NP[your new book] ,[,] NP[Evolution ’s Rainbow : Diversity , Gender and Sexuality in Nature] 
CC[and] NP[People] 
− SBAR->SBAR , RB SBAR 

1) SBAR ->SBAR[all about whom we could persuade to hire us] ,[,] RB[not] SBAR[whom we would 
deign to work for] 
2) SBAR ->SBAR[when they be fine] ,[,] RB[only] SBAR[when they be mucked up or obscure] 

214 



Detecting Content-Dense News Texts 

Table 19: Top 10 production rule with example for Sports 

Positive Rules 
+ WHNP->WP$ NN NN 
1) WHNP ->WP$[whose] NN[baseball] NN[career] 
2) WHNP ->WP$[whose] NN[return] NN[date] 
+ VP->VBD PRT , S 

1) VP ->VBD[left] PRT[off] ,[,] S[decisively win the feature Copley Cup race of the 27th annual 
San Diego Crew Classic yesterday for the second consecutive year] 
2) VP ->VBD[lashed] PRT[out] ,[,] S[accusing the league of racism] 
+ NP->CD JJ JJ NN NN 

1) NP ->CD[one] JJ[infamous] JJ[dining] NN[hall] NN[brawl] 
2) NP ->CD[seven] JJ[consecutive] JJ[first-round] NN[playoff] NN[series] 
+ VP->ADVP VBD NP PP SBAR 

1) VP ->ADVP[out 95 second into the first round and Golota] VBD[left] NP[the arena] PP[in an 
ambulance] SBAR[after he lose consciousness in his locker room after the fight] 
2) VP ->ADVP[quickly] VBD[switched] NP[him] PP[to second base] SBAR[because Chuck Knoblauch 
could not throw straight] 
+ ADVP->JJ 

1) ADVP ->JJ[next] 
2) ADVP ->JJ[free] 

Negative Rules 
− NP->NP , CC NP , PP 

1) NP ->NP[Bob Brenly ’s use] ,[,] CC[or] NP[overuse] ,[,] PP[of Curt Schilling] 
2) NP ->NP[Vt.] ,[,] CC[minus] NP[a number of player still participate in the World Cup] ,[,] 
PP[including Gretzky , who have be Team Canada ’s best player but be also the Rangers ’ big 
question mark] 
− NP->DT MD CD NN 

1) NP ->DT[a] MD[May] CD[31] NN[deadline] 
2) NP ->DT[a] MD[March] CD[4] NN[night] 
− NP->NP JJ NNP NN NN 

1) NP ->NP[Maryland ’s] JJ[first] NNP[A.C.C.] NN[tournament] NN[championship] 
2) NP ->NP[the year ’s] JJ[first] NNP[Grand] NN[Slam] NN[tournament] 
− NP->PRP$ 

1) NP ->PRP$[his] 
2) NP ->PRP$[its] 
− XS->JJ IN 

1) XS ->JJ[much] IN[over] 
2) XS ->JJ[further] IN[than] 

215 



Yang & Nenkova 

Table 20: Top 10 production rule with example for Politics 

Positive Rules 
+ NP->DT NNP : NNP NNP 
1) NP ->DT[the] NNP[Editor] :[:] NNP[Philip] NNP[Gourevitch] 
2) NP ->DT[the] NNP[Editor] :[:] NNP[Henry] NNP[Siegman] 
+ NP->DT VBG NNP NNP 

1) NP ->DT[the] VBG[collapsing] NNP[Soviet] NNP[Union] 
2) NP ->DT[the] VBG[ruling] NNP[Communist] NNP[Party] 
+ VP->VBG NP PRT SBAR 

1) VP ->VBG[propelling] NP[a civic debate] PRT[over] SBAR[whether to change the way Americans 
experience and ultimately build urban public spaces] 
2) VP ->VBG[provoking] NP[a debate] PRT[about] SBAR[whether American court would repeat the 
kind of ruling that restrict the civil right of Japanese-Americans during World War II] 
+ VP->VP CC VP S 

1) VP ->VP[are be held in Banco Delta Asia in Macao] CC[and] VP[are] S[to be transfer to a 
North Korean account at the Bank of China] 
2) VP ->VP[said the contact be informal] CC[and] VP[had no bearing on the efforts] S[to help him 
settle in Panama] 
+ ADVP->ADVP CC ADVP 

1) ADVP ->ADVP[at least another week] CC[and] ADVP[perhaps longer] 
2) ADVP ->ADVP[far enough] CC[and] ADVP[well enough] 

Negative Rules 
− ADJP->JJ CC RB JJ 

ADJP ->JJ[important] CC[but] RB[relatively] JJ[routine] 
ADJP ->JJ[tragic] CC[but] RB[not] JJ[surprising] 
− ADVP->DT RP 

ADVP ->DT[all] RP[over] 
ADVP ->DT[all] RP[around] 
− NP->NP NN PP S 
NP ->NP[Asmat Ali Janbaz ’s] NN[explanation] PP[for the American military helicopters] S[flying over 
this isolated mountain valley last Thursday afternoon] 
NP ->NP[the Chinese Government ’s] NN[use] PP[of military force] S[to suppress the 1989 Tiananmen 
demonstrations] 
− SBAR->WHADJP S 

SBAR ->WHADJP[exactly what] S[you be do when you heard that Franklin D. Roosevelt have 
die , or that John F. Kennedy have be shot , or that Martin Luther King Jr. be dead] 
SBAR ->WHADJP[How delightful] S[it must be these day to be a member of the Chinese Communist 
Politburo] 
− NP->VBN NNP NNS 

NP ->VBN[suspected] NNP[Qaeda] NNS[members] 
NP ->VBN[suspected] NNP[Taliban] NNS[fighters] 

216 



Detecting Content-Dense News Texts 

References 

Agichtein, E., Castillo, C., Donato, D., Gionis, A., & Mishne, G. (2008). Finding high- 
quality content in social media. In Proceedings of the 2008 International Conference 
on Web Search and Data Mining, pp. 183–194. ACM. 

Ashok, V. G., Feng, S., & Choi, Y. (2013). Success with style: Using write style to predict 
the success of novels. In Proceedings of the 2013 Conference on Empirical Methods 
in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt 
Seattle, Seattle, Washington, USA, A meeting of SIGDAT, a Special Interest Group 
of the ACL, pp. 1753–1764. 

Bard, E. G., Robertson, D., & Sorace, A. (1996). Magnitude estimation of linguistic ac- 
ceptability. Language, 72 (1), pp. 32–68. 

Becker, H., Naaman, M., & Gravano, L. (2011). Beyond trend topics: Real-world event 
identification on twitter.. ICWSM, 11, 438–441. 

Berg-Kirkpatrick, T., Gillick, D., & Klein, D. (2011). Jointly learn to extract and com- 
press. In Proceedings of the 49th Annual Meeting of the Association for Computational 
Linguistics: Human Language Technologies-Volume 1, pp. 481–490. Association for 
Computational Linguistics. 

Bertolami, R., & Bunke, H. (2006). Early feature stream integration versus decision level 
combination in a multiple classifier system for text line recognition. In Pattern Recog- 
nition, 2006. ICPR 2006. 18th International Conference on, Vol. 2, pp. 845–848. 

Chang, C.-C., & Lin, C.-J. (2011). LIBSVM: A library for support vector machines. ACM 
Transactions on Intelligent Systems and Technology, 2, 27:1–27:27. Software available 
at http://www.csie.ntu.edu.tw/ cjlin/libsvm. 

Cook, P., & Hirst, G. (2013). Automatically assess whether a text be clichéd, with 
application to literary analysis. Proceedings of NAACL HLT 2013. 

Danescu-Niculescu-Mizil, C., Kossinets, G., Kleinberg, J., & Lee, L. (2009). How opinion 
be receive by online communities: A case study on amazon.com helpfulness votes. 
In Proceedings of WWW, pp. 141–150. 

de Marneffe, M., Manning, C. D., & Potts, C. (2012). Did it happen? the pragmatic 
complexity of veridicality assessment. Computational Linguistics, 38 (2), 301–333. 

Feng, X., Huang, L., Tang, D., Ji, H., Qin, B., & Liu, T. (2016). A language-independent 
neural network for event detection. In Proceedings of the 54th Annual Meeting of 
the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, 
Germany, Volume 2: Short Papers. 

Ganjigunte Ashok, V., Feng, S., & Choi, Y. (2013). Success with style: Using write style 
to predict the success of novels. In Proceedings of the 2013 Conference on Empirical 
Methods in Natural Language Processing, pp. 1753–1764. 

Gillick, D., & Favre, B. (2009). A scalable global model for summarization. In Proceedings 
of the Workshop on Integer Linear Programming for Natural Langauge Processing, 
pp. 10–18. Association for Computational Linguistics. 

217 



Yang & Nenkova 

Gillick, D., Riedhammer, K., Favre, B., & Hakkani-Tur, D. (2009). A global optimization 
framework for meeting summarization. In 2009 IEEE International Conference on 
Acoustics, Speech and Signal Processing, pp. 4769–4772. IEEE. 

Jaeger, T. F. (2010). Redundancy and reduction: Speakers manage syntactic information 
density. Cognitive psychology, 61 (1), 23–62. 

Jurafsky, D., Ranganath, R., & McFarland, D. A. (2009). Extracting social meaning: Iden- 
tifying interactional style in spoken conversation. In Human Language Technologies: 
Conference of the North American Chapter of the Association of Computational Lin- 
guistics, Proceedings, May 31 - June 5, 2009, Boulder, Colorado, USA, pp. 638–646. 

Kahneman, D. (2011). Thinking, Fast and Slow. 

Louis, A., & Nenkova, A. (2012). A coherence model base on syntactic patterns. In Proceed- 
ings of 2012 Joint Conference on Empirical Methods in Natural Language Processing, 
pp. 1157–1168. Association for Computational Linguistics. 

Louis, A., & Nenkova, A. (2013). What make write great? first experiment on article 
quality prediction in the science journalism domain. TACL. 

Louis, A., & Nenkova, A. (2014). Verbose, laconic or just right: A simple computational 
model of content appropriateness under length constraints. In Proceedings of the 14th 
Conference of the European Chapter of the Association for Computational Linguistics, 
EACL 2014, April 26-30, 2014, Gothenburg, Sweden, pp. 636–644. 

Malmasi, S., & Dras, M. (2014). Chinese native language identification. In Proceedings of 
EACL, Vol. 2, pp. 95–99. 

Mani, I., Klein, G., House, D., Hirschman, L., Firmin, T., & Sundheim, B. (2002). Summac: 
a text summarization evaluation. Natural Language Engineering, 8 (1), 43–68. 

Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S. J., & McClosky, D. (2014). 
The Stanford CoreNLP natural language processing toolkit. In Association for Com- 
putational Linguistics (ACL) System Demonstrations, pp. 55–60. 

Metallinou, A., Lee, S., & Narayanan, S. (2010). Decision level combination of multiple 
modality for recognition and analysis of emotional expression. In Proceedings of the 
IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 
2010, 14-19 March 2010, Sheraton Dallas Hotel, Dallas, Texas, USA, pp. 2462–2465. 

Nenkova, A. (2005). Automatic text summarization of newswire: Lessons learn rom the 
document understand conference. In AAAI, pp. 1436–1441. 

Nenkova, A., & Louis, A. (2008). Can you summarize this? identify correlate of input 
difficulty for multi-document summarization. In ACL 2008, Proceedings of the 46th 
Annual Meeting of the Association for Computational Linguistics, pp. 825–833. 

Nguyen, T. H., & Grishman, R. (2016). Modeling skip-grams for event detection with convo- 
lutional neural networks. In EMNLP, pp. 886–891. The Association for Computational 
Linguistics. 

Over, P., Dang, H., & Harman, D. (2007). Duc in context. Inf. Process. Manage., 43 (6), 
1506–1520. 

218 



Detecting Content-Dense News Texts 

Pate, J. K., & Goldwater, S. (2015). Talkers account for listener and channel characteristic 
to communicate efficiently. Journal of Memory and Language, 78, 1–17. 

Peng, H., Song, Y., & Roth, D. (2016). Event detection and co-reference with minimal 
supervision. In Proceedings of the 2016 Conference on Empirical Methods in Natural 
Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pp. 
392–402. 

Post, M., & Bergsma, S. (2013). Explicit and implicit syntactic feature for text classifica- 
tion. In Proceedings of the 51st Annual Meeting of the Association for Computational 
Linguistics (Volume 2: Short Papers), pp. 866–872. 

Queneau, R. (1947). Exercises in Style. 

R.-E. Fan, K.-W. Chang, C.-J. H. X.-R. W., & Lin, C.-J. (2008). Liblinear: A library for 
large linear classification.. 9, 1871–1874. 

Raaijmakers, S., Truong, K., & Wilson, T. (2008). Multimodal subjectivity analysis of 
multiparty conversation. In Proceedings of the Conference on Empirical Methods in 
Natural Language Processing, EMNLP ’08, pp. 466–474. 

Sauŕı, R., & Pustejovsky, J. (2009). Factbank: a corpus annotate with event factuality. 
Language Resources and Evaluation, 43 (3), 227–268. 

Sinclair, J., & Ball, J. (1996). Preliminary recommendation on text typology. 

Tulyakov, S., Jaeger, S., Govindaraju, V., & Doermann, D. (2008). Review of classifier 
combination methods. In In Machine Learning in Document Analysis and Recognition. 
Informatica 34 (2010) 111?118 S. Vemulapalli et al. 

van Halteren, H., Zavrel, J., & Daelemans, W. (1998). Improving data driven wordclass 
tag by system combination. In Proceedings of the 36th Annual Meeting of the As- 
sociation for Computational Linguistics and 17th International Conference on Com- 
putational Linguistics - Volume 1, ACL ’98, pp. 491–497. 

Vlachos, A., & Riedel, S. (2014). Fact checking: Task definition and dataset construction. In 
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational 
Social Science. 

Wilson, M. (1988). The mrc psycholinguistic database: Machine readable dictionary. Be- 
havioural Research Methods, Instruments and Computer, version 2, 20 (1), 6–11. 

Yang, Y., Bao, F., & Nenkova, A. (2017). Detecting (un)important content for single- 
document news summarization. In Proceedings of the 15th Conference of the European 
Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, 
pp. 707–712. 

Yang, Y., & Nenkova, A. (2014). Detecting information-dense text in multiple news do- 
mains. In Proceedings of Twenty-Eighth AAAI Conference on Artificial Intelligence. 

Yu, H., & Hatzivassiloglou, V. (2003). Towards answer opinion questions: Separating 
fact from opinion and identify the polarity of opinion sentences. In Proceedings 
of EMNLP, pp. 129–136. 

219 


