


















































Do Deep Nets Really Need to be Deep? 
*** 

Draft for NIPS 2014 (not camera ready copy) 
*** 

Lei Jimmy Ba 
University of Toronto 

jimmy@psi.utoronto.ca 

Rich Caruana 
Microsoft Research 

rcaruana@microsoft.com 

Abstract 

Currently, deep neural network be the state of the art on problem such a speech 
recognition and computer vision. In this paper we empirically demonstrate that 
shallow feed-forward net can learn the complex function previously learn by 
deep net and achieve accuracy previously only achievable with deep models. 
Moreover, in some case the shallow neural net can learn these deep function 
use the same number of parameter a the original deep models. On the TIMIT 
phoneme recognition and CIFAR-10 image recognition tasks, shallow net can be 
train that perform similarly to complex, well-engineered, deeper convolutional 
architectures. 

1 Introduction 

You be give a training set with 1M label points. When you train a shallow neural net with one 
fully-connected feed-forward hidden layer on this data you obtain 86% accuracy on test data. When 
you train a deeper neural net a in [1] consist of a convolutional layer, pool layer, and three 
fully-connected feed-forward layer on the same data you obtain 91% accuracy on the same test set. 

What be the source of this improvement? Is the 5% increase in accuracy of the deep net over the 
shallow net because: a) the deep net have more parameters; b) the deep net can learn more complex 
function give the same number of parameters; c) the deep net have good bias and learns more 
interesting/useful function (e.g., because the deep net be deeper it learns hierarchical representation 
[5]); d) net without convolution can’t easily learn what net with convolution can learn; e) current 
learn algorithm and regularization method work good with deep architecture than shallow 
architectures[8]; f) all or some of the above; g) none of the above? 

There have be attempt to answer the question above. It have be show that deep net couple 
with unsupervised layer-by-layer pre-training technique[10] [19] work well. In [8], the author show 
that depth combine with pre-training provide a good prior for model weights, thus improve gen- 
eralization. There be well-known early theoretical work on the representational capacity of neural 
nets. For example, it be prove that a network with a large enough single hidden layer of sigmoid 
unit can approximate any decision boundary[4]. Empirical work, however, show that it be difficult 
to train shallow net to be a accurate a deep nets. For vision tasks, a recent study on deep con- 
volutional net suggests that deeper model be prefer under a parameter budget [7]. In [5], the 
author train shallow net on SIFT feature to classify a large-scale ImageNet dataset and show 
that it be challenge to train large shallow net to learn complex functions. And in [17], the author 
show that deeper model be more competitive than shallow model in speech acoustic modeling. 

1 

ar 
X 

iv 
:1 

31 
2. 

61 
84 

v7 
[ 

c 
.L 

G 
] 

1 
1 

O 
ct 

2 
01 

4 



In this paper we provide empirical evidence that shallow net be capable of learn the same 
function a deep nets, and in some case with the same number of parameter a the deep nets. We 
do this by first training a state-of-the-art deep model, and then training a shallow model to mimic the 
deep model. The mimic model be train use the model compression scheme described in the next 
section. Remarkably, with model compression we be able to train shallow net to be a accurate 
a some deep models, even though we be not able to train these shallow net to be a accurate a 
the deep net when the shallow net be train directly on the original label training data. If a 
shallow net with the same number of parameter a a deep net can learn to mimic a deep net with 
high fidelity, then it be clear that the function learn by that deep net do not really have to be deep. 

2 Training Shallow Nets to Mimic Deep Nets 

2.1 Model Compression 

The main idea behind model compression be to train a compact model to approximate the function 
learn by a larger, more complex model. For example, in [3], a single neural net of modest size 
could be train to mimic a much large ensemble of model — although the small neural net 
contain 1000 time few parameters, often they be just a accurate a the ensemble they be 
train to mimic. Model compression work by passing unlabeled data through the large, accurate 
model to collect the score produce by that model. This synthetically label data be then use to 
train the small mimic model. The mimic model be not train on the original labels—it be train 
to learn the function that be learn by the large model. If the compress model learns to mimic 
the large model perfectly it make exactly the same prediction and mistake a the complex model. 

Surprisingly, often it be not (yet) possible to train a small neural net on the original training data to be 
a accurate a the complex model, nor a accurate a the mimic model. Compression demonstrates 
that a small neural net could, in principle, learn the more accurate function, but current learn 
algorithm be unable to train a model with that accuracy from the original training data; instead, we 
must train the complex intermediate model first and then train the neural net to mimic it. Clearly, 
when it be possible to mimic the function learn by a complex model with a small net, the function 
learn by the complex model wasn’t truly too complex to be learn by a small net. This suggests 
to u that the complexity of a learn model, and the size of the representation best use to learn that 
model, be different things. In this paper we apply model compression to train shallow neural net 
to mimic deeper neural nets, thereby demonstrate that deep neural net may not need to be deep. 

2.2 Mimic Learning via Regressing Logit with L2 Loss 

On both TIMIT and CIFAR-10 we train shallow mimic net use data label by either a deep 
net, or an ensemble of deep nets, train on the original TIMIT or CIFAR-10 training data. The 
deep model be train in the usual way use softmax output and cross-entropy cost function. The 
shallow mimic models, however, instead of be train with cross-entropy on the 183 p value 
where pk = ezk/ 

∑ 
j e 

zj output by the softmax layer from the deep model, be train directly on 
the 183 log probability value z, also call logit, before the softmax activation. 

Training on these logit value make learn easy for the shallow net by place emphasis on 
all prediction targets. Because the logits capture the logarithm relationship between the proba- 
bility predictions, a student model train on logits have to learn all of the additional fine detailed 
relationship between label that be not obvious in the probability space yet be learn by the 
teacher model. For example, assume there be three target that the teacher predicts with probability 
[2e − 9, 4e − 5, 0.9999]. If we use these probability a prediction target directly to minimize a 
cross entropy loss function, the student will focus on the third target and easily ignore the first and 
second target. Alternatively, one can extract the logit prediction from the teacher model and obtain 
our new target [10, 20, 30]. The student will learn to regress the third target, yet it still learns the 
first and second target along with their relative difference. The logit value provide richer informa- 
tion to student to mimic the exact behaviour of a teach model. Moreover, consider a second training 
case where the teacher predicts logits [−10, 0, 10]. After softmax, these logits yield the same pre- 
dicted probability a [10, 20, 30], yet clearly the teacher have learn internally to model these two 
case very differently. By training the student model on the logits directly, the student be good able 

2 



to learn the internal model learn by the teacher, without suffer from the information loss that 
occurs after passing through the logits to probability space. 

We formulate the SNN-MIMIC learn objective function a a regression problem give training 
data {(x(1), z(1)),...,(x(T ), z(T )) }: 

L(W,β) = 1 
2T 

∑ 
t 

||g(x(t);W,β)− z(t)||22, (1) 

where, W be the weight matrix between input feature x and hidden layer, β be the weight from 
hidden to output units, g(x(t);W,β) = βf(Wx(t)) be the model prediction on the tth training data 
point and f(·) be the non-linear activation of the hidden units. The parameter W and β be update 
use standard error back-propagation algorithm and stochastic gradient descent with momentum. 

We have also experiment with other different mimic loss function, such a minimize the KL 
divergence KL(pteacher‖pstudent) cost function and L2 loss on the probability. Logits regression out- 
performs all the other loss function and be one of the key technique for obtain the result in the 
rest of this paper. We found that normalize the logits from the teacher model, by subtract the 
mean and divide the standard deviation of each target across the training set, can improve the L2 
loss slightly during training. Normalization be not crucial for obtain a good student model. 

2.3 Speeding-up Mimic Learning by Introducing a Linear Layer 

To match the number of parameter in a deep net, a shallow net have to have more non-linear hidden 
unit in a single layer to produce a large weight matrix W . When training a large shallow neural 
network with many hidden units, we find it be very slow to learn the large number of parameter in the 
weight matrix between input and hidden layer of size O(HD), where D be input feature dimension 
and H be the number of hidden units. Because there be many highly correlate parameter in this 
large weight matrix gradient descent converges slowly. We also notice that during learning, shallow 
net spend most of the computation in the costly matrix multiplication of the input data vector and 
large weight matrix. The shallow net eventually learn accurate mimic functions, but training to 
convergence be very slow (multiple weeks) even with a GPU. 

We found that introduce a bottleneck linear layer with k linear hidden unit between the input 
and the non-linear hidden layer speed up learn dramatically: we can factorize the weight matrix 
W ∈ RH×D into the product of two low rank matrices, U ∈ RH×k and V ∈ Rk×D, where 
k << D,H . The new cost function can be write as: 

L(U, V, β) = 1 
2T 

∑ 
t 

||βf(UV x(t))− z(t)||22 (2) 

The weight U and V can be learnt by back-propagating through the linear layer. This re- 
parameterization of weight matrix W not only increase the convergence rate of the shallow mimic 
nets, but also reduces memory space from O(HD) to O(k(H +D)). 

Factorizing weight matrix have be previously explore in [16] and [20]. While these prior work 
focus on use matrix factorization in the last output layer, our method be apply between input and 
hidden layer to improve the convergence speed during training. 

The reduce memory usage enables u to train large shallow model that be previously infeasible 
due to excessive memory usage. The linear bottle neck can only reduce the representational power 
of the network, and it can always be absorbed into a signle weight matrix W . 

3 TIMIT Phoneme Recognition 

The TIMIT speech corpus have 462 speaker in the training set. There be a separate development set 
for cross-validation include 50 speakers, and a final test set with 24 speakers. The raw waveform 
audio data be pre-processed use 25ms Hamming window shift by 10ms to extract Fourier- 
transform-based filter-banks with 40 coefficient (plus energy) distribute on a mel-scale, together 

3 



with their first and second temporal derivatives. We include +/- 7 nearby frame to formulate the 
final 1845 dimension input vector. The data input feature be normalize by subtract the mean 
and divide by the standard deviation on each dimension. All 61 phoneme label be represent in 
tri-state, i.e., 3 state for each of the 61 phonemes, yield target label vector with 183 dimension 
for training. At decode time these be mapped to 39 class a in [13] for scoring. 

3.1 Deep Learning on TIMIT 

Deep learn be first successfully apply to speech recognition in [14]. We follow the same 
framework and train two deep model on TIMIT, DNN and CNN. DNN be a deep neural net con- 
sisting of three fully-connected feedforward hidden layer consist of 2000 rectify linear unit 
(ReLU) [15] per layer. CNN be a deep neural net consist of a convolutional layer and max-pooling 
layer follow by three hidden layer contain 2000 ReLU unit [2]. The CNN be train use 
the same convolutional architecture a in [6]. We also form an ensemble of nine CNN models, 
ECNN. 

The accuracy of DNN, CNN, and ECNN on the final test set be show in Table 1. The error rate 
of the convolutional deep net (CNN) be about 2.1% good than the deep net (DNN). The table also 
show the accuracy of shallow neural net with 8000, 50,000, and 400,000 hidden unit (SNN-8k, 
SNN-50k, and SNN-400k) train on the original training data. Despite have up to 10X a many 
parameter a DNN, CNN and ECNN, the shallow model be 1.4% to 2% less accurate than the 
DNN, 3.5% to 4.1% less accurate than the CNN, and 4.5% to 5.1% less accurate than the ECNN. 

3.2 Learning to Mimic an Ensemble of Deep Convolutional TIMIT Models 

The most accurate single model we train on TIMIT be the deep convolutional architecture in [6]. 
Because we have no unlabeled data from the TIMIT distribution, we be force to use the same 1.1M 
point in the train set a unlabeled data for compression by throw away their labels.1 Re-using 
the train set reduces the accuracy of the mimic models, increase the gap between the teacher and 
mimic model on test data: model compression work best when the unlabeled set be much large 
than the train set, and when the unlabeled sample do not fall on train point where the teacher model 
be more likely to have overfit. To reduce the impact of the gap cause by perform compression 
with the original train set, we train the student model to mimic a more accurate ensemble of deep 
convolutional models. 

We be able to train a more accurate model on TIMIT by form an ensemble of 9 deep, convo- 
lutional neural nets, each train with somewhat different train sets, and with architecture with 
different kernel size in the convolutional layers. We use this very accurate model, ECNN, a the 
teacher model to label the data use to train the shallow mimic nets. As described in Section 2.2, 
the logits (log probability of the predict values) from each CNN in the ECNN model be average 
and the average logits be use a final regression target to train the mimic SNNs. 

We train shallow mimic net with 8k (SNN-MIMIC-8k) and 400k (SNN-MIMIC-400k) hidden 
unit on the re-labeled 1.1M training points. As described in Section 2.3, both mimic model have 
250 linear unit between the input and non-linear hidden layer to speed up learn — preliminary 
experiment suggest that for TIMIT there be little benefit from use more than 250 linear units. 

3.3 Compression Results For TIMIT 

The bottom of Table 1 show the accuracy of shallow mimic net with 8000 ReLUs and 400,000 
ReLUs (SNN-MIMIC-8k and -400k) train with model compression to mimic the ECNN. Surpris- 
ingly, shallow net be able to perform a well a their deep counter-parts when train with model 
compression to mimic a more accurate model. A neural net with one hidden layer (SNN-MIMIC-8k) 
can be train to perform a well a a DNN with a similar number of parameters. Furthermore, if we 
increase the number of hidden unit in the shallow net from 8k to 400k (the large we could train), 
we see that a neural net with one hidden layer (SNN-MIMIC-400k) can be train to perform com- 
parably to a CNN even though the SNN-MIMIC-400k net have no convolutional or pool layers. 

1That SNNs can be train to be a accurate a DNNs use only the original training data data highlight 
that it should be possible to train accurate SNNs on the original train data give good learn algorithms. 

4 



Architecture # Param. # Hidden unit PER 

SNN-8k 8k + dropout ∼12M ∼8k 23.1%trained on original data 
SNN-50k 50k + dropout ∼100M ∼50k 23.0%trained on original data 
SNN-400k 250L-400k + dropout ∼180M ∼400k 23.6%trained on original data 
DNN 2k-2k-2k + dropout ∼12M ∼6k 21.9%trained on original data 
CNN c-p-2k-2k-2k + dropout ∼13M ∼10k 19.5%trained on original data 
ECNN ensemble of 9 CNNs ∼125M ∼90k 18.5% 

SNN-MIMIC-8k 250L-8k ∼12M ∼8k 21.6%no convolution or pool layer 
SNN-MIMIC-400k 250L-400k ∼180M ∼400k 20.0%no convolution or pool layer 

Table 1: Comparison of shallow and deep models: phone error rate (PER) on TIMIT core test set. 

76 

77 

78 

79 

80 

81 

82 

83 

1 10 100 

A 
c 
c 
u 
ra 

c 
y 
o 

n 
T 

IM 
IT 

D 
e 
v 
S 

e 
t 

Number of Parameters (millions) 

ShallowNet 
DeepNet 

ShallowMimicNet 
Convolutional Net 
Ensemble of CNNs 

75 

76 

77 

78 

79 

80 

81 

82 

1 10 100 

A 
c 
c 
u 
ra 

c 
y 
o 

n 
T 

IM 
IT 

T 
e 
s 
t 

S 
e 
t 

Number of Parameters (millions) 

ShallowNet 
DeepNet 

ShallowMimicNet 
Convolutional Net 
Ensemble of CNNs 

Figure 1: Accuracy of SNNs, DNNs, and Mimic SNNs vs. # of parameter on TIMIT Dev (left) and 
Test (right) sets. Accuracy of the CNN and target ECNN be show a horizontal line for reference. 

This be interest because it suggests that a large single hidden¡ layer without a topology custom 
design for the problem be able to reach the performance of a deep convolutional neural net that 
be carefully engineer with prior structure and weight share without any increase in the number 
of training examples, even though the same architecture train on the original data could not. 

Figure 1 show the accuracy of shallow net and deep net train on the original TIMIT 1.1M data, 
and shallow mimic net train on the ECNN targets, a a function of the number of parameter in 
the models. The accuracy of the CNN and the teacher ECNN be show a horizontal line at the top 
of the figures. When the number of parameter be small (about 1 million), the SNN, DNN, and SNN- 
MIMIC model all have similar accuracy. As the size of the hidden layer increase and the number 
of parameter increases, the accuracy of a shallow model train on the original data begin to lag 
behind. The accuracy of the shallow mimic model, however, match the accuracy of the DNN until 
about 4 million parameters, when the DNN begin to fall behind the mimic. The DNN asymptote 
at around 10M parameters, while the shallow mimic continue to increase in accuracy. Eventually 
the mimic asymptote at around 100M parameter to an accuracy comparable to that of the CNN. 
The shallow mimic never achieves the accuracy of the ECNN it be try to mimic (because there 
be not enough unlabeled data), but it be able to match or exceed the accuracy of deep net (DNNs) 
have the same number of parameter train on the original data. 

5 



4 Object Recognition: CIFAR-10 

To verify that the result on TIMIT generalize to other learn problem and task domains, we ran 
similar experiment on the CIFAR-10 Object Recognition Task[12]. CIFAR-10 consists of a set 
of natural image from 10 different object classes: airplane, automobile, bird, cat, deer, dog, frog, 
horse, ship, truck. The dataset be a label subset of the 80 million tiny image dataset[18] and be 
divide into 50,000 train and 10,000 test images. Each image be 32x32 pixel in 3 color channels, 
yield input vector with 3072 dimensions. We prepared the data by subtract the mean and 
divide the standard deviation of each image vector to perform global contrast normalization. We 
then apply ZCA whiten to the normalize images. This pre-processing be the same use in [9]. 

4.1 Learning to Mimic a Deep Convolutional Neural Network 

Deep learn currently achieves state-of-the-art accuracy on many computer vision problems. The 
key to this success be deep convolutional net with many alternate layer of convolutional, pool 
and non-linear units. Recent advance such a dropout be also important to prevent over-fitting in 
these deep nets. 

We follow the same approach a with TIMIT: An ensemble of deep CNN model be use to label 
CIFAR-10 image for model compression. The logit prediction from this teacher model be use 
a regression target to train a mimic shallow neural net (SNN). CIFAR-10 image have a high 
dimension than TIMIT (3072 vs. 1845), but the size of the CIFAR-10 training set be only 50,000 
compare to 1.1 million example for TIMIT. Fortunately, unlike TIMIT, in CIFAR-10 we have 
access to unlabeled data from a similar distribution by use the super set of CIFAR-10: the 80 
million tiny image dataset. We add the first 1 million image from the 80 million set to the original 
50,000 CIFAR-10 training image to create a 1.05M mimic training (transfer) set. 

CIFAR-10 image be raw pixel for object view from many different angle and positions, 
whereas TIMIT feature be human-designed filter-bank features. In preliminary experiment we 
observe that non-convolutional net do not perform well on CIFAR-10 no matter what their depth. 
Instead of raw pixels, the author in [5] train their shallow model on the SIFT features. Similarly, 
[7] use a base convolution and pool layer to study different deep architectures. We follow the 
approach in [7] to allow our shallow model to benefit from convolution while keep the model 
a shallow a possible, and introduce a single layer of convolution and pool in our shallow mimic 
model to act a a feature extractor to create invariance to small translation in the pixel domain. The 
SNN-MIMIC model for CIFAR-10 thus consist of a convolution and max pool layer follow 
by fully connect 1200 linear unit and 30k non-linear units. As before, the linear unit be there 
only to speed learning; they do not increase the model’s representational power and can be absorbed 
into the weight in the non-linear layer after learning. 

Results on CIFAR-10 be consistent with those from TIMIT. Table 2 show result for the shallow 
mimic models, and for much-deeper convolutional nets. The shallow mimic net train to mimic 
the teacher CNN (SNN-CNN-MIMIC-30k) achieves accuracy comparable to CNNs with multiple 
convolutional and pool layers. And by training the shallow model to mimic the ensemble of 
CNNs (SNN-ECNN-MIMIC-30k), accuracy be improve an additional 0.9%. The mimic model 
be able to achieve accuracy previously unseen on CIFAR-10 with model with so few layers. 
Although the deep convolution net have more hidden unit than the shallow mimic models, because 
of weight sharing, the deeper net with multiple convolution layer have few parameter than the 
shallow fully-connected mimic models. Still, it be surprising to see how accurate the shallow mimic 
model are, and that their performance continue to improve a the performance of the teacher model 
improves (see further discussion of this in Section 5.2). 

5 Discussion 

5.1 Why Mimic Models Can Be More Accurate than Training on Original Labels 

It may be surprising that model train on the prediction target take from other model can be 
more accurate than model train on the original labels. There be a variety of reason why this can 
happen: 

6 



Architecture # Param. # Hidden unit Err. 

DNN 2000-2000 + dropout ∼10M 4k 57.8% 

SNN-30k 128c-p-1200L-30k ∼70M ∼190k 21.8%+ dropout input&hidden 
single-layer 4000c-p ∼125M ∼3.7B 18.4%feature extraction follow by SVM 
CNN[11] 64c-p-64c-p-64c-p-16lc ∼10k ∼110k 15.6%(no augmentation) + dropout on lc 
CNN[21] 64c-p-64c-p-128c-p-fc 

∼56k ∼120k 15.13%(no augmentation) + dropout on fc 
and stochastic pool 

teacher CNN 128c-p-128c-p-128c-p-1000fc 
∼35k ∼210k 12.0%(no augmentation) + dropout on fc 

and stochastic pool 
ECNN ensemble of 4 CNNs ∼140k ∼840k 11.0%(no augmentation) 
SNN-CNN-MIMIC-30k 64c-p-1200L-30k ∼54M ∼110k 15.4%trained on a single CNN with no regularization 
SNN-CNN-MIMIC-30k 128c-p-1200L-30k ∼70M ∼190k 15.1%trained on a single CNN with no regularization 
SNN-ECNN-MIMIC-30k 128c-p-1200L-30k ∼70M ∼190k 14.2%trained on ensemble with no regularization 

Table 2: Comparison of shallow and deep models: classification error rate on CIFAR-10. Key: c, 
convolution layer; p, pool layer; lc, locally connect layer; fc, fully connect layer 

• if some label have errors, the teacher model may eliminate some of these error (i.e., cen- 
sor the data), thus make learn easy for the student: on TIMIT, there be mislabeled 
frame introduce by the HMM forced-alignment procedure. 

• if there be region in the p(y|X) that be difficult to learn give the features, sample den- 
sity, and function complexity, the teacher may provide simpler, soft label to the student. 
The complexity in the data set have be wash away by filter the target through the 
teacher model. 

• learn from the original hard 0/1 label can be more difficult than learn from the 
teacher’s conditional probabilities: on TIMIT only one of 183 output be non-zero on each 
training case, but the mimic model see non-zero target for most output on most training 
cases. Moreover, the teacher model can spread the uncertainty over multiple output when 
it be not confident of it prediction. Yet, the teacher model can concentrate the probability 
mass on one (or few) output on easy cases. The uncertainty from the teacher model be far 
more informative to guide the student model than the original 0/1 labels. This benefit 
appear to be further enhance by training on logits. 

0 2 4 6 8 10 12 14 
Number of Epoches 

74.0 

74.5 

75.0 

75.5 

76.0 

76.5 

77.0 

77.5 

P 
h 
o 
n 
e 
R 

e 
c 
o 
g 
n 
it 

io 
n 
A 

c 
c 
u 
ra 

c 
y 

SNN-8k 

SNN-8k + dropout 

SNN-MIMIC-8k 

Figure 2: Training shallow mimic model prevents 
overfitting. 

The mechanism above can be see a form 
of regularization that help prevent overfitting in 
the student model. Shallow model train on 
the original target be more prone to overfit- 
ting than deep models—they begin to overfit 
before learn the accurate function learn 
by deeper model even with dropout (see Fig- 
ure 2). If we have more effective regularization 
method for shallow models, some of the per- 
formance gap between shallow and deep mod- 
el might disappear. Model compression ap- 
pear to be a form of regularization that be ef- 
fective at reduce this gap. 

7 



5.2 The Capacity and Representational 
Power of Shallow Models 

78 

79 

80 

81 

82 

83 

78 79 80 81 82 83 

A 
cc 

u 
ra 

cy 
o 

f 
M 

im 
ic 

M 
od 

el 
o 

n 
D 

ev 
S 

et 

Accuracy of Teacher Model on Dev Set 

Mimic with 8k Non-Linear Units 
Mimic with 160k Non-Linear Units 

y=x (no student-teacher gap) 

Figure 3: Accuracy of student model continue to 
improve a accuracy of teacher model improves. 

Figure 3 show result of an experiment with 
TIMIT where we train shallow mimic mod- 
el of two size (SNN-MIMIC-8k and SNN- 
MIMIC-160k) on teacher model of different 
accuracies. The two shallow mimic model be 
train on the same number of data points. The 
only difference between them be the size of the 
hidden layer. The x-axis show the accuracy of 
the teacher model, and the y-axis be the accu- 
racy of the mimic models. Lines parallel to the 
diagonal suggest that increase in the accuracy 
of the teacher model yield similar increase in 
the accuracy of the mimic models. Although 
the data do not fall perfectly on a diagonal, 
there be strong evidence that the accuracy of the 
mimic model continue to increase a the ac- 
curacy of the teacher model improves, suggest- 
ing that the mimic model be not (yet) run 
out of capacity. When training on the same targets, SNN-MIMIC-8k always perform bad than 
SNN-MIMIC-160K that have 10 time more parameters. Although there be a consistent performance 
gap between the two model due to the difference in size, the small shallow model be eventu- 
ally able to achieve a performance comparable to the large shallow net by learn from a good 
teacher, and the accuracy of both model continue to increase a teacher accuracy increases. This 
suggests that shallow model with a number of parameter comparable to deep model be likely 
capable of learn even more accurate function if a more accurate teacher and/or more unlabeled 
data become available. Similarly, on CIFAR-10 we saw that increase the accuracy of the teacher 
model by form an ensemble of deep CNNs yield commensurate increase in the accuracy of the 
student model. We see little evidence that shallow model have limited capacity or representational 
power. Instead, the main limitation appear to be the learn and regularization procedure use to 
train the shallow models. 

5.3 Parallel Distributed Processing vs. Deep Sequential Processing 

Our result show that shallow net can be competitive with deep model on speech and vision tasks. 
One potential benefit of shallow net be that training them scale well with the modern parallel 
hardware. In our experiment the deep model usually require 8–12 hour to train on Nvidia GTX 
580 GPUs to reach the state-of-the-art performance on TIMIT and CIFAR-10 datasets. Although 
some of the shallow mimic model have more parameter than the deep models, the shallow model 
train much faster and reach similar accuracy in only 1–2 hours. 

Also, give parallel computational resources, at run-time shallow model can finish computation in 
2 or 3 cycle for a give input, whereas a deep architecture have to make sequential inference through 
each of it layers, expend a number of cycle proportional to the depth of the model. This benefit 
can be important in on-line inference setting where data parallelization be not a easy to achieve 
a it be in the batch inference setting. For real-time application such a surveillance or real-time 
speech translation, a model that responds in few cycle can be beneficial. 

6 Future Work 

The tiny image dataset contains 80 million images. We be currently investigate if by label 
these 80M image with a teacher, it be possible to train shallow model with no convolutional or 
pool layer to mimic deep convolutional models. 

This paper focus on training the shallowest-possible model to mimic deep model in order to 
good understand the importance of model depth in learning. As suggest in Section 5.3, there be 
practical application of this work a well: student model of small-to-medium size and depth can be 
train to mimic very large, high accuracy deep models, and ensemble of deep models, thus yield 

8 



good accuracy with reduce runtime cost than be currently achievable without model compression. 
This approach allows one to adjust flexibly the trade-off between accuracy and computational cost. 

In this paper we be able to demonstrate empirically that shallow model can, at least in principle, 
learn more accurate function without a large increase in the number of parameters. The algorithm 
we use to do this—training the shallow model to mimic a more accurate deep model, however, 
be awkward. It depends on the availability of either a large unlabeled data set (to reduce the gap 
between teacher and mimic model) or a teacher model of very high accuracy, or both. Developing 
algorithm to train shallow model of high accuracy directly from the original data without go 
through the intermediate teacher model would, if possible, be a significant contribution. 

7 Conclusions 

We demonstrate empirically that shallow neural net can be train to achieve performance pre- 
viously achievable only by deep model on the TIMIT phoneme recognition and CIFAR-10 image 
recognition tasks. Single-layer fully-connected feedforward net train to mimic deep model can 
perform similarly to well-engineered complex deep convolutional architectures. The result suggest 
that the strength of deep learn may arise in part from a good match between deep architecture 
and current training procedures, and that it may be possible to devise good learn algorithm to 
train more accurate shallow feed-forward nets. For a give number of parameters, depth may make 
learn easier, but may not always be essential. 

Acknowledgements We thank Li Deng for generous help with TIMIT, Li Deng and Ossama Abdel- 
Hamid for code for the TIMIT convolutional model, Chris Burges, Li Deng, Ran Gilad-Bachrach, 
Tapas Kanungo and John Platt for discussion that significantly improve this work, and Mike Ault- 
man for help with the GPU cluster. 

References 
[1] Ossama Abdel-Hamid, Abdel-rahman Mohamed, Hui Jiang, and Gerald Penn. Applying con- 

volutional neural network concept to hybrid nn-hmm model for speech recognition. In 
Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on, 
page 4277–4280. IEEE, 2012. 

[2] Ossama Abdel-Hamid, Li Deng, and Dong Yu. Exploring convolutional neural network struc- 
tures and optimization technique for speech recognition. Interspeech 2013, 2013. 

[3] Cristian Bucilu, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Pro- 
ceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data 
mining, page 535–541. ACM, 2006. 

[4] George Cybenko. Approximation by superposition of a sigmoidal function. Mathematics of 
control, signal and systems, 2(4):303–314, 1989. 

[5] Yann N Dauphin and Yoshua Bengio. Big neural network waste capacity. arXiv preprint 
arXiv:1301.3583, 2013. 

[6] Li Deng, Jinyu Li, Jui-Ting Huang, Kaisheng Yao, Dong Yu, Frank Seide, Michael Seltzer, 
Geoff Zweig, Xiaodong He, Jason Williams, et al. Recent advance in deep learn for speech 
research at microsoft. ICASSP 2013, 2013. 

[7] David Eigen, Jason Rolfe, Rob Fergus, and Yann LeCun. Understanding deep architecture 
use a recursive convolutional network. arXiv preprint arXiv:1312.1847, 2013. 

[8] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, 
and Samy Bengio. Why do unsupervised pre-training help deep learning? The Journal of 
Machine Learning Research, 11:625–660, 2010. 

[9] Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. 
Maxout networks. In Proceedings of The 30th International Conference on Machine Learning, 
page 1319–1327, 2013. 

[10] G.E. Hinton and R.R. Salakhutdinov. Reducing the dimensionality of data with neural net- 
works. Science, 313(5786):504–507, 2006. 

9 



[11] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Im- 
prove neural network by prevent co-adaptation of feature detectors. arXiv preprint 
arXiv:1207.0580, 2012. 

[12] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layer of feature from tiny images. 
Computer Science Department, University of Toronto, Tech. Rep, 2009. 

[13] K-F Lee and H-W Hon. Speaker-independent phone recognition use hidden markov models. 
Acoustics, Speech and Signal Processing, IEEE Transactions on, 37(11):1641–1648, 1989. 

[14] Abdel-rahman Mohamed, George E Dahl, and Geoffrey Hinton. Acoustic model use 
deep belief networks. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1): 
14–22, 2012. 

[15] V. Nair and G.E. Hinton. Rectified linear unit improve restrict boltzmann machines. In Proc. 
27th International Conference on Machine Learning, page 807–814. Omnipress Madison, 
WI, 2010. 

[16] Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. 
Low-rank matrix factorization for deep neural network training with high-dimensional out- 
put targets. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International 
Conference on, page 6655–6659. IEEE, 2013. 

[17] Frank Seide, Gang Li, and Dong Yu. Conversational speech transcription use context- 
dependent deep neural networks. In Interspeech, page 437–440, 2011. 

[18] Antonio Torralba, Robert Fergus, and William T Freeman. 80 million tiny images: A large data 
set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence, 
IEEE Transactions on, 30(11):1958–1970, 2008. 

[19] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.A. Manzagol. Stacked denoising autoen- 
coders: Learning useful representation in a deep network with a local denoising criterion. The 
Journal of Machine Learning Research, 11:3371–3408, 2010. 

[20] Jian Xue, Jinyu Li, and Yifan Gong. Restructuring of deep neural network acoustic model 
with singular value decomposition. Proc. Interspeech, Lyon, France, 2013. 

[21] Matthew D Zeiler and Rob Fergus. Stochastic pool for regularization of deep convolutional 
neural networks. arXiv preprint arXiv:1301.3557, 2013. 

10 


1 Introduction 
2 Training Shallow Nets to Mimic Deep Nets 
2.1 Model Compression 
2.2 Mimic Learning via Regressing Logit with L2 Loss 
2.3 Speeding-up Mimic Learning by Introducing a Linear Layer 

3 TIMIT Phoneme Recognition 
3.1 Deep Learning on TIMIT 
3.2 Learning to Mimic an Ensemble of Deep Convolutional TIMIT Models 
3.3 Compression Results For TIMIT 

4 Object Recognition: CIFAR-10 
4.1 Learning to Mimic a Deep Convolutional Neural Network 

5 Discussion 
5.1 Why Mimic Models Can Be More Accurate than Training on Original Labels 
5.2 The Capacity and Representational Power of Shallow Models 
5.3 Parallel Distributed Processing vs. Deep Sequential Processing 

6 Future Work 
7 Conclusions 

