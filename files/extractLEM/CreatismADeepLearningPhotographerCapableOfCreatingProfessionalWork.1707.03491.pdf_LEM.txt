


















































Creatism: A deep-learning photographer capable of create professional work 

Hui Fang 
hfang@google.com 

Google 

Meng Zhang 
zhangmeng@google.com 

Google 

Abstract 

Machine-learning excels in many area with well- 
define goals. However, a clear goal be usually not 
available in art forms, such a photography. The 
success of a photograph be measure by it aesthetic 
value, a very subjective concept. This add to the 
challenge for a machine learn approach. 
We introduce Creatism, a deep-learning system for 
artistic content creation. In our system, we break 
down aesthetic into multiple aspects, each can be 
learn individually from a share dataset of pro- 
fessional examples. Each aspect corresponds to an 
image operation that can be optimize efficiently. 
A novel edit tool, dramatic mask, be introduce 
a one operation that improves dramatic light for 
a photo. Our training do not require a dataset with 
before/after image pairs, or any additional label to 
indicate different aspect in aesthetics. 
Using our system, we mimic the workflow of 
a landscape photographer, from frame for the 
best composition to carry out various post- 
processing operations. The environment for our 
virtual photographer be simulated by a collection of 
panorama image from Google Street View. We de- 
sign a “Turing-test”-like experiment to objectively 
measure quality of it creations, where professional 
photographer rate a mixture of photograph from 
different source blindly. Experiments show that a 
portion of our robot’s creation can be confuse with 
professional work. 

1 Introduction 
Great progress have be make in both camera hardware and 
computational photography, such that a modern cell phone 
can take technically solid photographs, in term of exposure 
level, noise level, pixel sharpness, color accuracy, etc. How- 
ever, a good photo should be not only technically solid, but 
also aesthetically pleasing. 

Aesthetics be vague and subjective, a metric hard to define 
scientifically. Multiple research exists [Murray et al., 2012] 
[Kong et al., 2016] to collect dataset to define aesthetic qual- 
ity. Generating image towards top aesthetic quality be an 
even harder problem. A naive approach use a single aes- 
thetic prediction be insufficient to capture different aspect in 
aesthetics, a we will show in experiment. 

In this paper, we introduce Creatism, a deep-learning sys- 
tem for artistic content creation. Here aesthetic be treat not 
a a single quantity, but a a combination of different aspects. 
Each aspect be learn independently from professional ex- 
amples, and be couple with an image operation that can mod- 
ify this aspect. By make image operation semi-orthogonal, 
we can efficiently optimize a photo one aspect at a time. 

Another advantage of couple an aesthetic aspect with an 
image operation be that we can simulate “negative” example 
tailor towards that aspect. This get rid of the need to col- 
lect before/after image pair from professional to indicate 
how to improve each aspect. In this project the dataset for aes- 
thetic aspect training be a collection of professional-quality 
photo with no additional labels. 

In addition to learn aesthetic aspect with know image 
operations, we show that it be also possible to define new op- 
erations from this unlabeled dataset. By combine a set of 
exist image filter to generate negative examples, we train 
a new image operation, dramatic mask, that can enhance dra- 
matic light in photos. 

One stand problem for current enhancement work be 
quality metric, especially on high end of aesthetics. A user- 
study with image comparison tell which image be better. 
But a “better” image may still remain mediocre. A direct 
score method be limited by the expertise of the evaluators, 
who be also susceptive to bias in a non-blind setting. 

What be the “ultimate” metric for the high aesthetic stan- 
dard human can define? Drawing inspiration from the famous 
Turing-test, we propose the follow metric: 

• A generate photo be at professional level, if profession- 
als, in a blind test, can not tell whether it be create by 
an algorithm, or by another professional. 

In our work we use “professional”, i.e. professional pho- 

ar 
X 

iv 
:1 

70 
7. 

03 
49 

1v 
1 

[ 
c 

.C 
V 

] 
1 

1 
Ju 

l 2 
01 

7 

lphilippe 
Texte tapé à la machine 
At first sight: not so efficient compare to professional... to be revise 



tographers, to represent best expert in photography. The 
standard can be further raise by replace that word with, 
say, “top 10 master photographer in the world”. 

We work with professional photographer to define 4 level 
of aesthetic quality, with the top level “pro”. In experiment 
we ask professional to rate a random mixture of photo into 
different levels, with photo take by our robot mixed in. For 
robot creation with high prediction scores, about 40% rating 
we receive be at semi-pro to pro level. While we didn’t beat 
our “Turing-test” consistently, we show that create photo 
use machine learn at professional quality be make possi- 
ble. 

Our contribution in this paper are: 
• A deep-learning system that learns different aspect of 

aesthetic and applies image operation to improve each 
aspect in a semi-orthogonal way, use a dataset of pro- 
fessional quality photo with no additional labels, or be- 
fore/after image pairs. 
• Introduce dramatic mask a a novel image operation to 

enhance dramatic light in photos. 
• In a “Turing-test”-like experiment, we show that our sys- 

tem can create photo from environment with some of 
them at semi-pro to pro level. 

The rest of the paper be organize a following: Related 
work be discuss in Section 2. We describe the framework 
of Creatism system in Section 3, include how to train deep- 
learn model for different aspect in aesthetics, and how 
to optimize each aspect independently in a photo use it 
associate image operation. Two special image operations, 
crop and dramatic mask, be discuss in Section 4. To 
evaluate photo create by our robot, aesthetic level be de- 
fin in Section 5. Experiments be present in Section 6. 

2 Related Work 
Datasets be create that contain rating of photograph 
base on aesthetic quality [Murray et al., 2012] [Kong et al., 
2016] [Lu et al., 2015a]. Machine-learning method have 
be use to train model on aesthetic quality use such 
datasets, with either hand-crafted feature [Murray et al., 
2012] [Marchesotti et al., 2014], or deep neural network [Lu 
et al., 2014] [Lu et al., 2015b]. 

Image enhancement can be guide by learn aesthetic 
model. In [Yan et al., 2016] color transformation be predict 
from pixel color and semantic information, learn from be- 
fore and after image pair edit by a photographer. 

Beyond judging a photo a simply “good” or “bad”, mul- 
tiple aspect of aesthetic can be learn to control image 
filters. In [Jaroensri et al., 2015], image with brightness 
and contrast variation be evaluate a acceptable or not 
by crowd computing, which in turn predicts acceptable range 
of these filter on new images. However, sample image in 
2 − D space of brightness and contrast and have them la- 
beled by people do not scale well. In this paper we present 
an approach that study each filter in it own subspace. We 
also get rid of human label in this step. 

AVA dataset be use to train image crop in [Mai 
et al., 2016], where a sliding-window be apply to an im- 
age to yield the best crop. A recent approach in [Chen et al., 

2017] be similar in spirit to ours, where the author learn a 
model for aesthetic composition by use random crop a 
negative examples. Our approach decouples multiple aes- 
thetic aspect include composition. In the special case of 
cropping, we show that a hybrid approach lead to more vari- 
ations of good crop candidates. 

Pair-wise image style transfer be another way to enhance 
images. Deep-learning approaches, pioneer by [Gatys 
et al., 2015], show huge advantage over traditional texture- 
synthesis base approach [Hertzmann et al., 2001]. Recent 
research also transfer style from photo to photo while pre- 
serve realistic detail in result [Luan et al., 2017]. How- 
ever, such transfer require the user to manually provide an 
image or painting a the target. The success of transfer heav- 
ily depends on how suitable the target is. 

In Generative-Adversarial Networks (GANs) [Goodfellow 
et al., 2014], a generative model G and a discriminative model 
D be simultaneously trained. It lead to amaze content cre- 
ation ability, be able to generate plausible image of differ- 
ent category [Radford et al., 2015] [Nguyen et al., 2016] 
[Zhu et al., 2017]. Conditions can be introduce to control 
GAN’s result [Mirza and Osindero, 2014]. Such condition 
may come from another image, such that generate result 
become a variation of the input image [Isola et al., 2016]. 

However, GAN result usually contain noticeable artifacts. 
This limit it application in image enhancement. In this pa- 
per we introduce an image operation “dramatic mask” that 
us a similar structure a DC-GAN [Isola et al., 2016]. In- 
stead of output pixel directly, it creates a low-resolution 
mask to modulate brightness, condition by original image. 
This operation enhances dramatic light in many photos. 

Evaluation on aesthetic be very subjective. Comparing to 
ground truth [Mai et al., 2016] [Chen et al., 2017] be only vi- 
able when ground-truth exists. User study be another option, 
where typically 2 or more image be compare to yield the 
best one [Yan et al., 2016]. However, the win image in 
a group may still be of low aesthetic quality. Previous study 
show that image comparison can produce absolute rank 
give enough pair [Mantiuk et al., 2012]. However collect- 
ing huge amount of pair from professional be not practical. 
Instead, we adapt an approach with an absolute score sys- 
tem. Our evaluation be a “Turing-test” on aesthetic quality, 
where professional be ask to give score to a mixture of 
photo with predefined absolute meaning. 

3 Problem Formulation 
Assume there exists a universal aesthetic metric, Φ, that 
give a high score for a photo with high aesthetic qual- 
ity. A robot be assign a task to produce the best photograph 
P , measure by Φ, from it environment E . The robot seek 
to maximize Φ by create P with optimal actions, control 
by parameter {x}: 

arg max 
{x} 

(Φ(P (E , {x}))) (1) 

The whole process of photo generation P (E , {x}) can be 
broken down into a series of sequential operation Ok, k = 



1...N , control by their own set of parameter {xOk}. Each 
Ok operates on it subject to yield a new photo: 

Pk = Ok({xk}) ◦ .... ◦O1({x1}) ◦ E 
A well-defined and differentiable model of Φ will conclude 

our paper at this point. But in practice, it be next to impossible 
to obtain a dataset that defines Φ. Here be several reason 
why: 

• Curse of Dimensionality 
By definition, Φ need to incorporate all aesthetic aspects, 

such a saturation, detail level, composition... To define Φ 
with examples, number of image need to grow exponen- 
tially to cover more aspect [Jaroensri et al., 2015]. 

To make thing worse, unlike traditional problem such a 
object recognition, what we need be not only natural images, 
but also pro-level photos, which be much less in quantity. 

• Meaningful Gradient 
Even if Φ manages to approximate aesthetic quality use 

examples, it gradient may not provide guidance on every as- 
pect in aesthetics. If a professional example be introduce 
mainly for it composition quality, it offer little insight on 
HDR strength for similar photos. 

• Before/After Pairs be Harder to Obtain 
To put thing to extremity, let’s say all pro-level photo 

in this dataset have a signature print somewhere, while all 
lower-quality photo don’t. Φ can be train to high accuracy 
just by detect signatures. Obviously such Φ provide no 
guidance to our goal. In reality it be similarly hard to force Φ 
to focus on aesthetic quality alone, instead of other distribu- 
tion imbalance between photo at different quality. 

One useful trick be to provide photo-pairs before and after 
post-processing by photographers. This way Φ be force to 
only look at difference the photographer made. However, it 
be even harder to collect such a dataset in large quantity. 

• Hard to Optimize 
It be difficult to optimize all aesthetic aspect in their joint 

high-dimensional space. 

3.1 Segmentation of Φ 
In our paper, we resolve above issue by choose opera- 
tions Ok to be approximately “orthogonal” to each other. 
With that we segment Φ into semi-orthogonal components: 
Φ := Σ 

k 
Φk + Φres , where Φk only measure aesthetic 

change thatOk be capable of causing. This way applyingOk 
have much less impact on all other Φj , j 6= k, which make the 
optimization problem separable. 

The objective function in Eq (3) be approximate as: 

Σ 
k 

(arg max 
{xOk} 

(Φi(Pi({xOk})))) + Φres(PN ) 

Here each arg max 
{xOk} 

(Φk(Pk({xOk}))) becomes a lot more 

tractable. Intuitively, if Ok be an image filter that change 
overall saturation, Pk({xOk}) := Ok({xOk}) ◦Pk−1 applies 
saturation filter, control by {xOk}, on a constant input 

photoPk−1 from last step. Φk be a metric that only care about 
how much saturation be aesthetically please for a photo. The 
optimization seek the right saturation amount that satisfies 
Φk. Such optimization happens in the dimension of {xOk}, 
which be much low compare to Eq (3), sometimes even in 
1d. 

Φres capture all remain aesthetic aspect miss by 
{Φk}. However, if {Ok} exhaust our operations, whose aes- 
thetic effect be already measure by {Φk}, there be liter- 
ally nothing we can do to improve Φres. 

However, aesthetic aspect in Φres can still be capture to 
evaluate overall aesthetic quality. We train a scorer Φ 

′ ∼ 
Φ := Σ 

k 
Φk + Φres use Inception v3 [Szegedy et al., 2015] 

to predict AVA rank score (see Section 5) directly from 
images. This scorer be late use to rank create photos. 

3.2 Selection of Operations {Ok} 
The intuition behind the requirement for {Ok} to be “orthog- 
onal”, be that we don’t want a late step Oj to damage aes- 
thetic quality Φi that be optimize by an early step Oi. 
But in practice, the pixel change cause by any image filter be 
hardly orthogonal to that of another. We use this term loosely, 
and manually pick operation {Ok} in follow order: 
• Composite an image from environment 
• Apply saturation filter 
• Apply HDR filter 
• Apply dramatic mask 
Their effect be roughly independent to each other, focus- 

ing on composition, saturation, detail level and low frequency 
brightness variation, respectively. In section 6.3 we will show 
how this semi-orthogonality help each Φk to focus on it re- 
spective aesthetic aspect. 

3.3 Operation-Specific Aesthetic Metrics {Φk} 
Let’s use a saturation filter Os a an example of image op- 
erations. We want to train a metric Φs, that focus only on 
aesthetic quality related to saturation, not on anything else. 

If a dataset contains photo with label overall aesthetic 
quality, it can not be directly use to train Φs because contri- 
bution of saturation be mixed with all other aesthetic aspects. 
Instead, we propose a method that only us pro-level photo 
a positive example for saturation training, with the maxi- 
mum Φs score assign to them. We then randomly perturb 
saturation level in these photo use Os. Its difference to 
the original photo serf a a penalty on Φs for the perturbed 
photo. 

A deep-learning model be then train to predict Φs for 
these photos. Since a high-score photo and it low-score 
counter-part only differ by the perturbation, the model fo- 
cu only on whatOs did, nothing else. This make Φs much 
easy to train than Φ. While gradient in Φ can be ill-defined, 
we can now find gradient for each Φk use it respectiveOk. 

Since negative example be generate on the fly, this 
method remove the need for before/after photo pair in 
dataset. We can start with a same set of professional photo 
to train different aspect in aesthetics. 



Require: Dataset of professional photo {M} 
Require: Image operation Ok: Mout = Ok({x}) ◦Min 
Require: A metric measure similarity between two photos: 

Sim(M1,M2) ∈ [0, 1]. 1.0 mean identical. 
1: D = {}, a set to hold (image, score) pairs. 
2: for Each image M in {M} do 
3: Insert (M, 1.0) in D 
4: # Randomly sample parameter within a range: 
5: for Each dimension d in {x} do 

offsetd = Random(xmind, xmaxd) 
6: M 

′ 
= Ok({offset}) ◦M 

7: Insert (M 
′ 
, Sim(M,M 

′ 
)) in D 

8: Train a model to predict score Φk from D 

The algorithm for training Φk be give below. 
When apply an operation on a photo, we optimize fil- 

ter parameter to maximize Φk on the input photo. When 
Ok have only one parameter, the optimization becomes a fast 
1d search. In this project, we optimize Saturation and HDR 
filter use this method. 

4 Special Operations 
Two special operations, crop and dramatic mask, deviate 
slightly from the algorithm in section 3.3. 

4.1 Image Composition 
In the first operation Ocrop, our robot find the best composi- 
tions from the environment. In our project, the environment 
be represent by a spherical panorama. We first do 6 cam- 
era projection to sample the panorama: each projection be 
separate by 60 degree to cover all directions, with pitch an- 
gle look slightly up at 10 degree, field of view 90 degree. 
This way each project overlap with it neighbor to increase 
chance for any composition to be contain in at least one 
projection. 

Φcrop be train to pick the best crop for each of these 6 im- 
ages. Its training be similarly to other Φk, where a perturbed 
image M 

′ 
be a random crop from M . Sim(M,M 

′ 
) be simply 

define a Area(M 
′ 
) 

Area(M) . Both M and M 
′ 

be resize to a square 
of fix size before training. 

Intuitively, the score say that in a professional photo M , 
the photographer chose current composition over a zoomed- 
in version to favor a good composition. Since crop only 
remove content, M 

′ 
never creates a case when a good com- 

position be encompass by unnecessary surroundings. While 
technique like image inpainting can potentially fake some 
surround to a photo, in practice we notice Φcrop performs 
reasonably well without such cases. 

However, crop need special treatment because the 
correlation between it operation Ocrop and other Φk. Intu- 
itively, if we change saturation of a photo, Φcrop be design 
to remain indifferent. But if we crop an image to a small 
patch, Φs may suffer a lot, because image content be totally 
changed. 

Thus we can not turn a blind eye to other aesthetic aspect 
during cropping. We instead use 

Φ 
′ 

crop(c) := c× Φcrop + (1− c)× Φ 
′ 

Φ 
′ 

be an approximate to Φ that judge overall aesthetic from 
Section 3.1. c be a constant to weigh in the importance of 
composition vs. overall aesthetic quality. In Section 6.2, 
experiment show that such a hybrid metric lead to good 
crop options. 

During optimization, a slide window at different size and 
aspect ratio search for crop with high Φ 

′ 

crop(c). 

4.2 Dramatic Mask 
A photographer often need to stay at one spot for hours, 
wait for the perfect lighting. When that doesn’t work out, 
post-process may also add dramatic or even surreal light to 
a photo. Vignetting be a commonly use filter that modulates 
brightness spatially with fix geometry, usually brighter near 
the center, darker at boundary. Changing light base on 
image content be typically a manual job. 

We want to learn a novel image operation, dramatic mask, 
that enhances dramatic light by modulate brightness 
gradually in a photo. Good example with such light must 
exist in {M}. However, {M} contains no additional label to 
indicate that. We show that it be still possible to learn such a 
specific operation from unlabeled dataset. 

Since we do not have an existingOdramatic to create {M 
′} 

a negative examples, we use a list of exist image filter 
{Fj} that be capable to change brightness in various ways: 
M 

′ 
= Randomly pick from {Fj ◦M} 

The selection of {Fj} be discuss in section 6.4. Here the 
assumption be M , be a professional photo, have a chance 
of contain dramatic lighting. If the light in the photo 
be change significantly by any Fj , it will likely lead to less 
ideal lighting. 

Since Odramatic do not exist yet, we no longer separate 
the training of Φdramatic from the apply of Odramatic 
a in section 3.3. Instead, they be train jointly in a 
Generative-Adversarial Network (GAN). 

GAN have demonstrate great capability in create appar- 
ently novel content from examples. In most case such cre- 
ation be seed by random number or control parameters. 
Its outcome be typically a fixed-size raster in image space, 
usually with some noticeable artifacts. Generating an image 
at arbitrary size without artifact be currently challenging. 

Instead of generate pixel directly, we target to generate 
the “editing” operation Odramatic use GAN, which be less 
susceptive to artifacts. This operation be parameterized by a 
low-frequency 8 × 8 mask to modulates brightness of input 
photo M ‘: 
Odramatic ◦M 

′ 
:= M 

′ 
+mask× ( Brighten ◦M ′ −M ′) 

Here “Brighten” be an image filter that make input 
brighter. During training mask be smoothly up-scaled to the 
size of M ‘ use bilinear interpolation. 

We learn how to generate the mask in the generative model 
G. The overall network architecture be DCGAN-based [Isola 
et al., 2016], with G condition by our input image. Its net- 
work architecture be depict in Figure 1. 



Figure 1: GAN structure for dramatic mask training. 

The discriminative model D try to distinguish image 
from {M} and {Odramatic ◦M 

′}. To encourage variation 
in G, we do not have a loss that compare pixel-difference be- 
tween Odramatic ◦M 

′ 
and the photo M ‘ be derive from. 

Intuitively, there should exist many different way to change 
the light in a scene such that it becomes more dramatic. 

Due to the competitive nature of GAN, it be very difficult 
for G to converge to a static state [Goodfellow et al., 2014]. 
Instead of wait for an optimal mask, we use GAN to pro- 
vide multiple candidates. We train multiple model with dif- 
ferent random initialization, collect snapshot over time. 
All snapshot form a set of candidate model {M}. In the 
end, Φ 

′ 
be use to pick the best result generate from all mem- 

bers of {M}. 
Overall, the algorithm of how our robot operates is: 

Require: The robot “travels” through environment {E} 
1: Results {R} = {} 
2: Photos to process {P} = {}. 
3: for Each environment E in {E} do 
4: Project E to 6 image {P0} in different direction 
5: for Each project image P0 do 
6: for composition importance c in {0, 0.5, 1} do 
7: Find best crop {P1} use Φ 

′ 

crop(c) 
8: Insert top k = 3 crop of {P1} into {P} 
9: for Each P1 in {P} do 

10: Maximize ΦHDR: P2 = OHDR ◦ P1 
11: Maximize Φsaturation: P3 = Osaturation ◦ P2 
12: Dramatic mask result {PM} = {}, 
13: for Each dramatic mask modelM in {M} do 
14: P3i = Odramatic(M) ◦ P2 
15: Insert P3i to {PM} 
16: P3 = image in {PM} with max Φ 

′ 
. 

17: Insert P3 in {R} 
18: For crop in {R} from same P0, keep one with max Φ 

′ 

19: Rank {R} with Φ′ . 

5 Aesthetic Scale 
In this paper, we target to generate photo towards profes- 
sional quality on a measurable aesthetic “scale”, such that 
they can be comparable with all other photo in the world. 

We work with professional photographer to empirically de- 
fine 4 level of aesthetic quality: 

• 1: point-and-shoot photo without consideration. 
• 2: Good photo from the majority of population without 

art background. Nothing artistic stand out. 
• 3: Semi-pro. Great photo show clear artistic aspects. 

The photographer be on the right track of become a 
professional. 
• 4: Pro-work. 
Clearly each professional have his/her unique taste that 

need calibration. We use AVA dataset to bootstrap a con- 
sensus among them. All AVA image be sort by their aver- 
age voting scores. A single percentage rank score between 
[0, 1] be then assign to each image. For example, an image 
with rank score 0.7 be ranked at top 30% among all AVA 
images. 

We empirically divide this rank score into 4 level use 
threshold 0.15, 0.7, 0.85, to roughly correspond to our aes- 
thetic scale. This correspondence be by no mean accurate. 
It only encourages a more even distribution of sample AVA 
image across different qualities. We sample image evenly 
from 4 levels, and mix them randomly. 

Each professional be ask to score these image base on 
the description of our aesthetic scale. After each round, we 
find average score for each photo a the consensus. For indi- 
vidual score deviate a lot from the consensus, we send the 
photo with consensus to the correspond professional for 
calibration. After multiple round we notice a significant 
drop in score deviation, from 0.74 to 0.47. 

At the end of this project, we ask professional for their 
own description of our aesthetic scale. Feedback be summa- 
rized in Table 1. 

To map AVA rank score Φ‘ to aesthetic scale in range 
[1, 4], we fit Φ̄ = a×Φ‘ + b to professionals’ score on AVA 
dataset. Φ̄ be use in experiment to predict score for each 
image. 

6 Experiments 
6.1 Dataset 
For professional landscape photo {M}, we collect ∼ 
15000 thumbnails, from high-rating landscape category on 
500px.com. Our training use thumbnail at a resolution up 
to 299× 299 pixel. 

To make overall style more consistent, we choose to target 
our goal a “colorful professional landscape”. We remove 
image from dataset that be black&white or low in satura- 
tion, with minimum average saturation per pixel set at 55%. 
Similar approach can be use to train towards other styles, 
such a “black&white landscape”. 

AVA dataset [Murray et al., 2012] be use to train Φ 
′ 
, a 

well a for calibration of professionals’ scores. 

6.2 Training of Cropping Filter 
M 

′ 
be randomly cropped from M in two batches. The first 

batch concentrate on variation close to original image, so 
we have more example to learn when composition be close 



to optimal. The cropped width be between (90%, 100%) of 
original image. Aspect ratio be randomly select between 
(0.5, 2), with the area contain within image space. The 
second batch deviate more from optimal composition, with 
width range (50%, 90%). Both batch be equal in number. 
Score be define a area ratio, a described in section 4.1. The 
training network be Inception v3 [Szegedy et al., 2015], which 
be use to predicts score from input image. 

For each project photo from panorama, we pick top 3 
candidate at each composition weight c ∈ {0, 0.5, 1}. All 
these candidate move forward in pipeline independently. At 
the end of the pipeline, the candidate with high Φ 

′ 
be se- 

lected to represent that photo. 
To further compare effect of composition weights, we 

conduct a separate experiment, where 4 professional be 
present with 3 cropped version of a same photo, use the 
top candidate from each composition weight c ∈ {0, 0.5, 1}. 
They be ask to pick the one with best composition, or se- 
lect “none”. For all 100 input images, after exclude 10.4% 
rating of “none”, 22% image receive a unanimous voting 
on one crop candidate. The distribution of c ∈ {0, 0.5, 1} 
for win candidate be 9.2%, 47.4% and 43.4%, respec- 
tively. 80% image have up to two win candidates. The 
distribution of winner be 13.2%, 41.4% and 45.4%, respec- 
tively. This show that with our hybrid approach, we can pro- 
duce good crop candidate than use a single metric. 

6.3 Training of Saturation and HDR Filters 
In our approach the choice of {Ok} be flexible. For saturation 
filter we use an implementation similar to the saturation ad- 
justment of “Tune Image” option in Snapseed. During train- 
ing M 

′ 
be obtain by set saturation parameter randomly 

between (0%, 80%) of filter’s range, where 0% turn a photo 
to black & white. Per-pixel-channel color difference δ be use 
to derive saturation score, with maximum difference capped 
at 6% for a score 0: 

score := Sim(M,M 
′ 
) := 

{ 
1− δ6% , δ <= 6% 
0, otherwise 

For each M , 6 variation of M 
′ 

be generate for train- 
ing, together with M . 

HDR filter be train in a same way, use an implemen- 
tation similar to “HDR Scape” in Snapseed. “Filter Strength” 
be the only parameter we modify. 

One difference here be “Filter Strength” only go posi- 
tively. While we can reduce saturation of an image, there 
be no option to add “negative” HDR effect , which be the di- 
rection we care more. Intuitively, we assume HDR effect 
already exists in some M . We want to generate M 

′ 
that con- 

tains “less” HDR, so we can learn how an HDR filter can 
help it to look more like M . We use a simple trick to mimic 
“negative” HDR effect by a per-pixel operation: 

F (−strength) ◦M = 2×M − F (strength) ◦M (2) 

This way we expand the range of “Filter Strength” 
from (0,max) to (−max,max). We generate {M ′} in 

two batches. For each M , we sample 6 variation of not- 
enough-HDR example M 

′ 

1, with “Filter Strength” in range 
(−max,−0.5max). We also sample 3 variation of too- 
much-HDR example M 

′ 

2, with “Filter Strength” in range 
(0.5max,max). Color difference δ be capped at 20%. 

Training of both aesthetic metric be same a that of crop- 
ping metric. 

During optimization for each photo, a quick 1d search on 
parameter be apply for each filter. Saturation parameter be 
try from 40% to 90%, with step 10%. For HDR, “Filter 
Strength” parameter be try from 0% to 70% of maximum 
range, with step 10%. The parameter of each filter with high- 
est score be commit to apply on the photo. 

In Figure 2, an example of change in {Φk} from different 
image operation be visualized. Saturation of input image be 
increase from 0 to maximum in Figure 2b. Saturation score 
show a distinctive peak, while score for HDR and compo- 
sition remain more flat. 

Note that overall aesthetic score Φ 
′ 

show continuous in- 
creasement, well into saturation range that make the image 
over-saturated. This be expected, because the training of Φ 

′ 
on 

AVA dataset only involves natural images. Since a large por- 
tion of higher-score image be more-saturated than typical 
user photo directly from a cell phone, Φ 

′ 
grows high with 

increase saturation. However, since AVA do not inten- 
tionally implant over-saturated image a negative examples, 
Φ 

′ 
do not penalize over-saturated images. This show that 

a naively-trained general aesthetic metric may not be suitable 
for optimize different aesthetic aspects. 

Similarly, in Figure 2c HDR score show a clear peak when 
HDR strength increases. Once again Φ 

′ 
remains high incor- 

rectly even when HDR strength be too strong. 
Figure 2d show the special case of image composition. 

Here a slide window of half image width move vertically 
across the image at center, with aspect ratio 1.8. Composi- 
tion score contains a peak near image center. However, both 
saturation and HDR score vary a lot. This be also expected, 
because image statistic regard saturation and detail level 
both change a the slide window moves. In other words, 
crop operation be less “orthogonal” to other image oper- 
ations, which lead to our hybrid approach on composition 
score. 

For some photo {Φk}may become less orthogonal. For ex- 
ample, when pixel over-saturate, detail be also lost, which 
may impact HDR score that measure detail level. Figure 3b 
show such an example, where a saturation increases, HDR 
score start to change too. In such a case, two linear search 
in saturation and HDR strength separately may not yield op- 
timal solution. Global method like gradient-descent can find 
good solution, at the cost of more expensive search. In this 
paper we use separate linear-search to generate all results. 

6.4 Training of Dramatic Mask 
We use follow image filter with equal chance to generate 
{Fj}. (Note that they can be replace by other reasonable 
alternatives.) 
• Snapseed “Tune Image”, brightness parameter randomly 

from (10%, 45%) and (55%, 90%) 



(a) Input image (b) Increase saturation to max (c) Increase HDR strength to max (d) Crop image with a slide windowfrom top to bottom 

Figure 2: Scores change when image (a) be modify by different operations. 

(a) Input image (b) Increase saturation to max 

Figure 3: Saturation and HDR score couple when increas- 
ing saturation for image (a). 

• Snapseed “Tune Image”, contrast parameter randomly 
from (10%, 45%) and (55%, 90%) 
• Snapseed “HDR Scape”, parameter randomly from 

(10%, 100%) and (−100%,−10%). Negative effect be 
simulated use Eq (6.3) 
• Snapseed “Vignette”, with “Outer Brightness” in 

(0%, 35%) to darken boundary, and (60%, 70%) to 
brighten boundary. 
• A curve-editing filter, where 6 control point evenly 

span brightness range, move within (−15%, 15%) 
of total range. 
• A flatten-brightness filter with strength (10%, 100%), 

and strength (−100%,−10%) use Eq (6.3) 
The flatten-brightness filter move each pixel’s brightness 

b towards the Gaussian smooth average of it neighborhood b 
with strength s ∈ (0, 1): bnew = b×s+b×(1−s). Gaussian 
radius be 0.05× (width + height). 

We train 37 model with different random initialization 
for about 2,000,000 step each. For each model, we collect 
snapshot of train parameter roughly every 100000 steps. 
We then apply all these model on a test dataset, and pick 
best mask for each image use Φ 

′ 
. We rank model by num- 

ber of final result they contributed, and keep top 50. After 
manual examination, 20 of these models, especially early in 
training, generate visible artifact to different degrees. After 
remove them, we eventually keep top 30 model a {M}. 

To good align 8×8 mask with photo content at full resolu- 
tion, we apply a joint bilateral upsampling [Kopf et al., 2007] 
on the mask against the photo before generate the final re- 
sult. The effect of dramatic mask varies by images. Some- 

Figure 4: Images before and after apply dramatic mask. 

time it mimic traditional Vignetting. But in more general 
case the brightness modulation be base on image content. 
Figure 4 show several examples. 

An end-to-end example be show in Figure 5, where a 
panorama in (a) be cropped into (b), with saturation and HDR 
strength enhance in (c), and finally with dramatic mask ap- 
ply in (d). 

6.5 Photo Creation 
The environment for our robot be simulated by a collection of 
panorama image from Google Street View. Most trail we 
chose be collect on foot, instead of by vehicles. Loca- 
tions we picked include Banff, Jasper and Glacier national 
park in Canada, Grand Canyon and Yellowstone National 
Parks in US, and foot trail in Alps, Dolomites and Big Sur. 
Panoramas be sample sparsely along each trail to reduce 
data amount and redundancy. Neighboring panorama be 
typically separate by ten of meter or more. Altogether 
∼ 40000 panorama be used. From them ∼ 31000 pho- 



(a) 

(b) (c) (d) 

Figure 5: A panorama (a) be cropped into (b), with saturation 
and HDR strength enhance in (c), and with dramatic mask 
apply in (d). 

tos be create with predict score Φ̄ >= 2.5. 
Street View imagery we use contains some artifacts, 

which have an impact on quality of our results. Over-exposure 
happens quite often in brighter area, such a on cloud, which 
wash out details. Misalignment and blurry part can also be 
notice in our results. 

During evaluation, we manually remove result with se- 
vere misalignment/blur, Street View equipments, black area 
after panorama stitching, large portion of highway and large 
human figures. 

6.6 Evaluation 
We work with 6 professional for evaluation. We select them 
with a minimum requirement of a bachelor’s degree in Pho- 
tography and 2+ year experience a a professional photog- 
rapher. To keep the evaluation a objective a possible, they 
be not inform of our image creation attempt. 

We conduct 6 round of calibration a described in Sec- 
tion 5, use ∼ 2200 AVA image in total. The score devia- 
tion per image drop from initial 0.74 to 0.47. 

We randomly sample 400+ photo from our creation, 
with predict score Φ̄ uniformly distribute between 
(2.5, 3.0). (Most photo receive a prediction score < 3.0 
after linear fitting of Φ̄.) They be randomly mixed with 800 
photo from AVA and other sources, sample across different 
quality. 

Figure 6 show how score distribute for photo at differ- 
ent predict scores. A high predict score lead to high 
score from professionals, which show Φ̄ be correlate to 
professional’s taste. More accurate prediction of profession- 
als’ score be still to be achieved. Average score per image be 
the average of individual score from all 6 professional for 
each image. 

For 173 evaluate photo with a high predict score >= 
2.9, 41.4% of individual score receive from professional 

(a) (b) 

(c) (d) 

Figure 6: Individual (a) and per-image average (b) score dis- 
tribution for image at different predict scores. As a com- 
parison, score distribution for top-ranking AVA photo be 
show in (c) and (d), which be arguably a reasonable col- 
lection of actual professional work. 

be at or above semi-pro level (>= 3.0), with 13% of them 
>= 3.5. 

As a comparison, score distribution for sample high- 
rank AVA photo be also show in Figure 6 (c) and (d). 
We can see even for top 5% photo ([0.95, 1] in graph) in AVA 
dataset, arguably all at professional quality, only 45% score 
be at or above 3.5. This may serve a an estimation of the 
“ceiling” for such an algorithm. 

Some example of create photo with prediction score 
> 2.7 be displayed in Figure 7. To demonstrate their qual- 
ity variation accord to the professionals, we select photo 
with average professional score < 2.0 in first row, ∼ 2.5 in 
second row, and > 3.0 in last row. 

More successful case be manually select from all of 
the results, and present in Figure 11. Photos in left column 
be create from panorama in right column. For each photo, 
predict score and average score from professional be dis- 
played. The chance to encounter result with similar score 
can be look up in Figure 6 (b). 

We compile a show-case webpage that contains ad- 
ditional result with nearby Street View panorama at: 
https://google.github.io/creatism 

Acknowledgement The author would like to acknowledge 
Vahid Kazemi for his early work in predict AVA rank 
score use Inception network, and Sagarika Chalasani, Nick 
Beato, Bryan Klingner and Rupert Breheny for their help in 
processing Google Street View panoramas. We would like to 
thank Peyman Milanfar, Tomáš Ižo, Christian Szegedy, Jon 
Barron and Sergey Ioffe for their helpful comments. Huge 
thanks to our anonymous professional photographers. We 
hope you be not reading this paper to introduce bias in your 
future ratings! 

https://google.github.io/creatism 


1.9 1.8 
1.9 

2.4 

2.5 

2.4 

3.3 3.0 3.0 

Figure 7: Example photo with predict score > 2.7. Av- 
erage professional score be displayed below. 

References 
[Chen et al., 2017] Chen, Y., Klopp, J., Sun, M., Chien, S., and Ma, 

K. (2017). Learning to compose with professional photograph 
on the web. CoRR, abs/1702.00503. 

[Gatys et al., 2015] Gatys, L. A., Ecker, A. S., and Bethge, 
M. (2015). A neural algorithm of artistic style. CoRR, 
abs/1508.06576. 

[Goodfellow et al., 2014] Goodfellow, I., Pouget-Abadie, J., Mirza, 
M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Ben- 
gio, Y. (2014). Generative adversarial nets. In Ghahramani, Z., 
Welling, M., Cortes, C., Lawrence, N. D., and Weinberger, K. Q., 
editors, Advances in Neural Information Processing Systems 27, 
page 2672–2680. Curran Associates, Inc. 

[Hertzmann et al., 2001] Hertzmann, A., Jacobs, C. E., Oliver, N., 
Curless, B., and Salesin, D. H. (2001). Image analogies. In 
Proceedings of the 28th Annual Conference on Computer Graph- 
ic and Interactive Techniques, SIGGRAPH ’01, page 327–340, 
New York, NY, USA. ACM. 

[Isola et al., 2016] Isola, P., Zhu, J., Zhou, T., and Efros, A. A. 
(2016). Image-to-image translation with conditional adversarial 
networks. CoRR, abs/1611.07004. 

[Jaroensri et al., 2015] Jaroensri, R., Paris, S., Hertzmann, A., By- 
chkovsky, V., and Durand, F. (2015). Predicting range of ac- 
ceptable photographic tonal adjustments. In IEEE International 
Conference on Computational Photography, Houston, TX. 

[Kong et al., 2016] Kong, S., Shen, X., Lin, Z., Mech, R., and 
Fowlkes, C. C. (2016). Photo aesthetic rank network with 
attribute and content adaptation. CoRR, abs/1606.01621. 

[Kopf et al., 2007] Kopf, J., Cohen, M. F., Lischinski, D., and Uyt- 
tendaele, M. (2007). Joint bilateral upsampling. ACM Transac- 
tions on Graphics (Proceedings of SIGGRAPH 2007), 26(3):to 
appear. 

[Lu et al., 2014] Lu, X., Lin, Z., Jin, H., Yang, J., and Wang, J. Z. 
(2014). Rapid: Rating pictorial aesthetic use deep learning. In 
Proceedings of the 22Nd ACM International Conference on Mul- 
timedia, MM ’14, page 457–466, New York, NY, USA. ACM. 

[Lu et al., 2015a] Lu, X., Lin, Z., Jin, H., Yang, J., and Wang, J. Z. 
(2015a). Rating image aesthetic use deep learning. IEEE 
Transactions on Multimedia, 17(11):2021–2034. 

[Lu et al., 2015b] Lu, X., Lin, Z., Shen, X., Mech, R., and Wang, 
J. Z. (2015b). Deep multi-patch aggregation network for image 
style, aesthetics, and quality estimation. In 2015 IEEE Interna- 
tional Conference on Computer Vision (ICCV), page 990–998. 

[Luan et al., 2017] Luan, F., Paris, S., Shechtman, E., and Bala, K. 
(2017). Deep photo style transfer. CoRR, abs/1703.07511. 

[Mai et al., 2016] Mai, L., Jin, H., and Liu, F. (2016). Composition- 
preserve deep photo aesthetic assessment. In 2016 IEEE Con- 
ference on Computer Vision and Pattern Recognition (CVPR), 
page 497–506. 

[Mantiuk et al., 2012] Mantiuk, R. K., Tomaszewska, A., and Man- 
tiuk, R. (2012). Comparison of four subjective method for image 
quality assessment. Comput. Graph. Forum, 31(8):2478–2491. 

[Marchesotti et al., 2014] Marchesotti, L., Murray, N., and Per- 
ronnin, F. (2014). Discovering beautiful attribute for aesthetic 
image analysis. CoRR, abs/1412.4940. 

[Mirza and Osindero, 2014] Mirza, M. and Osindero, S. (2014). 
Conditional generative adversarial nets. CoRR, abs/1411.1784. 

[Murray et al., 2012] Murray, N., Marchesotti, L., and Perronnin, 
F. (2012). Ava: A large-scale database for aesthetic visual anal- 
ysis. In 2012 IEEE Conference on Computer Vision and Pattern 
Recognition, page 2408–2415. 

[Nguyen et al., 2016] Nguyen, A., Yosinski, J., Bengio, Y., Doso- 
vitskiy, A., and Clune, J. (2016). Plug & play generative net- 
works: Conditional iterative generation of image in latent space. 
CoRR, abs/1612.00005. 

[Radford et al., 2015] Radford, A., Metz, L., and Chintala, S. 
(2015). Unsupervised representation learn with deep convo- 
lutional generative adversarial networks. CoRR, abs/1511.06434. 

[Szegedy et al., 2015] Szegedy, C., Vanhoucke, V., Ioffe, S., 
Shlens, J., and Wojna, Z. (2015). Rethinking the inception ar- 
chitecture for computer vision. CoRR, abs/1512.00567. 

[Yan et al., 2016] Yan, Z., Zhang, H., Wang, B., Paris, S., and Yu, 
Y. (2016). Automatic photo adjustment use deep neural net- 
works. ACM Transactions on Graphics, 35(2):11. 

[Zhu et al., 2017] Zhu, J., Park, T., Isola, P., and Efros, A. A. 
(2017). Unpaired image-to-image translation use cycle- 
consistent adversarial networks. CoRR, abs/1703.10593. 



Score Description 
1.0 Beginner: Point-and-shoot without consideration for composition, light etc. (it can still be technically good, i.e., good focus, good exposure...) 

- This photo have no attention to light, composition, or moment and there be no clear intent. Looks like a photo take by accident or like someone just held the 
camera up and press the shutter button... it’s not clear why anyone would take this photo or want to look at this photo. 
- This score represent failure to use the camera properly and/or create anything that resembles a passable photo. 
- These photograph clearly be point and shoot with no intention of a composition in mind. These photograph usually lack contrast or thought and have nothing 
interest in their subject matter. 
- No artistic merit or intention and do not follow basic rule of composition, light or focus. 

2.0 Amateur: Good photo for people without a background in photography. Nothing stand out a embarrassing. But nothing artistic either. Lighting andcomposition be OK. The majority of population fall between 1 and 2. 
- I’m simply bore when I look at this image and it creates a sense of indifference. The image do not engage me at all and there be nothing interest in the photo. 
This could be a combination of a poorly-lit image of a boring, cliched subject, and/or, an unsettled composition with poor tonal values. 
- This photo have a clear subject and thing like composition and moment be ok but not great. You can tell that this person saw something of interest and they 
document it, but they be still not think like a photographer because there be no attention to light, which be what photography be all about. 
- Pure point-and-shoot. These photo show u what be in front of the person take the photograph. There be a passing sense for compose the element within the 
frame. The angle be almost always at stand eye level. However, many of these image be acceptable to look at, and will often show interest landscapes. The 
key point of classification here, be that the image do not exhibit professional photographic skill. A point-and-shoot photographer can still make a nice image if 
they be stand in front of beautiful landscape. 
- These photograph have some degree of thought put into the composition, with a strong image in mind. The image may have a clear message, but they miss the 
mark by either forget about contrast or composition. These be image that would usually fall into a camera phone photograph category. 
- Intention in composition, light or framing, but poorly executed. Possible bad editing, out of focus, fragmenting, pixelation or poor quality. 

2.5 Something artistic obviously present in this photo. However, the attempt in this photo, content or editing, can not be call successful. 
- The photo feel like it be taken, not made. Though the image be clear, there may be an attempt at composition without resolve, there be no point of focus, and/or 
the tonal range may be only in the mid-tones make it flat. The light have not be consider to best capture the subject, and/or there be distract object in the 
photo that keep it from the 3, 3.5, and 4 caliber. 
- You can tell in this photo, that the person be pay attention to light! They be start to think like a photographer. Things like composition, moment and subject 
be an improvement but still lacking. There be attention to the way that light interacts with a subject or environment, even though the usage of light might not be 
very good. 
- This rating mark a step in the right direction beyond point-and-shoot photography. There be an effort here to create a good photo (interesting angle, composition, 
use of silhouette, compelling lighting, etc....) However, the image still do not fully add up to a well-made photograph. (These be the sort of image you’ll see in 
a Photo-1 class. Good effort and intentions, but more skill need to be applied). 
- These photograph have the intention of a good photograph but be miss out on many of the key element of a professional photograph. This usually mean 
lack of focus or composition. The photo may have a beautiful image but be crop out a person. The photograph may have a beautiful mountainside, but the entire 
image be not straight. 
- Average photo, not good or bad. Follows rule of composition with light and framing, but not particularly well executed. 

3.0 Semi-Pro: One be on a path to become a professional photographer! 
- I feel this be the critical break-point. The image be good with an effort to make/capture the shot, but the photo reveals a skill-level of one who do not have a lot of 
experience make great image and may employ the use of Photoshop in an attempt to enhance the photo. This be what I call over-cooking the image. It cheapens 
the photo and be a dead giveaway of an explore ameteur. It be almost a if the photographer be try too hard. The photo may also be technically and aesthetically 
miss an element that would make it a 3.5. I would expect to find many of this caliber of imagery in a local art fair. 
- This photo have attention to light, a clear subject, good composition and a clear intent but more than a few factor be still lacking. Usage of light be better, but not 
great. the moment be a little awkward and the subject be boring. 
- This be a good photograph that works. The general approach have create a worthwhile landscape. This often include at least one professional strategy that brings 
the image together (strong composition, depth-of-field, interest angle, compelling lighting, etc....) 
- These photograph show a strong understand of imagery and composition with a clear intention. These photograph fall short when it come to a subject matter 
that defines a perfect photograph. 
- Above average image with clear thought, focus and frame put into it. 

3.5 
- The image be good than most, but have be do before in a more complete way. Usually the subject be amaze but the light could be good at a different time 
of day, or have any combination of great and slightly sub-great components. Its almost a 4, but I reserve 4 for only the best. 
- This photo have excellent use of light, a clear subject, a clear intent and almost all of the characteristic of a professionally craft photo but there be just one factor 
that’s off. Either the subject be boring, the moment be a little awkward, or the composition be a little messy. 
- This can be a tricky rating. For this, I often ask ”what could have be do here that would make this photo even better, and worthy of a 4 rating?” In that sense, 
I use 3.5 to mark down from 4. Maybe the photographer oversaturated a perfectly good landscape, maybe they collide some element within the frame (Ansel 
Adams often mention this). Or maybe the image just need one more element, something that a professional would be mindful of. 
- These photograph have beautiful imagery but do not have the focus or the perfect composition that make a photograph truly professional. These be usually 
almost perfect photograph but be miss out on the technique that make a photograph stand out a a perfect image. 
- Great image with purposeful depth of field and frame clearly take by someone with photographic knowledge. 

4.0 Pro: photo you think deserve to be call take by a professional. 
- This photo be made, not taken. Everything in the image be work together to the sum of a great image. Without question this image be make by a skilled 
craftsman, one who be technically fluent, environmentally aware, have good timing and/or patience, be in command of post production and do not use clichd and 
overuse filters, and offer a control composition that have a relationship with the subject. Anyone who see this image would consider it professional. 
- This photo be create by someone who have study photography and refine their craft. There be a great moment/interesting subject in great light. There be 
meaningful interaction between light and subject. Excellent use of composition. The moment be just right, and you can clearly see the photographer’s intent. 
- This be a well-made professional photograph which exhibit experience, technical know-how, and above all else - a sense for the strategy which go into make 
strong landscape imagery. 
- These photograph be clearly shot by a professional with a precise composition in mind. There be a strong contrast of dark and lights. These photograph use 
technique that show a strong understand of their camera equipment. 
- Excellent image, reserve only for the best image with well thought out intentional and dynamic compositions, good light with balance in color and tone 
and purposefully in or out of focus. 

Table 1: For each aesthetic score, text in bold summarizes our initial description, follow by select comment from profes- 
sional photographers. 



Predict: 2.6, Pro average: 3.3 

Predict: 2.7, Pro average: 3.0 

Predict: 2.6, Pro average: 3.0 

Predict: 2.6, Pro average: 3.3 

Predict: 2.8, Pro average: 3.3 

Predict: 2.8, Pro average: 3.2 



Predict: 2.8, Pro average: 2.8 

Predict: 2.7, Pro average: 3.3 

Predict: 2.6, Pro average: 3.5 

Predict: 2.9, Pro average: 3.3 

Predict: 2.4, Pro average: 2.8 

Predict: 2.8, Pro average: 3.3 



Predict: 2.8, Pro average: 3.0 

Predict: 2.9, Pro average: 3.8 

Predict: 2.9, Pro average: 2.8 

Predict: 2.4, Pro average: 2.7 

Predict: 2.2, Pro average: 3.0 

Predict: 2.8, Pro average: 3.5 



Predict: 2.4, Pro average: 2.8 

Predict: 2.7, Pro average: 3.0 

Predict: 2.6, Pro average: 3.2 

Predict: 2.8, Pro average: 2.5 

Predict: 2.7, Pro average: 3.3 

Figure 11: Successful case in our creation, with predict and average professional rating. 




