












































Focal versus distribute temporal cortex activity for speech sound category assignment 


Focal versus distribute temporal cortex activity for 
speech sound category assignment 
Sophie Boutona,b,c,1, Valérian Chambona,d, Rémi Tyranda, Adrian G. Guggisberge, Margitta Seecke, Sami Karkarf, 
Dimitri van de Villeg,h, and Anne-Lise Girauda 

aDepartment of Fundamental Neuroscience, Biotech Campus, University of Geneva,1202 Geneva, Switzerland; bCentre de Recherche de l′Institut du 
Cerveau et de la Moelle Epinière, 75013 Paris, France; cCentre de Neuro-imagerie de Recherche, 75013 Paris, France; dInstitut Jean Nicod, CNRS UMR 8129, 
Institut d’Étude de la Cognition, École Normale Supérieure, Paris Science et Lettres Research University, 75005 Paris, France; eDepartment of Clinical 
Neuroscience, University of Geneva – Geneva University Hospitals, 1205 Geneva, Switzerland; fLaboratoire de Tribologie et Dynamique de Systèmes, École 
Centrale de Lyon, 69134 Ecully, France; gCenter for Neuroprosthetics, Biotech Campus, Swiss Federal Institute of Technology, 1202 Geneva, Switzerland; 
and hDepartment of Radiology and Medical Informatics, Biotech Campus, University of Geneva, 1202 Geneva, Switzerland 

Edited by Nancy Kopell, Boston University, Boston, MA, and approve December 29, 2017 (received for review August 29, 2017) 

Percepts and word can be decode from distribute neural activity 
measures. However, the existence of widespread representation 
might conflict with the more classical notion of hierarchical 
processing and efficient coding, which be especially relevant in 
speech processing. Using fMRI and magnetoencephalography dur- 
ing syllable identification, we show that sensory and decisional 
activity colocalize to a restrict part of the posterior superior 
temporal gyrus (pSTG). Next, use intracortical recordings, we 
demonstrate that early and focal neural activity in this region 
distinguishes correct from incorrect decision and can be machine- 
decode to classify syllables. Crucially, significant machine decode 
be possible from neuronal activity sample across different 
region of the temporal and frontal lobes, despite weak or absent 
sensory or decision-related responses. These finding show that 
speech-sound categorization relies on an efficient readout of focal 
pSTG neural activity, while more distribute activity patterns, 
although classifiable by machine learning, instead reflect collateral 
process of sensory perception and decision. 

categorical perception | speech sound | encode | decode | 
multivariate pattern analysis 

The discovery of spatially distribute cortical representations,exploitable for “mind reading,” in all domain of cognitive 
neuroscience during the past decade (1–5) raise fundamental 
issue about the nature of neural cod in the human brain. 
These findings, show that the stimulus present in our envi- 
ronment or mentally evoke be represent in distribute 
neural activity, be lead scientist even to reconsider the no- 
tion of local computational units, such a canonical microcircuit 
(6, 7). However, whether all the information that be encode and 
decodable in our brain contributes to our perceptual represen- 
tations and our decision remains an important issue in neuro- 
science. The relevance of this question be exemplify by the 
extreme scatter and redundancy of word-meaning represen- 
tations throughout the brain that be recently show use voxel- 
wide model of fMRI data (8). Decoding model probe 
multidimensional statistical dependency between experimental 
condition or stimulus feature and spatiotemporal activity pat- 
tern distribute across voxels/neuronal population require 
careful interpretation (9–11). Distributed neural activity pattern 
could be take to indicate either that the information they 
contain be critical to cognitive operation or simply that they 
could be use a such, e.g., for stimulus categorization (12). 
However, the sensitivity of decode model apply to neuro- 
physiological data and the multidimensional feature they rely on 
to give positive result do not necessarily parallel the capacity of 
our brain to make use of these neural pattern and multidi- 
mensional feature in specific task (11, 13–15). Data-driven 
result arise from multivariate decode model might lead 
u to conclude that spatially distribute activity pattern be use 
for perform cognitive operation when in fact they might only 

follow from these operations, reflect associative processes, or 
arise from processing redundancy. This concern be relevant at any 
scale, consider that the implicit assumption behind multivar- 
iate decode method be that there be functional meaning in the 
geometry of the decode pattern, whether this pattern be deco- 
ded across individual neuron or across voxels contain several 
hundred thousand neurons. 
Interpreting broadly distribute spatial map for speech 

sound can be particularly difficult. Unlike visual stimuli, whose 
identity relies heavily on spatial encoding, speech sound identity 
relies mainly on temporal encode (16, 17). Despite the rele- 
vance of hierarchical temporal processing in speech perception 
(18), wide cortex coverage with fMRI and more recently with 
electrocorticography (ECoG) indicates (i) that the original 
acoustic speech signal can be reliably reconstruct from broadly 
distribute high-frequency neural activity sample cross-regionally 
throughout the superior temporal lobe (19–21) and (ii) that local 
phonemic-identity information in speech be poorly encode by 
temporally resolve neural activity (2) but be finely represent 
by distribute cortical pattern cover a significant portion 
of the left temporal lobe (1). Because optimal decode occurs 
when redundant information from contiguous but functionally 
distinct territory be pool together, assign perceptual rele- 
vance to such large-scale representation be ultimately tricky and 

Significance 

When listen to speech, phoneme be represent in a dis- 
tributed fashion in our temporal and prefrontal cortices. How 
these representation be select in a phonemic decision con- 
text, and in particular whether distribute or focal neural in- 
formation be require for explicit phoneme recognition, be 
unclear. We hypothesize that focal and early neural encode of 
acoustic signal be sufficiently informative to access speech sound 
representation and permit phoneme recognition. We test this 
hypothesis by combine a simple speech-phoneme categoriza- 
tion task with univariate and multivariate analysis of fMRI, 
magnetoencephalography, intracortical, and clinical data. We 
show that neural information available focally in the temporal 
cortex prior to decision-related neural activity be specific enough 
to account for human phonemic identification. 

Author contributions: S.B. and A.-L.G. design research; S.B. perform research; S.B., 
V.C., R.T., and A.-L.G. contribute new reagents/analytic tools; S.B., V.C., R.T., and A.-L.G. 
analyze data; S.B., V.C., R.T., A.G.G., M.S., S.K., D.v.d.V., and A.-L.G. write the paper; 
A.G.G. and M.S. recruit the patients; and S.K. design the stimuli. 

The author declare no conflict of interest. 

This article be a PNAS Direct Submission. 

This open access article be distribute under Creative Commons Attribution-NonCommercial- 
NoDerivatives License 4.0 (CC BY-NC-ND). 
1To whom correspondence should be addressed. Email: sophie.l.bouton@gmail.com. 

This article contains support information online at www.pnas.org/lookup/suppl/doi:10. 
1073/pnas.1714279115/-/DCSupplemental. 

www.pnas.org/cgi/doi/10.1073/pnas.1714279115 PNAS Early Edition | 1 of 10 

N 
EU 

RO 
SC 

IE 
N 
CE 

PN 
A 
S 
PL 

U 
S 

http://crossmark.crossref.org/dialog/?doi=10.1073/pnas.1714279115&domain=pdf&date_stamp=2018-01-23 
https://creativecommons.org/licenses/by-nc-nd/4.0/ 
https://creativecommons.org/licenses/by-nc-nd/4.0/ 
mailto:sophie.l.bouton@gmail.com 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
www.pnas.org/cgi/doi/10.1073/pnas.1714279115 


might conflict with the notion that speech sound be first spectro- 
temporally encode in auditory cortex before be more abstractly 
recode in downstream area (22). Accordingly, focal lesion of the 
temporal lobe can selectively impair different speech-perception 
process (23), and recent study in monkey even show that au- 
ditory decision-making causally relies on focal auditory cortex 
activity (24). 
Listening to speech have long be know to elicit brain activity 

across both temporal and frontal cortex (25). Whether this ac- 
tivity reflect the use of distribute representation of phoneme 
a probed with decode model or focal and selective hierarchi- 
cal processing a probed with encode model be incompletely 
understood. It be questionable whether all neural activity that 
contributes to spatially distribute patterns, with their specific 
geometry, reflect a neural code use to perform specific cognitive 
operation (9, 11, 13), i.e., assign a speech sound to a category, 
or multiple/redundant code that be differentially use depend- 
ing on specific task and cognitive operations. In other words, 
although multivariate pattern analysis and model-based decode 
technique be promising in translational applications, for instance 
to decode inner speech in aphasic patient (26), their interpreta- 
tion in cognitive neuroscience depends on the neurophysiological 
relevance of the model apply for decode (27). For instance, 
demonstrate that a particular stimulus attribute or category can 
be decode from regional neuronal activity do not imply that 
the region be perform categorical processing. In summary, there 
be a conceptual discontinuity between machine and brain decode 
of neuronal activity. As an extreme example, demonstrate that 
phoneme could be classify use a multivariate machine- 
learn scheme apply to primary sensory afferent from the 
ear would not mean that the brain have yet decode these signal 
but only that there is, by definition, sufficient information in this 
auditory input to support subsequent hierarchical decoding. 
Using a combination of behavioral, fMRI, magnetoence- 

phalography (MEG), and ECoG data, we attempt to clarify this 
issue by assess the ability of a classifier to decode the stimulus 
category from neuronal response at various level in the auditory 
hierarchy and the ability of a linear model to estimate from neural 
response the perceptual process in a paradigm for assign 
speech sound to categories. We found that the assignment of 
speech sound to category rely on focal neural activity pre- 
sent in a circumscribed part of the auditory hierarchy, at specific 
peristimulus times, which be support by the observation that 
the task could not be perform in a patient with a selective lesion 
of this circumscribed region. Nevertheless, multivariate machine 
decode return positive result from a large brain network 
include region where no stimulus-related evoke activity could 
be detected, a find that, in isolation, could suggest that category 
assignment involve a distribute pattern of activity. 

Results 
We first explore explicit phoneme recognition use a simple 
syllable-categorization task and measure global neural activity 
with fMRI and MEG in 16 and 31 healthy volunteers, re- 
spectively (Methods and SI Text). The subject have to decide 
which syllable they heard in a /ba/ /da/ continuum in which the 
onset value of the second formant (F2) and the F2 slope linearly 
covaried in six step (Fig. 1A). These two first experiment serve 
to delineate at the whole-brain level those brain region that 
be sensitive to (i) linear variation of F2 and (ii) perceptual 
decisional effort a assess use behavior-based negative d′ 
value (Figs. 1B and 2A) (Methods and SI Text). Critically, be- 
cause the slope of the second formant be steeper for the /da/ than 
for the /ba/ phoneme, we expect the /da/ stimulus to activate a 
large cortical surface than the /ba/ stimulus and hence to be 
associate with a strong blood oxygenation level-dependent 
(BOLD) effect (SI Text). Both experiment converge to show 
that F2 variation be specifically tracked by neural activity in the 
right posterior superior temporal gyrus (pSTG), while perceptual 
decisional effort involve several region of the bilateral inferior 
prefrontal and posterior temporo/parietal cortex and the right 

anterior temporal pole (Figs. 1C and 2B and Fig. S2D). These 
activations, in particular the acoustic encode of F2 variations, 
remain focal even at a lenient statistical threshold (Fig. S1). 
The spatial selectivity of the acoustic track of F2 be con- 
firm by a second fMRI study in which participant have to 
decide whether they heard /da/ or /ta/. In this case the morph 
acoustic cue be no longer spectral (F2) but temporal (voice- 
onset time, VOT). We found that this acoustic cue be encode 
in a restrict region of the left superior temporal gyrus (STG) 
and superior temporal sulcus (STS) (SI Text and Fig. S2). In 
short, the right pSTG be recruit for encode the slope of the 
second formant in the ba–da continuum, and the left STG/STS 
be recruit for encode the duration of the consonant part in 
the da–ta continuum, reflect the hemispheric dominance for 
temporal vs. spectral acoustic processing (28). 
We use dynamic source model of the MEG data to ex- 

plore the dynamic of acoustic encode and perceptual decision. 
We found neural correlate of F2 parameter encode 120 m 
post stimulus onset in the right pSTG. Auditory perceptual 
decision-related activity appear in this region at 165 m and 
co-occurred with a second peak of F2 encode activity at 175 m 
(Fig. 2B). In addition to the spectral response profile within the 
right pSTG and left prefrontal cortex, a Granger causality (GC) 
analysis across the two area show that neural activity related 
to F2 and negative d′ (−d′) correspond to bottom-up encode 
and top-down decode activity, respectively. Both analysis be 
associate with neural activity in the high-gamma band for 
F2 variation and in the beta band for −d′, confirm the generic 
implication of these two frequency range in bottom-up and top- 
down process (Fig. 2C) (29–31). Here, we related decisional 
effort with the top-down process, in line with a predictive cod 

Fig. 1. fMRI results. (A) Spectrograms of the stimulus continuum between 
syllable /ba/ and /da/, synthesize with a linear increase in F2-parameters 
(1,650:100:2,150 Hz). Full spectrogram at the extreme of the continuum 
represent /ba/ (Left) and /da/ (Right) prototype syllables. Spectrograms in the 
middle be center on the F2 parameters. (B) Values for F2 parameter 
(iblue; Left), average d′ (red; Center), and percent of syllable identify a 
/ba/ (gray; Right). Data be show a mean ± SEM. (C) Results of the re- 
gression analysis. (Left) Percent signal change in the right pSTG. (Center, 
right hemisphere) Spatial localization of F2 parameter for neural encode 
(blue) and d′ (red) in the fMRI BOLD signal, express a beta coefficients. 
Significant cluster be found in the right pSTG (peak MNI coordinates, x, y, 
z = 42, −34, 7, T = 3.21) for the F2 track and in left posterior temporo- 
parietal (x, y, z = −51, −28, 16, T = 4.41) and bilateral inferior prefrontal (x, y, 
z = 45, 17, −5, T = 5.26; x, y, z = −48, 8, 22, T = 5.29) cortex for auditory 
perceptual decision, d′). Images be present at a whole-brain threshold of 
P < 0.001. (Right) Percent signal change in the left inferior prefrontal cortex. 
The BOLD signal increase with F2 parameter in the right pSTG and with 
auditory perceptual decision load in the left inferior prefrontal region. 

2 of 10 | www.pnas.org/cgi/doi/10.1073/pnas.1714279115 Bouton et al. 

http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
www.pnas.org/cgi/doi/10.1073/pnas.1714279115 


view of speech processing (32, 33), whereby the more ambiguous 
the acoustic input, the strong be the top-down predictions. 
As top-down and bottom-up signal be thought to be predomi- 
nantly associate with neural activity in the beta and gamma band, 
respectively, we probed the dominant frequency of informa- 
tion transfer between the inferior frontal gyrus (IFG) and pSTG 
use GC. Our result confirm that beta activity dominate from 
the IFG to the STG and that gamma activity dominate in the 
other direction (29, 31, 34, 35). The MEG finding thus support the 
straightforward scenario in which phoneme categorical decision 
could arise from a rather focal readout of the region that encodes 
the critical sensory cue (F2) by prefrontal region (36–38). 
Having establish the global representational validity of the 

region encode the sensory feature of interest (F2 variations), 
we then sought to examine the response of these region at a 
finer-grained scale use invasive electrophysiology. To maximize 
signal-to-noise ratio and spatial specificity in the exploration of 
coincident neural response to F2 and to auditory perceptual 
decision, we acquire intracortical EEG (i-EEG) data in three 
epileptic patient who together have 14 electrode shaft through- 

out the right temporal lobe (70 contacts). Among the electrode 
shafts, one penetrate the right temporal cortex through 
Heschl’s gyrus (Fig. 3B). The deepest contact of this auditory 
shaft strictly colocalized with the region that fMRI detect for 
F2 variation tracking. The patient perform the same syllable- 
categorization experiment on a ba/da/ga continuum in which the 
only change acoustic cue be the F2 parameter (Fig. 3A). 
Behavioral result show a good detection of ba and da and a 
slightly less frequent detection of ga (Fig. 3C). Strong evoke 
response to syllable be present only in the auditory shaft and 
be more marked/consistent in it two deepest contact (Fig. 
3D, Top Row); the response be weak to nonexistent elsewhere 
(Fig. 4A, color plots). Significant F2 track be consistently 
detect in all auditory contact (Methods), with strong and 
structure effect in the two deepest one (Fig. 3D, Middle Row). 
Fully consistent with the MEG results, F2 value be encode 
by broadband gamma activity (40–110 Hz) from about 150 m 
poststimulus onset onward, i.e., 50 m after F2 appear in the 
acoustic signal. Structured and strong neural activity related to 
F2 track be not observe in any of the other contact of the 
same patient (patient 1) (Fig. S5). These data suggest that the 

Fig. 2. MEG results. (A) Values for F2 parameter (blue, Left), average d′ 
(red, Center), and percent of syllable identify as/ba/ (gray, Right); data be 
show a mean ± SEM. (B, right-hemisphere) Dynamic spatial localization of 
the neural encode of F2 (blue) and d′ (red) in MEG signals, express a 
beta coefficients. Only the bootstrapped P = 0.05 significance threshold 
(Bonferroni-corrected) activation be represented. The right pSTG (in- 
dicated by black arrows) be first activate at 95–120 m for encode 
F2 parameter and then be reactivate for phonemic decision at ∼165 ms. 
(C, Upper) Spectral profile of beta coefficient from regression between 
F2 value and neural response in the right pSTG (Left), and between −d′ 
value and neural response in the left inferior prefrontal area (Right). F2 be 
dominantly tracked by gamma and high-gamma activity, whereas decisional 
activity be express in the low beta band. Thick black line indicate signif- 
icant beta coefficient at P < 0.05 (Bonferroni-corrected). (Lower) GC result 
between the right pSTG and the left IFG. Thick black line indicate significant 
Granger coefficient at P < 0.05 (Bonferroni-corrected). Shaded gray area 
highlight the correspondence between beta coefficient and GC peaks: high- 
gamma band for bottom-up activity from the right pSTG to the left IFG (Left); 
beta band for top-down activity from the left IFG to the right pSTG (Right). 

Fig. 3. i-EEG result in patient 1. (A) Spectrograms of /ba/, /da/, and /ga/ 
prototype stimulus synthesize with linear parametric F2 parameters. (B) Lo- 
cation of i-EEG contact in patient 1. The auditory shaft label shaft 
1 penetrate the right pSTG and Heschl’s gyrus. The patient have five other 
shaft distribute in the right temporal lobe. Bipolar montage from adja- 
cent contact be show in the Bottom Right figure. (C) Percentage of syl- 
lable identification for each category for the three patients. The shade 
zone indicates the SEM. The first 20 stimulus be categorize a /ba/, the next 
13 stimulus be categorize a /da/, and the last 11 stimulus be categorize 
a /ga/. (D, Top) Evoked activity, average across stimuli, on each bipolar 
montage from the deepest contact (number 1) to the most external contact 
(number 5) in the auditory shaft (shaft 1). (Middle) Time–frequency repre- 
sentations of beta coefficient from the regression of F2 value against evoke 
activity on each contact of the auditory shaft. Significant F2 track be 
found in all contact of the auditory shaft, with strong effect in the two 
deepest contacts. (Bottom) Time–frequency representation of beta coeffi- 
cients from regression of d′ value against evoke activity on each contact of 
the auditory shaft. Decisional effect be significant on the third auditory 
contact about 200 m poststimulus onset in the beta band. The vertical dash 
line indicate stimulus onset. The horizontal dash line indicate a change in 
the scale of the oscillatory power for each time point and each frequency, 
with a 0.5-Hz resolution below 20 Hz and a 1-Hz resolution above 20 Hz. Black 
contour indicate significant t test at q < 0.05 (FDR correction). 

Bouton et al. PNAS Early Edition | 3 of 10 

N 
EU 

RO 
SC 

IE 
N 
CE 

PN 
A 
S 
PL 

U 
S 

http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 


encode parameter of the discriminant acoustic cue be avail- 
able in the right pSTG for syllable recognition, confirm the 
spatial selectivity for F2 parameter encode in this region. 
Decisional effect be globally weak in i-EEG signal but 

be significant in the third auditory contact about 200 m 
poststimulus onset in the beta band, in agreement with the MEG 
results, and in the deepest auditory contact about 350 m post- 
stimulus onset in the gamma band (Fig. 3D, Bottom Row). Be- 
cause both fMRI and MEG show correlate of decisional 
effort at several other location in the frontal and temporal lobe, 
we broaden the analysis in this patient to all contact of each 
shaft (Fig. S6). Perceptual decision-related effect be weak, 
sporadic, and inconsistent. They be significant before 500 m 
poststimulus at only two other location outside Heschl’s gyrus: 
in the right inferior prefrontal cortex (shaft 6, contact 1; con- 
sistent with fMRI) (Fig. 1C) and in the anterior temporal lobe 
(shaft 4, contact 4; consistent with MEG) (Fig. 2B). 
We then sought to address whether focal neural activity could 

afford syllable categorization. In line with previous finding 
base on ECoG signal (1, 2), local evoke activity from one 
contact be sufficiently discriminable to permit syllable catego- 
rization use a machine-learning algorithm (maximum correla- 
tion coefficient classifier; see Methods). Decoding be possible 
from all individual auditory contact but work best from the 
deepest one (Fig. 4A and Fig. S9A). Within the other electrode 
shafts, univariate decode base on single-contact information 
be never possible. However, significant multivariate decode 
from pool all contact in each shaft be significant for shaft 
1, 2, 3, 4, and 6, even though it include nonresponsive contacts. 
Reciprocally, multivariate decode be not possible in the 
temporal pole shaft (shaft 5), even though we detect signifi- 
cant perceptual decision-related neural activity in this region 
with fMRI and MEG. 

We subsequently address the key question whether the in- 
formation use by the classifier correspond to that use in the 
human decisional process. We examine whether there be a 
temporal correspondence between the dynamic of decoding, a 
assess by time-resolved classification (39, 40), and the presence 
of time–frequency neural cue that inform the subject’s per- 
ceptual decision. For this analysis, to ensure the independence of 
the analyze dataset (SI Text), we no longer probed the decisional 
effort (the search for information, −d′) but the decisional out- 
come. We approximate the neural cue that be critical to the 
decisional outcome by the difference in the time–frequency re- 
sponses of correctly and incorrectly recognize prototype syllables. 
The correct-minus-incorrect contrast indicates the part of the 
neural signal that, if missing, be associate with an erroneous 
perceptual decision. Note that this contrast matches, a closely a 
possible, the output of the maximum-correlation coefficient clas- 
sifier, which test the extent to which a linear association can 
correctly predict syllable from neural activity. 
Significant time–frequency correlate of correct classification 

be found only in the three deepest contact of the auditory 
cortical shaft (Fig. S7); they be sporadic and inconsistent 
elsewhere (red frame in Fig. S7 show significant activity for t < 
500 ms). In the deepest auditory contact (contact 1 on shaft 1), 
where both F2 track and univariate classification be maxi- 
mal (Fig. 3 and Fig. S8), cue associate with correct perceptual 
decision be present a early a 150 ms, i.e., before the first 
significant decode peak at 200 m (Fig. 4B and Figs. S6 and 
S9B). This important find show that within 150 m the right 
pSTG have encode enough information about F2 onset fre- 
quency and slope to inform correct syllable recognition by the 
subject and that this information could be exploit by the 
classifier to distinguish across syllable (Discussion). 

Fig. 4. Decoding in patient 1. (A, right hemisphere) Colored panel show time–frequency representation of evoke activity on each shaft. A strong evoke 
response to syllable be present only in the auditory shaft. Bar graph show neural decode through univariate and multivariate classifiers. Histogram bar 
numbered from 1–5 show the univariate classifier result base on the activity from each contact of each shaft. The bar at the far right (“all”) show the mul- 
tivariate classifier result base on multidimensional information from all contact of each shaft. Stars above the black bar signal significant classification accuracy 
for specific contact within each electrode shaft (q < 0.05, FDR-corrected). Univariate classification be possible from all auditory contact of shaft 1 overlap 
fMRI F2 parameter activation (blue-shaded area) but work best in the deepest one. Univariate classification fail everywhere except for these auditory 
contacts, whereas syllable decode work above chance use the multivariate approach in shaft 1, 2, 3, 4, and 6. (B, Bottom) Time–frequency difference 
between correct-minus-incorrect classification compute on contact 1 of shaft 1. Black border indicate significant difference in neural activity between correct 
and incorrect classification scores, in comparison with a zero-mean normal distribution at q < 0.05, FDR-corrected. (Top to Bottom) Temporal relationship between 
the time course of machine decode from the deepest auditory contact (Top, black line), mean univariate classification from all auditory contact (Top, gray line), 
and the time–frequency cue use by the subject to make a correct perceptual decision (difference in the time–frequency response between correctly and in- 
correctly recognize syllables, Bottom). The thick gray line show significant result for each time point (significant decode accuracy, q < 0.05, FDR-corrected). 
(Middle) Cross-correlation coefficient between univariate decode accuracy and significant correct-minus-incorrect time–frequency clusters. Significant effect 
be found in the 60- to 80-Hz high-gamma band. The horizontal black line indicates significant cross-correlation coefficient at P < 0.05 (Bonferroni-corrected). 

4 of 10 | www.pnas.org/cgi/doi/10.1073/pnas.1714279115 Bouton et al. 

http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
www.pnas.org/cgi/doi/10.1073/pnas.1714279115 


Critically, the temporal coincidence between neural correlate 
of response correctness and machine decode (Fig. 4B) be only 
partial. It be fairly good for the first two significant decode 
peak (<200 ms) of both single auditory contact decode and 
mean univariate decode across all auditory contact but be 
poor for the late (and strongest) peak. The first two decode 
peak coincide precisely with transient high-gamma activity in 
the 60- to 80-Hz range (significant zero-lag cross-correlation) 
(Fig. 4B), in line with the observation make with MEG (Fig. 2) 
that F2 be specifically encode by neural activity in this fre- 
quency range and that 60- to 80-Hz activity precede decisional 
activity in the left IFG. However, the third decode peak have no 
match time–frequency event in the correct vs. incorrect activity 
profile (Fig. 4B). These observation indicate that the classifier do 
not systematically capture those neural cue that inform the 
subject’s decision. Thus, machine classification and human sub- 
jects do not exploit the same cue locally. Presumably, the out- 
come of the mean univariate classifier reflect distribute 
information that be no longer relevant for or assimilate into 
neuronal decision variables. The strong local decode peak 
occur at 370 ms, i.e., more than 250 m late than the first 
correctness effect and about 100 m after the last one and likely 
reflect postdecisional choice-correlated neural activity. 
So far, the result indicate that the phonemic decision be 

inform by focal early neural activity (<200 ms) that encodes 
F2 in sustain multifrequency oscillatory activity (Fig. 2). 
However, distribute subthreshold neural activity not detect 
by conventional (univariate) analysis of fMRI, MEG, and 
intracortical EEG data also might contribute to syllable-identity 
encoding. We therefore address whether decode be possi- 
ble even from contact where there be no detectable F2 and 
from perceptual decision-related activity (Figs. S5 and S6). We 
broaden the analysis to the 14 shaft of the three patients, 
include two additional patient who have electrode shaft over 
the right temporal lobe (n = 14), and perform time-resolved 
multivariate decode from all cortical contact (n = 36). Sig- 
nificant decode be found at 250, 300, and 600 ms, show 
that syllable could be decode from broadly distribute activity 
(Fig. 5). To address whether the distribute pattern of activity 
be driven by local auditory activity, we perform the same 

analysis without the contribution of the auditory shaft of patient 
1. Early classification (<300 ms) drop below statistical sig- 
nificance, but the late classification peak at 600 m remain 
unaffected (Fig. 5). This result demonstrates that decode 
remain possible from cortical contact that show neither F2- 
nor auditory perceptual decision-related activity. We even 
obtain significant late decode when deep structures, such a 
the amygdala and the hippocampus, be include in the mul- 
tivariate analysis (n = 70 contacts). As each penetrate shaft, 
except the auditory one, span functionally different territo- 
ries, from the cortex to deeper structures, these finding show 
that the possibility of decode neural activity in a multivariate 
approach do not allow one to conclude that the region sam- 
plead for decode amount to a meaningful neuronal represen- 
tation, define operationally in term of a correct perceptual 
categorization. Overall, classification analysis from the i-EEG 
data confirm the spatial selectivity of the early critical in- 
formation involve in ba/da/ga syllable categorization. They also 
show that syllable classification be possible from distribute 
activity (Figs. 4 and 5) that occur late than the perceptual 
decision-related effects, a detect with both MEG and i-EEG. 
Since the decode of i-EEG return positive result when 

contact in which no significant neural activity could be detect 
be pool together, we sought to explore the spatial distribution 
of /ba/ and /da/ category decode use the MEG dataset. The 
idea be to determine whether whole-brain decode would be 
restrict to region that show statistically significant activa- 
tion with all three approaches, MEG (Fig. 2), fMRI (Fig. 1), and 
i-EEG (Figs. S5 and S6), or would also be possible in region that 
do not critically participate in the task. This analysis be expect 
to provide time-resolved information to appraise whether decode 
reflect noncritical process downstream of sensory encode and 
early decisional steps. Such a find would concur with the i-EEG 
result suggest that decode be possible from brain region that 
be only collaterally involve in the cognitive process at stake. 
Using whole-brain MEG sensor data and a time-resolved 

multivariate learn algorithm (maximum correlation coeffi- 
cient classifier) (Methods) (41), we found that speech sound 
category could be decode from very early brain response in a 
focal region of the right pSTG (Fig. 6). When we focus our 

Fig. 5. Decoding in all patients. Time course of the decode accuracy from multivariate pattern classification with all shaft (Top) and without the auditory 
shaft of patient 1 (Middle). Early classification drop below statistical significance, while the late classification peak at 600 m remain unaffected. 
(Bottom, right hemisphere) Location of shaft (n = 14) from which neural activity be record during syllable identification (three patients, fixed-effects 
model). Colored dot show cluster-level significance from q > 0.10 to q < 0.01 (q-FDR corrected) multivariate classification perform on all shafts. Dot size be 
proportional to q. Significant classification be observe at 250, 300, and 600 ms, show that syllable could be decode from broadly distribute activity. 
At 300 ms, P1 refers to patient 1, and P2 refers to patient 2. 

Bouton et al. PNAS Early Edition | 5 of 10 

N 
EU 

RO 
SC 

IE 
N 
CE 

PN 
A 
S 
PL 

U 
S 

http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 


analysis on those sensor that contain significant information 
about syllable identity (Methods), we found that the activity 
record by the sensor MAG 1331 could be categorize a early 
a 100 ms, with up to 78% accuracy (t = 13.01, q < 0.001, Cohen’s 
d = 4.7). Critically, syllable identity could also be decode at 
late time points, first at 220 m on the sensor MAG 2011 and 
then at 350 m on the sensor MAG 0211, with score reach 
63% accuracy (t = 7.17, q < 0.001, Cohen’s d = 2.5) and 58% 
accuracy (t = 6.37, P < 0.001, Cohen’s d = 2.2), respectively. 
Corresponding source analysis then reveal that the decode 
at 220 m poststimulus onset arose from the left STG and STS, 
two region that be not primarily responsive to acoustic cue 
track and perceptual decision (Fig. 2). Source analysis further 
show that the decode at 350 m arose from the left IFG and 
thus correspond to late decisional effects. Together, these 
result show that, while decode be most accurate in the region 
that critically encodes the acoustic information (the right pSTG), 
it be also subsequently possible from noisier activity in a broad 
left-lateralized network that contains associative information 
about the select syllable. Interestingly, significant decode 
be see again in the right pSTG at 500 m poststimulus onset 
with very high accuracy (89%, t = 15.10, q < 0.001, Cohen’s d = 
5.5). This suggests that information propagation occur across 
the whole language network between 100 and 500 m contribute 
to improve the quality of the categorical representation at a 
postdecisional stage. 

Discussion 
In this series of studies, we address whether human listeners’ 
perceptual decision about the identity of speech sound arose 
from distribute representation (42) or whether, follow more 
closely the principle of hierarchical processing, syllable cate- 
gorization be inform by the efficient readout of a restrict 
cortical region that contains limited but key neural information, 
a recently show in monkey (24). Our data converge to show 
that correct decision about speech-sound identity be in- 
form by local and time-limited information (<300 ms) present 

in the right pSTG. Even though phonemic contrast be most 
often associate with activation of the left STG (43), the right 
STG performs low-level spectral-based acoustic processing that be 
relevant for speech decode (44, 45) and categorization (46, 47). 
Note that the right specificity of speech-related operation can 
easily be miss when acoustic processing be not explicitly or- 
thogonalized from decision-related neural activity (48). Moreover, 
hemispheric dominance in speech processing depends heavily on 
the task. Here, subject be force to process a single cue 
(spectral or temporal) to categorize ambiguous signals, which do 
not happen under natural listen condition in which contextual 
information and redundant acoustic cue be available. Our find- 
ings confirm that spectral- and time-based analysis of speech 
sound involve the right and left STG, respectively (Fig. 1 and Fig. 
S8). That we observe tonotopic encode of F2 in a region 
contiguous to, rather than within, right Heschl’s gyrus presumably 
reflect that the frequency range span by F2 be rather limited 
and more extensively process in a region specialized for vocalic 
formants (49). Our finding also confirm that in acoustically 
challenge situation such a those use in the current experi- 
mental design, the left IFG be mobilize (50) and interacts with the 
temporal cortex in sequential loop of bottom-up and top-down 
process (48, 51–53). Importantly, our fMRI and MEG result 
conjointly show that the focal readout of sensory encode region 
by prefrontal region account for the decision variability relative 
to the perceptual state and that the magnitude of neural activity 
associate with sensory process depends on the discrepancy 
between heard and expect signal (Figs. 1C and 2C) (54, 55). 
Whether stimulus identity be retrieve from focal neural ac- 

tivity or from distribute activity pattern be a fundamental issue 
in neural cod theory. Our MEG and i-EEG decode result 
both show that the right pSTG be critical for perceptual decisions, 
while distribute activation across the frontal and temporal 
cortex reflect the reuse of sensory information for higher-level 
operations, such a extraction of meaning, audiovisual integra- 
tion, and so forth. It have repeatedly be observe that behavior 
be explain a well by take the activity of one or a limited set of 
neuron a it be by consider a large-scale population (56, 57). 
This puzzle observation be back up by computational model 
show that pool together information from more neuron 
do not improve behavioral sensitivity. This could reflect the 
fact that neuronal response be strongly correlate both in- 
trinsically by neural noise and by stimuli, so that the information 
contain in a neural population saturates a the number of 
neuron increase (58, 59). Recently, a combination of experi- 
mental and theoretical data suggests that in most case behavior 
be best explain by optimal readout of a limited set of neuron 
(60). The author then conclude that the neural code be re- 
dundant and that some area be decode nearly optimally while 
others be not read out efficiently. They propose that deacti- 
vating these region should not affect behavior. 
Reciprocally, a we test in a brain-damaged patient that be 

carefully select with respect to lesion extent and localization (SI 
Text and Fig. S10), deactivate region that be optimally decode 
would be expect to impair behavior. The lesion of one of the 
two focal region that we identify a key for F2-based syllable 
categorization dramatically disrupt performance. The impair- 
ment be selective to the type of stimulus but not to the type of 
task, a categorization of similar syllable (da/ta) base on tem- 
poral cues, i.e., VOT, be spared. This also be expected, a syl- 
lable categorization base on VOT specifically involve the left 
middle temporal cortex (Fig. S2). The selectivity of the impair- 
ment with respect to the acoustic material show that for this type 
of task there be no distribute rescue system. These behavioral 
data in a single patient remain, of course, nongeneralizable, and 
further lesion data will be necessary to confirm our results. 
Moreover, a full demonstration that, even though phoneme cat- 
egories be represent in a distribute fashion, focal/early sensory 
information be predominantly use to categorize speech sound 
would, strictly speak require show that distribute lesion 
do not impair task performance. This demonstration, however, 

Fig. 6. Decoding of MEG data reveals bilateral temporo-frontal cortex in- 
volvement in speech-sound categorization. (A) Percentage of correct 
decode over time for each of the magnetometer (MAG 1331, MAG 2011, 
and MAG 0211) show significant decode activity. On the x axis, zero 
corresponds to stimulus onset; on the y axis, 50% indicates chance perfor- 
mance. Horizontal dark and light gray line indicate significant decode 
(q < 0.05, FDR-corrected). (B) Sensor topography depict the average 
decode response in magnetometer average within each of the five 
window of interest. Black dot indicate the position of the three magne- 
tometers show significant decode activity. (C) Source localization at key 
decode times. Source localization for MEG signal be displayed on a 
standard cortex at 100, 220, 300, 360, and 500 m poststimulus onset. Color 
bar at the right in B and C indicate Student t-test values. 

6 of 10 | www.pnas.org/cgi/doi/10.1073/pnas.1714279115 Bouton et al. 

http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
www.pnas.org/cgi/doi/10.1073/pnas.1714279115 


cannot be performed, because at the extreme, if the whole lan- 
guage system be injured, no linguistic operation can be achieve at 
all. Although, use a variety of approaches, we try to collect a 
many argument a possible to address how information be use to 
make perceptual decision about speech sounds, we acknowledge 
that the present study can be only partly conclusive. 
Critically, however, our result show that a distinction should 

be make between the classification of syllable-induced neuronal 
activity by machine learn and the neuronal decode make 
by the brain to achieve syllable categorization. We found that 
distribute noise-level neural information, which do not carry 
reproducible and statistically detectable information about the 
sensory cue (F2 parameters) or about the categorical decision 
process (61), be nonetheless sufficient to inform a classifier. 
While this confirms previous observation that phonemic in- 
formation be present in a distribute manner (1, 2), the fact that 
classification be possible only on late response once the re- 
gion encode the critical acoustic cue (F2 slope) be remove 
from the multivariate analysis suggests that distribute pho- 
nemic representation reflect the diffusion of the information 
throughout the language system. While distribute information 
might be useful for late association-type linguistic processes, it 
do not appear necessary for the categorization task, a cor- 
rectness effect occur focally and early in time. These finding 
show that neuronal activity contain information about speech 
sound category be not uniformly useful for categorize these 
sounds. More generally, our finding highlight the fact that 
decode algorithms, even if they can make use of distribute 
information that might reflect a brain state context (57) or 
mental association, do not indicate which region be necessary 
or causal for the cognitive process at stake—here a categorical 
choice. These result hence suggest that distribute information 
about category might reflect the redundancy of noise-level in- 
formation in the brain, i.e., associative neural activity, rather 
than a spatial neural code that be access in parallel when 
make a speech-category perceptual decision. 
Categorizing auditory syllable be a relatively low-level process, 

which in theory could be underpinned by distribute represen- 
tations, but in our data this appear not to be the case. What then 
might be denote by previously observe broadly distribute 
representation map (8) and, in particular, by phonemic map 
organize along articulatory feature (62)? Most of these im- 
portant finding (1, 52, 63) be obtain in natural listen 
conditions. In the work by Mesgarani et al. (1), for instance, 
map be drawn from cortical surface neural activity measure 
150 m after each phoneme in connect speech, imply that 
activity at each time point integrate the distribute activity of 
several precede phoneme and reflect coarticulation and 
contextual association from the precede words. When pas- 
sively listen to connect speech, there be likely no explicit 
serial phoneme decode but rather a global readout of senten- 
ces, which likely require superficially access many phonemic 
representation at once. In the same way a a computer keyboard 
space letter to minimize interference when type them, our 
brain might organize the phonemic space a a function of how we 
need to retrieve them for production (54), i.e., follow a feature- 
base logic. That such organization exist do not imply that they 
be use a such for explicit syllable recognition, just a reading 
word through a keyboard spatial organization would largely be 
suboptimal. While the present finding do confirm the existence of 
distribute phonemic representations, they also question the use 
our brain make of them in an explicit speech-perception context, 
a phoneme recognition do not seem to rely on distribute 
pattern of activity. Importantly however, it might be the case that, 
during natural speech perception, cross-hierarchical readout of 
redundant/correlated neural activity be genuinely exploit a a 
form of trade-off between accuracy of single phoneme identifi- 
cation (focal) and joint access to multiple representation (dis- 
tributed). It will be essential in the future to address whether 
suboptimal decode of large neuronal population could be an 
optimal way to handle access to multiple representations. 

Methods 
Subjects. Twenty-eight healthy subject participate in the MEG study (16 fe- 
males; age range: 22–31 y), and 16 participate in the fMRI study (nine fe- 
males; age range: 22–29 y). i-EEG be record in three epileptic patient (one 
female; ages: 44, 25, and 65 y) who underwent surgery for clinical monitoring 
and in one patient (female; age: 77 y) who be test behaviorally 8 mo after 
an ischemic stroke. All participant be right-handed, native French speaker 
and have no history of auditory or language disorders. The experimental 
protocol for the participation in the MEG and fMRI experiment be 
approve by the Inserm ethic committee in France (biomedical protocol C07- 
28), and the protocol for study in epileptic and stroke patient be approve 
by the University of Geneva Hospital in Switzerland (13–224). All participant 
provide write inform consent before the experiment. 

Stimulus Synthesis and Behavioral Testing. High-quality speech stimulus be 
synthesize use an original morph method base on a modify linear 
predictive cod (LPC) analysis synthesis scheme (64). Using an exponential 
pole-morphing approach, the second formant transition be morph to build 
a linear speech sound continuum between /ba/, /da/, and /ga/ (65, 66). The 
initial (prototypical) /ba/, /da/, and /ga/ syllable be natural voice signals, 
down-sampled to 16 kHz, align on their burst start time, and cut to the 
same length (360 ms). From the LPC analysis of natural voice sounds, the 
formant structure be extract for each prototypical syllable. For resynthesis, 
the excitation signal from the LPC analysis be discarded, and an artificial 
excitation signal be used. The excitation signal consist of a low-pass- 
filter pulse train for the voiced part, additional white noise for the burst, 
and a small amount of white noise throughout the entire stimulus. The time 
dependency of the fundamental frequency, f0 = f(t), be a piecewise constant 
and simplify version of that extract from the original /ba/ natural voice 
signal. Throughout the continuum, the excitation signal, the global amplitude 
of the stimulus, and the first and third formant transition be kept constant. 

A six-item /ba/ /da/ continuumwas present to healthy subject and to the 
stroke patient. A longer 48-item continuum, /ba/ /da/ /ga/, be use for test 
epileptic patient to obtain more response around syllable boundary to 
compare correct and incorrect categorization. Note that /ba/ and /da/ cate- 
gories differ only on the F2 dimensions, and hence processing this single 
cue be sufficient for correct perception. Another six-stimuli /da/ /ta/ con- 
tinuum be use for both the behavioral and second fMRI control experi- 
ments. In that continuum, we varied the length of the VOT by delete or 
add one or several voiced period in the signal, before or after the burst, 
use audio editor software. 

Tasks Design. Auditory stimulus be present use Psychophysics-3 Toolbox 
and additional custom script write for Matlab version 8.2.0.701 (MathWorks). 
Sounds be present binaurally at a sample rate of 44,100 Hz and at a 
comfortable hearing level individually set before the experiment via earphones. 
Before the experiment, each participant undertook a short session during which 
the minimum amplitude level lead to 100% categorization accuracy be 
estimate use an adaptive staircase procedure. This threshold be use to 
transmit the stimulus (mean 30 dB sensation level). Each continuumwas deliver 
to participant in two independent session of 240 trial each for fMRI record 
and 270 trial each for MEG recording. The experiment use for epileptic pa- 
tients comprise 144 trials. 

Participants be ask to perform an identification task. Each trial com- 
prise one sound (randomly chosen among the 6 or 48 stimulus of the contin- 
uum), follow by 1 s of silence; then, a response screen with the write 
syllable “ba” and “da” (in MEG, fMRI, and behavioral sessions) or “ba,” “da,” 
and “ga” (in i-EEG sessions) be displayed. Syllables be randomly displayed 
from right to left on the screen to prevent motor preparation and persever- 
ative responses. During fMRI recording, the appearance of the response screen 
be randomly jittered 100, 300, or 500 m after the silence gap. Participants 
indicate their response on the syllable by press the correspond left or 
right response button a quickly a possible. Subjects’ response be pur- 
posely delayed to avoid temporal overlap between perceptual process and 
motor effect due to button press. Response time hence do not constitute 
relevant data. To limit eye movements, subject be ask to fixate the 
central cross and to blink only after give their motor response. After the 
response, a jittered delay vary from 3 to 5 s lead to the next trial. 

MEG Recording and Preprocessing. Continuous cerebral activity be record 
use an Elekta Systems MEG device, with 102 triple-sensor elements, each 
composedof twoplanargradiometers andonemagnetometer.MEG signalswere 
record at a sample rate of 1 kHz and be online band-pass filter be- 
tween 0.1 and 300 Hz. A vertical electro-oculogramwas record simultaneously. 

Bouton et al. PNAS Early Edition | 7 of 10 

N 
EU 

RO 
SC 

IE 
N 
CE 

PN 
A 
S 
PL 

U 
S 



BeforeMEG recording, theheadshape for eachparticipantwas acquiredusing a 
Polhemus system. After the MEG session, an individual anatomical MRI be 
record [Tim-Trio; Siemens; 9-min anatomical T1-weighted magnetization- 
prepared rapid gradient-echo (MP-RAGE), 176 slices, field of view = 256, 
voxel size = 1 × 1 × 1 mm3]. MEG data be preprocessed, analyzed, and vi- 
sualized use dataHandler software (wiki.cenir.org/doku.php), the Brainstorm 
toolbox (67), and custom Matlab scripts. A principal component analysis (PCA) 
be perform through singular-value decomposition function of numerical 
recipe to correct artifact (low derivation). The first two component from the 
PCA be zeroed, and the signal matrix be recomputed. PCA rotate the 
original data to new coordinates, make the data a flat a possible. The data 
be then epoched from 1 s before syllable onset to 1 s after syllable offset. 
Another PCA be then perform on the epoched data when blink oc- 
curred. PCA component be visually inspect to reject the one capture 
blink artifacts. On average, 2.1 ± 0.7% of trial per participant (mean ± SEM) 
be contaminate by eye-movement artifact and be correct before 
further analyses. 

fMRI Recording and Preprocessing. Images be collect use a Verio 3.0 T 
(Siemens) whole-body and radio-frequency coil scanner. The fMRI BOLD 
signal be measure use a T2*-weighted echoplanar sequence (repetition 
time = 2,110 ms; echo time = 26 ms; flip angle = 90°). Forty contiguous slice 
(thickness = 3 mm; gap = 15%; matrix size = 64 × 64; voxel size = 3 × 3 × 
3 mm) be acquire per volume. A high-resolution T1-weighted anatomical 
image (repetition time = 2,300 ms; echo time = 4.18 ms; T1 = 900 ms; image 
matrix = 256 × 256; slab thickness = 176 mm; spatial resolution = 1 × 1 × 
1 mm) be collect for each participant after functional acquisition. Image 
preprocessing be perform use SPM8 (The Wellcome Trust Centre for 
Neuroimaging, University College London, London, www.fil.ion.ucl.ac.uk/ 
spm/). Each of the four scan session contain 400 functional volumes. 
All functional volume be realign to the first one to correct for inter- 
scan movement. Functional and structural image be spatially pre- 
process (realignment, normalization, smooth with an 8-mm FWHM 
isotropic Gaussian kernel) and temporally process use a high-pass filter 
with a cutoff frequency of 60 Hz. We then checked data for electronic 
and rapid-movement artifact use the ArtRepair toolbox (cibsr.stanford.edu/ 
tools/human-brain-project/artrepair-software.html). Artifacted volume be 
substitute by linear interpolation between contiguous volume and be ex- 
plicitly model in the follow statistical analyses. Estimated headmovements 
be small compare with voxel size (<1 mm); 3.2 ± 0.3% of the volume be 
exclude due to rapid head movement (>1.5 mm/s). 

i-EEG Recording and Preprocessing. Electrophysiological activity be record 
over array of depth electrode surgically implant to identify epilepsy 
focus. i-EEG be record (Ceegraph XL; Biologic System Corps.) use 
electrode array with eight stainless contact each (electrode diameter = 
3 mm, intercontact space = 10 mm; AD-Tech) implant in several brain 
region in the right hemisphere (Figs. 3–5). We determine the precise 
electrode shaft location by coregistering a postoperative compute to- 
mography scan with a high-resolution anatomical MRI template. For the 
i-EEG recordings, we use a bipolar montage in which each channel be 
reference to it adjacent neighbor. We sample the i-EEG signal at 1,024 Hz 
for patient 2 and at 2,048 Hz for patient 1 and 3. 

Steady-state frequency spectrum be estimate use a standard Fourier 
transform from 1 s before to 1 s after the offset of the stimulus. Time–fre- 
quency power be define a the single-trial square amplitude estimate of 
complex Fourier components. Time–frequency analysis be carry out 
use the Fieldtrip toolbox for MATLAB (68). The spectral power of MEG 
oscillation be estimate use a family of complex Morlet wavelets, 
result in an estimate of power at each time point and each frequency. We 
restrict the analysis to frequency between 2 and 150 Hz, span the 
whole range of relevant brain rhythms. Note that the time–frequency 
transform us frequency-dependent wavelet (from three to seven cycle 
per window), with decrease time-windows with increase frequency. 

Neural Encoding of Parametric Information. We regress out single trial of 
MEG, fMRI, and i-EEG signal against (i) the acoustic dimension, correspond 
to F2 parameter [the onset value of the second formant (F2) and the F2 slope 
linearly covaried in six steps] or to the VOT (the voice length before and after 
the consonant burst varied in six steps)], and (ii) the categorization difficulty 
dimension correspond to the inverse of the discriminability index from signal 
detection theory (−d′). These two dimension be naturally orthogonal (r = 0.02, 
P > 0.20). A general linear regression model be carry out separately for each 
dimension (sensory encode and decisional effort) along the stimulus and be 
finally average across participant to produce a group-level grand average. 

That approach be adopt to disentangle the neural correlate of basic 
bottom-up perceptual processing index the track of the acoustic cue from 
the neural correlate of the categorization difficulty reflect the distance of 
each stimulus from the phoneme identity criterion (48, 69). 

fMRI: Neural Encoding of Parametric Information. Statistical parametric t score 
be obtain from local fMRI signal use a linear multiple regression model 
with sensory encode (F2 parameter or VOT value for each condition) and 
decisional effort (−d′ value report by each subject for each trial) a cova- 
riates. Regression parameter be estimate in every voxel for each subject, 
and parameter estimate then be enter in a between-subject random- 
effect analysis to obtain statistical parametric maps. We identify brain ac- 
tivation show significant contrast of parameter estimate with a voxelwise 
(T = 3.21, P < 0.005, uncorrected) and clusterwise (P < 0.05, uncorrected) sig- 
nificance threshold. All report activation survive false discovery rate (FDR) 
correction for multiple comparison (P < 0.05) (70). Anatomical location be 
determine base on automate anatomical labeling. Regressors of interest 
be construct by convolve function represent the event with the 
canonical hemodynamic response function. For each continuum, a categorical 
regressor model the “sound” event use a Dirac function time lock to 
syllable onset. Two hierarchically orthogonalized parametric regressors (re- 
ferred to a “sensory encoding” and “decisional effort” regressors) be 
add to the sound regressor to capture the modulation of BOLD activity a a 
function of F2 variation track and categorization difficulty. For illustrative 
purpose (Fig. 1C and Fig. S2C), we use the rfx_plot toolbox (71) to split 
F2 and d′ parametric regressors into six new onset regressors (simple onset 
regressor without a parametric modulation), each contain all event for a 
particular level of the stimulus continuum. Beta weight be reestimated for 
each of these six onset regressors and be average across all subject to get 
the correspond percent of signal change. 

MEG: Neural Encoding of Parametric Information. We first use single-trial 
signal on each sensor to perform parametric regression at successive 
time from −0.2 to 1 s follow stimulus onset. For each participant and 
each sensor, we calculate the time course of beta coefficient and then 
compute cortical current map with Brainstorm use the weight 
minimum-norm estimation approach, meaning that the time series for each 
source be a linear combination of all time series record by the sensor (72). 
Sources be estimate for each subject on the basis of individual MRI im- 
ages. After realignment and deformation of each subject’s cortical surface, 
source be project onto the standard Montreal Neurological Institute 
(MNI)/Colin27 brain to perform grand mean averages. We then perform 
within-group statistic to show the sensitivity to sensory encode and de- 
cisional effort dimensions. Note that both of the two transformation ap- 
ply to the data (regression and source-projection) capture a linear 
relationship between the observe and the expect data and can thus be 
implement in either order. This method be use to localize the source of 
sensory and perceptual decision component and to demonstrate that sen- 
sory and decisional processing be hierarchically organize in time. 

Single-trial–evoked signal on each sensor be also use to compute 
source current map for each trial. The inverse operator be generate 
with the default MNE parameter and be apply at the single-trial level. 
The estimate source be morph to the MNI brain. We then extract 
single-trial neural activity from region of interest define accord to 
Destrieux’s atlas (73) (G_temp_sup-Plan_tempo, G_temp_sup-G_T_transv, 
G_front_inf-Opercular, G_front_inf-Triangul). Single-trial–evoked response 
project on these select source be use in two ways: 

i) For each participant, we regress out single-trial neural activity to es- 
timate spectral power of the beta coefficient via a standard Fourier 
transform. Time–frequency analysis be carry out accord to ex- 
actly the same parameter define in the previous paragraph, i-EEG Re- 
cord and Preprocessing.We thus estimate the trial-to-trial variability 
in neural signal from region of interest at a give frequency that de- 
scribe sensory encode or decisional effort (t test against zero, P < 
0.05, Bonferroni-corrected). 

ii) For each participant, source activity in the pSTG and in the left IFG be 
use to measure GC. While GC be classically use to ass causal influence 
between two time series, we here compute GC for nonstationary time 
series, such a oscillate neural signal (74, 75). We use a nonparamet- 
ric test by compute a spectral density matrix factorization technique 
on complex cross-spectra, obtain from the continuous wavelet trans- 
form of source-reconstructed MEG time series. We then assess the 
linear directional influence between two brain areas, the pSTG and 
the left IFG. 

8 of 10 | www.pnas.org/cgi/doi/10.1073/pnas.1714279115 Bouton et al. 

http://wiki.cenir.org/doku.php 
http://www.fil.ion.ucl.ac.uk/spm/ 
http://www.fil.ion.ucl.ac.uk/spm/ 
http://cibsr.stanford.edu/tools/human-brain-project/artrepair-software.html 
http://cibsr.stanford.edu/tools/human-brain-project/artrepair-software.html 
http://www.pnas.org/lookup/suppl/doi:10.1073/pnas.1714279115/-/DCSupplemental 
www.pnas.org/cgi/doi/10.1073/pnas.1714279115 


GC be compute twice: 

• From right pSTG to left IFG to determine whether activity in the IFG could 
be predict at trial t by include past activity from both the pSTG and 
the IFG. Here we assume the information flow to be bottom-up. 

• From left IFG to right pSTG to determine whether activity in the pSTG 
could be predict at trial t by include past activity from both the pSTG 
and the IFG. Here we assume the information flow to be top-down. 

Because we compute GC on nonstationary neural signal (i.e., evoke 
activity from reconstruct sources), GC spectrum be obtain in a non- 
parametric manner by use wavelet transform, without go through the 
multivariate autoregressive model fitting (74), and spectral GC be intend 
to reveal a frequency-resolved GC. To do so, we use a spectral matrix fac- 
torization technique on complex cross-spectra obtain directly from 
wavelet transforms of the data. Wavelet transforms be compute at any 
instant (1-ms resolution) of the syllable duration (360 ms) on a trial-by-trial 
basis for each subject. For each subject, we then compute the mean GC 
across trial and the correspond SD. The original GC spectrum be stan- 
dardized to obtain a vector of z-values, one for each frequency. 

We test for significant frequency peak separately for bottom-up and top- 
down GC direction, directly compare the z-transformed vector obtain from 
GC spectrum to a zero-mean normal distribution, and correct for multiple 
comparison with the Bonferroni method at P < 0.05. Our decision to focus on 
the left IFG be empirically motivated. Previous paper (e.g., refs. 76–78) have 
show that the left IFG be consistently involve in articulatory processing during 
speech perception and also in lexical information retrieval, both skill that be 
engage when categorize ambiguous speech sounds, i.e., when the internal 
perceptual decision criterion be difficult to reach, a in the current study. 

MEG: Decoding Analyses. Decoding analysis be perform with the Neural 
Decoding Toolbox (79), use a maximum correlation coefficient classifier on 
single-trial–induced response across all MEG sensors. Data from both 
magnetometer and gradiometers be used. The pattern classifier be 
train on the response give by the participant and compute the corre- 
lation between MEG data and the syllable identify (/ba/ or /da/) on each 
trial. More specifically, the classifier be train on 80% of the data, and it 
performance be evaluate on the withheld 20% of the test data. The 
splitting procedure between training and test data be perform 
50 time to reduce the variance of the performance estimate. The report 
final decode accuracy be the average accuracy across the 50 decode re- 
sults. Classification accuracy be report a the percentage of correct trial 
classify in the test set average over all cross-validation splits. 

Additionally, an ANOVA base on the second-level test across subject be 
apply to the test data to select those sensor that be significantly sensitive 
to syllable identity at each time point. We then assess statistical significance 
use a permutation test. To perform this test, we generate a null distribution 
by run the decode procedure 200 time use data with randomly 
shuffle label for each subject. Decoding performance above all point in the 
null distribution for the correspond time pointwas deem significant with 
P < 0.005 (1/200). The first time decode reach significantly above chance 
be define when accuracy be significant for five consecutive time points. 
Source localization associate with the decode result be compute from 
evoke trial use the MNE source-modeling method (see above). 

i-EEG: Neural Encoding of Parametric Information. We perform the same 
parametric regression on i-EEG recording from patient 1. These analysis be 
do only on that patient, a he be the only patient for whom one shaft 
show a significant induced response to syllable perception. Shaft 1 colo- 
calized to the site (the right pSTG) where spectral cue track be found with 
fMRI and MEG. We select the five deepest contact on each shaft; those 
contact be locate between the Heschl’s gyrus and the STG on shaft 1. 
Parametric regression be carry out at successive times, t, from −0.2 to 1 s 
poststimulus onset, on each select bipolar derivation, i.e., from the deepest 
(1) to the most external (5) contact. We compute the power in each fre- 
quency band at each time point of each beta coefficient, with a millisecond 
resolution, similar to the induced power (between 2 and 150 Hz, with a 0.5-Hz 

resolution below 20 Hz and with a 1-Hz resolution above 20 Hz) by apply a 
TF wavelet transform, use a family of complex Morlet wavelet (m = 3–7). 
For each contact, a null distribution be compute by repeat the identical 
regression procedure 1,000 time with shuffle regressors. We use standard 
parametric test (t test against zero) to ass the statistical significance of each 
parametric regression. The type 1 error rate (FDR) arise from multiple 
comparison be control for use nonparametric cluster-level statistic (80) 
compute across contacts, time samples, and frequencies. We do not need to 
correct for multiple comparison across electrode shafts, a statistical test 
be run independently for each contact of each electrode shaft. 

i-EEG: Decoding Analyses. We use a maximum correlation coefficient clas- 
sifier [Neural Decoding Toolbox (79)] on single-trial–induced responses. The 
classifier be train to classify the i-EEG data into three category that 
correspond to the three syllable identify by the patient (/ba/, /da/, 
or /ga/). We apply the decode procedure on time series use a cross- 
validation procedure in which the classifier be train on 90% of the trial 
and be test on the remain 10%. Our recording consist of three 
repetition of each stimulus condition (48 stimulus from /ba/ to /ga/) for patient 
1 (144 trial to be categorized) and six repetition of each condition (288 trial 
to be categorized) for patient 2 and 3. The cross-validation procedure be 
repeat 1,000 time with a different split between training and test 
datasets on each iteration, and the result be average across all iterations. 

We estimate single-trial decode of the neuronal response induced by 
different syllable use both uni- and multivariate classification. The univariate 
classification be apply to each bipolar derivation (i.e., to each of the five 
contact of each shaft),whereas themultivariate classificationwasperformedon 
neural activity from every bipolar derivation of one shaft (i.e., on the five 
contact of each shaft, pool together) and then on all bipolar derivation of 
patient 1 (on all contact of the six shafts, pool together), and finally on all 
three patient (on all contact of the 14 shafts, pool together). Single-shaft 
multivariate decode be compare with mean univariate decode com- 
put first fromeach contact and thenaveraged. Decoding accuracy be express 
a the percent of correctly classify trial in the test set. A null distribution be 
compute by repeat the identical classification procedure 1,000 time with 
shuffle labels. We define the number of classification repetition with respect 
to the number of multiple comparison do from each contact (FDR-corrected 
for univariate decode perform on each of the five contacts, time samples, 
and frequencies, FDR-corrected formultivariate decode perform on each of 
the six shafts, time samples, and frequencies, and FDR-corrected formultivariate 
decode perform on all shaft together, time samples, and frequencies). 
Decoding accuracy be consider significant at q < 0.05 if accuracy exceed 
the randomize classification at two consecutive time points. 

i-EEG: Correct-Minus-Incorrect Differences. The psychometric identification func- 
tion with percentage reporting /ba/, /da/, or /ga/ be define along the cor- 
respond continuum. Boundary separation determine the accuracy of 
categorical choice: The steeper the slope, the more accurate be the perceptual 
decision. Patient’s rating along the continuumwere use to split response into 
correct and incorrect trials. We subsequently compute the difference in neural 
activity from select bipolar derivation between correct and incorrect con- 
ditions and then compare it with the zero-mean normal distribution thresh- 
olding at q < 0.05 [FDR-corrected for multiple comparison on shaft (30 shaft 
test for patient 1), time, and frequency dimensions]. This procedure be re- 
peated 1,000 time with shuffle label for correct and incorrect conditions. 

ACKNOWLEDGMENTS. We thank Lorenzo Fontolan and Clio Coste for 
method support and Aaron Schurger, Luc Arnal, Andreas Kleinschmidt, 
David Poeppel, Virginie van Wassenhove, Corrado Corradi Dell’Acqua, Narly 
Golestani, and Clio Coste for comment and useful discussion about early 
version of this manuscript. This work be fund by European Research 
Council Compuslang Grant Agreement GA260347 and Swiss National Fund 
(SNF) Grant 320030_149319 (to A.-L.G.), SNF Grants 140332 and 146633 (to 
M.S.), Agnece Nationale pour la Research (ANR) Grants ANR-10-LABX-0087 
IEC (Institut d’Étude de la Cognition), ANR-10-IDEX-0001-02 PSL* (Paris Sci- 
ences et Lettres) (program “Investissements d’Avenir”), and ANR-16-CE37- 
0012-01 (to V.C.), and SNF Grant P300P1_167591 (to S.B.). 

1. Mesgarani N, Cheung C, Johnson K, Chang EF (2014) Phonetic feature encode in 

human superior temporal gyrus. Science 343:1006–1010. 
2. Chang EF, et al. (2010) Categorical speech representation in human superior temporal 

gyrus. Nat Neurosci 13:1428–1432. 
3. Yan Y, et al. (2014) Perceptual training continuously refines neuronal population 

code in primary visual cortex. Nat Neurosci 17:1380–1387. 
4. Hung CP, Kreiman G, Poggio T, DiCarlo JJ (2005) Fast readout of object identity from 

macaque inferior temporal cortex. Science 310:863–866. 

5. Carota F, Kriegeskorte N, Nili H, Pulvermüller F (2017) Representational similarity 
mapping of distributional semantics in left inferior frontal, middle temporal, and 
motor cortex. Cereb Cortex 27:294–309. 

6. Grill-Spector K, Weiner KS (2014) The functional architecture of the ven- 
tral temporal cortex and it role in categorization. Nat Rev Neurosci 15: 
536–548. 

7. Pinotsis DA, et al. (2017) Linking canonical microcircuit and neuronal activity: Dy- 
namic causal model of laminar recordings. Neuroimage 146:355–366. 

Bouton et al. PNAS Early Edition | 9 of 10 

N 
EU 

RO 
SC 

IE 
N 
CE 

PN 
A 
S 
PL 

U 
S 



8. Huth AG, de Heer WA, Griffiths TL, Theunissen FE, Gallant JL (2016) Natural speech 
reveals the semantic map that tile human cerebral cortex. Nature 532:453–458. 

9. Ritchie JB, Kaplan D, Klein C (2017) Decoding the brain: Neural representation and 
the limit of multivariate pattern analysis in cognitive neuroscience. bioRxiv: 
10.1101/127233. 

10. Weichwald S, Grosse-WentrupM (2017) The right tool for the right question—Beyond 
the encode versus decode dichotomy. arXiv:1704.08851. 

11. Hebart MN, Baker CI (August 4, 2017) Deconstructing multivariate decode for the 
study of brain function. Neuroimage, S1053-8119(17)30652-3. 

12. Eger E, Ashburner J, Haynes J-D, Dolan RJ, Rees G (2008) fMRI activity pattern in 
human LOC carry information about object exemplar within category. J Cogn 
Neurosci 20:356–370. 

13. Anderson ML, Oates T (2010) A critique of multi-voxel pattern analysis. Proceedings of 
the 32nd Annual Meeting of the Cognitive Science Society (Cognitive Science Society, 
Austin, TX), pp 1511–1516. 

14. Dilks DD, Julian JB, Paunov AM, Kanwisher N (2013) The occipital place area be causally 
and selectively involve in scene perception. J Neurosci 33:1331–1336a. 

15. Anderson ML (2015) Précis of after phrenology: Neural reuse and the interactive 
brain. Behav Brain Sci 39:1–22. 

16. Shamma S, Lorenzi C (2013) On the balance of envelope and temporal fine structure in 
the encode of speech in the early auditory system. J Acoust Soc Am 133:2818–2833. 

17. Moon IJ, et al. (2014) Optimal combination of neural temporal envelope and fine 
structure cue to explain speech identification in background noise. J Neurosci 34: 
12145–12154. 

18. de Heer WA, Huth AG, Griffiths TL, Gallant JL, Theunissen FE (2017) The hierarchical 
cortical organization of human speech processing. J Neurosci 37:6539–6557. 

19. Formisano E, De Martino F, Bonte M, Goebel R (2008) “Who” be say “what”? Brain- 
base decode of human voice and speech. Science 322:970–973. 

20. Ley A, Vroomen J, Formisano E (2014) How learn to abstract shape neural sound 
representations. Front Neurosci 8:132. 

21. Pasley BN, Knight RT (2013) Decoding speech for understand and treat aphasia. 
Prog Brain Res 207:435–456. 

22. Chevillet MA, Jiang X, Rauschecker JP, Riesenhuber M (2013) Automatic phoneme 
category selectivity in the dorsal auditory stream. J Neurosci 33:5208–5215. 

23. Mirman D, et al. (2015) Neural organization of spoken language reveal by lesion- 
symptom mapping. Nat Commun 6:6762. 

24. Tsunada J, Liu ASK, Gold JI, Cohen YE (2016) Causal contribution of primate auditory 
cortex to auditory perceptual decision-making. Nat Neurosci 19:135–142. 

25. Rauschecker JP (1998) Cortical processing of complex sounds. Curr Opin Neurobiol 8: 
516–521. 

26. Martin S, et al. (2016) Word pair classification during imagine speech use direct 
brain recordings. Sci Rep 6:25803. 

27. Santoro R, et al. (2017) Reconstructing the spectrotemporal modulation of real-life 
sound from fMRI response patterns. Proc Natl Acad Sci USA 114:4799–4804. 

28. Zatorre RJ, Belin P (2001) Spectral and temporal processing in human auditory cortex. 
Cereb Cortex 11:946–953. 

29. Fontolan L, Morillon B, Liegeois-Chauvel C, Giraud A-L (2014) The contribution of 
frequency-specific activity to hierarchical information processing in the human au- 
ditory cortex. Nat Commun 5:4694. 

30. Arnal LH, Giraud A-L (2012) Cortical oscillation and sensory predictions. Trends Cogn 
Sci 16:390–398. 

31. Fries P (2015) Rhythms for cognition: Communication through coherence. Neuron 88: 
220–235. 

32. Friston KJ, Trujillo-Barreto N, Daunizeau J (2008) DEM: A variational treatment of 
dynamic systems. Neuroimage 41:849–885. 

33. Di Liberto GM, Lalor EC, Millman RE (2018) Causal cortical dynamic of a predictive 
enhancement of speech intelligibility. Neuroimage 166:247–258. 

34. Morillon B, Baillet S (2017) Motor origin of temporal prediction in auditory atten- 
tion. Proc Natl Acad Sci USA 114:E8913–E8921. 

35. Schoffelen JM, et al. (2017) Frequency-specific direct interaction in the human 
brain network for language. Proc Natl Acad Sci USA 114:8083–8088. 

36. Donoso M, Collins AGE, Koechlin E (2014) Human cognition. Foundations of human 
reason in the prefrontal cortex. Science 344:1481–1486. 

37. Blumstein SE, Myers EB, Rissman J (2005) The perception of voice onset time: An fMRI 
investigation of phonetic category structure. J Cogn Neurosci 17:1353–1366. 

38. Rogers JC, Davis MH (2017) Inferior frontal cortex contribution to the recognition of 
spoken word and their constituent speech sounds. J Cogn Neurosci 29:919–936. 

39. Marti S, King J-R, Dehaene S (2015) Time-resolved decode of two processing chain 
during dual-task interference. Neuron 88:1297–1307. 

40. King J-R, Dehaene S (2014) Characterizing the dynamic of mental representations: 
The temporal generalization method. Trends Cogn Sci 18:203–210. 

41. Isik L, Meyers EM, Leibo JZ, Poggio T (2014) The dynamic of invariant object recog- 
nition in the human visual system. J Neurophysiol 111:91–102. 

42. Du Y, Buchsbaum BR, Grady CL, Alain C (2014) Noise differentially impact phoneme 
representation in the auditory and speech motor systems. Proc Natl Acad Sci USA 
111:7126–7131. 

43. Liebenthal E, Sabri M, Beardsley SA, Mangalathu-Arumana J, Desai A (2013) Neural 
dynamic of phonological processing in the dorsal auditory stream. J Neurosci 33: 
15414–15424. 

44. Obleser J, Eisner F, Kotz SA (2008) Bilateral speech comprehension reflect differential 
sensitivity to spectral and temporal features. J Neurosci 28:8116–8123. 

45. Liégeois-Chauvel C, Giraud K, Badier J-M, Marquis P, Chauvel P (2001) Intracerebral 
evoke potential in pitch perception reveal a functional asymmetry of the human 
auditory cortex. Ann N Y Acad Sci 930:117–132. 

46. Alain C, Snyder JS (2008) Age-related difference in auditory evoke response during 
rapid perceptual learning. Clin Neurophysiol 119:356–366. 

47. Alain C, Snyder JS, He Y, Reinke KS (2007) Changes in auditory cortex parallel rapid 
perceptual learning. Cereb Cortex 17:1074–1084. 

48. Binder JR, Liebenthal E, Possing ET, Medler DA, Ward BD (2004) Neural correlate of 
sensory and decision process in auditory object identification. Nat Neurosci 7:295–301. 

49. Moerel M, De Martino F, Formisano E (2014) An anatomical and functional topog- 
raphy of human auditory cortical areas. Front Neurosci 8:225. 

50. Giraud A-L, et al. (2004) Contributions of sensory input, auditory search and verbal 
comprehension to cortical activity during speech processing. Cereb Cortex 14: 
247–255. 

51. Lee Y-S, Turkeltaub P, Granger R, Raizada RDS (2012) Categorical speech processing in 
Broca’s area: An fMRI study use multivariate pattern-based analysis. J Neurosci 32: 
3942–3948. 

52. Arsenault JS, Buchsbaum BR (2015) Distributed neural representation of phonolog- 
ical feature during speech perception. J Neurosci 35:634–642. 

53. Blank H, Davis MH (2016) Prediction error but not sharpen signal simulate mul- 
tivoxel fMRI pattern during speech perception. PLoS Biol 14:e1002577. 

54. Bonte M, Hausfeld L, Scharke W, Valente G, Formisano E (2014) Task-dependent 
decode of speaker and vowel identity from auditory cortical response patterns. 
J Neurosci 34:4548–4557. 

55. Sohoglu E, Peelle JE, Carlyon RP, Davis MH (2012) Predictive top-down integration of 
prior knowledge during speech perception. J Neurosci 32:8443–8453. 

56. Kok P, Jehee JFM, de Lange FP (2012) Less be more: Expectation sharpens represen- 
tations in the primary visual cortex. Neuron 75:265–270. 

57. Weichwald S, et al. (2015) Causal interpretation rule for encode and decode 
model in neuroimaging. Neuroimage 110:48–59. 

58. Zohary E, Celebrini S, Britten KH, Newsome WT (1994) Neuronal plasticity that un- 
derlies improvement in perceptual performance. Science 263:1289–1292. 

59. Sompolinsky H, Yoon H, Kang K, Shamir M (2001) Population cod in neuronal 
system with correlate noise. Phys Rev E Stat Nonlin Soft Matter Phys 64:051904. 

60. Pitkow X, Liu S, Angelaki DE, DeAngelis GC, Pouget A (2015) How can single sensory 
neuron predict behavior? Neuron 87:411–423. 

61. Çukur T, Nishimoto S, Huth AG, Gallant JL (2013) Attention during natural vision 
warp semantic representation across the human brain. Nat Neurosci 16:763–770. 

62. Cheung C, Hamiton LS, Johnson K, Chang EF (2016) The auditory representation of 
speech sound in human motor cortex. Elife 5:e12577. 

63. Correia J, et al. (2014) Brain-based translation: fMRI decode of spoken word in 
bilingual reveals language-independent semantic representation in anterior tem- 
poral lobe. J Neurosci 34:332–338. 

64. Kroon P, Kleijn WB (1995) Linear-Prediction Based Analysis-by-Synthesis Coding 
(Elsevier Science, Amsterdam). 

65. Riede T, Suthers RA (2009) Vocal tract motor pattern and resonance during constant 
frequency song: The white-throated sparrow. J Comp Physiol A Neuroethol Sens 
Neural Behav Physiol 195:183–192. 

66. Goncharoff V, Kaine-Krolak M (1995) Interpolation of LPC spectrum via pole shifting. 
Proc IEEE Int Conf Acoust Speech Signal Process 1:780–783. 

67. Tadel F, Baillet S, Mosher JC, Pantazis D, Leahy RM (2011) Brainstorm: A user-friendly 
application for MEG/EEG analysis. Comput Intell Neurosci 2011:879716. 

68. Oostenveld R, Fries P, Maris E, Schoffelen JM (2011) FieldTrip: Open source software 
for advanced analysis of MEG, EEG, and invasive electrophysiological data. Comput 
Intell Neurosci 2011:156869. 

69. Liebenthal E, Binder JR, Spitzer SM, Possing ET, Medler DA (2005) Neural substrate of 
phonemic perception. Cereb Cortex 15:1621–1631. 

70. Genovese CR, Lazar NA, Nichols T (2002) Thresholding of statistical map in functional 
neuroimaging use the false discovery rate. Neuroimage 15:870–878. 

71. Gläscher J (2009) Visualization of group inference data in functional neuroimaging. 
Neuroinformatics 7:73–82. 

72. Hämäläinen MS, Ilmoniemi RJ (1994) Interpreting magnetic field of the brain: 
Minimum norm estimates. Med Biol Eng Comput 32:35–42. 

73. Destrieux C, Fischl B, Dale A, Halgren E (2010) Automatic parcellation of human 
cortical gyrus and sulcus use standard anatomical nomenclature. Neuroimage 53:1–15. 

74. Dhamala M, Rangarajan G, Ding M (2008) Analyzing information flow in brain net- 
work with nonparametric Granger causality. Neuroimage 41:354–362. 

75. Dhamala M, Rangarajan G, Ding M (2008) Estimating Granger causality from fourier 
and wavelet transforms of time series data. Phys Rev Lett 100:018701. 

76. Lyu B, Ge J, Niu Z, Tan LH, Gao J-H (2016) Predictive brain mechanism in sound-to- 
meaning mapping during speech processing. J Neurosci 36:10813–10822. 

77. Yoo S, Lee K-M (2013) Articulation-based sound perception in verbal repetition: A 
functional NIRS study. Front Hum Neurosci 7:540. 

78. Klein M, et al. (2015) Early activity in Broca’s area during reading reflect fast access to 
articulatory code from print. Cereb Cortex 25:1715–1723. 

79. Meyers EM (2013) The neural decode toolbox. Front Neuroinform 7:8. 
80. Maris E, Oostenveld R (2007) Nonparametric statistical test of EEG- and MEG-data. 

J Neurosci Methods 164:177–190. 

10 of 10 | www.pnas.org/cgi/doi/10.1073/pnas.1714279115 Bouton et al. 

www.pnas.org/cgi/doi/10.1073/pnas.1714279115 

