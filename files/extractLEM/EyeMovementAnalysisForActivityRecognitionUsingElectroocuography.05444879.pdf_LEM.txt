




































untitled 


Eye Movement Analysis for Activity Recognition 
Using Electrooculography 

Andreas Bulling, Student Member, IEEE, Jamie A. Ward, Hans Gellersen, and 

Gerhard Tröster, Senior Member, IEEE 

Abstract—In this work, we investigate eye movement analysis a a new sense modality for activity recognition. Eye movement data 

be record use an electrooculography (EOG) system. We first describe and evaluate algorithm for detect three eye 

movement characteristic from EOG signals—saccades, fixations, and blinks—and propose a method for assess repetitive pattern 

of eye movements. We then devise 90 different feature base on these characteristic and select a subset of them use minimum 

redundancy maximum relevance (mRMR) feature selection. We validate the method use an eight participant study in an office 

environment use an example set of five activity classes: copying a text, reading a print paper, take handwritten notes, watch a 

video, and browsing the Web. We also include period with no specific activity (the NULL class). Using a support vector machine 

(SVM) classifier and person-independent (leave-one-person-out) training, we obtain an average precision of 76.1 percent and recall of 

70.5 percent over all class and participants. The work demonstrates the promise of eye-based activity recognition (EAR) and open 

up discussion on the wider applicability of EAR to other activity that be difficult, or even impossible, to detect use common sense 

modalities. 

Index Terms—Ubiquitous computing, feature evaluation and selection, pattern analysis, signal processing. 

Ç 

1 INTRODUCTION 

HUMAN activity recognition have become an importantapplication area for pattern recognition. Research in 
computer vision have traditionally be at the forefront of 
this work [1], [2]. The grow use of ambient and body- 
worn sensor have pave the way for other sense 
modalities, particularly in the domain of ubiquitous 
computing. Important advance in activity recognition be 
achieve use modality such a body movement and 
posture [3], sound [4], or interaction between people [5]. 

There are, however, limitation to current sensor config- 
urations. Accelerometers or gyroscopes, for example, be 
limited to sense physical activity; they cannot easily be use 
for detect predominantly visual tasks, such a reading, 
browsing the Web, or watch a video. Common ambient 
sensors, such a reed switch or light sensors, be limited in 
that they only detect basic activity events, e.g., enter or 
leave a room or switch an appliance on or off. Further to 
these limitations, activity sense use subtle cues, such a 
user attention or intention, remains largely unexplored. 

A rich source of information, a yet unused for activity 
recognition, be the movement of the eyes. The movement 
pattern our eye perform a we carry out specific 
activity have the potential to reveal much about the 
activity themselves—independently of what we be 
look at. This include information on visual tasks, such 
a reading [6], information on predominantly physical 
activities, such a drive a car, but also on cognitive 
process of visual perception, such a attention [7] or 
saliency determination [8]. In a similar manner, location or 
a particular environment may influence our eye move- 
ments. Because we use our eye in almost everything that 
we do, it be conceivable that eye movement provide useful 
information for activity recognition. 

Developing sensor to record eye movement in daily life 
be still an active topic of research. Mobile setting call for 
highly miniaturized, low-power eye tracker with real-time 
processing capabilities. These requirement be increasingly 
address by commonly use video-based systems, of 
which some can now be worn a relatively light headgear. 
However, these remain expensive, with demand video 
processing task require bulky auxiliary equipment. 
Electrooculography (EOG)—the measurement technique 
use in this work—is an inexpensive method for mobile 
eye movement recordings; it be computationally light 
weight and can be implement use wearable sensor 
[9]. This be crucial with a view to long-term recording in 
mobile real-world settings. 

1.1 Paper Scope and Contributions 

The aim of this work be to ass the feasibility of 
recognize human activity use eye movement analysis, 
so-called eye-based activity recognition (EAR).1 The specific 
contribution are: 

IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 33, NO. 4, APRIL 2011 741 

. A. Bulling be with the Computer Laboratory, University of Cambridge, 15 
JJ Thomson Ave., William Gates Building, Cambridge CB3 0FD, UK, and 
the School of Computing and Communications, Lancaster University, 
InfoLab 21, South Drive, Lancaster LA1 4WA, UK. 
E-mail: andreas.bulling@acm.org. 

. G. Tröster be with the Department of Information Technology and 
Electrical Engineering, Swiss Federal Institute of Technology (ETH) 
Zurich, Gloriastrasse 35, 8092 Zurich, Switzerland. 
E-mail: troester@ife.ee.ethz.ch. 

. J.A. Ward and H. Gellersen be with the School of Computing and 
Communications, Lancaster University, InfoLab 21, South Drive, Lan- 
caster LA1 4WA, UK. E-mail: {j.ward, hwg}@comp.lancs.ac.uk. 

Manuscript receive 26 Nov. 2009; accepted 26 Feb. 2010; publish online 
30 Mar. 2010. 
Recommended for acceptance by B. Schiele. 
For information on obtain reprint of this article, please send e-mail to: 
tpami@computer.org, and reference IEEECS Log Number 
TPAMI-2009-11-0785. 
Digital Object Identifier no. 10.1109/TPAMI.2010.86. 1. An early version of this paper be publish in [10]. 

0162-8828/11/$26.00 � 2011 IEEE Published by the IEEE Computer Society 



1. the introduction of eye movement analysis a a new 
sense modality for activity recognition, 

2. the development and characterization of new algo- 
rithms for detect three basic eye movement type 
from EOG signal (saccades, fixations, and blinks) and 
a method to ass repetitive eye movement patterns, 

3. the development and evaluation of 90 feature 
derive from these eye movement types, and 

4. the implementation of a method for continuous EAR 
and it evaluation use a multiparticipant EOG data 
set involve a study of five real-world office 
activities. 

1.2 Paper Organization 

We first survey related work, introduce EOG, and describe 
the main eye movement characteristic that we identify a 
useful for EAR. We then detail and characterize the 
recognition methodology: the method use for remove 
drift and noise from EOG signals, and the algorithm 
developed for detect saccades, fixations, blinks, and for 
analyze repetitive eye movement patterns. Based on 
these eye movement characteristics, we develop 90 features; 
some directly derive from a particular characteristic, 
others devise to capture additional aspect of eye move- 
ment dynamics. 

We rank these feature use minimum redundancy 
maximum relevance (mRMR) feature selection and a 
support vector machine (SVM) classifier. To evaluate both 
algorithm on a real-world example, we devise an experi- 
ment involve a continuous sequence of five office 
activities, plus a period without any specific activity (the 
NULL class). Finally, we discus the finding gain from 
this experiment and give an outlook to future work. 

2 RELATED WORK 

2.1 Electrooculography Applications 

Eye movement characteristic such a saccades, fixations, 
and blinks, a well a deliberate movement pattern 
detect in EOG signals, have already be use for 
hands-free operation of static human-computer [11] and 
human-robot [12] interfaces. EOG-based interface have 
also be developed for assistive robot [13] or a a control 
for an electric wheelchair [14]. Such system be intend to 
be use by physically disabled people who have extremely 
limited peripheral mobility but still retain eye-motor 
coordination. These study show that EOG be a measure- 
ment technique that be inexpensive, easy to use, reliable, and 
relatively unobtrusive when compare to head-worn 
camera use in video-based eye trackers. While these 
application all use EOG a a direct control interface, our 
approach be to use EOG a a source of information on a 
person’s activity. 

2.2 Eye Movement Analysis 

A grow number of researcher use video-based eye 
track to study eye movement in natural environments. 
This have lead to important advance in our understand of 
how the brain process tasks, and of the role that the visual 
system play in this [15]. Eye movement analysis have a long 
history a a tool to investigate visual behavior. In an early 
study, Hacisalihzade et al. use Markov process to model 
visual fixation of observer recognize an object [16]. They 

transform fixation sequence into character string and 
use the string edit distance to quantify the similarity of eye 
movements. Elhelw et al. use discrete time Markov chain 
on sequence of temporal fixation to identify salient image 
feature that affect the perception of visual realism [17]. 
They found that fixation cluster be able to uncover the 
feature that most attract an observer’s attention. Dempere- 
Marco et al. present a method for training novice in 
assess tomography image [18]. They model the 
assessment behavior of domain expert base on the 
dynamic of their saccadic eye movements. Salvucci and 
Anderson evaluate mean for automate analysis of eye 
movement [19]. They described three method base on 
sequence-matching and hidden Markov model that inter- 
preted eye movement a accurately a human expert but 
in significantly less time. 

All of these study aim to model visual behavior 
during specific task use a small number of well-known 
eye movement characteristics. They explore the link 
between the task and eye movements, but do not recognize 
the task or activity use this information. 

2.3 Activity Recognition 

In ubiquitous computing, one goal of activity recognition be 
to provide information that allows a system to best assist 
the user with his or her task [20]. Traditionally, activity 
recognition research have focus on gait, posture, and 
gesture. Bao and Intille use body-worn accelerometer to 
detect 20 physical activities, such a cycling, walking, and 
scrub the floor, under real-world condition [21]. Logan 
et al. study a wide range of daily activities, such a use a 
dishwasher or watch television, use a large variety 
and number of ambient sensors, include RFID tag and 
infrared motion detector [22]. Ward et al. investigate the 
use of wrist-worn accelerometer and microphone in a 
wood workshop to detect activities, such a hammer or 
cut wood [4]. Several researcher investigate the 
recognition of reading activity in stationary and mobile 
setting use different eye track technique [6], [23]. 
Our work, however, be the first to describe and apply a 
general-purpose architecture for EAR to the problem of 
recognize everyday activities. 

3 BACKGROUND 

3.1 Electrooculography 

The eye can be model a a dipole with it positive pole at 
the cornea and it negative pole at the retina. Assuming a 
stable corneo-retinal potential difference, the eye be the 
origin of a steady electric potential field. The electrical 
signal that can be measure from this field be call the 
electrooculogram (EOG). 

If the eye move from the center position toward the 
periphery, the retina approach one electrode while the 
cornea approach the oppose one. This change in 
dipole orientation cause a change in the electric potential 
field and thus the measure EOG signal amplitude. By 
analyze these changes, eye movement can be tracked. 
Using two pair of skin electrode place at opposite side of 
the eye and an additional reference electrode on the forehead, 
two signal component (EOGh and EOGv), correspond to 
two movement components—a horizontal and a vertical— 
can be identified. EOG typically show signal amplitude 

742 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 33, NO. 4, APRIL 2011 



range from 5 to 20 �V =degree and an essential frequency 
content between 0 and 30 Hz [24]. 

3.2 Eye Movement Types 

To be able to use eye movement analysis for activity 
recognition, it be important to understand the different type 
of eye movement. We identify three basic eye movement 
type that can be easily detect use EOG: saccades, 
fixations, and blink (see Fig. 1). 

3.2.1 Saccades 

The eye do not remain still when view a visual scene. 
Instead, they have to move constantly to build up a mental 
“map” from interest part of that scene. The main reason 
for this be that only a small central region of the retina, the 
fovea, be able to perceive with high acuity. The simulta- 
neous movement of both eye be call a saccade. The 
duration of a saccade depends on the angular distance the 
eye travel during this movement: the so-called saccade 
amplitude. Typical characteristic of saccadic eye move- 
ments be 20 degree for the amplitude, and 10 to 100 m for 
the duration [25]. 

3.2.2 Fixations 

Fixations be the stationary state of the eye during which 
gaze be held upon a specific location in the visual scene. 
Fixations be usually define a the time between each two 
saccades. The average fixation duration lie between 100 
and 200 m [26]. 

3.2.3 Blinks 

The frontal part of the cornea be coat with a thin liquid 
film, the so-called “precornial tear film.” To spread this 
fluid across the corneal surface, regular opening and closing 
of the eyelids, or blinking, be required. The average blink 
rate varies between 12 and 19 blink per minute while at 
rest [27]; it be influence by environmental factor such a 
relative humidity, temperature, or brightness, but also by 
physical activity, cognitive workload, or fatigue [28]. The 
average blink duration lie between 100 and 400 m [29]. 

4 METHODOLOGY 

We first provide an overview of the architecture for EAR 
use in this work. We then detail our algorithm for 

remove baseline drift and noise from EOG signals, for 
detect the three basic eye movement types, and for 
analyze repetitive pattern of eye movements. Finally, we 
describe the feature extract from these basic eye move- 
ment type and introduce the minimum redundancy 
maximum relevance feature selection and the support 
vector machine classifier. 

4.1 Recognition Architecture 

Fig. 2 show the overall architecture for EAR. The method 
be all implement offline use MATLAB and C. Input 
to the processing chain be the two EOG signal capture 
the horizontal and the vertical eye movement components. 
In the first stage, these signal be process to remove any 
artifact that might hamper eye movement analysis. In the 
case of EOG signals, we apply algorithm for baseline drift 
and noise removal. Only this initial processing depends on 
the particular eye track technique used; all further stage 
be completely independent of the underlie type of 
eye movement data. In the next stage, three different eye 
movement type be detect from the process eye 
movement data: saccades, fixations, and blinks. The 
correspond eye movement event return by the 
detection algorithm be the basis for extract different 
eye movement feature use a slide window. In the last 
stage, a hybrid method selects the most relevant of these 
features, and us them for classification. 

4.2 EOG Signal Processing 

4.2.1 Baseline Drift Removal 

Baseline drift be a slow signal change superpose the EOG 
signal but mostly unrelated to eye movements. It have many 
possible sources, e.g., interfere background signal or 
electrode polarization [30]. Baseline drift only marginally 
influence the EOG signal during saccades; however, all 

BULLING ET AL.: EYE MOVEMENT ANALYSIS FOR ACTIVITY RECOGNITION USING ELECTROOCULOGRAPHY 743 

Fig. 1. Denoised and baseline drift remove horizontal (EOGh) and 
vertical (EOGv) signal components. Examples of the three main eye 
movement type be marked in gray: saccade (S), fixation (F), and 

blink (B). 

Fig. 2. Architecture for eye-based activity recognition on the example of 

EOG. Light gray indicates EOG signal processing; dark gray indicates 

use of a slide window. 



other eye movement be subject to baseline drift. In a five- 
electrode setup, a use in this work (see Fig. 8), baseline 
drift may also differ between the horizontal and vertical 
EOG signal component. 

Several approach to remove baseline drift from 
electrocardiography (ECG) signal have be propose 
(for example, see [31], [32], [33]). As ECG show repetitive 
signal characteristics, these algorithm perform sufficiently 
well at remove baseline drift. However, for signal with 
nonrepetitive characteristic such a EOG, develop 
algorithm for baseline drift removal be still an active area 
of research. We use an approach base on wavelet 
transform [34]. The algorithm first perform an approxi- 
mat multilevel 1D wavelet decomposition at level nine 
use Daubechies wavelet on each EOG signal component. 
The reconstruct decomposition coefficient give a base- 
line drift estimation. Subtracting this estimation from each 
original signal component yield the correct signal 
with reduce drift offset. 

4.2.2 Noise Removal 

EOG signal may be corrupt with noise from different 
sources, such a the residential power line, the measurement 
circuitry, electrodes, and wires, or other interfere physio- 
logical source such a electromyographic (EMG) signals. In 
addition, simultaneous physical activity may cause the 
electrode to loose contact or move on the skin. As mention 
before, EOG signal be typically nonrepetitive. This prohi- 
bit the application of denoising algorithm that make use of 
structural and temporal knowledge about the signal. 

Several EOG signal characteristic need to be preserve 
by the denoising. First, the steepness of signal edge need 
to be retain to be able to detect blink and saccades. 
Second, EOG signal amplitude need to be preserve to be 
able to distinguish between different type and direction of 
saccadic eye movements. Finally, denoising filter must not 
introduce signal artifact that may be misinterpret a 
saccade or blink in subsequent signal processing steps. 

To identify suitable method for noise removal, we 
compare three different algorithm on real and synthetic 
EOG data: a low-pass filter, a filter base on wavelet 
shrinkage denoising [35], and a median filter. By visual 
inspection of the denoised signal, we found that the median 
filter perform best; it preserve edge steepness of 
saccadic eye movements, retain EOG signal amplitudes, 
and do not introduce any artificial signal changes. It be 
crucial, however, to choose a window size Wmf that be small 
enough to retain short signal pulses, particularly those 
cause by blinks. A median filter remove pulse of a width 
small than about half of it window size. By take into 
account the average blink duration report earlier, we 
fix Wmf to 150 ms. 

4.3 Detection of Basic Eye Movement Types 

Different type of eye movement can be detect from the 
process EOG signals. In this work, saccades, fixations, 
and blink form the basis of all eye movement feature use 
for classification. The robustness of the algorithm for 
detect these be key to achieve good recognition 
performance. Saccade detection be particularly important 
because fixation detection, eye movement encoding, and the 

wordbook analysis be all reliant on it (see Fig. 2). In the 
following, we introduce our saccade and blink detection 
algorithm and characterize their performance on EOG 
signal record under constrain conditions. 

4.3.1 Saccade and Fixation Detection 

For saccade detection, we developed the so-called Contin- 
uous Wavelet Transform—Saccade Detection (CWT-SD) algo- 
rithm (see Fig. 3 for an example). Input to CWT-SD be the 
denoised and baseline drift remove EOG signal compo- 
nents EOGh and EOGv. CWT-SD first computes the 
continuous 1D wavelet coefficient at scale 20 use a Haar 
mother wavelet. Let s be one of these signal component 
and the mother wavelet. The wavelet coefficient Cab of s at 
scale a and position b be define 

Cab ðsÞ ¼ 
Z 

IR 

sðtÞ 1ffiffiffi 
a 
p t� b 

a 

� � 
dt: 

By apply an application-specific threshold thsd on the 
coefficient CiðsÞ ¼ C20i ðsÞ, CWT-SD creates a vector M with 
element Mi: 

Mi ¼ 
1; 8i : CiðsÞ < �thsd; 
�1; 8i : CiðsÞ > thsd; 
0; 8i : �thsd � CiðsÞ � thsd: 

8< 
: 

This step divide EOGh and EOGv in saccadic 
(M ¼ 1;�1) and nonsaccadic (fixational) (M ¼ 0) segments. 

Saccadic segment shorter than 20 m and longer than 
200 m be removed. These boundary approximate the 
typical physiological saccade characteristic described in 
literature [25]. CWT-SD then calculates the amplitude and 
direction of each detect saccade. The saccade amplitude 
SA be the difference in EOG signal amplitude before and 

744 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 33, NO. 4, APRIL 2011 

Fig. 3. Continuous Wavelet Transform—Saccade Detection algorithm. 
(a) Denoised and baseline drift remove horizontal EOG signal during 
reading with example saccade amplitude (SA); (b) the transform 
wavelet signal (EOGwl), with application-specific small (�thsmall) and 
large (�thlarge) thresholds; (c) marker vector for distinguish between 
small (Msmall) and large (Mlarge) saccades; and (d) example character 
encode for part of the EOG signal. 



after the saccade (c.f. Fig. 3). The direction be derive from 
the sign of the correspond element in M. Finally, each 
saccade be encode into a character represent the 
combination of amplitude and direction. For example, a 
small saccade in EOGh with negative direction get encode 
a “r” and a large saccade with positive direction a “L.” 

Humans typically alternate between saccade and fixa- 
tions. This allows u to also use CWT-SD for detect 
fixations. The algorithm exploit the fact that gaze remains 
stable during a fixation. This result in the correspond 
gaze points, i.e., the point in a visual scene that the gaze be 
direct at, to cluster together closely in time. Therefore, 
fixation can be identify by thresholding on the dispersion 
of these gaze point [36]. For a segment S of length n 
comprise of a horizontal Sh and a vertical Sv EOG signal 
component, the dispersion be calculate a 

DispersionðSÞ ¼ maxðShÞ �minðShÞ þmaxðSvÞ �minðSvÞ: 

Initially, all nonsaccadic segment be assume to 
contain a fixation. The algorithm then drop segment for 
which the dispersion be above a maximum threshold thfd of 
10,000 or if it duration be below a minimum threshold thfdt 
of 200 ms. The value of thfd be derive a part of the CWT- 
SD evaluation; that of thfdt approximates the typical average 
fixation duration report earlier. 

A particular activity may require saccadic eye movement 
of different distance and direction. For example, reading 
involves a fast sequence of small saccade while scan 
each line of text, while large saccade be require to jump 
back to the begin of the next line. We opt to detect 
saccade with two different amplitudes, “small” and “large.” 
This require two thresholds, thsdsmall and thsdlarge , to divide the 
range of possible value of C into three band (see Fig. 3): no 
saccade (�thsdsmall < C < thsdsmall ), small saccade (�thsdlarge < 
C < �thsdsmall or thsdsmall < C < thsdlarge ), and large saccade 
(C < �thsdlarge or C > thsdlarge ). Depending on it peak value, 
each saccade be then assign to one of these bands. 

To evaluate the CWT-SD algorithm, we perform an 
experiment with five participants—one female and four 
male (age: 25-59 years, mean ¼ 36:8, sd ¼ 15:4). To cover 
effect of difference in electrode placement and skin 
contact, the experiment be perform on two different 
days; in between day the participant take off the EOG 
electrodes. A total of 20 recording be make per 
participant, 10 per day. Each experiment involve track 
the participants’ eye while they follow a sequence of 
flash dot on a computer screen. We use a fix 
sequence to simplify label of individual saccades. The 
sequence be comprise of 10 eye movement consist of 
five horizontal and eight vertical saccades. This produce a 
total of 591 horizontal and 855 vertical saccades. 

By match saccade event with the annotate ground 
truth, we calculate true positive (TPs), false positive 
(FPs), and false negative (FNs), and from these, precision 
( TPTPþFP), recall ( 

TP 
TPþFN), and the F1 score (2 � 

precision�recall 
precisionþrecall). 

We then evaluate the F1 score across a sweep on the CWT- 
SD threshold thsd ¼ 1 . . . 50 (in 50 steps) separately for the 
horizontal and vertical EOG signal components. Fig. 4 
show the mean F1 score over all five participant with 
vertical line indicate the standard deviation for select 
value of thsd. What can be see from the figure be that 

similar threshold be use to achieve the top F1 score of 
about 0.94. It be interest to note that the standard 
deviation across all participant reach a minimum for a 
whole range of value around this maximum. This suggests 
that threshold that be also close to this point can be 
select that still achieve robust detection performance. 

4.3.2 Blink Detection 

For blink detection, we developed the Continuous Wavelet 
Transform—Blink Detection (CWT-BD) algorithm. Similarly to 
CWT-SD, the algorithm us a threshold thbd on the wavelet 
coefficient to detect blink in EOGv. In contrast to a saccade, 
a blink be characterize by a sequence of two large peak in 
the coefficient vector directly follow each other: one 
positive, the other negative. The time between these peak be 
small than the minimum time between two successive 
saccade rapidly perform in opposite direction. This be 
because, typically, two saccade have at least a short fixation 
in between them. For this reason, blink can be detect by 
apply a maximum threshold thbdt on this time difference. 

We evaluate our algorithm on EOG signal record in a 
stationary set from five participant look at different 
picture (two female and three males, age: 25-29 years, 
mean ¼ 26:4, sd ¼ 1:7). We label a total of 706 blink by 
visual inspection of the vertical EOG signal component. With 
an average blink rate of 12 blink per minute, this corresponds 
to about one hour of eye movement data. We evaluate CWT- 
BD over sweep of it two main parameters: thbd ¼ 
100 . . . 50;000 (in 500 steps) and thbdt ¼ 100 . . . 1;000 m (in 
10 steps). 

The F1 score be calculate by match blink event 
with the annotate ground truth. Fig. 6 show the F1 score 
for five select value of thbdt over all participants. CWT- 
BD performs best with thbdt between 400 and 600 m while 
reach top performance (F1 score: 0.94) use a thbdt of 
500 ms. Time difference outside this range, a exemplarily 
show for 300 and 1,000 ms, be already subject to a 
considerable drop in performance. This find nicely 
reflect the value for the average blink duration cite 
early from the literature. 

BULLING ET AL.: EYE MOVEMENT ANALYSIS FOR ACTIVITY RECOGNITION USING ELECTROOCULOGRAPHY 745 

Fig. 4. Evaluation of the CWT-SD algorithm for both EOG signal 
component use a sweep of it main parameter, the threshold thsd. 
The figure plot the mean F1 score over all five participants; vertical 
line show the standard deviation for select thsd. Maximum F1 score 
be indicate by a dash line. 



4.4 Analysis of Repetitive Eye Movement Patterns 

Activities such a reading typically involve characteristic 
sequence of several consecutive eye movement [6]. We 
propose encode eye movement by mapping saccade 

with different direction and amplitude to a discrete, 
character-based representation. Strings of these character 
be then collect in wordbook that be analyze to extract 
sequence information on repetitive eye movement patterns. 

4.4.1 Eye Movement Encoding 

Our algorithm for eye movement encode map the 
individual saccade information from both EOG component 
onto a single representation comprise of 24 discrete 
character (see Fig. 5a). This produce a representation that 

can be more efficiently process and analyzed. 
The algorithm take the CWT-SD saccade from the 

horizontal and vertical EOG signal component a it input. 
It first check for simultaneous saccade in both component 
a these represent diagonal eye movements. Simultaneous 

saccade be characterize by overlap saccade segment 
in the time domain. If no simultaneous saccade be detected, 

the saccade’s character be directly use to denote the eye 
movement. If two saccade be detected, the algorithm 
combine both accord to the follow scheme (see 
Fig. 5b): The character of two saccade with equally large 
EOG signal amplitude be merge to the character exactly in 
between (e.g., “l” and “u” become “n,” “R” and “U” become 
“B”). If simultaneous saccade differ by more than 50 percent 
in EOG signal amplitude, their character be merge to the 
closest neighbor character (e.g., “l” and “U” become “O”). 
This procedure encodes each eye movement into a distinct 
character, thus, mapping saccade of both EOG signal 
component into one eye movement sequence. 

4.4.2 Wordbook Analysis 

Based on the encode eye movement sequence, we propose a 
wordbook analysis to ass repetitive eye movement 
pattern (see Fig. 7). An eye movement pattern be define a 
a string of l successive characters. As an example with l ¼ 4, 
the pattern “LrBd” translates to large left (L)! small right (r) 
! large diagonal right (B) ! small down (d). A slide 
window of length l and a step size of one be use to scan the 
eye movement sequence for these patterns. Each newly 
found eye movement pattern be add to the correspond 
wordbook Wbl. For a pattern that be already include in 
Wbl, it occurrence count be increase by one. 

4.5 Feature Extraction 

We extract four group of feature base on the detect 
saccades, fixations, blinks, and the wordbook of eye 
movement patterns. Table 1 detail the name scheme 
use for all of these features. The feature be calculate 
use a slide window (window size Wfe and step size Sfe) 
on both EOGh and EOGv. From a pilot study, we be able 
to fix Wfe at 30 s and Sfe at 0.25 s. 

Features calculate from saccadic eye movement make 
up the large proportion of extract features. In total, 
there be 62 such feature comprise the mean, variance, 
and maximum EOG signal amplitude of saccade and the 
normalize saccade rates. These be calculate for both 

746 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 33, NO. 4, APRIL 2011 

Fig. 5. (a) Characters use to encode eye movement of different 
direction and distance: dark gray indicates basic and light gray diagonal 
directions. (b) Saccades detect in both EOG signal component and 
mapped to the eye movement sequence of the jumping point stimulus. 
Simultaneous saccade in both component be combine accord to 
their direction and amplitude (e.g., “l” and “u” become “n,” and “R” and 
“U” become “B”). 

Fig. 6. Evaluation of the CWT-BD algorithm over a sweep of the blink 
threshold thbd, for five different maximum time difference thbdt . The 
figure plot the mean F1 score over all participants; vertical line show 
the standard deviation for select thbd. Maximum F1 score be indicate 
by a dash line. 



EOGh and EOGv, for small and large saccades, for saccade 
in positive or negative direction, and for all possible 
combination of these. 

We calculate five different feature use fixations: the 
mean and variance of the EOG signal amplitude within a 
fixation,; the mean and the variance of fixation duration, 
and the fixation rate over window Wfe. 

For blinks, we extract three features: blink rate and the 

mean and variance of the blink duration. 
We use four wordbooks. This allows u to account for all 

possible eye movement pattern up to a length of four 
(l ¼ 4), with each wordbook contain the type and 
occurrence count of all pattern found. For each wordbook 
we extract five features: the wordbook size, the maximum 
occurrence count, the difference between the maximum and 
minimum occurrence counts, and the variance and mean of 
all occurrence counts. 

4.6 Feature Selection and Classification 

For feature selection, we chose a filter scheme over the 
commonly use wrapper approach because of the low 
computational cost and thus shorter runtime give the 
large data set. We use minimum redundancy maximum 
relevance feature selection for discrete variable [37], [38]. 
The mRMR algorithm selects a feature subset of arbitrary 
size S that best characterizes the statistical property of the 
give target class base on the ground truth labeling. In 
contrast to other method such a the F -test, mRMR also 
considers relationship between feature during the selec- 
tion. Among the possible underlie statistical measure 
described in the literature, mutual information be show 
to yield the most promising result and be thus select in 
this work. Our particular mRMR implementation combine 
the measure of redundancy and relevance among class 
use the mutual information difference (MID). 

For classification, we chose a linear support vector 
machine. Our SVM implementation us a fast sequential 
dual method for deal with multiple class [39], [40]. 
This reduces training time considerably while retain 
recognition performance. 

These two algorithm be combine into a hybrid feature 

selection and classification method. In a first step, mRMR 

rank all available feature (with S ¼ 90). During classifica- 
tion, the size of the feature set be then optimize with 

respect to recognition accuracy by sweep S. 

5 EXPERIMENT 

We design a study to establish the feasibility of EAR in a 

real-world setting. Our scenario involve five office-based 

activities—copying a text, reading a print paper, take 

handwritten notes, watch a video, and browsing the 

Web—and period during which participant take a rest (the 

NULL class). We chose these activity for three reasons. First, 

they be all commonly perform during a typical work 

day. Second, they exhibit interest eye movement pattern 

that be both structurally diverse and have vary level of 

complexity. We believe they represent the much broader 

range of activity observable in daily life. Finally, be able 

to detect these activity use on-body sensor such a EOG 

may enable novel attentive user interface that take into 

account cognitive aspect of interaction such a user inter- 

ruptibility or level of task engagement. 
Originally, we record 10 participants, but two be 

withdrawn due to poor signal quality: One participant have 

strong pathologic nystagmus. Nystagmus be a form of 

involuntary eye movement that be characterize by alternat- 

ing smooth pursuit in one direction and saccadic movement 

in the other direction. The horizontal EOG signal component 

turn out to be severely affected by the nystagmus and no 

reliable saccadic information could be extracted. For the 

second participant, most probably due to bad electrode 

placement, the EOG signal be completely distorted. 
All of the remain eight participant (two female and 

six males), age between 23 and 31 year (mean ¼ 26:1, 
sd ¼ 2:4) be daily computer users, reporting 6 to 14 hour 
of use per day (mean ¼ 9:5, sd ¼ 2:7). They be ask to 
follow two continuous sequences, each compose of five 

different, randomly order activities, and a period of rest 

(see Fig. 8b). For these, no activity be require of the 

participant but they be ask not to engage in any of the 

other activities. Each activity (including NULL) last about 

five minutes, result in a total data set of about eight hours. 

BULLING ET AL.: EYE MOVEMENT ANALYSIS FOR ACTIVITY RECOGNITION USING ELECTROOCULOGRAPHY 747 

Fig. 7. Example wordbook analysis for eye movement pattern of length 
l ¼ 3. A slide window scan a sequence of eye movement encode 
into character for repetitive patterns. Newly found pattern be add to 
the wordbook; otherwise, only the occurrence count (last column) be 
increase by one. 

TABLE 1 
Naming Scheme for the Features Used in this Work 

For a particular feature, e.g., S-rateSPHor, the capital letter represent 
the group—saccadic (S), blink (B), fixation (F), or wordbook (W)—and 
the combination of abbreviation after the dash describes the particular 
type of feature and the characteristic it covers. 



5.1 Apparatus 

We use a commercial EOG device, the Mobi8, from 
Twente Medical Systems International (TMSI). It be worn 
on a belt around each participant’s waist and record a 
four-channel EOG at a sample rate of 128 Hz. Participants 
be observe by an assistant who annotate activity 
change with a wireless remote control. Data record and 
synchronization be handle by the Context Recognition 
Network Toolbox [41]. 

EOG signal be picked up use an array of five 24 mm 
Ag/AgCl wet electrode from Tyco Healthcare place 
around the right eye. The horizontal signal be collect 
use one electrode on the nose and another directly across 
from this on the edge of the right eye socket. The vertical 
signal be collect use one electrode above the right 
eyebrow and another on the low edge of the right eye 
socket. The fifth electrode, the signal reference, be place 
in the middle of the forehead. Five participant (two female 
and three males) wore spectacle during the experiment. For 
these participants, the nose electrode be move to the side 
of the left eye to avoid interference with the spectacle (see 
Fig. 8a). 

The experiment be carry out in an office during 
regular work hours. Participants be seat in front of 
two adjacent 17 inch flat screen with a resolution of 1;280� 
1;024 pixel on which a browser, a video player, a word 
processor, and text for copying be on-screen and ready 
for use. Free movement of the head and upper body be 
possible throughout the experiment. 

5.2 Procedure 

For the text copying task, the original document be show 
on the right screen with the word processor on the left 
screen. Participants could copy the text in different ways. 
Some touch typed and only checked for error in the text 
from time to time; others continuously switch attention 
between the screen or the keyboard while typing. Because 
the screen be more than half a meter from the 
participants’ faces, the video be show full screen to elicit 
more distinct eye movements. For the browsing task, no 
constraint be impose concern the type of Website or 
the manner of interaction. For the reading and write tasks, 
a book (12 pt, one column with pictures) and a pad with a 
pen be provided. 

5.3 Parameter Selection and Evaluation 

The same saccade and blink detection parameter be use 
throughout the evaluation: thbd ¼ 23;438, thbdt ¼ 390 ms, 
thsdlarge ¼ 13;750, and thsdsmall ¼ 2;000. The selection of thsdsmall 
be base on the typical length of a short scan saccade 
during reading, and thsdlarge on the length of a typical 
newline movement. 

Classification and feature selection be evaluate use 
a leave-one-person-out scheme: We combine the data set 
of all but one participant and use this for training; test 
be do use both data set of the remain participant. 
This be repeat for each participant. The result train 
and test set be standardize to have zero mean and a 
standard deviation of one. Feature selection be always 
perform solely on the training set. The two main 
parameter of the SVM algorithm, the cost C and the 
tolerance of termination criterion �, be fix to C ¼ 1 and 
� ¼ 0:1. For each leave-one-person-out iteration, the predic- 
tion vector return by the SVM classifier be smooth 
use a slide majority window. Its main parameter, the 
window size Wsm, be obtain use a parameter sweep 
and fix at 2.4 s. 

6 RESULTS 

6.1 Classification Performance 

SVM classification be score use a frame-by-frame 
comparison with the annotate ground truth. For specific 
result on each participant or on each activity, class-relative 
precision and recall be used. 

Table 2 show the average precision and recall, and the 
correspond number of feature select for each 
participant. The number of feature use varied from only 
nine feature (P8) up to 81 feature (P1). The mean 
performance over all participant be 76.1 percent preci- 
sion and 70.5 percent recall. P4 report the bad result, 
with both precision and recall below 50 percent. In contrast, 
P7 achieve the best result, indicate by recognition 

748 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 33, NO. 4, APRIL 2011 

Fig. 8. (a) Electrode placement for EOG data collection (h: horizontal, 
v: vertical, and r: reference). (b) Continuous sequence of five typical 
office activities: copying a text, reading a print paper, take 
handwritten notes, watch a video, browsing the Web, and period 
of no specific activity (the NULL class). 

TABLE 2 
Precision, Recall, and the Corresponding Number of Features Selected by the Hybrid mRMR/SVM Method for Each Participant 

The participants’ gender be give in brackets; best and bad case result be indicate in bold. 



performance in the 80 and 90 and use a moderate-sized 
feature set. 

Fig. 9 plot the classification result in term of precision 
and recall for each activity and participant. The best result 
approach the top right corner, while bad result be close 
to the low left. For most activities, precision and recall fall 
within the top right corner. Recognition of reading and 
copying, however, completely fails for P4, and browsing 
also show noticeably low precision. Similar but less 

strong characteristic apply for the reading, writing, and 
browsing task for P5. 

The sum confusion matrix from all participants, 
normalize across ground truth rows, be give in Fig. 10. 
Correct recognition be show on the diagonal; substitution 
error be off-diagonal. The large between-class substitu- 

tion error not involve NULL fall between 12 and 
13 percent of their class times. Most of these error involve 
browsing that be falsely return during 13 percent each of 

read, write, and copy activities. A similar amount be 
substitute by read during browse time. 

6.2 Eye Movement Features 

We first analyze how mRMR ranked the feature on each 
of the eight leave-one-person-out training sets. The rank of a 
feature be the position at which mRMR select it within a 
set. The position corresponds to the importance with which 
mRMR ass the feature’s ability to discriminate between 
class in combination with the feature ranked before it. 
Fig. 11 show the top 15 feature accord to the median 
rank over all set (see Table 1 for a description of the type 
and name of the features). Each vertical bar represent the 
spread of mRMR ranks: For each feature, there be one rank 
per training set. The most useful feature be those found 
with the high rank (close to one) for most training sets, 
indicate by shorter bars. Some feature be not always 
include in the final result (e.g., feature 63 only appear in 
five sets). Equally, a useful feature that be ranked lowly by 
mRMR might be the one that improves a classification (e.g., 

BULLING ET AL.: EYE MOVEMENT ANALYSIS FOR ACTIVITY RECOGNITION USING ELECTROOCULOGRAPHY 749 

Fig. 9. Precision and recall for each activity and participant. Mean 

performance (P1 to P8) be marked by a star. 

Fig. 10. Summed confusion matrix from all participants, normalize 

across ground truth rows. 

Fig. 11. The top 15 feature select by mRMR for all eight training sets. The X-axis show feature number and group; the key on the right show the 
correspond feature name a described in Table 1; the Y-axis show the rank (top = 1). For each feature, the bar show: the total number of 
training set for which the feature be chosen (bold number at the top), the rank of the feature within each set (dots, with a number represent the 
set count), and the median rank over all set (black star). For example, a useful feature be 47 (S)—a saccadic feature select for all sets, in seven of 
which it be ranked one or two; less useful be 63 (B)—a blink feature use in only five set and ranked between 4 and 29. 



feature 68 be spread between rank five and 26, but be 
include in all eight sets). 

This analysis reveals that the top three features, a 
judged by high rank for all sets, be all base on 
horizontal saccades: 47 (S-rateSPHor), 56 (S-maxAmpPHor), 
and 10 (S-meanAmpSHor). Feature 68 (F-rate) be use in all 
sets, seven of which rank it highly. Feature 63 (B-rate) be 
select for five out of the eight sets, only one of which 
give it a high rank. Wordbook feature 77 (W-maxCount- 
l2) and 85 (W-maxCount-l3) be not use in one of the sets, 
but they be highly ranked by the other seven. 

We perform an additional study into the effect of 
optimize mRMR for each activity class. We combine all 
training set and perform a one-versus-many mRMR for 
each non-NULL activity. The top five feature select 
during this evaluation be show in Table 3. For example, 
the table reveals that reading and browsing can be 
described use wordbook features. Writing require addi- 
tional fixation features. Watching video be characterize by 
a mixture of fixation and saccade feature for all direction 
and—as reading—the blink rate, while copying involves 
mainly horizontal saccade features. 

7 DISCUSSION 

7.1 Robustness across Participants 

The developed algorithm for detect saccade and blink 
in EOG signal prove robust and achieve F1 score of up 
to 0.94 across several people (see Figs. 4 and 6). For the 
experimental evaluation, the parameter of both algorithm 
be fix to value common for all participants; the same 
applies to the parameter of the feature selection and 
classification algorithms. Under these conditions, despite 
person-independent training, six out of the eight partici- 
pant return best average precision and recall value of 
between 69 and 93 percent. 

Two participants, however, return result that be 
low than 50 percent. On closer inspection of the raw eye 
movement data, it turn out that for both the EOG, signal 
quality be poor. Changes in signal amplitude for saccade 
and blinks—upon which feature extraction and thus 
recognition performance directly depend—were not distinc- 
tive enough to be reliably detected. As be found in an early 
study [6], dry skin or poor electrode placement be the most 
likely culprits. Still, the achieve recognition performance be 
promising for eye movement analysis to be implement in 
real-world applications, for example, a part of a reading 
assistant, or for monitoring workload to ass the risk of 
burnout syndrome. For such applications, recognition per- 
formance may be further increase by combine eye move- 
ment analysis with additional sense modalities. 

7.2 Results for Each Activity 

As might have be expected, reading be detect with 
comparable accuracy to that report early [6]. However, 
the method use be quite different. The string match 
approach apply in the early study make use of a specific 
“reading pattern.” That approach be not suit for activity 
involve less homogeneous eye movement patterns. For 
example, one would not expect to find a similarly unique 
pattern for browsing or watch a video a there exists for 
reading. This be because eye movement show much more 
variability during these activity a they be driven by an 
ever-changing stimulus. As show here, the feature-based 
approach be much more flexible and scale good with the 
number and type of activity that be to be recognized. 

Accordingly, we be now able to recognize four 
additional activities—Web browsing, write on paper, 
watch video, and copying text—with almost, or above, 
70 percent precision and 70 percent recall. Particularly 
impressive be video, with an average precision of 88 percent 
and recall of 80 percent. This be indicative of a task where 
the user might be concentrate on a relatively small field of 
view (like reading), but follow a typically unstructured 
path (unlike reading). Similar example outside the current 
study might include interact with a graphical user 
interface or watch television at home. Writing be similar 
to reading in that the eye follow a structure path, albeit 
at a slow rate. Writing involves more eye “distractions”— 
when the person look up to think for example. Browsing 
be recognize less well over all participant (average 
precision 79 percent and recall 63 percent)—but with a 
large spread between people. A likely reason for this be that 
it be not only unstructured, but also it involves a variety of 
subactivities—including reading—that may need to be 
modeled. The copy activity, with an average precision of 
76 percent and a recall of 66 percent, be representative of 
activity with a small field of view that include regular 
shift in attention (in this case, to another screen). A 
comparable activity outside the chosen office scenario 
might be driving, where the eye be on the road ahead 
with occasional check to the side mirrors. Finally, the 
NULL class return a high recall of 81 percent. However, 
there be many false return (activity false negatives) for 
half of the participants, result in a precision of only 
66 percent. 

Three of these activities—writing, copying, and browsing 
—all include section of reading. From quick check over 
what have be write or copy to longer perusal of online 
text, reading be a pervasive subactivity in this scenario. This be 
confirm by the relatively high rate of confusion error 
involve reading, a show in Fig. 10. 

750 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 33, NO. 4, APRIL 2011 

TABLE 3 
The Top Five Features Selected by mRMR for Each Activity over All Training Sets (See Table 1 for Details on Feature Names) 



7.3 Feature Groups 

The feature group select by mRMR provide a snapshot 
of the type of eye movement feature useful for activity 
recognition. 

Features from three of the four propose groups— 
saccade, fixation, and wordbook—were all prominently 
represent in our study. The fact that each group cover 
complementary aspect of eye movement be promising for 
the general use of these feature for other EAR problems. 
Note that no one feature type performs well alone. The best 
result be obtain use a mixture of different features. 
Among these, the fixation rate be always selected. This 
result be akin to that of Canosa, who found that both fixation 
duration and saccade amplitude be strong indicator of 
certain activity [42]. 

Features derive from blink be less represent in the 
top ranks. One explanation for this be that for the short activity 
duration of only five minutes, the participant do not 
become fully engage in the tasks, and be thus less likely 
to show the characteristic blink rate variation suggest by 
Palomba et al. [43]. These feature may be found to be more 
discriminative for longer duration activities. Coupled with 
the ease by which they be extracted, we believe blink 
feature be still promising for future work. 

7.4 Features for Each Activity Class 

The analysis of the most important feature for each activity 
class be particularly revealing. 

Reading be a regular pattern characterize by a specific 
sequence of saccade and short fixation of similar duration. 
Consequently, mRMR chose mostly wordbook feature 
describe eye movement sequence in it top ranks, a 
well a a feature describe the fixation duration variance. 
The fifth feature, the blink rate, reflect that, for reading a 
an activity of high visual engagement, people tend to blink 
less [43]. 

Browsing be structurally diverse and—depending on the 
Website be viewed—may be comprise of different 
activities, e.g., watch a video, typing, or look at a 
picture. In addition to the small, horizontal saccade rate, 
mRMR also select several workbook feature of vary 
lengths. This be probably due to our participants’ browsing 
activity contain mostly sequence of variable length 
reading such a scan headline or search for a 
product in a list. 

Writing be similar to reading, but require great fixation 
duration (it take longer to write a word than to read it) and 
great variance. mRMR correspondingly select average 
fixation duration and it variance a well a a wordbook 
feature. However, write be also characterize by short 
think pauses, during which people invariably look up. 
This corresponds extremely well to the choice of the fixation 
feature that capture variance in vertical position. 

Watching a video be a highly unstructured activity, but be 
carry out within a narrow field of view. The lack of 
wordbook feature reflect this, a do the mixed selection 
of feature base on all three types: variance of both 
horizontal and vertical fixation positions, small positive and 
negative saccadic movements, and blink rate. The use of 
blink rate likely reflect the tendency toward blink inhibi- 
tion when perform an engage yet sedentary task [43]. 

Finally, copying involves many back and forth saccade 
between screens. mRMR reflect this by choose a mixture 

of small and large horizontal saccade features, a well a 
variance in horizontal fixation positions. 

These result suggest that for task that involve a know 
set of specific activity classes, recognition can be optimize 
by only choose feature know to best describe these 
classes. It remains to be investigate how well such 
prototype feature discriminate between activity class 
with very similar characteristics. 

7.5 Activity Segmentation Using Eye Movements 

Segmentation—the task of spot individual activity 
instance in continuous data—remains an open challenge 
in activity recognition. We found that eye movement can 
be use for activity segmentation on different level 
depend on the timescale of the activities. The low 
level of segmentation be that of individual saccade that 
define eye movement in different directions—“left,” 
“right,” and so on. An example for this be the end-of-line 
“carriage return” eye movement perform during reading. 
The next level include more complex activity that involve 
sequence compose of a small number of saccades. For 
these activities, the wordbook analysis propose in this 
work may prove suitable. In early work, such short eye 
movement patterns, so-called eye gestures, be success- 
fully use for eye-based human-computer interaction [44]. 
At the high level, activity be characterize by complex 
combination of eye movement sequence of potentially 
arbitrary length. Unless wordbook be use that span these 
long sequences, dynamic model of activity be required. 
For this, it would be interest to investigate method such 
a hidden Markov model (HMM), Conditional Random 
Fields (CRF), or an approach base on eye movement 
grammars. These method would allow u to model eye 
movement pattern at different hierarchical levels, and to 
spot composite activity from large stream of eye move- 
ment data more easily. 

7.6 Limitations 

One limitation of the current work be that the experimental 
scenario consider only a handful of activities. It be 
important to note, however, that the recognition architec- 
ture and feature set be developed independently of these 
activities. In addition, the method be not limited to EOG. All 
feature can be extract equally well from eye movement 
data record use a video-based eye tracker. This 
suggests that our approach be applicable to other activities, 
settings, and eye track techniques. 

The study also reveals some of the complexity one might 
face in use the eye a a source of information on a person’s 
activity. The ubiquity of the eyes’ involvement in everything 
a person do mean that it be challenge to annotate 
precisely what be be “done” at any one time. It be also a 
challenge to define a single identifiable activity. Reading be 
perhaps one of the easy to capture because of the intensity 
of eye focus that be require and the well-defined path that 
the eye follow. A task such a Web browsing be more 
difficult because of the wide variety of different eye move- 
ments involved. It be challenging, too, to separate relevant 
eye movement from momentary distractions. 

These problem may be solved, in part, by use video 
and gaze track for annotation. Activities from the current 
scenario could be redefine at a small timescale, break 
browsing into small activity such a “use scroll bar,” 

BULLING ET AL.: EYE MOVEMENT ANALYSIS FOR ACTIVITY RECOGNITION USING ELECTROOCULOGRAPHY 751 



“read,” “look at image,” or “type.” This would also allow u 
to investigate more complicate activity outside the office. 
An alternative route be to study activity at large timescales, 
to perform situation analysis rather than recognition of 
specific activities. Long-term eye movement features, e.g., 
the average eye movement velocity and blink rate over one 
hour, might reveal whether a person be walk along an 
empty or busy street, whether they be at their desk working, 
or whether they be at home watch television. Annotation 
will still be an issue, but one that maybe alleviate use 
unsupervised or self-labeling method [21], [45]. 

7.7 Considerations for Future Work 

Additional eye movement characteristic that be potentially 
useful for activity recognition—such a pupil dilation, 
microsaccades, vestibulo-ocular reflex, or smooth pursuit 
movements—were not use here because of the difficulty in 
measure them with EOG. These characteristic be still 
worth investigate in the future a they may carry informa- 
tion that complement that available in the current work. 

Eye movement also reveal information on cognitive 
process of visual perception, such a visual memory, 
learning, or attention. If it be possible to infer these 
process from eye movements, this may lead to cognition- 
aware system that be able to sense and adapt to a person’s 
cognitive state [46]. 

8 CONCLUSION 

This work reveals two main finding for activity recognition 
use eye movement analysis. First, we show that eye 
movement alone, i.e., without any information on gaze, 
can be use to successfully recognize five office activities. 
We argue that the developed methodology can be extend 
to other activities. Second, good recognition result be 
achieve use a mixture of feature base on the 
fundamental of eye movements. Sequence information on 
eye movement patterns, in the form of a wordbook analysis, 
also prove useful and can be extend to capture 
additional statistical properties. Different recognition task 
will likely require different combination of these features. 

The importance of these finding lie in their significance 
for eye movement analysis to become a general tool for the 
automatic recognition of human activity. 

REFERENCES 
[1] S. Mitra and T. Acharya, “Gesture Recognition: A Survey,” IEEE 

Trans. Systems, Man, and Cybernetics, Part C: Applications and Rev., 
vol. 37, no. 3, pp. 311-324, May 2007. 

[2] P. Turaga, R. Chellappa, V.S. Subrahmanian, and O. Udrea, 
“Machine Recognition of Human Activities: A Survey,” IEEE 
Trans. Circuits and Systems for Video Technology, vol. 18, no. 11, 
pp. 1473-1488, Nov. 2008. 

[3] B. Najafi, K. Aminian, A. Paraschiv-Ionescu, F. Loew, C.J. Bula, 
and P. Robert, “Ambulatory System for Human Motion Analysis 
Using a Kinematic Sensor: Monitoring of Daily Physical Activity 
in the Elderly,” IEEE Trans. Biomedical Eng., vol. 50, no. 6, pp. 711- 
723, June 2003. 

[4] J.A. Ward, P. Lukowicz, G. Tröster, and T.E. Starner, “Activity 
Recognition of Assembly Tasks Using Body-Worn Microphones 
and Accelerometers,” IEEE Trans. Pattern Analysis and Machine 
Intelligence, vol. 28, no. 10, pp. 1553-1567, Oct. 2006. 

[5] N. Kern, B. Schiele, and A. Schmidt, “Recognizing Context for 
Annotating a Live Life Recording,” Personal and Ubiquitous 
Computing, vol. 11, no. 4, pp. 251-263, 2007. 

[6] A. Bulling, J.A. Ward, H. Gellersen, and G. Tröster, “Robust 
Recognition of Reading Activity in Transit Using Wearable 
Electrooculography,” Proc. Sixth Int’l Conf. Pervasive Computing, 
pp. 19-37, 2008. 

[7] S.P. Liversedge and J.M. Findlay, “Saccadic Eye Movements and 
Cognition,” Trends in Cognitive Sciences, vol. 4, no. 1, pp. 6-14, 2000. 

[8] J.M. Henderson, “Human Gaze Control during Real-World Scene 
Perception,” Trends in Cognitive Sciences, vol. 7, no. 11, pp. 498-504, 
2003. 

[9] A. Bulling, D. Roggen, and G. Tröster, “Wearable EOG Goggles: 
Seamless Sensing and Context-Awareness in Everyday Environ- 
ments,” J. Ambient Intelligence and Smart Environments, vol. 1, no. 2, 
pp. 157-171, 2009. 

[10] A. Bulling, J.A. Ward, H. Gellersen, and G. Tröster, “Eye 
Movement Analysis for Activity Recognition,” Proc. 11th Int’l 
Conf. Ubiquitous Computing, pp. 41-50, 2009. 

[11] Q. Ding, K. Tong, and G. Li, “Development of an EOG (Electro- 
Oculography) Based Human-Computer Interface,” Proc. 27th Int’l 
Conf. Eng. in Medicine and Biology Soc., pp. 6829-6831, 2005. 

[12] Y. Chen and W.S. Newman, “A Human-Robot Interface Based on 
Electrooculography,” Proc. IEEE Int’l Conf. Robotics and Automa- 
tion, vol. 1, pp. 243-248, 2004. 

[13] W.S. Wijesoma, K.S. Wee, O.C. Wee, A.P. Balasuriya, K.T. San, and 
K.K. Soon, “EOG Based Control of Mobile Assistive Platforms for 
the Severely Disabled,” Proc. IEEE Int’l Conf. Robotics and 
Biomimetics, pp. 490-494, 2005. 

[14] R. Barea, L. Boquete, M. Mazo, and E. Lopez, “System for Assisted 
Mobility Using Eye Movements Based on Electrooculography,” 
IEEE Trans. Neural Systems and Rehabilitation Eng., vol. 10, no. 4, 
pp. 209-218, Dec. 2002. 

[15] M.M. Hayhoe and D.H. Ballard, “Eye Movements in Natural 
Behavior,” Trends in Cognitive Sciences, vol. 9, pp. 188-194, 2005. 

[16] S.S. Hacisalihzade, L.W. Stark, and J.S. Allen, “Visual Perception 
and Sequences of Eye Movement Fixations: A Stochastic Modeling 
Approach,” IEEE Trans. Systems, Man, and Cybernetics, vol. 22, no. 
3, pp. 474-481, May/June 1992. 

[17] M. Elhelw, M. Nicolaou, A. Chung, G.-Z. Yang, and M.S. Atkins, 
“A Gaze-Based Study for Investigating the Perception of Visual 
Realism in Simulated Scenes,” ACM Trans. Applied Perception, 
vol. 5, no. 1, pp. 1-20, 2008. 

[18] L. Dempere-Marco, X. Hu, S.L.S. MacDonald, S.M. Ellis, D.M. 
Hansell, and G.-Z. Yang, “The Use of Visual Search for Knowl- 
edge Gathering in Image Decision Support,” IEEE Trans. Medical 
Imaging, vol. 21, no. 7, pp. 741-754, July 2002. 

[19] D.D. Salvucci and J.R. Anderson, “Automated Eye-Movement 
Protocol Analysis,” Human-Computer Interaction, vol. 16, no. 1, 
pp. 39-86, 2001. 

[20] D. Abowd, A. Dey, R. Orr, and J. Brotherton, “Context-Awareness 
in Wearable and Ubiquitous Computing,” Virtual Reality, vol. 3, 
no. 3, pp. 200-211, 1998. 

[21] L. Bao and S.S. Intille, “Activity Recognition from User-Annotated 
Acceleration Data,” Proc. Second Int’l Conf. Pervasive Computing, 
pp. 1-17, 2004. 

[22] B. Logan, J. Healey, M. Philipose, E. Tapia, and S.S. Intille, “A 
Long-Term Evaluation of Sensing Modalities for Activity Recog- 
nition,” Proc. Ninth Int’l Conf. Ubiquitous Computing, pp. 483-500, 
2007. 

[23] F.T. Keat, S. Ranganath, and Y.V. Venkatesh, “Eye Gaze Based 
Reading Detection,” Proc. IEEE Conf. Convergent Technologies for the 
Asia-Pacific Region, vol. 2, pp. 825-828, 2003. 

[24] M. Brown, M. Marmor, and Vaegan, “ISCEV Standard for Clinical 
Electro-Oculography (EOG),” Documenta Ophthalmologica, vol. 113, 
no. 3, pp. 205-212, 2006. 

[25] A.T. Duchowski, Eye Tracking Methodology: Theory and Practice. 
Springer-Verlag New York, Inc., 2007. 

[26] B.R. Manor and E. Gordon, “Defining the Temporal Threshold for 
Ocular Fixation in Free-Viewing Visuocognitive Tasks,” 
J. Neuroscience Methods, vol. 128, nos. 1/2, pp. 85-93, 2003. 

[27] C.N. Karson, K.F. Berman, E.F. Donnelly, W.B. Mendelson, J.E. 
Kleinman, and R.J. Wyatt, “Speaking, Thinking, and Blinking,” 
Psychiatry Research, vol. 5, no. 3, pp. 243-246, 1981. 

[28] R. Schleicher, N. Galley, S. Briest, and L. Galley, “Blinks and 
Saccades a Indicators of Fatigue in Sleepiness Warnings: Looking 
Tired?” Ergonomics, vol. 51, no. 7, pp. 982-1010, 2008. 

[29] H.R. Schiffman, Sensation and Perception: An Integrated Approach, 
fifth ed. John Wiley & Sons, 2001. 

752 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 33, NO. 4, APRIL 2011 



[30] J.J. Gu, M. Meng, A. Cook, and G. Faulkner, “A Study of Natural 
Eye Movement Detection and Ocular Implant Movement Control 
Using Processed EOG Signals,” Proc. IEEE Int’l Conf. Robotics and 
Automation, vol. 2, pp. 1555-1560, 2001. 

[31] N. Pan, V.M. I, M.P. Un, and P.S. Hang, “Accurate Removal of 
Baseline Wander in ECG Using Empirical Mode Decomposition,” 
Proc. Joint Meeting Sixth Int’l Symp. Noninvasive Functional Source 
Imaging of the Brain and Heart and the Int’l Conf. Functional 
Biomedical Imaging, pp. 177-180, 2007. 

[32] V.S. Chouhan and S.S. Mehta, “Total Removal of Baseline Drift 
from ECG Signal,” Proc. 17th Int’l Conf. Computer Theory and 
Applications, pp. 512-515, 2007. 

[33] L. Xu, D. Zhang, and K. Wang, “Wavelet-Based Cascaded 
Adaptive Filter for Removing Baseline Drift in Pulse Waveforms,” 
IEEE Trans. Biomedical Eng., vol. 52, no. 11, pp. 1973-1975, Nov. 
2005. 

[34] M.A. Tinati and B. Mozaffary, “A Wavelet Packets Approach to 
Electrocardiograph Baseline Drift Cancellation,” Int’l J. Biomedical 
Imaging, vol. 2006, pp. 1-9, 2006. 

[35] D.L. Donoho, “De-Noising by Soft-Thresholding,” IEEE Trans. 
Information Theory, vol. 41, no. 3, pp. 613-627, May 1995. 

[36] D.D. Salvucci and J.H. Goldberg, “Identifying Fixations and 
Saccades in Eye-Tracking Protocols,” Proc. Symp. Eye Tracking 
Research & Applications, pp. 71-78, 2000. 

[37] H. Peng, F. Long, and C. Ding, “Feature Selection Based on 
Mutual Information Criteria of Max-Dependency, Max-Relevance, 
and Min-Redundancy,” IEEE Trans. Pattern Analysis and Machine 
Intelligence, vol. 27, no. 8, pp. 1226-1238, Aug. 2005. 

[38] H. Peng, “mRMR Feature Selection Toolbox for MATLAB,” 
http://research.janelia.org/peng/proj/mRMR/, Feb. 2008. 

[39] K. Crammer and Y. Singer, “Ultraconservative Online Algorithms 
for Multiclass Problems,” J. Machine Learning Research, vol. 3, 
pp. 951-991, 2003. 

[40] C.-J. Lin, “LIBLINEAR—A Library for Large Linear Classifica- 
tion,” http://www.csie.ntu.edu.tw/~cjlin/liblinear/, Feb. 2008. 

[41] D. Bannach, P. Lukowicz, and O. Amft, “Rapid Prototyping of 
Activity Recognition Applications,” IEEE Pervasive Computing, 
vol. 7, no. 2, pp. 22-31, Apr.-June 2008. 

[42] R.L. Canosa, “Real-World Vision: Selective Perception and Task,” 
ACM Trans. Applied Perception, vol. 6, no. 2, pp. 1-34, 2009. 

[43] D. Palomba, M. Sarlo, A. Angrilli, A. Mini, and L. Stegagno, 
“Cardiac Responses Associated with Affective Processing of 
Unpleasant Film Stimuli,” Int’l J. Psychophysiology, vol. 36, no. 1, 
pp. 45-57, 2000. 

[44] A. Bulling, D. Roggen, and G. Tröster, “It’s in Your Eyes— 
Towards Context-Awareness and Mobile HCI Using Wearable 
EOG Goggles,” Proc. 10th Int’l Conf. Ubiquitous Computing, pp. 84- 
93, 2008. 

[45] T. Huynh, M. Fritz, and B. Schiele, “Discovery of Activity Patterns 
Using Topic Models,” Proc. 10th Int’l Conf. Ubiquitous Computing, 
pp. 10-19, 2008. 

[46] A. Bulling, D. Roggen, and G. Tröster, “What’s in the Eyes for 
Context-Awareness?” IEEE Pervasive Computing, 2010, 
doi:10.1109/MPRV.2010.49. 

Andreas Bulling receive the MSc degree in 
computer science from the Technical University 
of Karlsruhe, Germany, in 2006, and the PhD 
degree in information technology and electrical 
engineering from the Swiss Federal Institute of 
Technology (ETH) Zurich, Switzerland, in 2010. 
His research interest be in cognition-aware 
system and multimodal activity and context 
recognition with application in ubiquitous com- 
put and human-computer interaction. He be 

currently a postdoctoral research associate at the University of Cam- 
bridge, United Kingdom, and Lancaster University, United Kingdom, 
fund by a Feodor Lynen Research Fellowship of the Alexander von 
Humboldt Foundation, Germany. He be a student member of the IEEE. 
For more details, see http://www.andreas-bulling.eu/. 

Jamie A. Ward receive the BEng degree with 
joint honor in computer science and electronics 
from the University of Edinburgh, Scotland, in 
2000, and the PhD degree in activity recognition 
(AR) use body-worn sensor from the Swiss 
Federal Institute of Technology (ETH) Zurich in 
2006. He spent a year work a an analogue 
circuit designer in Austria before join the 
Wearable Computing Lab at ETH in Zurich, 
Switzerland. In addition to his work on AR, he be 

actively involve in develop improve method for evaluate AR 
performance. He be currently a research associate at Lancaster 
University, United Kingdom. 

Hans Gellersen receive the MSc and PhD 
degree in computer science from the Technical 
University of Karlsruhe, Germany, in 1996. From 
1993 to 1996, he be a research assistant at the 
Telematics Institute, and from 1996 to 2000, be 
a director of the Telecooperation Office. Since 
2001, he have be a professor for interactive 
system at Lancaster University, United King- 
dom. His research interest be in ubiquitous 
compute and context-aware systems. He be 

actively involve in the formation of the ubiquitous compute research 
community and have initiate the UbiComp conference series. He also 
serf a an editor of the Personal and Ubiquitous Computing Journal. 

Gerhard Tröster receive the MSc degree in 
electrical engineering from the Technical 
University of Karlsruhe, Germany, in 1978, 
and the PhD degree in electrical engineering 
from the Technical University of Darmstadt, 
Germany, in 1984. During eight year at 
Telefunken, Germany, he be responsible 
for various research project focus on key 
component of digital telephony. Since 1993, 
he have be a professor for wearable 

compute at the Swiss Federal Institute of Technology (ETH) in 
Zurich, Switzerland. His field of research include wearable 
computing, smart textiles, electronic packaging, and miniaturize 
digital signal processing. He be a senior member of the IEEE. 

. For more information on this or any other compute topic, 
please visit our Digital Library at www.computer.org/publications/dlib. 

BULLING ET AL.: EYE MOVEMENT ANALYSIS FOR ACTIVITY RECOGNITION USING ELECTROOCULOGRAPHY 753 


