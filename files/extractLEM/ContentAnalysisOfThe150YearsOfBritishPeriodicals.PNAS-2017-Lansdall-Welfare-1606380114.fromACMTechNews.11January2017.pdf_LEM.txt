


















































PN 
A 

S 
PL 

U 
S 

CO 
M 

PU 
TE 

R 
SC 

IE 
N 

CE 
S 

SO 
CI 

A 
L 

SC 
IE 

N 
CE 

S 

Content analysis of 150 year of British periodical 
Thomas Lansdall-Welfarea, Saatviga Sudhahara, James Thompsonb, Justin Lewisc, FindMyPast Newspaper Teamd,1, 
and Nello Cristianinia,2 

aIntelligent Systems Laboratory, University of Bristol, Bristol BS8 1UB, United Kingdom; bDepartment of History, University of Bristol, Bristol BS8 1TB, United 
Kingdom; cSchool of Journalism, Media and Cultural Studies, University of Cardiff, Cardiff CF10 3NB, United Kingdom; and dFindMyPast Newspaper Archive 
Limited (www.britishnewspaperarchive.co.uk), Dundee DD2 1TP, Scotland 

Edited by Kenneth W. Wachter, University of California, Berkeley, CA, and approve November 30, 2016 (received for review April 21, 2016) 

Previous study have show that it be possible to detect macro- 
scopic pattern of cultural change over period of century by 
analyze large textual time series, specifically digitize books. 
This method promise to empower scholar with a quantitative 
and data-driven tool to study culture and society, but it power 
have be limited by the use of data from book and simple ana- 
lytics base essentially on word counts. This study address 
these problem by assemble a vast corpus of regional newspa- 
pers from the United Kingdom, incorporate very fine-grained 
geographical and temporal information that be not available for 
books. The corpus span 150 year and be form by million of 
articles, represent 14% of all British regional outlet of the 
period. Simple content analysis of this corpus allow u to detect 
specific events, like wars, epidemics, coronations, or conclaves, 
with high accuracy, whereas the use of more refine technique 
from artificial intelligence enable u to move beyond counting 
word by detect reference to name entities. These tech- 
niques allow u to observe both a systematic underrepresenta- 
tion and a steady increase of woman in the news during the 20th 
century and the change of geographic focus for various concepts. 
We also estimate the date when electricity overtook steam and 
train overtook horse a a mean of transportation, both around 
the year 1900, along with observe other cultural transitions. We 
believe that these data-driven approach can complement the 
traditional method of close reading in detect trend of conti- 
nuity and change in historical corpora. 

artificial intelligence | digital humanity | computational history | 
data science | Culturomics 

The idea of exploit large textual corpus to detect macro-scopic and long-term cultural trend have be discuss for 
many year (1, 2), promising to empower historian and other 
humanity scholar with a tool for the study of culture and soci- 
ety. Many study have be publish over the past few year 
(3–6), some go a far a to propose a quantitative and data- 
driven approach to the study of cultural change and continuity, 
owe a much to the method of modern genomics a to those 
of the humanities. 

A seminal study of 5 million English-language book pub- 
lished over the arc of 200 year (1) show the potential of this 
approach, generate a debate about the possible advantage and 
drawback of this new methodology. The study make various 
claim about both the evolution of language and that of culture 
(for example, measure the time require by various technolo- 
gy to become establish or the duration of celebrity for vari- 
ous category of people a well a study change in English 
grammar). However, one of the key criticism be that it be 
base almost entirely on counting words, ignore both seman- 
tic and context (7). Additional criticism be that it do not cover 
periodical (8) and that the data sample might have be biased, 
represent only those book found in the library (9). 

A late study (10) discuss the possible benefit of mining 
corpus of digitize newspaper and propose the use of “dis- 
tant reading” technique (11) in this domain, but it be severely 
constrain by the tool that it used, which only allow for the 

query of individual words. It conclude by advocate for the 
use of big data method for newspaper analysis and propose 
specific criterion for the design of such experiments. 

Although the “Culturomics” study (1) be base on the idea 
of introduce quantitative and measurable aspect to the study 
of cultural change, use high-throughput method for data 
acquisition and analysis, additional development in the field of 
Natural Language Processing (NLP) now allow for more sophis- 
ticated information to be extract from text, allow previous 
criticism to be overcome in many way (12, 13). 

In this study, follow on from a series of article pioneer- 
ing the use of high-throughput data for the study of culture 
(1, 4–6, 14, 15) and draw on the debate that follow their 
publication (7–9), we assemble a massive dataset of newspa- 
pers and periodical aim at verify or contextualizing some 
of the finding of the study on book (1) use unique and 
more refine method and incorporate into the interpretation 
of result various valuable lesson learn from the subsequent 
debate. 

We first present n-gram trend a use in the Culturomics 
paper before move beyond simple word counting method 
to incorporate more semantic information about name enti- 
tie and their properties. The corpus that we assemble be 
form by 28.6 billion word from 120 regional or local news 
outlet contain in 35.9 million article that be publish 
in the United Kingdom between 1800 and 1950. This sam- 
ple represent approximately 14% of all regional newspaper 
publish over that period in the United Kingdom and cover 

Significance 

The use of large datasets have revolutionize the natural sci- 
ences and be widely believe to have the potential to do so 
with the social and human sciences. Many digitization effort 
be underway, but the high-throughput method of data pro- 
duction have not yet lead to a comparable output in analysis. 
A notable exception have be the previous statistical analy- 
si of the content of historical books, which start a debate 
about the limitation of use big data in this context. This 
study move the debate forward use a large corpus of his- 
torical British newspaper and tool from artificial intelligence 
to extract macroscopic trend in history and culture, includ- 
ing gender bias, geographical focus, technology, and politics, 
along with accurate date for specific events. 

Author contributions: T.L.-W., J.T., and N.C. design research; T.L.-W., S.S., and N.C. per- 
form research; T.L.-W., S.S., J.T., J.L., and N.C. analyze data; F.N.T. supply the data 
and the description of it generation; and T.L.-W., J.T., J.L., and N.C. write the paper. 

Conflict of interest statement: F.N.T. be a team within the company FindMyPast. Its main 
role have be to provide part of the data and relative text. 

This article be a PNAS Direct Submission. 

Freely available online through the PNAS open access option. 

Data deposition: The data report in this paper be available online at data.bris.ac.uk/ 
data/dataset/dobuvuu00mh51q773bo8ybkdz. 
1The FindMyPast Newspaper Team: Amy Gregor, Boon Low, Toby Atkin-Wright, Malcolm 

Dobson, and Richard Callison. 
2To whom correspondence should be addressed. Email: nello.cristianini@bristol.ac.uk. 

www.pnas.org/cgi/doi/10.1073/pnas.1606380114 PNAS Early Edition | 1 of 9 

http://www.britishnewspaperarchive.co.uk 
http://data.bris.ac.uk/data/dataset/dobuvuu00mh51q773bo8ybkdz 
http://data.bris.ac.uk/data/dataset/dobuvuu00mh51q773bo8ybkdz 
mailto:nello.cristianini@bristol.ac.uk 
http://www.pnas.org/cgi/doi/10.1073/pnas.1606380114 
http://crossmark.crossref.org/dialog/?doi=10.1073/pnas.1606380114&domain=pdf&date_stamp=2017-01-04 


newspaper obtain from all of the main geographical region 
in the United Kingdom. We make various effort to ensure that 
the data sample be a representative a possible of United King- 
dom local newspapers, cover all main regions, time periods, 
and key outlets. 

To keep this study focus on the trend that we extract and 
not on the engineering technique that be used, we have only 
make use of method that have already be deployed in other 
publish study and can be consider stable. Drawing on the 
subject expertise of the multidisciplinary research team, knowl- 
edge of the historical, media, and sociological context be use 
to inform each stage of the study design: from the careful selec- 
tion of newspaper and the selection of keywords to the interro- 
gation and interpretation of the results. Where appropriate, the 
data query be sample and read closely to address potential 
noise in the optical character recognition (OCR) text or ensure 
that concept be be accurately tracked. 

The study be intentionally wide-ranging, enable a broad 
assessment of the potential of the approach. Given space con- 
straints, the discussion of historical context be render neces- 
sarily concise. Contextual awareness was, however, central to 
make sense of the findings. To give an example, analyze the 
term “suffragette”—a word popularize by a specific segment 
of the medium a a politicize exercise in “catchword” creation— 
can only be understood in relation to both the history of medium 
and the history of the struggle for voting right of woman in 
Britain. 

Our hope be to concentrate the attention of the reader on the 
main important point that we be try to make: it be possible 
today to detect long-term trend and change in history by mean 
of big data analysis of the vast corpus that be become avail- 
able. These finding can include study about politics, technol- 
ogy, the economy, values, gender, and much more. These trend 
and changes, which might otherwise go unnoticed, can be dis- 
cover by machine, enable a complementary approach with 
closer investigation by traditional scholars. 

Results 
Differences Between Books and Newspapers. A start point for 
our study be to compare some result for our corpus with those 
for the Google book corpus (1), show the similarity and dif- 
ferences between use a corpus of book and one of newspaper 
and highlight that we can find the same trend in our corpus 
but also, that an analysis of newspaper may be more sensitive to 
certain cultural shifts—notably because of their closer relation- 
ship to current events—than books. 

Using a similar approach, we compute the use frequency of 
1-grams and n-grams over time, where a 1-gram be a string of 
character uninterrupted by a space that include word (“adven- 
turous” and “puppy”), number (“1.215”), and typographical 
error (“wrods”), whereas an n-gram be an n-length sequence of 
1-grams, such a the phrase “United Kingdom” (a 2-gram) and 
“in the past” (a 3-gram). The use frequency for an n-gram be 
compute by divide the number of instance of the n-gram in a 
give year by the total number of word in the corpus that year. 
We restrict n to three and limited our study to n-grams that 
occur at least 10 time within the corpus. 

We found that the impact of key events, such a coronations, 
conclaves, wars, and epidemics, be much more obvious in our 
corpus, with peak allow u to identify specific year in which 
event occurred. For the book corpus, the impact of key event 
be much less clear (Fig. 1), highlight that regional newspa- 
pers be much closer than book to the event cover in both 
time and space. Fig. 1 help to show the difference between 
the two type of write medium, with newspaper offering a 
closer representation of historical shifts, whereas book be more 
reflective in nature and less time-bound (for example, a book’s 
narrative might be set in the past). 

Fig. 1. Comparison between (A, C, E, and G) our corpus of British period- 
icals and (B, D, F, and H) the Google book corpus (1) use n-gram trend 
identify (A and B) major wars, (C and D) coronations, (E and F) conclaves, 
and (G and H) epidemic between 1800 and 1950 in the United Kingdom. 
Events be clearly identifiable in the periodical corpus, whereas it be more 
difficult to distinguish exact year of event in the book corpus. 

Open-Ended Measurements. We then look at more open-ended 
questions, which include measurement of more general and 
less well-established relations. We divide our analysis into the 
follow spheres: value and beliefs, United Kingdom politics, 
technology, economy, social change, and popular culture. Again, 
we select topic and keywords in a way to avoid ambigui- 
tie and perform close reading of some of the article identi- 
fied by our analysis to ensure that the keywords represent the 
intend topic. 

In value and beliefs, we test the hypothesis put forward by 
Gibbs and Cohen (3) of a decline in so-called “Victorian values” 
during the period under investigation. We find that mention of 
certain key Victorian value (3) be in overall decline, although 
term like “duty,” “courage,” and “endurance” find new impe- 
tus in time of war, whereas other key terms, notably “thrift” and 
“patience,” do not exhibit a downward trend, qualify straight- 
forward account of the suppose demise of Victorian value 
(Fig. 2A and B). 

In United Kingdom politics, Gladstone and Disraeli be often 
see a the key political figure of the 19th century; however, our 
finding suggest that Gladstone be significantly more newswor- 
thy during the 19th century itself than Disraeli (Fig. 2C). This 
find could be partly because of Gladstone’s great political 

2 of 9 | www.pnas.org/cgi/doi/10.1073/pnas.1606380114 Lansdall-Welfare et al. 

http://www.pnas.org/cgi/doi/10.1073/pnas.1606380114 


PN 
A 

S 
PL 

U 
S 

CO 
M 

PU 
TE 

R 
SC 

IE 
N 

CE 
S 

SO 
CI 

A 
L 

SC 
IE 

N 
CE 

S 

Fig. 2. Values, beliefs, and United Kingdom politics. n-Gram trend show- 
ing (A and B) a decline in Victorian value a put forward by Gibbs and 
Cohen (3), (C) that Gladstone be much more newsworthy than Disraeli, 
(D) that liberal be more mention than conservative until the 1930s, 
and (E and F) that reference to British identity take off in the 20th 
century. 

longevity, although it be notable that Gladstone receive more 
coverage even during Disraeli’s year a Prime Minister and be 
a tower figure in press coverage of the period in a way that 
Disraeli be not. 

Overall, the Conservative and Liberal Parties receive broadly 
similar level of coverage during the 19th century, although they 
be both eclipse from the 1920s onward by the Labour Party 
(Fig. 2D). This change cannot, of course, be assume to reflect 
level of political support, but it do suggest that the emergence 
and growth of the Labour Party be set the agenda for the 
regional and local press from 1920 to 1950 (notably after the first 
Labour Party government in 1924). 

Our finding also suggest a very clear timeline in the emer- 
gence of “Britishness” a a popular idea, with the term “British” 
overtake the term “English” at the end of the 19th century 
(Fig. 2 E and F). Thereafter, we see a significant increase in 
the use of the term British in the first half of the 20th cen- 
tury, with dramatic increase during both world wars. The term 
English decline during the same period (and indeed, suffers 
small dip during World War 1 and World War 2)—to such an 
extent that the term “Scottish” overtakes it in the late 1940s, 
suggest that British replace English a a default national 
identifier. Although scholarship suggests that the development 
of Britishness predates this rise (16), these data suggest that the 
dominance of Britishness in the popular imagination be a 20th 
century phenomenon. 

In technology, we track the spread of innovation in energy, 
transportation, and communications. In the first area, we observe 
the steady decline of steam and the constant increase of electric- 
ity, with a cross point in 1898 (Fig. 3A). In the area of trans- 
portation, we observe how train overtook horse in popularity 
in 1902, well after the dawn of the railway age that begin in the 
1840s, show the cultural significance of horsepower through- 
out the 19th century (Fig. 3B). 

In the area of communications, we examine the rate of adop- 
tion of the telegraph, telephone, radio, and television, support 
previous finding (1) that observe an ever-increasing rate of 
uptake of new technology that culminate with the rapid rise 
of television (Fig. 3C). 

In economy, we find that discussion of the economy a a dis- 
tinct concept and field begin in late Victorian times. The decline 
in reference to political economy and the growth of reference to 
the economy manifest the emergence of a sharper idea of the 
economy a a distinct knowable entity with it own feature and 
rhythms, separable from those of politics (Fig. 3D). It be impor- 
tant, however, to note that, on closer reading, reference to the 
economy seem to be about the need for savings, which be appar- 
ent in 1922 and 1932. It be the secular trend evident compare 
the economy with political economy that be suggestive. 

We also find that it be strike that the term panic emerges 
a correspond to volatile downward financial market with- 
out need to involve concern about morality or crime, link- 
ing clearly when inspect under closer reading to banking 
crisis with pronounce peak in 1826, 1847, 1857, and 1866 
(Fig. 3E). This conjecture can be further explain and exam- 
ined collectively with 19th century press and financial history but 
would be difficult to express without this complementary, distant- 
reading approach. More speculatively, it be notable that sample 
regional newspaper and thus, mitigate “London-centric” bias, 
nonetheless, reveal the centrality of financial market in the City 
of London to discussion of panic. 

In social change, we observe sharp temporal boundary in 
phenomena, such a the suffragette movement and the period 
of anarchist activity; we observe the peak of unrest that cor- 
respond with well-known period of strike action in 1912 and 
1919, whereas the expression revolt corresponds with tension in 

Fig. 3. Technology and economy. n-Gram trend show (A) the steady 
decline of steam and the rise of electricity, (B) the wan popularity of 
horse and the increase in trains, (C) the rate of uptake for different com- 
munication technologies, (D) “the economy” a a concept begin in 
late Victorian time after a decline in “political economy,” and (E) that the 
four large peak for “panic” correspond with negative market movement 
link to banking crisis in 1826, 1847, 1857, and 1866. 

Lansdall-Welfare et al. PNAS Early Edition | 3 of 9 



British colonies, notably the Lower Canadian rebellion of 1837– 
1838 and the “Indian mutiny” of 1857 (Fig. 4A). 

The frequency of suffragette have a clearly delimit time inter- 
val (1906–1918) (Fig. 4B), which corresponds with the period from 
the popularize of the term in response to the disruption of pub- 
lic meeting to the achievement of suffrage for many, although not 
all, adult woman in 1918. Despite the many year of political cam- 
paigning that precede it, we see a sharp rise in coverage of the suf- 
fragettes (and suffragists) follow the dramatic death of Emily 
Wilding Davidson, who be trample to death by the King’s horse 
at Ascot. This sharp rise in coverage is, perhaps, an early 20th cen- 
tury example of the importance of a “media event” to a political 
campaign and it ability to capture the journalistic imagination. 

The time interval for anarchism be mostly present in the inter- 
val from 1882 to 1920, correspond to the heyday of concern 
over anarchist direct action before the rise of fascism and bol- 
shevism, whereas slavery include the movement for abolitionism 
and the American Civil War (Fig. 4C). 

Fig. 4. Social change and popular culture. n-Gram trend show that (A) 
“unrest” corresponds with well-known period of social tension, whereas 
“revolt” corresponds with tension in British colonies; (B) the suffragette 
movement fall within a delimit time interval; (C) “slavery” include the 
movement for abolitionism and the American Civil War, whereas “anarchist” 
corresponds to the heyday of concern over anarchist direct action before 
the rise of fascism and bolshevism; (D) the gender gap in mention of men 
and woman be closing, with woman make advance during the two wars; 
(E) the gender gap be also closing when measure use the pronoun he and 
she; (F) actors, singers, and dancer begin to increase in the 1890s, rise sig- 
nificantly thereafter, whereas reference to politicians, by contrast, gradu- 
ally decline from the early 20th century; and (G) football be more prominent 
than cricket from 1909 on. 

As we might expect, the n-gram “men” be mention more 
often than “women,” and the same be true for the n-gram “he” 
compare with “she,” indicate that we be access informa- 
tion about the actual number of men and woman in the news. 
It be interest to note that the relative proportion of men and 
woman be not very different in today’s news (17). Additional anal- 
ysis with more sophisticated method be report below, support- 
ing this conclusion. We can also see a slow increase in the men- 
tions of woman and she over the course of 150 year (Fig. 4 D 
and E), suggest a steady increase in the role of woman in pub- 
lic life over the whole period, with a more dramatic rise in the 
consciousness of woman a a group in the 20th century during 
the two world wars. In both cases, we measure the slope of the 
line of best fit for the time series represent the ratio between 
the relative frequency of the n-grams woman and men a well 
a that for the n-grams she and he, find both to be positive. 

In popular culture, medium scholar have document the 
growth in human interest news (and the proportionate decline 
in public affairs), with these data suggest a clear timeline 
for the increase importance of popular culture in news cov- 
erage. For example, we see reference to “actors,” “singers,” 
and “dancers” begin to increase in the 1890s, rise significantly 
thereafter, whereas reference to “politicians,” by contrast, grad- 
ually decline from the early 20th century (Fig. 4F). We see the 
same pattern in the increase coverage of n-grams “football” 
and “cricket,” with football more prominent than cricket from a 
early a 1909 (Fig. 4G). 

Beyond Counting Words. Techniques from NLP allow u to move 
beyond simply counting word frequency and focus instead on 
the frequency with which give entity be mention in the text. 
Named entity include people, locations, and organizations, and 
reference to them can be form by set of n-grams: generally, 
multiple reference can be use for the same entity. It be possi- 
ble to automatically resolve these coreferences, therefore creat- 
ing an automate way to generate multiple n-grams related to a 
give entity. 

This step move u closer to the level of concept and seman- 
tic and also allows u to bypass many of the risk associate 
with the selection of keywords (Materials and Methods). It be fur- 
ther possible to automatically link name entity with exist 
database of entity that have recently become available that 
offer an authoritative list of people, locations, and organizations. 
These open-source list include Yago (18) and DBpedia (19), 
and they allow u to automate the inclusion of external infor- 
mation about different entity that be not present in the corpus 
itself, such a the gender and occupation of a person or the coor- 
dinates of a location. Parsing the text in this way result in the 
extraction of 263,813,326 mention to 1,009,848 different entity 
in the corpus. 

Discovering every time that a person mention within the 
corpus be also present in DBpedia (19) or another knowledge 
base often enables u to map them to an occupation type. This 
procedure allows u to automate the study (1) of fame for peo- 
ple in different career over their lifetime (Fig. 5A). 

Among other things, we confirm their find that politician 
and writer be most likely to achieve notoriety within their life- 
times, whereas scientist and mathematician be less likely to 
achieve fame; however, we also observe a decline for politician 
and writer in news that be not observe in books, whereas 
time seem to be kinder to scientist and mathematicians. This 
method have enormous potential for medium content analysis, 
allow researcher to do widespread and detailed analysis of 
the source use in news and explore, for example, the predom- 
inant political and ideological affiliation of the source use in 
news reporting. 

We also extract every single mention of a person in the cor- 
pu (regardless of whether they be present in external resources) 

4 of 9 | www.pnas.org/cgi/doi/10.1073/pnas.1606380114 Lansdall-Welfare et al. 

http://www.pnas.org/cgi/doi/10.1073/pnas.1606380114 


PN 
A 

S 
PL 

U 
S 

CO 
M 

PU 
TE 

R 
SC 

IE 
N 

CE 
S 

SO 
CI 

A 
L 

SC 
IE 

N 
CE 

S 

Fig. 5. People in history. (A) Replicating the study (1) on famous personality by occupation use all extract entity associate with a Wikipedia entry, 
we found that politician and writer be most likely to achieve notoriety within their lifetimes, whereas scientist and mathematician be less likely to 
achieve fame but decline less sharply. (B) We compute the probability that a give reference to a person be to a male or a female person. We find that, 
although male be more present than female during the entire period under investigation, there be a slow but steady increase of the presence of woman 
after 1900, although it be difficult to attribute this to a single factor at the time. 

and infer gender use the ANNIE plugin of GATE, a standard 
tool for NLP (20). This process give u over 35 million refer- 
ences to people with a resolve gender, allow u to calculate 
the overall probability that a person mention in the news be 
male (or female) and finally, study how this probability change 
over time (Fig. 5B). 

This result confirms—with high sophistication—the result 
obtain use the n-grams trend (Fig. 4 E and F), show 
that woman be consistently represent less than men during 
the entire period under investigation, and it allows u to explore 
the nuance and character of various assumption make about 
gender. This more refine approach also show a slow but steady 
increase of the presence of woman after 1900. These result can 
be read in combination with analogous one for modern news 
(17), show that gender bias within the medium do not seem 
to have change very much, with approximately three time a 
many male a female in modern newspapers. 

Furthermore, revisit the concept that we explore with 
n-gram trends, we compile geographical map for the United 
Kingdom for each of the term display a gradual increase or 
decline (rather than a spike of activity) (Fig. 6). We extract all 
location found in the article that mention one of the concepts, 
disambiguate them again use DBpedia (19), and retrieve 
their geographical coordinates. 

We observe that the term British and English be reasonably 
widespread in use across most of the United Kingdom in 1854. By 
1940, the use of English have dwindled, with British become the 
default national identifier (Fig. 6A). 

During 1885, we can see scatter mention of the Liberal 
Party around the United Kingdom, with a focus on London, 
whereas there be very little mention of the yet to be form Labour 
Party. However, by 1924, this situation have changed, when the 
Labour Party achieve it first minority government and replace 
the Liberal Party a the party mention across the country, 
again with a geographical focus around London (Fig. 6B). 

The geographical focus of technological advance over time 
be also observed, which we show for the transition from steam 
to electricity (Fig. 6C) and from horse to train (Fig. 6D). For 
steam, we can see that mention during it high use year 
in 1854 be widespread, with concentration focus around 
major ports. However, the adoption of electricity replaces steam 
by 1947, with electricity be mention particularly in refer- 
ence to London, Leeds, and area of the Southwest (Fig. 6C). 
During the early peak of attention to horse in 1823, we see 
that mention be mainly diffuse across the country without 

a distinctive pattern, indicative of their use in rural commu- 
nities, and there be only the odd mention of train, which on 
closer reading, be reveal to be generally in a different con- 
text (referring to animal training or processions). By 1948, the 
decline of horse have clearly take effect, all but disappear 
from that map, whereas train be heavily mentioned, particu- 
larly around major cities, display a similar pattern to that of 
electricity. 

Discussion 
The key aim of this study be to show an approach to under- 
stand continuity and change in history base on the distant 
reading of vast news corpora, which be complementary to the tra- 
ditional close reading by historians. We show that change and 
continuity detect in newspaper content can reflect property 
of culture, bias in representation, or actual real-world events. 

With this approach, historian can explore the complex rela- 
tionship between public discourse and live experience by detect- 
ing trend in statistical signal extract from large-scale textual 
corpora. The method be intend to be use in combination with 
traditional approaches, which be need for both the design of 
the study and the interpretation of the findings. Nevertheless, it 
provide conjecture and answer that would be very difficult to 
formulate by use close reading alone. 

In particular, we show that computational approach can 
establish a meaningful relationship between a give signal in 
large-scale textual corpus and verifiable historical moments, 
which be show in the trend for coronation and epidemic dis- 
played in Fig. 1, and that newspaper provide increase clarity to 
the analysis of these event that may not be possible in other cul- 
tural forms, such a books. We further show that the approach 
can reveal or confirm way in which news medium represent par- 
ticular people or issue over time, a evidence by the existence 
of a gender bias that be still present in the medium today (17), and 
that historical trend in public discourse can be make accessible 
through the same means. 

Importantly, this complementary approach provide a layer of 
cultural understand that can be use to augment establish 
historical perspectives, evidence in this study by the temporal 
and geographical pattern in the uptake of various technology 
and concept show in Fig 6, which can provide benefit to tradi- 
tional economic and technical history of the period. 

In this study, special care be devote to the choice of event 
that be use for analysis and the keywords chosen to repre- 
sent them, because we should all be aware of the risk of detect 

Lansdall-Welfare et al. PNAS Early Edition | 5 of 9 



Fig. 6. Changes in geography over time. Maps of the United Kingdom 
show the change in geographical focus of location extract from arti- 
cles contain the term (A) British and English, (B) Liberal Party and Labour 
Party, (C) steam and electricity, and (D) horse and train for the year in which 
each concept receive it peak attention. 

spurious signal in large datasets. As recommend by Nicholson 
(10), one should try to choose word that have high sensitiv- 
ity and specificity for the concept be investigate and at the 
same time, be not too susceptible to semantic shift and error 
in the OCR process. Each of these step could, in principle, 
be formalize and automate to some extent: for example, the 
use of Automatic Query Expansion (21) could likely return a 
viable set of word that pertains to a specific concept [this set 
approximate the lexical field (22) of that concept]. However, 
we feel that it be ultimately the role of the historian to use her 
judgment and cultural knowledge in the choice of these key- 
word and the subsequent interpretation of the results. In this 
way, we focus our analysis on event and keywords that be 
not ambiguous: coronations, wars, technological artifacts, etc., 

all of which be event and trend that can be represent by 
a small set of specific word and often have clear date attach 
to them. 

Various author have voiced concern that digital humanity 
might be just a colonization of the humanity by the sciences, 
but do so be not the purpose of this study. On the contrary, we 
feel that the practice of close reading cannot be replace by algo- 
rithmic means. Indeed, our method can only detect increase 
or decrease attention toward a give topic or idea over the 
decades, offering a complementary approach to close reading, 
but they cannot explain the reason behind those changes, which 
be best understood by other means. We believe, however, that 
other criticism be less warranted: the inability of computational 
method to introduce contextual knowledge, access semantic 
information, or work in the presence of OCR noise or the issue 
related to bias in the original corpus selection be probably all 
issue that can be solve or account for over time. 

Future work will indeed include denoising of the data, link 
of the data with other corpus or data sources, good disam- 
biguation of entities, and more refine information extraction 
within a context. Additionally, the evaluation of suitable key- 
word could be partially automate by include information 
about OCR noise to help guide the analyst, with recent develop- 
ments also offering the promise of capture the extent to which 
the meaning of a specific word have change over time (23). These 
direction be part of engineering work already underway. 

What cannot be automate be the understand of the impli- 
cation of these finding for people, which will always be the 
realm of the humanity and social science and will never be that 
of machines. 

Materials and Methods 
Data Source Background. The British Library’s newspaper collection be 
among the fine in the world, contain most of the run of newspaper 
publish in the United Kingdom since 1800. The scale of the newspaper 
publishing industry from the early 19th century onward be enormous, with 
many city and town publishing several newspaper simultaneously and 
other newspaper that aim for a wider county circulation provide an 
unrivalled picture of provincial life span the whole of the 19th century 
and half of the 20th century (24). 

In May of 2010, FindMyPast begin a partnership with the British Library 
to digitize million of page of these historic newspaper and make them 
available for the public to search online at www.britishnewspaperarchive. 
co.uk. 

New page be be scan all of the time a part of the 10-year 
project, which once finished, will contain over 40 million newspaper page 
from the British Library’s newspaper collection. To date, FindMyPast have 
make available over 12 million page from 535 different newspaper title 
publish between 1710 and 1959, add over 8,000 new page each day. 

The newspaper collection be further supplement with digitize news- 
paper record provide by the Joint Information Systems Committee (JISC) 
that cover the same geographic region and time period. The digitization of 
these newspaper be fund by the JISC to provide a representative sample 
of United Kingdom newspaper span all geographical regions, make 
them suitable for a large-scale automate content analysis of Britain dur- 
ing the 19th and 20th centuries. The data from the JISC form approximately 
20% of the result corpus. 

In this study, we select a subset of the entire corpus that have be 
scan at the time, aim to assemble a corpus for the study of Britain 
between 1800 and 1950. To do so, we undertook several significant step 
relate to the selection of news outlet to provide a balance represen- 
tation in term of geographic region, time period and quality of the texts, 
the digitization process and extraction of the associate metadata, and the 
extraction of information from the raw text of the corpus. 

The corpus be accessible under a subscription model at www.britishnews- 
papersarchive.co.uk, whereas enquires about bulk access to raw data 
should be direct to FindMyPast. The exact list of article and newspa- 
per outlet from FindMyPast along with secondary data produce dur- 
ing this study be openly available, include time series of the million 
most frequent n-grams and the 100,000 most frequent name entity 
extract by AIDA (25), which be available at data.bris.ac.uk/data/dataset/ 
dobuvuu00mh51q773bo8ybkdz (26). 

6 of 9 | www.pnas.org/cgi/doi/10.1073/pnas.1606380114 Lansdall-Welfare et al. 

http://www.britishnewspaperarchive.co.uk 
http://www.britishnewspaperarchive.co.uk 
http://www.britishnewspapersarchive.co.uk 
http://www.britishnewspapersarchive.co.uk 
http://data.bris.ac.uk/data/dataset/dobuvuu00mh51q773bo8ybkdz 
http://data.bris.ac.uk/data/dataset/dobuvuu00mh51q773bo8ybkdz 
http://www.pnas.org/cgi/doi/10.1073/pnas.1606380114 


PN 
A 

S 
PL 

U 
S 

CO 
M 

PU 
TE 

R 
SC 

IE 
N 

CE 
S 

SO 
CI 

A 
L 

SC 
IE 

N 
CE 

S 

Data Selection. Newspaper issue be select from those that have be 
digitize by FindMyPast from the British Library archive with an eye to form 
a representative sample of newspapers. The selection be perform by 
committee, and the criterion for inclusion be (i) the completeness of the 
run of issues, (ii) the number of year that an issue covers, (iii) the geo- 
graphical region that the issue be from, (iv) the quality of the OCR output 
for the issue, and (v) the political bias of issues. 

Our principal aim be to cover all geographical region and time interval 
a fairly a allow by the available data. Issues be first separate into the 
register geographical region for the publisher, and within each region, 
newspaper issue be ranked by a combination of the number of year cov- 
ered (favoring issue with continuous coverage of many years), their aver- 
age OCR quality, and the total size of data available for the issue. Issues 
be then select from the rank until the region have good coverage. 
Using domain knowledge, consideration be also take to ensure that the 
selection of newspaper issue represent the balance of political opinion in 
the regional press at the time. 

In total, the corpus include 120 title select to best cover the follow 
12 geographical region of the United Kingdom use the above criterion for 
inclusion: East, East Midlands, Northern Ireland, London, Northwest, North- 
east, Scotland, Southeast, Southwest, Wales, West Midlands, and Yorkshire. 

We estimate the number of regional newspaper within the United 
Kingdom during the period from 1800 to 1950 use statistic on the num- 
ber of newspaper in circulation (27). Specifically, we take the average num- 
ber of paper in circulation from newspaper directory for 1847, 1877, and 
1907. This calculation give u an estimate of 835 newspaper title in exis- 
tence through the period. Our corpus contains 120 newspaper titles, give 
u an estimate that the corpus cover approximately 14% of the regional 
paper for the United Kingdom during that time. 

Data and Associated Metadata. 
Digitization process. The original newspaper be provide by the British 
Library to the FindMyPast Newspaper Team a either microfilm or the orig- 
inal bound newspapers. Original bound newspaper be scan use 
Zeutschel A0 Sheet-Bed Scanners, create high-quality digital copy of the 
newspaper a TIFF image at 400 dot per inch (dpi) in 24-bit color before 
be convert to JPEG2000 format image for archiving. Images create 
from digitize the microfilm result in grayscale image at 300 dpi. 

The raw image create during digitization be digitally cropped, 
cleaned, and contrast enhance before be segment into classify 
area correspond to the type of content, such a news articles, advertise- 
ments, and obituaries; structural information, such a page numbers, head- 
ers, and footers; and title information, such a issue title, date of publica- 
tion, and price. 

The image be then pass through an OCR process to identify the text 
use in each section of the page, whereas the associate metadata for each 
issue be pass through a quality assurance check to correct any mistake 
in the structural extraction step. 

Data provide via the JISC collection be digitize use a similar work- 
flow through an external supplier (28). 
Structure extraction. The raw image from the scan of the original bound 
newspaper or the microfilm after cropping, cleaning, and contrast correc- 
tion be next process in a step to segment each page into classify 
areas. This process be perform by FindMyPast in two different ways. 

The majority of the corpus (78%) be manually segment into differ- 
ent classify area relate to the content of the page, structural informa- 
tion, or title information. It be found that this manual process be pro- 
hibitively expensive after a certain point within the project, and therefore, 
the remain corpus be process use an automate method use the 
CCS docWorks software (29). 
OCR. OCR be perform on the digital image within the CCS docWorks 
software (29) by the FindMyPast team. This process output the recognize 
text in the image along with associate information (such a the location 
and layout on the page) and percentage word accuracy for each word 
in the standard Metadata Object Description Schema (MODS), Metadata 
Encoding and Transmission Standard (METS), and Analyzed Layout and Text 
Object (ALTO) format (30). 

The percentage word accuracy for OCR be calculate automatically by the 
OCR software and use a a measure of how confident the software be that 
the character make up the word be interpret correctly. Each individ- 
ual character in each word be assign a character score between zero and 
nine (with nine be 100% confidence) for how confidence the software 
be that the character have be read correctly. The overall score for the word 
be then calculate by take the average confidence score for the letter 
that compose it. More widely, the word accuracy score be average over a 

set of word to assign a quality score to how well the text in the image have 
be recognize at the article level. 

Because error in OCR can be affected by a number of systematic fac- 
tors, include but not limited to font, size, and physical condition of the 
original paper copy, the error rate varies across titles; therefore, we aim 
to select those title that have the best possible OCR quality for inclusion 
in our corpus so a not to detrimentally affect the analysis by introduce 
low-quality texts. Of course, certain typographical consideration must also 
be consider when analyze the data, such a the common use of the 
long s, which can often be mistaken for an f. Additional work can cer- 
tainly be do to account for these type of mistake and will improve in 
the future. 

Overall, in the corpus select for analysis, the average percentage word 
accuracy be estimate to be 77.0%, with an SD of 5.78, take the average 
score assign to each newspaper outlet per year by CCS docWorks (29) 
weight by volume across all article in the corpus. 
Metadata. The process that be undertaken by the British Library and the 
FindMyPast Newspaper Team to annotate the data be again manage 
through the process management pipeline base on CCS docWorks (29). 
Metadata relative to each outlet be manually enter at the time of the 
digitization base on the British Library newspaper catalog. The location 
assign to each outlet be identify base on the location of original pub- 
lication. The date be extract from each newspaper issue by human oper- 
ators and then, validate during quality control checks. The page segmen- 
tation and headline OCR for material process early in the project be 
manually correct by operators; late in the project, these step be per- 
form without human intervention. A human editor be use to run qual- 
ity control check on the structural data extract by the software, and the 
workflow software identify systematic issue that be then manually cor- 
rected by operators. This process include verify that the outlet name be 
correct, the date of the issue be correct, and the page have be segment 
into correctly identify type along with any other quality assurance 
step taken. 

Automated Content Analysis. After the digitization process have be com- 
pleted, the FindMyPast team provide the Bristol team with a collection 
of document contain the textual content from the newspaper article 
along with associate metadata relate to the title of the article, the 
date of publication, the title that publish the article, the location for 
the publisher, and so forth. Documents be convert from the METS, 
MODS, and ALTO format into JavaScript Object Notation (31) document 
and store with their associate metadata in a MongoDB NoSQL collection 
(https://www.mongodb.com/). 

Each document in the database be then subject to an information 
extraction procedure (described below), which aim to allow u to gener- 
ate time series of any n-gram, extract reference to entity within the text 
and resolve the entities, and link the entity to external database where 
possible to enrich the information contain within each document. 

n-Grams. n-Grams be extract from the main textual content of each 
document, begin with tokenizing the text, counting the frequency of 
each n-gram across the entire corpus, and then, filter the n-grams so a 
to only keep those that occur a minimum number of time (in this case, at 
least 10 times). 
Generation. Raw text data be store a a string of characters, with no 
explicit word information. Tokenization split the string of character up 
into a string of words, also refer to a tokens, terms, or 1-grams, for 
which we can then compute the frequency. Tokenization be perform 
use the assumption that contiguous sequence of alphabetic character 
form a single 1-gram in the vocabulary, which be separate by whitespace 
characters. Numeric character that form contiguous sequence be also 
consider a 1-gram, whereas special characters, such a punctuation, be 
treat in different way depend on the specific character. For our pur- 
poses, the alphabet use be the Unicode Transformation Format 8 (UTF-8) 
character set. 

The tokenization be implement use the Word Break rule from 
the Unicode Text Segmentation algorithm follow the specification 
in the Unicode Standard Annex 29 (unicode.org/reports/tr29/). n-Grams 
be further process to remove possessive (trailing “’s” at the end 
of words), lowercased, and stem use the Porter stemmer algorithm 
(32). Tokenization be perform use the Lucene analyzer library avail- 
able at https://lucene.apache.org/core/4 0 0/analyzers-common/overview- 
summary.html. 
Frequency of n-grams. We calculate the frequency of each n-gram (up to 
a length of three) in the corpus by first counting how often each n-gram 

Lansdall-Welfare et al. PNAS Early Edition | 7 of 9 

https://www.mongodb.com/ 
http://unicode.org/reports/tr29/ 
https://lucene.apache.org/core/4_0_0/analyzers-common/overview-summary.html 
https://lucene.apache.org/core/4_0_0/analyzers-common/overview-summary.html 


occurs within each document with a publication date of the same year in 
the corpus and then, divide this number by the total volume of term 
(1-grams) occur within the document publish in the same year. This 
calculation give a relative importance to each n-gram at the time of it use. 
This calculation be compute within the Hadoop map-reduce framework 
available at hadoop.apache.org/, allow u to distribute the computation 
and work on the large corpus use in this study. 

When estimate the relative frequency of an n-gram for each year, 
we also calculate a confidence interval for that estimate use the Yate’s 
score interval (33). The result confidence bar be not discernible when 
plotted, a they be very small because of the very large size of the data 
use to calculate the time series. As an example, compare steam with elec- 
tricity, the size of the change between 1800 and 1950 be at least two order 
of magnitude large than the mean confidence interval relative to those 
time points. 

Entities. We use standard text engineering tool to extract name enti- 
tie from the text, link them with external source of information where 
possible. Entities be extract use AIDA (https://github.com/yago-naga/ 
aida), a framework for entity detection and disambiguation (25) include 
both person and location types. Additionally, we extract reference to 
people, include those not necessarily present in any external databases, 
use the ANNIE plugin of the General Architecture for Text Engineering 
(https://sourceforge.net/projects/gate/) (20). 

Although we note that both of these tool do not have 100% accuracy, 
in detect the entity from the text or link it with the correct infor- 
mation from external sources, it be important to also note that we miti- 
gate the risk by remove those entity for which high confidence could 
not be achieve a explain below. Although performance of the tool 
cannot be assess on the historical corpus (for lack of a “ground truth”), 
each tool does, however, achieve high performance on benchmarking tasks, 
with AIDA reporting a mean average precision of 89.05% on the Association 
for Computational Linguistics’ Special Interest Group on Natural Language 
Learning Conference on Computational Natural Language Learning (CoNLL) 
2003 dataset (25), and our entity extraction tool base on the ANNIE plugin 
for GATE achieve an accuracy of 97.1% on news medium from the web (17). 
Furthermore, for each of the tools, we developed quality control check and 
filter to ensure that we only keep the prediction for which we have a 
high level of confidence, because it should be note that these tool be not 
specifically train on digitize historical newspapers. 

For the entity extract use AIDA (25), although only more prominent 
people or location be identify (because they must first appear in an exter- 
nal database, such a Wikipedia), it be sufficient for our purpose of iden- 
tifying different personality by their specific occupation (e.g., scientists, 
writers, politicians, etc.). In do so, we be able to replicate the Google 
book study (1) use all personality that we extract from the corpus 
rather than limit ourselves to the top 25. Overall, there be 263,813,326 
mention of 1,009,848 different entity mention in Wikipedia. 

This study be perform by group all personality by their occu- 
pation type in DBpedia (available for download from wiki.dbpedia.org) 
(19) a extract by AIDA before resolve hyponym to their hyper- 
nym occupation type use the WordNet ontology (34) (available at 
https://wordnet.princeton.edu/wordnet/download/). Personalities be fil- 
tered to remove spuriously extract entities, where entity be identify 
a spurious by first retrieve the birth date for each entity from DBpedia 
and then, remove those for which the majority of their mention occur 
before the entity be born. In this way, we reduce the number of personality 
that have be erroneously link to a specific Wikipedia entry. 

For study gender balance over the course of history within the cor- 
pus, we want to avoid any systematic effect cause by our gender detec- 
tion procedure. A method base on link entity to DBpedia (which be 
base on Wikipedia) would likely suffer from the same gender imbalance 
discover in Wikipedia (35). Therefore, we use instead the ANNIE plugin 
of GATE to extract every reference to a person within the corpus and classify 
their gender into male, female, or unknown use contextual information 
(such a titles, first names, pronouns, etc.). We consider only those refer- 
ences for which we could obtain an unambiguous gender, discard more 
ambiguous entity where we receive more than one distinct gender label 
for the entity. In do so, we be able to show the number of reference 
to male and female over the course of 150 year in United Kingdom news- 
papers. In total, there be 25,896,979 unambiguous reference to males, 
10,198,490 unambiguous reference to females, and only 309,098 ambigu- 
ous reference (assigned to both gender by the tool) found within the cor- 
pus, show that we can unambiguously find the gender of an entity for 
99.15% of the entity in the historical newspaper corpus. 

We additionally compare our finding with those come from the 
Google Books n-gram corpus (1) along with our own result use the inde- 
pendent n-gram method. This combine use of large number of reference 
and the comparison with independent source of information give u con- 
fidence that we can separate the signal from the noise. 

Locations be also extract use AIDA (25), disambiguate each men- 
tion of a location with it Wikipedia page. Geographical coordinate be 
retrieve from DBpedia for each location or parse from the live Wikipedia 
page when no coordinate be resolve from DBpedia. 

Geographical focus map of different concepts, such a British or train 
a displayed in Fig. 6, be generate by visualize all location that be 
present in any article contain the concept n-gram and that occur a min- 
imum of three time in any article contain the concept n-gram in the 
same year. This threshold be use to both filter very low-frequency loca- 
tions and obtain a more readable map. Location marker be size accord- 
ing to a combination of the natural log of their total mention in the corpus 
(more mention location be give great weight) and the probability 
that, within a give year, a location be mention in the same article a the 
concept n-gram (the size of the intersection between a concept and a loca- 
tion with a year be a measure of how related they be at that time). 

Statistical Robustness of Methods. When work in a data-driven high- 
throughput way, which be the case in this distant reading project, it be neces- 
sary to automate most steps, and this automation do create the prob- 
lem that each step might introduce errors: OCR will corrupt some char- 
acters, name entity recognition might fail to recognize a location, and 
disambiguation step might link an entity to the wrong entry in external 
resources. However, the size of the dataset and careful design can be use 
to mitigate this risk. 

Our focus be on detect large statistical patterns, and therefore, we can 
tolerate less than perfect performance at each stage of the analysis and still 
extract a reliable signal—if we carefully design the analysis. In this way, we 
be not different from previous Culturomics study (1), and a Nicholson (10) 
observed, “this be the price one must pay to explore the ‘vast terra incognita’ 
of 19th century print culture.” 

Sanity check perform at the end of the pipeline show that indeed— 
for all of the error that may be introduced—we can still reliably detect 
historical events, such a coronations, war and epidemics. Among the many 
design choice involve in the study, we compare relative frequency of a 
give word (e.g., train vs. horse) or relative change in the ratio between 
male and female entities, ensure that we be compare signal that be 
affected by the same type of noise. Additional sanity checks, by compare 
time series generate by words, such a he and she or men and women, with 
those generate by the overall mention of male and female entities, show 
that any noise found in the processing pipeline do not cancel the signal. 

On the Selection of Keywords and Other Signals. As point out by Nicholson 
(10), one of the key design choice in these study be the selection of key- 
words. There be various risk involve in this step: a word might not repre- 
sent well the concept under investigation, perhaps because it be ambiguous, 
or it might not be semantically stable during the period under study; per- 
hap that word might not be robust under OCR noise. Indeed, we might 
want to look at several word to represent a concept (as we do for Victo- 
rian values) or sometimes, entire lexical field (22). Our approach have be 
to use judgment base on historical knowledge for the assessment of the 
relevance and stability of each word, make use of carefully select list of 
word already use in previous relevant studies, ass keywords by close 
reading some of the article match them, and use automate mean to 
go beyond counting word and therefore, bypass the risk associate with 
select keywords entirely. 

There be a second risk involve in the selection of keywords: when min- 
ing vast corpora, there be always the risk of find a spurious signal (for 
example, a time series that have accidental resemblance with some historical 
trend). The risk be high when use high-throughput method because of 
the statistical phenomenon of “multiple testing”: even if each keyword have 
a very low chance of show accidental correlations, when we can analyze 
ten of thousand of keywords, this risk be multiply accordingly. The prob- 
lem be further increase by the inherent ambiguity of the task described in 
this study: the lexical field (22) relative to an event or cultural phenomenon 
be not well-defined a priori, and therefore, there be significant freedom for 
the analyst to—involuntarily—select word that confirm a hypothesis. 

These risk can be mitigate by various technical and statistical 
approaches. For example, make use of precompiled list of keywords from 
previous studies, such a the Victorian values, be a standard statistical method 
to account for multiple test by reduce the space of possible test 

8 of 9 | www.pnas.org/cgi/doi/10.1073/pnas.1606380114 Lansdall-Welfare et al. 

http://hadoop.apache.org/ 
https://github.com/yago-naga/aida 
https://github.com/yago-naga/aida 
https://sourceforge.net/projects/gate/ 
http://wiki.dbpedia.org 
https://wordnet.princeton.edu/wordnet/download/ 
http://www.pnas.org/cgi/doi/10.1073/pnas.1606380114 


PN 
A 

S 
PL 

U 
S 

CO 
M 

PU 
TE 

R 
SC 

IE 
N 

CE 
S 

SO 
CI 

A 
L 

SC 
IE 

N 
CE 

S 

be conducted, whereas it should also be possible to generate a list of key- 
word that relate to a specific concept by use technique from the field of 
Automatic Query Expansion (21), therefore approximate it lexical field. 
However, ultimately, it will be the job of the analyst to make careful judg- 
ments and use the finding with the necessary care. We have make every 
effort to select nonambiguous term and event to avoid the risk of gener- 
ating a spurious signal, ensure that we generate the keywords for analysis 
in a way that be independent of their temporal behavior in the corpus. 

ACKNOWLEDGMENTS. We thank Ilias Flaounas for his help in the initial 
stage of the study and Patrick Fleming and the JISC for make their cor- 
pu available. Part of the work be carry out use the computational 
facility of the Advanced Computing Research Center, University of Bris- 
tol. This study be make possible by FindMyPast Newspapers Archive Ltd 
(www.britishnewspaperarchive.co.uk), which share the data. Most Uni- 
versity of Bristol member of the team (T.L.-W., S.S., and N.C.) be sup- 
port by European Research Council Advanced Grant ThinkBIG award 
to N.C. 

1. Michel JB, et al. (2011) Quantitative analysis of culture use million of digitize 
books. Science 331(6014):176–182. 

2. Reddy R, StClair G (2001) The Million Book Digital Library Project. (Carnegie Mellon 
University, Piittsburgh). Available at www.rr.cs.cmu.edu/mbdl.htm. Accessed Decem- 
ber 19, 2016. 

3. Gibbs FW, Cohen DJ (2011) A conversation with data: Prospecting Victorian word 
and ideas. Vic Stud 54(1):69–77. 

4. Mauch M, MacCallum RM, Levy M, Leroi AM (2015) The evolution of popular music: 
USA 1960–2010. R Soc Open Sci 2 (5):150081. 

5. Leetaru K (2011) Culturomics 2.0: Forecasting large-scale human behavior use 
global news medium tone in time and space. First Monday 16(9). 

6. Flaounas I, et al. (2013) Research method in the age of digital journalism: Massive- 
scale automate analysis of news-content—topics, style and gender. Digital Journal- 
ism 1(1):102–116. 

7. Gooding P (2013) Mass digitization and the garbage dump: The conflict need of 
quantitative and qualitative methods. Lit Ling Comput 28(3):425–431. 

8. Morse-Gagné EE (2011) Culturomics: Statistical trap muddy the data. Science 
332(6025):35. 

9. Schwartz T (2011) Culturomics: Periodicals gauge culture’s pulse. Science 
332(6025):35–36. 

10. Nicholson B (2012) Counting culture; or, how to read Victorian newspaper from a 
distance. J Vic Cult 17(2):238–246. 

11. Moretti F (2013) Distant Reading (Verso Books, London). 
12. Borin L, et al. (2013) Mining semantics for culturomics: Towards a knowledge-based 

approach. Proceedings of the 2013 International Workshop on Mining Unstructured 
Big Data Using Natural Language Processing, ed Liu X, Chen M, Ding Y, Song M 
(ACM, New York), pp 3–10. 

13. Suchanek FM, Preda N (2014) Semantic culturomics. Proc VLDB Endowment 
7(12):1215–1218. 

14. Lansdall-Welfare T, Sudhahar S, Veltri GA, Cristianini N (2014) On the coverage of 
science in the media: A big data study on the impact of the Fukushima disaster. Pro- 
ceedings of the 2014 IEEE International Conference on Big Data (Big Data), ed Lin J, 
Pei J, Lin TY (IEEE, New York), pp 60–66. 

15. Flaounas I, et al. (2010) The structure of the the EU mediasphere. PLoS One 
5(12):e14243. 

16. ColleyL(2005)Britons: Forging the Nation, 1707–1837 (YaleUnivPress,NewHaven,CT). 
17. Jia S, Lansdall-Welfare T, Sudhahar S, Carter C, Cristianini N (2016) Women be see 

more than heard in online newspapers. PLoS One 11(2):e0148434. 
18. Suchanek FM, Kasneci G, Weikum G (2007) Yago: A core of semantic knowledge. Pro- 

ceedings of the 16th International Conference on World Wide Web, ed Williamson C, 
Zurko ME, Patel-Schneider P, Shenoy P (ACM, New York), pp 697–706. 

19. Lehmann J, et al. (2015) DBpedia–a large-scale, multilingual knowledge base ex- 
tracted from wikipedia. Semant Web, ed Williamson C, Zurko ME, Patel-Schneider P, 
Shenoy P 6(2):167–195. 

20. Cunningham H, Maynard D, Bontcheva K, Tablan V (2002) GATE: A framework and 
graphical development environment for robust NLP tool and applications. Proceed- 
ings of the 40th Annual Meeting of the Association for Computational Linguistics, ed 
Isabelle P (ACL, Stroudsburg, PA), pp 168–175. 

21. Carpineto C, Romano G (2012) A survey of automatic query expansion in information 
retrieval. ACM Comput Surv 44(1):1–50. 

22. Öhmann E, Trier J (1931) Der Deutsche Wortschatz im Sinnbezirk de Verstandes 
(C. Winter, Heidelberg). 

23. Weston J, Ratle F, Mobahi H, Collobert R (2012) Deep learn via semi-supervised 
embedding. Neural Networks: Tricks of the Trade, ed Montavon G, Orr GB, Mller K-R 
(Springer, Berlin), pp 639–655. 

24. Findmypast Newspaper Archive Limited (2016) About the British Newspaper Archive. 
Available at www.britishnewspaperarchive.co.uk/help/about. Accessed September 
26, 2016. 

25. Hoffart J, et al. (2011) Robust disambiguation of name entity in text. Proceedings 
of the Conference on Empirical Methods in Natural Language Processing, ed Merlo P, 
Barzilay R, Johnson M (Association for Computational Linguistics Stroudsburg, PA), 
pp 782–792. 

26. Lansdall-Welfare T, et al. (2016) FindMyPast Yearly n-Grams and Entities Dataset. 
Available at data.bris.ac.uk/data/dataset/dobuvuu00mh51q773bo8ybkdz. Accessed 
December 19, 2016. 

27. Walker A (2006) The development of the provincial press in England c. 1780–1914: An 
overview. Journal Stud 7(3):373–386. 

28. Shaw J (2009) British Library Newspapers Digitisation Report. Available 
at www.webarchive.org.uk/wayback/archive/20140614080134/www.jisc.ac.uk/media/ 
documents/programmes/digitisation/blfinal.pdf. Accessed September 26, 2016. 

29. CCS (2016) Content Conversion Specialists - Digitization Services. Available at https:// 
content-conversion.com/#digitization-services. Accessed September 26, 2016. 

30. Impact Centre of Competence in Digitisation (2016) Recommendations on Formats 
and Standards Useful in Digitisation. Available at www.digitisation.eu/training/ 
recommendations-for-digitisation-projects/recommendations-formats-standards-rec- 
ommendations/. Accessed September 26, 2016. 

31. Bray T (2014) The JavaScript Object Notation (JSON) Data Interchange Format (IETF, 
Fremont, CA). 

32. Porter MF (1980) An algorithm for suffix stripping. Program 14(3):130–137. 
33. Wallis S (2013) Binomial confidence interval and contingency tests: Mathematical 

fundamental and the evaluation of alternative methods. J Quant Linguist 20(3): 
178–208. 

34. Miller GA, Beckwith R, Fellbaum C, Gross D, Miller KJ (1990) Introduction to wordnet: 
An on-line lexical database. Int J Lexicogr 3(4):235–244. 

35. Wagner C, Garcia D, Jadidi M, Strohmaier M (2015) It’s a man’s wikipedia? Assessing 
gender inequality in an online encyclopedia. Proceedings of the Ninth International 
Conference on Web and Social Media, ICWSM 2015, ed Quercia D, Cha M, Mascolo 
C, Sandvig C (AAAI Press, Palo Alto, CA), pp 454–463. 

Lansdall-Welfare et al. PNAS Early Edition | 9 of 9 

http://www.britishnewspaperarchive.co.uk 
https://www.rr.cs.cmu.edu/mbdl.htm 
http://www.britishnewspaperarchive.co.uk/help/about 
http://data.bris.ac.uk/data/dataset/dobuvuu00mh51q773bo8ybkdz 
http://www.webarchive.org.uk/wayback/archive/20140614080134/www.jisc.ac.uk/media/documents/programmes/digitisation/blfinal.pdf 
http://www.webarchive.org.uk/wayback/archive/20140614080134/www.jisc.ac.uk/media/documents/programmes/digitisation/blfinal.pdf 
https://content-conversion.com/#digitization-services 
https://content-conversion.com/#digitization-services 
http://www.digitisation.eu/training/recommendations-for-digitisation-projects/recommendations-formats-standards-recommendations/ 
http://www.digitisation.eu/training/recommendations-for-digitisation-projects/recommendations-formats-standards-recommendations/ 
http://www.digitisation.eu/training/recommendations-for-digitisation-projects/recommendations-formats-standards-recommendations/ 



