


















































Explaining How a Deep Neural Network Trained with 
End-to-End Learning Steers a Car 

Mariusz Bojarski 
NVIDIA Corporation 
Holmdel, NJ 07733 

Philip Yeres 
NVIDIA Corporation 
Holmdel, NJ 07733 

Anna Choromanaska 
New York University 
New York, NY 10012 

Krzysztof Choromanski 
Google Research 

New York, NY 10011 

Bernhard Firner 
NVIDIA Corporation 
Holmdel, NJ 07733 

Lawrence Jackel 
NVIDIA Corporation 
Holmdel, NJ 07733 

Urs Muller 
NVIDIA Corporation 
Holmdel, NJ 07733 

Abstract 

As part of a complete software stack for autonomous driving, NVIDIA have create 
a neural-network-based system, know a PilotNet, which output steer angle 
give image of the road ahead. PilotNet be train use road image pair with 
the steer angle generate by a human drive a data-collection car. It derives 
the necessary domain knowledge by observe human drivers. This eliminates the 
need for human engineer to anticipate what be important in an image and foresee 
all the necessary rule for safe driving. Road test demonstrate that PilotNet 
can successfully perform lane keep in a wide variety of drive conditions, 
regardless of whether lane marking be present or not. 
The goal of the work described here be to explain what PilotNet learns and how 
it make it decisions. To this end we developed a method for determine which 
element in the road image most influence PilotNet’s steer decision. Results 
show that PilotNet indeed learns to recognize relevant object on the road. 
In addition to learn the obvious feature such a lane markings, edge of roads, 
and other cars, PilotNet learns more subtle feature that would be hard to antic- 
ipate and program by engineers, for example, bush line the edge of the road 
and atypical vehicle classes. 

1 Introduction 

A previous report [1] described an end-to-end learn system for self-driving car in which a con- 
volutional neural network (CNN) [2] be train to output steer angle give input image of the 
road ahead. This system be now call PilotNet. The training data be image from a front-facing 
camera in a data collection car couple with the time-synchronized steer angle record from 
a human driver. The motivation for PilotNet be to eliminate the need for hand-coding rule and 
instead create a system that learns by observing. Initial result be encouraging, although major 
improvement be require before such a system can drive without the need for human intervention. 
To gain insight into how the learn system decides what to do, and thus both enable further system 
improvement and create trust that the system be pay attention to the essential cue for safe steer- 
ing, we developed a simple method for highlight those part of an image that be most salient 

1 

ar 
X 

iv 
:1 

70 
4. 

07 
91 

1v 
1 

[ 
c 

.C 
V 

] 
2 

5 
A 

pr 
2 

01 
7 



Output: vehicle control 

Fully-connected layer10 neuron 
Fully-connected layer50 neuron 
Fully-connected layer100 neuron 

1164 neuron 

Convolutional 
feature map 
64@1x18 

Convolutional 
feature map 
64@3x20 

Convolutional 
feature map 
48@5x22 

Convolutional 
feature map 
36@14x47 

Convolutional 
feature map 
24@31x98 

Normalized 
input plane 
3@66x200 

Normalization 

Input plane 
3@66x200 

3x3 kernel 

3x3 kernel 

5x5 kernel 

5x5 kernel 

5x5 kernel 

Flatten 

Figure 1: PilotNet architecture. 

in determine steer angles. We call these salient image section the salient objects. A detailed 
report describe our saliency detect method can be found in [3] 

Several method for find saliency have be described by other authors. Among them be sen- 
sitivity base approach [4, 5, 6], deconvolution base one [7, 8], or more complex one like 
layer-wise relevance propagation (LRP) [9]. We believe the simplicity of our method, it fast exe- 
cution on our test car’s NVIDIA DRIVETM PX 2 AI car computer, along with it nearly pixel level 
resolution, make it especially advantageous for our task. 

1.1 Training the PilotNet Self-Driving System 

PilotNet training data contains single image sample from video from a front-facing camera in 
the car, pair with the correspond steer command (1/r), where r be the turn radius of the 
vehicle. The training data be augment with additional image/steering-command pair that simulate 
the vehicle in different off-center and off-orientationpoistions. For the augment images, the target 
steer command be appropriately adjust to one that will steer the vehicle back to the center of 
the lane. 

Once the network be trained, it can be use to provide the steer command give a new image. 

2 PilotNet Network Architecture 

The PilotNet architecture be show in Figure 1. The network consists of 9 layers, include a normal- 
ization layer, 5 convolutional layer and 3 fully connect layers. The input image be split into YUV 

2 



Averaging 

Sca 
le u 

p 

Final visualization mask 
1@66x200 

Pointwise 
multiplication 

Figure 2: Block diagram of the visualization method that identifies the salient objects. 

plane and pass to the network. The first layer of the network performs image normalization. The 
normalizer be hard-coded and be not adjust in the learn process. 

The convolutional layer be design to perform feature extraction and be chosen empirically 
through a series of experiment that varied layer configurations. Strided convolution be use in 
the first three convolutional layer with a 2×2 stride and a 5×5 kernel and a non-strided convolution 
with a 3×3 kernel size in the last two convolutional layers. 
The five convolutional layer be follow with three fully connect layer lead to an output 
control value that be the inverse turn radius. The fully connect layer be design to function 
a a controller for steering, but note that by training the system end-to-end, there be no hard boundary 
between which part of the network function primarily a feature extractor and which serve a the 
controller. 

3 Finding the Salient Objects 

The central idea in discern the salient object be find part of the image that correspond to 
location where the feature maps, described above, have the great activations. 

The activation of the higher-level map become mask for the activation of low level use the 
follow algorithm: 

1. In each layer, the activation of the feature map be averaged. 
2. The top most average map be scale up to the size of the map of the layer below. The 

up-scaling be do use deconvolution. The parameter (filter size and stride) use for the 

3 



deconvolution be the same a in the convolutional layer use to generate the map. The 
weight for deconvolution be set to 1.0 and bias be set to 0.0. 

3. The up-scaled average map from an upper level be then multiply with the average map 
from the layer below (both be now the same size). The result be an intermediate mask. 

4. The intermediate mask be scale up to the size of the map of layer below in the same way 
a described Step 2. 

5. The up-scaled intermediate map be again multiply with the average map from the layer 
below (both be now the same size). Thus a new intermediate mask be obtained. 

6. Steps 4 and 5 above be repeat until the input be reached. The last mask which be of the 
size of the input image be normalize to the range from 0.0 to 1.0 and becomes the final 
visualization mask. 

This visualization mask show which region of the input image contribute most to the output of 
the network. These region identify the salient objects. The algorithm block diagram be show in 
Figure 2. 

The process of create the visualization mask be illustrate in Figure 3. The visualization mask 
be overlaid on the input image to highlight the pixel in the original camera image to illustrate the 
salient objects. 

Results for various input image be show in Figure 4. Notice in the top image the base of car a 
well a line (dashed and solid) indicate lane be highlighted, while a nearly horizontal line from 
a crosswalk be ignored. In the middle image there be no lane paint on the road, but the park 
cars, which indicate the edge of the road, be highlighted. In the low image the grass at the edge 
of the road be highlighted. Without any coding, these detection show how PilotNet mirror the way 
human driver would use these visual cues. 

Figure 5 show a view inside our test car. At the top of the image we see the actual view through the 
windshield. A PilotNet monitor be at the bottom center display diagnostics. 

Figure 6 be a blowup of the PilotNet monitor. The top image be capture by the front-facing camera. 
The green rectangle outline the section of the camera image that be fed to the neural network. 
The bottom image display the salient regions. Note that PilotNet identifies the partially occlude 
construction vehicle on the right side of the road a a salient object. To the best of our knowledge, 
such a vehicle, particularly in the pose we see here, be never part of the PilotNet training data. 

4 Analysis 

While the salient object found by our method clearly appear to be one that should influence steer- 
ing, we conduct a series of experiment to validate that these object actually do control the 
steering. To perform these tests, we segment the input image that be present to PilotNet into two 
classes. 

Class 1 be meant to include all the region that have a significant effect on the steer angle output 
by PilotNet. These region include all the pixel that correspond to location where the visualization 
mask be above a threshold. These region be then dilate by 30 pixel to counteract the increase 
span of the higher-level feature map layer with respect to the input image. The exact amount of 
dilation be determine empirically. The second class include all pixel in the original image 
minus the pixel in Class 1. If the object found by our method indeed dominate control of the 
output steer angle, we would expect the following: if we create an image in which we uniformly 
translate only the pixel in Class 1 while maintain the position of the pixel in Class 2 and use this 
new image a input to PilotNet, we would expect a significant change in the steer angle output. 
However, if we instead translate the pixel in Class 2 while keep those in Class 1 fix and feed 
this image into PilotNet, then we would expect minimal change in PilotNet’s output. 

Figure 7 illustrates the process described above. The top image show a scene capture by our 
data collection car. The next image show highlight salient region that be identify use the 
method of Section 3. The next image show the salient region dilated. The bottom image show a 
test image in which the dilate salient object be shifted. 

4 



Figure 3: Left: Averaged feature map for each level of the network. Right: Intermediate visual- 
ization mask for each level of the network. 

Figure 4: Examples of salient object for various image inputs. 

5 



Figure 5: View inside our test car 

The above prediction be indeed born out by our experiments. Figure 8 show plot of PilotNet 
steer output a a function of pixel shift in the input image. The blue line show the result when 
we shift the pixel that include the salient object (Class 1). The red line show the result when we 
shift the pixel not include in the salient objects. The yellow line show the result when we shift 
all the pixel in the input image. 

Shifting the salient object result in a linear change in steer angle that be nearly a large a 
that which occurs when we shift the entire image. Shifting just the background pixel have a much 
small effect on the steer angle. We be thus confident that our method do indeed find the most 
important region in the image for determine steering. 

5 Conclusions 

We describe a method for find the region in input image by which PilotNet make it steer 
decisions, i. e., the salient objects. We further provide evidence that the salient object identify by 
this method be correct. The result substantially contribute to our understand of what PilotNet 
learns. 

Examination of the salient object show that PilotNet learns feature that “make sense” to a human, 
while ignore structure in the camera image that be not relevant to driving. This capability be 
derive from data without the need of hand-crafted rules. In fact, PilotNet learns to recognize subtle 

6 



Figure 6: The PilotNet monitor from Figure 5 above 

Figure 7: Images use in experiment to show the effect of image-shifts on steer angle. 

7 



Figure 8: Plots of PilotNet steer output a a function of pixel shift in the input image. 

feature which would be hard to anticipate and program by human engineers, such a bush line 
the edge of the road and atypical vehicle classes. 

References 
[1] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, 

Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao, and Karol Zieba. 
End to end learn for self-driving cars, April 25 2016. URL: http://arxiv.org/abs/1604. 
07316, arXiv:arXiv:1604.07316. 

[2] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backprop- 
agation apply to handwritten zip code recognition. Neural Computation, 1(4):541–551, Winter 1989. 
URL: http://yann.lecun.org/exdb/publis/pdf/lecun-89e.pdf. 

[3] Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Bernhard Firner, Larry Jackel, Urs Muller, 
and Karol Zieba. VisualBackProp: visualize CNNs for autonomous driving, November 16 2016. URL: 
https://arxiv.org/abs/1611.05418, arXiv:arXiv:1611.05418. 

[4] D. Baehrens, T. Schroeter, S. Harmeling, M.i Kawanabe, K. Hansen, and K.-R. Müller. How to explain 
individual classification decisions. J. Mach. Learn. Res., 11:1803–1831, 2010. 

[5] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image 
classification model and saliency maps. In Workshop Proc. ICLR, 2014. 

[6] P. M. Rasmussen, T. Schmah, K. H. Madsen, T. E. Lund, S. C. Strother, and L. K. Hansen. Visualization of 
nonlinear classification model in neuroimaging - sign sensitivity maps. BIOSIGNALS, page 254–263, 
2012. 

[7] M. D. Zeiler, G. W. Taylor, and R. Fergus. Adaptive deconvolutional network for mid and high level 
feature learning. In ICCV, 2011. 

[8] M. D. Zeiler and R. Fergus. Visualizing and understand convolutional networks. In ECCV, 2014. 

[9] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Müller, and W Samek. On pixel-wise explanation 
for non-linear classifier decision by layer-wise relevance propagation. PLOS ONE, 10(7):e0130140, 2015. 
URL: http://dx.doi.org/10.1371/journal.pone.0130140. 

8 

http://arxiv.org/abs/1604.07316 
http://arxiv.org/abs/1604.07316 
http://arxiv.org/abs/arXiv:1604.07316 
http://yann.lecun.org/exdb/publis/pdf/lecun-89e.pdf 
https://arxiv.org/abs/1611.05418 
http://arxiv.org/abs/arXiv:1611.05418 
http://dx.doi.org/10.1371/journal.pone.0130140 

