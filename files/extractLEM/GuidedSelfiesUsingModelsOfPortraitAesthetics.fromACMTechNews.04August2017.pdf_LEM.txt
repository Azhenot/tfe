
















































Guided Selfies use Models of Portrait Aesthetics 


Guided Selfies use Models of Portrait Aesthetics 
Qifan Li, Daniel Vogel 

Cheriton School of Computer Science, University of Waterloo 
{qifanli, dvogel}@uwaterloo.ca 

synthetic dataset aesthetic model guidance 
adjustment 

selfie 

feature detection 

camera 
app 

fp(x,y,r) 

fl(u,v) 

fs(r)face size 

position 

light 

rating 

Figure 1: Methodology and technique to enable interactive aesthetic guidance when take a selfie on an unmodified smartphone. 

ABSTRACT 
We introduce technique enable interactive guidance for 
good self-portrait photo (“selfies”) use a smartphone cam- 
era. Aesthetic quality be estimate use empirical model for 
three parameterized composition principles: face size, face 
position, and light direction. The model be built use 
2,700 crowdworker assessment of highly-controlled synthetic 
selfies. These be generate by manipulate a virtual camera 
and light when render a realistic 3D model of a human 
to methodically explore the parameter space. A camera appli- 
cation us the model to estimate the aesthetic quality of a 
live selfie preview base on parameter measure by computer 
vision. The photographer be guide towards a good selfie by 
directional hint overlaid on the live preview. A study show 
the technique provide a 26% increase in aesthetic quality 
compare to a standard camera application. 

ACM Classification Keywords 
H.5.2 User Interfaces: Input device and strategy 

Author Keywords 
mobile computing; computational photography; 

INTRODUCTION 
Smartphone self-portrait photographs, call “selfies”, account 
for more than 30% of picture take by people age 18 to 
24 [15]. However, not everyone have photography skills, so 
these be often unsatisfying images. Visual aesthetic can be 
improve after a selfie be take by edit and re-touching [8], 
but this require extra effort, reduces realism, may degrade res- 
olution, and aspect like face distortion and light direction 

Permission to make digital or hard copy of all or part of this work for personal or 
classroom use be grant without fee provide that copy be not make or distribute 
for profit or commercial advantage and that copy bear this notice and the full citation 
on the first page. Copyrights for component of this work own by others than ACM 
must be honored. Abstracting with credit be permitted. To copy otherwise, or republish, 
to post on server or to redistribute to lists, require prior specific permission and/or a 
fee. Request permission from Permissions@acm.org. 
DIS 2017, June 10-14, 2017, Edinburgh, United Kingdom. 
Copyright© 2017 ACM ISBN 978-1-4503-4922-2/17/06...$15.00. 
DOI: http://dx.doi.org/10.1145/3064663.3064700 

be hard to correct. Using a “selfie stick” or a smartphone case 
with a built-in ring light [23, 24] can reduce distortion and im- 
prove lighting, but use accessory be not always convenient. 

We introduce a methodology and technique for interactive aes- 
thetic guidance when take a selfie on standard smartphone 
(Fig. 1). Instead of encode rule-of-thumb photographic prin- 
ciples [26, 27, 33], we derive quantitative aesthetic model 
empirically. Rather than learn principle from a dataset of 
near-random image take from the internet (e.g. [18, 10, 36, 9, 
22, 38]), we generate highly-controlled synthetic self-portrait 
photograph use 3D rendering. This enables a methodical ex- 
ploration and validation of underlie compositional features, 
in our case: face size, face position, and light direction. 

Our dataset of synthetic selfies be use in 2,700 crowdworker 
assessments. These assessment be transform into three 
quantitative models, each mapping a compositional feature 
configuration to an aesthetic score. We then describe a smart- 
phone application to analyse the preview image with computer 
vision method when take a selfie. With this analysis, the 
configuration of each feature may be calculate and fed to our 
model to find the current aesthetic score and direction of 
improvement. Corresponding directional hint be then over- 
laid on the preview image, guide the photographer to move 
the smartphone to improve one type of aesthetic quality. In 
a study, selfies take with our system be rat 26% high 
compare to those take by a standard camera application. 
To make the problem tractable, our focus be on single-person 
selfies without a dominant background object. However, our 
system be design so the photographer can selectively ig- 
nore some guidance to include other people or compensate for 
background like tourist landmarks. We make the follow 
contributions: 

• A systematic assessment of three feature of selfie aesthet- 
ics: face position, face size, and light direction. 

• Empirically-derived model to estimate the aesthetic score 
and direction of improvement for each feature. 

Design Case Studies & Methods 2 (Technology) DIS 2017, June 10–14, 2017, Edinburgh, UK 

179 

http://www.acm.org/about/class/ccs98-html#H.5.2 
lphilippe 
Note 
Is it linear??? On which scale??? 



• A smartphone camera application to guide novice photogra- 
phers to take good single person selfies use our model 
and a method to estimate dominant lighting. 

• A control experiment validate the application’s usabil- 
ity and capability to increase aesthetic quality. 

BACKGROUND AND RELATED WORK 
Although photography be an art form, rules-of-thumb have 
be propose to make even casual photograph more aes- 
thetically pleasing. Child [7] emphasizes that composition 
be important to attract and keep the viewer’s attention and it 
complement the communication between the viewer and the 
photograph. One of the most common composition rule be the 
rule-of-thirds [31]. This be a 3×3 grid form by divide the 
image horizontally and vertically into nine equal portions. The 
grid line intersection point be call “power-points” which 
be commonly consider the best location to place signifi- 
cant elements. For portrait photography, guide recommend 
how to use the rule-of-thirds to compose portrait style such a 
“head-and-shoulders”, “three-quarter”, and “full-length” [17]. 
For example, place the eye in the top third for single person 
portrait [11]. A related compositional principle be the size of 
the subject [22, 25, 29]. In portraiture, some recommend fill- 
ing the frame with the face [11], but face size be also dependent 
on the intend portrait style [17]. 

In addition to these spatial composition principles, Hurter [17] 
emphasizes the importance of light composition. Hurter 
argues light be the dominant factor to represent a three- 
dimensional reality in a two-dimensional space, “Just a a 
sculptor model clay to create the illusion of depth, so light 
model the shape of the face to give it depth and form.” Dif- 
ferent rule have be propose for portraiture lighting. Some 
suggest a side-light so a pattern of light and shadow defines 
the face [7, 16], others recommend a frontal light to eliminate 
shadow and flatten the face [34]. 

Computational Aesthetics 
Since such rules-of-thumb be imprecise and questionable, 
researcher have attempt to calculate the aesthetic quality 
of image use computational methods. One approach be to 
model aesthetic quality a a machine learn problem with an 
unconstrained number of features. A large corpus of image 
be rat by people to establish a ground truth for aesthetic 
quality. Many global and local feature be extract from the 
same image that be rat (typically image statistic related 
to colour, edges, contours, saliency, etc.). Then, machine 
learn algorithm be train use the rating and extract 
feature to automatically rate image for aesthetics. Most 
investigation use photograph acquire from an ad hoc online 
database [18, 10, 36, 9, 22, 38] since large amount of data be 
need for training. However, model use many feature 
and with uncontrolled training image often span many 
class (e.g landscapes, animals, groups, portraits, etc.) make 
it difficult to find underlie aesthetic rule [25]. 

This general approach have also be apply to portrait pho- 
tographs. Khan and Vogel [20] model aesthetic quality of 
individual portrait use a small set of feature from photog- 
raphy tutorials, include face position and lighting. Males 

et al. [27] train on feature they argue be use by professional 
photographers, include composition, face size, and feature 
related to light, like contrast, lightness, and highlights. Redi 
et al. [33] use feature inspire by photographic guides, includ- 
ing compositional rules. Chen et al. [6] develop a method to 
extract feature of artistic lighting. 

These work validates our focus on face position, face size, 
and lighting, but they still use highly variable training image 
from online databases. Although Redi et al.’s [33] result 
suggest portrait aesthetic be independent of age, gender, and 
ethnicity, other work suggest otherwise. Mazza et al. [29] find 
that dress and gender influence the perception of “head-shot” 
photograph and Xu et al. [37] found people rating photo 
for aesthetic quality often comment on non-compositional 
feature like smile and mood. Also, Manovich et al.’s “Selfie 
City” [28] — a theoretic, artistic, and quantitative study of 
selfies — provide compelling evidence that age, gender, pose, 
and facial expression be link to highly rat photographs. 

This suggests that rating uncontrolled image to train model 
for compositional aesthetic quality be problematic. In addition 
to non-aesthetic confounds, precisely extract high-level 
feature like light direction be difficult for arbitrary image 
and many feature change from image to image so understand- 
ing why a rater chose a certain image be difficult. Moreover, 
many of these work create feature base on rules-of-thumb, 
but recent work suggests some rules, like the rule-of-thirds, 
may not improve aesthetic rating [37, 2]. These factor all 
introduce error and uncertainty into the aesthetic model. 

Our highly control synthetic image corpus eliminates these 
potential confounds when rating aesthetics. In addition, our 
corpus isolates compositional feature when rating, disentan- 
gling complex interaction result from comparative rating 
when multiple feature vary. 

Realtime Aesthetic Guidance 
The work above develop aesthetic model to evaluate exist 
images. Some explicitly use evaluate image a example 
to teach amateur photographer method to improve aesthet- 
ic [38], but they do not assist while take a photograph. Our 
goal be to model aesthetic to create an interactive camera ap- 
plication with aesthetic guidance. This be complementary to 
guidance for low level feature like exposure, luminance, and 
motion blur [30, 3]. 

Previous system exist for realtime aesthetic guidance for 
photography and video. Ma et al. [26] develop an aesthetic 
model to suggest where to pose a person in a landscape pho- 
tograph (visualized by a rectangle). However, the technique 
be not implement on a smartphone or test for interactive 
use. San pedro and Church [35] describe a smartphone cam- 
era application to guide photographer use 22 composition 
and 7 exposure feature applicable to a wide class of image 
(proposed by Obrador et al. [31]). Feedback us a musical 
composition mapped to feature scores, a small pilot study be 
inconclusive whether aesthetic quality be improved. 

NudgeCam be a smartphone application to record product 
demonstration video [4]. Text message and a colour box 
around the face indicate when there be problem with face 

Design Case Studies & Methods 2 (Technology) DIS 2017, June 10–14, 2017, Edinburgh, UK 

180 



size, face location, and scene brightness, tilt, and stability. A 
related system by Kumano et al. [21] us icon to indicate 
when video motion be poor and to suggest when to zoom or 
pan the shot. These two system encode exist videography 
rules-of-thumb, and neither be formally evaluated. 

There be two direct precursor to our work. A commercial 
app call Camera51 [1] “intelligently analyzes a scene and 
look for lines, shapes, and people” [12] to provide guidance 
to a single, globally optimal composition. The underlie al- 
gorithms and aesthetic rule be unknown, and the user have no 
ability to selectively improve specific compositional principle 
while ignore others. 

Xu et al. [37] developed a realtime guidance system for portrait 
photographs. Face position and subject size be guide use 
a rule-based aesthetic model derive directly from the rule- 
of-thirds: the face should be on a grid line or power point, 
and the subject width should be one-third of the image. Their 
system require a special laptop and external three-camera 
array. A study conduct outdoors evaluates whether the 
system improves aesthetic compare to a baseline system 
show only a static rule-of-thirds grid. Unlike Xu et al., 
our system be specifically focus on self-portraits, it work 
on a standard smartphone, it considers light direction in 
addition to face position and size, our guidance be base on 
empirically-derived model derive from a synthetic dataset, 
and our final evaluation environment be highly-controlled. 

DATASET AND AESTHETIC QUALITY RATINGS 
Our goal be to help novice photographer take self-portrait 
that average people find more aesthetically pleasing. We ac- 
complish this by render hundred of synthetic portrait that 
systematically vary three key compositional features: face 
size, face position, and light direction. These portrait be 
use to gather aesthetic rating in an online crowdsourcing 
experiment. We explain how these rating be transform into 
model to guide people when take selfies in a late section. 

3D Rendered Synthetic Selfie Dataset 
All synthetic portrait be 3D rendering of realistic-looking 
3D human model in front of a uniform 18% grey background. 
We use 3 female and 3 male models, with each gender pair 
have Caucasian, Asian, or Black ethnicity features. Models 
be select to be consistent and “average” look with 
neutral facial expressions, typical hair styles, no glasses, and 
similar poses. The arm of our 3D model be not poseable, so 
our synthetic selfies do not capture the subject though hold 
the smartphone. However, it be unlikely this small detail alters 
aesthetic ratings. By generate portrait across all six models, 
we eliminate confounds like background, gender, and smile 
when rat for aesthetic quality. 

Renderings be generate use Blender (www.blender.com), 
an open-source 3D model program. Virtual camera prop- 
erties be set to simulate the front camera of an iPhone 6 
(focal length 45mm, field-of-view 54.2◦, aspect ratio 3:4), and 
a parallel light source be configure to simulate the sun or 
dominant indoor light source. Ambient light be set to soften 
shadow and simulate typical light condition outside or in 
brightly lit interiors. Using a Python API, the position of the 

human model, virtual camera, and parallel light source be 
manipulate to explore each aesthetic feature a explain 
below. 

Note that dataset generation and aesthetic quality rating be 
interleave a the three feature be explore in sequence. 
First, portrait to explore face size be generate and as- 
sessed. These result establish which face size to use when 
generate portrait to explore face position. After face posi- 
tion be assessed, the result be use to select face position 
and size when explore the light direction feature. 

Face Size 
Face size be an important feature because a face in a portrait 
should be large enough to be the centre of interest, but not 
so close that facial feature distort [13]. We define face size 
a the ratio between the width of face — twice the distance 
between the eye — and one-third of the image width. This 
ratio normalizes the feature and relates it to the rule-of-thirds 
guideline which suggests subject should be size to one cell 
in a rule-of-thirds grid. A face size of 1.0 match this exist 
guideline. 

To generate portrait explore face size, we position the vir- 
tual camera and human model to render 19 image with face 
size range from 0.2 to 2.0 in step of 0.1 (Fig. 2). The range 
of ratio be base on measure real selfies take a close a 
possible without face crop and a far a possible use a 
standard “selfie stick.” The face position and light direction 
be constant: face position be centred, and the light source 
shine straight onto the face. 

Figure 2: Example face size portraits: 0.4, 0.8, 2.0 use Asian female. 

Face Position 
The rule-of-thirds be commonly use to position subject in 
photograph [7, 17] and previous research have employ it 
for aesthetic rating [26, 9] and guidance [37]. We define 
face position a the 2D location of the centroid of the eye 
and parametrize it use a subdivide 3×3 rule-of-thirds grid. 
Each grid cell be divide by 4 to construct a 12×12 grid with 
face position denote a (x,y), where x and y be multiple 
of 112 in term of normalize image position. This way, face 
position (4,4), (4,8), (8,8), and (8,4) correspond to rule-of- 
third “power points” assume to be prefer position [20]. 

Based on the distribution of face size aesthetic rating (recall 
dataset generation and rating be interleaved), we explore 
face position use 4 face sizes: 0.3, 0.5, 0.8, and 1.0. For 
each face size, 81 image be render with the virtual cam- 
era place to create face position range from (2,2) to (10,10) 
— a 9×9 subset of the 12×12 grid parametrization that avoids 

Design Case Studies & Methods 2 (Technology) DIS 2017, June 10–14, 2017, Edinburgh, UK 

181 



extremely cropped face (Fig. 3). The face be kept orient 
towards the camera by align the model’s face direction (rep- 
resent a the normal of a transparent plane) to the camera 
film plane. As in face size images, the light source shine 
straight onto the face. 

Figure 3: Example face position portraits: (4,4), (6,6), (9,2) use Asian 
male with face size 0.8. 

Lighting Direction 
The dominant light source be assume to be omnidirectional 
with parallel ray like the sun. We define light direction a 
the light source position in spherical coordinate azimuth and 
polar angle (θ,φ), with the origin at the centre of the head. For 
example, the light be directly in front of the face at (0◦,90◦), to 
the right side and half-way above at (−90◦,45◦), and directly 
above at (0◦,0◦). 

To reduce the space to explore, we assume light should not 
come from below or behind the face. This restricts the range of 
θ to [−120◦,120◦] and φ to [0◦,90◦]. With step size of 30◦ for 
θ and 11.25◦ for φ, each component have 9 setting create 81 
total combinations. We use aesthetic rating result from face 
size and face position to create 4 set of light images: size 
0.3, position (6,2); size 0.5, position (6,3); size 0.8, position 
(6,4); and size 1.0, position (6,4). The virtual camera position 
be fix to maintain the face size and position a the light 
source partially orbit around the head position (Fig. 4). 

Figure 4: Examples of light direction portraits: (-60°,67°), (0°,90°), 
(90°,90°) use Black female with face size 1.0, face position (6,4). 

Aesthetic Rating 
These set of synthetic selfies be rat for aesthetic quality on 
Amazon Mechanical Turk (AMT). Workers picked a multiple 
best and bad image for each set of image portray a 
single 3D human model for each compositional feature. 

Participants 
We recruit 2,700 AMT worker without any experience, age, 
or location criterion (though the majority live in the United 

States). Our objective be to get aesthetic rating from “average 
people.” Workers be paid between $0.10 and $0.30 per task 
(called a HIT on AMT). The full protocol be approve by a 
research ethic board. 

Task and Implementation 
The task be to view a set of synthetic portrait of a single 
model explore a single compositional feature, and pick at 
least N good and N bad images. For example, pick 8 good 
and 8 bad image among 81 portrait of the Caucasian male 
model with different face positions. Note that compare two 
image at a time would not be practical — there would be 
3,240 comparison in just one face size image set. 

User interface for AMT task must be clear and efficient to 
encourage worker to complete the task correctly and honestly 
[32]. We iteratively developed our interface with these goal 
(Fig. 5). Thumbnails of image be arrange in a 9×9 grid 
(for face position and light direction), or a 1×19 row (for 
face size). Clicking on a thumbnail display it a a full image 
and selects it (when the task begins, a random thumbnail be 
selected). Thumbnails can be select and view quickly by 
drag the mouse or use cursor keys. 

Clicking on a rating button, or use a shortcut key, classifies 
the select image and indicates the decision with a colour out- 
line: green for ‘good’ (shortcut key ‘1’), blue for ‘undecided’ 
(key ‘2’), and red for ‘bad’ (key ‘3’). An undecided decision 
be automatically assign when a thumbnail be select for 
more than 2 seconds, but no good or bad rating provided. The 
worker can change their rating at any time. The remain 
number of good and bad rating be show in the rating but- 
tons. Once the minimum number of rating be completed, a 
submit button be enabled. The task be implement a a web 
application use AngularJS. 

(a) 

(b) 

(c) 

Figure 5: Aesthetic rating task interface (Caucasian male, rating face 
position): (a) large image view; (b) rating buttons; (c) thumbnails. 

Design 
Recall that we generate one set of image for face size, four 
set of image for face position, and four set of image for 
light direction, make nine set in total. Each task rate 
image in one set for one human model: for face size, worker 
have to pick at least 3 good and 3 bad images; for face position 
and light direction, they have to pick at least 8 good and 
8 bad images. With 9 set and 6 human model there be 

Design Case Studies & Methods 2 (Technology) DIS 2017, June 10–14, 2017, Edinburgh, UK 

182 



9∗6 = 54 task variations. For each task variation, we recruit 
50 workers, require 2,700 worker in total. 

The procedure be a typical AMT HIT. When the HIT be 
accepted, the worker acknowledge a consent form and then 
view a video demonstrate the task interface. Instructions 
clarify that the goal be to “choose good and bad example of 
image when consider them a selfies.” 

Results 
To calculate the score of each image (representing a specific 
configuration for one of the principle be evaluate with a 
specific model), we sum all rating use the follow 
tally: +1 for each ‘good’, -1 for each ‘bad’, and 0 otherwise. 
Each image tally be then divide by the number of time that 
image be rat (regardless if ‘good’, ‘bad’, or ‘undecided’) 
to normalize score to a range between -1 and 1. 

To examine the consistency of score and actual sample size 
for each image, we calculate the standard error of the mean 
(SEM) and percentage of task each image be rated. Rating 
score be consistent, all SEMs be less than 0.05. Actual 
sample size be excellent for face size, with all image rat 
by at least 74% of workers, and good for face position and 
light direction with all image rat by at least 30% and 
45% of worker respectively. 

Face Size 
Figure 6 illustrates the results. The prefer face size be 
0.8 (score 0.33) and 0.5 (score 0.32). The score dip to 0.22 
between those peak and down to 0.13 for the small face 
size 0.2. However, all face size less than 1.4 have a positive 
rating. In contrast, score for large face size fall sharply, 
decrease to -0.6 for face size 2.0. 

This suggests people prefer face to be 50% of a rule-of-thirds- 
grid cell (16% of image width) or 80% to 90% of a rule-of- 
third grid cell (27% to 30% of image width). Note that face 
very far from the camera, approximately less than 30% of a 
rule-of-thirds grid cell (10% of image width), be rat lower. 
Most pronounce be low rating for face very close to the 
camera, with negative score when face be large than 130% 
of a rule-of-thirds grid cell (43% of the image width). 

Face Position 
Figure 6 illustrates the results. A mass of positive score 0.7 
or great can be see when the face be roughly horizontally 
centred. This positive mass move vertically up the image a 
the face size becomes smaller. Positions near the bottom and 
side be negative, most below -0.3, likely due to cropping. 

Higher aesthetic rating for a centre face break from the ac- 
cepted rule-of-thirds principle [9]. Rows and column label 
4 and 8 represent rule-of-thirds “power lines”, and cell at 
position (4,4), (8,4), (4,8), and (8,8) represent rule-of-thirds 
“power points.” There be no indication of high rating for 
power-lines or power points. This add empirical support to 
Xu et al.’s [37] informal observations. 

Lighting Direction 
The pattern of score for the four set of light direction 
score appear very similar. Figure 6 illustrates the score for 

the four set and the aggregate results. A mass of positive 
score can be see when θ be between −30◦ to 30◦ and φ be 
between 67.5◦ to 78.75◦, with a notable positive extension 
down to φ = 90◦ at θ = 0◦, and out to θ = ±60◦ at φ = 78.75◦. 
Higher aesthetic rating for light shin directly (or nearly 
directly) on the face reduces shadow and evenly light the 
entire face. This be exactly the result when use a smartphone 
case with a built-in ring light [23, 24], but it contradicts some 
past work [20, 6] suggest that light a face from one side 
be preferable. 

Summary 
These rating suggest clear preference for face size, face 
position, and light when take selfies: face should not 
be too big, face should be centre and near the top without 
get too close to the edge, and light should shine from the 
front. Although these may appear elementary, they have not 
be derive empirically before. Moreover, have matrix 
of score will enable our guidance system to suggest local 
direction of improvement (i.e. “move your face right”) which 
be more flexible for user than simply suggest the globally 
prefer location (i.e. “move your face here”). 

AESTHETIC MODELS 
In order to use the rating for real-time guidance, we transform 
the matrix of score into three models. Each model estimate 
an aesthetic score and direction of improvement for one com- 
positional feature give the current state of compositional 
feature parameters. The main challenge be how to transform 
empirically know score at discrete parameter setting into 
continuous function that return a score for any measure face 
size, face position, and light direction. 

The general form of each model be (s, d) = f (ω0,ω1, ...,ωn). It 
be a function f which accepts compositional feature parameter 
{ω0,ω1, ...,ωn} to calculate a score s and a vector d describe 
the direction in compositional feature space that will improve 
the score. 

The specific model are: 

• Face Size: (ss, ds) = fs(r) 
The model us an interpolate lookup into the 1×19 row 
matrix of face size scores. Given the detect face size 
ratio r, the score s be a linear interpolation between the two 
know score at face size define the interval where r lies. 
If r be outside the 1×19 row matrix, the s be extrapolate 
use the two closest know scores. The one-dimensional 
direction of improvement d be the direction towards the 
large interval score. 

• Face Position: (sp, dp) = fp(x,y,r) 
Recall there be four 8×8 matrix of face position score 
for four face size ratio (0.3, 0.5, 0.8, 1.0). The model 
us a two-step interpolate lookup into these four matrices. 
The detect face size ratio r be use to construct an 8×8 
matrix by linearly interpolate correspond element of 
matrix form the interval around r. If r be outside all 
matrices, element be extrapolated. Then the score sp be 
a bilinear interpolation (or extrapolation) of the element 

Design Case Studies & Methods 2 (Technology) DIS 2017, June 10–14, 2017, Edinburgh, UK 

183 



0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 

size 

1.2 1.3 1.4 2.01.5 1.6 1.7 1.8 1.9 

-0.8 

-0.4 

0.0 

0.4 

0.8 

-120 -90 -60 -30 0 30 60 90 120 
theta (°) 

5 
6 

7 
8 

4 
3 

2 
1 

0 
ph 

i ( 
×1 

1. 
25 

°) 
2 3 4 5 6 7 8 9 10 

2 
3 

4 
5 

6y 

x 

7 
8 

9 
10 

2 3 4 5 6 7 8 9 10 

2 
3 

4 
5 

6y 

x 

7 
8 

9 
10 

2 3 4 5 6 7 8 9 10 

2 
3 

4 
5 

6y 

x 
7 

8 
9 

10 

(0.3) (0.5) 

(0.8) (1.0) 

face size 

light (average) light (at size ...) 

position (at size 0.3) position (at size 0.5) position (at size 0.8) position (at size 1.0) 

2 3 4 5 6 7 8 9 10 

2 
3 

4 
5 

6y 

x 

7 
8 

9 
10 

Figure 6: Visualizations of aesthetic rating for size, position, and lighting. Note light be average across size give similar pattern for specific sizes. 

surround the 2D interval where the detect position 
(x,y) resides in the interpolate matrix. The direction dp be 
the 2D direction towards the surround element with the 
high score. 

• Lighting Direction: (sl, dl) = fl(u,v) 
Unlike face position or face size, we cannot directly measure 
3D light direction Θ and Φ from a 2D image. Instead, 
the model us (u,v), vector component to represent the di- 
rection and magnitude of the brightest patch of skin around 
the nose. We describe our algorithm to compute (u,v) later. 
These light direction component be transform into 
the best estimate for Θ and Φ by find the near neigh- 
bour to a set of canonical component (u∗,v∗) compute 
use the 3D human model and know Θ and Φ. With Θ 
and Φ, the correspond score can be found in the single 
aggregate 8×8 light score matrix. The direction dl be 
the 2D direction towards the surround matrix element 
with the high score. 

SMARTPHONE APPLICATION 
We create a smartphone “app” that detects the current face 
size, face position, and light direction use computer vi- 
sion technique and pass these to the aesthetic models. The 
result be use to provide interactive guidance by suggest 
how to move the smartphone to improve aesthetics. Our app 
run on an iPhone 6 with iOS 9.3 use OpenCV 2.4.10 for 
computer vision. 

Feature Detection and Calculation use Computer Vision 
Each frame of the camera preview be capture and downsam- 
plead to 240×320 pixels. The native IOS CIDetector be use 
to find the bound box of the face, mouth, and eyes. An 

OpenCV Haar classifier be use to detect the nose, but to im- 
prove speed and robustness, only the region bound by the 
eye and mouth be searched. Face and eye detection run at 
12 FPS and nose detection at 8 FPS, acceptable for real time 
photo guidance. These detect feature be use to compute 
model parameter a follows. 

Face Size (r) and Face Position (x,y) 
Recall face size and face position be express relative to 
a rule-of-thirds grid. The current face size (r) be the ratio 
of twice the distance between the detect eye position to 
one-third of the width of the preview image. The current face 
position (x,y) be the centroid of the eye express in twelfth 
of the preview image width and height. A low-pass filter [5] be 
apply to stabilize the face size and face position ratio across 
frames. 

Lighting Direction (u,v) 
To calculate the light direction, we examine the pattern 
of luminance on and around the nose. The protrude nose 
geometry produce local shadow and highlight that capture 
the global light information. Previous work use the pattern 
of light on the nose for 2D track [14]. 

The detect nose region-of-interest (ROI) be downsampled 
to 100×100 px and convert to HSV colour space. Eight 
9×9 px patch be define radially around the centre of the 
nose ROI, each with a correspond patch direction vector vi 
from nose ROI centre to patch centre (Figure 7a). The ratio 
of the median luminance of each patch li to the luminance 
at nose ROI centre lc be use to scale each correspond 
direction vector: (li/lc)× vi. For robustness, the step above 
be repeat for patch at radius from 9 px to 45 px. The final 
light direction vector component (u,v) be the sum of the 

Design Case Studies & Methods 2 (Technology) DIS 2017, June 10–14, 2017, Edinburgh, UK 

184 



scale vector from all iterations. A low-pass filter [5] be also 
apply to stabilize (u,v) across frames. 

Figure 7: Lighting direction detection: (a) algorithm illustration; (b) test 
examples. 

We informally test this algorithm in two stages. First, we 
ran the algorithm on the synthetic image use for the light 
direction ratings. To prevent a confound from imperfect nose 
detection, the nose region be define in Blender by render 
a rectangle. A comparison of the calculate light direction 
with know light direction for different value of Φ and 
Θ suggest the algorithm work well when the face be lit from 
the front (Θ ∈ [−90◦,90◦]), but becomes noisy when lit from 
behind (Θ ∈ [−120◦,−90◦]∪ [90◦,120◦]) or top (Φ = 0◦). 
Second, we ran the algorithm on real photo (1 female and 2 
males) take at different orientation to the sun (examples in 
Figure 7b). The calculate light direction be consistent 
for all three set with similar noisy measurement when lit 
from the top and behind. It also appear to be robust to the 
presence of eyeglass and facial hair. 

In practice, we found it work well inside a well, long a 
there be a reasonably dominant light source. Diffuse light or 
very low light introduces some noise, but in those condition 
light direction have no effect. The noisy estimate when lit from 
behind or above will likely guide people towards some good 
light position, albeit in a less consistent direction. 

User Interface Design 
The primary interface component be the guidance icon for 
the three compositional features. When a face be detected, a 
circle surround the face indicates track be work and 
guidance icon be overlaid for each feature. 

Face Size Guidance – The direction of small radial arrow 
indicates whether the face should be make large (by move 
the phone closer) or small (by move the phone farther 
away) (e.g. Figure 8-a). The transparency of the arrow re- 
flects the difference between the current face size score and 
the high possible face size score. This mean the arrow be 
very opaque when the score be poor, so face size adjustment 
appear more strongly suggested. If the arrow disappear, the 
score be near optimal. 

Face Position Guidance – The direction of a large arrow ema- 
nating from the track circle indicates the direction to move 
the face. The direction be constrain to 8 cardinal compass 
direction for simplicity. Face movement be accomplish by 
slightly tilt the phone, for example, if the arrow point NW, 

then slightly tilt the phone SE move the face up (N) and 
left (W). Like face size, arrow transparency reflect the relative 
difference between the current face position score and the best 
possible score. 

Lighting Direction Guidance – Arrows be drawn around a sun 
icon locate in the top-left corner. Horizontal arrow indicate 
the phone should be orbit around the head to adjust side- 
light (azimuth, Θ) and vertical arrow indicate an up or 
down tilt to adjust polar-angle (Φ), In practice, these motion 
be accomplish by fix both the camera and the face and 
rotate or bending the body. For example, arrow point 
up and right indicate that the smartphone should be tilt up 
and orbit right to improve light (Figure 8-b). As before, 
the transparency of the arrow indicates how close the current 
light score be to the optimal score. 

Once the face be positioned, the picture be take by tap a 
circular shutter button (or press either volume button for 
convenience). A debug mode, activate by double-tapping, be 
use for test and demonstration. In this mode, the guidance 
visualization be augment with tracked position for the face, 
eyes, mouth, and nose, the estimate light direction vector, 
a well a the numeric score for all three features. 

Since guidance provide direction of improvement for each 
feature rather than a direct path to a single global maximum, 
the application indirectly handle selfies other than simple self- 
portraits. For example, when take a self-portrait in front of 
a tourist landmark like a statue or building, the photographer 
can ignore the position guidance to keep that object in frame, 
but still optimize the size and light of the face. 

Figure 8: Guidance user interface: (left) inward arrow suggest mak- 
ing the face small and arrow around sun icon suggest rotate up 
and right to improve lighting; (right) transparent inward arrow indi- 
cate face size be optimal, light be near optimal with some right rota- 
tion possible, and position can be slightly improve indicate by nearly 
transparent upward arrow at the top of face circle. 

EVALUATION 
We evaluate our app from two perspectives: usability and 
effectiveness at improve selfie photograph aesthetics. In 
our study, participant take selfie photo without and with our 
app in an indoor control setting. Usability be evaluate 
by examine log usage pattern and subjective ratings. 
Aesthetic effectiveness be evaluate by ask AMT worker 

Design Case Studies & Methods 2 (Technology) DIS 2017, June 10–14, 2017, Edinburgh, UK 

185 



to rate the best photo take by each participant without and 
with our app. 

Although Xu et al.’s [37] system be not design for self- 
ies, we consider use it a a baseline. Since it ran on a 
landscape-oriented “Ultrabook PC” with a 3-camera array, 
this would have require re-designing the visual guidance to 
work in portrait aspect ratio and create a simplify version 
suitable for a smartphone with a single-camera. This would 
have significantly alter the goal and fidelity of their system, 
but more importantly reduce the comparison to one between 
their rule-of-thirds model and our data-driven model. Xu et al. 
already discus how rule-of-thirds do not correlate with bet- 
ter photo aesthetic and our empirical result already provide 
further evidence of this. For these reasons, we use a standard 
camera app a our baseline. 

Participants 
We recruit 20 participant from a university campus (mean 
age 24.4, 11 female). Recruitment be limited to people who 
could view a smartphone screen without eye glass (since the 
eye detection algorithm be less reliable with dark-rimmed eye 
wear). Our participant exhibit visible diversity in term 
of skin pigment and facial features. Participants have a range 
of experience: 7 take selfies daily or weekly, 10 monthly or 
yearly, and 3 almost never. Only 1 participant have take a 
course in photography. 

Apparatus 
Participants use the smartphone app described above, con- 
figure to run without and with visible guidance. Regardless 
whether guidance be shown, the app ran the full composi- 
tional feature analysis and compute the score and direction 
of improvement. These be logged, even when invisible to 
the user. This ensure the refresh rate of the preview mode 
be the same regardless of guidance, and most importantly, 
provide quantitative data to test whether aesthetic rating (as 
determine by our models) be improve use guidance. A 
launch button provide a timestamp that together with a shut- 
ter button or volume button timestamp, enable calculation of 
picture-taking time. 

Instead of conduct the study outdoors with natural sunlight, 
we built an indoor studio so that light and background be 
control and consistent (Fig. 9-a). A 2.7 m wide roll of grey 
backdrop paper be hung from the ceiling like a curtain to 
create a circular space 4m in diameter. A bright light simulat- 
ing the sun be place high at one end and 5 additional bulb 
be space around the circle for ambient fill light. To keep 
participant at the centre of the circle, they sat on a swivel 
office chair throughout the study. Note this make light di- 
rection adjustment by swivel somewhat more convenient 
than turn once body while standing. 

Task 
The task be to take ten selfie photos. First, five without guid- 
ance (baseline condition) and then, five with guidance from 
our app (guided condition). The presentation order be fix 
due to strong carry-over effect if guidancewas perform first. 
After both part be completed, the participant select their 
best baseline photo and their best guide photo separately. 

Design and Protocol 
At the begin of the session, participant be ask to fo- 
cu on three compositional principles, face size, face position, 
and light direction, but the optimum configuration for those 
principle be not conveyed. In each part, they be told to 
take the five best photo they can and that they would pick 
the best one afterwards. To avoid other factor like smile af- 
fecting their choice or aesthetic quality, we specifically ask 
participant to use the same neutral expression in all photos. 

At the begin of the second part use the guide condition, 
participant be told they would now be use a camera 
application with guidance (they do not know this before). The 
meaning of the guidance icon be explain without any 
hint of what the best setting be for the three compositional 
principles. In fact, they be told they could follow or ignore 
the guidance to make the evaluation realistic — if they be 
told to always follow guidance, everyone would end with 
global optimum setting determine by the model. 

Finally, A post-experiment questionnaire gather subjective 
score for guide (only) regard Ease of Learning, Ease 
of Use, Accuracy of Guidance, Operation Speed, and Hand 
Fatigue. All score be on a continuous scale from 1 to 5, 
with 1 be bad and 5 be best. 

This be primarily a within subject design. Dependent mea- 
sures for usability be photo-taking time and compositional 
feature score for baseline and guided, a well a subjective 
score for guided. The study take 30 min on average. The de- 
pendent measure for aesthetic effectiveness be a score assign 
by AMT worker to the best baseline and guide photos. 

Results 
Given only two-level independent variables, and all dependent 
variable be continuous, interval, and ordinal, a t-test with .05 
critical value be use for statistical tests. 

Photo-Taking Time 
The photo-taking time be the duration between press the 
“launch” button until the moment the photo be taken. The aver- 
age time for take photo with baseline be 20 s (sd 22), sig- 
nificantly low than 33 s (sd 27) with guide (t(198) = −3.72, 
p < .001). These long time may be an effect of the study 
setting. Regardless, the relative difference indicates follow- 
ing guidance have a time cost. Perhaps because more time be 
spent consider composition option before take a photo. 
The average picture-taking time when participant take their 
single “best” photo support this theory: the average time to 
take the best baseline photo be 28 s (sd 28) compare to 36 
sec (sd 36) for guided. A significant difference t(38) = −0.821, 
p < .042, but small in magnitude. 

Subjective Ratings of User Experience 
Since participant rat user experience quality on a contin- 
uous scale from 1 to 5, we examine mean value (Table 1). 
All rating be positive, range between 3.7 and 4.3. Partic- 
ipant comment indicate that Ease of Use and Accuracy of 
Guidance be negatively affected by flicker arrow for 
light guidance. The score for Hand Fatigue be affected 
by the need to hold the arm still to read and follow guidance. 

Design Case Studies & Methods 2 (Technology) DIS 2017, June 10–14, 2017, Edinburgh, UK 

186 



(a) (b) 

“sun” light 

ambient light 
swivel chair 

backdrop 

4m 

Figure 9: Evaluation: (a) control environment use for study; (b) example of “best” photo pairs, left baseline, right guided. 

Ease of Learning 4.2 (±0.18) 
Ease of Use 4.0 (±0.19) 

Accuracy of Guidance 3.7 (±0.24) 
Operation Speed 4.3 (±0.21) 

Hand Fatigue 3.8 (±0.26) 
Table 1: Mean user experience rating (± SEM) for guide 

baseline guide t-test 
Size 0.170 (±0.027) 0.245 (±0.023) t(198)=-2.05, p<.05 

Position 0.714 (±0.037) 0.723 (±0.022) t(198)=-0.20, n.s. 
Lighting -0.019 (±0.012) 0.049 (±0.024) t(198)=-2.46, p<.02 

Table 2: Mean feature score (± SEM) for baseline and guided. 

Note that although participant only rat guided, there be an 
imply comparison to baseline, since guide be complete 
second and baseline represent a real-world baseline familiar 
to participants. 

Feature Scores 
We compare mean feature score for the best guide and 
baseline photo select by each participant. The score for 
face size and light direction be significantly high for 
guide (values and t-tests in Table 2). This indicates that our 
app be successful in guide participant to improve those 
two feature accord to our aesthetic models. This suggests 
that an increase in overall aesthetic quality, when the photo 
be rat in the follow section, may be attribute to our 
app provide useful guidance. Similar face position feature 
score be likely the result of people instinctively place their 
face near the centre of the frame. 

Aesthetic Effectiveness 
To measure aesthetic effectiveness, we recruit 100 AMT 
worker to rate each pair of best photo take by the partic- 
ipants. We do not specify any experience, age, or location 
criterion for the worker (though the majority live in the United 
States). Xu et al. [37] report rating by AMT and expert 
photographer follow the same pattern, and our focus be on 
aesthetic attractiveness to the general population. 

The task interface be model after the one use by Xu et al. 
A pair of photo be displayed with the best photo present 
randomly on the left or right. The worker select a rating 
between 0 and 100 for each photo use a slider. They be 
also instruct to consider the aesthetic quality of the face size, 
face position, and light direction, similar to the synthetic 
photo ranking described earlier. A text box be available for 
additional feedback. 

We remove five participant from the set to be rat because 
they use very different expression or pose in their pair of 
photos. For example, they be smile in one and frown 
in the other, or they turn their head to the side in one and 
look straight at the camera in the other. The concern be that 
these pair introduce a “pose and expression confound” that 
could alter a worker’s aesthetic judgement. In all, 15 pair of 
photo be rat by each worker, total 3,000 ratings. 

Results 
Table 3 summarizes mean ratings. Overall, selfie pho- 
tographs take with guide have significantly high ratings; 
t(2998) = 17.37, p < .0001. The average rating for guide be 
68.9 compare to 54.8 for baseline, an absolute difference 
of 14.1 — a 26% improvement in aesthetic quality. The low 
Standard Error of the Mean (SEM) and the relatively large 
absolute difference show this be a large and stable effect. The 
pair with the most improvement for guide (the “highest pair”) 
improve by 71% and the pair with the least improvement (the 
“lowest pair”) decrease by -1%. The SEMs across all pair 
be low, suggest consistency among workers. The SEM of 
2.4 for the “lowest pair” be among the high across all pairs, 
suggest this be the most difficult pair to judge. 

In their additional feedback, worker often cite a central face 
position and a face lit with even, bright light a reason for 
high ratings. Some worker comment that photo appear- 
ing “washed out” be less attractive, and this be sometimes 
cause by frontal light encourage by the guidance system. 
This could be remedied with more attention to the overall 

Design Case Studies & Methods 2 (Technology) DIS 2017, June 10–14, 2017, Edinburgh, UK 

187 



baseline guide Improvement 
All Pairs 54.8 (±0.6) 68.9 (±0.5) 26% 

Highest Pair 45.4 (±1.6) 77.6 (±2.0) 71% 
Lowest Pair 61.5 (±2.6) 61.1 (±2.4) -1% 

Table 3: Mean aesthetic rating (± SEM) for baseline and guided. “high- 
est” be the pair with great mean improvement, “lowest” be the pair 
with least mean improvement. 

light exposure of the face. Comments pertain to face 
size be less consistent: some prefer a large face, others 
a small one. This may be related to the two-peaked distri- 
bution for face size discover by the synthetic selfie ratings. 
Overall, worker feedback support our finding in the first 
rating experiment and provide evidence that worker be 
focus on compositional principle for their assessment. 

DISCUSSION 
Our result show that our methodology to build highly- 
control aesthetic models, our computer vision algorithms, 
and our guidance interface, combine into a system that be 
effective at help people take good selfies. 

Limited Aesthetic Style 
It be important to note that system encourages a symmetric 
and plain aesthetic style, no doubt due to the limited range 
of synthetic selfies use for aesthetic quality measurements. 
People also appreciate selfies with asymmetric compositions, 
dramatic lighting, or a tightly cropped face [19, 28]. Similarly, 
the neutral grey background may have affected how people 
rat the synthetic selfies. Generating and evaluate a wider 
range of synthetic selfies could diversify our system to guide 
people also to these more artistic type of selfies. 

Implications for Human-Computer Interaction 
Our aesthetic rating methodology relies on a fast, usable, and 
accurate interface for rating a large set of related images. We 
believe our solution for rating a synthetic dataset have applica- 
tions in other field such a rating option for visual design or 
ground truth dataset creation in machine learning. 

Our system relies heavily on effective guidance visualization 
package in a usable and responsive system. Using arrow 
for guidance be effective, but also make the camera preview 
more clutter and partially obscures the subject. An inter- 
esting future direction be to evaluate how subtle this style of 
feedback can be, while still be effective at communicate 
direction of improvement. 

Learning about Aesthetic Preferences from the Models 
When ponder what the model suggest, one may conclude 
that the recommendation be obvious: do not make the face 
too big or too small, put it near the top-centre, and make sure 
it be bright. Before passing judgement, recall that center the 
face contradicts the commonly reference rule-of-thirds and 
there be compete rules-of-thumb for light that suggest 
use side-lighting to create shadows. Moreover, stretch 
the arm to keep the size from be too large be not natural, 
but sometimes such awkward movement be necessary to 
produce a good quality photograph. After all, our goal be to 

discover, and empirically validate, what the optimum compo- 
sitional rule be for self-portrait photograph a determine 
by “normal people.” 

Technical Limitations 
Our system be restrict by computer vision capabilities. Face 
detection be constantly improving, but track be lose under 
extreme light or cropping. While our light estimation 
algorithm be successful at provide guidance, the light 
arrow sometimes flicker when the discretized model be at 
a threshold position. This creates oscillation between two 
different recommend directions. Adding anti-hysteresis 
method and increase the resolution of the light model 
(more score at intermediate angles) would correct this small 
issue. 

CONCLUSION 
Our work contributes a systematic assessment of three basic 
feature of selfie aesthetic use synthetic photograph and 
thousand of ratings. We transform these rating to create 
interpolation-based model to estimate the aesthetic score and 
direction of improvement for each feature. With the help of a 
computer vision algorithm to estimate the dominant light 
direction in a person’s face, we design a smartphone cam- 
era application to guide novice photographer to take good 
selfies. A control experiment validate the usability of the 
application and it capability to increase aesthetic quality. 

We see our three compositional feature a an initial test of 
our methodology and guidance interface. Other compositional 
feature like colour, texture, and balance could be included, 
a well a feature like head tilt, facial expression, and back- 
ground contrast. We think scale our model to more feature 
be possible. There be likely some inter-feature independence al- 
low some feature to be evaluate independently. If not, the 
same “pipeline” approach that we use for position and light- 
ing can be used: the result of one feature reduce the search 
space of the next. Note that although alternative machine- 
learn regression technique can handle high dimensional 
data, they result in a black box prevent u from learn 
about aesthetic base on the final models. 

Currently, if a photographer wish to take a selfie in front of 
a background object (such a a tourist landmark) they have 
the option to ignore some of our system’s guidance (such a 
position). An enhance system could recognize salient back- 
ground object and alter the guidance accordingly. Models 
which consider the compositional relationship between the 
face and the object could be empirically generate use the 
same methodology use here. Synthetic selfies could be gen- 
erated with a generic background object (perhaps a sphere) in 
different compositional position relative to the person and the 
photograph frame. Finally, our methodology could be apply 
to photo of two or more people, specific class of people 
(e.g babies, athletes), picture-taking setting (e.g. restaurant, 
beach), or entirely different subject (e.g. cars, landscapes). 

We hope our work demonstrates how a methodical and con- 
troll study of aesthetic can lead to usable and useful appli- 
cation which can increase aesthetic quality. 

Design Case Studies & Methods 2 (Technology) DIS 2017, June 10–14, 2017, Edinburgh, UK 

188 



ACKNOWLEDGMENTS 
This work be make possible by funding from NSERC Dis- 
covery Grant (#402467) and a Google Focused Award project 
on context-aware mobile computing. 

REFERENCES 
1. ArtInCam LTD. 2014. Camera51. (2014). 
https://www.camera51.com 

2. Subhabrata Bhattacharya, Rahul Sukthankar, and 
Mubarak Shah. 2010. A Framework for Photo-quality 
Assessment and Enhancement Based on Visual 
Aesthetics. In Proceedings of the International 
Conference on Multimedia (MM ’10). ACM, New York, 
NY, USA, 271–280. DOI: 
http://dx.doi.org/10.1145/1873951.1873990 

3. Stephen A. Brewster and Jody Johnston. 2008. 
Multimodal Interfaces for Camera Phones. In 
Proceedings of the 10th International Conference on 
Human Computer Interaction with Mobile Devices and 
Services (MobileHCI ’08). ACM, New York, NY, USA, 
387–390. DOI: 
http://dx.doi.org/10.1145/1409240.1409295 

4. Scott Carter, John Adcock, John Doherty, and Stacy 
Branham. 2010. NudgeCam: Toward Targeted, Higher 
Quality Media Capture. In Proceedings of the 18th ACM 
International Conference on Multimedia (MM ’10). 
ACM, New York, NY, USA, 615–618. DOI: 
http://dx.doi.org/10.1145/1873951.1874034 

5. Géry Casiez, Nicolas Roussel, and Daniel Vogel. 2012. 
1€ filter: a simple speed-based low-pass filter for noisy 
input in interactive systems. In Proceedings of the 
SIGCHI Conference on Human Factors in Computing 
Systems. ACM, 2527–2530. 

6. Xiaowu Chen, Xin Jin, Hongyu Wu, and Qinping Zhao. 
2015. Learning template for artistic portrait light 
analysis. IEEE Transactions on Image Processing 24, 2 
(2015), 608–618. 

7. John Child. 2008. Studio Photography: Essential Skills. 
Focal Press. 

8. Casio Computer Co. 2016. Life Style Digital Cameras. 
(2016). 
http://www.casio-intl.com/asia-mea/en/dc/lineup/ 

Hardware: Digital Cameras. 

9. Ritendra Datta, Dhiraj Joshi, Jia Li, and JamesZ. Wang. 
2006. Studying Aesthetics in Photographic Images Using 
a Computational Approach. In Computer Vision – ECCV 
2006, Aleš Leonardis, Horst Bischof, and Axel Pinz 
(Eds.). Lecture Notes in Computer Science, Vol. 3953. 
Springer Berlin Heidelberg, 288–301. DOI: 
http://dx.doi.org/10.1007/11744078_23 

10. S. Dhar, V. Ordonez, and T.L. Berg. 2011. High level 
describable attribute for predict aesthetic and 
interestingness. In Computer Vision and Pattern 
Recognition (CVPR), 2011 IEEE Conference on. 
1657–1664. DOI: 
http://dx.doi.org/10.1109/CVPR.2011.5995467 

11. Christina Dickson. 2009. 6 Tips for Perfect Composition 
in Portrait Photography. (Sept. 2009). 
http://digital-photography-school.com/ 

6-tips-for-perfect-composition-in-portrait-photography/ 

12. Felix Esser. 2014. Camera51 Android app make you a 
master photographer by guide your framing. (Oct. 
2014). http://www.digitaltrends.com/photography/camera51-android- 
app-makes-master-photographer-guiding-framing/ 

13. Ohad Fried, Eli Shechtman, Dan B. Goldman, and Adam 
Finkelstein. 2016. Perspective-aware Manipulation of 
Portrait Photos. ACM Trans. Graph. 35, 4, Article 128 
(July 2016), 10 pages. DOI: 
http://dx.doi.org/10.1145/2897824.2925933 

14. Dmitry O Gorodnichy and Gerhard Roth. 2004. Nouse 
‘use your nose a a mouse’perceptual vision technology 
for hands-free game and interfaces. Image and Vision 
Computing 22, 12 (2004), 931–942. 

15. Melanie Hall. 2013. Family album fade a the young put 
only themselves in picture. (June 2013). 
http://www.telegraph.co.uk/technology/ 

16. Darlene Hildebrandt. 2013. Lighting Ratios to Make or 
Break your Portrait. (2013). 
http://digital-photography-school.com/ 

lighting-ratios-to-make-or-break-your-portrait/ 

17. Bill Hurter. 2007. Portrait Photographer’s Handbook (3 
ed.). Amherst Media. 

18. Wei Jiang, A.C. Loui, and C.D. Cerosaletti. 2010. 
Automatic aesthetic value assessment in photographic 
images. In Multimedia and Expo (ICME), 2010 IEEE 
International Conference on. 920–925. DOI: 
http://dx.doi.org/10.1109/ICME.2010.5582588 

19. Mahdi M Kalayeh, Misrak Seifu, Wesna LaLanne, and 
Mubarak Shah. 2015. How to take a good selfie?. In 
Proceedings of the 23rd ACM international conference on 
Multimedia. ACM, 923–926. 

20. Shehroz S. Khan and Daniel Vogel. 2012. Evaluating 
Visual Aesthetics in Photographic Portraiture. In 
Proceedings of the Eighth Annual Symposium on 
Computational Aesthetics in Graphics, Visualization, and 
Imaging (CAe ’12). Eurographics Association, 
Aire-la-Ville, Switzerland, Switzerland, 55–62. 
http://dl.acm.org/citation.cfm?id=2328888.2328898 

21. Masahito Kumano, Kuniaki Uehara, and Yasuo Ariki. 
2006. Online training-oriented video shoot navigation 
system base on real-time camerawork evaluation. In 
Multimedia and Expo, 2006 IEEE International 
Conference on. IEEE, 1281–1284. 

22. Congcong Li and Tsuhan Chen. 2009. Aesthetic Visual 
Quality Assessment of Paintings. Selected Topics in 
Signal Processing, IEEE Journal of 3, 2 (April 2009), 
236–252. DOI: 
http://dx.doi.org/10.1109/JSTSP.2009.2015077 

Design Case Studies & Methods 2 (Technology) DIS 2017, June 10–14, 2017, Edinburgh, UK 

189 

https://www.camera51.com 
http://dx.doi.org/10.1145/1873951.1873990 
http://dx.doi.org/10.1145/1409240.1409295 
http://dx.doi.org/10.1145/1873951.1874034 
http://www.casio-intl.com/asia-mea/en/dc/lineup/ 
http://dx.doi.org/10.1007/11744078_23 
http://dx.doi.org/10.1109/CVPR.2011.5995467 
http://digital-photography-school.com/6-tips-for-perfect-composition-in-portrait-photography/ 
http://digital-photography-school.com/6-tips-for-perfect-composition-in-portrait-photography/ 
http://www.digitaltrends.com/photography/camera51-android-app-makes-master-photographer-guiding-framing/ 
http://www.digitaltrends.com/photography/camera51-android-app-makes-master-photographer-guiding-framing/ 
http://dx.doi.org/10.1145/2897824.2925933 
http://www.telegraph.co.uk/technology/ 
http://digital-photography-school.com/lighting-ratios-to-make-or-break-your-portrait/ 
http://digital-photography-school.com/lighting-ratios-to-make-or-break-your-portrait/ 
http://dx.doi.org/10.1109/ICME.2010.5582588 
http://dl.acm.org/citation.cfm?id=2328888.2328898 
http://dx.doi.org/10.1109/JSTSP.2009.2015077 


23. LuMee LLC. 2016a. LuMee Phone Case. (2016). 
https://lumee.com/collections/all [Hardware: Phone 
Accessory]. 

24. Ty-Lite LLC. 2016b. Ty-Lite Phone Case. (2016). 
https://ty-lite.com [Hardware: Phone Accessory]. 

25. Wei Luo, Xiaogang Wang, and Xiaoou Tang. 2011. 
Content-based photo quality assessment. In Computer 
Vision (ICCV), 2011 IEEE International Conference on. 
2206–2213. DOI: 
http://dx.doi.org/10.1109/ICCV.2011.6126498 

26. Shuang Ma, Yangyu Fan, and Chang Wen Chen. 2014. 
Finding your spot: A photography suggestion system for 
place human in the scene. In 2014 IEEE International 
Conference on Image Processing (ICIP). 556–560. DOI: 
http://dx.doi.org/10.1109/ICIP.2014.7025111 

27. M. Males, A. Hedi, and M. Grgic. 2013. Aesthetic quality 
assessment of headshots. In ELMAR, 2013 55th 
International Symposium. 89–92. 

28. Lev Manovich. 2014. SelfieCity. (2014). 
http://selfiecity.net 

29. Filippo Mazza, Matthieu Perreira Da Silva, Patrick 
Le Callet, and IEJ Heynderickx. 2015. What do you think 
of my picture? Investigating factor of influence in profile 
image context perception. In IS&T/SPIE Electronic 
Imaging. International Society for Optics and Photonics, 
93940D. 

30. Christopher McAdam, Craig Pinkerton, and Stephen A. 
Brewster. 2010. Novel Interfaces for Digital Cameras and 
Camera Phones. In Proceedings of the 12th International 
Conference on Human Computer Interaction with Mobile 
Devices and Services (MobileHCI ’10). ACM, New York, 
NY, USA, 143–152. DOI: 
http://dx.doi.org/10.1145/1851600.1851625 

31. P. Obrador, L. Schmidt-Hackenberg, and N. Oliver. 2010. 
The role of image composition in image aesthetics. In 
Image Processing (ICIP), 2010 17th IEEE International 

Conference on. 3185–3188. DOI: 
http://dx.doi.org/10.1109/ICIP.2010.5654231 

32. Bahareh Rahmanian and Joseph G Davis. 2014. User 
interface design for crowdsourcing systems. In 
Proceedings of the 2014 International Working 
Conference on Advanced Visual Interfaces. ACM, 
405–408. 

33. Miriam Redi, Nikhil Rasiwasia, Gaurav Aggarwal, and 
Alejandro Jaimes. 2015. The Beauty of Capturing Faces: 
Rating the Quality of Digital Portraits. CoRR 
abs/1501.07304 (2015). http://arxiv.org/abs/1501.07304 

34. Patrick Rice. 2005. Professional Techniques for Black & 
White Digital Photography. Amherst Media, Buffalo, NY. 

35. Jose San pedro and Karen Church. 2013. The Sound of 
Light: Induced Synesthesia for Augmenting the 
Photography Experience. In CHI ’13 Extended Abstracts 
on Human Factors in Computing Systems (CHI EA ’13). 
ACM, New York, NY, USA, 745–750. DOI: 
http://dx.doi.org/10.1145/2468356.2468489 

36. Lai-Kuan Wong and Kok-Lim Low. 2009. 
Saliency-enhanced image aesthetic class prediction. In 
Image Processing (ICIP), 2009 16th IEEE International 
Conference on. 997–1000. DOI: 
http://dx.doi.org/10.1109/ICIP.2009.5413825 

37. Yan Xu, Joshua Ratcliff, James Scovell, Gheric Speiginer, 
and Ronald Azuma. 2015. Real-time Guidance Camera 
Interface to Enhance Photo Aesthetic Quality. In 
Proceedings of the 33rd Annual ACM Conference on 
Human Factors in Computing Systems (CHI ’15). ACM, 
New York, NY, USA, 1183–1186. DOI: 
http://dx.doi.org/10.1145/2702123.2702418 

38. Lei Yao, Poonam Suryanarayan, Mu Qiao, James Z 
Wang, and Jia Li. 2012. Oscar: On-site composition and 
aesthetic feedback through exemplar for photographers. 
International Journal of Computer Vision 96, 3 (2012), 
353–383. 

Design Case Studies & Methods 2 (Technology) DIS 2017, June 10–14, 2017, Edinburgh, UK 

190 

https://lumee.com/collections/all 
https://ty-lite.com 
http://dx.doi.org/10.1109/ICCV.2011.6126498 
http://dx.doi.org/10.1109/ICIP.2014.7025111 
http://selfiecity.net 
http://dx.doi.org/10.1145/1851600.1851625 
http://dx.doi.org/10.1109/ICIP.2010.5654231 
http://arxiv.org/abs/1501.07304 
http://dx.doi.org/10.1145/2468356.2468489 
http://dx.doi.org/10.1109/ICIP.2009.5413825 
http://dx.doi.org/10.1145/2702123.2702418 

Introduction 
Background and Related Work 
Computational Aesthetics 
Realtime Aesthetic Guidance 

Dataset and Aesthetic Quality Ratings 
3D Rendered Synthetic Selfie Dataset 
Face Size 
Face Position 
Lighting Direction 

Aesthetic Rating 
Participants 
Task and Implementation 
Design 

Results 
Face Size 
Face Position 
Lighting Direction 
Summary 


Aesthetic Models 
Smartphone Application 
Feature Detection and Calculation use Computer Vision 
Face Size (r) and Face Position (x, y) 
Lighting Direction (u, v) 

User Interface Design 

Evaluation 
Participants 
Apparatus 
Task 
Design and Protocol 

Results 
Photo-Taking Time 
Subjective Ratings of User Experience 
Feature Scores 

Aesthetic Effectiveness 
Results 


Discussion 
Limited Aesthetic Style 
Implications for Human-Computer Interaction 
Learning about Aesthetic Preferences from the Models 
Technical Limitations 


Conclusion 
Acknowledgments 
REFERENCES 



