


















































Learning Cross-modal Embeddings for Cooking Recipes and Food Images 

Amaia Salvador1∗ Nicholas Hynes2∗ Yusuf Aytar2 Javier Marin2 Ferda Ofli3 

Ingmar Weber3 Antonio Torralba2 
1Universitat Politècnica de Catalunya 2Massachusetts Institute of Technology 

3Qatar Computing Research Institute, HBKU 
amaia.salvador@upc.edu, nhynes@mit.edu, {yusuf,jmarin,torralba}@csail.mit.edu, {fofli,iweber}@qf.org.qa 

Abstract 

In this paper, we introduce Recipe1M, a new large-scale, 
structure corpus of over 1m cooking recipe and 800k food 
images. As the large publicly available collection of recipe 
data, Recipe1M affords the ability to train high-capacity 
model on aligned, multi-modal data. Using these data, we 
train a neural network to find a joint embed of recipe 
and image that yield impressive result on an image-recipe 
retrieval task. Additionally, we demonstrate that regulariza- 
tion via the addition of a high-level classification objective 
both improves retrieval performance to rival that of human 
and enables semantic vector arithmetic. We postulate that 
these embeddings will provide a basis for further exploration 
of the Recipe1M dataset and food and cooking in general. 
Code, data and model be publicly available1. 

1. Introduction 

There be few thing so fundamental to the human expe- 
rience a food. Its consumption be intricately link to our 
health, our feeling and our culture. Even migrant start 
a new life in a foreign country often hold on to their ethnic 
food longer than to their native language. Vital a it be to 
our lives, food also offer new perspective on topical chal- 
lenges in computer vision like find representation that 
be robust to occlusion and deformation (as occur during 
ingredient processing). 

The profusion of online recipe collection with user- 
submit photo present the possibility of training ma- 
chine to automatically understand food preparation by 
jointly analyze ingredient lists, cooking instruction and 
food images. Far beyond application solely in the realm of 
culinary arts, such a tool may also be apply to the plethora 
of food image share on social medium to achieve insight 
into the significance of food and it preparation on public 

∗contributed equally. 
1http://im2recipe.csail.mit.edu 

Figure 1: Learning cross-modal embeddings from recipe- 
image pair collect from online resources. These enable 
u to achieve in-depth understand of food from it ingre- 
dients to it preparation. 

health [4] and cultural heritage [14]. Developing a tool for 
automate analysis require large and well-curated datasets. 

The emergence of massive label datasets [19, 26] and 
deeply-learned representation [10, 20, 5] have redefine the 
state-of-the-art in object recognition and scene classification. 
Moreover, the same technique have enable progress in 
new domain like dense label and image segmentation. 
Perhaps the introduction of a new large-scale food dataset– 
complete with it own intrinsic challenges–will yield a simi- 
lar advancement of the field. For instance, categorize an 
ingredient’s state (e.g., sliced, diced, raw, baked, grilled, or 
boiled) provide a unique challenge in attribute recognition– 
one that be not well pose by exist datasets. Furthermore, 
the free-form nature of food suggests a departure from the 
concrete task of classification in favor of a more nuanced 
objective that integrates variation in a recipe’s structure. 

Existing work, however, have focus largely on the use 
of medium-scale datasets for perform categorization 
[1, 8, 16, 13]. For instance, Bossard et al. [1] introduce the 
Food-101 visual classification dataset and set a baseline of 
50.8% accuracy. Even with the impetus for food image cat- 
egorization, subsequent work by [13], [16] and [17] could 
only improve this result to 77.4%, 79% and 80.9%, respec- 

1 

http://im2recipe.csail.mit.edu 


Figure 2: Dataset statistics. Prevalence of course category and number of instruction and ingredient per recipe. 

Partition # Recipes # Images 

Training 720,639 619,508 
Validation 155,036 133,860 
Test 154,045 134,338 

Total 1,029,720 887,706 

Table 1: Recipe1M dataset. Number of sample in training, 
validation and test sets. 

tively, which indicates that the size of the dataset may be 
the limit factor. Although Myers et al. [16] build upon 
Food-101 to tackle the novel challenge of estimate a meal’s 
energy content, the segmentation and depth information use 
in their work be not make available for further exploration. 

In this work, we address data limitation by introduce 
the large-scale Recipe1M dataset which contains one million 
structure cooking recipe and their images. Additionally, to 
demonstrate it utility, we present the im2recipe retrieval task 
which leverage the full dataset–images and text–to solve 
the practical and socially relevant problem of demystify 
the creation of a dish that can be see but not necessarily de- 
scribed. To this end, we have developed a multi-modal neural 
model which jointly learns to embed image and recipe in 
a common space which be semantically regularize by the 
addition of a high-level classification task. The performance 
of the result embeddings be thoroughly evaluate against 
baseline and humans, show remarkable improvement 
over the former while fare comparably to the latter. With 
the release of Recipe1M, we hope to spur advancement on 
not only the im2recipe task but also heretofore unimagined 
objective which require a deep understand of the domain 
and it modalities. 

2. Dataset 

Given the relevance of understand recipes, it be surpris- 
ing that there be not a large body of work on the topic. We 
estimate that this be due to the absence of a large, general 
collection of recipe data. To our knowledge, virtually all of 
the readily available food-related datasets either contain only 

categorize image [16, 1, 8, 24] or simply recipe text [11]. 
Only recently have a few datasets be release that include 
both recipe and images. The first of which [23] have 101k 
image divide equally among 101 categories; the recipe 
for each be however raw HTML. In a late work, Chen and 
Ngo [6] present a dataset contain 110,241 image anno- 
tat with 353 ingredient label and 65,284 recipes, each 
with a brief introduction, ingredient list, and preparation 
instructions. Of note be that the dataset only contains recipe 
for Chinese cuisine. 

Although the aforementioned datasets constitute a large 
step towards learn richer recipe representations, they be 
still limited in either generality or size. As the ability to 
learn effective representation be largely a function of the 
quantity and quality of the available data, we create and 
release publicly a new, large-scale corpus of structure recipe 
data that include over 1m recipe and 800k images. In 
comparison to the current large dataset in this domain, 
Recipe1M include twice a many recipe a [11] and eight 
time a many image a [6]. In the follow subsection 
we outline how the dataset be collect and organize and 
provide an analysis of it contents. 

2.1. Data Collection 

The recipe be scrap from over two dozen popu- 
lar cooking website and process through a pipeline that 
extract relevant text from the raw HTML, download 
link images, and assemble the data into a compact JSON 
schema in which each datum be uniquely identified. As 
part of the extraction process, excessive whitespace, HTML 
entities, and non-ASCII character be remove from the 
recipe text. 

2.2. Data Structure 

The content of the Recipe1M dataset may logically be 
grouped into two layers. The first contains basic information 
include title, a list of ingredients, and a sequence of instruc- 
tions for prepare the dish; all of these data be provide a 
free text. The second layer build upon the first and include 
any image with which the recipe be associated–these be 
provide a RGB in JPEG format. Additionally, a subset of 



recipe be annotate with course label (e.g., appetizer, side 
dish, dessert), the prevalence of which be summarize in 
Figure 2. 

2.3. Analysis 

The average recipe in the dataset consists of nine ingre- 
dients which be transform over the course of ten instruc- 
tions. Approximately half of the recipe have image which, 
due to the nature of the data sources, depict the fully pre- 
par dish. Recipe1M include approximately 0.4% dupli- 
cate recipe and 2% duplicate image (different recipe may 
share same image). Excluding those 0.4% recipes, 20% 
of recipe have non-unique title but symmetrically differ 
by a median of 16 ingredients. 0.2% of recipe share the 
same ingredient but be relatively simple (e.g., spaghetti, 
granola), have a median of six ingredients. Regarding 
our experiments, we carefully remove any exact duplicate 
or recipe share the same image in order to avoid over- 
lap between training and test subsets. As detailed in 
Table 1, around 70% of the data be label a training, and 
the remainder be split equally between the validation and test 
sets. 

In Figure 2, one can easily observe that the distribution of 
data be heavy tailed. For instance, of the 16k unique ingredi- 
ents that have be identified, only 4,000 account for 95% of 
occurrences. At the low end of instruction count–particularly 
those with one step–one will find the dread Combine all 
ingredients. At the other end be lengthy recipe and ingredi- 
ent list associate with recipe that include sub-recipes. A 
similar issue of outlier exists also for images: a several of 
the include recipe collection curate user-submitted images, 
popular recipe like chocolate chip cooky have order of 
magnitude more image than the average. Notably, 25% of 
image be associate with 1% of recipe while half of all 
image belong to 10% of recipes; the size of the second layer 
in number of unique recipe be 333k. 

3. Learning Embeddings 
In this section we introduce our neural joint embed 

model. Here we utilize the pair (recipe and image) data 
in order to learn a common embed space a sketch in 
Figure 1. Next, we discus recipe and image representation 
and then we introduce our neural joint embed model 
that build upon recipe and image representations. 

3.1. Representation of recipe 

There be two major component of a recipe: it ingredi- 
ents and cooking instructions. We develop a suitable repre- 
sentation for each of these components. 

Ingredients. Each recipe contains a set of ingredient text a 
show in Figure 1. For each ingredient we learn an ingre- 
dient level word2vec [15] representation. In order to do so, 

the actual ingredient name be extract from each ingre- 
dient text. For instance in “2 tbsp of olive oil” the olive oil 
be extract a the ingredient name and treat a a single 
word for word2vec computation. The initial ingredient name 
extraction task be solve by a bi-directional LSTM that per- 
form logistic regression on each word in the ingredient text. 
Training be perform on a subset of our training set for 
which we have the annotation for actual ingredient names. 
Ingredient name extraction module work with 99.5% accu- 
racy test on a held-out set. 

Cooking Instructions. Each recipe also have a list of cooking 
instructions. As the instruction be quite lengthy (averaging 
208 words) a single LSTM be not well suit to their rep- 
resentation a gradient be diminish over the many time 
steps. Instead we propose a two-stage LSTM model which 
be design to encode a sequence of sequences. First, each 
instruction/sentence be represent a a skip-instructions vec- 
tor and then an LSTM be train over the sequence of these 
vector to obtain the representation of all instructions. The 
result fixed-length representation be fed into to our joint 
embed model (see instructions-encoder in Figure 3). 

Skip-instructions. Our cooking instruction representation, 
refer a skip-instructions, be the product of a sequence- 
to-sequence model [21]. Specifically, we build upon the 
technique of skip-thoughts [9] which encodes a sentence 
and us that encode a context when decoding/predicting 
the previous and next sentences. Our modification to this 
method include add start- and end-of-recipe “instructions” 
and use an LSTM instead of a GRU. In either case, the 
representation of a single instruction be the final output of 
the encoder. As before, this be use a the instruction input 
to our embed model. 

3.2. Representation of food image 

For the image representation we adopt two major state-of- 
the-art deep convolutional networks, namely VGG-16 [20] 
and Resnet-50 [5] models. In particular, the deep resid- 
ual network have a proven record of success on a variety 
of benchmark [5]. Although [20] suggests training very 
deep network with small convolutional filters, deep residual 
network take it to another level use ubiquitous identity 
mapping that enable training of much deeper architecture 
(e.g., with 50, 101, 152 layers) with good performance. We 
incorporate these model by remove the last softmax classi- 
fication layer and connect the rest to our joint embed 
model a show in the right side of Figure 3. 

4. Joint Neural Embedding 
Building upon the previously described recipe and im- 

age representations, we now introduce our joint embed 
method. The recipe model, displayed in Figure 3, include 
two encoders: one for ingredient and one for instructions, 



Figure 3: Joint neural embed model with semantic regularization. Our model learns a joint embed space for food 
image and cooking recipes. 

the combination of which be design to learn a recipe 
level representation. The ingredient encoder combine the 
sequence of ingredient word vectors. Since the ingredient 
list be an unordered set, we choose to utilize a bidirectional 
LSTM model, which considers both forward and backward 
orderings. The instruction encoder be implement a a 
forward LSTM model over skip-instructions vectors. The 
output of both encoders be concatenate and embed 
into a recipe-image joint space. The image representation 
be simply project into this space through a linear transfor- 
mation. The goal be to learn transformation to make the 
embeddings for a give recipe-image pair “close.” 

Formally, assume that we be give a set of the recipe- 
image pairs, (Rk, vk) in which Rk be the kth recipe 
and vk be the associate image. Further, let Rk = 
({stk} 

nk 
t=1, {gtk} 

mk 
t=1, vk), where {stk} 

nk 
t=1 be the sequence of 

nk cooking instructions, {gtk} 
mk 
t=1 be the sequence of mk in- 

gredient tokens. The objective be to maximize the cosine 
similarity between positive recipe-image pairs, and mini- 
mize it between all non-matching recipe-image pairs, up to 
a specify margin. 

The ingredient encoder be implement use a bi- 
directional LSTM: at each time step it take two ingredient- 
word2vec representation of gtk and g 

m−t+1 
k , and eventually 

it produce the fixed-length representation hgk for ingredi- 
ents. The instruction encoder be implement through a 
regular LSTM. At each time step it receives an instruction 
representation from the skip-instructions encoder, and finally 
it produce the fixed-length representation hsk. h 

g 
k and h 

s 
k 

be concatenate in order to obtain the recipe representation 
hRk . Then the recipe and image representation be mapped 
into the joint embed space as: φR =WRhRk + b 

R and 
φv =W vvk+ b 

v , respectively. WR and W v be embed 

matrix which be also learned. Finally the complete model 
be train end-to-end with positive and negative recipe-image 
pair (φR, φv) use the cosine similarity loss with margin 
define a follows: 

Lcos((φ 
R, φv), y) = 

{ 
1 − cos(φR, φv), if y = 1 
max(0, cos(φR, φv) − α), if y = −1 

where cos(.) be the normalize cosine similarity and α be the 
margin. 

5. Semantic Regularization 
We incorporate additional regularization on our embed- 

ding through solve the same high-level classification prob- 
lem in multiple modality with share high-level weights. 
We refer to this method a semantic regularization. The key 
idea be that if high-level discriminative weight be shared, 
then both of the modality (recipe and image embeddings) 
should utilize these weight in a similar way which brings 
another level of alignment base on discrimination. We 
optimize this objective together with our joint embed 
loss. Essentially the model also learns to classify any image 
or recipe embed into one of the food-related semantic 
categories. We limit the effect of semantic regularization a 
it be not the main problem that we aim to solve. 

Semantic Categories. We start by assign Food-101 cate- 
gories to those recipe that contain them in their title. How- 
ever, after this procedure we be only able to annotate 13% 
of our dataset, which we argue be not enough label data 
for a good regularization. Hence, we compose a large set of 
semantic category purely extract from recipe titles. We 
first obtain the top 2,000 most frequent bigram in recipe 
title from our training set. We manually remove those that 



im2recipe recipe2im 

medR R@1 R@5 R@10 medR R@1 R@5 R@10 

random rank 500 0.001 0.005 0.01 500 0.001 0.005 0.01 
CCA w/ skip-thoughts + word2vec (GoogleNews) + image feature 25.2 0.11 0.26 0.35 37.0 0.07 0.20 0.29 
CCA w/ skip-instructions + ingredient word2vec + image feature 15.7 0.14 0.32 0.43 24.8 0.09 0.24 0.35 

joint emb. only 7.2 0.20 0.45 0.58 6.9 0.20 0.46 0.58 
joint emb. + semantic 5.2 0.24 0.51 0.65 5.1 0.25 0.52 0.65 

Table 2: im2recipe retrieval comparisons. Median rank and recall rate at top K be report for baseline and our method. 
Note that the joint neural embed model consistently outperform all the baseline methods. 

Joint emb. method im2recipe recipe2im 

medR-1K medR-5K medR-10K medR-1K medR-5K medR-10K 

VGG-16 
fix vision 15.3 71.8 143.6 16.4 76.8 152.8 
finetuning (ft) 12.1 56.1 111.4 10.5 51.0 101.4 
ft + semantic reg. 8.2 36.4 72.4 7.3 33.4 64.9 

ResNet-50 
fix vision 7.9 35.7 71.2 9.3 41.9 83.1 
finetuning (ft) 7.2 31.5 62.8 6.9 29.8 58.8 
ft + semantic reg. 5.2 21.2 41.9 5.1 20.2 39.2 

Table 3: Ablation studies. Effect of the different model component to the median rank (the low be better). 

contain unwanted character (e.g., n’, !, ? or &) and those 
that do not have discriminative food property (e.g., best 
pizza, super easy or 5 minutes). We then assign each of the 
remain bigram a the semantic category to all recipe 
that include it in their title. By use bigram and Food-101 
category together we obtain a total of 1,047 categories, 
which cover 50% of the dataset. chicken salad, grill veg- 
etable, chocolate cake and fry fish be some example 
among the category we collect use this procedure. All 
those recipe without a semantic category be assign to an 
additional background class. Although there be some overlap 
in the generate categories, 73% of the recipe in our dataset 
(excluding those in the background class) belong to a single 
category (i.e., only one of the generate class appear in 
their title). For recipe where two or more category appear 
in the title, the category with high frequency rate in the 
dataset be chosen. 

Classification. To incorporate semantic regularization to the 
joint embed we use a single fully connect layer. Given 
the embeddings φv and φr, class probability be obtain 
with pr = W cφr and pv = W cφv follow by a softmax 
activation. W c be the matrix of learn weights, which be 
share between image and recipe embeddings to promote 
semantic alignment between them. Formally, we express the 
semantic regularization loss a Lreg(φr, φv, cr, cv) where 
cr,cv be the semantic category label for recipe and image, 
respectively. Note that cr and cv be the same if (φr, φv) be 
a positive pair. Then we can write the final objective as: 

L(φr, φv, cr, cv, y) = Lcos((φ 
r, φv), y)+ 

λLreg(φ 
r, φv, cr, cv) 

Optimization. We follow a two-stage optimization proce- 
dure while learn the model. If we update both the recipe 
encode and image network at the same time, optimiza- 
tion becomes oscillatory and even divergent. Previous work 
on cross-modality training [2] suggests training model for 
different modality separately and fine tune them jointly 
afterwards to allow alignment. Following this insight, we 
adopt a similar procedure when training our model. We 
first fix the weight of the image network, which be found 
from pre-training on the ImageNet object classification task, 
and learn the recipe encodings. This way the recipe net- 
work learns to align itself to the image representation and 
also learns semantic regularization parameter (W c). Then 
we freeze the recipe encode and semantic regularization 
weights, and learn the image network. This two-stage pro- 
ce be crucial for successful optimization of the objective 
function. After this initial alignment stage, we release all the 
weight to be learned. However, the result do not change 
much in this final, joint optimization. 
Implementation Details All the neural model be imple- 
mented use the Torch7 framework2. The margin α be 

2http://torch.ch/ 

http://torch.ch/ 


Figure 4: Retrieval examples. From left to right: (1) the 
query image, (2) it associate ingredient list, (3) the re- 
trieved ingredient and (4) the image associate to the re- 
trieved recipe. 

select a 0.1 in joint neural embed models. The reg- 
ularization hyperparameter be set a λ = 0.02 in all our 
experiments. While optimize the cosine loss we pick a 
positive recipe-image pair with 20% probability and a ran- 
dom negative recipe-image pair with 80% probability from 
the training set. The model be train on 4 NVIDIA Titan 
X with 12GB of memory for three days. 

6. Experiments 
We begin with the evaluation of our learn embeddings 

for the im2recipe retrieval task. We then study the effect of 
each component of our model and compare our final system 
against human performance. We also analyze the property 
of our learn embeddings through unit visualization and 
vector arithmetic in the embed space. 

6.1. im2recipe retrieval 

We evaluate all the recipe representation for im2recipe 
retrieval. Given a food image, the task be to retrieve it recipe 
from a collection of test recipes. We also perform recipe2im 
retrieval use the same setting. All result be report for 
the test set. 
Comparison with the baselines. Canonical Correlation 
Analysis (CCA) be one of the strong statistical model for 
learn joint embeddings for different feature space when 
pair data be provided. We use CCA over many high-level 
recipe and image representation a our baseline. These 
CCA embeddings be learn use recipe-image pair from 
the training data. In each recipe, the ingredient be repre- 

sented with the mean word2vec across all it ingredient in 
the manner of [12]. The cooking instruction be represent 
with mean skip-thoughts vector [9] across the cooking in- 
structions. A recipe be then represent a concatenation of 
these two features. We also evaluate CCA over mean in- 
gredient word2vec and skip-instructions feature a another 
baseline. The image feature utilized in the CCA baseline 
be the ResNet-50 feature before the softmax layer. Al- 
though they be learn for visual object categorization task 
on ImageNet dataset, these feature be widely adopt by 
the computer vision community, and they have be show 
to generalize well to different visual recognition task [3]. 

For evaluation, give a test query image, we use cosine 
similarity in the common space for rank the relevant 
recipe and perform im2recipe retrieval. The recipe2im 
retrieval set be evaluate likewise. We adopt the test 
procedure from image2caption retrieval task [7, 22]. We 
report result on a subset of randomly select 1,000 recipe- 
image pair from the test set. We repeat the experiment 
10 time and report the mean results. We report median 
rank (MedR), and recall rate at top K (R@K) for all the 
retrieval experiments. To clarify, R@5 in the im2recipe task 
represent the percentage of all the image query where the 
correspond recipe be retrieve in the top 5, hence high 
be better. The quantitative result for im2recipe retrieval be 
show in Table 2. 

Our model greatly outperforms the CCA baseline in all 
measures. As expected, CCA over ingredient word2vec and 
skip-instructions perform good than CCA over word2vec 
train on GoogleNews [15] and skip-thoughts vector that 
be learn over a large-scale book corpus [9]. In 65% of all 
evaluate queries, our method can retrieve the correct recipe 
give a food image. The semantic regularization notably 
improves the quality of our embed for im2recipe task 
which be quantify with the medR drop from 7.2 to 5.2 in Ta- 
ble 2. The result for recipe2im task be also similar to those 
in the im2recipe retrieval setting. Figure 4 compare the 
ingredient from the original recipe (true recipes) with the 
retrieve recipe (coupled with their correspond image) 
for different image queries. As can be observe in Figure 4, 
our embeddings generalize well and allow overall satisfac- 
tory recipe retrieval results. However, at the ingredient level, 
one can find that in some case our model retrieves recipe 
with miss ingredients. This usually occurs due to the lack 
of fine-grained feature (e.g., confusion between shrimp 
and salmon) or simply because the ingredient be not vis- 
ible in the query image (e.g., blueberry in a smoothie or 
beef in a lasagna). 
Ablation studies. We also analyze the effect of each com- 
ponent in our our model in several optimization stages. The 
result be report in Table 3. Note that here we also report 
medR with 1K, 5K and 10K random selection to show how 
the result scale in large retrieval problems. As expected, 



Figure 5: Localized unit activations. We find that ingredient detector emerge in different unit in our embeddings, which 
be align across modality (e.g., unit 352: “cream”, unit 22: “sponge cake” or unit 571: “steak”). 

visual feature from the ResNet-50 model show a substan- 
tial improvement in retrieval performance when compare 
to VGG-16 features. Even with “fixed vision” network 
the joint embed achieve 7.9 medR use ResNet-50 
architecture (see Table 3). Further “finetuning” of vision net- 
work slightly improves the results. Although it becomes a 
lot harder to decrease the medR in small numbers, additional 
“semantic regularization” improves the medR in both cases. 

6.2. Comparison with human performance 

In order to good ass the quality of our embeddings we 
also evaluate the performance of human on the im2recipe 
task. The experiment be perform through Amazon Me- 
chanical Turk (AMT) service3. For quality purposes, we 
require each AMT worker to have at least 97% approval rate 
and have perform at least 500 task before our experiment. 
In a single evaluation batch, we first randomly choose 10 
recipe and their correspond images. We then ask an 
AMT worker to choose the correct recipe, out of the 10 pro- 
vided recipes, for the give food image. This multiple choice 
selection task be perform 10 time for each food image in 
the batch. The accuracy of an evaluation batch be define a 
the percentage of image query correctly assign to their 
correspond recipe. 

The evaluation be perform for three level of diffi- 
culty. The batch (of 10 recipes) be randomly chosen 
from either all the test recipe (easy), recipe share the 
same course (e.g., soup, salad, or beverage; medium), or 
recipe share the name of the dish (e.g., salmon, pizza, 
or ravioli; hard). As expected–for our model a well a the 
AMT workers–the accuracy decrease a task become more 

3http://mturk.com 

specific. In both coarse and fine-grained tests, our method 
performs comparably to or good than the AMT workers. As 
hypothesized, semantic regularization further improves the 
result (see Table 4). 

In the “all recipes” condition, 25 random evaluation 
batch (25× 10 individual task in total) be select from 
the entire test set. Joint embed with semantic regulariza- 
tion performs the best with 3.2 percentage point improve- 
ment over average human accuracy. For the course-specific 
tests, 5 batch be randomly select within each give 
meal course. Although, on average, our joint embedding’s 
performance be slightly low than the humans’, with seman- 
tic regularization our joint embed surpasses humans’ 
performance by 6.8 percentage points. In dish-specific tests, 
five random batch be select if they have the dish name 
(e.g., pizza) in their title. With slightly low accuracy in 
general, dish-specific result also show similar behavior. Par- 
ticularly for the “beverage” and “smoothie” results, human 
performance be good than our method, possibly because 
detailed analysis be need to elicit the homogenize ingre- 
dients in drinks. Similar behavior be also observe for the 
“sushi” result where fine-grained feature of the sushi roll’s 
center be crucial to identify the correct sushi recipe. 

6.3. Analysis of the learn embed 

To gain further insight into our neural embedding, we 
perform a series of qualitative analysis experiments. We 
explore whether any semantic concept emerge in the neuron 
activation and whether the embed space have certain 
arithmetic properties. 

Neuron Visualizations. Through neural activation visual- 
ization we investigate if any semantic concept emerge in the 

http://mturk.com 


all recipe course-specific recipe dish-specific recipe 

dessert salad bread beverage soup-stew course-mean pasta pizza steak salmon smoothie hamburger ravioli sushi dish-mean 

human 81.6 ± 8.9 52.0 70.0 34.0 58.0 56.0 54.0 ± 13.0 54.0 48.0 58.0 52.0 48.0 46.0 54.0 58.0 52.2 ± 04.6 
joint-emb. only 83.6 ± 3.0 76.0 68.0 38.0 24.0 62.0 53.6 ± 21.8 58.0 58.0 58.0 64.0 38.0 58.0 62.0 42.0 54.8 ± 09.4 
joint-emb.+semantic 84.8 ± 2.7 74.0 82.0 56.0 30.0 62.0 60.8 ± 20.0 52.0 60.0 62.0 68.0 42.0 68.0 62.0 44.0 57.2 ± 10.1 

Table 4: Comparison with human performance on im2recipe task. The mean result be highlight a bold for good 
visualization. Note that on average our method with semantic regularization performs good than average AMT worker. 

- 

- 

- 

+ 

+ 

+ 

= 

= 

= 

(a) Image 

- 

- 

- 

+ 

+ 

+ 

= 

= 

= 

(b) Recipe 

- 

- 

- 

+ 

+ 

+ 

= 

= 

= 

(c) Cross-modal 

Figure 6: Arithmetics use image embeddings (left), recipe embeddings (middle) and cross-modal arithmetic between 
image and recipe embeddings (right). We represent the average vector of a query with the image from it 4 near neighbors. 
In the case of the arithmetic result, we show the near neighbor only. 

neuron in our embed vector despite not be explicitly 
train for that purpose. We pick the top activate images, 
ingredient lists, and cooking instruction for a give neuron. 
Then we use the methodology introduce by Zhou et al. [25] 
to visualize image region that contribute the most to the 
activation of specific unit in our learn visual embeddings. 
We apply the same procedure on the recipe side to also obtain 
those ingredient and recipe instruction to which certain 
unit react the most. Figure 5 show the result for the same 
unit in both the image and recipe embedding. We find that 
certain unit display localize semantic alignment between 
the embeddings of the two modalities. 

Semantic Vector Arithmetic. Different work in the lit- 
erature [15, 18] have use simple arithmetic operation 
to demonstrate the capability of their learn represen- 
tations. In the context of food recipes, one would expect that 
v(“chicken pizza”)−v(“pizza”)+v(“salad”) = v(“chicken 
salad”), where v represent the map into the embed 
space. We investigate whether our learn embeddings have 
such property by apply the previous equation template 
to the average vector of recipe that contain the query 
word in their title. We apply this procedure in the image and 
recipe embed space and show result in Figures 6(a) 
and 6(b), respectively. Our finding suggest that the learn 
embeddings have semantic property that translate to simple 
geometric transformation in the learn space. 

Finally, we apply the same arithmetic operation to em- 
bedding across modalities. In particular, we explore the 
case of modify a recipe by linearly combine it image 
embed with a variety of text-originated embeddings. For 

example, give an image of a chocolate cake, we try to trans- 
form it into a chocolate cupcake by remove and add the 
mean recipe embeddings of cake and cupcake, respectively. 
Figure 6(c) show the results, which we find to be compa- 
rable to those use embeddings within the same modality. 
This suggests that the recipe and image embeddings learn 
in our model be semantically aligned, which broach the 
possibility of application in recipe modification (e.g., ingre- 
dient replacement, calorie adjustment) or even cross-modal 
generation. 

7. Conclusion 
In this paper, we present Recipe1M, the large structure 

recipe dataset to date, the im2recipe problem, and neural em- 
bedding model with semantic regularization which achieve 
impressive result for the im2recipe task. More generally, 
the method present here could be gainfully apply to 
other “recipes” like assembly instructions, tutorials, and in- 
dustrial processes. Further, we hope that our contribution 
will support the creation of automate tool for food and 
recipe understand and open door for many less explore 
aspect of learn such a compositional creativity and pre- 
dicting visual outcome of action sequences. 

8. Acknowledgements 
This work have be support by CSAIL-QCRI collab- 

oration project and the framework of project TEC2013- 
43935-R and TEC2016-75976-R, finance by the Spanish 
Ministerio de Economia y Competitividad and the European 
Regional Development Fund (ERDF). 



References 
[1] L. Bossard, M. Guillaumin, and L. Van Gool. Food-101– 

mining discriminative component with random forests. In 
European Conference on Computer Vision, page 446–461. 
Springer, 2014. 1, 2 

[2] L. Castrejon, Y. Aytar, C. Vondrick, H. Pirsiavash, and A. Tor- 
ralba. Learning align cross-modal representation from 
weakly align data. In Computer Vision and Pattern Recog- 
nition (CVPR), 2016 IEEE Conference on. IEEE, 2016. 5 

[3] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, 
E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti- 
vation feature for generic visual recognition. arXiv preprint 
arXiv:1310.1531, 2013. 6 

[4] V. R. K. Garimella, A. Alfayad, and I. Weber. Social medium 
image analysis for public health. In CHI, page 5543–5547, 
2016. 1 

[5] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn 
for image recognition. arXiv preprint arXiv:1512.03385, 
2015. 1, 3 

[6] C.-w. N. Jing-jing Chen. Deep-based ingredient recognition 
for cooking recipe retrival. ACM Multimedia, 2016. 2 

[7] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignment 
for generate image descriptions. In Proceedings of the IEEE 
Conference on Computer Vision and Pattern Recognition, 
page 3128–3137, 2015. 6 

[8] Y. Kawano and K. Yanai. Foodcam: A real-time food recog- 
nition system on a smartphone. Multimedia Tools and Appli- 
cations, 74(14):5263–5287, 2015. 1, 2 

[9] R. Kiros, Y. Zhu, R. Salakhutdinov, R. Zemel, A. Torralba, 
R. Urtasun, and S. Fidler. Skip-thought vectors. In NIPS, 
page 3294–3302, 2015. 3, 6 

[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet 
classification with deep convolutional neural networks. In 
NIPS, 2012. 1 

[11] T. Kusmierczyk, C. Trattner, and K. Norvag. Understanding 
and predict online food recipe production patterns. In 
HyperText, 2016. 2 

[12] Q. V. Le and T. Mikolov. Distributed representation of sen- 
tences and documents. arXiv preprint arXiv:1405.4053, 2014. 
6 

[13] C. Liu, Y. Cao, Y. Luo, G. Chen, V. Vokkarane, and Y. Ma. 
Deepfood: Deep learning-based food image recognition for 
computer-aided dietary assessment. In International Confer- 
ence on Smart Homes and Health Telematics, page 37–48. 
Springer, 2016. 1 

[14] Y. Mejova, S. Abbar, and H. Haddadi. Fetishizing food in 
digital age: #foodporn around the world. In ICWSM, page 
250–258, 2016. 1 

[15] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient 
estimation of word representation in vector space. CoRR, 
abs/1301.3781, 2013. 3, 6, 8 

[16] A. Myers, N. Johnston, V. Rathod, A. Korattikara, A. Gorban, 
N. Silberman, S. Guadarrama, G. Papandreou, J. Huang, and 
K. Murphy. Im2calories: Towards an automate mobile vision 
food diary. In ICCV, page 1233–1241, 2015. 1, 2 

[17] F. Ofli, Y. Aytar, I. Weber, R. Hammouri, and A. Torralba. Is 
saki #delicious? the food perception gap on instagram and it 

relation to health. In Proceedings of the 26th International 
Conference on World Wide Web. International World Wide 
Web Conferences Steering Committee, 2017. 1 

[18] A. Radford, L. Metz, and S. Chintala. Unsupervised represen- 
tation learn with deep convolutional generative adversarial 
networks. arXiv preprint arXiv:1511.06434, 2015. 8 

[19] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, 
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Ima- 
genet large scale visual recognition challenge. International 
Journal of Computer Vision, 115(3):211–252, 2015. 1 

[20] K. Simonyan and A. Zisserman. Very deep convolutional 
network for large-scale image recognition. arXiv preprint 
arXiv:1409.1556, 2014. 1, 3 

[21] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence 
learn with neural networks. In NIPS, page 3104–3112, 
2014. 3 

[22] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: 
A neural image caption generator. In Proceedings of the IEEE 
Conference on Computer Vision and Pattern Recognition, 
page 3156–3164, 2015. 6 

[23] X. Wang, D. Kumar, N. Thome, M. Cord, and F. Precioso. 
Recipe recognition with large multimodal food dataset. In 
ICME Workshops, page 1–6, 2015. 2 

[24] R. Xu, L. Herranz, S. Jiang, S. Wang, X. Song, and R. Jain. 
Geolocalized model for dish recognition. IEEE Trans. 
Multimedia, 17(8):1187–1199, 2015. 2 

[25] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. 
Object detector emerge in deep scene cnns. International 
Conference on Learning Representations, 2015. 8 

[26] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. 
Learning deep feature for scene recognition use place 
database. In Advances in neural information processing sys- 
tems, page 487–495, 2014. 1 


