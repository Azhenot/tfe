





































Detecting Subject Boundaries Within Text: A Language Independent Statistical Approach 


Detecting Subject Boundaries Within Text: A Language 
Independent Statistical Approach 

K o r i n R i c h m o n d 
( k o r : i . n © c o g s c ± . ed. ac . uk) 

A n d r e w S m i t h 
(aj s©cogsci, ed. ac . uk) 

(Joint Authorship) 
Centre for Cognitive Science 

2 Buccleuch Place 
Edinburgh EH8 9LW 

SCOTLAND 

E i n a t A m i t a y 
( e i n a t @ c o g s c i . ed. ac . uk) 

A b s t r a c t 

We describe here an algorithm for detect- 
ing subject boundary within text base 
on a statistical lexical similarity measure. 
Hearst have already tackle this problem 
with good result (Hearst, 1994). One of 
her main assumption be that a change in 
subject be accompany by a change in vo- 
cabulary. Using this assumption, but by 
introduce a new measure of word signif- 
icance, we have be able to build a ro- 
bust and reliable algorithm which exhibit 
improve accuracy without sacrifice lan- 
guage independency. 

1 I n t r o d u c t i o n 

Automatic detection of subject division within a 
text be consider to be a very difficult task even for 
humans, let alone machines. But such subject di- 
vision be use in more complex task in text pro- 
cessing such a text summarisation. An automatic 
method for mark subject boundary be highly de- 
sirable. Hearst (Hearst, 1994) address this prob- 
lem by apply a statistical method for detect 
subject within text. 

Hearst describes an algorithm for what she call 
Text Tiling, which be a method for detect subject 
boundary within a text. The underlie assump- 
tion of this algorithm be that there be a high proba- 

bility that word which be related to a certain sub- 
ject will be repeat whenever that subject be men- 
tioned. Another basic assumption be that when a 
new subject emerges the choice of vocabulary will 
change, and will stay consistent within the subject 
boundary until the next change in subject. These 
basic notion of vocabulary consistency within sub- 
ject boundary lead to a method for divide text 
base on calculate vocabulary similarity between 
two adjacent window of text. 

Each potential subject boundary be identify and 
assign a correspondence value base on the lexical 
similarity between two window of text, one on ei- 
ther side of the subject boundary. The value for all 
potential boundary be plot on a graph, create 
peak and troughs. The trough represent change 
in vocabulary use and therefore, accord to the 
underlie assumption, a change in subject. A divi- 
sion mark be insert where a significant local min- 
imum be detect on the graph. Hearst measure 
approximately 80% success in detection of subject 
boundary on some texts. 

We decide to adopt Hearst's underlie assump- 
tion that a change in subject will entail a change in 
vocabulary. Our aim be to make the algorithm a 
language independent and computationally expedi- 
ent a possible , while also improve accuracy and 
reliability. 

4 7 



I Preprocessing } STAGE I 

¥ 
ICalculatc a significance 1 STAGE 2 

value for each word J 

V 
ICalculate bias lexical] STAGE 3 

correspondenfes J 

- v - 
I Smooth result 1 STAGE 4 

¥ 
I Insert break ] STAGE 5 

Figure 1: Algorithm Structure. 

2 D e s i g n 

The algorithm be divide into five distinct stages. 
Figure 1 show the sequential, modular structure of 
the algorithm. Each stage of the algorithm be de- 
scribed in more detail below. 

2.1 P r e p r o c e s s i n g ( s t age 1) 

In her implementation of the TextTiling algorithm 
Hearst ignores preprocessing, claim it do not af- 
fect the result (Hearst, 1994). By preprocessing we 
mean lemmatizing, stemming, convert upper to 
low case etc. Testing this assumption on her algo- 
r i thm indeed seem not to change the results. How- 
ever, use preprocessing in conjunction with stage 
2 of our algorithm, do improve results. It be impor- 
tant for our algorithm that morphological difference 
between semantically related word be resolved, so 
that word like "bankrupt" and "bankruptcy", for 
example, be identify a the same word. 

2.2 C a l c u l a t i n g a s ign i f icance va lue fo r 
each w o r d ( s t age 2) 

Hearst treat a text more or less a a bag of word 
in it statistical analysis. But natural language 
be no doubt more structure than this. Different 
word have differ semantic function and rela- 
tionships with respect to the topic of discourse. We 
can broadly distinguish two extreme category of 
words; content word versus function words. Con- 
tent word introduce concepts, and be the mean 
for the expression of idea and facts, for example 
nouns, proper nouns, adjective and so on. Function 

word (for example determiners, auxiliary verb etc.) 
support and coordinate the combination of content 
word into meaningful sentences. Obviously, both 
be need to form meaningful sentences, but, intu- 
itively, it be the content word that carry most weight 
in define the actual topic of discourse. Based on 
this intuition, we believe it would be advantageous 
to identify these content word in a text. It would 
then be possible to bias the calculation of lexical 
correspondence (stage 3) take into account the 
high significance of these word relative to func- 
tion words. 

We would ideally like firstly to reduce the effect 
of noisy non-content word on the algorithm's per- 
formance, and secondly to pay more attention to 
word with a high semantic content. In her imple- 
mentation, Hearst at tempts to do this by have a 
finite list of problematic word that be filter out 
from the text before the statistical analysis take 
place (Hearst, 1994). These problematic word be 
primarily function word and low semantic content 
words, such a determiners, conjunctions, preposi- 
tions and very common nouns. 

Church and Gale (Church and Gale, 1995) men- 
tion the correlation between a word's semantic 
content and various measure of it distribution 
throughout corpora. They show that: "Word rate 
vary from genre to genre, topic to topic, author 
to author, document to document, section to sec: 
tion, paragraph to paragraph. These factor tend 
to decrease the entropy and increase the other test 
variables". One of these other test variable men- 
tioned by Church and Gale be burstiness. They at- 
tribute the innovation of the notion of burstiness 
to Slava Katz, who, pertain to this topic, writes 
(Katz, 1996): "The notion of burstiness.., will be 
use for the characterisation of two closely related 
but distinct phenomena: (a) document-level bursti- 
ness, i.e. multiple occurrence of a content word or 
phrase in a single text document, which be contrast 
with the fact that most other document contain no 
other instance of this word or phrase at all; and (b) 
within-document burstiness (or burstiness proper), 
i.e. close proximity of all or some individual in- 
stance of a content word or phrase within a doc- 
ument exhibit multiple occurrence." Katz have 
highlight many interest feature of the distri- 
bution of content words, which do not conform to 
the prediction of statistical model such a the Pois- 
son. Katz (Katz, 1996) state that, when a concept 
name by a content word be topical for the document, 
then that content word tends to be characterise 
by multiple and bursty occurrence. He claim that, 
while a single occurrence of a topically use content 

4 8 



word or phrase be possible, it be more likely that a 
newly introduce topical entity, will be repeated, "if 
not for break the monotonous effect of pronoun 
use, then for emphasis or clarity". He also claim 

o 

that, unlike function words, the number of instance 
E 

of a specific content word be not directly associate 
with the document length, but be rather a function ~ 
of how much the document be about the concept ex- i 

~5 press by that word. 

z 

Therefore, the characteristic distribution pattern 
of topical content words, which contrast markedly 
with that of non-topical and non-content words, 
could provide a useful aid in identify the seman- 
tically relevant word within a text. Brief mention 
should be make of the work do by Justeson and 
Katz (Justeson and Katz, 1995), which, to a certain 
degree, relates to the requirement of our task. In 
their paper, Justeson and Katz describe some lin- 
guistic property of technical terminology, and use 
them to formulate an algorithm to identify the tech- 
nical term in a give document. However, their al- 
gorithm deal with complex noun phrase only, and, 
although the technical term identify by their al- 
gorithm be generally highly topical, the algorithm 
do not provide the context sensitive information 
of how topical each incidence of a give meaning- 
ful phrase is, relative to it direct environment. It 
be precisely this information that be need to judge 
the content of a particular segment of text. 

Although Katz (Katz, 1996) acknowledges what 
he call two distinct, but closely related, form 
of burstiness, he concentrate on model the 
inter-document distribution of content word and 
phrases. He then us the inter-document distri- 
butions to make inference about probability of 
the repeat occurrence of content word and phrase 
within a single document. Another divergence be- 
tween what Katz have do so far and what the task 
of subject boundary insertion requires, be that he 
decides to ignore the issue of coincidental repeti- 
tions of non-topically use content word and sim- 
ply equates "single occurrence with non-topical oc- 
currence, and multiple occurrence with topical occur- 
rence." 

10 

0 

6 

4 

2 

i i t i t i t t i 

0 O.Oi 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 
Propol ' f i~i Freiltlerlcy of Word 

Figure 2: Calculation of number of near neigh- 
bours. 

n 

significance(x) = -1 × Z arctan ( Dx,i 
n i = l tWicd ] (1) 

where x be an individual word in the document and 
Dx,i be the distance between word x and it ith near- 
est neighbour. The 1st near neighbour of word x 
be the near occurrence of the same word. The 2nd 
near neighbour of x be the near occurrence of 
the same word ignore the 1st near neighbour. In 
general, the ith near neighbour of x be the near- 
est occurrence of the same word ignore the 1st, 
2nd, 3 rd , . . . , ( i - 1)th near neighbours. W be the 
total number of word in the text. w be the number 
of occurrence of the word like x. n be the number 
of near neighbour to include in the calculation 
and depends on the overall frequency of the word in 
the text. This formula will yield a significance score 
that lie within the range 0 to ~ (high significance 
to low significance). This number be then normalise 
to between 0 and 1, with 0 indicate a very low sig- 
nificance, and 1 indicate a very high significance. 
The exact value of n be calculate separately for each 
distinct word, use the follow formula: 

( 8 ) 
n- - - - 1 + e-2°~-(~ -°°2) + 2 (2) 

We have implement a method which assigns an 
estimate significance score base on a measure of 
two context dependent properties; local burstiness 
and global frequency. The heart of our solution 
to the problem of assign context-based value of 
topical significance to all word in a text, can be 
sum up in the follow formula: 

This be essentially a sigmoid function with the 
range vary between two and ten, a show in Fig- 
ure 2. The constant scale and translate the function 
to yield the desire behaviour, which be derive 
empirically. The number of near neighbour to 
consider in equation 1 increase with the word's fre- 
quency. For example, when calculate the signif- 

4 9 



0.9 
i 

0.8 

0.7 

0.6 

0.5 

0.4 

0.3 

0.2 

0.I 

Q O 

2 h e ° • 

CI 
I0 O 

o & o 
o 

2¢ •o t , 

$ • • 

i i i i i i 

~00 400 600 800 1000 1200 1400 
Word Posi~on in Text 

Figure 3: Significance Values. 

icance of the least frequent words, only two near- 
est neighbour be considered. But for the most 
frequently occur words, the number of near 
neighbour be ten. Figure 3 show the main feature 
of the performance of this significance assignment al- 
gorithm when test on a sample text. The result 
for three very different word be shown. 

Two general trend be the most important fea- 
tures of this graph. Firstly, elevate significance 
score be associate with local cluster of a word. 
For example the cluster of three occurrence of "soft- 
ware" (a content word) at the end of the document 
have high significance scores. This contrast with 
the relatively isolated occurrence of the word "soft- 
ware" in the middle of the document, which be 
deem to be little more significant than several oc- 
currences of the word "the" (a function word). Sec- 
ondly, frequent word tend to receive low signifi- 
cance scores. For example, even local cluster of the 
word "the" only receive relatively low significance 
scores, simply because the word have a high frequency 
throughout the document. Conversely, "McNealy" 
(a high semantic content word), which only occurs in 
a cluster of three, receives a high significance value. 
The important result show by the graph be that 
content word (real name such a "McNealy") re- 
ceive high significance value than function word 
("the"). 

We found that an optimal solution to the problem 
of balance local density against global frequency 
be rather elusive. For example, the word at the 
centre of a cluster automatically receive a high 
score, whereas it may be more desirable to have all 
the member of a cluster assign a score lie in a 
narrower range. There be many other contentious 
issue which need to be investigated, such a the use 

of the ratio of all the occurrence of a word in a give 
text to the total length of that text in order to calcu- 
late the relative significance measure. Based on in- 
tuition, partly derive from Katz's discussion (Katz, 
1996) of the relationship between document length 
and word frequency, the exact nature of this rela- 
tionship across various document length may not 
be reliable enough. It may be more consistent to 
consider this ratio within a constant window size, 
e.g. 1000 words. 

The advantage of this simple statistical method 
of distinguish significant content word from non- 
content word be that no word need to be remove 
before allow the algorithm to proceed. The out- 
put of this stage be a normalise significance score 
(0-1) for each word in the text. This significance 
score can then be take into account when analyse 
the text for subject boundaries. 

2.3 C a l c u l a t e B i a s e d Lex ica l 
C o r r e s p o n d e n c e s ( s t ag e 3) 

Let u consider two set of words, set A and set 
B. The main aim of this stage of the processing be 
concerned with calculate a correspondence mea- 
sure between two such set depend on how similar 
they are, where similarity be define a a measure off 
lexical correspondence. If many word be share by 
both set A and B, then the lexical correspondence 
between the two set be high. If the two set do not 
share many words, then the correspondence be low. 
Now let A t be the subset of A that contains only 
those word that occur somewhere in B. And let B' 
be the subset of B that contains only those word 
that occur somewhere in A. The lexical correspon- 
dence between set A and B can then be calculate 
use the simple formula: 

Correspondence- I~ + ~L 
2 

This yield a value within the range 0 to 1. IAI can 
be re-written a 1+1+1+1+1 .... by add a 1 for 
every word in A. Each word have already be give 
a significance value a described in stage 2 of the 
algorithm and this information be take into account 
by re-defining IAI a s l+s2+s3+. . , where sl be the 
significance value assign to the first word in A, s2 
the second and so on. The same can be do for A ', 
B and B ~. The formula now take the average of 
the bias ratios. All this mean be that instead of 
each word counting for '1' in a set, it count for it 
significance value (a value between 0 (insignificant) 
and 1 (highly significant)). The result be that each 
word affect the correspondence measure accord 
to it significance in the text. 

50 



Set A Set B 

Figure 4: Word Sets. 

So far, a word that occurs only in A and not in B, 
contributes zero to JAn[. This mean that a highly 
significant word occur only in A have exactly the 
same effect a an insignificant word occur only 
in A. In other word the significance biasing be only 
take place for word that appear in both A and B. 
Therefore, the formula actually use is: 

Correspondence= L~I ~ -~ "k I-~P-~I 
2 

where A" be the subset of A which contains only 
those word that occur in A and not in B. Sim- 
ilarly, B ~ be the subset of B which contains only 
those word that occur in B and not in A. This be 
show in Figure 4. 

Recall that [A[, [m'[, [A"[, [B], [B'] and IS"[ be 
not calculate by add one for each word in each 
set, but by sum the significance value of the 
word in each set. 

This stage of the processing look at the output 
from the significance calculation stage and considers 
every sentence break in turn - start at the top 
of the document and work down. The algorithm 
assigns a correspondence measure to each sentence 
break a follows: Firstly, set A be generate by tak- 
ing all the word in the previous fifteen sentences. 
Next, set B be generate by take all the word 
in the follow fifteen sentences. 1 Now set A p, 
A ~, B ~ and B ~ be generate a described and then 
the formula above be apply which assigns a cor- 
respondence value to the sentence break currently 
under consideration. The algorithm then move to 
the next sentence break and repeat the process. 

The output from this stage of the algorithm be a 
list of sentence break number (1..n, with n = num- 
ber of sentence in the document) and a lexical cor- 
respondence measure. These number provide the 
input for stage four - smoothing. 

1Fi f teen s e n t e n c e s t u r n s ou t to be t h e o p t i m u m win- 
dow size for t h e va t m a j o r i t y of t ex t s . T h i s be b e c a u s e 
i t be a b o u t t h e s a m e a t h e ave rage s e g m e n t size. 

2.4 S m o o t h i n g ( s t age 4) 

A graph can be plot with lexical correspondence 
along the y-axis and sentence number along the x- 
axis. In order to distinguish the significant peak 
and trough from the many minor fluctuations, a 
simple smooth algorithm be used. Taking three 
neighbour point on the graph, P1, P2, P3: 

P3 

o A. 
..~......~:~ 

..................................................................... .......................... i iUi::: 

~ X 

Figure 5: Smoothing. 

The line P1P3 be bisect and this point be label 
A. P2 be perturbed by a constant amount (not dee- 
pendent on the distance between A and P2) towards 
A. This new point be label B and becomes the 
new P2. This be perform simultaneously on ev- 
ery point on the graph. The process be k then iterate 
a fix number of times. The result be that noise 
be flatten out while the large peak and trough 
remain (although slightly smaller). 

The output from this stage be simply the sentence 
break number and their new, smooth correspon- 
dence values. 

2.5 I n s e r t i n g s u b j e c t b o u n d a r i e s ( s t age 5) 

Considering the graph described in the previous sec- 
tion, generate subject boundary be simply a mat- 
ter of identify local minimum on the graph. The 
confidence of a boundary be calculate from the 
'depth' of the local minimum. This depth be calcu- 
lated simply by take the average of the height of 
the 'peak' (relative to the height of the minimum) on 
either side of the minimum. This now yield a list 
of candidate subject boundary and an associate 
confidence measure for each one. Breaks be then in- 
serted into the original text at the place correspond- 
ing to the local minimum if their confidence value sat- 
isfies a 'minimum confidence' criterion. This cut-off 
criterion be arbitrary, and in our implementation can 
be specify at run time. 

3 R e s u l t s 

Figure 6 show the result of processing the first 
800 sentence from an edition of The Times newspa- 
per. The sentence number (x-axis) be plot against 
the correspondence (y-axis) between the two win- 
dows of text on either side of that sentence. 

51 



. i 

10 

0 

- 10 

- 20 

- 30 

- 40 

- 50 

- 50 

- 70 

- aO 

-gO 
0 

10 

0 • 

- 10 

- 20 

- 30 

- 40 

- 60 

- 50 

- 70 

- 50 

"g~O0 

-10 

- 20 

- 30 

- 40 

- 50 

- 50 

- 70 

- 50 

- 90 
400 

- 4O 

-50 

- 90 
500 

| 

5O 
| 

1 O0 
Sen tence 

I | I 

2 5 O 3 0 q 3 5 O 
~ e n t e n c e 

| 

4 6 0 
i l 

600 550 
Sen tence 

| i | 

550 700 750 
Sen tence 

Figure 6:800 Sentences from The Times newspaper. 

52 



Actua l sub jec t 
boundar y 

36 
61 
779 
109 

146 
165 
175 
203 

244 
278 
304 
333 
356 
376 

Boundar y found Er ro r 
by a lgor i thm 

36 0 
60 1 
79 0 
109 0 
134 + 
145 1 
165 0 
174 1 
203 0 
214 + 
244 0 
278 0 
304 0 
332 1 
355 1 
375 1 

Table 1: The Times 

A large negative value indicates a low degree of 
correspondence and a small negative value or a pos- 
itive value indicates a high degree of correspondence. 
The vertical line mark actual article boundaries. 

The advantage of use a text such a this be that 
there can be no doubt from any human judge a to 
where the boundary occur, i.e. between articles. 
The local min ima on the graph signify the bound- 
aries a determine by the algorithm. The vertical 
bar signify the actual article boundaries. The re- 
sults of the first 400 sentence be summarise in 
table 1. 

The algorithm locate 53% of the article bound- 
aries precisely and 95% of the boundary to within 
an accuracy of a single sentence. Every article 
boundary be identify to within an accuracy of 
two sentences. The algorithm make no use of end- 
of-paragraph markers. It also found some additional 
subject boundary in the middle of articles. These 
be denote by a ' + ' in the error column. Many ex- 
t ra subject boundary be found in the long article 
(starting at sentence 430). It be worth note that 
the min ima occur within this article be not a 
pronounce a the actual article boundary them- 
selves. This section of the graph reflect a long arti- 
cle which contains a number of different subtopics. 

A newspaper be an easy test for such an algorithm 
though. Figure 7 show a graph for an expository 
text - a 200 sentence psychology paper write by 
a fellow student. Again the local min ima indicate 
where the algorithm considers a subject boundary 
to occur and the vertical line be the obvious break 
in the text (mainly before new headings) a judged 
by the author. The result be summarise in table 
2. 

This t ime the algorithm precisely locate 50% of 
the boundaries. It found 63% of the boundary to 
within an accuracy of a single sentence and 88% to 

Actua l subjec t 
boundar y 

7 
22 

59 
72 

96 
121 

162 

184 

Boundar y found E r ro r 
by a lgor i thm 

77 0 
22 0 
42 + 
58 1 
772 0 
77 + 
92 4 

118 3 
137 + 
156 + 
161 1 
177 + 
184 0 
191 + 

Table 2: Expository Text 

within an accuracy of two sentences. This level of 
accuracy be obtain consistently for a variety of 
different texts. Again, it should be mention that 
the algorithm found more break than be immedi- 
ately obvious to a human judge. However, it should 
be note that these extra break be usually de- 
note by small minima, and on inspection the vast 
majori ty of them be in sensible places. 

The algorithm have a certain resolve power. As 
the subject mat ter becomes more and more homoge- 
neous, the number of subject break the algorithm 
find decreases. For some texts, this result in very 
few division be made. By take a small win- 
dow size (the number of sentence to look at either 
side of each possible sentence break), the resolve 
power 'of the algorithm can be increase make it 
more sensitive to change in the vocabulary. How- 
ever, the reliability of the algorithm decrease with 
the increase resolve power. The default window 
size be fifteen sentence and this work well for all 
but the most homogeneous of texts. In this case a 
window size of around six be more effective. A low 
window size increase the resolve power, but de- 
crease the accuracy of the algorithm. The window 
size be a parameter of our implementat ion. 

o 

- 1 o 

- 3 o 

- 70 
~ o 1 ¢ o 1 s o 

Figure 7: Expository Text. 

53 



4 S u m m a r y 

Based on our investigation, we believe that Hearst's 
original intuition that lexical correspondence can be 
exploit to identify subject boundary be a sound 
one. The addition of the significance measure repre- 
sent an improvement on Hearst's algorithm imple- 
mented by the Berkeley Digital Library Project. 

Furthermore, this algorithm be language indepen- 
dent except for the preprocessing stage (which can 
be omit with only a modest degradation in per- 
formance). In order to improve accuracy, language 
dependent method could be considered. Such meth- 
od might include the insertion of conventional dis- 
course marker in order to detect prefer break 
point (e.g. repetition of the same syntactic struc- 
ture, and conventional paragraph opening such as: 
"On the other hand...", "The above...", etc.). An- 
other method would be to make use of a thesaurus, 
since we have found that human judgement be often 
base on synonymous information such a real syn- 
onyms or anaphora. The above issue be discuss 
in various article (Morris and Hirst, 1991); (Mor- 
ris, 1988) and (Givon, 1983) which study discourse 
marker and synonymous information. 

Another interest line of research would be to 
use the information from stage two of the algorithm 
to discover the significant word of a section, and 
thereby attach a label to it. This would be particu- 
larly useful for information retrieval applications. 

5 A c k n o w l e d g e m e n t s 

This problem be set a an assignment on the Data 
Intensive Linguistics course organise by Chris Brew 
at the HCRC, Edinburgh University. Thanks to him, 
Jo Calder and Marc Moens for guidance and advice 
throughout the project. Thanks to ESRC and EP- 
SRC for funding. 

Katz, S. M. 1996. Distribution of context word and 
phrase in text and language modelling. Natural 
Language Engineering, 2(1):15-59. 

Morris, J. 1988. Lexical cohesion, the thesaurus, 
and the structure of text. Technical Report CSRI- 
219, Computer Systems Research Institute, Uni- 
versity of Toronto. 

Morris, J. and G. Hirst. 1991. Lexical cohesion 
compute by thesaural relation a an indicator 
of the structure of text. Computational Linguis- 
tics, 17(1):21-48. 

R e f e r e n c e s 

Church, K. W. and W. A. Gale. 1995. Poisson mix- 
tures. Natural Language Engineering, 1(2):163- 
190. 

Givon, T. 1983. Topic Continuity in Discourse: 
A Quantitative Cross-Language Study. Philadel- 
phia: John Benjamins Publishing Company. 

Hearst, M. A. 1994. Multi-paragraph segmentation 
of expository text. In ACL '94, Las Cruces, NM. 

Justeson, J. S. and S. M. Katz. 1995. Technical ter- 
minology: some linguistic property and an algo- 
rithm for identification in text. Natural Language 
Engineering, 1(1) :9-27. 

54 


