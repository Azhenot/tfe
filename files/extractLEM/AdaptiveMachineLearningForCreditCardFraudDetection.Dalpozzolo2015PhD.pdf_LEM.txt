




















































Adaptive Machine Learning for Credit Card Fraud Detection 


Université Libre de Bruxelles 
Computer Science Department 

Machine Learning Group 

Adaptive Machine Learning for 
Credit Card Fraud Detection 

A thesis submit in fulfillment of the requirement 
for the Ph.D. degree in Computer Science 

Author: 
Andrea Dal Pozzolo 

Supervisor: 
Prof. Gianluca Bontempi 

December 2015 



Declaration of Authorship 

I, Andrea Dal Pozzolo, declare that this thesis title “Adaptive Machine Learning for 

Credit Card Fraud Detection” have be compose by myself and contains original work 

of my own execution. Some of the report work have be do in collaboration with a 

number of co-authors whose contribution be acknowledge in the relevant sections. 

This thesis have be write under the supervision of Prof. Gianluca Bontempi. 

The member of the jury are: 

• Prof. Bart Baesens (Katholieke Universiteit Leuven, Belgium) 

• Prof. Cesare Alippi (Politecnico di Milano, Italy) 

• Prof. Gianluca Bontempi (Université Libre de Bruxelles, Belgium) 

• Prof. Tom Lenaerts (Université Libre de Bruxelles, Belgium) 

• Prof. Yves De Smet (Université Libre de Bruxelles, Belgium) 

• Dr. Olivier Caelen (Worldline S.A., Belgium) 

ii 



“The cure for boredom be curiosity. There be no cure for curiosity.” 

Dorothy Parker 



UNIVERSITÉ LIBRE DE BRUXELLES 

Computer Science Department 

Machine Learning Group 

Abstract 
Adaptive Machine Learning for 

Credit Card Fraud Detection 

by Andrea Dal Pozzolo 

Billions of dollar of loss be cause every year by fraudulent credit card transactions. 

The design of efficient fraud detection algorithm be key for reduce these losses, and 

more and more algorithm rely on advanced machine learn technique to assist fraud 

investigators. The design of fraud detection algorithm be however particularly challeng- 

ing due to the non-stationary distribution of the data, the highly unbalanced class 

distribution and the availability of few transaction label by fraud investigators. At 

the same time public data be scarcely available for confidentiality issues, leave unan- 

swered many question about what be the best strategy. In this thesis we aim to provide 

some answer by focus on crucial issue such as: i) why and how undersampling be 

useful in the presence of class imbalance (i.e. fraud be a small percentage of the trans- 

actions), ii) how to deal with unbalanced and evolve data stream (non-stationarity 

due to fraud evolution and change of spending behavior), iii) how to ass performance 

in a way which be relevant for detection and iv) how to use feedback provide by in- 

vestigators on the fraud alert generated. Finally, we design and ass a prototype of a 

Fraud Detection System able to meet real-world work condition and that be able to 

integrate investigators’ feedback to generate accurate alerts. 



UNIVERSITÉ LIBRE DE BRUXELLES 

Département d’Informatique 

Machine Learning Group 

Résumé 
Adaptive Machine Learning for 

Credit Card Fraud Detection 

par Andrea Dal Pozzolo 

Des milliard de dollar de pertes sont réalisés chaque année en raison de transaction 

frauduleuses sur le carte de crédit. La clé pour réduire ce pertes est le développe- 

ment d’algorithmes de détection de fraude efficaces. De plus en plus d’algorithmes se 

basent sur de technique de machine learn avancées pour assister le détecteurs de 

fraude. La conception d’algorithmes de détection de la fraude est toutefois particulière- 

ment difficile en raison de la distribution non-stationnaire de données, de la distribution 

trés déséquilibrée de class et la disponibilité de seulement quelques transaction cata- 

loguées par le détecteurs de fraude. Dans le même temps, de données publiques ne sont 

guère disponibles pour de raisons de confidentialité, laissant de nombreuses question 

sans réponse à propos de ce qui est la meilleure stratégie pour traiter avec eux. Dans 

cette thèse, nous proposons quelques réponses en se concentrant sur de question cru- 

ciales telles que: i) pourquoi et comment undersampling est utile en présence de class 

déséquilibrées (c-à-d le fraudes sont un petit pourcentage de transactions), ii) com- 

ment traiter l’évolution du flux de données non-balancées (non-stationnaire en raison de 

l’évolution et le changement de comportement de dépense fraude), iii) le évaluations de 

la performance qui sont pertinentes pour la détection et iv) l’utilisation de évaluations 

fournies par le détecteurs sur le alertes de fraude générées. Enfin, nous concevons un 

système de détection de la fraude en mesure de satisfaire le condition de travail dans 

le monde réel et qui est capable d’intégrer le détecteurs avec rétroaction pour générer 

de alertes de fraude précises. 



Acknowledgements 

Foremost, I would like to express my deep and sincere gratitude to my supervisor Prof. 

Gianluca Bontempi for his inspiration, patience and encouragement. He introduce me 

to the field of Machine Learning and provide lot of good idea and advice. I would 

have never complete this thesis without his help. Besides Gianluca, I be deeply grateful 

to Dr. Olivier Caelen for his suggestion and guidance throughout my PhD study. His 

great idea and unfailing support be essential to finish this thesis. 

I would like to thank also Prof. Giacomo Boracchi and Prof. Cesare Alippi for their 

supervision and fruitful discussion on the problem of learn in non-stationary envi- 

ronments during my research visit at Politecnico di Milano. A great thanks also to 

Prof. Nitesh V. Chawla and Dr. Reid A. Johnson for share their precious knowledge 

and expertise on the topic of unbalanced classification during my research visit at Notre 

Dame University. My sincere thanks also go to the PhD committee member and the 

examiner of this thesis for the insightful comment and hard question throughout my 

PhD study. 

A well-known quote say that behind every great man there’s a great woman. While 

I’m not a great man, there’s a great woman behind me and her name be Katia. All my 

achievement also depends on her, so thank you my love for have always support me 

during these four years. 

If I be able to arrive to the PhD study it be because I have two great parents, Francesco 

and Francesca, that always believe in what I be doing. I wish to convey special thanks 

to my family for their endless support and understanding. 

Last but not least, I would like to thank my friend (from Italy and Belgium) and 

colleague both from the Computer Science Department of ULB and Worldline S.A. 

(former and current) for their continuous encouragement in these 4 years. A big thanks 

also to Serge Waterschoot (Fraud Risk Manager at Worldline S.A.) that make possible 

this PhD and Innoviris1 (Brussels Region) that fund the Doctiris project. 

1Innoviris be the Brussels institute for the encouragement of scientific research and innovation. 

vi 



Contents 

Declaration of Authorship ii 

Abstract iv 

Résumé v 

Acknowledgements vi 

List of Figures xi 

List of Tables xiii 

List of Acronyms xv 

I Overview 1 

1 Introduction 3 
1.1 The problem of Fraud Detection . . . . . . . . . . . . . . . . . . . . . . . 3 
1.2 The impact of fraud . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 
1.3 Credit Card Fraud Detection . . . . . . . . . . . . . . . . . . . . . . . . . 7 
1.4 Challenges in Data Driven Fraud Detection Systems . . . . . . . . . . . . 8 
1.5 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 

1.5.1 Understanding sample method . . . . . . . . . . . . . . . . . . . 10 
1.5.2 Learning from evolve and unbalanced data stream . . . . . . . . 11 
1.5.3 Formalization of a real-world Fraud Detection System . . . . . . . 11 
1.5.4 Software and Credit Card Fraud Detection Dataset . . . . . . . . . 12 

1.6 Publications and research activity . . . . . . . . . . . . . . . . . . . . . . 12 
1.7 Financial support and project objective . . . . . . . . . . . . . . . . . . . 14 
1.8 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 
1.9 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 

2 Preliminaries 17 
2.1 Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 

2.1.1 Formalization of supervise learn . . . . . . . . . . . . . . . . . 18 
2.1.2 The problem of classification . . . . . . . . . . . . . . . . . . . . . 21 
2.1.3 Bias-variance decomposition . . . . . . . . . . . . . . . . . . . . . . 23 

vii 



Contents viii 

2.1.4 Evaluation of a classification problem . . . . . . . . . . . . . . . . . 24 
2.2 Credit Card Fraud Detection . . . . . . . . . . . . . . . . . . . . . . . . . 27 

2.2.1 Fraud Detection System work condition . . . . . . . . . . . . . 27 
2.2.1.1 FDS Layers: . . . . . . . . . . . . . . . . . . . . . . . . . 28 
2.2.1.2 Supervised Information . . . . . . . . . . . . . . . . . . . 29 
2.2.1.3 System Update . . . . . . . . . . . . . . . . . . . . . . . . 29 

2.2.2 Features augmentation . . . . . . . . . . . . . . . . . . . . . . . . . 30 
2.2.3 Accuracy measure of a Fraud Detection System . . . . . . . . . . . 31 

3 State-of-the-art 35 
3.1 Techniques for unbalanced classification task . . . . . . . . . . . . . . . . 35 

3.1.1 Data level method . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 
3.1.2 Algorithm level method . . . . . . . . . . . . . . . . . . . . . . . . 39 

3.2 Learning with non-stationarity . . . . . . . . . . . . . . . . . . . . . . . . 41 
3.2.1 Sample Selection Bias . . . . . . . . . . . . . . . . . . . . . . . . . 42 
3.2.2 Time evolve data . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 

3.3 Learning with evolve and unbalanced data stream . . . . . . . . . . . . 46 
3.4 Algorithmic solution for Fraud Detection . . . . . . . . . . . . . . . . . . 47 

3.4.1 Supervised Approaches . . . . . . . . . . . . . . . . . . . . . . . . . 48 
3.4.2 Unsupervised Approaches . . . . . . . . . . . . . . . . . . . . . . . 51 

II Contribution 55 

4 Techniques for unbalanced classification task 57 
4.1 When be undersampling effective in unbalanced classification tasks? . . . . 58 

4.1.1 The warp effect of undersampling on the posterior probability . 59 
4.1.2 Warping and class separability . . . . . . . . . . . . . . . . . . . . 62 
4.1.3 The interaction between warp and variance of the estimator . . 63 
4.1.4 Experimental validation . . . . . . . . . . . . . . . . . . . . . . . . 67 
4.1.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 

4.2 Using calibrate probability with undersampling . . . . . . . . . . . . . . 73 
4.2.1 Adjusting posterior probability to new prior . . . . . . . . . . . . 73 
4.2.2 Warping correction and classification threshold adjustment . . . . 76 
4.2.3 Experimental result . . . . . . . . . . . . . . . . . . . . . . . . . . 77 
4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 

4.3 Racing for sample method selection . . . . . . . . . . . . . . . . . . . . 81 
4.3.1 Racing for strategy selection . . . . . . . . . . . . . . . . . . . . . . 82 
4.3.2 Experimental result . . . . . . . . . . . . . . . . . . . . . . . . . . 83 
4.3.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 

4.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 

5 Learning from evolve data stream with skewed distribution 89 
5.1 Learning strategy in credit card fraud detection . . . . . . . . . . . . . . 91 

5.1.1 Formalization of the learn problem . . . . . . . . . . . . . . . . 91 
5.1.2 Strategies for learn with unbalanced and evolve data stream 91 
5.1.3 Experimental assessment . . . . . . . . . . . . . . . . . . . . . . . . 95 
5.1.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 



Contents ix 

5.2 Using HDDT to avoid instance propagation . . . . . . . . . . . . . . . . . 107 
5.2.1 Hellinger Distance Decision Trees . . . . . . . . . . . . . . . . . . . 108 
5.2.2 Hellinger Distance a weight ensemble strategy . . . . . . . . . . 109 
5.2.3 Experimental assessment . . . . . . . . . . . . . . . . . . . . . . . . 110 
5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 

5.3 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 

6 A real-world Fraud Detection Systems: Concept Drift Adaptation with 
Alert-Feedback Interaction 117 
6.1 Realistic work condition . . . . . . . . . . . . . . . . . . . . . . . . . . 118 
6.2 Fraud Detection with Alert-Feedback Interaction . . . . . . . . . . . . . . 119 
6.3 Learning strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120 

6.3.1 Conventional Classification Approaches in FDS . . . . . . . . . . . 121 
6.3.2 Separating delayed Supervised Samples from Feedbacks . . . . . . 121 
6.3.3 Two Specific FDSs base on Random Forest . . . . . . . . . . . . . 123 

6.4 Selection bias and Alert-Feedback Interaction . . . . . . . . . . . . . . . . 124 
6.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 

6.5.1 Separating feedback from delayed supervise sample . . . . . . . 127 
6.5.2 Artificial dataset with Concept Drift . . . . . . . . . . . . . . . . . 128 
6.5.3 Improving the performance of the feedback classifier . . . . . . . . 130 
6.5.4 Standard accuracy measure and classifier ignore AFI . . . . . . 132 
6.5.5 Adaptive aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . 133 
6.5.6 Final strategy selection and classification model analysis . . . . . . 136 

6.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 
6.7 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 

7 Conclusions and Future Perspectives 143 
7.1 Summary of contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . 143 
7.2 Learned lesson . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144 
7.3 Open issue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 
7.4 The Future: go towards Big Data solution . . . . . . . . . . . . . . . . 146 
7.5 Added value for the company . . . . . . . . . . . . . . . . . . . . . . . . . 149 
7.6 Concluding remark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149 

A The unbalanced package 151 
A.1 Methods for unbalanced classification . . . . . . . . . . . . . . . . . . . . . 151 
A.2 Racing for strategy selection . . . . . . . . . . . . . . . . . . . . . . . . . . 154 
A.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 

B FDS software module 157 
B.1 Model training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157 
B.2 Scoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 
B.3 Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158 

C Bias and Variance of an estimator 159 



Contents x 

Bibliography 161 



List of Figures 

1.1 The Credit Card Fraud Detection process. . . . . . . . . . . . . . . . . . . 5 
1.2 Fraudulent and genuine distribution in September 2013. . . . . . . . . . . 10 
1.3 Partners of the Doctiris project. . . . . . . . . . . . . . . . . . . . . . . . . 14 

2.1 The player of supervise learning. . . . . . . . . . . . . . . . . . . . . . . 19 
2.2 The Receiving Operating Characteristic (ROC) curve . . . . . . . . . . . . 26 
2.3 The layer of a Fraud Detection System . . . . . . . . . . . . . . . . . . . 27 

3.1 Resampling method for unbalanced classification. . . . . . . . . . . . . . 37 
3.2 Different type of Concept Drift. . . . . . . . . . . . . . . . . . . . . . . . 45 

4.1 How undersampling work . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 
4.2 p and p at different β . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 
4.3 Synthetic datasets with different class overlap. . . . . . . . . . . . . . . . . 62 
4.4 p a a function of β for two univariate binary classification tasks. . . . . . 62 
4.5 p − p a a function of δ, where δ = ω+ − ω−. . . . . . . . . . . . . . . . . 63 
4.6 dpsdp a a function of p and β. . . . . . . . . . . . . . . . . . . . . . . . . . . 66 
4.7 Terms of inequality 4.20 for a classification task with non separable classes. 67 
4.8 Terms of inequality 4.20 for a classification task with separable classes. . . 67 
4.9 Plot of dpsdp percentile and 

√ 
νs 
ν for the synthetic dataset 1. . . . . . . . . 68 

4.10 Regions where undersampling should work in synthetic dataset 1. . . . . . 69 
4.11 Plot of dpsdp percentile and 

√ 
νs 
ν for the synthetic dataset 2. . . . . . . . . 70 

4.12 Difference between the Kendall rank correlation of p̂s and p̂ with p in UCI 
datasets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 

4.13 Ratio between the number of sample satisfy condition (4.20) and all 
the instance available in each dataset average over all the βs. . . . . . . 72 

4.14 Learning framework for compare model with and without undersam- 
pling use Cross Validation (CV). . . . . . . . . . . . . . . . . . . . . . . 78 

4.15 Boxplot of AUC for different value of β in the Credit-card dataset. . . . . 80 
4.16 Boxplot of BS for different value of β in the Credit-card dataset. . . . . . 80 
4.17 The race algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 
4.18 Comparison of strategy for unbalanced data with different classifier over 

all datasets of Table 4.3 in term of G-mean (the high the better). . . . 84 
4.19 Comparison of strategy for unbalanced data with different classifier on 

cam and ecoli datasets in term of G-mean (the high the better). . . . . 84 

5.1 The Static approach. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 
5.2 The Updating approach. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 
5.3 The Propagate and Forget approach. . . . . . . . . . . . . . . . . . . . . . 94 

xi 



List of Figures xii 

5.4 Weight α′j for variable TERM_MCC. . . . . . . . . . . . . . . . . . . . . . 97 
5.5 Comparison of static strategy use sum of rank in all batches. . . . . . 99 
5.6 Comparison of update strategy use sum of rank in all batches. . . . . 101 
5.7 Comparison of forget strategy use sum of rank in all batches. . . . 103 
5.8 Comparison of different balance technique and strategy use sum of 

rank in all batches. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 
5.9 Comparison of different balance technique and strategy in term of 

average prediction time (in seconds) over all batches. . . . . . . . . . . . . 105 
5.10 Comparison of all strategy use sum of rank in all batches. . . . . . . . 106 
5.11 Batch average result in term of Area Under the ROC Curve (AUC) 

(higher be better) use different sample strategy and batch-ensemble 
weight method with C4.5 and HDDT over all UCI datasets. . . . . . . 112 

5.12 Batch average result in term of computational time (lower be better) 
use different sample strategy and batch-ensemble weight method 
with C4.5 and HDDT over all UCI datasets. . . . . . . . . . . . . . . . . . 113 

5.13 Batch average result in term of computational AUC (higher be better) 
use different sample strategy and batch-ensemble weight method 
with C4.5 and HDDT over all drift MOA datasets. . . . . . . . . . . . 113 

5.14 Batch average result in term of AUC (higher be better) use different 
sample strategy and batch-ensemble weight method with C4.5 and 
HDDT over the Credit Card dataset. . . . . . . . . . . . . . . . . . . . . . 114 

5.15 Comparison of different strategy use the sum of rank in all batch 
for the Creditcard dataset in term of AUC. . . . . . . . . . . . . . . . . . 114 

6.1 The supervise sample available. . . . . . . . . . . . . . . . . . . . . . . . 120 
6.2 Learning strategy for feedback and delayed transaction occur in 

the two day (M = 2) before the feedback (δ = 7). . . . . . . . . . . . . . 122 
6.3 Number of daily fraud for datasets in Table 6.2. . . . . . . . . . . . . . . 126 
6.4 Average Pk per day for classifier on dataset 2013. . . . . . . . . . . . . . 128 
6.5 Comparison of classification strategy use sum of rank in all batch 

and pair t-test base upon on the rank of each batch. . . . . . . . . . . 129 
6.6 Average Pk per day for classifier on datasets with artificial concept drift. 130 
6.7 A classifier Rt train on all recent transaction occur between t and 

t− δ. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 
6.8 Detection accuracy measure use different performance measure on the 

2013 dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 
6.9 Feedbacks request by AWt and it component Ft and WDt . . . . . . . . 135 
6.10 Posterior probability PFt(+|x) and PWDt (+|x) for different days. . . . . . 137 
6.11 Training set size and time to train a RF for the feedback and delayed 

classifier in the 2013 dataset . . . . . . . . . . . . . . . . . . . . . . . . . 138 
6.12 Average feature importance measure by the mean decrease in accuracy 

calculate with the Gini index in the BRF model of WDt in the 2013 
dataset. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 

B.1 Software module of the FDS prototype present in Chapter 6. . . . . . . 157 



List of Tables 

1.1 Probability Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 
1.2 Learning Theory Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 
1.3 Fraud Detection Notation. . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 

2.1 Confusion Matrix. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 
2.2 Confusion Matrix of a classifier with TPR=99% and TNR=99%. . . . . . 25 

4.1 Ranking correlation between p̂ (p̂s) and p for different value of β in the 
classification task of Figure 4.9. . . . . . . . . . . . . . . . . . . . . . . . . 69 

4.2 Ranking correlation between p̂ (p̂s) and p for different value of β in the 
classification task of Figure 4.11. . . . . . . . . . . . . . . . . . . . . . . . 70 

4.3 Selected datasets from the UCI repository [1] . . . . . . . . . . . . . . . . 71 
4.4 Different level of undersampling in a dataset with 1,000 positive in 10,000 

observations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 
4.5 Sum of rank and p-values of the pair t-test between the rank of p̂ and 

p̂′ and between p̂ and p̂s for different metrics. . . . . . . . . . . . . . . . . 79 
4.6 Comparison of CV and F-race result in term of G-mean for Random 

Forest (RF) classifier. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 
4.7 Comparison of CV and F-race result in term of G-mean for Support 

Vector Machine (SVM) classifier. . . . . . . . . . . . . . . . . . . . . . . . 86 

5.1 Strengths and weakness of the different learn approaches. . . . . . . . 95 
5.2 Fraudulent dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 
5.3 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 

6.1 Classifiers use in the chapter. . . . . . . . . . . . . . . . . . . . . . . . . 123 
6.2 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 
6.3 Average Pk for the slide and ensemble strategy . . . . . . . . . . . . . 127 
6.4 Datasets with Artificially Introduced CD . . . . . . . . . . . . . . . . . . . 129 
6.5 Average Pk in the month before and after CD for the slide window . . . 129 
6.6 Average Pk in the month before and after CD for the ensemble . . . . . . 130 
6.7 Average Pk for the slide and ensemble approach when δ = 15. . . . . . . 131 
6.8 Average Pk of Ft with method for SSB correction. . . . . . . . . . . . . . 131 
6.9 Average Pk for the slide approach with more than 100 feedback per day. 132 
6.10 Average AP, AUC and Pk for the slide approach . . . . . . . . . . . . . 133 
6.11 Average Pk of AWt with adaptive αt . . . . . . . . . . . . . . . . . . . . . . 136 

xiii 





List of Acronyms 

AFI Alert-Feedback Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 

AP Average Precision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 

AUC Area Under the ROC Curve . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii 

BER Balanced Error Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 

BRF Balanced Random Forest. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .123 

CD Concept Drift. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .44 

CNN Condensed Nearest Neighbor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 

CV Cross Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi 

EDR Expert Driven Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 

DDM Data Driven Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8 

ENN Edited Nearest Neighbor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 

FD Fraud Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 

FDS Fraud Detection System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 

FN False Negative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 

FNR False Negative Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 

FP False Positive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 

FPR False Positive Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 

HD Hellinger Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 

HDDT Hellinger Distance Decision Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 

xv 



List of Tables xvi 

IG Information Gain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 

IID Independent and Identically Distributed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 

LB Logit Boost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 

ML Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 

MME Mean Misclassification Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 

NNET Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 

NCL Neighborhood Cleaning Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 

OSS One Sided Selection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .38 

kNN k-Nearest Neighbor. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .39 

RF Random Forest. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xiii 

ROC Receiving Operating Characteristic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi 

SSB Sample Selection Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .41 

SVM Support Vector Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii 

TNR True Negative Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 

TP True Positive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 

TPR True Positive Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 



To my future wife Katia 

xvii 





Part I 

Overview 

1 





Chapter 1 

Introduction 

1.1 The problem of Fraud Detection 

Fraud be a old a humanity itself and can take an unlimited variety of different forms. 

Moreover, the development of new technology provide additional way in which crim- 

inals may commit fraud [2], for instance in e-commerce the information about the card 

be sufficient to perpetrate a fraud. The use of credit card be prevalent in modern day 

society and credit card fraud have kept on grow in recent years. Financial loss due 

to fraud affect not only merchant and bank (e.g. reimbursements), but also individual 

clients. If the bank loses money, customer eventually pay a well through high interest 

rates, high membership fees, etc. Fraud may also affect the reputation and image of a 

merchant cause non-financial loss that, though difficult to quantify in the short term, 

may become visible in the long period. For example, if a cardholder be victim of fraud 

with a certain company, he may no longer trust their business and choose a competitor. 

The action take against fraud can be divide into fraud prevention, which attempt 

to block fraudulent transaction at source, and fraud detection, where successful fraud 

transaction be identify a posteriori. Technologies that have be use in order to 

prevent fraud be Address Verification Systems (AVS), Card Verification Method (CVM) 

and Personal Identification Number (PIN). AVS involves verification of the address with 

zip code of the customer while CVM and PIN involve check of the numeric code that 

be keyed in by the customer. For prevention purposes, financial institution challenge all 

transaction with rule base filter and data mining method a neural network [3]. 

Fraud detection is, give a set of credit card transactions, the process of identify if 

a new authorize transaction belongs to the class of fraudulent or genuine transaction 

[4]. A Fraud Detection System (FDS) should not only detect fraud case efficiently, 

3 



Chapter 1. Introduction 4 

but also be cost-effective in the sense that the cost invest in transaction screen 

should not be high than the loss due to fraud [5]. Bhatla [6] show that screen 

only 2% of transaction can result in reduce fraud loss accounting for 1% of the total 

value of transactions. However, a review of 30% of transaction could reduce the fraud 

loss drastically to 0.06%, but increase the cost exorbitantly. In order to minimize 

cost of detection it be important to use expert rule and statistical base model (e.g. 

Machine Learning) to make a first screen between genuine and potential fraud and ask 

the investigator to review only the case with high risk. 

Typically, transaction be first filter by check some essential condition (e.g. suf- 

ficient balance) and then score by a predictive model (see Figure 1.1). The predictive 

model score each transaction with high or low risk of fraud and those with high risk 

generate alerts. Investigators check these alert and provide a feedback for each alert, 

i.e. true positive (fraud) or false positive (genuine). These feedback can then be use 

to improve the model. A predictive model can be built upon experts’ rules, i.e. rule 

base on knowledge from fraud experts, but these require manual tune and human 

supervision. Alternatively, with Machine Learning (ML) technique [7] we can efficiently 

discover fraudulent pattern and predict transaction that be most likely to be fraud- 

ulent. ML technique consist in infer a prediction model on the basis of a set of 

examples. The model be in most case a parametric function, which allows predict the 

likelihood of a transaction to be fraud, give a set of feature describe the transaction. 

In the domain of fraud detection, the use of learn technique be attractive for a num- 

ber of reasons. First, they allow to discovery pattern in high dimensional data streams, 

i.e. transaction arrive a a continuous stream and each transaction be define by many 

variables. Second, fraudulent transaction be often correlate both over time and space. 

For examples, fraudsters typically try to commit fraud in the same shop with different 

card within a short time period. Third, learn technique can be use to detect and 

model exist fraudulent strategy a well a identify new strategy associate to un- 

usual behavior of the cardholders. Predictive model base on ML technique be also 

able to automatically integrate investigators’ feedback to improve the accuracy of the 

detection, while in the case of expert system, include investigator feedback require 

rule revision that can be tedious and time consuming. 

When a fraud cannot be prevented, it be desirable to detect it a rapidly a possible. In 

both cases, prevention and detection, the problem be magnify by a number of domain 

constraint and characteristics. First, care must be take not to prevent too many 

legitimate transaction or incorrectly block genuine cards. Customer irritation be to be 

avoided. Second, most bank process vast number of transactions, of which only a 

small fraction be fraudulent, often less than 0.1% [3]. Third, only a limited number of 

transaction can be checked by fraud investigators, i.e. we cannot ask a human person 



Chapter 1. Introduction 5 

Predic;ve&model& 

Rejected& 

Terminal&check& 
Correct'Pin?'' 

Sufficient'Balance'?'' 
Blocked'Card?' 

Feedback& 

Inves;gators& 
Fraud&score& 

Figure 1.1: The Credit Card Fraud Detection process. Credit card payment need 
to pas a first terminal check and then if not reject be score by a predictive model 
that raise alert for the most suspicious transactions. Investigators provide feedback 
on the alert (transaction label a fraud or genuine) that can be use to improve the 

accuracy of the predictive model. 

to check all transaction one by one if it be fraudulent or not. In other word company 

and public institution need automatic system able to support fraud detection [8]. 

Credit card fraud may occur in various way [9]: just to mention some, we can have 

steal card fraud, cardholder-not-present fraud and application fraud: 

• Stolen card fraud be the most common type of fraud where the fraudster usually 
try to spend a much a possible and a quickly a possible. The detection of 

such a fraud typically relies on the discovery of an unexpected usage pattern of 

the credit card (generally unexpectedly important) with respect to the common 

practice. 

• Cardholder-not-present fraud be often observe in e-business. Here the fraudster 
need the information about a credit card but not the card itself. This fraud 

demand a prompt detection since, unlike the previous case, the official card owner 

be not aware that his own data have be stolen. 

• Application fraud corresponds to the application for a credit card with false per- 
sonal information. This kind of fraud occurs more rarely since it could be detect 

during the application by check the information of the applier, contrary to other 

fraud that can not be anticipated. 



Chapter 1. Introduction 6 

The previous classification however be not exhaustive, since fraud be continuously evolv- 

ing. For a more general discussion of different fraud type we refer the reader to [10]. 

Whenever a fraud method be detected, criminal adapt their strategy and try others. 

At the same time, everyday there be new criminal take part in the game and try 

new and old strategies. In this set it be important to update the detection tools, but 

keep old one a well [2]. Exchange of idea for detection tool be difficult a fraud- 

sters could benefit from it by test their strategies. For the same reason datasets be 

typically not publicly available to the research community. 

1.2 The impact of fraud 

With this extensive use of credit cards, fraud appear a a major issue in the credit card 

business. It be hard to have some figure on the impact of fraud, since company and 

bank do not like to disclose the amount of loss due to frauds. Another problem in 

credit-card fraud loss estimation be that we can measure the loss of only those fraud that 

have be detected, and it be not possible to ass the size of unreported/undetected 

frauds. Other fraud be report long after the criminal have complete the crime [11]. 

However, the Association for Payment Clearing Services (APACS) have estimate that 

total loss through credit card fraud in the United Kingdom have be grow rapidly 

from £122 million in 1997 to £440.3 million in 2010 [8]. According to The Nilson Re- 

port [12], Global Credit, Debit, and Prepaid Card Fraud loss reach $11.27 Billion 

in 2012 - Up 14.6% Over 2011. Gross fraud loss account for 5.22% total volume, 

up from 5.07% in 2011. In 2012, only in the USA fraud loss reach $5.33 billion. 

According to the Lexis Nexis [13], in 2014 fraudulent card transaction worldwide have 

reach around $11 billion a year, and the USA may account for about half of that. 

The European Central Bank (ECB) report [14] that, in 2012, e 1 in every e 2’635 

spent on credit and debit card issue within SEPA (the European Union, Iceland, 

Liechtenstein, Monaco, Norway and Switzerland) be lose to fraud. The total value of 

fraud be estimate reach e 1.33 billion in 2012, register an increase of 14.8% 

compare with 2011. In particular 60% of these fraud come from card-not-present 

(CNP) payment (i.e. payment via post, telephone or the internet), 23% from point-of- 

sale (POS) terminal and 17% from ATMs. The introduction of EMV security standard 

(chip on cards) have reduce fraud share (0.048% in 2008 and 0.038% in 2012) on the 

total number of transactions.1 However, from 2011 to 2012, CNP fraud have increase 

by 21%, follow the grow of CNP payments, which rise by around 15% to 20% a 

year between 2008 and 2012, while all the other transaction rise by 4%. The ECB report 
1With EMV card the cardholder be ask to enter a PIN. 



Chapter 1. Introduction 7 

show also that credit card be more affected by fraud than debit card, estimate that 

for every e 1000 we have e 1 of loss due to fraud in credit while e 1 for every e 5’400 

in debit card. Another interest fact be that CNP fraud be usually more frequent in 

mature card markets, whereas POS fraud be more common in less developed markets. 

The 2015 Cybersource report [15] show that business be reluctant towards the adop- 

tion of 3-D Secure method [16] (online authentication base on a three-domain model: 

acquirer, issuer and interoperability), because it may cause poor customer experience 

and the risk of customer abandon their purchases. 

1.3 Credit Card Fraud Detection 

Credit card fraud detection relies on the analysis of record transactions. Transaction 

data be mainly compose of a number of attribute (e.g. credit card identifier, transac- 

tion date, recipient, amount of the transaction). Automatic system be essential since 

it be not always possible or easy for a human analyst to detect fraudulent pattern in 

transaction datasets, often characterize by a large number of samples, many dimen- 

sion and online updates. Also, the cardholder be not reliable in reporting the theft, loss 

or fraudulent use of a card [17]. In the follow we will now discus advantage and 

disadvantage of Expert Driven and Data Driven approach to fraud detection. 

The Expert Driven approach us domain knowledge from fraud investigator to define 

rule that be use to predict the probability of a new transaction to be fraudulent. Let u 

imagine that the investigator know from experience that a transaction do on a bet 

website with an amount great than $10000 be almost certain to be fraudulent. Then 

we can automatize the detection by mean of a rule a “IF transaction amount > $10000 

& Betting website THEN fraud probability = 0.99”. In the same spirit we can define a 

set of rule for different scenarios. Typically, expert rule can be distinguish between 

score rule and block rules. The former assigns a score to a transaction base on the 

risk the investigator associate to a certain pattern; the latter can block the transaction 

because the risk of fraud be too high. The advantage of expert rule are: i) they be easy 

to develop and to understand, ii) they explain why an alert be generate and iii) they 

exploit domain expert knowledge. However, they have a number of drawbacks: i) they be 

subjective (if you ask 7 expert you may get 7 different opinions), ii) they detect only easy 

correlation between variable and fraud (it be hard for a human analyst to think in more 

the three dimension and explore all possible pattern combinations), iii) they be able to 

detect only know fraudulent strategies, iv) they require human monitoring/supervision 

(update in case of performance drop) and v) they can become obsolete soon due to fraud 

evolution. 



Chapter 1. Introduction 8 

A different way to automatize the detection be by mean of Data Driven methods, i.e. 

set up a FDS base on Machine Learning able to learn from data in a supervise or 

unsupervised manner which pattern be the most probably related to a fraudulent be- 

havior. With Machine Learning we let the computer to discover fraudulent pattern in 

the data. Data Driven approach have also advantage and disadvantages, for instance 

with Machine Learning algorithm we can: i) learn complex fraudulent configuration 

(use all feature available), ii) ingest large volume of data, iii) model complex distribu- 

tions, iv) predict new type of fraud (anomalies from genuine patterns) and v) adapt 

to change distribution in the case of fraud evolution. However, they have also some 

drawbacks: i) they need enough samples, ii) some model be black box, i.e. they be 

not easily interpretable by investigator and they do not provide an understand of the 

reason why an alert be generated. 

Data Driven and Expert Driven method usually work in parallel and their combination 

be often the best solution. Typically, real-world FDSs like the one of our project partner 

(Worldline S.A. from Brussels, Belgium) use Data Driven method to obtain precise 

alert (reduce False Positive (FP)), while Expert Driven one be mostly use to make 

sure that all the fraud be detect (reduce False Negative (FN)) at the cost of have 

few false alerts.2 Data Driven solution should not be see a a tool to replace expert- 

base system, but rather a a support to obtain a more accurate detection. A detailed 

description of how these two type of method work and how they be combine will be 

provide in Section 2.2.1. 

This thesis will concern automatic data driven method base on Machine Learning 

techniques. The design of a FDS base on Data Driven Models (DDMs) be not an 

easy task, it require the practitioner to decide which feature to use, strategy (e.g. 

supervise or unsupervised), algorithm (e.g. decision trees, neural network, support 

vector machine), frequency of update of the model (once a year, monthly or every time 

new data be available), etc. We hope that, by the end of the thesis, the reader will have 

a good understand of how to design and implement an effective data driven fraud 

detection solution. 

1.4 Challenges in Data Driven Fraud Detection Systems 

The design of a FDSs employ DDMs base on Machine Learning algorithm be par- 

ticularly challenge for the follow reasons: 
2From personal communication with the project collaborator it have emerge that Expert Driven 

method be more recall oriented, while Data Driven one be optimize for precision, i.e. the first aim 
to reduce miss fraud and the second to reduce false alerts. 



Chapter 1. Introduction 9 

1. Frauds represent a small fraction of all the daily transaction [18]. 

2. Frauds distribution evolves over time because of seasonality and new attack strate- 

gy [19]. 

3. The true nature (class) of the majority of transaction be typically know only 

several day after the transaction take place, since only few transaction be timely 

checked by investigator [20]. 

The first challenge be also know a the unbalanced problem [21], since the distribution 

of the transaction be skewed towards the genuine class. The distribution of genuine 

and fraud sample be not only unbalanced, but also overlap (see the plot over the 

first two principal component in Figure 1.2). Most Machine Learning algorithm be 

not design to cope with a both unbalanced and overlap class distribution [22]. 

The change in fraudulent activity and costumer behavior be the main responsible of 

non-stationarity in the stream of transactions. This situation be typically refer to 

a concept drift [23] and be of extreme relevance for FDSs which have to be constantly 

update either by exploit the most recent supervise sample or by forget outdated 

information that might be no more useful whereas not misleading. FDS strategy that 

be not update or revisit frequently be often lose their predictive accuracy in the 

long term [18]. 

The third challenge be related to the fact that, in a real-world setting, it be impossible 

to check all transactions. The cost of human labour seriously constrains the number of 

alerts, return by the FDS, which can be validate by investigators. Investigators check 

FDS alert by call the cardholders, and then provide the FDS with feedback indicate 

whether the alert be related to fraudulent or genuine transactions. These feedbacks, 

which refer to a tiny fraction of the daily transaction amount, be the only real-time 

information that can be provide to train or update classifiers. The class (fraudulent / 

non-fraudulent) of the rest of transaction be know only several day later. Classes can 

be automatically assign when a certain time period have passed, e.g. by assume a 

certain reaction time for customer to discover and then report frauds. Standard FDSs 

ignore feedback from investigator often provide less accurate alert than FDSs able 

to use efficiently both feedback and the other supervise sample available [20]. 

1.5 Contributions 

The contribution of the thesis address the challenge discuss in the previous section. 



Chapter 1. Introduction 10 

(a) 08/09/2013 (b) 09/09/2013 

(c) 10/09/2013 (d) 11/09/2013 

(e) 12/09/2013 (f) 13/09/2013 

Figure 1.2: Plot of genuine and fraudulent transaction between the 8th and 13th of 
September 2013 over the first two principal components. Frauds represent a small part 
of all observation available. The distribution of fraud and genuine sample be highly 

overlap and change over the time. 

1.5.1 Understanding sample method 

The first main contribution of this thesis be the formalization of sample method 

adopt in unbalanced classification tasks. In particular we focus on undersampling 

which be a standard technique for balance skewed distributions. Despite it popularity 

in the machine learn community, there be no detailed analysis about the impact of 

undersampling on the accuracy of the final classifier. Chapter 4 reveals the condition un- 

der which undersampling will improve the rank of the posterior probability return 

by a classifier. Then we propose a method to calibrate the posterior probability of a clas- 

sifier apply after undersampling. However, there be no guarantee that undersampling 



Chapter 1. Introduction 11 

be the best method to adopt for a give classifier and dataset, so we propose to use a 

race algorithm to adaptively select the best unbalanced strategy. 

1.5.2 Learning from evolve and unbalanced data stream 

The second main contribution be related to the non-stationary nature of transaction 

distribution in credit cards. The continuous evolution in the way fraudsters attempt 

and commit illegal activity require a continuous adaptation of the learn algorithm 

use to detect frauds. At the same time the distribution be highly skewed towards the 

genuine class, make standard technique for evolve data stream not suitable. In 

Chapter 5 we investigate multiple solution for unbalanced data stream with the goal of 

show in practice which strategy be the best to adopt for fraud detection. We show 

that retain historical transaction be useful, but it be also important to forget outdated 

sample for the model to be precise. Also, propagation of fraudulent instance along the 

stream be another effective technique to deal with such unbalanced data streams. When 

propagation of old transaction in the stream be not desire (e.g. for computational 

reason), we suggest to use an ensemble of Hellinger Distance Decision Tree (HDDT) a 

they show good performance than approach base on instance propagations. 

1.5.3 Formalization of a real-world Fraud Detection System 

In Chapter 6 we present a prototype of a FDS able to meet real-world work condi- 

tions, which be the third main contribution of the thesis. Most FDSs monitor stream of 

credit card transaction by mean of classifier return alert for the riskiest payments. 

Fraud Detection differs from conventional classification task because, in a first phase, 

only a small set of supervise sample be provide by human investigators. Labels of the 

vast majority of transaction be make available only several day later, when customer 

have possibly report unauthorized transactions. The delay in obtain accurate la- 

bel and the interaction between alert and supervise information have to be carefully 

take into consideration when learn in a concept-drifting environment. We show that 

investigator’s feedback and delayed label have to be handle separately and require 

different classification strategies. Finally, base on our results, we argue that the best 

detection solution consists in training two separate classifier (on feedback and delayed 

labels, respectively), and then aggregate the outcomes. 



Chapter 1. Introduction 12 

1.5.4 Software and Credit Card Fraud Detection Dataset 

Besides these main contributions, during this thesis we also developed a software package 

call unbalanced [24] available for the R language [25]. This package be present in 

Appendix A, it implement some of most well-known technique for unbalanced classi- 

fication and proposes a race algorithm [26] to select adaptively the most appropriate 

strategy for a give unbalanced task [27]. 

Another contribution of this thesis be the release to the public of a dataset contain 

information about credit card transaction with example of fraudulent samples. The 

dataset be first use in the experiment of our paper [28] and then make available 

at: http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata. It contains 31 

numerical input variables. Feature “Time” denotes the second elapse between each 

transaction and the first transaction in the dataset. The feature “Amount” be the trans- 

action amount, which can be use for example-dependent cost-sensitive learning. Feature 

“Class” be the response variable and it take value 1 in case of fraud and 0 otherwise. 

The dataset be highly unbalanced; fraud represent 0.172% of all transaction (492 fraud 

out of 284807 transactions). For confidentiality reason, the meaning of most variable be 

not reveal and the feature have be transform by mean of principal components. 

The cardholder identifier be also not available so each transaction can be consider in- 

dependent from the others. This be one of the rare datasets on fraud detection available 

to the community. 

1.6 Publications and research activity 

The list of work publish during this thesis be summarize below, by category and 

chronological order. 

Peer review international journal article 

• Andrea Dal Pozzolo, Giacomo Boracchi, Olivier Caelen, Cesare Alippi, and Gi- 
anluca Bontempi. Credit Card Fraud Detection with Alert-Feedback Interaction. 

Submitted to IEEE Transactions on Neural Networks and Learning Systems. 

• Andrea Dal Pozzolo, Olivier Caelen, Yann-Ael Le Borgne, Serge Waterschoot, and 
Gianluca Bontempi. Learned lesson in credit card fraud detection from a practi- 

tioner perspective. Expert Systems with Applications, 41(10):4915-4928, 2014. 

http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata 


Chapter 1. Introduction 13 

Peer review international conference paper 

• Andrea Dal Pozzolo, Olivier Caelen, and Gianluca Bontempi. When be under- 
sample effective in unbalanced classification tasks?. In European Conference on 

Machine Learning. ECML-KDD, 2015. 

• Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. 
Calibrating Probability with Undersampling for Unbalanced Classification. In Sym- 

posium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015. 

• Andrea Dal Pozzolo, Giacomo Boracchi, Olivier Caelen, Cesare Alippi, and Gian- 
luca Bontempi. Credit Card Fraud Detection and Concept-Drift Adaptation with 

Delayed Supervised Information. In Neural Networks (IJCNN), The 2015 Interna- 

tional Joint Conference on. IEEE, 2015. 

• Andrea Dal Pozzolo, Reid A. Johnson, Olivier Caelen, Serge Waterschoot, Nitesh 
V Chawla, and Gianluca Bontempi. Using HDDT to avoid instance propagation 

in unbalanced and evolve data streams. In Neural Networks (IJCNN), The 2014 

International Joint Conference on. IEEE, 2014. 

• Andrea Dal Pozzolo, Olivier Caelen, Serge Waterschoot, Gianluca Bontempi, Rac- 
ing for unbalanced method selection. Proceedings of the 14th International Con- 

ference on Intelligent Data Engineering and Automated Learning (IDEAL), IEEE, 

2013 

Peer review international conference poster 

• Andrea Dal Pozzolo, Olivier Caelen, and Gianluca Bontempi. Comparison of bal- 
ancing technique for unbalanced datasets. PhD Student Forum of the International 

Conference on Data Mining (ICDM), IEEE, 2012. 

Research visit and grant 

During my PhD I’ve visit the follow research groups: 

• Data, Inference Analytics, and Learning (DIAL) Lab at the University of Notre 
Dame, Indiana, USA in November 2013 and May 2014 (hosted by Prof. Nitesh V 

Chawla) 



Chapter 1. Introduction 14 

• Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB) at Politecnico 
di Milano, Italy in November 2014 and from April to May 2015 (hosted by Prof. 

Giacomo Boracchi) 

The first visit at DIAL lead to the conference paper at IJCNN 2014 [19] and the visit at 

DEIB to the paper at IJCNN 2015 [20]. At the begin of the PhD studies, I take also 

part to the organization of the International Conference in Data Mining (ICDM) 2012. 

In the last year of the PhD I receive two travel grants: i) ULB CCCI travel grant for 

the second research visit at DEIB (e 1500) and ii) INNS travel grant for the IJCNN 2015 

conference ($800). 

1.7 Financial support and project objective 

The work present in this thesis be fund by the Doctiris project title “Adaptive 

real time machine learn for credit card fraud detection”, support by Innoviris which 

be the Brussels institute for the encouragement of scientific research and innovation. The 

objective of the 4 year project be to design, ass and validate a machine learn 

framework able to calibrate in an automatic, real-time and adaptive manner the fraud 

detection strategy of the industrial partner. The academic partner be the Machine Learn- 

ing Group, from the Computer Science Department of the Université Libre de Bruxelles. 

The business partner be Worldline S.A., which be a company base in Brussels and leader 

in e-payment services. 

Figure 1.3: Partners of the Doctiris project “Adaptive real time machine learn for 
credit card fraud detection”. 

1.8 Outline 

The thesis be organize a follows: Chapter 2 introduces the notion and tool that 

will be consider in the thesis. The chapter be divide in two main parts, the first 

(Section 2.1) provide the reader preliminary knowledge about Machine Learning and 

the problem of classification. The second (Section 2.2) be devote to the problem of 

Fraud Detection (FD) and describes the main layer of a FDS. 



Chapter 1. Introduction 15 

Chapter 3 review the different approach which have be investigate in the litera- 

ture to address the challenge introduce in Section 1.4. Each section present relevant 

work that have be propose in the literature for solve different problem typical of 

Fraud Detection. Section 3.1 be dedicate to method for unbalanced classification task 

and Section 3.2 present solution to the problem of learn in non-stationary environ- 

ments. Section 3.3 review work on unbalanced data stream and Section 3.4 introduces 

algorithmic solution that have be propose for Fraud Detection. 

Chapter 4, 5 and 6 represent the main contribution of the thesis. In Chapter 4 we tackle 

the problem of learn in the presence of unbalanced class distribution with a focus on 

undersampling, how it works, how it can improve the performance of a classifier and 

to choose the best strategy for a give dataset. Then Chapter 5 provide the reader 

with an experimental comparison of several method for unbalanced data streams. The 

first part (Section 5.1) investigates which solution work best on a credit card data 

stream, while the second (Section 5.2) illustrates an algorithm for avoid propagation 

of instance along the stream. Chapter 6 present a prototype of a realistic FDS, where 

the interaction between the classifier generate the alert and investigator provide 

feedback be model and exploit to improve the detection. 

Finally, Chapter 7 provide a summary of the result present in the thesis and proposes 

future research directions. The remainder of the present chapter provide the notation 

that will be use throughout the follow chapters. 

1.9 Notation 

Throughout the thesis, random variable be write with boldface letter (y), their 

realization be in normal font (y) and estimation wear a hat (ŷ). P(y) denotes the 
probability distribution of the random variable y. Uppercase letter (Y) be use for 

set and uppercase calligraphic font (L) for learn algorithms. 



Chapter 1. Introduction 16 

Table 1.1: Probability Notation 

P(y) probability that the discrete random variable y take the value y, i.e., P(y = y) 
P(y) probability mass distribution of the random variable y 
P(y|x) the conditional probability that y = y know that x = x 
P(y|x) the conditional distribution function of the target y know that x = x 
E[y] expect value of y. 
V ar[y] variance of y. 
H(y) entropy of y 
H(y|x) conditional entropy of y give x 
I(y;x) mutual information of y and x 
|X| the number of element of the set X 
ŷ statistical estimation of y 
P̂(y|x) estimation of P(y|x) 

Table 1.2: Learning Theory Notation 

L Learning algorithm 
y ∈ Y unidimensional discrete random variable represent the output of L 
Y ⊂ R the domain of the random variable y 
y a realization of the random variable y 
x ∈ X a multidimensional discrete random variable represent the input of L 
X ⊂ Rn the domain of the random variable x 
n dimension of the input space X 
x a realization of x 
TN the training set, compose of N realization (xi, yi), i ∈ {1, 2, ..., N} 
N number of sample in the training set 
xi the ith realization of the variable x in TN 
Λ ⊂ Rd Model parameter space. 
θ ∈ Λ Model parameter vector. 
Λθ Class of model with parameter θ. 
h(x, θ) ∈ Λθ Prediction model with input x and parameter θ. 
L(y, h(x, θ)) Loss function. 
Remp(θ) Empirical risk. 
GN Generalization error. 

Table 1.3: Fraud Detection Notation. 

k Number of alert that investigator be able to validate everyday. 
At Alerts raise at day t, where |At| = k. 
Pk(t) Precision on At. 
Dt−δ Delayed supervise couple available at day t and occur at t− δ. 
Ft Feedbacks at t: recent supervise couple available at day t. 
Bt Batch of transaction available at day t. 
δ Number of day for which feedback be provided. 
M Number of day for which delayed sample be provided. 



Chapter 2 

Preliminaries 

Machine Learning play an important role in Data Driven Fraud Detection System, so 

in this chapter we will first introduce the reader to the problem of learn from data 

before move to the specific application domain. In particular we will focus on the 

problem of learn with supervise data, a.k.a. supervise learning, where the learn 

algorithm be train on label examples. Section 2.1 introduces the basic of learn 

from annotate data that we will late build upon in future chapters. Section 2.2 be 

devote to the specificity of the Fraud Detection application. 

2.1 Machine Learning 

Machine Learning play a key role in many scientific discipline and it application be 

part of our daily life. It be use for example to filter spam email, for weather predic- 

tion, in medical diagnosis, product recommendation, face detection, fraud detection, etc. 

Machine Learning (ML) study the problem of learning, which can be define a the 

problem of acquire knowledge through experience. This process typically involves ob- 

serve a phenomenon and construct a hypothesis on that phenomenon that will allow 

one to make prediction or, more in general, to take rational actions. For computers, the 

experience or the phenomenon to learn be give by the data, hence we can define ML a 

the process of extract knowledge from data [29]. 

Machine learn be closely related to the field of Statistics, Pattern Recognition and 

Data Mining [7]. At the same time, it emerges a a subfield of computer science and 

give special attention to the algorithmic part of the knowledge extraction process. In 

summary, the focus of ML be on algorithm that be able to learn automatically the 

pattern hidden in the data. 

17 



Chapter 2. Preliminaries 18 

This thesis be about supervise learning, where ML algorithm be train on some 

annotate data (the training set) to build predictive models, or learners, which will enable 

u to predict the output of new unseen observations. It be call supervise because the 

learn process be do under the supervision of an output variable, in contrast with 

unsupervised learn where the response variable be not available. 

Supervised learn assumes the availability of label samples, i.e. observation anno- 

tat with their output, which can be use to train a learner. In the training set we can 

distinguish between input feature and an output variable that be assume to be depen- 

dent on the inputs. The output, or response variable, defines the class of observation 

and the input feature be the set of variable that have some influence on the output 

and be use to predict the value of the response variable. Depending on the type of 

output variable we can distinguish between two type of supervise task: i) classification 

and ii) regression. The first assumes a categorical output, while the latter a continuous 

one. Fraud detection belongs to the first type since observation be transaction that 

can be either genuine or fraudulent, while in other problem such a stock price predic- 

tions the response be a continuous variable. On the other hand, in both classification and 

regression tasks, input feature can include both quantitative and qualitative variables. 

In this thesis we will also refer to qualitative variable a categorical, discrete and factor 

variables. 

2.1.1 Formalization of supervise learn 

Let x be the realization of a input random vector x define in the input domain X, and 

y denote the output value of the response variable y, take it value in the output 

domain Y. In supervise learn we have four main player [30]: 

• G, the generator of input vector x ∈ X ⊂ Rn from an unknown probability 
distribution Fx(x). 

• S, the supervisor which assigns to every input x an output value y ∈ Y ⊂ R 
accord to the conditional probability Fy(y|x). 

• T, the training sample which be Independent and Identically Distributed (IID) 
sample (x, y) drawn from the joint distribution Fx,y(x, y). 

• LM, the learn machine that, give some training sample T, return a class of 
parametric function (or hypothesis) h(x, θ), θ ∈ Λ, estimate S for an input x and 
define a mapping between the input and output domains, where Λ defines the 

domain for the parameter θ. 



Chapter 2. Preliminaries 19 

Supervisor* yx 

ŷ 

Generator* 

Training*samples* 

FX (x) FY (y | x) 

FXY (x, y) 

h(x,θ ) 

Learning*Machine* 

Figure 2.1: The player of supervise learning. A generator return an input vector 
x accord to Fx(x) and a supervisor associate an output value y to x on the basis 
of Fy(y|x). A collection of input and output value defines a training set accord 
to Fx,y(x, y). The learn machine produce an hypothesis h(x, θ) on the basis of a 

training set and predicts the output ŷ for a new observation. 

The relationship that link the input x to the output y, denote a Fy(y|x), be define 
by S and it can be express a a function of the inputs, namely f(x) and know a 

target operator, where y = f(x) + � with � be some noise. In general Fx(x), Fy(y|x) 
and Fx,y(x, y) be unknown, but some training sample T be available for LM. On the 

basis of T it be possible to learn a LM that approximate Fy(y|x). A model or learner L 
be define a one of the many hypothesis h(x, θ) in the parameter space Λ ⊂ Rd. It be 
generate by LM, and associate to each input vector x an output value or prediction 

ŷ = h(x, θ), with ŷ ∈ Y. When Y ∈ R we have a regression task, while for classification 
Y be a set of class or categories. In particular, this thesis will be concerned with binary 

classification task in which Y ∈ {+,−}, where the positive output will be also cod 
a 1 and the negative a 0. With respect to fraud detection, positive observation will 

denote fraudulent transaction and negative genuine (legitimate) ones. 

LM explores multiple nest class of hypothesis Λ1 ⊂ Λ2 . . . ⊂ Λq ⊂ Λ for a give T 
and generates different hypothesis by set distinct parameter θ in h(x, θ). In order 

to find the optimal hypothesis h(x, θ∗) we have first to define a loss function L(y, ŷ) that 

measure the discrepancy between ŷ and y, where ŷ = h(x, θ). Given a loss function 

we can then define a risk function R(θ) that measure how well a hypothesis h(x, θ) 

approximates f(x) over the X×Y domain. 

R(θ) = 

∫ 
X,Y 

L(y, ŷ)dFy(y|x)dFx(x) (2.1) 



Chapter 2. Preliminaries 20 

The optimal model h(x, θ∗) be define a the hypothesis have the parameter θ∗ mini- 

mizing R(θ): 

θ∗ = arg min 
θ∈Λ 

R(θ) (2.2) 

The mapping f∗ : X→ Y define by h(x, θ∗) be also know a target function. A training 
set TN = {(xi, yi), i = 1, . . . , N} be define a a collection of N sample from T. For a 
give training set TN we can compute only what be call the empirical risk (Remp) and 

the hypothesis minimize this quantity may be different from h(x, θ∗). In the Empirical 

Risk Minimization procedure [30] the parameter θ̂ be identify by minimize Remp(θ): 

Remp(θ) = 
1 

N 

N∑ 
i=1 

L(yi, h(xi, θ)) (2.3) 

θ̂ = arg min 
θ∈Λ 

Remp(θ) (2.4) 

It be important to stress that the same hypothesis h(x, θ̂) achieves different Remp(θ̂) when 

the training set change (θ̂ depends on TN ), and the training set itself can be see a a 

realization of a random variable TN . For this reason the accuracy of a learn algorithm 

be often measure use the generalization error (GN ), which be the mean of Remp(θ̂) over 

all possible training sets: 

GN = ETN [Remp(θ̂)] (2.5) 

The choice of class of hypothesis have a great impact on the error GN that a learn 

algorithm L be able to achieve. When the model be too simple h(x, θ̂) could poorly 
approximate the target operator f(x) on the training set (high Remp(θ̂)), but be 

general enough to fit well multiple datasets, while a too complex model could fit perfectly 

the training set (low Remp(θ̂)), but have poor generalization on new datasets. This 

interaction can lead to the problem of underfitting or overfitting and it be often refer 

to a the Bias-Variance trade-off of a model [31]. 

The bias of a model be the error from erroneous assumption in the learn algorithm, and 

the variance be the error from sensitivity to small fluctuation in the training set (formal 

definition of bias and variance available in Appendix C). Section 2.1.3 will illustrate how 

GN have both a bias and a variance component. It be commonly say that a hypothesis 

underfits the data when it have large bias but low variance, while it overfits the data 

when it have low bias but large variance. In both cases, the hypothesis give a poor 

representation of the target and a reasonable trade-off need to be found. On the basis 

of the available training set, the goal of the practitioner be then to search for the optimal 

trade-off between the variance and the bias terms. 



Chapter 2. Preliminaries 21 

Typically, the supervise learn procedure minimizes the error GN by mean of a two- 

step nest search process [32]. The inner process be know a parametric identification, 

and the outer a structural identification. The parametric identification step considers a 

single class of hypothesis Λj , with j = 1, . . . , q, and selects a hypothesis h(·, θjN ) with 
θjN ∈ Λj by mean of a learn algorithm L. The role of the learn algorithm be to 
find the model that minimizes the empirical error on a give training set. Then h(·, θjN ) 
be select a the one minimize an estimation of the generalization error ĜjN by mean 

of a validation technique (e.g. cross validation or bootstrap).1 

θ̂jN = arg min 
θ̂N∈Λj 

ĜjN (2.6) 

The structural identification step range over different class of hypothesis Λj , 1 ≤ j ≤ q 
and call for each of them the parametric routine which return the vector θ̂jN . The final 

model to be use for prediction be select in the set {θ̂jN , j = 1, . . . , q}, a the one have 
the small generalization error across all the class of hypotheses. This last process be 

call model selection. 

2.1.2 The problem of classification 

A classification task can be explain a the problem of define the association between a 

categorical dependent variable and independent variable that can take either continuous 

or discrete values. As mention in the previous section, the dependent variable y ∈ Y 
be allow to take value among a small set of K possible class Y = {c1, . . . , cK} and 
the input/output value of a sample (x, y) be drawn from the joint distribution Fx,y. A 

classifier K be a hypothesis h(x, θ) that return ŷ = ĉ ∈ Y. 

In general we can identify three alternative approach to solve the classification problem: 

• Estimate the posterior probability P(y = ck|x = x), and then subsequently use 
decision theory to assign a class ŷ to an input value x. 

• Estimate the class-conditional probability P(x = x|y = ck) and prior probability 
P(y = ck) separately and compute posterior probability use Bayes’ theorem: 

P(y = ck|x = x) = 
P(x = x|y = ck)P(y = ck)∑K 
k=1 P(x = x|y = ck)P(y = ck) 

Then use decision theory to assign each new x to one of the classes. 
1Note that we can compute only an estimation of GN , because typically we have access only to a 

sample of all possible datasets of size N . 



Chapter 2. Preliminaries 22 

• Find a function g(x), know a discriminant function [7], which map each input 
x directly onto a class label ŷ (no probability estimate needed). 

Approaches that model the posterior probability directly be call discriminative mod- 

els, while a generative approach model the class-conditional and prior probability for 

each class, and then calculates posterior probability use Bayes’ theorem. 

In the case of binary classification K = 2 and a popular loss function be the zero-one loss 

L0/1 which assigns loss equal to one in case of wrong prediction and zero otherwise: 

L0/1 : Y×Y → {0, 1} 

y, ŷ 7→ 
{ 

0 if y 6= ŷ 
1 if y = ŷ 

(2.7) 

However, in many application the cost of misclassification be class dependent. For ex- 

ample in cancer treatment, the cost of not predict correctly a sick patient (a False 

Negative) be much high than make a wrong prediction when the patient be healthy 

(a False Positive). Assume Y = {1, 0}, where 1 denotes positive instance and 0 nega- 
tive ones. Let li,j be the loss (cost) incur in decide i when the true class be j and 

p = P(y = 1|x = x). We can define the risk of predict an instance a positive or 
negative a follows: 

r+ = (1− p)l1,0 + pl1,1 

r− = (1− p)l0,0 + pl0,1 

Standard decision-making process base on decision theory [7, 33, 34] defines the optimal 

class of a sample a the one minimize the risk (expected value of the loss function). 

Bayes decision rule for minimize the risk can be state a follows: assign the positive 

class to sample with r+ ≤ r−, and the negative class otherwise. 

ŷ = 

{ 
1 if r+ ≤ r− 

0 if r+ > r− 
(2.8) 

As show by Elkan [35], (2.8) be equivalent to predict a sample a positive when p > τ 

and the threshold τ is: 

τ = 
l1,0 − l0,0 

l1,0 − l0,0 + l0,1 − l1,1 
(2.9) 

Typically, the cost of a correct prediction be zero, hence l0,0 = 0 and l1,1 = 0. If we set 

l1,0 = l0,1 = 1 we obtain L0/1 and τ = 0.5. 



Chapter 2. Preliminaries 23 

2.1.3 Bias-variance decomposition 

As show in the previous section, a binary classification problem can be solve by esti- 

mating the posterior probability and then use a threshold τ to define the class. Since 

the posterior probability be a continuous value, the probability estimation problem can 

be treat a a regression problem. In regression, the unknown conditional distribution 

Fy(y|x) be typically estimate use the conditional expectation E[y|x] [30]. In the case 
of binary classification with Y = {1, 0} we can write: 

E[y|x] = 1 · P(y = 1|x) + 0 · P(y = 0|x) = P(y = 1|x) 

In this way, the classification problem can be see a a regression problem where the 

output take value in {0, 1}. Let p = P(y = 1|x = x) and p̂ it approximation give 
by the estimator p̂. A common choice of loss function in regression be the quadratic loss 

give by L(p, p̂) = (p − p̂)2. In the case of the quadratic loss function, it can be show 
that the generalization error GN can be decompose into a bias and a variance term [36]. 

GN = ETN [(p− p̂)2] = Bias[p̂]2 + V ar[p̂] (2.10) 

where Bias[p̂] = ETN [p̂]− p and V ar[p̂] = ETN [(p̂−ETN [p̂])2] (derivation available in 
Appendix C). The first term Bias[p̂]2 measure the “bias” of the model, it give an idea 

of how far the average of our estimator p̂ be from the true value p. The “variance” term 

(V ar[p̂]) quantifies how much p̂ vary around it mean for different training set TN . It 

therefore quantifies the sensitivity of the model prediction for a give training set. The 

variance be independent of the true probability p, and be null for the classifier that have 

the same prediction across all training datasets. Domingos [37] provide a more general 

bias-variance decomposition that work also for the L0/1. 

Typically, complex model have high variance and a low bias while simple model have 

low variance and a high bias. This be because complex model can approximate well the 

target function (low bias), but be highly affected by the variability of the training set 

(leading to high variance). The opposite occurs with simple models. Simple learner can 

show good performance than more complex one (e.g., [38, 39]). This be because both 

bias and variance influence the predictive error. The optimal trade-off between bias and 

variance be often domain or application specific. Several work have found that ensemble 

of model very often outperform a single model (e.g., [40]), because average multiple 

model often (though not always) reduces the variance of single models. 

Well-known ensemble method be bagging [41] and boost [42]. In bagging multiple 

hypothesis be construct by use different bootstrap sample of the original data 

set. The result model be then combine into an ensemble to make predictions. 



Chapter 2. Preliminaries 24 

Breiman [41] show that bagging allows one to transform an unbiased classifier into 

a nearly optimal one by reduce the variance term of (2.10). In the case of boosting, 

weak learner be combine to obtain accurate prediction [42] and the improvement in 

the generalization error over a single classifier can also be related to the bias-variance 

decomposition by introduce the notion of a margin [37], which measure the confidence 

in the prediction of the ensemble. 

A powerful ensemble algorithm base on bagging be Random Forests [43]. Random 

Forest (RF) be an ensemble of decision trees, where each tree be train on different 

bootstrap sample of the original training set and us a random subset of all the feature 

available. This return a forest of decision tree that be very different from each other. 

Diversity in the model generate an ensemble be a key factor for variance reduction [44]. 

2.1.4 Evaluation of a classification problem 

In a classification problem an algorithm be assess on it overall accuracy to predict 

the correct class of new unseen observation and usually ĜN be assess in term of 

Mean Misclassification Error (MME). Let Y1 be the set of positive instances, Y0 the 

set of negative instances, Ŷ1 the set of instance predict a positive and Ŷ0 the one 

predict a negative. For a binary classification problem it be conventional to define a 

confusion matrix (Table 2.1). 

Y1 Y0 
Ŷ1 TP FP 
Ŷ0 FN TN 

Table 2.1: Confusion Matrix. 

In the case of zero-one loss we can then define MME = |FP∧FN|N , where operator |·| defines 
the cardinality of a set and N the size of the dataset. In the follow we will write FP 

to denote |FP| and similarly for all the other entry of the confusion matrix. From MME 
we can then define the accuracy of a model a 1 - MME. However, in some situations, the 

accuracy be not a good measure of performance, especially in unbalanced classification 

problem where one class be much more frequent than the other [45, 46]. For example 

consider the case where we have N = 10000 transaction and only 1% be fraudulent 

(100 positives). Predicting all transaction a genuine (negative) would return to an 

accuracy of 0.99, but we would miss to detect all fraudulent cases. 

Other classification measure base on the confusion matrix be [47, 48]: 



Chapter 2. Preliminaries 25 

• Precision: TPTP+FP 

• Recall: TPTP+FN , also call True Positive Rate (TPR), Sensitivity or Hit rate 

• True Negative Rate (TNR): TNFP+TN , also call Specificity 

• False Positive Rate (FPR): FPFP+TN 

• False Negative Rate (FNR): FNTP+FN 

• Balanced Error Rate (BER): 0.5( FPTN+FP + FNFN+TP ) 

• G-mean: √Sensitivity× Specificity 

• F-measure: 2Precision×RecallPrecision+Recall , also call F-score or F1. 

In an unbalanced classification problem, it be also well know that quantity like TPR 

and TNR be mislead assessment measure [49]. Imagine a classifier with TPR=99% 

and TNR=99% (Table 2.2), if we have 1% of positive samples, then Precision be only 0.5. 

Even worse, if we have only 0.1% of positives, then Precision be 0.09. 

(a) Class percentage 

Y1 Y0 
Ŷ1 99% 1% 
Ŷ0 1% 99% 

(b) Frequency 

Y1 Y0 
Ŷ1 99 99 
Ŷ0 1 9801 

100 9900 

Table 2.2: Confusion Matrix of a classifier with TPR=99% and TNR=99% when we 
have 100 positive instance in 10000 observations. 

BER may be inappropriate too because of different cost of misclassification false nega- 

tives and false positives. Precision and Recall have opposite behavior, have high Pre- 

cision mean bad Recall and vice versa. F-measure give equal importance to Precision 

and Recall into a metric range between 0 and 1 (the high the better). F-measure and 

G-mean be often consider to be relevant measure in unbalanced problem [21, 50–52]. 

In general these measure can be compute only once a confusion matrix be available, 

which mean that their value depend on the threshold use for classification define 

by (2.9). Changing the threshold corresponds to use different misclassification costs. 

The Receiving Operating Characteristic (ROC) curve [53, 54] be a well-know assessment 

technique that allows evaluate the performance of a classifier over a range of different 

thresholds. It be obtain by plot TPR against FPR (see Figure 2.2), where each 

point of the curve corresponds to a different classification threshold. A classifier K be say 
to be more accurate than a classifier W in the ROC space only if the curve of K always 
dominates the curve of W. The best classifier corresponds to the point (0,1) in the ROC 



Chapter 2. Preliminaries 26 

FPR 

T 
P 

R 
0. 

0 
0. 

2 
0. 

4 
0. 

6 
0. 

8 
1. 

0 

0.0 0.2 0.4 0.6 0.8 1.0 

K 
W 

Figure 2.2: The Receiving Operating Characteristic (ROC) curve for two classifier 
K and W. The gray line represent the performance of a random model. 

space (no false negative and no false positives), while a classifier predict at random 

would have performance along the diagonal connect the bottom left corner to the 

top right. When there be not a clear winner (e.g. classifier K dominates W only in one 
part of the ROC space), the comparison be usually do by calculate the Area Under 

the ROC Curve (AUC). AUC be also a well-accepted measure for unbalanced datasets 

and it have become the de facto standard in classification [19, 51, 55–57]. 

However, AUC be insensitive to class prior since both TPR and FPR do not change 

with a different class ratio. Precision-Recall (PR) curve be instead sensitive to change 

in the class distribution, but there be there a strong connection between ROC and PR 

curves. A curve dominates in ROC space if and only if it dominates in PR space, and 

algorithm that optimize the area under PR curve be guaranteed to optimize the area 

under ROC curve [58]. 

When evaluate the output of a classifier it be also important to ass the quality of the 

estimate probability [59]. A well-known measure of quality be Brier Score (BS) [60]. 

BS be a measure of average square loss between the estimate probability and the 

actual class value. It allows evaluate how well probability be calibrated, the low 

the BS the more accurate be the probabilistic prediction of a model. Let P̂(yi|xi) be 
the probability estimate of sample xi to have class yi ∈ {1, 0}, BS be define as: 

BS = 
1 

N 

N∑ 
i=1 

[yi − P̂(yi|xi)]2 (2.11) 



Chapter 2. Preliminaries 27 

2.2 Credit Card Fraud Detection 

2.2.1 Fraud Detection System work condition 

In this section we describe the work condition of a real-world Fraud Detection Sys- 

tem (FDS). Figure 2.3 describes the hierarchy of layer in a FDS, each control 

whether the transaction be genuine or should be rather report a a fraud: i) Terminal, 

ii) Transaction Blocking Rules, iii) Scoring Rules, iv) Data Driven Model, v) Investiga- 

tors. The first four element of the FDS be fully automatized, while the last one require 

human intervention and it be the only non-automatic and offline part of the FDS. Auto- 

matic tool have to decide whether the transaction request (or the transaction attempt) 

have to be approve in Real Time (i.e., decision have to be take immediately) or in Near 

Real Time (i.e. decision can be take in a short time). Blocking and score rule be 

Expert Driven Rules (EDR), i.e. rule design by investigator base upon their expe- 

rience. On the contrary, the DDM us annotate transaction a source of information 

to extract knowledge about fraudulent and genuine patterns. In the follow we will 

explain the role played by each component of the FDS. 
Transaction Request 

Transaction Attem 
pt 

Real Time 

Transaction Denied 

Correct PIN? 
Suf!cient Balance? 

Active Account? 

InvestigatorsTerminal Blocking 
Rules 

Data Driven 
Model 

Scoring 
Rules 

Transaction Denied 

Near Real Time Of"ine 

Automatic Tools 

Authorized Transaction 

Alerts 

Feedback 

Human Supervision 

FDS part we ModelExpert Driven Data Driven 

Transaction 

Figure 2.3: The layer of a FDS. In this thesis we focus only on the data driven part. 



Chapter 2. Preliminaries 28 

2.2.1.1 FDS Layers: 

The Terminal represent the first layer of conventional security check in a FDS. At this 

stage security check such a correct PIN code, number of attempts, status of the card 

(active or blocked), sufficient balance and expenditure limit provide a first filter of the 

transaction that can get through [61]. These condition be evaluate in real time and 

if any of them be not satisfied, the transaction be denied. All transaction passing this 

first filter raise a transaction request to the second layer control by the block rules. 

Transaction Blocking Rules be design by experienced investigator and they can block 

a transaction request before it be authorized. These rule operate in Real Time, and be 

very precise, they deny a transaction request only if it clearly represent a fraud attempt.2 

Blocking Rules be if-then (-else) rule that associate the class fraud or genuine to a 

transaction when specific condition be met. An example can be the following: “IF a 

PIN-based transaction be do in Europe AND in Asia within 5 min from the same 

cardholder THEN deny the transaction”, since the customer cannot physically be in 

Europe and Asia over a short time interval.3 Transaction request passing Blocking 

Rules be authorize and become transaction (the payment be executed). Before each 

authorize transaction be sent to the successive security layer of FDS it be enrich with 

aggregate information (e.g. average expenditure, number of transaction in the same 

day of the same cardholder, etc.). The result feature vector describe the transaction 

be then analyze by both Scoring Rules and DDMs. These control be perform in Near 

Real Time (typically less than 6 [61]) since there be no need to provide an immediate 

response give that the transaction have be already authorized. 

Scoring Rules be EDR define by investigator base upon their experience that act like 

a rule-based classifier. They assign a score to each authorize transaction: the large the 

score, the more likely the transaction be to be a fraud. In practice score rule contain 

simple, human-understandable condition (e.g. IF Internet transaction in fiscal paradise 

and amount > 1000$, THEN fraud score = 0.9) and a such can be easily design by 

investigator or other experts. These rule be easy to understand and fast to deploy, but 

require manual revision when their performance drops. As a consequence, these rule 

can be expensive to maintain. 

Data Driven Model (DDM) layer relies on a predictive model to assign a fraud score to 

each transaction. Usually this phase us ML algorithm that return for each transac- 

tion an estimate of the probability to be a fraud. These algorithm can learn complex 

correlation in the data use large volume of data with high dimensionality. They be 
2Cards should be block only if there be a high risk of frauds, because it prevents the cardholder to 

make any future payments. 
3Note that this be just an illustrative example, block rule be confidential information. 



Chapter 2. Preliminaries 29 

usually more robust than EDR, but most of them be black box, i.e. it be not possible 

to convert them into rule that be easy to interpret. DDMs be able to consider all 

the information associate to a transaction, while EDR return condition base on few 

feature of the transactions. 

Investigators be fraud expert that check fraud alerts, i.e. transaction that have re- 

ceived a high fraud score by either EDR or DDM. These suspicious transaction be 

displayed in a Case Management Tool (CMT) where investigator can see the associate 

fraud score, check where they come from (EDR or DDM) and annotate them a genuine 

or fraud after verification. In a realistic scenario only few alert can be checked give 

a limited number of investigator [62]. For this reason, they typically investigate only 

transaction with the high fraud score. The role of investigator be to contact the 

cardholder, check if the transaction be fraudulent or not and annotate it with it correct 

label. This process can be long, tedious, and at the same time the number of transaction 

to check be usually very large, so it be important to minimize false alerts. The label 

process generates annotate transaction that be use by the DDM. In the follow 

we use the term feedback to refer to these label transactions. 

2.2.1.2 Supervised Information 

Investigators’ feedback be the most recent supervise information available to the FDS 

and they represent a tiny part of all transaction process everyday [20]. Additional 

label transaction be provide by cardholder reporting unauthorized transaction [20, 

61]. However, the number of customer reporting fraud not detect by the FDS be 

usually small and hard to model, since cardholder have different habit when it come 

to check the transcript of credit card transaction give by the bank. We do not know 

the class of all the other transaction that have not be checked, they can be either 

genuine or fraud miss by the FDS and ignore by the cardholders. However, after 

a certain time period when no further miss fraud be reported, we can assume that 

unchecked transaction be genuine. These transaction can then be use to train a new 

DDM. 

2.2.1.3 System Update 

Expert driven system be update manually, while the data-driven component be up- 

date automatically. Alerts generate today define the feedback that will be use to 

train an algorithm that detects the fraud of tomorrow. This mean that Alert-Feedback 

Interaction (AFI) governs the type of information available to the algorithm that regu- 

lates the data driven phase. 



Chapter 2. Preliminaries 30 

Typically, DDMs require a large set of sample to train an accurate model, so the ML 

algorithm use in the data driven layer be train at the end of the day, when a 

sufficient batch of feedback be available. Feedbacks of alert transaction be give 

by investigator during the day, but only at the end of the day we have all the feedback 

for the alert generate by the FDS. Therefore, the DDM be usually update everyday 

at midnight and the detection model be then apply to all the transaction occur in 

the next days. 

The focus of the thesis be to improve the DDM part of the FDS and to model the 

interaction between Data Driven Methods base on ML and investigators. In particular, 

we will consider a scenario where the DDM be the only element of the FDS responsible for 

the alert give to fraud expert and the algorithm be able to learn from the feedback 

provided. In the remainder of the thesis we will use the term FDS to indicate only the 

FDS layer we model: DDM and investigators. 

2.2.2 Features augmentation 

When a transaction be authorized, it be enter in a database contain few variable 

such a cardholder ID (CARD_ID), amount, datetime, merchant, etc. Starting from 

these feature it be possible to compute new variable that can describe the cardholder 

behaviour. This process be call feature augmentation, because the ultimate goal be to 

add to the original feature new informative features. Standard feature augmentation 

consists into compute variable such a average expenditure of the cardholder in the 

last week/month, number of transaction in the same shop, number of daily transactions, 

etc. [18, 62–65]. Van Vlasselaer et al. [61] use also network feature to consider the role 

of the cardholder in the network of transactions. 

These new feature be also call aggregate feature a they provide an aggregate 

view of the cardholder behaviour over a certain time period. It be important to include 

aggregate feature for the DDM to learn the behavior of the customer over time and 

detect anomalous transaction w.r.t. typical customer usage of the credit card. Trans- 

action aggregation be computationally expensive; therefore aggregate feature be often 

compute for each cardholder offline use historical transaction (e.g. transaction of 

the same cardholder in the previous month). Once aggregate variable be available they 

be merge with the original feature to define a feature vector describe the transaction 

that be then use to train the DDM. After transaction authorization and feature aug- 

mentation, each feature vector be score by both DDMs and Scoring Rules which operate 

in Near-Real Time. 



Chapter 2. Preliminaries 31 

In a test environment, we should remove CARD_ID from the feature vector. A model 

that receives a input the variable CARD_ID may have performance too optimistic on 

future fraud from the same cardholder if not remove from the dataset. On the contrary, 

in a real work environment, a card be block after the first fraud be found, so we cannot 

see future transaction from the same card. To replicate the real set we could remove 

all the transaction of the compromise card, but this would reduce the number of fraud 

available for test our algorithm, worsen the class unbalance problem in our dataset. 

Another option be to remove the CARD_ID variable from the feature vector. In this 

case, each fraud can be treat independently and the FDS cannot leverage CARD_ID 

for detect frauds. Note that CARD_ID be use to compute aggregate variables, so 

we can safely remove CARD_ID only once the new variable be include in the feature 

vectors. These new feature contain information about the cardholder, so there be not 

loss of information in remove CARD_ID after feature augmentation. 

2.2.3 Accuracy measure of a Fraud Detection System 

Choosing a good performance measure be not a trivial task in the case of fraud detection. 

Among others, fraud detection must deal with the follow challenges: i) unbalanced 

class sizes, ii) cost structure of the problem (the cost of a fraud be not easy to define), 

iii) time to detection (a card should be block a soon a it be found victim of fraud, 

quick reaction to the appearance of the first can prevent other frauds), iv) error in class 

label (quantify unreported frauds), v) reputation’s cost for the company, etc. 

As already mention in Section 2.1.4, standard classification measure such a MME, 

BER, TPR and TNR be mislead assessment measure in unbalanced class prob- 

lem [49]. A well-accepted measure for unbalanced classification be the Area Under the 

ROC Curve (AUC) [56]. This metric give a measure of how much the ROC curve be 

close to the point of perfect classification. Hand [66] considers standard calculation of 

the AUC a inappropriate, since this translates into make an average of different mis- 

classification cost for the two classes. An alternative way of estimate AUC be base on 

the use of the Mann–Whitney statistic and consists in rank the transaction accord 

to the posterior probability to be fraudulent and measure the likelihood that a fraud 

rank high than a genuine transaction [67]. By adopt the rank-based formulation of 

AUC we can avoid the problem raise by Hand of use different probability thresholds. 

In many FDS (e.g. [68–70]), cost-based measure be define to quantify the monetary 

loss due to fraud [71] by mean of a cost-matrix that associate a cost to each entry of 

the confusion matrix. Elkan [35] claim that it be safer to ass cost-sensitive problem 

in term of benefit (inverse of cost), since there be the risk of use different baseline 



Chapter 2. Preliminaries 32 

when use a cost-matrix to measure overall cost. To avoid this problem, normalize 

cost or saving [70] be use to ass the performance w.r.t. the maximum loss. When 

define a cost measure, one could consider the cost of a FN fix or dependent on the 

transaction amount. In the first case each fraud be equally costly, while in the latter the 

cost be example dependent. An argument for use fix cost be to give equal importance 

to small and large fraud (fraudsters usually test a card with small amounts), while 

transaction-dependent cost allows one to quantify the real loss that a company have to 

face. 

In the transaction-dependent case, the cost of a miss fraud (FN) be often assume 

to be equal to the transaction amount [35, 71], because it have to be reimburse to the 

customer. Cost of correct or false alert be consider to be equivalent to the cost of a 

phone call, because the investigator make a phone call to the cardholder to verify if it be 

the case of a false alert or a real fraud. The cost of a phone call be negligible compare to 

the loss that occurs in case of a fraud. However, when the number of false alert be too 

large or the card be block by error, the impossibility to make transaction can translate 

into big loss for the customer. 

The cost should also include the time take by the detection system to react. The 

shorter be the reaction time, the large be the number of fraud that it be possible to 

prevent. Typically, once fraudsters successfully perpetuate a fraud, then they try to 

spend all the money available on the card. As a consequence, when evaluate a FDS 

we should also consider the spending limit of each card: i.e. detect a fraud on a 

card have a large spending limit (e.g. corporate cards) result in high saving than 

detect a fraud on a card have a small spending limit [72]. For all these reasons, 

define a cost measure be a challenge problem in credit card detection and there be not 

agreement on which be the right way to measure the cost of frauds. 

The performance of a detection task (like fraud detection) be not necessarily well de- 

scribed in term of classification [73]. In a detection problem what matter most be 

whether the algorithm can rank the few useful item (e.g. frauds) ahead of the rest. 

In a scenario with limited resources, fraud investigator cannot revise all transaction 

marked a fraudulent from a classification algorithm. They have to put their effort into 

investigate transaction with the high risk of fraud, which mean that the detection 

system be ask to return the transaction ranked by their posterior fraud probability. 

The goal then be not to predict accurately each class, but to return a correct rank of the 

minority classes. 

In this context a good detection algorithm should be able to give a high rank to relevant 

item (frauds) and low score to non-relevant. A well-known detection measure be Average 

Precision (AP) [73]. Let N+ be the number of positive (fraud) case in the original dataset 



Chapter 2. Preliminaries 33 

and TPk be the number of true positive in the first k ranked transaction (TPk ≤ k). 
Let u denote Precision and Recall at k a Pk = TPkk and Rk = 

TPk 
N+ 

. We can then define 

Average Precision as: 

AP = 
N∑ 
k=1 

Pk(Rk − Rk−1) (2.12) 

where N be the total number of observation in the dataset. The good the rank, the 

great the AP and the optimal algorithm that rank all the fraud ahead of the legiti- 

mate have AP = 1. Note that AP be also an estimate of the area under the Precision-Recall 

curve [74]. 

As explain in Section 2.2.1, each time a fraud alert be generate by the detection system, 

it have to be checked by investigator before proceed with action (e.g. customer 

contact or card stop). Given the limited number of investigators, it be crucial to have the 

best rank within the maximum number k of alert that they can investigate. In this 

set it be important to have the high Precision within the first k alerts, namely Pk. 

Note that, while AUC and AP give a measure of the quality of the rank on the whole 

datasets, Pk focus only on a subset of k transactions. Precision at k (Pk) be also call 

alert Precision and it have emerge a a standard accuracy measure of FDSs [18, 20, 64]. 

Assessment of two FDSs (e.g. exist versus new version) can hide undesired bias. 

When evaluate between choose an exist FDS and a new solution, Hand [75] warns 

against the bias that favor the old FDS. The data use to train a new FDS depend on 

the alert and detection give by the FDS in place at the moment of training, i.e. we can 

train a new FDS only on the fraud discover by the old FDS and the undetected one 

cannot be use for training / evaluation. Therefore, there be a selection bias in the data 

collection process that be due to the performance of the previous FDS, a a result the 

evaluation be bias in favor of the exist system. This issue be similar to the problem 

of evaluate classifier in the presence of Alert-Feedback Interaction (see Chapter 6), 

where we know the feedback only of the learner request the labels. The performance 

evaluation make sense only conditional on the learner generate the alerts, so it be 

incorrect to compare two alternative algorithm when only one of the two request the 

feedbacks. 





Chapter 3 

State-of-the-art 

This chapter present a review of the approach that have be adopt for deal 

with the challenge of a Data Driven FDS present in Section 1.4. Section 3.1 provide 

an overview of state-of-the-art method for unbalanced classification and Section 3.2 

review the principal adaptation technique propose for the problem of learn in non- 

stationary distribution. Then in Section 3.3 we present method that address the problem 

of class unbalance in evolve data streams. Finally, Section 3.4 present state-of-the-art 

algorithmic solution propose for credit card fraud detection. 

3.1 Techniques for unbalanced classification task 

Learning from unbalanced datasets be a difficult task since most learn algorithm be 

not design to cope with a large difference between the number of case belonging to 

different class [22]. There be several method that deal with this problem and we can 

distinguish between method that operate at the data and algorithmic level [76]. At 

the data level, the unbalanced strategy be use a a pre-processing step to rebalance 

the dataset or to remove the noise between the two classes, before any algorithm be 

applied. At the algorithmic level, algorithm be themselves adjust to deal with the 

minority class detection. Data level method can be grouped into five main categories: 

sampling, ensemble, cost-based, distance-based and hybrid. Within algorithmic method 

instead we can distinguish between: i) classifier that be specifically design to deal 

with unbalanced distribution and ii) classifier that minimize overall classification cost. 

The latter be know in the literature a cost-sensitive classifier [35]. Both data and 

algorithm level method that be cost-sensitive target the unbalanced problem by use 

different misclassification cost for the minority and majority class. At the data level, 

cost-based method sample the data to reproduce the different cost associate to each 

35 



Chapter 3. State-of-the-art 36 

class (see translation theorem [77]). Cost-sensitive classifier instead directly minimize 

the cost by use cost specific loss function. Alternatively, wrapper method can be 

use to convert a cost-insensitive (or cost-blind) classifier into a cost-sensitive one (e.g. 

Metacost [78]). 

All the method present in the follow section will discus the unbalanced problem 

a refer to between class imbalance, i.e. imbalance in class frequency. However, 

class imbalance can exist also within the class [79, 80] (due to small cluster within one 

class), and this problem be often link to the presence of rare case [81]. Within-class 

imbalance and rare case be closely related to the problem of small disjuncts, which 

hinder classification performance [80, 82–84]. Small disjuncts be rule that cover a small 

cluster of example result from concept that be underrepresented [46, 85, 86]. For an 

in-depth analysis of other issue related to unbalanced classification we invite the reader 

to have a look at [87]. 

3.1.1 Data level method 

Sampling method 

Typically, sample method be use to rebalance the datasets, because study have 

show that standard classifier have good performance when train on a balance 

training set [88–90]. Sampling technique do not take into consideration any class in- 

formation in remove or add observations, yet they be easy to implement and to 

understand. 

Undersampling [91] consists in downsize the majority class by remove observation 

at random. In an unbalanced problem it be realistic to assume that many observation 

of the majority class be redundant and that by remove some of them at random the 

result distribution should not change much. However, the risk of remove relevant 

observation from the dataset be still present, since the removal be do in an unsupervised 

manner. In practice, this technique be often adopt since it be simple and speed up the 

learn phase. 

Oversampling [91] consists in up-sizing the small class at random decrease the level 

of class imbalance. By replicate the minority class until the two class have equal 

frequency, oversampling increase the risk of overfitting [91] by biasing the model towards 

the minority class. Other drawback of this approach be that it do not add any new 

informative minority example and that it increase the training time. This can be 

particularly ineffective when the original dataset be fairly large. 



Chapter 3. State-of-the-art 37 

SMOTE [92] oversamples the minority class by generate synthetic example in the 

neighborhood of observe ones. The idea be to form new minority example by interpo- 

lating between sample of the same class. This have the effect of create cluster around 

each minority observation. By create synthetic observation the classifier build large 

decision region that contain nearby instance from the minority class. SMOTE have 

show to improve the performance of a base classifier in many application [92], but 

it have also some drawbacks. Synthetic observation be generate without consider 

neighbor examples, lead to an increase of overlap between the two class [93]. 

Borderline-SMOTE [94] and ADASYN [95] have be propose to overcome this prob- 

lem. 

Undersampling- Oversampling- SMOTE- 

Figure 3.1: Resampling method for unbalanced classification. The negative and pos- 
itive symbol denote majority and minority class instances. In red the new observation 

create with oversampling methods. 

Cost-based method 

Cost proportional sample [35] consists into sample training instance from the ma- 

jority and minority class by take into consideration the ratio of the misclassification 

costs. Let li,j denote the misclassification cost of class j predict a i, so that the cost of 

a FP and a FN be write a l1,0 and l0,1. When we want to keep all minority examples, 

then the number of majority instance should be multiply by l1,0l0,1 . If we assume that 

the cost of a FP be lower, i.e. l1,0 < l0,1, this boil down to undersampling the majority 



Chapter 3. State-of-the-art 38 

class. Alternatively, cost proportional sample can be achieve by replicate observa- 

tions from the minority class l0,1l1,0 time (oversampling). As in the case of oversampling, 

replicate sample from the minority class may induce overfitting [77]. 

Costing [77] adopts “rejection sampling” to select the instance in the final dataset. Each 

instance in the original training set be drawn once, and accepted into the sample with the 

accept probability lj,iZ where Z be an arbitrary constant such that Z ≥ max lj,i. When 
Z = max lj,i, this be equivalent to keep all example of the rare class, and sample 

the majority class without replacement accord to l1,0l0,1 . 

Distance-based method 

The follow method make use of distance measure between input point either to 

undersample or to remove noisy and borderline example of each class. These method 

be very time consume since they require compute distance between observations. 

Tomek link [96] remove observation from the negative class that be close to the positive 

region in order to return a dataset that present a good separation between the two 

classes. Let u consider two input example xi and xj belonging to different classes, 

and let d(xi, xj) be their distance. A (xi, xj) pair be call a Tomek link if there be no 

example xk, such that d(xi, xk) < d(xi, xj) or d(xj , xk) < d(xi, xj). If two example 

form a Tomek link, then one of these example be noisy or both be borderline. Negative 

instance that be Tomek link be then remove reduce the majority class. This 

method be particularly useful in noisy datasets a it remove those sample for which 

noise can lead to a misclassification [97]. 

Condensed Nearest Neighbor (CNN) [98] be use to select a subset S from the original 

unbalanced set T which be consistent with T in the sense that S classifies T correctly 

with the one-nearest neighbor rule (1-NN). The idea behind this implementation of a 

consistent subset be to eliminate the example from the majority class that be distant 

from the decision border, since these sort of example might be consider less relevant 

for learning. Since noisy sample be likely to be misclassified, many of them will be 

add to the S set which mean CNN rule be extremely sensitive to noise [99]. Moreover, 

noisy training data will misclassify several of the subsequent test examples. 

One Sided Selection (OSS) [100] be an undersampling method result from the combi- 

nation of Tomek link follow by the application of CNN. Tomek link be use a an 

undersampling method and remove noisy and borderline majority class examples. Bor- 

derline example can be consider unsafe since a small amount of noise can make them 

fall on the wrong side of the decision border [97]. CNN aim to remove example from 



Chapter 3. State-of-the-art 39 

the majority class that be distant from the decision border. The remain examples, 

i.e. safe majority class instance and all minority class examples, be use for learning. 

Edited Nearest Neighbor (ENN) [101] remove any example whose class label differs from 

the class of at least two of it three near neighbors. In this way majority example 

that fall in the minority region be remove and likewise isolated minority example be 

removed. To avoid the risk of lose relevant minority example ENN be edit to remove 

only negative example that be misclassified by their 3 near neighbors. 

Neighborhood Cleaning Rule (NCL) [89] modifies the ENN method by increase the 

role of data cleaning. Firstly, NCL remove negative example that be misclassified by 

their 3-nearest neighbors. Secondly, the neighbor of each positive example be found 

and the one belonging to the majority class be removed. Since this algorithm remove 

noisy instances, a well a close-border points, the decision boundary becomes smoother 

with a consequent reduction of the risk of overfitting [97]. 

Other hybrid strategy can be easily create by combine sampling, ensemble or distance- 

base technique [27]. 

3.1.2 Algorithm level method 

Algorithm orient method be essentially a modification (or extension) of exist clas- 

sification algorithm for unbalanced tasks. Depending on their application we distin- 

guish between imbalanced learn and cost-sensitive learning. In first case, the goal be 

to improve accuracy of the minority class, while in the second case the objective be to 

minimize the cost associate to the classification task. 

Imbalance learn 

Standard decision tree such a C4.5 [102] use Information Gain (IG) a splitting criterion 

in each node of the tree. However, this splitting criterion return rule that be bias 

towards the majority class. Liu et al. [57] use the Class Confidence Proportion (CCP) 

metric for splitting in presence of class imbalance, while Cieslak and Chawla [103] suggest 

splitting with Hellinger Distance (HD). They show that HD be skew-insensitive and the 

propose HDDT have good performance compare to standard C4.5. Other study 

have also report the negative effect of skewed class distribution not only on decision 

tree [21, 82], but also for Neural Networks [82, 104], k-Nearest Neighbor (kNN) [100, 

105, 106] and SVM [107, 108]. 



Chapter 3. State-of-the-art 40 

A SVM optimize in term of F-measure be present by Callut and Dupont [109], while 

Li et al. [110] use SVM with RBF kernel a base classifier for AdaBoost. In the family of 

lazy learn classifiers, Liu and Chawla [111] propose a kNN weight strategy design 

for handle the problem of class unbalance. The algorithm, call CCW-kNN (Class 

Confidence Weights kNN), be able to correct the inherent bias towards the majority class 

in exist kNN classifiers. 

Association rule mining with class unbalance be often achieve by specify multiple 

support level for each class to account for the difference in class frequency [112]. Within 

the family of rule-based classifiers, Verhein and Chawla [113] developed a classifier call 

SPARCCC that be specifically design for unbalanced classification. In all these algo- 

rithms the general idea be to modify the original classifier in order to learn good pattern 

from the minority class. Weiss [114] argues that these algorithmic solution should be pre- 

ferred to data level method cause they be able to deal with the class unbalance directly 

without biasing the classifier towards one class. 

Following the great success of ensemble learn in machine learning, lot of ensemble 

strategy have be propose for imbalanced learning, with Bagging [41] and Boost- 

ing [115] be the most popular method to aggregate classifiers. Usually, ensemble 

method combine an unbalanced strategy with a classifier to explore the majority and 

minority class distribution. BalanceCascade [116] be a supervise strategy to under sam- 

ple the majority class. This method iteratively remove the majority class instance that 

be correctly classify by a boost algorithm. The idea be that observation of the 

majority class that be easy to classify be redundant and that by remove them the 

algorithm can concentrate on the hard cases. The drawback be that a classification algo- 

rithm have to be apply several time to reduce the majority class lead to an increase 

in computational needs. 

EasyEnsemble [116] and UnderBagging [117] combine different model that learn distinct 

aspect of the original majority class. This be do by create different balance training 

set by undersampling, learn a model for each dataset and then combine all predic- 

tions a in bagging. In the case of EasyEnsemble, boost be use a classifier so that the 

method be able to integrate the advantage of boost and bagging. In the same spirit, 

several study have integrate oversampling/undersampling in ensemble of SVMs [118– 

121]. SMOTEBoost [122] combine boost with SMOTE. Similarly, DataBoost-IM [45] 

generates synthetic sample a well within the boost framework to improve the predic- 

tive accuracy of both the majority and minority classes. RareBoost [123] modifies the 

boost algorithm to increase accuracy on the rare class by emphasize the difference 



Chapter 3. State-of-the-art 41 

of TN from FN, and TP from FP at each iteration of boosting. JOUS-Boost [124] gener- 

ate duplicate of the minority class with oversampling, but also introduce perturbation 

(“jittering”) by add IID noise to minority examples. 

Cost-sensitive learn 

In unbalanced classification tasks, it be usually more important to correctly predict pos- 

itive (minority) instance than negative (majority) instances. This be often achieve by 

associate different cost to erroneous prediction of each class. Cost-based method 

operating at the algorithm level be able to consider misclassification cost in the learn- 

ing phase without the need of sample the two classes. Example of these classifier be 

cost-sensitive boost [125, 126], SVM [127] and Neural Network [128]. 

In the family of decision tree classifiers, cost-based splitting criterion be use to minimize 

cost [129]; or cost information determine whether a subtree should be prune [130]. In 

general, prune allows improve the generalization of a tree classifier since it remove 

leaf with few sample on which we expect poor probability estimates. Although, when 

the classification task be unbalanced, leaf contain few sample be often the one 

associate to the minority class and the first remove with pruning. 

With Metacost [78] Domingos propose a general framework that allows transform 

any non cost-sensitive classifier into a cost-sensitive one. Similarly, thresholding [131] 

allows use cost-insensitive algorithm for cost minimization via different classification 

thresholds. These last method be also call cost-sensitive meta-learners, since they be 

wrapper that minimize misclassification cost use standard classification algorithm. 

In many applications, however, cost be not explicitly available or easy to estimate, 

complicate the use of cost-sensitive algorithm [132]. 

3.2 Learning with non-stationarity 

A standard assumption in machine learn be that the training and test set be 

drawn from the same underlie distribution. Let Ptr(x, y) and Pts(x, y) denote joint 
probability of a sample (x, y) accord to training and test distribution. In a non- 

stationary environment we have Ptr(x, y) 6= Pts(x, y). When there be a distributional 
mismatch between the test and training data, the model fit use the training data 

will be sub-optimal for the test scenario. Typically, this mismatch be generate by the 

follow two causes: i) Sample Selection Bias (SSB) or ii) time evolve data. 



Chapter 3. State-of-the-art 42 

In the first case the selection process of the training sample be responsible for a bias 

training set and depend on the bias different solution exist to correct this bias. The 

second cause of non-stationarity be link to stream data where the data distribution 

evolve with time. 

3.2.1 Sample Selection Bias 

Sample Selection Bias (SSB) occurs when the training set available be bias because 

it have be select in a way that be not representative of the whole population. For 

example, consider the problem where a bank want to predict whether someone who be 

apply for a credit card will be able to repay the credit at the end of the month. The 

bank have data available on customer whose application have be approved, but have 

no information on reject customers. This mean that the data available to the bank be 

a bias sample of the whole population. The bias in this case be intrinsic to the dataset 

collect by the bank. 

We formalize the sample bias by mean of a random variable s ∈ {0, 1} where sample 
include in the bias training set have s = 1, and s = 0 otherwise. Then the joint 

distribution of a sample (x, y) in the bias training set be P(x, y|s = 1) and P(x, y) 
be the distribution in the case when the training set be unbiased. SSB require what 

be know a the support condition [133]: the support of the probability measure of the 

training data have to be a subset of the support of the probability measure of the test data. 

Under this condition Ptr(y, x) = P(y, x|s = 1) and Pts(y, x) = P(y, x). The selection 
bias can be of different types: class prior bias, feature bias (also call covariate shift) 

and complete bias. 

Class prior bias, also call prior probability shift, be essentially a change in class priors: 

P(y|s = 1) 6= P(y). It corresponds to the case when the selection be independent of 
the feature x give the class y: P(s = 1|x, y) = P(s = 1|y). As a consequence of 
prior change, class-conditional probability be different (P(y|x, s = 1) 6= P(y|x)), while 
within-class distribution remain unchanged (P(x|y, s = 1) = P(x|y)). This type of 
bias can be introduce voluntarily for unbalanced classification task a in the case of 

undersampling and oversampling technique present in Section 3.1.1. As a result of 

class prior bias, classifier have probability estimate that be poorly calibrate [28]. 

Well-know method for correct the prior bias be give by Saerens et al. [134] and 

Elkan [35]. 

Feature bias refers to change in the distribution of the input variable without affect 

the conditional probability P(y|x). The selection be independent of the class label y give 
the feature x (missing at random): P(s = 1|x, y) = P(s = 1|x). This condition implies 



Chapter 3. State-of-the-art 43 

P(y|x, s = 1) = P(y|x). Feature bias appear to be most study type of bias [135–138] 
and a standard solution consist into re-weighting training instance [139]. 

Finally, complete bias be the most general case where the selection depend on both 

y and x (missing not at random). The most famous method for correct complete 

bias be the one of Heckman [140] that give him the Nobel Prize. Heckman defines two 

linear models, one for estimate s and one for y, which use different subset of features. 

The two-step procedure for bias correction consists into model s with ordinary least 

square and then use the output a an additional feature in the model use to estimate 

y. If the same feature be use to estimate both linear model estimate s and y, then 

the additional variable may end up highly correlate with the bias estimate of y and 

the Heckman procedure be not effective in correct the bias [141]. Following the same 

idea Zadrozny and Elkan [142] use a classifier to predict P(s = 1|x) and then incorporate 
this probability estimate a input feature in a new classifier use to predict P(y|x), but 
there be no theoretical guarantee that this method should correct SSB. For a recent 

survey on SSB we refer to [141]. 

The machine learn community have extensively investigate the SSB problem [35, 

77, 142–145]. A standard remedy to SSB be importance weight which consists of 

re-weighting the cost of training sample error to more closely reflect that of the test 

distribution [77, 143, 144]. Using the Bayes formula we can write P(x, y) in term of 
P(x, y|s = 1): 

P(x, y) = P(x, y|s = 1)P(s = 1)P(s = 1|x, y) = 
P(s = 1) 
P(s = 1|x, y)P(x, y|s = 1) (3.1) 

Hence, we can correct SSB use a weight-sensitive algorithm, where a training instance 

(x, y) receive a weight P(s=1)P(s=1|x,y) . The probability P(s = 1) can easily be estimate 
know the proportion of sample examples, however compute P(s = 1|x, y) be not 
straightforward. In the case of feature bias we can rewrite (3.1) as: 

P(x, y) = P(s = 1)P(s = 1|x)P(x, y, s = 1) (3.2) 

Now we can estimate term P(s = 1|x) use a classifier that distinguish between sample 
point and instance not include in the training set. However, Cortes et al. [133] argue 

that the re-weighting approach be able to remove the bias a long a the weight be 

estimate correctly. With a poor estimate of the weight we be not guaranteed to be 

able to remove the bias. In estimate P(s = 1|x) we might have some value equal 
to zero, so it may be safer to assume complete bias [146]. SSB be closely related to the 

problem of learn under time evolve data streams, where importance weight be 

use to correct the concept drift due to covariate shift [147]. 



Chapter 3. State-of-the-art 44 

3.2.2 Time evolve data 

Most of the times, the main cause of difference between training and test set in data 

stream be due to a change in the data generate process. Using Bayes rule we can write 

the joint distribution of a sample (x, y) as: 

P(x, y) = P(y|x)P(x) = P(x|y)P(y) (3.3) 

Usually, in classification we be interested in a estimation of P(y|x), from (3.3) we have: 

P(y|x) = P(x|y)P(y)P(x) (3.4) 

Using (3.4), a change between the distribution of the data stream at time t and t + 

1, Pt(x, y) 6= Pt+1(x, y), can come from [135]: i) P(y|x), ii)P(x|y), iii) P(y) and iv) 
combination of the previous. Note that a change in P(x) do not affect y and can 
thus be ignore [148]. Most often, regardless of type of term varying, a change in the 

distribution be refer in literature a Concept Drift [23] or Dataset Shift [149]. 

Change in the probability prior (Pt(y) 6= Pt+1(y)) can cause well-calibrated classifier 
to become miscalibrated [28]. Concept drift due to Pt(x|y) 6= Pt+1(x|y) affect the 
distribution of the observation within the class, but leaf the class boundary unchanged 

(Pt(y|x) = Pt+1(y|x)). This type of drift be often call covariate shift [141]. When 
Pt(y|x) 6= Pt+1(y|x), there be a change in the class boundary that make any previously 
learn classifier biased. The latter be the bad type of drift, because it directly affect 

the performance of a classifier, a the distribution of the features, with respect to the 

class, have change [148] (see Figure 3.2). In general it be hard to say where the change 

come from, because we have access to only estimation of the previous probability and 

we have no knowledge (or control) of the data generate process. 

Learning algorithm operating in non-stationary environment typically rely only on the 

supervise information that be up-to-date (thus relevant), and remove obsolete training 

sample [150]. However, concept learn in the past may re-occur in the future, there- 

fore when remove obsolete training samples, we could remove information that be still 

relevant. This be know a the stability-plasticity dilemma [151]. 

Concept Drift (CD) adaptation approach can be grouped into two families: i) active and 

ii) passive adaptation [150]. The former [152–155], reacts to CD when a change be detect 

(e.g. by mean of a Change Detection Test). Active approach be mostly adopt for 

data distribution that change abruptly, because change detection with gradual drift 

be typically more difficult [156]. When a change be identify the classifier be update 

or remove and replace by a new one train on the most recent sample available. 



Chapter 3. State-of-the-art 45 

t +1 

t 

Class%priors%change% Within%class%change% Class%boundary%change% 

Figure 3.2: Illustrative example of different type of Concept Drift. Change in class 
prior be due to Pt(y) 6= Pt+1(y), while within class change (covariate shift) occurs when 
Pt(x|y) 6= Pt+1(x|y). Class boundary change happens when Pt(y|x) 6= Pt+1(y|x). 

The change detector can work by: a) check feature distribution or b) analyze 

misclassification errors. In first case feature be inspect to detect possible evolve 

distributions, whereas misclassification error be use to identify change in the class 

boundary. CD detection be particularly challenge in the presence of class unbalanced, 

because we might have to wait a while before have enough sample from the minority 

class [148]. 

In the case of passive approach the classifier be continuously update a soon a new 

supervise sample become available (no trigger mechanism involved) [18, 157, 158]. 

Generally, these approach be use in the presence of gradual drift and recur 

concept [158]. Passive approach typically rely on ensemble of classifiers, where CD 

adaptation be obtain by weight the ensemble member and creating/removing the 

classifier compose the ensemble. By integrate several classifiers, the ensemble com- 

bine what be learn from new observation and the knowledge acquire before [159–161]. 

The weight of each member in the ensemble be compute to reflect how well a classi- 

fier be still relevant for the current concepts. If the weight of a classifier drop below a 

certain threshold, then the classifier be usually replaced, see for example SEA [160] and 

DWM [162]. Other ensemble technique [159–161] use ensemble of classifier in order to 

combine what be learn from new observation and the knowledge acquire before. 

Alternatively, passive CD adaptation be achieve by training a classifier over a slide 

window of the recent supervise sample (e.g. STAGGER [163] and FLORA [164]). 



Chapter 3. State-of-the-art 46 

It becomes critical then to set the rate of forgetting, define by the window size, in 

order to match the rate of change in the distribution [165]. The simplest strategy us a 

constant forget rate, which boil down to consider a fix window of recent observation 

to retrain the model. FLORA approach [164] us a variable forget rate where the 

window be shrunk if a change be detect and expand otherwise. 

For a recent and in-deep review on CD adaptation we refer the reader to [156]. This 

thesis we will focus on passive approaches, where the data stream be receive in batch 

of daily transactions. In the case of fraud detection, the data stream present not only 

non-stationary distributions, but also unbalanced classes. The problem of learn in 

the case of unbalanced data have be widely explore in the static learn setting 

[82, 91, 92, 116]. Learning from non-stationary data stream with skewed class distribution 

be however a relatively recent domain and will be treat in the next section. 

3.3 Learning with evolve and unbalanced data stream 

In many application (e.g. network traffic monitoring and web access data) the data be 

receive over time with high frequency and it be not possible to store all historical samples. 

The data have to be process in real time and it may not be feasible to revisit previous 

transactions. This restriction be know in the literature a one-pass constraint [166]. 

One popular algorithm for mining data stream be Very Fast Decision Tree (VFDT) by 

Domingos and Hulten [167]. VFDT learns incrementally from each new observation 

without store any examples, i.e. use constant memory and constant time per exam- 

ple, and us Hoeffding bound to determine the conversion of a tree leaf to a tree node. 

Since the seminal paper of Domingos, several similar algorithm have be proposed, e.g. 

CVFDT [168] us a slide window for concept drift adaptation, VFDTc [169] extends 

VFDT for continuous data and CD use a Naive Bayes classifier in the leaves. 

When the data stream be unbalanced, a certain time have to elapse in order to ac- 

cumulate enough sample from the minority class. Since a classifier require a large 

set of sample to learn an accurate model, typically the training be do batch wise, 

i.e. when a sufficient number of sample from both class be available. Despite the 

large literature on data stream studies, few work have try to address the problem 

of learn with non-stationary data stream with unbalanced class distribution. In 

these work the class unbalance problem be typically address my adopt one of the 

resampling method present in Section 3.1.1 to rebalance the batch of the data 

stream [19, 170–173]. For example, Ditzler and Polikar propose Learn++.NIE [172] 



Chapter 3. State-of-the-art 47 

where they extend Learn++.NSE [174] for unbalanced data stream by training clas- 

sifiers on multiple balance batch obtain with undersampling. Following the same 

idea, Learn++.CDS [173] rebalances the training set of each classifier in the ensemble 

use SMOTE [92]. 

Rebalancing a batch with few positive (minority) sample could mean remove many 

negative sample (in the case of undersampling) or significant replication of the minority 

class (oversampling) with a high risk of overfitting. A different way to compensate 

the class imbalance within the batch be to propagate minority class sample along 

the stream. For example Gao et al. [170, 171] combine minority class propagation and 

undersampling of the majority class. The positive example be accumulate along the 

stream until they represent 40% of the observations. When this happens, the old 

positive example be replace by the new observation from the minority class. This 

propagation method ignores the similarity of the minority class instance to the current 

concept, rely only on it similarity in time. 

Lichtenwalter and Chawla [175] suggest propagate not only minority samples, but also 

observation from the majority class that have be previously misclassified to increase 

the boundary definition between the two classes. Chen and He propose REA [176] where 

they recommend propagate only example from the minority class select use a kNN 

algorithm. Similarly, SERA [177] propagates to the last batch only minority class that 

belong to the same concept use Mahalanobis distance. Hoens and Chawla [148] use 

instead an instance propagation mechanism base on a Naïve Bayes classifier. In this 

case, Naïve Bayes be use to select old positive instance which be relevant to the current 

minority class context. This method relies on find instance that be similar to the 

current minority class context. 

Wang, Minku and Yao [178] propose Sampling-based Online Bagging (SOB) to deal 

with unbalanced data streams. Their algorithm, be essentially a modification of Online 

Bagging [179], in which the sample rate of the instance belonging to one class be deter- 

mine adaptively base on the current imbalance status and classification performance. 

The problem with this approach be that it be not design to handle CD a it aim to 

maximize G-mean greedily over all receive example [178]. 

3.4 Algorithmic solution for Fraud Detection 

In the literature, both supervise [64, 180, 181] and unsupervised [11, 182] ML algo- 

rithms have be propose for credit card fraud detection. As explain in Section 2.1, 



Chapter 3. State-of-the-art 48 

supervise technique assume the availability of annotate datasets, i.e. transaction la- 

beled a genuine or fraudulent. In this case a model be train under the supervision 

of the class information to discover pattern associate to the fraudulent and genuine 

class. On the contrary, unsupervised method work with datasets that contain unlabeled 

samples. These method consist in outlier detection or anomaly detection technique 

that associate fraudulent behaviour to any transaction that do not conform to the 

majority, without knowledge on transaction class [11]. Unsupervised technique usually 

generate too many false alert so it be often a good idea to combine both supervise and 

unsupervised method a in [62]. 

The detection problem can be see from the transaction and card level. At the card 

level be possible to group transaction from the same card and learn behavioral model 

of individual cards. Behavioral model be typically unsupervised method that aim to 

characterize the genuine behavior of each individual card over the time. These model 

only consider the previous history of each card but do not attempt to identify global 

pattern of fraudulent behaviors; they only try to detect change in behavior. Examples 

of these be give by Fawcett and Provost [183], Bolton [11] and Weston [184]. The 

problem with this approach be that a change in behavior may not be due to fraud. For 

example, the change in spending behavior during holiday season may be not necessary 

link to fraudulent activities. 

Models that operate at the transaction level try to differentiate legitimate transac- 

tions from fraudulent one without know the behavior of a card. These customer- 

independent model can serve a a second line of defense, the first be customer- 

dependent model (behavioral models). This strategy considers only transaction in 

isolation from each other. Neither the previous history of the associate account, nor 

other transaction be take into consideration. Alternatively, it be possible to work at 

the transaction level and include customer behavior in the model by use aggregate 

variable of the cardholder a present in Section 2.2.2. 

3.4.1 Supervised Approaches 

In supervise algorithm fraudulent and non-fraudulent example be use to predict the 

class of a new observation. Supervised technique developed for fraud detection can be 

group into: i) supervise profiling, ii) classification, iii) cost-sensitive and iv) network 

methods. 



Chapter 3. State-of-the-art 49 

Supervised Profiling 

When label transaction be available, it be possible to profile the distribution of rel- 

evant variable for genuine and fraudulent card [185]. This mean that it be possible 

to create different profile for each class. At this point every new transaction can be 

compare to see which profile be more similar. For example, Siddiqi [186] proposes to 

use Weight Of Evidence (WOE) a similarity measure between two profile in credit 

risk. The same metric can be use to compare the genuine and fraudulent profile [185]. 

Profiles can also be create use rule-based methods. 

Rules can come from human expert or from statistical model and they have the advan- 

tage of be easy to understand and to implement. A set of rule be define for each 

profile and if a new transaction match these rule it be assume to have the same pro- 

file. Chan [181] for instance us rule to filter out safe transactions. An adaptive user 

profile method be propose by Fawcett and Provost [187] but for telecommunication 

frauds. Cortes [188] defines an account signature for profile account in data streams. 

As the criminal activity and legitimate user-behavior evolves, fraudulent profile have to 

be update a well. This mean that statistical-based rule have to be update periodi- 

cally. Alternatively a weight ensemble approach can be use to include new rule while 

maintain old rule [189]. Profiles must be update to reflect the dynamic pattern of 

criminal activity a well a change in legitimate user behavior. This present a chal- 

lenge for static rule-based method that be learn off-line, a they must be frequently 

validate and retrained. 

Classification 

Classification appear to be the standard way to approach Fraud Detection [190] and 

several classification algorithm have be used, e.g. Neural Networks [180, 191–193], 

Logistic Regression [65], Association Rules [194], Support Vector Machines [63], Fisher 

Discriminant Analysis [69], and Decision Trees [19, 68, 70]. 

Decision tree have found many application in fraud detection, for both credit card 

fraud detection [126] and credit risk score [195]. Neural Networks a well have be 

extensively apply in detection system such a Falcon [196], Minerva (Dorronsoro [193]), 

FDS [191], Cardwatch [192] and Visa [197]. Dorronsoro [193] show that a three-layer net 

be capable of deal with the highly skewed class distributions. However, a explain in 

Section 3.1, supervise method in general suffer from the problem of unbalanced class 

sizes: the legitimate transaction generally far outnumber the fraudulent ones. 



Chapter 3. State-of-the-art 50 

Neural network be able to learn difficult class boundaries, but they be black-boxes a 

it be not possible for a human be to understand how they behave. Rules Extraction 

and Decision Tree be widely use in fraud detection because unlike black-box technique 

allow a good grasp of the classification. This be important because, often, the analyst 

be not Machine Learning specialist and need to understand classification mechanism 

in order to trust and use them. These approach be often base on the extraction 

of conjunction and disjunction of rule that be representational of the choice of the 

classification and directly understandable. 

Probabilistic graphical model be use a well in fraud detection such a Bayesian 

Belief Networks [185]. Card’s activity can be model use a Hidden Markov Models. 

Sudjianto [185] show that be possible to use HMM to monitor transaction of a card 

define different status of a card. Finally, between all the algorithm propose in the 

literature, we recommend use Random Forest, because several study have show that 

it achieves the best result among different classifier [18, 20, 61, 63, 64, 71]. 

Cost-sensitive method 

Since business be interested in reduce the monetary loss due to fraudulent activities, 

there be a large body of work on cost-sensitive classifiers. These method be also call 

cost-sensitive algorithms, because they be able to take into account the different cost in 

misclassify a transaction a fraud or legitimate. As show in Section 3.1.2, cost-sensitive 

learn [78], [35] be an alternative way to deal with the unbalanced problem that consist 

into assign large cost to error make on the minority class. Many cost-sensitive algo- 

rithms available in the literature be base on boosting; see for example AsymBoost [198], 

AdaCost [126], CSB [199], DataBoost [45], AdaUBoost [200] and SMOTEBoost [122]. 

Traditional cost-sensitive learn such a AdaCost [126] assumes that the cost be 

fix and class-dependent, however in fraud detection the cost be proportional to the 

transaction amount. The large the amount, the great the potential loss in case of 

fraud. In these setting the cost of miss a fraud (a false negative) be not fixed, 

but proportional to the transaction amount. Examples of cost-sensitive classifier us- 

ing transaction-dependent cost be [68–70]. Mahmoudi and Duman [69] use Modified 

Fisher Discriminant Analysis to consider example-dependent cost for each transaction 

to maximize total profit. Similarly Bahnsen et al. [70] and Sahin et al.[68] use example- 

dependent cost-sensitive decision tree for maximize the savings. Cost-based method 

be useful when the primary goal be to minimize some cost / maximize the benefit of the 

detection or saving since the loss function take into account the financial loss occur 

in each prediction. 



Chapter 3. State-of-the-art 51 

Given the high cost of misclassifying a fraud than a genuine transaction, cost-based 

algorithm could in principle prefer to produce a false alert rather than take the risk to 

predict a transaction a legitimate when it be not. As a consequence, these algorithm 

can generate many false positive and be of no practical use for investigator who require 

precise alert (see Section 2.2.1). 

Detection of Fraud Networks 

The detection of link between data can also lead to fraud discovering. For example, in 

the telecommunication domain, expert have notice that a fraudulent account be often 

connect to another by some call give between fraudsters. By analyze these links, 

they discover fraud networks. Such approach seem complementary to individual fraud 

detection [201]. 

When the fraudulent activity be spread over many transaction and card, illegal activity 

can be uncover by analyze pattern of related transactions. In particular link analysis 

and graph mining method may be able to detect these group of fraudulent transaction 

[185]. Recently Van Vlasselaer et al. [61] have propose APATE, a framework for credit 

card fraud detection that allows include network information a additional feature to 

the original feature vector describe a transaction. They show that feature include 

network information be able to improve significantly the performance of a standard 

supervise algorithm. 

3.4.2 Unsupervised Approaches 

Unsupervised method be use when there be no prior set of legitimate and fraudulent 

observations. Since they be not base on example of fraud or genuine transactions, 

unsupervised strategy have the advantage of be independent of their selection, and 

be able, in theory, to discover fraud still unobserved, that have not be detect by 

an expert. Yet they be not affected by the problem of mislabeled dataset and class 

imbalance. However unsupervised credit card fraud detection have not receive a lot of 

attention in the literature [11]. 

Techniques employ in fraud detection be usually a combination of profile and outlier 

detection methods. They model a baseline distribution that represent normal behavior 

and then attempt to detect observation that show the great departure from this norm 

[202]. One of these method be Peer Group Analysis [184] which cluster customer into 

different profile and identifies fraud a transaction depart from the customer profile 



Chapter 3. State-of-the-art 52 

(see also the recent survey by Phua [203]). Others model cardholder behavior by mean 

of self-organizing map [5, 204, 205]. 

In order to detect outlier or anomaly it be important first to define when an example 

be an outlier. Grubbs [206] give the follow definition: “An outlying observation, or 

outlier, be one that appear to deviate markedly from other member of the sample in 

which it occurs”. For Barnett & Lewis [207] an outlier is: “An observation (or subset of 

observations) which appear to be inconsistent with the remainder of that set of data”. 

When it be possible to define a region or distribution of the data represent the normal 

behavior then all observation fall outside can be flag a outlier. This approach 

however come with several challenges: i) define a normal region which encompasses 

every possible normal behavior be very difficult, ii) normal behavior keep evolve and 

an exist notion of normal behavior might not be sufficiently representative in the 

future, iii) the boundary between normal and outlying behavior be often fuzzy, iv) fraud 

adapt themselves to make the outlying observation appear like normal, thereby make 

the task of define normal behavior more difficult and v) data contains noise which be 

similar to the actual outliers. 

In general we can distinguish between two form of outliers: local and global outliers. A 

global outlier be an observation anomalous to the entire data set. In the case of fraud 

detection, an example of global outlier can be a transaction of e 10000 when all the other 

transaction in the dataset have small amount. A local outlier be an observation that 

look anomalous when compare to subgroup of the data. When a card be mostly use 

in a restrict geographic area, a purchase make by the same card in foreign country be 

a local outlier within it transactions, but be not anomalous with respect to all possible 

transactions. Depending on the type of outlier that we want to detect we can use local 

and global approaches. 

Local Approaches 

In [209], the author aim to detect outlier by analyze local information of the space. 

To achieve this, they introduce the notion of Rough Membership Function that computes 

the degree of deviance of data, depend on the density of it local neighborhood. 

In [11], Bolton et al. have base their work on the hypothesis that a fraud differs 

accord to the consider data, similarly to what be propose in [187, 210]. The idea of 

Peer Group Analysis [11] be that time series that be in some sense similar be grouped 

together to form a peer group. Transactions that deviate strongly from their peer group 

be flag a potentially fraudulent. The behaviour of the peer group be summarize 



Chapter 3. State-of-the-art 53 

at each subsequent time point and the behaviour of a time series compare with the 

summary of it peer group. The author present their analysis more a a way to alert 

investigator of anomalous data than a a fraud classifier. In the paper, the author 

consider time series of data that be cluster on the basis a distance function in group 

of define size. They next propose to resume each group by a model and then compute 

their deviance. They finally present result in a graph style that allows visualize 

breakpoints. This approach be interest because it be understandable and it be not 

base on an annotate set. It be then, a-priori, capable to detect any kind of fraud. 

Global Approaches 

The work present in [211] address high dimensional spaces. In this kind of spaces, 

any observation can be consider a an outlier because of the sparsity of the data. The 

author propose to project data in few dimension space to avoid these phenomenon 

and study the density of distribution to detect frauds. They split the original space into k 

subspace of same depth on each dimension and then try to combine these space to form 

hypercubes of dimension n (n little enough) that be of a particularly weak density. To 

combine dimension the best, they use an evolutionary approach that combine randomly 

dimension to extract the best candidates. Experimental result show a quality close to 

systematic approach and a good quality of detection. Another benefit of this approach 

is, by project data, it be capable to deal with data have miss attributes. Its 

drawback be again it rendering, even if the select dimension can partially explain the 

analysis. 

In [212], the question be to detect outlier in data have categorical attributes. The 

author formalize the problem this way: “finding a small subset of a target dataset such 

that the degree of disorder of the resultant dataset after the removal of this subset be 

minimized”. This degree of “disorder” be measure with the entropy of the dataset. Their 

technique aim to detect the k most deviant outlier in the dataset. They estimate that 

this parameter be not hard to define because, accord to them, the problem be give this 

way in concrete case (find for example the 5% of fraudsters). They develop an iterative 

algorithm that try to optimize the set by minimize the entropy. This approach, which 

could appear expensive, be in fact estimate experimentally a almost linear w.r.t. the 

size of the dataset and the number of outlier to extract (k). Results be interest 

though the author make the strong assumption that attribute be independent. 





Part II 

Contribution 

55 





Chapter 4 

Techniques for unbalanced 

classification task 

Results present in this chapter have be publish in the follow papers: 

• Andrea Dal Pozzolo, Olivier Caelen, and Gianluca Bontempi. When be under- 
sample effective in unbalanced classification tasks?. In European Conference on 

Machine Learning. ECML-KDD, 2015. 

• Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. 
Calibrating Probability with Undersampling for Unbalanced Classification. In Sym- 

posium on Computational Intelligence and Data Mining (CIDM). IEEE, 2015. 

• Andrea Dal Pozzolo, Olivier Caelen, Serge Waterschoot, Gianluca Bontempi, Rac- 
ing for unbalanced method selection. Proceedings of the 14th International Con- 

ference on Intelligent Data Engineering and Automated Learning (IDEAL), IEEE, 

2013 

The chapter be divide into three main parts. The first part of the chapter (Section 4.1) 

be base on [213] and it analysis a well-known technique call undersampling (see Sec- 

tion 3.1.1), which consists into rebalancing (typically by resampling) of the class before 

proceed with the learn of the classifier. Though this seem to work for the majority 

of cases, no detailed analysis exists about the impact of undersampling on the accuracy 

of the final classifier. In particular we will propose a theoretical analysis specify under 

which condition undersampling be recommend and expect to be effective. 

In the second part of the chapter (Section 4.2), which be base on [28], we show how 

sample data use to train a model induces an artificial bias into the compute poste- 

rior probabilities, for which we show a corrective method. Although this bias do not 

57 



Chapter 4. Techniques for unabalanced classification task 58 

affect the rank order return by the posterior probability, it significantly impact the 

classification accuracy and probability calibration. We use Bayes Minimum Risk theory 

to find the correct classification threshold and show how to adjust it after undersampling. 

Finally, the third part of the chapter (Section 4.3) be base on [27]. In this section, we 

propose to use a race algorithm to select adaptively the most appropriate strategy for 

a give unbalanced task. Racing allows one to test rapidly a large set of alternative and 

we use it to compare undersampling against a large set of technique for unbalanced 

classification. However, a confirm by our experimental comparison, no technique 

appear to work consistently good in all conditions. The result show that race be 

able to adapt the choice of the strategy to the specific nature of the unbalanced problem 

and to select rapidly the most appropriate strategy without compromise the accuracy. 

4.1 When be undersampling effective in unbalanced classifi- 

cation tasks? 

When the data be unbalanced, standard machine learn algorithm that maximize over- 

all accuracy tend to classify all observation a majority class instances. This translates 

into poor accuracy on the minority class (low recall), which be typically the class of inter- 

est. Degradation of classification performance be not only related to a small number of 

example in the minority class in comparison to the number of example in the majority 

class (expressed by the class imbalance ratio), but also to the minority class decompo- 

sition into small sub-clusters [214] (also know in the literature a small disjuncts [80]) 

and to the overlap between the two class [84, 215–217]. In these study it emerges 

that performance degradation be strongly cause by the presence of both unbalanced 

class distribution and a high degree of class overlap. Additionally, in unbalanced clas- 

sification tasks, the performance of a classifier be also affected by the presence of noisy 

example [218, 219]. 

One possible way to deal with this issue be to adjust the algorithm themselves [77, 78, 82]. 

Here we will consider instead a data-level strategy know a undersampling [21]. In an 

unbalanced problem, it be often realistic to assume that many observation of the majority 

class be redundant and that by remove some of them at random the data distribution 

will not change significantly. Nevertheless, the risk of remove relevant observation 

from the dataset be still present, since the removal be perform in an unsupervised 

manner. In practice, sample method be often use to balance datasets with skewed 

class distribution because several classifier have empirically show good performance 

when train on balance dataset [88, 90]. However, these study do not imply that 



Chapter 4. Techniques for unabalanced classification task 59 

Undersampling-- 

Unbalanced- Balanced- 

Figure 4.1: Undersampling: remove majority class observation until we have the 
same number of instance in the two classes. 

classifier cannot learn from unbalanced datasets. For instance, other study have also 

show that some classifier do not improve their performance when the training dataset 

be balance use sample technique [82, 106]. As a result, for the moment the only 

way to know if sample help the learn process be to run some simulations. Despite 

the popularity of undersampling, we have to remark that there be not yet a theoretical 

framework explain how it can affect the accuracy of the learn process. 

In this chapter we aim to analyze the role of the two side effect of undersampling on 

the final accuracy. The first side effect be that, by remove majority class instances, 

we perturb the a priori probability of the training set and we induce a warp in the 

posterior distribution [35, 134]. The second be that the number of sample available for 

training be reduce with an evident consequence in term of accuracy of the result 

classifier. We study the interaction between these two effect of undersampling and we 

analyze their impact on the final rank of posterior probabilities. In particular we show 

under which condition an undersampling strategy be recommend and expect to be 

effective in term of final classification accuracy. 

4.1.1 The warp effect of undersampling on the posterior probability 

Let u consider a binary classification task f : Rn → {0, 1}, where X ∈ Rn be the 
input and Y ∈ {0, 1} be the output. In the follow we will also use the label negative 
(resp. positive) to denote the label 0 (resp. 1). Suppose that the training set TN = 

{(x1, y1), . . . , (xN , yN )} of size N be unbalanced (i.e. the number N+ of positive case be 
small compare to the number N− of negative ones) and that rebalancing be perform 

by undersampling, i.e. the result dataset contains a subset of the negatives. Let u 

introduce a random binary selection variable s associate to each sample in TN , which 

take the value 1 if the point be sample and 0 otherwise. We now derive how the 

posterior probability of a model learn on a balance subset relates to the one learn 

on the original unbalanced dataset, on the basis of [149]. Let u assume that the selection 



Chapter 4. Techniques for unabalanced classification task 60 

variable s be independent of the input x give the class y (class-dependent selection): 

P(s|y, x) = P(s|y) (4.1) 

where P(s = 1|y, x) be the probability that a sample (x, y) be include in the balance 
training sample. This assumption implies P(x|y, s) = P(x|y), i.e. by remove observa- 
tion at random in the majority class we do not change within-class distributions. With 

undersampling there be a change in the prior probability (P(y|s = 1) 6= P(y)) and a 
a consequence the class-conditional probability be different a well, P(y|x, s = 1) 6= 
P(y|x). Let the sign + denote y = 1 and − denote y = 0, e.g. P(+|x) = P(y = 1|x) 
and P(−|x) = P(y = 0|x). From Bayes’ rule we can write: 

P(+|x, s = 1) = P(s = 1|+, x)P(+|x)P(s = 1|+, x)P(+|x) + P(s = 1|−, x)P(−|x) (4.2) 

Using condition (4.1) in (4.2) we obtain: 

P(+|x, s = 1) = P(s = 1|+)P(+|x)P(s = 1|+)P(+|x) + P(s = 1|−)P(−|x) (4.3) 

With undersampling we keep all positive and a subset of negative (see Figure 4.1), 

therefore we have: 

P(s = 1|+) = 1 (4.4) 

and 
P(+) 
P(−) ≤ P(s = 1|−) < 1 (4.5) 

Note that if we set P(s = 1|−) = P(+)P(−) , we obtain a balance dataset where the number of 
positive and negative instance be the same. At the same time, if we set P(s = 1|−) = 1, 
no negative instance be remove and no undersampling take place. Using (4.4), we 

can rewrite (4.3) a 

P(+|x, s = 1) = P(+|x)P(+|x) + P(s = 1|−)P(−|x) (4.6) 

Let u denote β = P(s = 1|−) a the probability of select a negative instance with 
undersampling, p = P(+|x) a the true posterior probability of class + on the origi- 
nal dataset, and p = P(+|x, s = 1) a the true posterior probability of class + after 
sampling. We can rewrite equation (4.6) as: 

p = 
p 

p+ β(1− p) (4.7) 



Chapter 4. Techniques for unabalanced classification task 61 

Equation (4.7) quantifies the amount of warp of the posterior probability due to 

undersampling.1 From it, we can derive p a a function of ps: 

p = 
βps 

βps − p + 1 
(4.8) 

The relation between p and p (parametric in β) be illustrate in Figure 4.2. The top 

Figure 4.2: p and p at different β. When β be low, undersampling be strong, which 
mean it be remove a lot of negatives, while for high value the removal be less strong. 

Low value of β lead to a more balance problem. 

curve of Figure 4.2 refers to the complete balance which corresponds to β = P(+)P(−) ≈ N 
+ 

N− , 

assume that N 
+ 

N− provide an accurate estimation of the ratio of the prior probabilities. 

Figure 4.4 illustrates the warp effect for two univariate (n = 1) classification task 

(see Figure 4.3). In both task the two class be normally distribute (X− ∼ N (0, σ) 
and X+ ∼ N (µ, σ)), σ = 3 and P(+) = 0.1 but the degree of separability be different (on 
the left large overlap for µ = 3 and on the right small overlap for µ = 15). It be easy to 

remark that the warp effect be large in the low separable case. 

As a final remark, consider that when β = N 
+ 

N− , the warp due to undersampling map 

two close and low value of p into two value p with a large distance. The opposite 

occurs for high value of p. In Section 4.1.3 we will show how this have an impact on the 

rank return by estimation of p and ps. 
1In the case of oversampling it can be show that p = αpαp+1−p , where α denotes the number of 

time a positive instance be replicated. The large the class imbalanced, the large α have to be in order 
to obtain a balance distribution. 



Chapter 4. Techniques for unabalanced classification task 62 

3 15 

0 

500 

1000 

1500 

−10 0 10 20 −10 0 10 20 
x 

C 
ou 

nt class 
0 
1 

Figure 4.3: Synthetic datasets with positive and negative observation sample from 
two different normal distributions. Positives account for 10% of the 10,000 random 
values. On the left we have a difficult problem with overlap class (µ = 3), on the 

right an easy problem where the class be well-separated (µ = 15). 

Figure 4.4: Posterior probability p a a function of β for two univariate binary 
classification task with norm class conditional density X− ∼ N (0, σ) and X+ ∼ 
N (µ, σ) (on the left µ = 3 and on the right µ = 15, in both example σ = 3). Note 

that the original probability p corresponds to p when β = 1. 

4.1.2 Warping and class separability 

In this section we be go to show how the impact of warp depends on the sepa- 

rability nature of the classification task. Let ω+ and ω− denote the class conditional 

probability P(x|+) and P(x|−), and π+ (π+s ) the class prior before (after) under- 
sampling. It be possible to derive the relation between the warp and the difference 

δ = ω+−ω− between the class conditional distributions. From Bayes’ theorem we have: 

p = 
ω+π+ 

ω+π+ + ω−π− 
(4.9) 

Suppose δ = ω+ − ω−, we can write (4.9) as: 

p = 
ω+π+ 

ω+π+ + (ω+ − δ)π− = 
ω+π+ 

ω+(π+ + π−)− δπ− = 
ω+π+ 

ω+ − δπ− (4.10) 



Chapter 4. Techniques for unabalanced classification task 63 

Figure 4.5: ps−p a a function of δ, where δ = ω+−ω− for value of ω+ ∈ {0.01, 0.1} 
when π+s = 0.5 and π+ = 0.1. Note that δ be upper bound to guarantee 0 ≤ p ≤ 1 

and 0 ≤ p ≤ 1. 

since π+ + π− = 1. Similarly, since ω+ do not change with undersampling: 

p = 
ω+π+s 

ω+ − δπ−s 
(4.11) 

Now we can write p − p as: 

p − p = 
ω+π+s 

ω+ − δπ−s 
− ω 

+π+ 

ω+ − δπ− (4.12) 

Since p ≥ p because of (4.7), 0 ≤ p ≤ 1 and 0 ≤ p ≤ 1 we have: 0 ≤ p − p ≤ 1. In 
Figure 4.5 we plot p − p a a function of δ when π+s = 0.5 and π+ = 0.1. For small 
value of the class conditional density it appear that the difference have the high 

value for δ value close to zero. This mean that the warp be high for similar class 

conditional probability (i.e. low separable configurations). 

4.1.3 The interaction between warp and variance of the estimator 

Section 4.1.1 discuss the first consequence of undersampling, i.e. the transformation of 

the original conditional distribution p into a warp conditional distribution p accord 

to equation (4.7). The second consequence of undersampling be the reduction of the 

training set size, which inevitably lead to an increase of the variance of the classifier. 

This section discus how these two effect interact and their impact on the final accuracy 

of the classifier, by focus in particular on the accuracy of the rank of the minority 

class (typically the class of interest). 

Undersampling transforms the original classification task (i.e. estimate the conditional 

distribution p) into a new classification task (i.e. estimate the conditional distribution 

ps). In what follow we aim to ass whether and when undersampling have a beneficial 

effect by change the target of the estimation problem. 



Chapter 4. Techniques for unabalanced classification task 64 

Let u denote by p̂ (resp. p̂s) the estimation of the conditional probability p (resp. ps). 

Assume we have two distinct test point have probability p1 < p2 where ∆p = p2−p1 
with ∆p > 0. A correct classification aim to rank the most probable positive sample 

should rank p2 before p1, since the second test sample have an high probability of 

belonging to the positive class. Unfortunately the value p1 and p2 be not know and 

the rank should rely on the estimate value p̂1 and p̂2. For the sake of simplicity we 

will assume here that the estimator of the conditional probability have the same bias and 

variance in the two test points. This implies p̂1 = p1 + �1 and p̂2 = p2 + �2, where �1 and 

�2 be two realization of the random variable ε ∼ N (b, ν) where b and ν be the bias 
and the variance of the estimator of p. Note that the estimation error �1 and �2 may 

induce a wrong rank if p̂1 > p̂2. 

What happens if instead of estimate p we decide to estimate ps, a in undersampling? 

Note that because of the monotone transformation (4.7), p1 < p2 ⇒ ps,1 < ps,2. Is the 
rank base on the estimation of ps,1 and ps,2 more accurate than the one base on 

the estimation of p1 and p2? 

In order to answer this question let u suppose that also the estimator of p be bias but 

that it variance be large give the small number of samples.2 Then p̂s,1 = ps,1 + η1 
and p̂s,2 = ps,2 + η2, where η ∼ N (bs, νs), νs > ν and ∆ps = ps,2 − ps,1. Let u now 
compute the derivative of p w.r.t. p. From (4.7) we have: 

dp 
dp 

= 
β 

(p+ β(1− p))2 (4.13) 

correspond to a concave function. In particular for p = 0 we have dp = 1βdp, while 

for p = 1 it hold dp = βdp. We will now show that dpsdp be bound in the range [β, 
1 
β ]. 

Let λ be the value of p for which dpsdp = 1: 

λ = 

√ 
β − β 

1− β 

from (4.13) we have 

1 < 
dp 
dp 

< 
1 

β 
, when 0 < p < λ 

and 

β < 
dp 
dp 

< 1 when λ < p < 1. 

It follow that: 

β ≤ dp 
dp 
≤ 1 
β 

(4.14) 

2It be well-known that training a classifier on a reduce dataset lead to probability estimate with 
large variance [220]. 



Chapter 4. Techniques for unabalanced classification task 65 

Let u now suppose that the quantity ∆p be small enough to have an accurate approxi- 

mation ∆ps∆p ≈ 
dp 
dp . We can define the probability of obtain a wrong rank of p̂1 and 

p̂2 as: 

P(p̂2 < p̂1) = P(p2 + �2 < p1 + �1) 
= P(�2 − �1 < p1 − p2) = P(�1 − �2 > ∆p) 

where �2 − �1 ∼ N (0, 2ν).3 By make a hypothesis of normality we have 

P(�1 − �2 > ∆p) = 1− Φ 
( 

∆p√ 
2ν 

) 
(4.15) 

where Φ be the cumulative distribution function of the standard normal distribution. 

Similarly, the probability of a rank error with undersampling is: 

P(p̂s,2 < p̂s,1) = P(η1 − η2 > ∆ps) 

and 

P(η1 − η2 > ∆ps) = 1− Φ 
( 

∆ps√ 
2νs 

) 
(4.16) 

We can now say that a classifier learn after undersampling have good rank w.r.t. a 

classifier learn with unbalanced distribution when 

P(�1 − �2 > ∆p) > P(η1 − η2 > ∆ps) (4.17) 

or equivalently from (4.15) and (4.16) when 

1− Φ 
( 

∆p√ 
2ν 

) 
> 1− Φ 

( 
∆ps√ 

2νs 

) 
⇐⇒ Φ 

( 
∆p√ 

2ν 

) 
< Φ 

( 
∆ps√ 

2νs 

) 
which boil down to 

∆p√ 
2ν 

< 
∆ps√ 

2νs 
⇐⇒ ∆ps 

∆p 
> 

√ 
νs 
ν 
> 1 (4.18) 

since Φ be monotone non decrease and we have assume that νs > ν. Then from (4.18), 

it follow that undersampling be useful in term of more accurate rank if: 

dp 
dp 

> 

√ 
νs 
ν 

(4.19) 

or, use (4.13), when: 
β 

(p+ β(1− p))2 > 
√ 
νs 
ν 

(4.20) 

3We assume that the bias of p̂1 and p̂2 be similar: E[�2 − �1] = b2 − b1 = 0. 



Chapter 4. Techniques for unabalanced classification task 66 

(a) (b) 

Figure 4.6: Left: dpsdp a a function of p. Right: 
dp 
dp a a function of β 

The value of this inequality depends on several terms: the rate of undersampling β, the 

ratio of the variance of the two classifier and the posteriori probability p of the test 

point. Also the nonlinearity of the first left-hand term suggests a complex interaction 

between the involve terms. For instance if we plot the left-hand term of (4.20) a a 

function of the posteriori probability p (Figure 4.6(a)) and of the value β (Figure 4.6(b)), 

it appear that the most favorable configuration for undersampling occur for the low 

value of the posteriori probability (e.g. non separable or badly separable configurations) 

and intermediate β (neither too unbalanced nor too balanced). However if we modify 

β, this have an impact on the size of the training set and consequently on the right-hand 

term (i.e. variance ratio) too. Also, though the designer can control the β term, the 

other two term vary over the input space. This mean that the condition (4.20) do 

not necessarily hold for all the test points. 

In order to illustrate the complexity of the interaction, let u consider two univariate 

(n = 1) classification task where the minority class be normally distribute around zero 

and the majority class be distribute a a mixture of two Gaussians. Figure 4.7 and 4.8 

show the non separable and separable case, respectively: on the left side we plot the 

class conditional distribution (thin lines) and the posterior distribution of the minority 

class (thicker line), while on the right side we show the left and the right term of the 

inequality (4.20) (solid: left-hand term, dotted: right-hand term). What emerges form 

the figure be that the least separable region (i.e. the region where the posteriori of the 

minority class be low) be also the region where undersampling help more. However, 

the impact of undersampling on the overall accuracy be difficult to be predict since 

the region where undersampling be beneficial change with the characteristic of the 

classification task and the rate β of undersampling. 



Chapter 4. Techniques for unabalanced classification task 67 

−2 −1 0 1 2 

0. 
0 

0. 
2 

0. 
4 

0. 
6 

0. 
8 

1. 
0 

1. 
2 

x 

Po 
st 

er 
io 

r p 
ro 

ba 
bi 

lit 
y 

(a) Class conditional distribution (thin lines) and 
the posterior distribution of the minority class 
(thicker line). 

(b) dp 
dp 

(solid lines), 
√ 

νs 
ν 

(dotted lines). 

Figure 4.7: Non separable case. On the right we plot both term of inequality 4.20 
(solid: left-hand, dotted: right-hand term) for β = 0.1 and β = 0.4 

−2 −1 0 1 2 

0. 
0 

0. 
2 

0. 
4 

0. 
6 

0. 
8 

1. 
0 

1. 
2 

x 

Po 
st 

er 
io 

r p 
ro 

ba 
bi 

lit 
y 

(a) Class conditional distribution (thin lines) and 
the posterior distribution of the minority class 
(thicker line). 

(b) dp 
dp 

(solid lines), 
√ 

νs 
ν 

(dotted lines). 

Figure 4.8: Separable case. On the right we plot both term of inequality 4.20 (solid: 
left-hand, dotted: right-hand term) for β = 0.1 and β = 0.4 

4.1.4 Experimental validation 

In this section we ass the validity of the condition (4.20) by perform a number of 

test on synthetic and real datasets. We first simulate two unbalanced synthetic task 

(5% and 25% of positive samples) with overlap class and generate a test set 

and several training set from the same distribution. Figures 4.9(a) and Figure 4.11(a) 

show the distribution of the test set for the two tasks. 

In order to compute the variance of p̂ and p̂s in each test point, we generate 1000 time 

a training set and we estimate the conditional probability on the basis of sample mean 

and covariance. 



Chapter 4. Techniques for unabalanced classification task 68 

In Figure 4.9(b) (first task) we plot 
√ 

νs 
ν (dotted line) and three percentile (0.25, 0.5, 0.75) 

of dpsdp vs. the rate of undersampling β. It appear that for at least 75% of the test 

points, the term dpsdp be high than 
√ 

νs 
ν . In Figure 4.10(a) the point surround with 

a triangle be those one for which dpsdp > 
√ 

νs 
ν hold when β = 0.053 (balanced dataset). 

For such sample we expect that rank return by undersampling (i.e. base on p̂s ) 

be good than the one base on the original data (i.e. base on p̂). The plot show that 

undersampling be beneficial in the region where the majority class be situated, which be 

also the area where we expect to have low value of p. Figure 4.10(b) show also that 

this region move towards the minority class when we do undersampling with β = 0.323 

(90% negatives, 10% positive after undersampling). 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 
● 

● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

●● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

0 

5 

10 

15 

20 

0 5 10 15 

X1 

X 
2 

class 

● 

● 

0 

1 

(a) Synthetic dataset 1 (b) 
√ 

νs 
ν 

and dp 
dp 

for different β 

Figure 4.9: Left: distribution of the test set where positive account for 5% of the 
total. Right: plot of dpsdp percentile (25 

th, 50th and 75th) and of 
√ 

νs 
ν (black dashed). 

In order to measure the quality of the ranking base on p̂s and p̂ we compute the Kendall 

rank correlation of the two estimate with p, which be the true posterior probability of 

the test set that defines the correct ordering. In Table 4.1 we show the rank 

correlation of p̂s (and p̂) with p for the sample where the condition (4.20) (first five 

rows) hold and where it do not (last five rows). The result indicate that point for 

which condition (4.20) be satisfied have indeed good rank with p̂s than p̂. 

We repeat the experiment for the second task have a large proportion of positive 

(25%) (dataset 2 in Figure 4.11(a)). From the Figure 4.11(b), plot dpsdp and 
√ 

νs 
ν a 

a function of β, it appear that only the first two percentile be over 
√ 

νs 
ν . This mean 

that less point of the test set satisfy the condition (4.20). This be confirm from 

the result in Table 4.2 where it appear that the benefit due to undersampling be less 

significant than for the first task. 



Chapter 4. Techniques for unabalanced classification task 69 

(a) Undersampling with β = 0.053 (b) Undersampling with β = 0.323 

Figure 4.10: Regions where undersampling should work. Triangles indicate the test 
sample where the condition (4.20) hold for the dataset in Figure 4.9. 

Table 4.1: Classification task in Figure 4.9: Ranking correlation between the poste- 
rior probability p̂ (p̂s) and p for different value of β. The value K (Ks) denotes the 
Kendall rank correlation without (with) undersampling. The first (last) five line refer 

to sample for which the condition (4.20) be (not) satisfied. 

β K Ks Ks −K %points satisfy (4.20) 
0.053 0.298 0.749 0.451 88.8 
0.076 0.303 0.682 0.379 89.7 
0.112 0.315 0.619 0.304 91.2 
0.176 0.323 0.555 0.232 92.1 
0.323 0.341 0.467 0.126 93.7 
0.053 0.749 0.776 0.027 88.8 
0.076 0.755 0.773 0.018 89.7 
0.112 0.762 0.764 0.001 91.2 
0.176 0.767 0.761 -0.007 92.1 
0.323 0.768 0.748 -0.020 93.7 

Now we ass the validity of the condition (4.20) on a number of real unbalanced binary 

classification task obtain by transform some datasets from the UCI repository [1] 

(Table 4.3)4. 

Given the unavailability of the conditional posterior probability function, we first ap- 

proximate p by fitting a Random Forest over the entire dataset in order to compute the 

left-hand term of (4.20). Then we use a bootstrap procedure to estimate p̂ and apply 
4 Transformed datasets be available at http://www.ulb.ac.be/di/map/adalpozz/ 

imbalanced-datasets.zip 

http://www.ulb.ac.be/di/map/adalpozz/imbalanced-datasets.zip 
http://www.ulb.ac.be/di/map/adalpozz/imbalanced-datasets.zip 


Chapter 4. Techniques for unabalanced classification task 70 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

●● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

●● 

● 
● 

● 

● ● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 
● 

● ● 

● 

● 

● 

● ● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 
● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● ● 
● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● ● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

−5 

0 

5 

10 

15 

0 10 20 

X1 

X 
2 

class 

● 

● 

0 

1 

(a) Synthetic dataset 2 (b) 
√ 

νs 
ν 

and dp 
dp 

for different β 

Figure 4.11: Left: distribution of the test set where positive account for 25% 
of the total. Right: plot of dpsdp percentile (25 

th, 50th and 75th) and of 
√ 

νs 
ν (black 

dashed). 

Table 4.2: Classification task in Figure 4.11: Ranking correlation between the pos- 
terior probability p̂ (p̂s) and p for different value of β. The value K (Ks) denotes the 
Kendall rank correlation without (with) undersampling. The first (last) five line refer 

to sample for which the condition (4.20) be (not) satisfied. 

β K Ks Ks −K % point satisfy (4.20) 
0.333 0.586 0.789 0.202 66.4 
0.407 0.588 0.761 0.172 66.6 
0.500 0.605 0.738 0.133 68.1 
0.619 0.628 0.715 0.087 70.3 
0.778 0.653 0.693 0.040 73 
0.333 0.900 0.869 -0.030 66.4 
0.407 0.899 0.875 -0.024 66.6 
0.500 0.894 0.874 -0.020 68.1 
0.619 0.885 0.869 -0.016 70.3 
0.778 0.870 0.856 -0.014 73 

undersampling to the original dataset to estimate p̂s. We repeat bootstrap and under- 

sample 100 time to compute the right hand term 
√ 

νs 
ν . This allows u to define the 

subset of point for which the condition (4.20) holds. 

Figure 4.12 report the difference between Kendall rank correlation of p̂s and p̂, av- 

eraged over different level of undersampling (proportions of majority vs. minority: 

90/10, 80/20, 60/40, 50/50). Higher difference mean that p̂s return a good order 

than p̂ (assuming that the rank provide by p be correct). The plot distinguishes 

between sample for which condition (4.20) be satisfied and not. In general we see that 

point with a positive difference corresponds to those have the condition satisfied and 



Chapter 4. Techniques for unabalanced classification task 71 

Table 4.3: Selected datasets from the UCI repository [1] 

Datasets N N+ N− N+/N 
ecoli 336 35 301 0.10 
glass 214 17 197 0.08 
letter-a 20000 789 19211 0.04 
letter-vowel 20000 3878 16122 0.19 
ism 11180 260 10920 0.02 
letter 20000 789 19211 0.04 
oil 937 41 896 0.04 
page 5473 560 4913 0.10 
pendigits 10992 1142 9850 0.10 
PhosS 11411 613 10798 0.05 
satimage 6430 625 5805 0.10 
segment 2310 330 1980 0.14 
boundary 3505 123 3382 0.04 
estate 5322 636 4686 0.12 
cam 18916 942 17974 0.05 
compustat 13657 520 13137 0.04 
covtype 38500 2747 35753 0.07 

the opposite for negative differences. These result seem to confirm the experiment with 

synthetic data, where a good order be give by p̂s when the condition (4.20) holds. 

Figure 4.12: Difference between the Kendall rank correlation of p̂s and p̂ with p, 
namely Ks and K, for point have the condition (4.20) satisfied and not. Ks and K 

be calculate a the mean of the correlation over all βs. 

In Figure 4.13 we show the ratio of sample in each dataset satisfy condition (4.20) 

average over all the βs. The proportion of point in which undersampling be useful 

change heavily with the dataset considered. For example, in the datasets vehicle, yeast, 

german and pima, underdamping return a good order for more than 80% of the 

samples, while the proportion drop to less than 50% in the page dataset. 

This seem to confirm our intuition that the right amount of undersampling depends on 

the classification task (e.g. degree of non separability), the learn algorithm and the 

target test set. It follow that there be no reason to believe that undersampling until 

the two class be perfectly balance be the default strategy to adopt. 



Chapter 4. Techniques for unabalanced classification task 72 

It be also worth to remark that the check of the condition (4.20) be not easy to be done, 

since it involves the estimation of 
√ 

νs 
ν (ratio of the variance of the classifier before and 

after undersampling) and of dpsdp , which demand the knowledge of the true posterior 

probability p. In practice since p be unknown in real datasets, we can only rely on a data 

driven approximation of dpsdp . Also the estimation of 
√ 

νs 
ν be a hard statistical problem, 

a know in the statistical literature on ratio estimation [221]. 

Figure 4.13: Ratio between the number of sample satisfy condition (4.20) and all 
the instance available in each dataset average over all the βs. 

4.1.5 Discussion 

Undersampling have become the de facto strategy to deal with skewed distributions, but, 

though easy to be justified, it conceals two major effects: i) it increase the variance 

of the classifier and ii) it produce warp posterior probabilities. The first effect be 

typically address by the use of average strategy (e.g. UnderBagging [117]) to reduce 

the variability while the second require the calibration of the probability to the new 

prior of the test set [134]. Despite the popularity of undersampling for unbalanced 

classification tasks, it be not clear how these two effect interact and when undersampling 

lead to good accuracy in the classification task. 

In this first part of the chapter, we aim to analyze the interaction between undersam- 

pling and the rank error of the posterior probability. We derive the condition (4.20) 

under which undersampling can improve the rank and we show that when it be satis- 

fied, the posterior probability obtain after sample return a more accurate order of 

test instances. To validate our claim we use first synthetic and then real datasets, and 

in both case we register a good rank with undersampling when condition (4.20) 

be met. It be important to remark how this condition show that the beneficial impact 

of undersampling be strongly dependent on the nature of the classification task (degree 

of imbalance and non separability), on the variance of the classifier and a a consequence 

be extremely dependent on the specific test point. We think that this result shed light 



Chapter 4. Techniques for unabalanced classification task 73 

on the reason why several discordant result have be obtain in the literature about 

the effectiveness of undersampling in unbalanced tasks. 

However, the practical use of this condition be not straightforward since it require the 

knowledge of the posteriori probability and of the ratio of variance before and after 

undersampling. It follow that this result should be use mainly a a warn against a 

naive use of undersampling in unbalanced task and should suggest instead the adoption 

of specific adaptive selection technique (e.g. race [27], see Section 4.3) to perform a 

case-by-case use (and calibration) of undersampling. 

4.2 Using calibrate probability with undersampling 

In the previous section we show with equation (4.7) that undersampling induces an 

artificial bias (warping) into the posterior probability return by a model. We now 

demonstrate how to correct for this bias use the equation derive in Section 4.1.1. 

Although this method do not affect the rank order return by the posterior proba- 

bility, it significantly impact the classification accuracy and probability calibration. We 

use Bayes Minimum Risk theory [30] to find the correct classification threshold and show 

how to adjust it after undersampling. Experiments on several real-world unbalanced 

datasets validate our results. 

4.2.1 Adjusting posterior probability to new prior 

The first part of the chapter show that undersampling be responsible for a drift in pos- 

terior probability and induces warp probability estimate (see Section 4.1). However, 

the first and most direct effect of undersampling be the change in the class priors. To 

show this effect, let u use an illustrative example. 

Let’s suppose we have an unbalanced problem where the positive account for 10% of 

10,000 observation (i.e., we have 1,000 positive and 9,000 negatives). Suppose we want 

to have a balance dataset β = N 
+ 

N− ≈ 0.11, where ≈ 88.9% (8000/9000) of the negative 
instance be discharged. Table 4.4 show how, by reduce β, the original unbalanced 

dataset becomes more balance and small a negative instance be removed. After 

undersampling, the number of negative be N−s = βN−, while the number of positive 

stay the same N+s = N+. The percentage of negative (perc−) in the dataset decrease 

a N−s → N+. 

After a classification model be learn on a balance training set, it be normally use to 

predict a test set, which be likely to have an unbalanced distribution similar to the 



Chapter 4. Techniques for unabalanced classification task 74 

Table 4.4: Undersampling a dataset with 1,000 positive in 10,000 observations. Ns 
defines the size of the dataset after undersampling and N−s (N+s ) the number of negative 
(positive) instance for a give β. When β = 0.11 the negative sample represent 50% 

of the observation in the dataset. 

Ns N 
− 
s N 

+ 
s β perc 

− 

2,000 1,000 1,000 0.11 50.00 
2,800 1,800 1,000 0.20 64.29 
3,700 2,700 1,000 0.30 72.97 
4,600 3,600 1,000 0.40 78.26 
5,500 4,500 1,000 0.50 81.82 
6,400 5,400 1,000 0.60 84.38 
7,300 6,300 1,000 0.70 86.30 
8,200 7,200 1,000 0.80 87.80 
9,100 8,100 1,000 0.90 89.01 
10,000 9,000 1,000 1.00 90.00 

original training set. This mean that the posterior probability of a model learn on the 

balance training set should be adjust for the change in prior between the training 

and test sets. In this section we propose to use equation (4.8) to correct the posterior 

probability estimate after undersampling. Let u call p′ the bias-corrected probability 

obtain from p use (4.8): 

p′ = 
βps 

βps − p + 1 
(4.21) 

Equation (4.21) can be see a a special case of the framework propose by Saerens et 

al. [134] and Elkan [35] for correct the posterior probability in the case of test and 

training set share the same priors. 

Let pt = p(yt = +|xt) be the posterior probability for a test instance (xt, yt), where 
the test set have priors: π−t = 

N−t 
Nt 

and π+t = 
N+t 
Nt 

. In the unbalanced training set 

we have π− = N 
− 

N , π 
+ = N 

+ 

N and after undersampling the training set π 
− 
s = 

βN− 

N++βN− , 

π+s = 
N+ 

N++βN− . If we assume that the class conditional distribution P(x|+) and P(x|−) 
remain the same between the training and test sets, Saerens et al. [134] show that, 

give different prior between the training and test sets, the posterior probability can 

be correct with the follow equation: 

pt = 

π+t 
π+s 
p 

π+t 
π+s 
p + 

π−t 
π−s 

(1− ps) 
(4.22) 

Let u assume that the training and test set share the same priors: π+t = π+ and 

π−t = π 
−: 

pt = 

π+ 

π+s 
p 

π+ 

π+s 
p + 

π− 

π−s 
(1− ps) 



Chapter 4. Techniques for unabalanced classification task 75 

Then, since 
π+ 

π+s 
= 

N+ 

N++N− 

N+ 

N++βN− 

= 
N+ + βN− 

N+ +N− 
(4.23) 

π− 

π−s 
= 

N− 

N++N− 

βN− 

N++βN− 

= 
N+ + βN− 

β(N+ +N−) 
(4.24) 

we can write 

pt = 

N++βN− 

N++N− p 
N++βN− 

N++N− p + 
N++βN− 

β(N++N−)(1− ps) 

pt = 
p 

p + 
(1−ps) 
β 

= 
βps 

βps − p + 1 

Hence, the transformation propose by Saerens et al. [134] be essentially equivalent 

to (4.21). Similarly, Elkan [35] proposes to adjust the posterior probability after un- 

dersampling with the follow equation: 

pt = π 
+ 
t 

p − π+s p 
π+s − π+s p + π+t p − π+t π+s 

(4.25) 

pt = 
(1− π+s )ps 

π+s 
π+t 

(1− ps) + p − π+s 

use equation (4.23) and the assumption that π+t = π+ and π 
− 
t = π 

−: 

pt = 

βN− 

N++βN− p 
N++N− 

N++βN− (1− ps) + p − N 
+ 

N++βN− 

pt = 
βN−ps 

(N+ +N−)(1− ps) + (N+ + βN−)ps −N+ 

pt = 
βN−ps 

N− −N−ps + βN−ps 
= 

βps 
βps − p + 1 

Equation (4.25) be equivalent to (4.21) and therefore to the one propose by Saerens et 

al. [134]. 

In summary, when we know the prior in the test set we can correct the probability 

with Elkan’s and Saerens’ equations. However, these probability be usually unknown 

and must be estimated. If we make the assumption that training and test have the 

same prior we can use (4.21) for calibrate ps. Note that the above transformation will 

not affect the rank produce by ps. Equation (4.21) defines a monotone transforma- 

tion, hence the rank of p will be the same a p′. While p be estimate use all the 

sample in the unbalanced dataset, p and p′ be compute consider a subset of the 

original sample and therefore their estimation be subject to high variance [213]. 



Chapter 4. Techniques for unabalanced classification task 76 

4.2.2 Warping correction and classification threshold adjustment 

As previously described in Section 2.1.2, a classifier typically defines the optimal class 

of a sample a the one minimize the risk. In practice, this translates into calculate 

the posterior probability and predict an instance a positive or negative when the 

probability be above a certain threshold. If we assume that there be no cost in case of 

correct prediction, then from (2.9) the threshold minimize the risk is: 

τ = 
l1,0 

l1,0 + l0,1 

where li,j be the cost occur in decide i when the true class be j. In an unbalanced 

problem, the cost of miss a positive instance (false negative) be usually high than 

the cost of miss a negative (false positive). If the cost of a false negative and false 

positive be unknown, a natural solution be to set these cost use the prior (π− and 

π+). Let l1,0 = π+ and l0,1 = π−. Then, since π− > π+ we have l0,1 > l1,0 a desired. 

We can then write 

τ = 
l1,0 

l1,0 + l0,1 
= 

π+ 

π+ + π− 
= π+ (4.26) 

since π+ + π− = 1. This be also the optimal threshold in a cost-sensitive application 

where the goal be to minimize overall cost and the misclassification cost be define 

use the prior [35]. 

Even if undersampling produce warp probability estimates, it be often use to balance 

datasets with skewed class distribution because several classifier have empirically show 

good performance when train on a balance dataset [88, 90]. Let τs denote the 

threshold use to classify an observation after undersampling, from (4.26) we have τs = 

π+s , where π+s be the positive class prior after undersampling. In the case of undersampling 

with β = N 
+ 

N− (balanced training set) we have τs = 0.5. 

When correct p with (4.21), we must also correct the probability threshold to main- 

tain the predictive accuracy define by τs (this be need otherwise we would use different 

misclassification cost for p′). Let τ ′ be the threshold for the unbiased probability p′. 

From Elkan [35]: 
τ ′ 

1− τ ′ 
1− τs 
τs 

= β (4.27) 

τ ′ = 
βτs 

(β − 1)τs + 1 
(4.28) 

Using τs = π+s , (4.28) becomes: 

τ ′ = 
βπ+s 

(β − 1)π+s + 1 



Chapter 4. Techniques for unabalanced classification task 77 

τ ′ = 
β N 

+ 

N++βN− 

(β − 1) N+ 
N++βN− + 1 

= 
N+ 

N+ +N− 
= π+ 

The optimal threshold to use with p′ be equal to the one for p. As an alternative to 

classify observation with p and τs, we can obtain equivalent result with p′ and 

τ ′. In summary, a a result of undersampling, a high number of observation be 

predict a positive, but the posterior probability be bias due to a change in the 

priors. Equation (4.28) allows u find the threshold that guarantee equal accuracy after 

the posterior probability correction. Therefore, in order to classify observation with 

unbiased probability after undersampling, we have to first obtain p′ from p with (4.21) 

and then use τ ′ a a classification threshold. 

4.2.3 Experimental result 

The rational of the follow experiment be to compare the probability estimate of two 

models, one learn in the presence and the other in the absence of undersampling, and 

test the benefit of probability calibration after undersampling. We use G-mean a a 

measure of classification accuracy, AUC to ass the quality of the rank produce by 

the probability and Brier Score (BS) a a measure of probability calibration (see Section 

2.1.4). In our experiment we use the same datasets of Section 4.1.4 (Table 4.3). For 

each dataset we use 10-fold CV to test our model and we repeat the CV 10 times. 

In particular, we use a stratify CV, where the class proportion in the datasets be 

kept the same over all the folds. As the original datasets be unbalanced, the result 

fold be unbalanced a well. For each fold of CV we learn two models: one use 

all the observation and the other with the one remain after undersampling. Then 

both model be test on the same test set (Figure 4.14). We use several supervise 

classification algorithm available in R [25] with default parameters: RF [222], SVM [223] 

and Logit Boost (LB) [224]. 

We denote a p̂s and p̂ the posterior probability estimate obtain with and without 

undersampling and a p̂′ the bias-corrected probability obtain from p̂s with equa- 

tion (4.21). Let τ , τs and τ ′ be the probability threshold use for p̂, p̂s and p̂′ re- 

spectively, where τ = π+, τs = π+s and τ ′ = π+. The goal of these experiment be to 

compare which probability estimate return the high rank (AUC), calibration (BS) 

and classification accuracy (G-mean) when couple with the threshold define before. 

In undersampling, the amount of sample define by β be usually set to be equal to N 
+ 

N− , 

lead to a balance dataset where π+s = π−s = 0.5. However, there be no reason to 

believe that this be the optimal sample rate. Often, the optimal rate can be found only 

a posteriori after try different value of β. For this reason we replicate the CV with 



Chapter 4. Techniques for unabalanced classification task 78 

Test%set% Train%set% 

Undersampling%Unbalanced%Model% 

Balanced%Model%p̂ 

ˆ!p 

τ 

τ ' 

τ s 

Fold%1% Fold%2% Fold%3% Fold%4% Fold%10% 

Unbalanced%Dataset% 

.%.%.%.% 

p̂s 

Figure 4.14: Learning framework for compare model with and without undersam- 
pling use CV. We use one fold of the CV a test set and the others for training, 

and iterate the framework to use all the fold once for testing. 

different β such that {N+N− ≤ β ≤ 1} and for each CV the accuracy be compute a the 
average G-mean (or AUC) over all the folds. 

In Table 4.5 we report the result over all the datasets. For each dataset, we rank the 

probability estimate p̂s, p̂ and p̂′ from the bad to the best perform for different 

value of β. We then sum the rank over all the value of β and over all datasets. More 

formally, let Ri,k,b ∈ {1, 2, 3} be the rank of probability i on dataset k when β = b. 
The probability with the high accuracy in k when β = b have Ri,k,b = 3 and the one 

with the low have Ri,k,b = 1. Then the sum of rank for the probability i be define as∑ 
k 

∑ 
bRi,k,b. The high the sum, the high the number of time that one probability 

have high accuracy than the others. 

For AUC, a high rank sum mean a high AUC and hence a good rank return 

by the probability. Similarly, with G-mean, a high rank sum corresponds to high 

predictive accuracy. However, in the case of BS, a high rank sum mean poorer prob- 

ability calibration (larger bias). Table 4.5 have in bold the probability with the best 

rank sum accord to the different metrics. For each metric and classifier it report the 

p-values of the pair t-test base on the rank between p̂ and p̂′ and between p̂ and p̂s. 

In term of AUC, we see that p̂s and p̂′ have good performance than p̂ for LB and 

SVM. The rank sum be the same for p̂s and p̂′ since the two probability be link by 

a monotone transformation (equation (4.21)). If we look at G-mean, p̂s and p̂′ return 

good accuracy than p̂ two time out of three. In this case, the rank sum of p̂s and p̂′ 

be the same since we use τs and τ ′ a the classification threshold, where τ ′ be obtain 



Chapter 4. Techniques for unabalanced classification task 79 

from τs use (4.28). If we look at the p-values, we can strongly reject the null hypothesis 

that the accuracy of p̂s and p̂ be from the same distribution. For all classifiers, p̂ be the 

probability estimate with the best calibration (lower rank sum with BS), follow by p̂′ 

and p̂s. The rank sum of p̂′ be always low than the one of p̂s, indicate that p̂′ have 

low bias than p̂s. This result confirms our claim that equation (4.21) allows one to 

reduce the bias introduce by undersampling. 

In summary from this experiment we can conclude that undersampling do not always 

improve the rank or classification accuracy of an algorithm, but when it be the case 

we should use p̂′ instead of p̂s because the first have always good calibration. 

Table 4.5: Sum of rank and p-values of the pair t-test between the rank of p̂ and 
p̂′ and between p̂ and p̂s for different metrics. In bold the probability with the best 

rank sum (higher for AUC and G-mean, low for BS). 

Metric Algo 
∑ 
Rp̂ 

∑ 
Rp̂s 

∑ 
Rp̂′ ρ(Rp̂, Rp̂s) ρ(Rp̂, Rp̂′) 

AUC LB 22,516 23,572 23,572 0.322 0.322 
AUC RF 24,422 22,619 22,619 0.168 0.168 
AUC SVM 19,595 19,902.5 19,902.5 0.873 0.873 

G-mean LB 23,281 23,189.5 23,189.5 0.944 0.944 
G-mean RF 22,986 23,337 23,337 0.770 0.770 
G-mean SVM 19,550 19,925 19,925 0.794 0.794 

BS LB 19809.5 29448.5 20402 0.000 0.510 
BS RF 18336 28747 22577 0.000 0.062 
BS SVM 17139 23161 19100 0.001 0.156 

We now consider a real-world dataset, compose of credit card transaction from Septem- 

ber 2013 make available by our industrial partner.5 It contains a subset of online trans- 

action that occur in two days, where we have 492 fraud out of 284,807 transactions. 

The dataset be highly unbalanced, where the positive class (frauds) account for 0.172% 

of all transactions, and the minimum value of β be ≈ 0.00173. In Figure 4.15 we have the 
AUC for different value of β. The boxplots of p̂s and p̂′ be identical because of (4.21), 

they increase with β → N+ 
N− and have high median than the one of p̂. This example 

show how in case of extreme class imbalance, undersampling can improve predictive 

accuracy of several classification algorithms. 

In Figure 4.16 we have the BS for different value of β. The boxplots of p̂′ show in 

general small calibration error (lower BS) than those of p̂s and the latter have high 

BS especially for small value of β. This support our previous results, which found that 

the loss in probability calibration for p̂s be great the strong the undersampling. 
5The dataset be available at http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata 

http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata 


Chapter 4. Techniques for unabalanced classification task 80 

LB RF SVM 

●●●●●●●●●● ●●●●●●●●●● 
●●●●●●●●●● ●●●●●●●●●● 

●●●●●●●●●●●●●●●●●●●● ●●●●●●●●●●●●●●●●●●●● 

●●●●●● ●●●●●● 

●●●●●● ●●●●●● 

●●●●●● ●●●●●● 

●●●●●●●● ●●●●●●●● 

●●●●●●●● ●●●●●●●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

0.900 

0.925 

0.950 

0.975 

1.000 

0. 
1 
0. 

2 
0. 

3 
0. 

4 
0. 

5 
0. 

6 
0. 

7 
0. 

8 
0. 

9 1 
0. 

1 
0. 

2 
0. 

3 
0. 

4 
0. 

5 
0. 

6 
0. 

7 
0. 

8 
0. 

9 1 
0. 

1 
0. 

2 
0. 

3 
0. 

4 
0. 

5 
0. 

6 
0. 

7 
0. 

8 
0. 

9 1 

beta 

A 
U 

C 

Probability 
p 
p' 
p 

Credit−card 

Figure 4.15: Boxplot of AUC for different value of β in the Credit-card dataset. 

LB RF SVM 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 

●●●●●●●●●● 

●●●●●●●●●● 

●●●●●● 

●●●●●● 

● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 
● 

●●●●●● 

●●●●●● 

●●●●●●●● ●●●●●●●● 

3e−04 

6e−04 

9e−04 

0. 
1 
0. 

2 
0. 

3 
0. 

4 
0. 

5 
0. 

6 
0. 

7 
0. 

8 
0. 

9 1 
0. 

1 
0. 

2 
0. 

3 
0. 

4 
0. 

5 
0. 

6 
0. 

7 
0. 

8 
0. 

9 1 
0. 

1 
0. 

2 
0. 

3 
0. 

4 
0. 

5 
0. 

6 
0. 

7 
0. 

8 
0. 

9 1 

beta 

B 
S 

Probability 
p 
p' 
p 

Credit−card 

Figure 4.16: Boxplot of BS for different value of β in the Credit-card dataset. 

4.2.4 Discussion 

The warp due to the instance selection procedure in undersampling be essentially 

equivalent to the bias that occurs with a change in the prior when class-within distribu- 

tions remain stable. With undersampling, we create a different training set, where the 

class be less unbalanced. However, if we make the assumption that the training and 

test set come from the same distribution, it follow that the probability estimate 

obtain after undersampling be biased. As a result of undersampling, the posterior 

probability p̂s be shift away from the true distribution, and the optimal separation 

boundary move towards the majority class so that more case be classify into the 

minority class. 



Chapter 4. Techniques for unabalanced classification task 81 

By make the assumption that prior probability do not change from training and 

testing, i.e. they both come form the same data generate process, we propose the 

transformation give in equation (4.21), which allows u to remove the drift in p̂s due to 

undersampling. 

The bias on p̂s register by BS get large for small value of β, which mean strong un- 

dersampling produce probability with poorer calibration (larger loss). With synthetic, 

UCI and Credit-card datasets, the drift-corrected probability (p̂′) have significantly good 

calibration than p̂s (lower Brier Score). 

Even if undersampling produce poorly calibrate probability estimate p̂s, several stud- 

y have show that it often provide good predictive accuracy than p̂ [88, 90]. To 

improve the calibration of p̂s we propose to use p̂′ since this transformation do not af- 

fect the ranking. In order to maintain the accuracy obtain with p̂s and the probability 

threshold τs, we propose to use p̂′ together with τ ′ to account for the change in priors. 

By change the undersampling rate β we give different cost to false positive and false 

negatives, combine p̂′ with τ ′ allows one to maintain the same misclassification cost 

of a classification strategy with p̂s and τs for any value of β. 

Finally, we consider a highly unbalanced dataset (Credit-card), where the minority 

class account for only 0.172% of all observations. In this dataset, the large improvement 

in accuracy obtain with undersampling be couple with poor calibrate probability 

(large BS). By correct the posterior probability and change the threshold we be 

able to improve calibration without lose predictive accuracy. 

4.3 Racing for sample method selection 

As already see in Section 4.1, the degree of imbalance be not the only factor that 

determines the difficulty of a classification/detection task. Another influential factor 

be the amount of overlap of the class of interest [86]. Prati [84] show that class 

unbalance, by itself, do not seem to be a problem. Most study [78, 82, 84, 106] 

propose one method that seem to work well under certain conditions, however there be 

no empirical evidence than one technique be superior to all the others. In general the best 

method do not exits, however in some case some technique be good than others, 

this be know in the literature a no-free-lunch Theorem [225, 226]. 

All these support the idea that under different conditions, such a distinct datasets, al- 

gorithms, metrics, the best method may change. Since in real large task it be hard to 

know a priori the nature of the unbalanced tasks, the user be recommend to test all 



Chapter 4. Techniques for unabalanced classification task 82 

technique with a consequent computational overhead. We make an exhaustive compar- 

ison of these method on a real credit-card fraud dataset and nine public benchmark 

datasets. The result show that there be no balance technique which be consistently 

the best one and that the best method depends on the algorithm apply a well a the 

dataset used. For this reason, we propose the adoption of a race strategy [227] to 

automatically select the most adequate technique for a give dataset. The rationale of 

the race strategy consists in test multiple balance strategy on a subset of the 

dataset and to remove progressively the alternative that be significantly worse. Our re- 

sults show that by adopt a race strategy we be able to select in an efficient manner 

either the best balance method or a method that be not significantly different from the 

best one. Moreover, race be able to reduce consistently the computation need before 

find the right method for the dataset. 

4.3.1 Racing for strategy selection 

The variety of approach discuss in Section 3.1 suggests that in a real situation where 

we have no prior information about the data distribution, it be difficult to decide which 

unbalanced strategy to use. In this case test all alternative be not an option either 

because of the associate computational cost. 

A possible solution come form the adoption of the Racing approach which be propose 

in [227] to perform efficiently model selection in a learn task. The principle of Racing 

consists in test in parallel a set of alternative and use a statistical test to determine 

if an alternative be significantly bad than the others. In that case such alternative be 

discard from the competition, and the computational effort be devote to differentiate 

the remain ones. Historically the first example of race method be call Hoeffding 

Race since it relies on the Hoeffding theorem to decide when a model be significantly bad 

than the others. The F-race version be propose in [26] and combine the Friedman test 

with Hoeffding Races [227] to eliminate inferior candidate a soon a enough statistical 

evidence arises against them. In F-race, the Friedman test be use to check whether there 

be evidence that at least one of the candidate be significantly different from others and 

post-tests be apply to eliminate those candidate that be significantly bad than the 

best one. 

Here we adopt F-Race to search efficiently for the best strategy for unbalanced data. 

The candidate be assess on different subset of data and, each time a new assessment 

be made, the Friedman test be use to dismiss significantly inferior candidates. We use a 

10 fold CV to provide the assessment measure to the race. If a candidate be significantly 

good than all the others then the race be terminate without the need of use the whole 



Chapter 4. Techniques for unabalanced classification task 83 

dataset. In case there be no evidence of worse/better methods, the race terminates when 

the entire dataset be explore and the best candidate be the one with the best average 

result. 

Candidate(1 Candidate(2 Candidate(3 
subset(1( 0.50 0.47 0.48 
subset(2 0.51 0.48 0.30 
subset(3 0.51 0.47 
subset(4 0.49 0.46 
subset(5 0.48 0.46 
subset(6 0.60 0.45 
subset(7 0.59 
subset(8 
subset(9 
subset(10 

Original(Dataset 

Tim 
e 

Figure 4.17: Illustrative example of Racing: test in parallel a set of candidate use 
a subset of the dataset and remove from the race those that be significantly bad than 
the best. The race continue with the remain candidate until only one be selected. 

4.3.2 Experimental result 

In these experiment we test some of the technique for unbalanced classification 

discuss in Section 3.1.1 on the datasets of Table 4.3 and the credit card data use 

in [28]. In particular, we consider the follow techniques: undersampling, oversam- 

pling, SMOTE, CNN, ENN, NCL, OSS and Tomek Link. 

We start by perform a CV for each technique with different classification algo- 

rithms: RF [222], Neural Network (NNET) [228], SVM [228], LB [224] and Decision 

Tree [229]. Figure 4.18 display the result of the CV for all the algorithm and datasets. 

For each dataset we compute the average G-mean in the CV and then calculate the 

average accuracy over all the datasets. We also include the performance in the case of 

unbalanced datasets a benchmark. In this study, we see that the combination of RF 

and undersampling appear to return the large accuracy. 

However, the result be highly dependent on the dataset and classifier considered. In 

Figure 4.19 we show the accuracy on two datasets. In the case of the cam dataset under- 

sample be the best technique for all the classification algorithm used. On the contrary, 

we see that for ecoli dataset, there be not a single technique that clearly outperforms the 

others. 



Chapter 4. Techniques for unabalanced classification task 84 

0.0 

0.2 

0.4 

0.6 

0.8 

Un 
ba 

l 
Ov 

er 

Un 
de 

r 

SM 
OT 

E 
OS 

S 
CN 

N 
EN 

N 
NC 

L 

To 
m 

ek 

G 
− 

m 
ea 

n 

Algo 
LB 
NNET 
RF 
SVM 
TREE 

Figure 4.18: Comparison of strategy for unbalanced data with different classifier 
over all datasets of Table 4.3 in term of G-mean (the high the better). 

cam ecoli 

0.00 

0.25 

0.50 

0.75 

Un 
ba 

l 
Ov 

er 

Un 
de 

r 

SM 
OT 

E 
OS 

S 
CN 

N 
EN 

N 
NC 

L 

To 
m 

ek 

Un 
ba 

l 
Ov 

er 

Un 
de 

r 

SM 
OT 

E 
OS 

S 
CN 

N 
EN 

N 
NC 

L 

To 
m 

ek 

G 
− 

m 
ea 

n 

Algo 
LB 
NNET 
RF 
SVM 
TREE 

Figure 4.19: Comparison of strategy for unbalanced data with different classifier 
on cam and ecoli datasets in term of G-mean (the high the better). 

In these experiment the classification accuracy be measure in term of G-mean, but we 

could have different outcome when use another metric [27]. In general there be no single 

strategy that be coherently superior to all the others in all condition (i.e. algorithm and 

dataset). Even if sometimes it be possible to find a strategy that be statistically good 

than others it be computationally demand test all strategy on several datasets 

and algorithms. For this reason in [27] we propose to adopt the F-race algorithm to 

automatize the way to select the best strategy for unbalanced data. The algorithm be 

available in the companion unbalanced package [24] available for the R language (see 

Appendix A). Tables 4.6 and 4.7 show the result of the F-race. 

The first thing we notice be that for almost all datasets F-race be able to return the best 



Chapter 4. Techniques for unabalanced classification task 85 

Dataset Exploration Method Ntest % Gain Mean Sd 

ecoli Race Under 46 49 0.836 0.04CV SMOTE 90 - 0.754 0.112 

letter-a Race Under 34 62 0.952 0.008CV SMOTE 90 - 0.949 0.01 

letter-vowel Race Under 34 62 0.884 0.011CV Under 90 - 0.887 0.009 

letter Race SMOTE 37 59 0.951 0.009CV Under 90 - 0.951 0.01 

oil Race Under 41 54 0.629 0.074CV SMOTE 90 - 0.597 0.076 

page Race SMOTE 45 50 0.919 0.01CV SMOTE 90 - 0.92 0.008 

pendigits Race Under 39 57 0.978 0.011CV Under 90 - 0.981 0.006 

PhosS Race Under 19 79 0.598 0.01CV Under 90 - 0.608 0.016 

satimage Race Under 34 62 0.843 0.008CV Under 90 - 0.841 0.011 

segment Race SMOTE 90 0 0.978 0.01CV SMOTE 90 - 0.978 0.01 

estate Race Under 27 70 0.553 0.023CV Under 90 - 0.563 0.021 

covtype Race Under 42 53 0.924 0.007CV SMOTE 90 - 0.921 0.008 

cam Race Under 34 62 0.68 0.007CV Under 90 - 0.674 0.015 

compustat Race Under 37 59 0.738 0.021CV Under 90 - 0.745 0.017 

creditcard Race Under 43 52 0.927 0.008CV SMOTE 90 - 0.924 0.006 

Table 4.6: Comparison of CV and F-race result in term of G-mean for RF classifier. 

method accord to CV. In the case where there be no agreement between F-race and CV 

on the best method, the difference in performance be however not significant. The main 

advantage of Racing be that bad method be not test on the whole dataset reduce 

the computation needed. Taking into consideration the 8 method and the unbalanced 

case, in a 10 fold CV we have 90 test to make (10 fold x 9 methods). In the case of 

F-race the number of total test depends upon how many fold be need before F-race 

find the best method. The Gain column of Tables 4.6 and 4.7 show the computational 

gain (in percentage of the CV tests) obtain by use F-race. In the case of the segment 

dataset and RF classifier the gain be 0, meaning that the Race do not find significant 

bad candidates. In all the other case F-race allows a significant computational save 

with no loss in performance. 

4.3.3 Discussion 

Recent literature in data mining and machine learn have plenty of research work 

on strategy to deal with unbalanced data. However, a definitive answer on the best 



Chapter 4. Techniques for unabalanced classification task 86 

Dataset Exploration Method Ntest % Gain Mean Sd 

ecoli Race SMOTE 55 39 0.83 0.068CV OSS 90 - 0.613 0.361 

letter-a Race SMOTE 31 66 0.96 0.009CV SMOTE 90 - 0.961 0.008 

letter-vowel Race SMOTE 31 66 0.878 0.007CV SMOTE 90 - 0.876 0.005 

letter Race SMOTE 31 66 0.96 0.007CV SMOTE 90 - 0.96 0.007 

oil Race Over 90 0 0.356 0.155CV Tomek 90 - 0.311 0.272 

page Race SMOTE 31 66 0.91 0.015CV SMOTE 90 - 0.905 0.012 

pendigits Race SMOTE 63 30 0.992 0.003CV SMOTE 90 - 0.992 0.003 

PhosS Race Under 31 66 0.571 0.008CV Under 90 - 0.509 0.112 

satimage Race Under 41 54 0.842 0.011CV SMOTE 90 - 0.837 0.012 

segment Race SMOTE 84 7 0.971 0.021CV CNN 90 - 0.972 0.014 

boundary Race Under 34 62 0.401 0.051CV Under 90 - 0.423 0.079 

estate Race Under 31 66 0.56 0.068CV Under 90 - 0.593 0.016 

covtype Race SMOTE 34 62 0.927 0.007CV SMOTE 90 - 0.924 0.006 

cam Race Under 31 66 0.684 0.026CV Under 90 - 0.686 0.017 

compustat Race Under 27 70 0.742 0.012CV Under 90 - 0.741 0.013 

creditcard Race SMOTE 42 53 0.919 0.011CV Under 90 - 0.916 0.008 

Table 4.7: Comparison of CV and F-race result in term of G-mean for SVM classifier. 

strategy to adopt be yet to come. Our experimental result support the idea that the 

final performance be extremely dependent on the data nature and distribution. 

This consideration have lead u to adopt the F-race strategy where different candidate 

(unbalanced methods) be test simultaneously. We have show that this algorithm be 

able to select few candidate that perform good than other without explore the whole 

dataset. F-race be able to get result similar to the cross validation for most of the 

datasets. 

In general we saw that undersampling and SMOTE together with RF be often the meth- 

od return the large accuracy. As far a the fraud dataset be concerned, we prefer 

undersampling over SMOTE because it reduces the dataset set allow faster training of 

the classifier. However, a the fraud evolve over the time the same method could become 

sub-optimal in the future. In this context the F-race contribution to the selection of the 

best strategy be crucial in order to have a detection system that quickly adapts to the 

new data distribution. Within the UCI datasets we notice that some task be much 

easy (high accuracy) than the others and they may not have an unbalanced method 



Chapter 4. Techniques for unabalanced classification task 87 

that performs significantly good than the others. 

4.4 Conclusion 

A standard approach to deal with unbalanced classification task be to rebalance the 

dataset before training a learn algorithm. One of the most straightforward way to 

achieve a balance distribution be to use undersampling, i.e. remove observation from 

the majority class. Despite the popularity of this technique, no detailed analysis about 

the impact of undersampling on the accuracy of the final classifier be available yet. 

This chapter aim to fill this gap by propose an integrate analysis of the two element 

which have the large impact on the effectiveness of an undersampling strategy: the 

increase of the variance due to the reduction of the number of sample and the warp 

of the posterior distribution due to the change of priori probabilities. 

We propose a theoretical analysis specify under which condition undersampling be 

recommend and expect to be effective. It emerges that undersampling be not al- 

way the best solution and several factor affect it power (e.g. variance of the classifier, 

the degree of imbalance, class separability and the value of the posterior probability.) 

However, when it appear to improve predictive accuracy we show that it be important 

to re-calibrate the posterior probability of a classifier to account for the change in class 

priors. Finally, we propose to use a Racing algorithm to choose between multiple strate- 

gy for unbalanced classification in order to avoid test all possible technique and 

configurations. 





Chapter 5 

Learning from evolve data 

stream with skewed distribution 

Results present in this chapter have be publish in the follow papers: 

• Andrea Dal Pozzolo, Olivier Caelen, Yann-Ael Le Borgne, Serge Waterschoot, and 
Gianluca Bontempi. Learned lesson in credit card fraud detection from a practi- 

tioner perspective. Expert Systems with Applications, 41(10):4915-4928, 2014. 

• Andrea Dal Pozzolo, Reid A. Johnson, Olivier Caelen, Serge Waterschoot, Nitesh 
V Chawla, and Gianluca Bontempi. Using HDDT to avoid instance propagation 

in unbalanced and evolve data streams. In Neural Networks (IJCNN), The 2014 

International Joint Conference on. IEEE, 2014. 

Fraud Detection problem be typically address in two different ways. In the static 

learn setting, a detection model be periodically retrain from scratch (e.g. once a 

year or month). In the online learn setting, the detection model be update a soon a 

new data arrives. Though this strategy be the most adequate to deal with issue of non- 

stationarity, little attention have be devote in the literature to the unbalanced problem 

in a change environment. Another problematic issue in credit card fraud detection be 

the scarcity of available data due to confidentiality issue that give little chance to the 

community to share real datasets and ass exist techniques. 

The first part of the chapter (Section 5.1) be base on [18] and it aim at make an 

experimental comparison of several state-of-the-art algorithm and model technique 

on one real dataset, focus in particular on some open question like: Which machine 

learn algorithm should be used? Is it enough to learn a model once a month or it be 

necessary to update the model everyday? How many transaction be sufficient to train 

89 



Chapter 5. Learning from evolve data stream with skewed distribution 90 

the model? Should the data be analyze in their original unbalanced form? If not, which 

be the best way to rebalance them? Which performance measure be the most adequate to 

ass results? 

We address these question with the aim of assess their importance on real data and 

from a practitioner perspective. These be just some of potential question that could 

rise during the design of a detection system. We do not claim to be able to give a definite 

answer to the problem, but we aim to give some guideline to other people in the field. 

Our goal be to show what work and what do not in a real case study. 

Section 5.1 start by formalize the credit card fraud detection task and present a way 

to create new feature in the datasets that can trace the cardholder spending habits. 

Then, we propose and compare three approach for online learn in order to identify 

what be important to retain or to forget in a change and non-stationary environment. 

We show the impact of the rebalancing technique on the final performance when the 

class distribution be skewed. In do this we merge technique developed for unbalanced 

static datasets with online learn strategies. Our experimental analysis show that to 

adapt to change environments, it be imperative to update the learn algorithm. A 

static approach that never update the learn algorithm often fails to maintain good 

predictive performances. 

The second part of the chapter (Section 5.2) build upon this result and proposes an 

algorithm solution for unbalanced data stream base on Hellinger Distance Decision Tree 

(HDDT) [19], which remove the need of sample technique or instance propagation. 

HDDT [230] have be previously use for static datasets with skewed distribution and it 

have show good performance than standard decision trees. In unbalanced data streams, 

state-of-the-art technique use instance propagation and decision tree (e.g. C4.5 [102]) 

to cope with the unbalanced problem. However, it be not always possible to either revisit 

or store old instance of a data stream. Using HDDT allows u to: i) remove instance 

propagation between batch and ii) use all information available in a batch without 

need to rebalance the class before training a classifier. This have several benefits, for 

example: i) improve predictive accuracy, ii) speed, and iii) single-pass through the 

data. We also use a Hellinger weight ensemble of HDDTs to combat concept drift and 

increase the accuracy of single classifiers. We test our framework on several stream 

datasets with unbalanced class and concept drift. 



Chapter 5. Learning from evolve data stream with skewed distribution 91 

5.1 Learning strategy in credit card fraud detection 

5.1.1 Formalization of the learn problem 

In this section, we formalize the credit card fraud detection task a a statistical learn 

problem. Each transaction be described by a feature vector x contain basic information 

such a amount of the expenditure, the shop where it be performed, the currency, 

etc. However, these variable do not provide any information about the normal card 

usage. The normal behavior of a cardholder can be measure by use a set of historical 

transaction from the same card. For example, a previously explain in Section 2.2.2, 

we can get an idea of the cardholder spending habit by look at the average amount 

spent in different merchant category (e.g. restaurant, online shopping, gas station, etc.) 

in the last 3 month precede the transaction. 

Let xij be the transaction number j of a card number i and dt(xij) be the correspond 

transaction date-time. We assume that the transaction be order in time such that 

if xiv occurs before xiw then dt(xiv) < dt(xiw). Let xiλ be a new incoming transaction 

and ∆t denote the time-frame of a set of historical transaction for the same card. Hiλ 
be then the set of the historical transaction occur in the time-frame ∆t before xiλ 
such that Hiλ = {xij}, where dt(xiλ) > dt(xij) ≥ dt(xiλ) − ∆t. For instance, with 
∆t = 90 days, Hiλ be the set of transaction for the same card occur in the 3 month 

precede dt(xiλ). The card behavior can be summarize use classical aggregation 

method (e.g. mean, max, min or count) on the set Hiλ. This mean that it be possible 

to create new aggregate variable that can be add to the original variable in x to 

include information of the card. In this way we have include information about the user 

behavior at the transaction level. As a consequence, transaction from cardholder with 

similar spending habit will share analogous aggregate variables. Let x̄k indicate the 

feature vector associate to a transaction xij obtain by add aggregate features. 

At day t, a classifier Kt be train on a batch of supervise transactions, denote a 
Bt = {(x̄k, yk), k = 1, . . . , N}, to predict P(+|x̄λ), the probability of a new incoming 
transaction x̄λ to be fraudulent. Note that in this formulation we ignore the status of 

the card. We also assume to know the true class of all transactions, i.e. no mislabeled 

sample due to undetected fraud nor fraud report with a delay by the cardholders. 

5.1.2 Strategies for learn with unbalanced and evolve data stream 

The most conventional way to deal with sequential fraud data be to adopt a Static ap- 

proach (see Figure 5.1 and Algorithm 1), which creates once in a while a classification 

model and us it a a predictor during a long horizon. Though this approach reduces the 



Chapter 5. Learning from evolve data stream with skewed distribution 92 

Time%Sta)c%approach% 

Fraudulent%transac)ons% 
Genuine%transac)ons% 

Bt�2 Bt�1 Bt Bt+1 Bt+2 Bt+3 Bt+4 

Mt 

Figure 5.1: Static approach: a model Mt be train on K = 3 batch and use to 
predict future batches. 

Algorithm 1 Static approach 
Require: K: number of batch use for training, B: total number of batch available (B > K), K: 
classification algorithms, sampling: sample method. 
T ← empty set of transactions. 
for t ∈ {1, . . . ,K} do 

Bt ← tth batch of transactions. 
T← merge Bt with T. 

M← buildModel(T, K, sampling) 
for t ∈ {K + 1, . . . , B} do 

p̂← useM to predict P(+, x̄) for transaction in Bt. 
acct ← predictive accuracy of p̂ . measure by AUC, AP, Pk. 

return Average(acc) 

learn effort, it main problem resides in the lack of adaptivity that make it insensitive 

to any change of distribution in the upcoming batches. 

On the basis of the state-of-the-work described in Section 3.3, it be possible to conceive 

two alternative strategy to address both the incremental and the unbalanced nature of 

the fraud detection problem. The first approach, denote a the Update approach and 

illustrate in Figure 5.2 and Algorithm 2, be inspire by Wang et al. [189]. It us a set 

of M model and a number K of batch to train each model. Note that for M > 1 

and K > 1 the training set of the M model be overlapping. This approach adapts 

to change environment by forget batch at a constant rate. The classifier use to 

make prediction be an ensemble of last M models. 

The second approach denote a the Propagate and Forget approach and illustrate in 

Figure 5.3 and Algorithm 3 be inspire by Gao et all’s work [170]. In order to mitigate 

the unbalanced effects, each time a new batch be available, a model be learn on the 

genuine transaction of the previous Kgen batch and all past fraudulent transactions. 



Chapter 5. Learning from evolve data stream with skewed distribution 93 

Time%Update%approach% 

Bt�2 Bt�1 Bt Bt+1 

Mt�2 Mt�1 MtMt�3 

Et 

Bt�5 Bt�4 Bt�3 

Fraudulent%transac)ons% 
Genuine%transac)ons% 

Figure 5.2: Updating approach for K = 3 and M = 4. For each new batch a model be 
train on the K late batches. Single model be use to predict the follow batch 

or can be combine into an ensemble Et. 

Algorithm 2 Update approach 
Require: K: number of batch use for training, B: total number of batch available (B > K), M : 
number of model in the ensemble, K: classification algorithms, sampling: sample method. 
T ← empty set of transactions. 
Et ← empty array of models. 
k ← 0 
m← 0 
for t ∈ {1, . . . , B} do 

Bt ← tth batch of transactions. 
k ← k + 1. 
T← merge Bt with T. 
if k ≥ K then 

remove Bt−K from T 
k ← k − 1. 

m← m+ 1. 
Mt ← buildModel(T, K, sampling) 
Et ← addMt to Et. 
if m ≥M then 

removeMt−M from Et 
m← m− 1. 

p̂← use Et to predict P(+, x̄) for transaction in Bt+1. 
acct+1 ← predictive accuracy of p̂ . measure by AUC, AP, Pk. 

return Average(acc) 

Since this approach lead to training set which grow in size over the time, a maximum 

training size be set to avoid overloading. Once this size be reached, old observation 

be remove in favor of the more recent ones. An ensemble of model be obtain by 

combine the last M model a in the update approach. 

Note that in all these approach (including the static one), a balance technique (e.g. 

undersampling, SMOTE) can be introduce to reduce the skewness of the training set. 



Chapter 5. Learning from evolve data stream with skewed distribution 94 

Time% 
Propagate%and%Forget%approach% 

Bt�2 Bt�1 Bt Bt+1Bt�3 

Mt�2 Mt�1 MtMt�3 

Et 
Fraudulent%transac)ons% 
Genuine%transac)ons% 

Figure 5.3: Propagate and Forget approach: for each new batch a model be create 
by keep all previous fraudulent transaction and a small set of genuine transaction 
from the last 2 batch (Kgen = 2). Single model be use to predict the follow 

batch or can be combine into an ensemble (M = 4). 

Algorithm 3 Propagate and Forget approach 
Require: Kgen: number of batch from which to retain genuine transactions, B: total number of 
batch available (B > K), M : number of model in the ensemble, N : max size of the training set, 
K: classification algorithms, sampling: sample method. 
T ← empty set of transactions. 
Et ← empty array of models. 
k ← 0 
m← 0 
for t ∈ {1, . . . , B} do 

Bt ← tth batch of transactions. 
k ← k + 1. 
T← merge Bt with T. 
if k ≥ Kgen then 

remove genuine transaction of Bt−Kgen from T 
k ← k − 1. 

if |T| > N then 
remove old fraudulent transaction from T until |T| = N 

m← m+ 1. 
Mt ← buildModel(T, K, sampling) 
Et ← addMt to Et. 
if m ≥M then 

removeMt−M from Et 
m← m− 1. 

p̂← use Et to predict P(+, x̄) for transaction in Bt+1. 
acct+1 ← predictive accuracy of p̂ . measure by AUC, AP, Pk. 

return Average(acc) 

In Table 5.1 we have summarize the strength and weakness of these approaches. The 

Static strategy have the advantage of be fast a the training of the model be do only 

once, but this do not return a model that follow the change in the distribution of the 



Chapter 5. Learning from evolve data stream with skewed distribution 95 

Table 5.1: Strengths and weakness of the different learn approaches. 

Approach Strengths Weaknesses 

Static • Speed • No CD adaptation 
Update • No instance propagation • Needs several batch 

• CD adaptation for the minority class 
Propagate and Forget • Accumulates minority • Propagation lead to 

instance faster large training time 
• CD adaptation 

data. The other two approach on the contrary can adapt to Concept Drift (CD). They 

differ essentially in the way the minority class be accumulate in the training batches. The 

Propagate and Forget strategy propagates instance between batch lead to big 

training set and computational burden. 

5.1.3 Experimental assessment 

In this section we perform an extensive experimental assessment on the basis of real data 

in order to address common issue that the practitioner have to solve when face large 

credit card fraud datasets. 

Dataset and feature transformation 

The credit card fraud dataset contains a subset of the transaction from the first of 

February 2012 to the twentieth of May 2013 (details in Table 5.2). The dataset be 

divide in daily batch and contain e-commerce fraudulent transactions. 

Table 5.2: Fraudulent dataset 

# Days # Features # Transactions Period 
422 45 2’202’228 1Feb12 - 20May13 

This dataset be strongly unbalanced (the percentage of fraudulent transaction be low 

than 0.4%) and have a total of 422 batch contain daily transaction with an average 

of 5218 transaction per batch. 

The original variable available in the dataset include the transaction amount, point 

of sale, currency, country of the transaction, merchant type and many others. However, 

these variable do not explain cardholder behavior, so aggregate feature be add to 

the original one in order to profile the user behavior (see Section 5.1.1). For example, 

the transaction amount and the CARD_ID be use to compute the average expenditure 



Chapter 5. Learning from evolve data stream with skewed distribution 96 

per week and per month of one card, the difference between the current and previous 

transaction and many others. For each transaction and card we take 3 month (∆t = 

90 days) to compute the aggregate variables. 

A great number of the variable in the original dataset be discrete feature take sev- 

eral values. However, some classifier such a NNET be not able to handle discrete 

variable, so these feature have to be transform into numerical features. In the R envi- 

ronment [25], discrete variable be also know a factor and their value a levels. One 

straightforward way to transform factor into numerical feature be to compute the risk 

of have a fraudulent transaction for each level of a factor. Let’s consider for example 

a factor variable f . We can compute the probability of the jth level of factor f to be 

fraudulent, denote a βj , a follows: βj = 
N+f=j 
Nf=j 

, where N+f=j be the number of fraudulent 

transaction and Nf=j the total number of transaction for factor f with level j. 

As first option we could use βj to associate to each level a fraud score, but this would 

lead to the follow issues: i) with few fraudulent transactions, many level be go 

to have βj = 0 make the distribution of the new feature skewed towards zero, and ii) 

βj could be the same for level with big and small frequency. For example if we have a 

level with 1 fraudulent transaction out of 10 we have βj = 0.1 a for a level with 100 

fraudulent transaction out of 1000. In the first example however we compute βj use 

only 10 observation whereas in the second we can use 1000 observations, therefore in 

the second scenario we be more confident about the probability calculation. 

In our work we use a score function which combine some a-priori fraud probability 

with βj . The idea be that when there’s not much information (levels with few transac- 

tions) we should use what we call the Average Fraud Probability (AFP) that be the 

mean probability of have a fraud in a day. On the other hand, when we have enough 

information we want to consider the fraud probability come from the factor level (de- 

fin by βj). We use a weight average of AFP and βj where the weight αj give 

to βj be define by the proportion of transaction for that level to all the transactions: 

αj = 
Nf=j 
N . As a result a factor can be transform into a numerical feature by associ- 

ating to each level a fraud score Sj a follow: 

Sj = αjβj + (1− αj)AFP (5.1) 

In this score function however, even with the maximum value of αj (which be always 

less than 1) we still have the influence of the a-priori part give by (1 − αj)AFP. To 
avoid this problem, we can transform the previous function to bind the weight to be in 

the range [0, 1] use a new weight α′j = 
αj−minαj 

maxαj−minαj . We can now rewrite (5.1) as: 

Sj = α 
′ 
jβj + (1− α′j)AFP (5.2) 



Chapter 5. Learning from evolve data stream with skewed distribution 97 

A new problem with this new type of score function emerges when there be a factor 

that do not have a uniform distribution of the transaction over it levels. For example 

the factor variable TERM_MCC (terminal Merchant Category Code) have few frequent 

level and many rare levels. With this factor we would end up with few level have a 

big weight αj and many level with small αj (Figure 5.4(a)). In this kind of situation 

we can apply the log function to have a smoother α′j (Figure 5.4(b)). 


! 

(a) α′j = 
αj−minαj 

maxαj−minαj 

! 

(b) α′j = 
log (αj−minαj) 

log (maxαj−minαi) 

Figure 5.4: Weight α′j for variable TERM_MCC. 

Solved the factor problem, the dataset now contains only numerical variables, where the 

original discrete variable have be transform into numerical feature range from 0 

(low risk) to 1 (high risk). 

Results 

Our experimental analysis allows one to compare several strategy for credit card fraud 

detection. Based on the approach present in Section 5.1.2, our experimental finding 

aim to give guideline to practitioner work on credit card fraud detection. To evaluate 

our results, we measure the performance of the detection in term of AUC, AP and Pk 
(see Sections 2.2.3). 

Influence of algorithm and training set size in a static approach 

The static approach (described in Section 5.1.2) be one of the most commonly use by 

practitioner because of it simplicity and rapidity. However, open question remain 

about which learn algorithm should be use and the consequent sensitivity of the 

accuracy to the training size. We test three different supervise algorithms: RF, 



Chapter 5. Learning from evolve data stream with skewed distribution 98 

NNET and SVM provide by the R software [25]. We use R version 3.0.1 with package 

randomForest [222], e1071 [228], unbalanced [24] and MASS [231]1. 

In order to ass the impact of the training set size (in term of days/batches) we carry 

out the prediction with different window (K = 30, 60 and 90). All training set be 

rebalanced use undersampling and all experiment be replicate five time to reduce 

the variance cause by the sample implicit in unbalanced techniques. Figure 5.5 show 

the sum of the rank from the Friedman test [232] for each strategy in term of AP, AUC 

and Pk. For each batch, we rank the strategy from the least to the best performing. 

Then we sum the rank over all batches. More formally, let rs,b ∈ {1, ..., S} be the rank 
of strategy s on batch b and S be the number of strategy to compare. The strategy with 

the high accuracy in b have rs,b = S and the one with the low have rs,b = 1. Then 

the sum of rank for the strategy s be define a 
∑B 

b=1 rs,b, where B be the total number 

of batches. The high the sum, the high be the number of time that one strategy be 

superior to the others. The white bar denote model which be significantly bad than 

the best (paired t-test base on the rank of each batch). 

The strategy name follow a structure built on the follow options: 

• Algorithm use (RF, SVM, NNET) 

• Sampling method (Under, SMOTE, EasyEnsemble) 

• Model update frequency (One, Daily, 15days, Weekly) 

• Number of model in the ensemble (M) 

• Learning approach (Static, Update, Propagate and Forget) 

• Learning parameter (K, Kgen) 

Then the strategy option be concatenate use the dot a separation point (e.g. 

RF.Under.Daily.10M.Update.60K). In both datasets, Random Forests clearly outper- 

form it competitor and, a expected, accuracy be improve by increase the training 

size (Figure 5.5). Because of the significative superiority of Random Forests with re- 

spect to the other algorithms, in what follow we will limit to consider only this learn 

algorithm. 

Advantage of update model 

Here we ass the advantage of adopt the update approach described in Section 5.1.2. 

Figure 5.6 report the result for different value of K and M . The strategy be call 
1For each classifier we use the default setting provide by the package. 



Chapter 5. Learning from evolve data stream with skewed distribution 99 

NNET.Under.One.1M.Static.60K 

SVM.Under.One.1M.Static.60K 

NNET.Under.One.1M.Static.90K 

SVM.Under.One.1M.Static.90K 

RF.Under.One.1M.Static.30K 

RF.Under.One.1M.Static.60K 

RF.Under.One.1M.Static.90K 

0 500 1000 1500 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: AP 

(a) Metric: AP 

SVM.Under.One.1M.Static.60K 

SVM.Under.One.1M.Static.90K 

NNET.Under.One.1M.Static.60K 

NNET.Under.One.1M.Static.90K 

RF.Under.One.1M.Static.30K 

RF.Under.One.1M.Static.60K 

RF.Under.One.1M.Static.90K 

0 500 1000 1500 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: AUC 

(b) Metric: AUC 

NNET.Under.One.1M.Static.60K 

SVM.Under.One.1M.Static.60K 

NNET.Under.One.1M.Static.90K 

SVM.Under.One.1M.Static.90K 

RF.Under.One.1M.Static.30K 

RF.Under.One.1M.Static.60K 

RF.Under.One.1M.Static.90K 

0 500 1000 1500 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: PrecisionRank 

(c) Metric: Pk 

Figure 5.5: Comparison of static strategy use sum of rank in all batches. 



Chapter 5. Learning from evolve data stream with skewed distribution 100 

daily if a model be built every day, weekly if once a week or 15days if every 15 days. 

We compare ensemble strategy (M > 1) with model built on single batch (K = 1) 

against single model strategy (M = 1) use several batch in the training (K > 1). 

For all metrics, the best strategy be RF.Under.Daily.5M.Update.90K. It creates a new 

model at each batch use previous 90 day (K = 90) for training and keep the last 5 

model create (M = 5) for predictions. In the case of AP however this strategy be not 

statistically good than the ensemble approach ranked a second. 

For all metrics, the strategy that use only the current batch to build a model (K = 1) 

be coherently the worst. This confirms the result of previous analysis show that a 

too short window of data (and consequently a very small fraction of frauds) be insufficient 

to learn a reliable model. When compare the update frequency of the model use 

the same number of batch for training (K = 90), daily update be rank always good 

than weekly and 15days. This confirms the intuition that the fraud distribution be always 

evolve and therefore it be good to update the model a soon a possible. 

Benefit of propagate old fraud 

This section ass the accuracy of the Propagate and Forget approach described in 

Section 5.1.2 whose rationale be to avoid discard old fraudulent observations. 

Accumulating old fraud lead to less unbalanced batches. In order to avoid have 

batch where the accumulate fraud outnumber the genuine transactions, two option 

be available: i) forget some of the old fraud ii) accumulate old genuine trans- 

action a well. In the first case when the accumulate fraud represent 40% of the 

transactions, new fraud replace old fraud a in Gao [171]. In the second case we accu- 

mulate genuine transaction from previous Kgen batches, where Kgen defines the number 

of batch use (see Figure 5.3). 

Figure 5.7 show the sum of rank for different strategy where the genuine transaction 

be take from a different number of day (Kgen). The best strategy accord to AP 

and Pk us an ensemble of 5 model for each batch (M = 5) and 30 day for genuine 

transaction (Kgen = 30). The same strategy rank third in term of AUC and be 

significantly bad than the best. To create ensemble we use a time-based array of 

model of fix sizeM , which mean that when the number of model available be great 

than M , the most recent in time model replaces the M th model in the array remove 

the old model in the ensemble. 

In general we see good performance when Kgen increase from 0 to 30 and only in few 

case Kgen > 30 lead to significantly good accuracy. Note that in all our strategy 



Chapter 5. Learning from evolve data stream with skewed distribution 101 

RF.Under.Daily.90M.Update.1K 
RF.Under.Daily.60M.Update.1K 
RF.Under.Daily.15M.Update.1K 
RF.Under.Daily.30M.Update.1K 
RF.Under.Daily.1M.Update.30K 

RF.Under.15days.1M.Update.90K 
RF.Under.Weekly.1M.Update.90K 

RF.Under.Daily.1M.Update.60K 
RF.Under.Daily.1M.Update.90K 

RF.Under.Daily.30M.Update.90K 
RF.Under.Daily.15M.Update.90K 
RF.Under.Daily.5M.Update.90K 

0 1000 2000 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: AP 

(a) Metric: AP 

RF.Under.Daily.90M.Update.1K 
RF.Under.Daily.60M.Update.1K 
RF.Under.Daily.15M.Update.1K 
RF.Under.Daily.30M.Update.1K 
RF.Under.Daily.1M.Update.30K 

RF.Under.15days.1M.Update.90K 
RF.Under.Daily.1M.Update.60K 

RF.Under.Weekly.1M.Update.90K 
RF.Under.Daily.1M.Update.90K 

RF.Under.Daily.30M.Update.90K 
RF.Under.Daily.15M.Update.90K 
RF.Under.Daily.5M.Update.90K 

0 1000 2000 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: AUC 

(b) Metric: AUC 

RF.Under.Daily.90M.Update.1K 
RF.Under.Daily.60M.Update.1K 
RF.Under.Daily.15M.Update.1K 
RF.Under.Daily.30M.Update.1K 
RF.Under.Daily.1M.Update.30K 

RF.Under.15days.1M.Update.90K 
RF.Under.Daily.1M.Update.60K 

RF.Under.Weekly.1M.Update.90K 
RF.Under.Daily.1M.Update.90K 

RF.Under.Daily.30M.Update.90K 
RF.Under.Daily.15M.Update.90K 
RF.Under.Daily.5M.Update.90K 

0 500 1000 1500 2000 2500 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: PrecisionRank 

(c) Metric: Pk 

Figure 5.6: Comparison of update strategy use sum of rank in all batches. 



Chapter 5. Learning from evolve data stream with skewed distribution 102 

after select the observation to include in the training set we use undersampling to 

make sure we have the two class equally represented. 

Impact of balance technique on detection accuracy 

So far we consider exclusively undersampling a balance technique in our experi- 

ments. In this section we ass the impact of use alternative method like SMOTE 

and EasyEnsemble. Experimental result (Figure 5.8) show that they both over-perform 

undersampling. 

In our datasets, the number of fraud be on average 0.4% of all transaction in the batch. 

Undersampling randomly selects a number of genuine transaction equal to the num- 

ber of frauds, which mean remove about 99.6% of the genuine transaction in the 

batch. EasyEnsemble be able to reduce the variance of undersampling by use several 

sub-models for each batch, while SMOTE creates new artificial fraudulent transactions. 

In our experiment we use 5 sub-models in EasyEnsemble. For all balance tech- 

niques, between the three approach present in Section 5.1.2, the static approach be 

consistently the worse. 

In Figure 5.9 we compare the previous strategy in term of average prediction time over 

all batches. SMOTE be computationally heavy since it consists in oversampling, lead 

to big batch sizes. EasyEnsemble replicates undersampling and learns from several 

sub-batches. This give high computational time than undersampling. Between the 

different incremental approaches, static have the low time a the model be learn once 

and not retrained. Propagate and Forget strategy have the high prediction time over 

all balance methods. This be expect since it retains old transaction to deal with 

unbalanced batches. 

Analysis of the best overall strategy 

The large number of possible alternative (in term of learn classifier, balance tech- 

nique and incremental learn strategy) require a joint assessment of several combina- 

tions in order to come up with a recommend approach. Figure 5.10 summary the 

best strategy in term of different metrics. The combination of EasyEnsemble with 

Propagate and Forget emerge a best for all metrics. SMOTE with update be not signifi- 

cantly bad of the best for AP and Pk, but it be not rank well in term of AUC. The 

fact that within the best strategy we see different balance technique confirms that 

in unbalanced data streams, the adopt balance strategy may play a major role. As 

expect the static approach rank low in Figures 5.10 a it be not able to adapt to the 



Chapter 5. Learning from evolve data stream with skewed distribution 103 

RF.Under.Daily.1M.Forget.0Kgen 

RF.Under.Daily.30M.Forget.30Kgen 

RF.Under.Daily.1M.Forget.90Kgen 

RF.Under.Daily.1M.Forget.60Kgen 

RF.Under.Daily.1M.Forget.30Kgen 

RF.Under.Daily.1M.Forget.15Kgen 

RF.Under.Daily.15M.Forget.30Kgen 

RF.Under.Daily.10M.Forget.30Kgen 

RF.Under.Daily.5M.Forget.30Kgen 

0 1000 2000 3000 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: AP 

(a) Metric: AP 

RF.Under.Daily.1M.Forget.0Kgen 

RF.Under.Daily.1M.Forget.15Kgen 

RF.Under.Daily.30M.Forget.30Kgen 

RF.Under.Daily.1M.Forget.30Kgen 

RF.Under.Daily.15M.Forget.30Kgen 

RF.Under.Daily.10M.Forget.30Kgen 

RF.Under.Daily.5M.Forget.30Kgen 

RF.Under.Daily.1M.Forget.60Kgen 

RF.Under.Daily.1M.Forget.90Kgen 

0 1000 2000 3000 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: AUC 

(b) Metric: AUC 

RF.Under.Daily.1M.Forget.0Kgen 

RF.Under.Daily.30M.Forget.30Kgen 

RF.Under.Daily.1M.Forget.15Kgen 

RF.Under.Daily.15M.Forget.30Kgen 

RF.Under.Daily.1M.Forget.30Kgen 

RF.Under.Daily.10M.Forget.30Kgen 

RF.Under.Daily.1M.Forget.90Kgen 

RF.Under.Daily.1M.Forget.60Kgen 

RF.Under.Daily.5M.Forget.30Kgen 

0 1000 2000 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: PrecisionRank 

(c) Metric: Pk 

Figure 5.7: Comparison of forget strategy use sum of rank in all batches. 



Chapter 5. Learning from evolve data stream with skewed distribution 104 

RF.Under.One.1M.Static.90K 

RF.SMOTE.One.1M.Static.90K 

RF.EasyEnsemble.One.1M.Static.90K 

RF.Under.Daily.1M.Update.90K 

RF.Under.Daily.1M.Forget.90Kgen 

RF.EasyEnsemble.Daily.1M.Update.90K 

RF.SMOTE.Daily.1M.Forget.90Kgen 

RF.EasyEnsemble.Daily.1M.Forget.90Kgen 

RF.SMOTE.Daily.1M.Update.90K 

0 500 1000 1500 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: AP 

(a) Metric: AP 

RF.SMOTE.One.1M.Static.90K 

RF.Under.One.1M.Static.90K 

RF.EasyEnsemble.One.1M.Static.90K 

RF.Under.Daily.1M.Update.90K 

RF.SMOTE.Daily.1M.Forget.90Kgen 

RF.Under.Daily.1M.Forget.90Kgen 

RF.SMOTE.Daily.1M.Update.90K 

RF.EasyEnsemble.Daily.1M.Update.90K 

RF.EasyEnsemble.Daily.1M.Forget.90Kgen 

0 500 1000 1500 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: AUC 

(b) Metric: AUC 

RF.Under.One.1M.Static.90K 

RF.SMOTE.One.1M.Static.90K 

RF.EasyEnsemble.One.1M.Static.90K 

RF.Under.Daily.1M.Update.90K 

RF.Under.Daily.1M.Forget.90Kgen 

RF.SMOTE.Daily.1M.Forget.90Kgen 

RF.EasyEnsemble.Daily.1M.Update.90K 

RF.EasyEnsemble.Daily.1M.Forget.90Kgen 

RF.SMOTE.Daily.1M.Update.90K 

0 500 1000 1500 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: PrecisionRank 

(c) Metric: Pk 

Figure 5.8: Comparison of different balance technique and strategy use sum of 
rank in all batches. 



Chapter 5. Learning from evolve data stream with skewed distribution 105 

EasyEnsemble SMOTE Under 

0 

200 

400 

Da 
ily 

.1 
M 

.F 
or 

ge 
t.9 

0K 
ge 

n 
Da 

ily 
.1 

M 
.U 

pd 
at 

e.9 
0K 

On 
e.1 

M 
.S 

ta 
tic 

.9 
0K 

Da 
ily 

.1 
M 

.F 
or 

ge 
t.9 

0K 
ge 

n 
Da 

ily 
.1 

M 
.U 

pd 
at 

e.9 
0K 

On 
e.1 

M 
.S 

ta 
tic 

.9 
0K 

Da 
ily 

.1 
M 

.F 
or 

ge 
t.9 

0K 
ge 

n 
Da 

ily 
.1 

M 
.U 

pd 
at 

e.9 
0K 

On 
e.1 

M 
.S 

ta 
tic 

.9 
0K 

Strategy 

C 
om 

pu 
ta 

tio 
na 

l t 
im 

e 

algo 
RF 

Figure 5.9: Comparison of different balance technique and strategy in term 
of average prediction time (in seconds) over all batches. Experiments run on a HP 
ProLiant BL465c G5 blade with 2x AMD Opteron 2.4 GHz, 4 core each and 32 GB 

DDR3 RAM. 

change distribution. The Propagate and Forget approach be significantly good than 

Update for EasyEnsemble, while SMOTE give good rank with update. 

It be worth to note that strategy which combine more than one model (M > 1) to- 

gether with undersampling be not superior to the prediction with a single model and 

EasyEnsemble. EasyEnsemble learns from different sample of the majority class, which 

mean that for each batch different concept of the majority class be learned. 

5.1.4 Discussion 

The need to detect fraudulent pattern in huge amount of data demand the adoption of 

automatic methods. The scarcity of public available dataset in credit card transaction 

give little chance to the community to test and ass the impact of exist technique 

on real data. The goal of this first part of the chapter be to give some guideline to 

practitioner on how to tackle the detection problem. 

Credit card fraud detection relies on the analysis of record transactions. However, sin- 

gle transaction information be not consider sufficient to detect a fraud occurrence [11] 

and the analysis have to take into consideration the cardholder behavior. To this purpose 

we include cardholder information into the transaction by compute aggregate vari- 

ables on historical transaction of the same card. As new credit-card transaction keep 

arriving, the detection system have to process them a soon a they arrive and avoid re- 

taining in memory too many old transactions. We compare three alternative approach 



Chapter 5. Learning from evolve data stream with skewed distribution 106 

NNET.Under.One.1M.Static.60K 
SVM.Under.One.1M.Static.60K 

NNET.Under.One.1M.Static.90K 
NNET.Under.One.1M.Static.120K 

SVM.Under.One.1M.Static.90K 
SVM.Under.One.1M.Static.120K 

RF.Under.One.1M.Static.30K 
RF.Under.One.1M.Static.60K 

RF.SMOTE.One.1M.Static.90K 
RF.Under.One.1M.Static.90K 

RF.EasyEnsemble.One.1M.Static.90K 
RF.Under.One.1M.Static.120K 

RF.Under.15days.1M.Update.90K 
RF.Under.Weekly.1M.Update.90K 

RF.Under.Daily.1M.Update.90K 
RF.Under.Daily.1M.Forget.90Kgen 

RF.Under.Daily.30M.Forget.30Kgen 
RF.Under.Daily.30M.Update.90K 
RF.Under.Daily.15M.Update.90K 

RF.Under.Daily.5M.Update.90K 
RF.Under.Daily.15M.Forget.30Kgen 
RF.Under.Daily.10M.Forget.30Kgen 

RF.EasyEnsemble.Daily.1M.Update.90K 
RF.SMOTE.Daily.1M.Forget.90Kgen 

RF.Under.Daily.5M.Forget.30Kgen 
RF.SMOTE.Daily.1M.Update.90K 

RF.EasyEnsemble.Daily.1M.Forget.90Kgen 

0 1000 2000 3000 4000 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: AP 

(a) Metric: AP 

SVM.Under.One.1M.Static.60K 
SVM.Under.One.1M.Static.120K 
SVM.Under.One.1M.Static.90K 

NNET.Under.One.1M.Static.60K 
NNET.Under.One.1M.Static.90K 

NNET.Under.One.1M.Static.120K 
RF.Under.One.1M.Static.30K 
RF.Under.One.1M.Static.60K 

RF.SMOTE.One.1M.Static.90K 
RF.Under.One.1M.Static.90K 

RF.EasyEnsemble.One.1M.Static.90K 
RF.Under.Daily.30M.Forget.30Kgen 

RF.Under.One.1M.Static.120K 
RF.Under.Daily.15M.Forget.30Kgen 

RF.Under.15days.1M.Update.90K 
RF.Under.Daily.10M.Forget.30Kgen 
RF.Under.Daily.5M.Forget.30Kgen 
RF.Under.Weekly.1M.Update.90K 

RF.SMOTE.Daily.1M.Forget.90Kgen 
RF.Under.Daily.1M.Update.90K 

RF.Under.Daily.30M.Update.90K 
RF.Under.Daily.1M.Forget.90Kgen 
RF.SMOTE.Daily.1M.Update.90K 
RF.Under.Daily.15M.Update.90K 

RF.Under.Daily.5M.Update.90K 
RF.EasyEnsemble.Daily.1M.Update.90K 

RF.EasyEnsemble.Daily.1M.Forget.90Kgen 

0 1000 2000 3000 4000 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: AUC 

(b) Metric: AUC 

NNET.Under.One.1M.Static.60K 
SVM.Under.One.1M.Static.60K 

NNET.Under.One.1M.Static.90K 
SVM.Under.One.1M.Static.90K 

NNET.Under.One.1M.Static.120K 
SVM.Under.One.1M.Static.120K 

RF.Under.One.1M.Static.30K 
RF.Under.One.1M.Static.60K 

RF.SMOTE.One.1M.Static.90K 
RF.Under.One.1M.Static.90K 

RF.EasyEnsemble.One.1M.Static.90K 
RF.Under.One.1M.Static.120K 

RF.Under.15days.1M.Update.90K 
RF.Under.Daily.30M.Forget.30Kgen 

RF.Under.Weekly.1M.Update.90K 
RF.Under.Daily.15M.Forget.30Kgen 
RF.Under.Daily.10M.Forget.30Kgen 

RF.Under.Daily.1M.Update.90K 
RF.Under.Daily.30M.Update.90K 

RF.SMOTE.Daily.1M.Forget.90Kgen 
RF.Under.Daily.5M.Forget.30Kgen 
RF.Under.Daily.1M.Forget.90Kgen 

RF.Under.Daily.15M.Update.90K 
RF.Under.Daily.5M.Update.90K 

RF.EasyEnsemble.Daily.1M.Update.90K 
RF.SMOTE.Daily.1M.Update.90K 

RF.EasyEnsemble.Daily.1M.Forget.90Kgen 

0 1000 2000 3000 4000 

Sum of the rank 

Best significant 
FALSE 
TRUE 

Metric: PrecisionRank 

(c) Metric: Pk 

Figure 5.10: Comparison of all strategy use sum of rank in all batches. 



Chapter 5. Learning from evolve data stream with skewed distribution 107 

(Static, Update and Propagate and Forget) to study which approach be good to adopt 

in unbalanced and non-stationary credit card data streams. 

In the static learn set a wide range of technique have be propose to deal 

with unbalanced datasets. However, in online/incremental learn few attempt have 

try to deal with unbalanced data stream [52, 148, 170]. In these works, the most 

common balance technique consists in undersampling the majority class in order to 

reduce class imbalance. In our work we adopt two alternative to undersampling: 

SMOTE and EasyEnsemble. In particular we show that they be both able to return 

high accuracies. Our framework can be easily extend to include other data-level 

balance techniques. 

The experimental part have show that in data streams, with standard classification 

algorithms, when the distribution be skewed towards one class it be important maintain 

previous minority example in order to learn a good separation of two classes. Instance 

propagation from previous batch have the effect of increase the minority class in the 

current batch, but it be of limited impact give the small number of frauds. We test 

several ensemble and single model strategy use different number of batch for 

training. In general we see that model train on multiple batch have good accuracy 

than single batch models. Multi-batches model learn on overlap training sets, when 

this happens single model strategy can beat ensembles. 

Our framework address the problem of non-stationarity in data stream by create a 

new model every time a new batch be available. This approach have show good result 

than update the model at a low frequency (weekly or every 15days). Updating the 

model be crucial in a non-stationary environment, this intuition be confirm by the bad 

result of the static approach. In our dataset, overall we saw Random Forest beating 

Neural Network and Support Vector Machine. The final best strategy implement the 

Propagate and Forget approach together with EasyEnsemble and daily update. 

5.2 Using HDDT to avoid instance propagation 

The result from the previous section have show that, in fraud detection, it be important 

to update the model a soon a new data arrives in order to adapt to possible change 

in the data distributions. In this second part of the chapter we aim to improve the 

update approach present in Section 5.1.2 by propose Hellinger Distance Decision 

Tree (HDDT) [103] a a base learner for data streams. 

Several state-of-the-art technique for unbalanced data stream have address the imbal- 

ance problem by propagate minority observation between batches, with C4.5 decision 



Chapter 5. Learning from evolve data stream with skewed distribution 108 

tree be the most common algorithm use [148, 171, 175, 176]. These technique re- 

tain previous minority class instance in order to provide the algorithm balance batches, 

since the classifier be not able to learn from skewed distributions. However, when data 

arrive a a continuous stream of transactions, it can become impossible to store or prop- 

agate previous observations. Therefore, there be the need of tool that be able to process 

stream of data a soon a they arrive, without revise old observations. 

This section be base on our previous work [19], where we use HDDT [103] a a base 

classifier for data stream in order to avoid instance propagation between subsequent 

batch of data. We show that HDDT be able to produce superior performance than 

standard C4.5 decision tree in term of predictive accuracy, computational time and 

resource needed. 

In order to combat concept drift we have use a batch-ensemble model combination 

base on Hellinger Distance and Information Gain a in [175]. This choice have prove to 

be beneficial in the presence of change distribution in the data. We have test our 

framework with different type of datasets: unbalanced datasets without know concept 

drift, artificial datasets with know concept drift and a highly unbalanced credit card 

fraud dataset with concept drift. 

5.2.1 Hellinger Distance Decision Trees 

Originally introduce to quantify the similarity between two probability distribution [233], 

the Hellinger Distance (HD) have be recently propose a a splitting criterion in decision 

tree to improve the accuracy in unbalanced problem [103, 230]. In the context of data 

streams, it have produce excellent result in detect classifier performance degradation 

due to concept drift [175, 234]. Let P1 and P2 be two discrete probability distribution 

take value φ ∈ Φ, Hellinger Distance be define as: 

HD(P1, P2) = 

√∑ 
φ∈Φ 

(√ 
P1(φ)− 

√ 
P2(φ) 

)2 
(5.3) 

Hellinger Distance have several properties: 

• HD(P1, P2) = HD(P2, P1) (symmetric) 

• HD(P1, P2) >= 0 (non-negative) 

• HD(P1, P2) ∈ [0, 
√ 

2] 

Starting from (5.3), Cieslak and Chawla [103] derive a new decision tree splitting criterion 

base on Hellinger Distance that be skew insensitive. They start from the assumption 



Chapter 5. Learning from evolve data stream with skewed distribution 109 

that all numerical feature be partition into q bins, so that the result dataset be 

make of only categorical variables. Then for each feature f , they compute the distance 

between the class over all the feature’s partitions. In the case of a binary classification 

problem where f+ denotes the instance of the positive class and f− the negatives, the 

Hellinger distance between f+ and f− is: 

HD(f+, f−) = 

√√√√√ q∑ 
j=1 

√ |f+j | 
|f+| − 

√ 
|f−j | 
|f−| 

2 (5.4) 
where j defines the jth bin of feature f and |f+j | be the number of positive instance of 
feature f in the jth bin. At each node of the tree, HD(f+, f−) be compute for each 

feature and then the feature with the maximum distance be use to split. The author 

of [55, 103, 235] recommend to leave the tree unpruned and to use Laplace smooth 

for obtain posterior probabilities.2 Note that the class prior do not appear explicitly 

in equation (5.4), which mean that class imbalance ratio do not influence the distance 

calculation. 

5.2.2 Hellinger Distance a weight ensemble strategy 

In evolve data stream it be important to understand how similar two consecutive data 

batch be in order to decide whether a model learn on a previous batch be still valid. 

Lichtenwalter and Chawla [175] propose to employ HD a a measure of the distance 

between two separate batches. Let u define a Bt the batch at time t use for training 

and Bt+1 a the subsequent test batch. First numeric feature be discretized into 

equal-width bins, then Hellinger distance between Bt and Bt+1 for a give feature f be 

calculate as: 

HD(Bft ,B 
f 
t+1) = 

√√√√√∑ 
v∈f 

√ |Bf=vt | 
|Bt| 

− 

√ 
|Bf=vt+1 | 
|Bt+1| 

2 (5.5) 
where |Bf=vt | be the number of instance of feature f take value v in the batch at time 
t, while |Bt| be the total number of instance in the same batch. 

Equation (5.5) do not account for difference in feature relevance. In general, feature 

distance should have a high weight when the feature be relevant, while a small weight 

should be assign to a weak feature. Making the assumption that feature relevance 

remains stable over time, Lichtenwalter and Chawla [175] suggest use the information 

gain to weight the distances. For a give feature f of a batch B, the Information Gain 
2When the class distribution be unbalanced, prune would remove those leaf with few sample 

which be also the one more likely to be associate with the minority class. 



Chapter 5. Learning from evolve data stream with skewed distribution 110 

be define a the decrease in entropy H of a class c: 

IG(B, f) = H(Bc)−H(Bc|Bf ) (5.6) 

where Bc defines the class of the observation in batch B and Bf the observation of 

feature f . For the test batch we cannot compute IG(B, f), a the label be not 

provided, therefore the feature relevance be calculate on the training batch. We can now 

define a new distance function that combine Information Gain and Hellinger Distance 

as: 

HDIG(Bt,Bt+1, f) = HD(B 
f 
t ,B 

f 
t+1) ∗ (1 + IG(Bt, f)) (5.7) 

HDIG(Bt,Bt+1, f) provide a relevance-weighted distance for each single feature. The 

final distance between two batch be then compute by take the average over all the 

features. 

AHDIG(Bt,Bt+1) = 

∑ 
f∈Bt HDIG(Bt,Bt+1, f) 

|f ∈ Bt| 
(5.8) 

We learn a new model a soon a a new batch be available and combine learn model into 

an ensemble where the weight of the model be inversely proportional to the batches’ 

distances. The low the distance between two batch the more similar be the concept 

between them. In a stream environment with concept drift we should expect good 

performance on the current batch from model learn on similar concepts. With this 

reason in mind, the ensemble weight should be high for small distances. We use 

the same transformation of [175], where the weight wt of a classifier train on Bt and 

that predicts on Bt+1 be define a follows: 

wt = AHDIG(Bt,Bt+1)−M (5.9) 

where M represent the ensemble size. 

5.2.3 Experimental assessment 

Many of the data stream framework for concept drift and unbalanced data use 

C4.5 [102] decision tree a the base learner [148, 171, 175, 236]. In our experiment 

we compare the result of C4.5 to the Hellinger Distance Decision Tree (HDDT) with 

the parameter suggest in [103] (unpruned and with Laplace smoothing). The com- 

parison be do use different propagation/sampling method and model combination 

(ensemble vs. single models). 

In an unbalanced data stream, for each batch/chunk, the positive class example repre- 

sent the minority of the observations. Each batch can be consider a a small unbalanced 

dataset, permit all the technique already developed for static unbalanced datasets 



Chapter 5. Learning from evolve data stream with skewed distribution 111 

to be implemented. In a stream environment, however, it be possible to collect minor- 

ity observation from previous batch to combat class imbalance. For our experiment 

we consider instance propagation method that assume no sub-concepts within the 

minority class. In particular we use Gao’s [171] and Lichtenwalter’s [175] propagation 

method present in Section 3.3 and two other benchmark method (UNDER and BL): 

• SE (Gao’s [171] propagation of rare class instance and undersampling at 40%) 

• BD (Lichtenwalter’s Boundary Definition [175]: propagate rare-class instance 
and instance in the negative class that the current model misclassifies.) 

• UNDER (Undersampling: no propagation between batches, undersampling at 40%) 

• BL (Baseline: no propagation, no sampling) 

The first two method can be consider a oversampling method since the minority 

proportion in the batch be augmented. From now on, for simplicity we will call all the 

previously discuss instance propagation method sample strategies. 

For each of the previous sample strategy we tested: i) HDIG: ensemble with weight 

from (5.9) and ii) No ensemble: single classifier. In the first case, an ensemble be built 

combine all model learn with weight give by (5.9). In the second case, we use the 

model learn in the current batch to predict the incoming batch. This option have the 

advantage of be faster a no model be store during the learn phase. 

In all our experiment we report the result in term of AUC. The framework be 

write in Java and we use the Weka [237] implementation of C4.5 and HDDT. We 

use UCI datasets [1] to first study the unbalanced problem without worry about 

concept drift. These datasets be not inherently sequential and exhibit no concept drift; 

we render them a data stream by randomize the order of instance and processing 

them in batch a in [175]. Then we use the MOA [238] framework to generate some 

artificial datasets with drift feature to test the behavior of the algorithm under 

concept drift. Finally we use a real-world credit card dataset which be highly unbalanced 

and whose fraud be change in type and distribution. This dataset contains credit 

card transaction from online payment between the 5th of September 2013 and the 25th 

of September 2013, where only 0.15% of the transaction be fraudulent. 

We first test the different sample strategy use HDDT and C4.5. On the left 

side of Figure 5.11 we see the result where HDIG distance (discussed in Section 5.2.2) 

be use to weight the model from different batch accord to (5.9). On the right 

be the result where only the model of the current batch be use for prediction. The 

column indicate the batch mean AUC for each strategy average over all UCI datasets. 



Chapter 5. Learning from evolve data stream with skewed distribution 112 

Table 5.3: Datasets 

Dataset Source # Instances # Features Imbalance Ratio 
Adult UCI 48,842 14 3.2:1 
can UCI 443,872 9 52.1:1 

compustat UCI 13,657 20 27.5:1 
covtype UCI 38,500 10 13.0:1 
football UCI 4,288 13 1.7:1 
ozone-8h UCI 2,534 72 14.8:1 
wrds UCI 99,200 41 1.0:1 
text UCI 11,162 11465 14.7:1 

DriftedLED MOA 1,000,000 25 9.0:1 
DriftedRBF MOA 1,000,000 11 1.02:1 
DriftedWave MOA 1,000,000 41 2.02:1 
Creditcard FRAUD 3,143,423 36 658.8:1 

This mean that for each dataset we compute the mean AUC over all batch and then 

average the result between all datasets. In general we notice that HDDT be able to 

HDIG No Ensemble 

0.00 

0.25 

0.50 

0.75 

BD BL SE UNDER BD BL SE UNDER 

Sampling 

AU 
RO 

C Algorithm 
C4.5 
HDDT 

Figure 5.11: Batch average result in term of AUC (higher be better) use different 
sample strategy and batch-ensemble weight method with C4.5 and HDDT over 

all UCI datasets. 

outperform C4.5. For each sample method we see that the ensemble counterpart of 

the single model have high accuracy. 

In Figure 5.12 we display the average computational time. As expected, when a single 

classifier be use the framework be much faster, but it come at the cost of low accuracy 

(see Figure 5.11). When undersampling be use in the framework we have the small 

computational time, a it us a subset of the observation in each batch and no instance 

be propagate between batches. 

Figure 5.13 show the result for the datasets with concept drift generate use the 

MOA framework. HDIG-based ensemble return good result than a single classifier 

and HDDT again give good accuracy than C4.5. 



Chapter 5. Learning from evolve data stream with skewed distribution 113 

HDIG No Ensemble 

0 

1000 

2000 

3000 

4000 

BD BL SE UNDER BD BL SE UNDER 

Sampling 

TI 
M 

E Algorithm 
C4.5 
HDDT 

Figure 5.12: Batch average result in term of computational time (lower be better) 
use different sample strategy and batch-ensemble weight method with C4.5 

and HDDT over all UCI datasets. 

HDIG No Ensemble 

0.0 

0.2 

0.4 

0.6 

0.8 

BD BL SE UNDER BD BL SE UNDER 

Sampling 

AU 
RO 

C Algorithm 
C4.5 
HDDT 

Figure 5.13: Batch average result in term of computational AUC (higher be better) 
use different sample strategy and batch-ensemble weight method with C4.5 

and HDDT over all drift MOA datasets. 

Figure 5.14 display the result on the Creditcard dataset. This dataset be a good example 

of an unbalanced data stream with concept drift. Once again HDDT be always good than 

C4.5, however the increase in performance give by the ensemble be less important than 

the one register with the UCI datasets. From Figure 5.14, it be hard to discriminate 

the best strategy, a many of them have comparable results. 

Figure 5.15 show the sum of the rank for each strategy over all the batches. As explain 

in Section 5.1.3, for each batch, we assign the high rank to the most accurate strategy 

and then sum the rank over all batches. The high the sum, the high the number of 

time one strategy be superior to the others. 

The strategy with the high sum of rank (BL_HDIG_HDDT ) combine BL with 

HDIG ensemble of HDDTs. BL method leaf the batch unbalanced, which mean 

that the best strategy be actually the one avoid instance propagation/sampling. A 



Chapter 5. Learning from evolve data stream with skewed distribution 114 

HDIG No Ensemble 

0.00 

0.25 

0.50 

0.75 

1.00 

BD BL SE UNDER BD BL SE UNDER 

Sampling 

AU 
RO 

C Algorithm 
C4.5 
HDDT 

Figure 5.14: Batch average result in term of AUC (higher be better) use different 
sample strategy and batch-ensemble weight method with C4.5 and HDDT over 

the Credit Card dataset. 

pair t-test on the rank be then use to compare each strategy with the best. Based 

on this test, we saw that the strategy with the second-highest sum of rank (UN- 

DER_HDIG_HDDT ) be not significantly bad than the first. Compared to the first, 

this strategy implement undersampling at each batch instead of BL. Figure 5.15 con- 

firm that HDDT be good than C4.5. The C4.5 implementation of the win strategy 

(BL_HDIG_C4.5 ) be significantly bad than the best (BL_HDIG_HDDT ). The same 

happens for the second best rank strategy (UNDER_HDIG_HDDT rank high 

than UNDER_HDIG_C4.5 ). 

BL_C4.5 
SE_C4.5 
BD_C4.5 

UNDER_C4.5 
BL_HDIG_C4.5 

SE_HDDT 
SE_HDIG_C4.5 
UNDER_HDDT 

BL_HDDT 
BD_HDIG_C4.5 

BD_HDDT 
BD_HDIG_HDDT 

UNDER_HDIG_C4.5 
SE_HDIG_HDDT 

UNDER_HDIG_HDDT 
BL_HDIG_HDDT 

0 100 200 

Sum of the rank 

Best significant 
FALSE 
TRUE 

AUROC 

Figure 5.15: Comparison of different strategy use the sum of rank in all batch 
for the Credit Card dataset in term of AUC. In gray be the strategy that be not 

significantly bad than the best have the high sum of ranks. 

5.2.4 Discussion 

To our knowledge, our work be the first to evaluate the use of the HDDT tree algorithm 

for stream data. Many of the state-of-the-art technique use the C4.5 algorithm 



Chapter 5. Learning from evolve data stream with skewed distribution 115 

combine with sample or instance propagation to balance the batch before training. 

We have show that when use in data streams, HDDT without sample typically lead 

to good result than C4.5 with sampling. Thus, HDDT can offer good performance 

than C4.5, while actually remove sample from the process. 

The removal of the propagation/sampling step in the learn process have several benefits: 

• It allows a single-pass approach (the observation be process a soon a they 
arrive, avoid several pass throughout the batch for instance propagation). 

• It reduces the computational cost/resources need (this be important since with 
massive amount of data it may no longer be possible to store/retrieve old in- 

stances). 

• It avoids the problem of find previous minority instance from the same concept 
(in the case of a new concept in the minority class, it may not be possible to find 

previous observation to propagate). 

We have use artificial datasets to test how different strategy work under concept drift. 

The use of HDIG a an ensemble weight strategy have increase the performance of 

the single classifiers, not only in artificial datasets with know drift (MOA datasets), but 

even in datasets whose distribution be assume to be more or less stable (UCI datasets). 

Finally, we test our framework on a proprietary dataset contain credit card trans- 

action from online payment. This be a particularly interest dataset, a it be extremely 

unbalanced and exhibit concept drift within the minority class. HDDT performs very 

well when combine with BL (no sampling) and undersampling. An important feature of 

these basic sample strategy be the fact that framework implement them be much 

faster (see Figure 5.12) since no observation be store from previous batches. When 

these two sample strategy give comparable results, the practitioner could prefer un- 

dersampling a it be more memory efficient since it us a reduce part of the batch for 

training. By use undersampling, however, a lot of instance from the majority class 

be not considered. 

5.3 Conclusion 

In the first part of the chapter (Section 5.1) we have first formalize the problem of fraud 

detection and then present three different approach for unbalanced data streams. We 

have compare a static approach against two dynamic way to learn in the presence of 

non-stationarity and unbalanced distribution. From our experimental assessment the 



Chapter 5. Learning from evolve data stream with skewed distribution 116 

static approach be consistently the bad in term of predictive accuracy, a strong signal 

that the data stream be evolve and update the detection strategy be beneficial. From 

the experiment it emerges also that it be important to use strategy to rebalance the 

batch of the stream before training a classification algorithm, either by resampling (e.g. 

undersampling, SMOTE) or by propagate instance from the minority class along the 

stream. 

However, it be not always possible to store or revisit previous transaction in high fre- 

quency data streams. For this reason, in the second part of the chapter (Section 5.2) 

we have present a classification algorithm (HDDT) optimize for unbalanced datasets 

and use it in a data stream environment in order to avoid the propagation of instance 

between batches. HDDT show good performance without the need of rebalancing the 

two class (e.g. no undersampling required) and prove to perform good than standard 

decision tree (e.g. C4.5). 



Chapter 6 

A real-world Fraud Detection 

Systems: Concept Drift Adaptation 

with Alert-Feedback Interaction 

Part of the result present in this chapter have be publish in the follow paper: 

• Andrea Dal Pozzolo, Giacomo Boracchi, Olivier Caelen, Cesare Alippi, and Gian- 
luca Bontempi. Credit Card Fraud Detection and Concept-Drift Adaptation with 

Delayed Supervised Information. In Neural Networks (IJCNN), The 2015 Interna- 

tional Joint Conference on. IEEE, 2015. 

Most FDSs monitor stream of credit card transaction by mean of classifier return 

alert for the riskiest payments. Fraud detection differs from conventional classification 

because, in a first phase, human investigator who have time to ass only a reduce 

number of alert provide a small set of supervise sample denote a feedbacks. Labels 

of the vast majority of transaction be make available only several day later, when 

customer have possibly report unauthorized transactions. These transaction define 

an additional set of supervise sample call delayed samples. The delay in obtain 

accurate label and the interaction between alert and supervise information have to be 

carefully take into consideration when learn in a concept-drifting environment. 

In this chapter we address a realistic fraud detection set and we show that feedback 

and delayed sample have to be handle separately. We design two prototype of FDS 

on the basis of an ensemble and a sliding-window approach (Section 6.3.1) and we show 

that the win strategy consists in training two separate classifier (on feedback and 

delayed samples, respectively), and then aggregate the outcomes. Experiments on large 

117 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 118 

datasets show that the alert precision, which be the primary concern of investigators, can 

substantially be improve by the propose approach. 

In order to obtain precise alerts, feedback sample have to receive large weight than 

non-alerted transaction and method that diminish their role in the learn process 

lead to loss of predictive accuracy. 

6.1 Realistic work condition 

As discuss in Section 3.4, FDSs typically relay on classification algorithm to iden- 

tify transaction at risk of fraud that generate alerts, but be not able to integrate the 

feedback that investigator provide on the alert raise by the FDS. As a consequence, 

most FDSs available in the literature (e.g. [11, 180, 181]) ignore Alert-Feedback Interac- 

tion (AFI), make the unrealistic assumption that all transaction be correctly label 

by a supervisor. 

With a limited number of investigator only a restrict quantity of alert can be checked, 

which mean a small set of label transaction return a feedback. Non-alerted trans- 

action be a large set of unsupervised sample that can be either fraud or genuine. 

Additional label observation be obtain by mean of cardholder that report unau- 

thorized transaction [20, 61]. The number of customer reporting fraud not detect by 

the FDS be usually small and hard to model since cardholder have different habit when 

it come to check the transcript of credit card transaction give by the bank. Then, 

every day in addiction to investigators’ feedback, we have historical supervise sample 

for which the label can safely assume to be correct after some time. In summary, 

we can distinguish between two type of supervise samples: i) feedback provide by 

investigator and ii) historical transaction whose label be receive with a large delay. 

We will call the latter delayed sample to stress the fact that their label be available only 

after a while. 

In this formulation we assume that the FDS be update everyday at midnight and the 

detection model be then apply to all the transaction occur the follow day. Feed- 

back of alert transaction be give by investigator during the day and by the end of 

the day we have all the feedback for the alert generate by the FDS. In these settings, 

the ML algorithm learns from the batch of feedback available at the end of the day 

and be not train from each transaction incrementally, i.e. the algorithm be train only 

when a sufficient batch of supervise sample be provided. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 119 

6.2 Fraud Detection with Alert-Feedback Interaction 

As in Section 5.1.1, we formulate the fraud detection problem a a binary classification 

task where each transaction be associate to a feature vector x and a label y. Features 

in x could be the transaction amount, the shop id, the card id, the timestamp or the 

country, a well a feature extract from the customer profile. Because of the time- 

vary nature of the transactions’ stream, typically, FDSs train (or update) a classifier 

Kt every day (t). In general, FDSs receive a continuous stream of transaction and they 
have to score each transaction online (i.e. in few milliseconds), however, the classifier 

be update once a day, to gather a sufficient amount of supervise transactions. The 

set of transaction arrive at day t, denote a Bt, be process by the classifier Kt−1 
train in the previous day (t − 1). The k riskiest transaction of Bt be report to 
the investigators, where k > 0 represent the number of alert the investigator be able 

to validate. The report alert At be determine by rank the transaction of Bt 
accord to the posterior probability P̂Kt−1(+|x), which be the estimate, return by 
Kt−1, of the probability for x to be a fraud. More formally, At be define as: 

At = {x ∈ Bt s.t. r(x) ≤ k} (6.1) 

where r(x) ∈ {1, . . . , |Bt|} be the rank of the transaction x accord to PKt−1(+|x). In 
other terms, the transaction with the high probability rank first (r(x) = 1) and the 

one with the low probability rank last (r(x) = |Bt|). 

Once the alert At be generated, investigator provide feedback Ft, define a set of k 

supervise couples: Ft = {(x, y), x ∈ At}, which represent the most recent information 
that the FDS receives. At day t, we also receive the label of all the transaction process 

at day t− δ, provide a set of delayed supervise couple Dt−δ = {(x, y), x ∈ Bt−δ}, see 
Figure 6.1. Though investigator have not personally checked these transactions, they be 

by default assume to be genuine after δ days, a far a customer do not report frauds.1 

As a result, the label of all the transaction old than δ day be provide at day t. 

The problem of receive delayed label be also refer to a verification latency [239]. 

Feedbacks Ft can either refer to fraud (correct alerts) or genuine transaction (false 

alerts): correct alert be then True Positives (TPs), while false alert be FPs. Similarly, 

Dt−δ contains both fraud (false negative) and genuine transaction (true negatives), 

although the vast majority of transaction belongs to the genuine class. 

The goal of a FDS be to return accurate alerts: when too many FPs be reported, in- 

vestigators might decide to ignore forthcoming alerts. Thus, what actually matter be 
1 Investigators typically assume that fraud miss by the FDS be report by customer themselves 

(e.g. after have checked their credit card balance), within a maximum time-interval of δ days. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 120 

t −1 t t +1 

Feedbacks)Delayed)Samples) 

t −δ 

All)fraudulent)transac6ons)of)a)day) 

All)genuine)transac6ons)of)a)day) 

Fraudulent)transac6ons)in)the)feedback) 
Genuine)transac6ons)in)the)feedback) 

FtFt�1Ft�3Ft�4Ft�5Ft�6 Ft�2Dt�7Dt�8 Bt+1 

Figure 6.1: The supervise sample available at day t include: i) feedback of the 
first δ day and ii) delayed couple occur before the δth day. In this Figure we set 

δ = 7. 

to achieve the high precision in At. As show in Section 2.2.3, this precision can be 

measure by the quantity 

Pk(t) = 
|TPk(t)| 

k 
(6.2) 

where TPk(t) = {(x, y) ∈ At s.t. y = +}.2 Pk(t) be then the proportion of fraud in the 
top k transaction with the high estimate likelihood of be fraud [64]. 

6.3 Learning strategy 

The fraud detection scenario described in Section 6.2 suggests that learn from feed- 

back Ft be a different problem than learn from delayed sample in Dt−δ. The first 

difference be evident: Ft provide recent, up-to-date, information while Dt−δ might be 

already obsolete once it be available. The second difference concern the percentage of 

fraud in Ft and Dt−δ. While it be clear that the class distribution in Dt−δ be always 

skewed towards the genuine class, the number of fraud in Ft actually depends on the 

performance of classifier Kt−1: when Pk(t) = 0.5 we have feedback Ft with a balance 
distribution, while for Pk(t) > 0.5 we have more fraud than genuine transaction in 

Ft. The third, and probably the most subtle, difference be that supervise couple in Ft 
be not independently drawn, but be instead select by Kt−1 among those transaction 
that be more likely to be frauds. As such, a classifier train on Ft learns how to label 

transaction that be most likely to be fraudulent, and might be in principle not precise 

on the vast majority of genuine transactions. Therefore, beside the fact that Ft and Dt−δ 
might require different resampling methods, Ft and Dt−δ be also representative of two 

2Note that in this formulation |Ft| = |At| and |At| = k. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 121 

different classification problem and, a such, they have to be separately handled. In 

the following, two traditional fraud detection approach be present (Section 6.3.1), 

and further developed to handle separately feedback and delayed supervise couple 

(Section 6.3.2). Experiments in Section 6.5 show that this be a valuable strategy, which 

substantially improves the alert precision. 

6.3.1 Conventional Classification Approaches in FDS 

During the FDS operation, both feedback Ft and delayed supervise sample Dt−δ can be 

exploit for training or update the classifier Kt. In particular, we train the FDS con- 
sidering the feedback from the last δ day (i.e. {Ft,Ft−1, . . . ,Ft−(δ−1)}) and the delayed 
supervise pair from the lastM day before the feedbacks, i.e. {Dt−δ, . . . ,Dt−(δ+M−1)}. 

In the follow we present two conventional solution for concept-drift adaptation [171, 

182] built upon a classification algorithm prove an estimate of the probability P(+|x). 

• Wt: a slide window classifier that be daily update over the supervise sample 
receive in the last δ +M days, i.e. {Ft, . . . ,Ft−(δ−1),Dt−δ, . . . ,Dt−(δ+M−1)} (see 

Figure 6.2(a)). 

• Et: an ensemble of classifier {M1,M2, . . . ,MM ,F }, where Mi be train on 
Dt−(δ+i−1) and Ft be train on all the feedback of the last δ day {Ft, . . . ,Ft−(δ−1)} 
(see Figure 6.2(b)). The posterior probability PEt(+|x) be estimate by average 
the posterior probability of the individual classifiers, PMi(+|x), i = 1, . . . ,M and 
PFt(+|x). Note that we use a single classifier to learn from the set of feedback 
since their size be typically small. Everyday, Ft be re-trained consider the new 
feedbacks, while a new classifier be train on the new delayed supervise couple 

provide (Dt−δ) and include in the ensemble. At the same time, the most obsolete 

classifier be remove from the ensemble. 

These solution implement two basic approach for handle concept drift that can be 

further improve by adopt dynamic slide window or adaptive ensemble size [240]. 

6.3.2 Separating delayed Supervised Samples from Feedbacks 

As explain in Section 6.3, our intuition be that feedback and delayed transaction 

have to be treat separately. Therefore, at day t we train a specific classifier Ft on 
the feedback of the last δ day {Ft, . . . ,Ft−(δ−1)} and denote by PFt(+|x) it posterior 
probability. We then train a second classifier on the delayed sample by mean either of 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 122 

FtFt�1Ft�3Ft�4Ft�5Ft�6 Ft�2Dt�7Dt�8 

Wt 

FtWDt 

AWt 
(a) Sliding window strategy 

FtFt�1Ft�3Ft�4Ft�5Ft�6 Ft�2Dt�7Dt�8 

FtM1M2 

EDt AEt 

Et 

M1M2 Ft 

(b) Ensemble strategy 

Figure 6.2: Learning strategy for feedback and delayed transaction occur in 
the two day (M = 2) before the feedback (δ = 7). 

a sliding-window or an ensemble mechanism (see Figure 6.2): Let u denote by WDt the 
classifier train on a slide window of delayed sample {Dt−δ, . . . ,Dt−(δ+M−1)} and 
by PWDt (+|x) it posterior probabilities, while E 

D 
t denotes the ensemble of M classi- 

fiers {M1,M2, . . . ,MM} where each individual classifier Mi be train on Dt−δ−i, i = 
1, . . . ,M . Then, the posterior probability PEDt (+|x) be obtain by average the poste- 
rior probability of the individual classifiers. 

Both “delayed” classifier WDt and EDt have to be aggregate with Ft to exploit informa- 
tion provide by feedbacks. However, to raise alerts, we be not interested in aggregation 

method at the label level but rather at the posterior probability level. For the sake of 

simplicity we adopt the most straightforward approach base on a linear combination of 

the posterior probability of the two classifier (Ft and one among WDt and EDt ). Let u 
denote by AEt the aggregation of Ft and EDt where PAEt (+|x) be define as: 

PAEt (+|x) = αtPFt(+|x) + (1− αt)PEDt (+|x) (6.3) 

A similar definition hold for AWt , the aggregation of Ft and WDt : 

PAWt (+|x) = αtPFt(+|x) + (1− αt)PWDt (+|x) (6.4) 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 123 

Note that Ft andWDt jointly use the training set ofWt and, similarly, the two classifier 
Ft and EDt jointly use the same training sample of Et (see Figure 6.2). 

Feedbacks represent a small portion of the supervise sample use for training classifier 

Wt, hence they have little influence on PWt(+|x). Similarly, Ft represent one of the 
classifier of the ensemble Et, hence feedback influence PEt(+|x) only for 1M+1 , while 
delayed sample for MM+1 . By aggregate accord to (6.3) and (6.4) we be able to 

give a large weight to feedback via the parameter αt, e.g. a large αt give great 

influence to Ft in the aggregate posteriori probability PAWt and PAEt . 

Experiments in Section 6.5 show that handle feedback separately from delayed su- 

pervised sample provide much more precise alerts, and by aggregate with αt = 0.5 

we be able to outperform standard FDSs train on feedback and delayed supervise 

sample pool together (like Wt and Et).3 

Table 6.1 summarizes the classifier use in this chapter. 

Table 6.1: Classifiers use in the chapter. 

Ft A feedback classifier daily update use feedback from t to t− δ. 
Wt A slide window classifier daily update use δ +M days. 
WDt A delayed slide window classifier daily update use M days. 
AWt An aggregation of WDt and Ft accord to (6.3). 
Et An ensemble of classifier daily update use δ +M days. 
EDt A delayed ensemble of classifier daily update use M days. 
Mi ith member of an ensemble of classifiers. 
AEt An aggregation of EDt and Ft accord to (6.4). 
αt Weight assign to Ft in aggregation AEt and AWt . 

6.3.3 Two Specific FDSs base on Random Forest 

The FDSs present in the previous section have be implement use a Random For- 

est [43] with 100 trees. In particular, forWDt ,Wt and for allMi, i = 1, . . . ,M , we use a 
Balanced Random Forest (BRF) [50] where each tree be train on a balance bootstrap 

sample, obtain by randomly undersampling the majority class while preserve all the 

minority class sample in the correspond training set. Each tree of BRF receives a 

different random sample of the genuine transaction and the same sample from the fraud 

class in the training set, yield a balance training set. This undersampling strategy 

allows one to learn tree with balance distribution and to exploit many subset of the 

majority class. At the same time, this resampling method reduces training time. A 
3In the specific case of the ensemble approach and αt = 0.5, the aggregation of Ft and EDt (AEt ) 

corresponds to assign a large weight to Ft in Et. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 124 

drawback of undersampling be that we be potentially remove relevant training sam- 

ples form the dataset, however this problem be mitigate by the fact that we learn 100 

different trees. Using undersampling allows u to rebalance the batch without propa- 

gate minority class observation along the stream a in [171]. In contrast, for Ft that 
be train on feedback we adopt a standard RF where no resampling be performed. 

6.4 Selection bias and Alert-Feedback Interaction 

In this section we discus the problem of training a classifier on feedback samples. A 

conventional assumption in ML be that the learn algorithm receives training and test 

sample drawn accord to the same distribution. However, this assumption do not 

hold in our case, since Alert-Feedback Interaction (AFI) provide a recent bias subset of 

supervise transaction (Ft), which be not representative of all the transaction occur 

in a day (Bt). As a consequence, classifier Ft learns under a selection bias govern by 
AFI. 

Let u define a random selection variable s that associate to each sample in Bt value 1 if 

the transaction be in Ft and 0 otherwise. When training a classifier Ft on Ft instead of Bt 
we get an estimate of PFt(+|x, s = 1) rather than PFt(+|x). As show in Section 3.2.1 
(see (3.1)), a standard solution to SSB consist into training a weight-sensitive algorithm, 

where a training sample (x, y) receives a weight w: 

w = 
P(s = 1) 
P(s = 1|x, y) (6.5) 

Alternatively, if the learn algorithm be not able to accept weight training samples, 

it suffices to resample the training set with replacement and probability equal to the 

weight [133]. 

In equation (6.5), P(s = 1) can be easily estimate by calculate the proportion of 
feedback in a day. However, P(s = 1|x, y) be not easy to estimate since we know the 
label of only few feedback samples. An assumption often make in literature [135–138] be 

that the selection variable s be independent of the class y give the input x (covariate 

shift): P(s|y, x) = P(s|x). Then equation (6.5) becomes: 

w = 
P(s = 1) 
P(s = 1|x) (6.6) 

We can get an estimate of P(s = 1|x) by building a classifier that predicts s a the class 
label, which be a classifier that learn to discriminate feedback and non-feedbacks sample 

between t and t− δ. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 125 

The problem with all these correction be that, since feedback be select accord 

to P(+|x), we have that P(s = 1|x) and P(+|x) be highly correlated. Moreover, we 
expect P(s = 1|x) and P(s = 1|x, y) to be large for feedback samples, lead to 
small weight in (6.5) and (6.6). This mean that importance weight technique 

be expect to low the influence of feedback in the learn process. As show in 

our previous work [20], strategy reduce the weight of feedback sample be often 

return less precise alert (lower Pk). For this reason, re-weighting be not expect 

to bring improvement with AFI. Additionally, the covariate shift assumption be hard 

to defend, because the percentage of fraud in the feedback be much high than the one 

register in typical day, i.e. the probability of select a feedback cannot be say to be 

independent of the class of the transaction (P(s|y, x) 6= P(s|x)). 

6.5 Experiments 

In this work we use three large datasets contain credit card transaction make by 

European cardholder via on-line websites. The first, refer to a 2013, have transac- 

tions from September 2013 to January 2014, the second one, refer to a 2014, have 

transaction from August to December 2014 and the third, refer to a 2015, from 

January 2015 to end of May 2015. In Table 6.2 we have report additional information 

about the data. The datasets contain both original and aggregate feature calculate a 

show in Section 5.1.3. It be easy to notice that these datasets be highly unbalanced, i.e. 

fraud account for about 0.2% of all transactions. The number of fraud be also vary 

significantly over the year (see Figure 6.3). 

Table 6.2: Datasets 

Id Start day End day # Days # Transactions # Features % Fraud 
2013 2013-09-05 2014-01-18 136 21’830’330 51 0.19% 
2014 2014-08-05 2014-12-31 148 27’113’187 51 0.22% 
2015 2015-01-05 2015-05-31 146 27’651’197 51 0.26% 

In the first experiment we process all datasets to ass the importance of separate 

feedback from delayed supervise samples. Though we expect these stream to be 

affected by Concept Drift (CD), since they span a quite long time range, we do not have 

any ground truth to investigate the reaction to CD of the propose FDS. To this purpose, 

we design the second experiment where we juxtapose batch of transaction acquire in 

different time of the year to artificially introduce CD in a specific day in the transaction 

stream. 

In both experiment we test FDSs built on Random Forests a present in Section 6.3.3. 

We consider both the slide window and ensemble approach and compare the 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 126 

2013 2014 2015 

● 

● 
●●● 

● 
● 

● 

●● 
● 
● 

● 

●● 
● 

● 

● 

● 

●●● 

● 

● 

●● 

● 

● 

● 

●●● 

● 

● 

● 

● 

● 

● 
●● 
● 
●●● 

●● 
● 

● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

● 
● 

● 
● 

● 

● 
● 

● 
● 

● 

● 
● 

● 
● 
● 

● 

● 

● 

● 

● 

● 
● 
● 

● 

●●● 
● 
● 

● 

●● 
● 

● 
● 

● 
● 
●● 

● 
● 

● 

● 

● 

● 

● 
●●● 

● 
● 
● 
● 
●● 
●● 
●●● 

● 
●● 
●● 
●●● 

● 
● 
● 

● 

● 

● 
● 
●●● 

● 

● 

● 

●● 
● 
● 

●● 

● 
● 

● 

● 

● 

● 

●● 

● 

● 

● 
● 

● 

● 

● 
● 

● 

● 

● 
●● 

●● 

●● 

● 
● 

● 

● 

● 
● 
● 

● 

●● 

● 

● 
●●●● 

●● 
● 

● 
● 

● 
● 

● 

●● 
● 
● 

●● 

● 

● 

● 
●●● 

●● 

● 
● 
● 

● 

● 
●● 
●● 
●● 
● 
●●●● 

● 

● 

● 

● 

● 

● 
● 

● 
● 
● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

●● 
●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●●● 

●● 
● 

● 

● 
● 

● 

● 
● 

●●● 
● 
● 

●● 
●● 
●●● 

●● 

● 
● 

● 
● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 
● 

● 
● 
●●● 

● 

●● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

●●● 
● 

● 

● 

● 
● 
●● 
● 
● 
● 

● 

● 
● 

● 

● 
● 

● 

● 
● 
● 

● 

● 

● 

● 
● 

● 

● 

● 

●● 

● 

●● 

● 

● 

● 
● 

● 

●● 
●●● 

● 

● 
● 

● 
● 
● 
● 

● 

● 

● 
●● 

● 

● 
●● 

●● 

● 

● 

● 

● 
● 

● 

● 

●● 
●●● 

● 
● 
● 

● 

● 

● 
● 

●● 
● 

● 

● 

● 

● 

● 

●● 

● 
● 
● 

● 

● 

● 
● 
● 

● 

●●● 
● 

● 

0 

500 

1000 

1500 

2000 

2500 

Sep Oct Nov Dec Jan Aug Sep Oct Nov Dec JanJan Feb Mar Apr May Jun 

Day 

N 
um 

be 
r 

of 
fr 

au 
d 



Figure 6.3: Number of daily fraud for datasets in Table 6.2. 

accuracy of pool feedback and delayed supervise sample together (Wt and Et) 
against learn separate classifier (Ft, WDt and EDt ) that be then aggregate (AWt 
and AEt ). Let u recall that each test classifier raise alert differently. This mean 
that also the feedback return to the classifier might be different. This have to be 

consider when compare several classifiers, for instance, when compare Wt and 
WDt , the supervise information provide be not the same because, in the first case alert 
be raise by Wt while in the second by WDt . 

At first we assume that after δ = 7 day all the transaction label be provide (delayed 

supervise information) and that we have a budget of k = 100 alert that can be checked 

by the investigators: thus, Ft be train on a window of 700 feedbacks. Then in the 
experiment of Section 6.5.3 we use δ = 15 and we allow Ft to be train on a large 
number of feedback per day (|Ft| ≥ 100) to see how these parameter influence the 
performance of the detection. In the same section we test some SSB correction technique 

to see whether they can actually improve the performance of Ft. 

Experiments from Section 6.5.4 compare the propose classifiers’ aggregationAWt against 
a classifier Rt train on all recent transaction that make the unrealistic assumption 
that all daily transaction can be checked by investigators. This experiment aim to com- 

pare a classifier Rt ignore AFI with the aggregation propose use different accuracy 
measures, Pk, AUC and AP. 

In Section 6.5.5 we run experiment to study whether adapt the weight αt give to 

Ft in (6.3) and (6.4) have an impact on the performance of AWt and AEt . It emerges that 
combine classifier with a standard mean (αt = 0.5 for all the data stream) be often 

competitive to more complicate weight strategies. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 127 

For all experiments, we set M = 16 so that WDt be train on a window of 16 day 
and EDt (resp. Et) be an ensemble of 16 (resp. 17) classifiers.4 Each experiment be 
repeat 10 time to reduce the results’ variability due to bootstrapping of the training 

set in the random forests. The FDS performance be assess by mean of the average Pk 
over all the batch (the high the better) and use a pair t-test to ass whether the 

performance gap between each pair of test classifier be significant or not. We compute 

the pair t-test on the rank result from the Friedman test [232] a recommend by 

Demsar [241] (see Section 5.1.3). 

6.5.1 Separating feedback from delayed supervise sample 

In order to evaluate the benefit of learn on feedback and delayed sample separately, 

we first compare the performance of classifier Wt against Ft, WDt and the aggregation 
AWt . The aggregation AWt and AEt be compute use αt = 0.5 over all the data 
stream. In addition to the classifier present in Section 6.3, we consider also a static 

classifier call St that be train once on the first M day and never updated. In absence 
of concept drift we expect St to perform similarly to WDt (WDt be just St update every 
day). Table 6.3(a) show the average Pk over all the batch for the three datasets 

separately. 

Table 6.3: Average Pk for the slide and ensemble strategy (δ = 7, M = 16 and 
αt = 0.5). 

(a) slide approach 

dataset classifier mean sd 
2013 F 0.62 0.25 
2013 WD 0.54 0.22 
2013 W 0.57 0.23 
2013 S 0.48 0.25 
2013 AW 0.70 0.21 
2014 F 0.59 0.29 
2014 WD 0.58 0.26 
2014 W 0.60 0.26 
2014 S 0.54 0.24 
2014 AW 0.69 0.24 
2015 F 0.67 0.25 
2015 WD 0.66 0.21 
2015 W 0.68 0.21 
2015 S 0.58 0.23 
2015 AW 0.75 0.20 

(b) ensemble approach 

dataset classifier mean sd 
2013 F 0.62 0.25 
2013 ED 0.50 0.24 
2013 E 0.58 0.24 
2013 S 0.47 0.24 
2013 AE 0.69 0.21 
2014 F 0.60 0.30 
2014 ED 0.49 0.28 
2014 E 0.56 0.27 
2014 S 0.53 0.25 
2014 AE 0.68 0.26 
2015 F 0.66 0.25 
2015 ED 0.61 0.19 
2015 E 0.67 0.20 
2015 S 0.58 0.23 
2015 AE 0.74 0.20 

In all datasets, AWt outperforms the other FDSs in term of Pk. The barplots of Figure 
6.5 show the sum of rank for each classifier and the result of the pair t-tests. Figure 

6.5 indicates that in all datasets (Figures 6.5(a), 6.5(b) and 6.5(c)) AWt be significantly 
4We ran several experiment with M = 1, 8, 16, 24 and found M = 16 a a good trade-off between 

performance, computational load, and the number of day that can be use for test in each stream. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 128 

good than all the other classifiers. Ft achieves high average Pk and high sum of 
rank than WDt and Wt: this confirms that feedback be very important to increase Pk. 

Figure 6.4(a) display the value of Pk for AWt and Wt in each day, average in a neigh- 
borhood of 15 days. After December there be a substantial performance drop, which can 

be see a a CD due to a change in cardholder behaviour in the period before and after 

Christmas. However, AWt dominates Wt along the whole 2013 dataset, which confirms 
that a classifier AWt that learns on feedback and delayed transaction separately outper- 
form a classifierWt train on all the supervise information pool together (feedbacks 
and delayed transactions). 

Table 6.3(b) and Figures 6.5(d), 6.5(e) and 6.5(f) confirm this claim also when the 

FDS implement an ensemble of classifiers.5 In particular, Figure 6.4(b) display the 

smooth average Pk of classifier AEt and Et. For the whole dataset AEt have good Pk 
than Et. 

W 
AW 

(a) Sliding window strategy 

AE 
E 

(b) Ensemble strategy 

Figure 6.4: Average Pk per day (the high the better) for classifier on dataset 2013 
smooth use move average of 15 days. In the slide window approach classifier 
AWt have high Pk than Wt, and in the ensemble approach AEt be superior than Et. 

6.5.2 Artificial dataset with Concept Drift 

The rationale of this experiment be to test the reaction of the propose FDS to abrupt 

CD. To this purpose, in this section we artificially introduce an abrupt CD in specific 

day by juxtaposing transaction acquire in different time of the year. Table 6.4 report 

the three datasets that have be generate by concatenate batch of the dataset 2013 

with batch from 2014. The number of day after CD be set such that the FDS have the 

time to forget the information from the previous concept. 

Table 6.5(a) show the value of Pk average over all the batch in the month before the 

change for the slide window approach, while Table 6.5(b) show Pk in the month after 

the CD. AWt report the high Pk before and after CD. Similar result be obtain with 
5Please note that classifier Ft return different result between 6.3(a) and 6.3(b) because of the 

stochastic nature of RF. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 129 

SWDWFAW 

(a) Sliding window strate- 
gy on dataset 2013 

SWDFWAW 

(b) Sliding window strate- 
gy on dataset 2014 

SWDFWAW 

(c) Sliding window strate- 
gy on dataset 2015 

EDSEFAE 

(d) Ensemble strategy on 
dataset 2013 

EDSEFAE 

(e) Ensemble strategy on 
dataset 2014 

SE EDFAE 

(f) Ensemble strategy on 
dataset 2015 

Figure 6.5: Comparison of classification strategy use sum of rank in all batch 
and pair t-test base upon on the rank of each batch (classifiers have the same 
label on their bar be not significantly different with a confidence level of 0.95). In all 
datasets (2013, 2014 and 2015), classifier aggregation AWt and AEt be significantly 

good that the others. 

Table 6.4: Datasets with Artificially Introduced CD 

Id Start 2013 End 2013 Start 2014 End 2014 
CD1 2013-09-05 2013-09-30 2014-08-05 2014-08-31 
CD2 2013-10-01 2013-10-31 2014-09-01 2014-09-30 
CD3 2013-11-01 2013-11-30 2014-08-05 2014-08-31 

Table 6.5: Average Pk in the month before and after CD for the slide window 

(a) Before CD 
CD1 CD2 CD3 

classifier mean sd mean sd mean sd 
F 0.411 0.142 0.754 0.270 0.690 0.252 
WD 0.291 0.129 0.757 0.265 0.622 0.228 
W 0.332 0.215 0.758 0.261 0.640 0.227 
AW 0.598 0.192 0.788 0.261 0.768 0.221 

(b) After CD 
CD1 CD2 CD3 

classifier mean sd mean sd mean sd 
F 0.635 0.279 0.511 0.224 0.599 0.271 
WD 0.536 0.335 0.374 0.218 0.515 0.331 
W 0.570 0.309 0.391 0.213 0.546 0.319 
AW 0.714 0.250 0.594 0.210 0.675 0.244 

the ensemble approach (Tables 6.6(a), 6.6(b)). In all these experiments, AEt be also faster 
than standard classifier Et and Wt to react in the presence of a CD (see Figure 6.6). 
The large variation of Pk over the time reflect the non-stationarity of the data stream. 

Expect for dataset CD1, we have on average low Pk after Concept Drift. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 130 

Table 6.6: Average Pk in the month before and after CD for the ensemble 

(a) Before CD 
CD1 CD2 CD3 

classifier mean sd mean sd mean sd 
F 0.585 0.183 0.731 0.267 0.706 0.245 
ED 0.555 0.318 0.563 0.217 0.562 0.223 
E 0.618 0.313 0.696 0.276 0.648 0.245 
AE 0.666 0.222 0.772 0.272 0.751 0.221 

(b) After CD 
CD1 CD2 CD3 

classifier mean sd mean sd mean sd 
F 0.696 0.270 0.477 0.235 0.610 0.270 
ED 0.551 0.298 0.286 0.182 0.486 0.265 
E 0.654 0.266 0.373 0.235 0.581 0.268 
AE 0.740 0.232 0.575 0.227 0.659 0.245 

AW 
W 

(a) Sliding window strategy on dataset CD1 

AW 
W 

(b) Sliding window strategy on dataset CD2 

W 
AW 

(c) Sliding window strategy on dataset CD3 

AE 
E 

(d) Ensemble strategy on dataset CD3 

Figure 6.6: Average Pk per day (the high the better) for classifier on datasets with 
artificial concept drift (CD1, CD2 and CD3) smooth use move average of 15 
days. In all datasets AWt have high Pk than Wt. For the ensemble approach we show 
only dataset CD3, where AEt dominates Et for the whole dataset (similar result be 
obtain on CD1 and CD2, but they be not include for compactness). The vertical 

bar denotes the date of the concept drift. 

6.5.3 Improving the performance of the feedback classifier 

The goal of these experiment be to see how the performance of Ft be influence by: 
i) the number of day of feedback available (defined by δ), ii) method correct the 

selection bias, and iii) the number of feedback available everyday. To this purpose we 

first test strategy with Ft train on 15 day (δ = 15). In Table 6.7 we see that, with 
δ = 15, Ft have high Pk than when it be train with δ = 7 (see Table 6.3). The 
aggregation AWt and AEt also improve their performance with δ = 15. 

The rationale of the second experiment be to see where or not the method for SSB 

would improve the performance of the feedback classifier Ft. In Table 6.8 we test two 
type of method for correct the selection bias: importance weight with weight 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 131 

Table 6.7: Average Pk for the slide and ensemble approach when δ = 15. 

(a) slide approach 

dataset classifier mean sd 
2013 F 0.67 0.24 
2013 WD 0.54 0.22 
2013 AW 0.72 0.20 
2014 F 0.67 0.27 
2014 WD 0.56 0.27 
2014 AW 0.71 0.25 
2015 F 0.71 0.21 
2015 WD 0.62 0.21 
2015 AW 0.76 0.19 

(b) ensemble approach 

dataset classifier mean sd 
2013 F 0.67 0.23 
2013 ED 0.48 0.23 
2013 AE 0.72 0.21 
2014 F 0.67 0.28 
2014 ED 0.49 0.27 
2014 AE 0.70 0.25 
2015 F 0.71 0.22 
2015 ED 0.56 0.18 
2015 AE 0.75 0.20 

provide by (6.5) and Joint Probability Averaging [242]. In the first case we use a 

weight-sensitive algorithm an implementation of the Random Forest base on conditional 

inference tree [243] available in the party package [244]. In the second case we use 

the semi-supervised version (SJA) of the algorithm propose by Fan et al. [242] for 

SSB correction. These result be obtain with the slide approach when δ = 15, 

but equivalent one can be obtain with the ensemble since Ft be the same for both 
approaches. If we compare the result of Table 6.8 with the one of Ft in Table 6.7 we see 
that technique for SSB do not improve the performance of Ft, and weight technique 
can actually deteriorate it performance. 

Table 6.8: Average Pk of Ft with method for SSB correction when δ = 15. 

dataset SSB correction mean sd 
2013 SJA 0.66 0.24 
2013 weigthing 0.64 0.25 
2014 SJA 0.66 0.27 
2014 weigthing 0.65 0.28 
2015 SJA 0.71 0.22 
2015 weigthing 0.68 0.23 

In the remainder of the section we investigate how request a large number of feedback 

(i.e. generate a large number of alerts) influence the performance of Ft and it 
aggregation. If have more feedbacks, lead to good performances, then the company 

should hire more investigators. In Table 6.9 we allow Ft to be train on a large 
number of feedback (|Ft| ≥ 100) and evaluate the performance with Pk use k = 100 
for all experiments. It emerges that, the company would obtain a good detection by 

increase the number of investigators. However, the increase in accuracy should be 

evaluate against the cost of hire more investigators, perhaps good accuracy be not 

sufficient to justify the high cost of investigation. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 132 

Table 6.9: Average Pk for the slide approach with more than 100 feedback per day 
when δ = 15 and k = 100. 

(a) dataset 2013 
|Ft| classifier mean sd 
100 F 0.68 0.23 
100 AW 0.72 0.20 
300 F 0.74 0.22 
300 AW 0.78 0.19 
500 F 0.76 0.20 
500 AW 0.79 0.18 

(b) dataset 2014 
|Ft| classifier mean sd 
100 F 0.68 0.27 
100 AW 0.71 0.24 
300 F 0.73 0.25 
300 AW 0.76 0.22 
500 F 0.75 0.23 
500 AW 0.78 0.21 

(c) dataset 2015 
|Ft| classifier mean sd 
100 F 0.71 0.21 
100 AW 0.76 0.19 
300 F 0.80 0.18 
300 AW 0.82 0.17 
500 F 0.82 0.17 
500 AW 0.83 0.17 

6.5.4 Standard accuracy measure and classifier ignore AFI 

In this section we compare the performance of the feedback classifier Ft and it aggre- 
gation AWt against a classifier Rt that be train on all recent transaction occur 
between t and t − δ (feedbacks and non-alerted transactions, see Figure 6.7). In this 
experiment we want to see how a classifier Rt ignore Alert-Feedback Interaction com- 
pares to Ft (trained only on feedbacks).6 It be important to remark than Rt and Ft be 
train on the same number of day (defined by δ), while WDt be train on a window 
of delayed samples. We ass the result use Pk and two other detection measure 

present in Section 2.2.3, namely AUC and AP. Note that these last figure of merit 

can be consider a global rank measures, because they ass the rank quality 

over all the instances, not only in the top k a Pk does. As explain in Section 6.1, 

most work in fraud detection use AUC a performance measure and train a classifier 

use all available information like Rt without consider investigators’ feedback. 

FtFt�1Ft�3Ft�4Ft�5Ft�6 Ft�2Dt�7Dt�8 

FtWDt 

AWt 

Rt 

Figure 6.7: A classifier Rt train on all recent transaction occur between t and 
t − δ make the unrealistic assumption that all these transaction have be checked 

and label by investigators. In this figure we use δ = 7 and M = 2. 

Table 6.10 report the result for the three datasets. We notice that, when use global 

rank measure such a AUC and AP, Rt always outperforms Ft and the latter be 
6Note that in a real scenario we cannot compute Rt, since label be available only for feedback 

sample between t and t− δ. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 133 

often the bad between all classifier considered. On the contrary, when the accuracy 

be evaluate in term of Pk, then Ft be a good alternative to Rt. These result can 
be interpret in the follow way: when the objective be to get accurate rank of 

the most suspicious transaction (maximize Pk) we should use a classifier train on 

transaction that be a risky a the one we want to predict, hence Ft should be favor 
because it contains only risky sample (feedbacks). On the contrary, a classifier train 

on all daily transaction (which be mostly genuine), like Rt, will be a good choice if 
we want to obtain a good rank on all sample (e.g. maximize AUC). 

From Table 6.10 and Figure 6.8 we see also that Rt have in general good accuracy than 
WDt , which confirms the presence of a non-stationary environment and training on more 
recent transaction allows one to react faster to CD. The aggregation of Ft and WDt in 
AWt return high Pk than Rt, but it be beaten by Rt in term of AUC. In a nutshell, to 
have good prediction on the k transaction with the large fraud risk it be good to train 

a specific classifier on the feedback (previous risky transactions). When the objective be 

instead to obtain good overall accuracy (not only on the top k) we should perhaps use a 

standard classifier that use all information available. 

Table 6.10: Average AP, AUC and Pk for the slide approach (δ = 15, M = 16 and 
αt = 0.5). 

(a) dataset 2013 
metric classifier mean sd 
AP F 0.31 0.13 
AP WD 0.28 0.15 
AP R 0.33 0.15 
AP AW 0.40 0.14 
AUC F 0.83 0.06 
AUC WD 0.94 0.03 
AUC R 0.96 0.01 
AUC AW 0.94 0.03 
Pk F 0.67 0.24 
Pk WD 0.54 0.22 
Pk R 0.60 0.22 
Pk AW 0.72 0.20 

(b) dataset 2014 
metric classifier mean sd 
AP F 0.30 0.15 
AP WD 0.29 0.17 
AP R 0.35 0.19 
AP AW 0.39 0.16 
AUC F 0.81 0.08 
AUC WD 0.94 0.03 
AUC R 0.96 0.02 
AUC AW 0.93 0.03 
Pk F 0.67 0.27 
Pk WD 0.56 0.27 
Pk R 0.63 0.25 
Pk AW 0.71 0.25 

(c) dataset 2015 
metric classifier mean sd 
AP F 0.27 0.11 
AP WD 0.30 0.13 
AP R 0.39 0.16 
AP AW 0.37 0.12 
AUC F 0.81 0.07 
AUC WD 0.95 0.02 
AUC R 0.97 0.01 
AUC AW 0.94 0.02 
Pk F 0.71 0.21 
Pk WD 0.62 0.21 
Pk R 0.68 0.20 
Pk AW 0.76 0.19 

6.5.5 Adaptive aggregation 

The rational of follow experiment be to investigate how to adapt the aggregation AWt 
and AEt in the presence of non-stationary environments. Let u recall that the posterior 
probability of AWt be define a follow: PAWt (+|x) = αtPFt + (1 − αt)PWDt , where 
we have introduce the compact notation PFt , PWDt to denote PFt(+|x), PWDt (+|x). 
Similar definition hold for AEt (aggregation of Ft and EDt , see (6.4)). Perhaps the most 
straightforward adaptation strategy be to set the weight at day t + 1 (αt+1) depend 

on the performance of Ft and WDt (or EDt ). Operating with different value of αt 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 134 

(a) Ranking measure in term of AP (b) Ranking measure in term of AUC 

(c) Ranking measure in term of Pk 

Figure 6.8: Detection accuracy of AWt , Ft, WDt and Rt measure use different 
performance measure on the 2013 dataset. AUC and AP be measure of global rank 
while Pk be a measure of rank in the top k transaction with the large probability 

of be fraudulent. 

corresponds to cut the plane (PFt ,PWDt ) use straight line have different angular 
coefficient (see Figure 6.10). 

In our aggregation we would like to give a large weight to the probability that be more 

accurate, so if PFt be more accurate than PWDt (or PEDt ), at day t+1 we want αt+1 > 0.5. 
In order to define the weight αt+1 we have first to decide how to measure the accuracy 

of Ft when the classifier generate the alert (and request feedbacks) be AWt .7 

Let FAWt be the feedback request by A 
W 
t and FFt be the feedback request by 

Ft (see Figure 6.9). Let Y+t (resp. Y−t ) be the subset of fraudulent (resp. genuine) 
transaction in day t, we define the follow sets: CORRFt = FAWt ∩FFt∩Y 

+ 
t , ERRFt = 

FAWt ∩ FFt ∩ Y 
− 
t , and MISSFt = FAWt ∩ {Bt \ FFt} ∩ Y 

+ 
t . In the example of Figure 6.9 

we have |CORRFt | = 4, |ERRFt | = 2 and |MISSFt | = 3. We can now compute some 
accuracy measure of Ft in the feedback request by AWt :8 

• accFt = 1− 
|ERRFt | 

k 

• accMissFt = 1− 
|ERRFt |+|MISSFt | 

k 

• precisionFt = 
|CORRFt | 

k 

7We cannot compute Pk of Ft and WDt because feedback be request by AWt . 
8These accuracy measure be inspire by standard classification metric present in Section 2.1.4 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 135 

• recallFt = 
|CORRFt | 
|FAWt 

∩Y+t | 

• aucFt = probability that fraudulent feedback rank high than genuine feedback 
in FAWt accord to PFt . 

• probDifFt = difference between the mean of PFt calculate on the fraud and the 
mean on the genuine in FAWt . 

FAWt 
FFt 

FWDt 

Bt 

Figure 6.9: Feedbacks request by AWt (FAWt ) be a subset of all the transaction of 
day t (Bt). FFt ( FWDt ) denotes the feedback request by Ft (W 

D 
t ). The symbol + 

be use for fraud and − for genuine transactions. 

Similarly, we can compute the accuracy of classifier WDt (or EDt ) in the feedback re- 
quest by AWt . For example, in Figure 6.9 we have |CORRWDt | = 3, |ERRWDt | = 5 and 
|MISSWDt | = 4. By choose one of the previous accuracy measure we obtain different 
way to combine the probability and adapt to CD at the aggregation level. Let’s imag- 

ine we choose aucFt a metric for Ft and similarly aucWDt for W 
D 
t , then we compute 

αt+1 a follows: 

αt+1 = 
aucFt 

aucFt + aucWDt 
(6.7) 

Using (6.7) ensures that αt+1 ∈ [0, 1]. In Table 6.11 we present the average Pk of classifier 
AWt when δ = 15 with adaptive αt, i.e. αt change everyday accord to one of the 
accuracy measure present before. 

We have also consider the ideal case in which we could use everyday the value of αt 
return the best results, by test different value αt ∈ {0.1, . . . , 0.9} and select α∗t 
a the one allow the aggregation to have the high Pk. Note that in a real scenario it 

be not possible to compute α∗t since we cannot ask feedback for more than one classifier 

and all the feedback be available only at the end of the day. Table 6.11 report result 

obtain with α∗t under the name best for α adaptation. In general, the performance 

increase with α∗t be marginal w.r.t. all the other strategy considered. Also, it appear 

that adapt the weight be not significantly good than keep everyday αt = 0.5. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 136 

Table 6.11: Average Pk of AWt with adaptive αt (δ = 15). 

(a) dataset 2013 
α adaptation mean sd 
best(α∗t ) 0.73 0.21 
probDif 0.70 0.22 
acc 0.72 0.21 
accMiss 0.72 0.21 
precision 0.72 0.21 
recall 0.71 0.21 
auc 0.72 0.21 
none(αt = 0.5) 0.72 0.20 

(b) dataset 2014 
α adaptation mean sd 
best(α∗t ) 0.72 0.24 
probDif 0.68 0.26 
acc 0.71 0.24 
accMiss 0.70 0.24 
precision 0.71 0.24 
recall 0.70 0.25 
auc 0.71 0.24 
none(αt = 0.5) 0.71 0.24 

(c) dataset 2015 
α adaptation mean sd 
best(α∗t ) 0.76 0.19 
probDif 0.73 0.21 
acc 0.76 0.19 
accMiss 0.75 0.19 
precision 0.75 0.19 
recall 0.74 0.20 
auc 0.75 0.19 
none(αt = 0.5) 0.76 0.19 

6.5.6 Final strategy selection and classification model analysis 

The aim of this section be to select the final strategy for the FDS, make an analysis 

of the time complexity of the propose solution and understand which feature of the 

dataset be the most informative. In the previous experiment we saw that aggregate a 

feedback classifier (Ft) with a delayed classifier (WDt or EDt ) use (6.3) or (6.4) be often 
the best solution. Overall we found the best performance use the configuration of 

Table 6.7: δ = 15,M = 16 and αt = 0.5. If we have to choose between the slide window 

or ensemble approach we would recommend the second. As show in Figure 6.11(b), the 

ensemble EDt have low training time than the slide window, because the first train 
everyday a modelMt use only transaction from Dt−d. On the contrary, in the slide 
window a modelWDt be built use transaction from {Dt−δ, . . . ,Dt−(δ+M−1)}, henceWDt 
be a train on a much large training set. 

Despite the large difference in the training set size between the ensemble and slide 

window for the delayed classifier (see Figure 6.11(a)), the difference in the training time 

be small (see Figure 6.11(b)), because in WDt and EDt each tree of the Random Forest 
us a balance bootstrap sample of the original training set (see Section 6.3.3). The 

training time of Ft be negligible since it be train on few feedback samples. As a 
consequence, the training time of aggregation AWt and AEt be essentially equivalent to 
the one of the delayed classifier WDt and EDt . 

The RF algorithm can be easily parallelize by distribute the training of each decision 

tree on multiple cores/machines. In this way it be possible to drastically reduce the 

compute time request for training the FDS. In the ideal case of no overhead due to 

parallelization, the theoretical speedup be equivalent to the number of cores/machines 

use for training the RF. 

Louppe [245] derives three bound for the time complexity of RF training procedure: i) 

best O(qmNb log2Nb), ii) bad O(qmN2b logNb) and iii) average O(qmNb log 
2Nb), where 

Nb be the number of boostrap sample in each tree, m the number of tree in the forest 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 137 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

●● ● 
● 

●● 

● 

● 

● 

● 
● ● 

●●● 

● 

●●●●● 
● 

● 

●●●●●●●●●●●● 

● 

●●● 

● 

● 

● 

● 

●● 
●● 
● 
●●● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 
● 

● 

● 

● 

● 

●● 
● 
● 

●● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

●● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

●●● 

● 

● 

● 

● 

● 

●● 
● 

● 

● 
●● 

●●●● 

● 

● 

● 

● 

●●●●●● 
● 
●●●●● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

●●● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

●●● 

● 

●●●●●●●●●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●●●●●● 

● 

● 

●●●●●● 

● 

● 

●●●●●●●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● ● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

●● 
● 

●● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● ● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

●● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 
● 

● 

● 

● 

●● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● ●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● ● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● ● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● ●●● 

●● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 
● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 
● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
●● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 

● ● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
●● ● 

● 

● 

● 

● 

● 

● 

● ● 

0.00 

0.25 

0.50 

0.75 

1.00 

0.00 0.25 0.50 0.75 1.00 
PF(+|x) 

P 
W 

D 
(+ 

|x 
) Class 

● 

● 

+ 
− 

Alert 
● FALSE 

TRUE 

Pk: 0.91 alpha: 0.599 

(a) 6th of October 2013 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

●● 
●● 

●● 
● 

● 

● 

● 

● 

● 
● 

●●● 
● 

● 

●●●● 

● 

●●●●●●● 

● 

●● 

● 

● 

●●●●●●●●●●●●●●●●● 

● 

●● 

● 

●●●●●●●● 

● 

●●●●●● 

● 

●● 
● 

● 

● 

● 

●● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

●● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
●●● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●●● 
● 

● 

● 

●● 

● 

●● 

● 

● 

● 

● 
●●● ● ● ● 

●●● 

●●●● 

● 

●●●●● 

● 

●● 
● 

●● ●●● 

● 

● 

● 

●● 

●●●●●●●●●●●●●●●●●●●●●●● 

● 

● 

● 

● 

● 

● 

●●● 

● 

● 

●●● 

● 

●●●●●●●●●●●●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● ● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 
● 

● 
● 

● ● 
● 

● 

● 
● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 
● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● ●● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● ●● 
● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● ● 
● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 
● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

●● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

0.00 

0.25 

0.50 

0.75 

1.00 

0.00 0.25 0.50 0.75 1.00 
PF(+|x) 

P 
W 

D 
(+ 

|x 
) Class 

● 

● 

+ 
− 

Alert 
● FALSE 

TRUE 

Pk: 0.99 alpha: 0.789 

(b) 7th of October 2013 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

●● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● ● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● ● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 
● 

● 

● 

● 

● 
●● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 
● 

● ● 
● 

● 
● 

● 

● 
● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● ● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 
● 

●● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● ● 

● 

● 
● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 
● 

● 

● 

● 

● 

● 

●● 

● 

●● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● ● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● ● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 
● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 
● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

●● 

● 

● 
● 

● ● 

● 

● 

● 

● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

●●● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● ● ●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● ● 

● 

● 

● 

● 

●● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

●● 

● 

● ● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

0.00 

0.25 

0.50 

0.75 

1.00 

0.00 0.25 0.50 0.75 1.00 
PF(+|x) 

P 
W 

D 
(+ 

|x 
) Class 

● 

● 

+ 
− 

Alert 
● FALSE 

TRUE 

Pk: 0.6 alpha: 0.192 

(c) 8th of October 2013 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● ● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

●●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

● 

● 
● 
● 

● ● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

●● 
● 

● 

● 

● ● 
● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
●● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● ● 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 
● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● ● 

● 

● 
● 

● 

● 

● 

● 

● 

●● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

●● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

●● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● ● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
●● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 
● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● ●● 

● 

● 

● 

● 

● 

● 

●● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● ● 

● 

● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● ● 
● 

● ● 

● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● ● 

● 

● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 
● 

● 

● 
● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 
● 

● 

● ● 

● 

0.00 

0.25 

0.50 

0.75 

1.00 

0.00 0.25 0.50 0.75 1.00 
PF(+|x) 

P 
W 

D 
(+ 

|x 
) Class 

● 

● 

+ 
− 

Alert 
● FALSE 

TRUE 

Pk: 0.86 alpha: 0.554 

(d) 9th of October 2013 

Figure 6.10: Posterior probability PFt(+|x) and PWDt (+|x) for different days. Feed- 
back transaction be denote with triangle and red color be use for frauds. In this 
example, feebacks be request by AWt with αt compute a in (6.7). Operating with 
different value of αt corresponds to cut the plane (PFt ,PWDt ) use straight line 

have a angular coefficient −αt(1−αt) . 

and q the number of feature use to split each tree of the RF (q ≤ n). The best case 
corresponds to the case when sample be always partition at the tree node into two 

balance subset of N2 samples. The bad corresponds to the case of split that create 

a subset with only one sample and the other subset with the remain N − 1 instances. 
The average corresponds to the average time complexity. In all our experiment only the 

term Nb be change between the different classifiers, while m = 100 and q = 7.9 In the 
9In the randomForest package [222], the default value of q be obtain a q = 

√ 
n. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 138 

●●● 

●●●●●●●●●●●●● ●●●●●●●●●●●●●●●0e+00 

1e+06 

2e+06 

3e+06 

delayed feedback 

Classifier 

T 
ra 

in 
in 

g 
se 

t s 
iz 

e 

adaptation 
slide 
ensemble 

(a) Training set size 

●● 
● 
● 

● ●●0 

100 

200 

300 

400 

delayed feedback 

Classifier 

T 
ra 

in 
in 

g 
tim 

e 

adaptation 
slide 
ensemble 

(b) Training time 

Figure 6.11: Training set size and time (in seconds) to train a RF for the feedback 
(Ft) and delayed classifier (WDt and EDt ) in the 2013 dataset. All the simulation be 
run use a single core in order to minimize the compute resource request to the 

university cluster a in Section 5.1.3. 

case of BRF, Nb = 2N+ where N+ be the number of fraud available for training (see 

Section 6.3.3). 

Finally, in Figure 6.12 we plot a measure of feature relevance extract from the BRF 

model of WDt . The most informative feature be RISK_TERM_MIDUID which mea- 
sures the risk associate to a terminal identifier (MIDUID). Within the top 10 most 

informative feature we see variable measure the amount of the transaction (e.g. 

SUM_AMT_HIS) and feature measure the risk associate to the country and con- 

tinent of the terminal (e.g. RISK_TERM_COUNTRY). All variable with the name 

start with RISK be feature that be originally provide a categorical variable 

and that have be convert into numerical one by mean of the transform present 

in Section 5.1.3. 

6.6 Discussion 

Let u now discus the accuracy improvement achieve by classifier AWt and AEt pro- 
pose in Section 6.3.2. First of all, we notice that the classifier learn on recent feedback 

be more accurate that those train on delayed samples. This be make explicit by Ta- 

ble 6.3 show that Ft often outperforms WDt (and EDt ), and Wt (and Et). We deem 
that Ft outperforms WDt (resp. EDt ) since WDt (resp. EDt ) be train on less recent 
supervise couples. As far a the improvement with respect toWt (and Et) be concerned, 
our interpretation be that this be due to the fact thatWt (and Et) be train on the entire 
supervise dataset, then weaken the specific contribution of feedbacks. 

Our result instead show that aggregation prevents the large amount of delayed super- 

vised sample to dominate the small set of immediate feedbacks. This boil down to 

assign large weight to the most recent than to the old samples, which be a golden rule 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 139 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

TX_ACCEPTED 
HAD_REFUSED 

NB_REFUSED_HIS 
TX_INTL 

HAD_TRX_SAME_SHOP 
TX_3D_SECURE 

IS_NIGHT 
HAD_LOT_NB_TX 

RISK_GENDER 
NB_TRX_SAME_SHOP_HIS 

RISK_CARD_BRAND 
RISK_D_AGE 

RISK_LANGUAGE 
HAD_TEST 

RISK_D_AMT 
RISK_TERM_MCC_GROUP 

TX_HOUR 
RISK_BROKER 

RISK_TERM_MCCG 
RISK_LAST_COUNTRY_HIS 

AGE 
RISK_D_SUM_AMT 

RISK_TERM_REGION 
RISK_TERM_MCC 

NB_TRX_HIS 
RISK_TERM_COUNTRY 

AMOUNT 
RISK_TERM_CONTINENT 

MIN_AMT_HIS 
SUM_AMT_HIS 

RISK_LAST_MIDUID_HIS 
RISK_TERM_MIDUID 

0 20 40 60 

Feature importance 

RF model day 20130906 

Figure 6.12: Average feature importance measure by the mean decrease in accuracy 
calculate with the Gini index in the RF model of WDt in the 2013 dataset. The 
randomForest package [222] available in R use the Gini index a splitting criterion. 
As state in the package documentation: “The mean decrease in accuracy measure be 
compute from permute the Out Of Bag (OOB) data: For each tree, the error rate on 
the OOB portion of the data be recorded. Then the same be do after permute each 
predictor variable. The difference between the two be then average over all trees, and 

normalize by the standard deviation of the differences”. 

when learn in non-stationary environments. The aggregation AWt with αt = 0.5 be 
indeed an effective way to attribute high importance to the information include in 

the feedbacks. At the same time AEt with αt = 0.5 be a way to balance the contribution 
of Ft and the remain M model of Et. 

Another motivation of the accuracy improvement with the aggregation be that classifier 

train on feedback and delayed sample address two different classification task (see 

Section 6.3). In particular Ft learns to discriminate those sample that have high risk 
of be frauds, i.e. those transaction yield a large value of the posterior probability 

of the classifier use to generate alerts. On the contrary, classifier train on delayed 

sample rely on heterogeneous sample that be not select depend on the posterior 

of the classifier, and include both fraud and genuine transaction that be not consider 

risky. For this reason too, it be not convenient to pool the two type of supervise 

sample together. Finally, we notice that a constant and equal weight (αt = 0.5) in the 

aggregation AWt and AEt be often perform a well a adaptive weight strategy 
present in Section 6.5.5. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 140 

The interaction between the FDSs (raising alerts) and the investigator (providing true 

labels) recall solution where the classifier interact with an oracle for additional label 

typical of Active Learning [246]. The goal of Active Learning be to minimize the request of 

label data ask to oracles, by determine which be the most informative instance 

to query. Unfortunately, in a FDS, we have a small budget devote to fraud alert 

investigation, i.e. the money that company dedicate to fraud investigation be limited. 

The few investigator available have to focus on the most suspicious transaction with 

the goal of detect the large number of frauds. We cannot demand the investigator 

to check genuine transaction for the sake of obtain informative patterns. Validation 

of transaction with low risk would come at the cost of not control highly risky 

transaction with a consequent impact on the detection accuracy. In the ideal case of 

perfect detection all transaction label by investigator should be of class fraud. 

In our formulation (Section 6.2) we select transaction to alert use the probability of the 

sample to be fraudulent. Alternatively, Baesens et al. [10] recommend generate alert 

for transaction with expect fraud loss (fraud probability× transaction amount) great 
than a certain threshold. Similarly, Fan et al. [126] suggest to alert only transaction 

have the expect fraud loss high than the overhead (cost of review an alert, 

i.e. cost of investigation). In this work we define the alert without consider the 

transaction amount, because we want to give equal importance to fraud of small and 

large amount. If we detect fraud of small amounts, we can block the card and prevent 

large one before they occur, because fraudsters typically try to steal money with small 

amount first and then, if successful, with large ones. 

It be worth to remark that the formulation propose in Section 6.2 be still a simplify 

description of the process regulate company analyze credit card transactions. 

For instance, it be typically not possible to extract the alert At by rank the whole 

set Bt, since transaction have to be immediately pass to investigators; similarly, de- 

layed supervise couple Dt−δ do not come all at once, but be provide over time. 

Notwithstanding, we deem that the most important aspect of the problem (i.e. the 

Alert-Feedback Interaction and the time-varying nature of the stream) be already con- 

tained in our formulation and that further detail would unnecessarily make the problem 

set complex. 

A limitation of the current study be that we report a feedback only those transaction 

generate an alert. However, when the investigator call a cardholder, they typically 

check the status of also previous transaction make by the same card. This mean that 

one alert can generate multiple supervise transaction (|Ft| ≥ |At|). In this setting, 
it be more interest measure alert precision at the card level instead of the transac- 

tion level, i.e. measure how many fraudulent card be detect in the k card that 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 141 

investigator be able to check. Preliminary results, not include in this chapter, seem 

to confirm that, also in term of alert precision measure at the card level, the best 

performance be obtain by aggregate two classifier one train on feedback and 

the other on delayed samples. 

6.7 Conclusion 

In this chapter we formalize a framework that reproduces the work condition of 

real-world FDSs. In a real-world fraud-detection scenario, the only recent supervised- 

information be provide on the basis of the alert generate by the FDS and feedback 

provide by investigators, i.e. Alert-Feedback Interaction (AFI). All the other supervise 

sample be available with a much large delay. 

Our intuition be that: i) AFI have to be explicitly consider in order to improve alert 

precision and ii) feedback and delayed sample have to be separately handle when 

training a realistic FDS. To this purpose, we have consider two general approach for 

fraud detection: a slide window and an ensemble of classifiers. We have then compare 

FDSs that separately learn on feedback and delayed sample against FDSs that pool all 

the available supervise information together. Experiments run on real-world stream of 

transaction show that the former strategy provide much more precise alert than the 

latter, and that it also adapts more promptly in concept-drifting environments. 

The majority of the work present in the literature (e.g. [18, 61]) assume that we have 

the label all transaction (ignoring AFI) and use AUC a accuracy measure. We claim 

that in a real world scenario, the main goal of a FDS be to return accurate alerts, i.e. high 

Pk (precision within the k transaction that we report a alert to the investigators). In 

this chapter we have show that in order to get precise alert (high Pk) it be mandatory 

to give large importance to feedback samples. Strategies lower their influence in 

the learn process (e.g. SSB correction technique and Wt or Et classifiers) be often 
return less precise alert (lower Pk). 

The feedback classifier Ft provide accurate rank of the most suspicious transaction 
(high Pk), but it be not the best option when the goal be to achieve a good rank of all 

transaction (low AUC). Classifier train on everyday transaction (e.g. WDt , EDt , Rt) 
have low Pk, but return a good global rank (higher AUC). By increase δ, the 

number of day in which we receive feedbacks, Ft have good performance in term of 
Pk. When Ft have high Pk also it aggregation AWt and AEt have high accuracy. 

Adaptation technique be important in the presence of CD, this be make clear by the 

poor performance of the static classifier St which use the same model along the stream. 



Chapter 6. Concept-Drift Adaptation with Alert-Feedback Interaction 142 

Aggregating Ft and WDt (or EDt ) with AWt (or AEt ) be often the best solution to achieve 
high Pk even in the presence of abrupt CD (see result from Section 6.5.2). We have 

also test several method to compute the weight αt use in the aggregations, but it 

appear that a simple average (αt = 0.5) be often the best solution. In the future work 

we want to study non-linear aggregation of classifier use in AWt and AEt , perhaps 
good performance can be achieve by use a rank algorithm receive a input the 

posterior probability and the class of the transaction. 

Currently, unsupervised (non-feedback) transaction between t and t− δ be not use in 
the learn process. We think that be worth test semi-supervised learn algorithm 

to exploit both supervise (feedbacks) and unsupervised sample occur between t 

and t − δ. Another direction worth investigate be to use more than k transaction for 
training Ft if the rank be accurate (when Pk ≈ 1). When Ft have high Pk we expect 
that the rank produce by the posterior probability PFt be accurate not only in the 
top k transactions, but also let’s say in the top k+γ. In this case we could train Ft use 
k+ γ samples, where transaction ranked between k and k+ γ be label a fraudulent. 

Finally, we can say that the result present in this chapter be in line with the per- 

formance of our industrial partner (sometimes even better). However, for confidentiality 

reason, we be not allow to disclose figure regard the performance of our partner. 

Appendix B present the software module of the propose FDS. 



Chapter 7 

Conclusions and Future Perspectives 

Fraud detection be a particularly challenge and complex task. Fraudulent activity 

be rare event that be hard to model and in constant evolution. The large volume of 

transaction happen everyday demand automatic tool to support investigation, and 

the human resource devote to investigation have to concentrate on the most suspicious 

cases. This thesis investigate how machine learn algorithm could be use to address 

some of these issues. In particular, we focus on the design of a framework that be able 

to report the riskiest transaction to investigator by mean of algorithm that can deal 

with unbalanced and evolve data streams. This chapter summarizes the main result 

of the thesis, discus open issue and present future research directions. 

7.1 Summary of contribution 

A standard solution to deal with classification problem have unbalanced class dis- 

tribution (like fraud detection) be to rebalance the class before training a model. A 

popular rebalancing technique in the machine learn community be undersampling. In 

Chapter 4 we show the condition under which undersampling be expect to improve 

the rank of fraudulent and genuine transaction give by the posterior probability of 

a classifier. We also study the effect of class-separability on probability calibration and 

how to set a threshold to make predictions. It emerges that, without control on the data 

distribution, it be not possible to know beforehand whether undersampling be beneficial. 

For this reason in Section 4.3 we propose a race algorithm to select rapidly the best 

sample technique when multiple solution be available. The race algorithm be 

make available open source a a software package for the R language [25] (see package 

unbalanced [24] present in Appendix A). 

143 



Chapter 7. Conclusions and Future Perspectives 144 

In order to deal with the non-stationarity distribution of credit card transactions, Chap- 

ter 5 investigates multiple strategy for concept drift adaptation in the presence of skewed 

distribution. We notice that update regularly the FDS be a much good alternative 

than use the same model over all the year. When choose the sample to train a 

classifier it be important to retain historical transaction a well a forget outdated sam- 

ples for the model to be precise. Also, resampling method (notably undersampling) 

significantly improve the performance of a FDS. Propagation of fraudulent transaction 

along the stream be another effective way to rebalance the class distribution. However, 

the latter solution lead to computational overhead and it can be avoid without loss 

of accuracy by adopt a FDS base on Hellinger Distance Decision Tree. 

Typically, fraud alert generate by a FDS be checked by human investigator that 

annotate alert transaction a genuine or fraudulent. Feedbacks from investigator 

provide recent supervise sample that be highly informative. In Chapter 6 we present 

a prototype of a FDS that be able to include investigators’ feedback in the learn 

process. We show that, for the FDS to produce accurate alerts, feedback have to receive 

large weight than the other supervise sample available. Combining two classifiers, 

one train on feedback and one train on delayed samples, be often the best way to 

provide accurate alert in the presence of concept drift. 

With this thesis we also make available to the machine learn community a dataset 

contain observation of credit card transactions. This dataset have be use in [28] 

and it include example of fraudulent samples, information that be rarely available.1 

7.2 Learned lesson 

In the follow we summarize what we have learn during this PhD project with the 

intent of provide the reader and practitioner with some take home messages: 

• As show in Chapter 4, rebalancing a training set with undersampling be not guar- 
anteed to improve performances, several factor influence the final effectiveness 

of undersampling and most of them cannot be control (e.g. the variance of a 

classifier and the sample where the condition (4.20) be satisfied). 

• The optimal degree of sample depends on the datasets considered. In Section 4.1 
we show that the right amount of sample with undersampling (defined by β) be 

dataset specific. When sample be too aggressive we have less point for which 

undersampling be beneficial. Characteristics of the classification task, such a class 
1Dataset available at http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata. 

http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata 


Chapter 7. Conclusions and Future Perspectives 145 

separability and imbalance ratio, also influence the performance of the classifier 

and the effectiveness of sample methods. 

• The best technique for unbalanced classification do not exist and the best for 
a give dataset can vary with the algorithm and accuracy measure adopt (see 

Section 4.3). Adaptive selection strategy (e.g. F-Race [26]) can be effective to 

rapidly provide an answer on which technique to use. 

• The data stream define by credit card transaction have non-stationary distri- 
butions, i.e. change in fraudulent and genuine behavior severely affect the perfor- 

mances of a FDS. This be make clear by the poor performance of static approach 

a show in Section 5.1.3. Updating the FDS be often a good idea and retain 

historical transaction can improve predictive accuracy. 

• For a real-world FDS it be mandatory to produce precise alerts, i.e. provide an 
accurate rank of transaction with the high risk of be fraudulent. As 

explain in Section 6.2, investigator trust and follow the alert of a Data Driven 

Model a long a it do not generate too many false alerts. Poor alert precision 

(low Pk) mean also few recent fraudulent transaction and the FDS have to relay 

only on old fraudulent pattern for the detection. 

• The best learn algorithm for a FDS depends on the accuracy measure that we 
want to maximize. As show in Chapter 6, a classifier train only on feedback 

sample (Ft) can provide more precise alert (higher Pk) than one train on all 
recent transaction (Rt). However, the latter becomes a much good choice when 
use standard classification measure such a AUC. 

7.3 Open issue 

In this section we want to discus some open issue in fraud detection that we believe 

be worth investigating, such as: i) define a good performance measure, ii) model 

Alert-Feedback Interaction and iii) use the supervise and unsupervised information 

available with AFI. 

Despite everybody agree on the fact that miss a fraud be much bad than generat- 

ing a false alert, there be no agreement on which be the best way to measure fraud detection 

performance. Indeed, the machine learn community have propose several cost mea- 

sures, some be transaction-dependent [35, 70, 71], others be class-dependent [2, 247]. 

Other work avoid use cost-based accuracy measure by make the implicit assumption 

that it be more important to provide correct prediction [18, 64]. 



Chapter 7. Conclusions and Future Perspectives 146 

We think that there be not a correct and wrong way to measure detection performances, 

company have different idea about what be the best figure of merit. However, if a 

cost-based measure be prefer then we recommend use normalize cost [63] instead of 

saving [70], because the second can be negative which be counter intuitive.2 Alternatively 

one should use a benefit matrix a propose by Elkan [35]. When use standard clas- 

sification metrics, we suggest AUC estimation base on the Mann-Whitney (Wilcoxon) 

statistic [18] (see Section 2.2.3). In this thesis we deem that, from an investigator per- 

spective, the most relevant measure be the alert precision, denote a Pk, i.e. precision 

within the k most risky transaction [20]. 

Chapter 6 have propose a framework that be able to exploit investigators’ feedback to 

improve detection performance. In our formulation, feedback be available at the end 

of the day altogether, while in reality, the feedback mechanism be much more complex. 

Feedbacks be provide during the day a soon a investigator call the cardholder. More- 

over, investigator check all previous transaction of one card once it be found to be victim 

of fraud. This mean that we could have more the one feedback transaction per alert 

generate and the model train on feedback should learn online, i.e. a soon a feed- 

back be received. Additionally, feedback can be provide with a delay of more than 24 

hour and historical transaction could receive their label after months. Despite these 

practical constraint we deem that the framework propose in Chapter 6 be able to meet 

essential work condition of a real-world FDS present in Section 2.2.1. 

Improvements of the FDS present in Chapter 6 could come from use all available 

transactions, not only supervise samples. For example, semi-supervised strategy could 

exploit also unlabeled data (non-feedback transaction in the first δ days) to improve the 

detection. Alternatively, even if investigator require the FDS to provide accurate alerts, 

we could use few alert (e.g. 5%) to query unrisky, but interest sample for the sake 

of obtain more precise alerts. This last option would allow explore a small part of 

unsupervised sample a in active learning. 

7.4 The Future: go towards Big Data solution 

In this section we will discus future research direction that in the meantime lead to the 

definition of a new research project call BruFence sponsor by Innoviris.3 BruFence 

be a three year joint project between three research group of two university and three 

company base in Brussels, Belgium. The partner of the project are: Machine Learning 
2In some cases, give the small number of frauds, the cost of predict all transaction a genuine 

can be low than the cost register by a classification algorithm, lead to negative savings. 
3Innoviris be the Brussels institute for the encouragement of scientific research and innovation. 



Chapter 7. Conclusions and Future Perspectives 147 

Group from Université Libre de Bruxelles, Machine Learning Group from Université 

Catholique de Louvain, QualSec from Université Libre de Bruxelles, Worldline S.A., 

Steria and NViso. 

The goal of the project be the design of mechanism base on machine learn and 

big data mining technique that allow to automatically detect attack and fraudulent 

behavior in large amount of transactions. BruFence aim at develop a real-time 

framework that be able to compare in parallel a large number of alternative model 

in term of nature (expert-based or data driven), feature (e.g. history or customer 

related), data (e.g. supervise or unsupervised), predictive method (e.g. decision trees, 

neural networks), scalability (e.g. conventional versus Map/Reduce implementation) 

and quality criterion (e.g. readability vs. accuracy). The framework be expect to be 

scalable with respect to the amount of data and resource available. The project will also 

investigate the exploitation of network data (social networks, communication networks, 

etc.) for fraud, privacy and security purposes. The use of network data be currently a 

highly study field, subject of much recent work in various area of science [10]. 

Currently the IT infrastructure use for the FDS be base on classical data warehouse 

architectures. These architecture be well design for business reporting but not for 

apply analytics to big volume of data. In this context, it be infeasible to do analyt- 

ic directly on the whole historical data set with standard machine learn algorithms. 

Though an easy solution consists in use only a portion of data for training the algo- 

rithms, this have detrimental effect on the result predictive accuracy. 

Companies that want to stay at the cut edge of security technology (like the sponsor 

of this project) be more and more interested to enter the big data paradigm. However, 

though the introduction of big data technology (e.g. Hadoop4, Spark5) in the everyday 

business process be claimed to be straightforward by many vendors, in practice it de- 

mands a major redesign of exist functionalities. This be particularly true for analytics 

and business intelligence applications, where the number of off-the-shelf solution be still 

limited and the require data processing be not trivial. A major goal of the project will 

be to adapt and, when necessary, rewrite machine learn and adaptive functionality 

to make them scalable for huge amount of transactional and log data. In particular 

we will target problem characterize by large amount of noise, large dimensionality, 

non-stationarity and demand a rapid and accurate identification of threaten or 

fraudulent configurations. 

As show by Van Vlasselaer et al. [61], network data be a powerful source of information 

in order to improve the detection of fraudulent behaviors. The rationale be that network 
4http://hadoop.apache.org/ 
5http://spark.apache.org/ 

http://hadoop.apache.org/ 
http://spark.apache.org/ 


Chapter 7. Conclusions and Future Perspectives 148 

connectivity provide information that can improve the accuracy of the prediction model. 

For example, it be well know that fraudulent activity be link to each other. Knowing 

that a merchant be target by many fraudsters may provide useful information on the 

likelihood that a transaction on the same shop be fraudulent. The project will investigate, 

develop, and compare different predictive model in order to determine to which extent 

prediction accuracy can be improve by use internal and external network data. The 

project will also investigate and compare different predictive model (graph-based semi- 

supervise classification [248]) for private information discovery (information not publicly 

available), in order to determine to which extent, hidden information can be infer from 

the network (e.g. age and sex of the cardholder if not available). Eventually, we will 

develop new measure identify the most critical or vulnerable connect node whose 

removal result in splitting the network. This can be do with traffic information or 

without traffic data. 

Credit card fraud detection have traditionally focus on look for factor such a trans- 

action amount, point of sales, location, etc. available inside the organization. As show 

in Section 2.2.2, from these basic variable it be possible to compute new aggregate fea- 

tures to model the behavior of the cardholder. Typically, company use a small sample 

of historical transaction for each cardholder to build account-level variables. Because 

it be computationally demand to compute aggregates, these feature be usually cal- 

culated offline and then add to the feature vector represent the transaction when 

it be authorized. Using a small part of the information available may translate into a 

loss of predictive accuracy. The introduction of big data technology allows overcome 

these issues, i.e. compute aggregate in real time and use a large set of historical 

transactions. 

The Big Data solution that we envisage will be able to process massive amount of 

structure and unstructured data from a hybrid of source a well. This will enable the 

exploitation of both exist in-house and public data (i.e. social media, websites, blogs). 

Models and algorithm will take advantage of these richer source to build more accurate 

detection models. For example, social medium can use to check whether a cardholder be 

travel and validate a transaction from an unusual location. 

The project will deliver an online learn framework that be able to process the data 

stream where the account-level information and the network of transaction be con- 

sidered. The learn process will have to handle the unbalanced nature of the data 

in real time without have to store/recall previous transaction a in Dal Pozzolo et 

al. [19]. The algorithm will be implement use scalable architecture that will allow 

the integration of exist and external source of information. 



Chapter 7. Conclusions and Future Perspectives 149 

7.5 Added value for the company 

In this section we want to discus how the result of Doctiris project (presented in 

Section 1.7) could be valorized by the industrial partner Worldline S.A. 

The FDS in production at Worldline be currently adopt a static approach (see Section 

5.1.2), i.e. a classification model update once/twice a year. The result of Chapter 5 

show to the company that there be a clear performance gain when the model be update 

more frequently, e.g. once a week or every 15 days. The work do in Chapter 5 allows 

Worldline to ass the performance of several learn strategy in the presence of CD 

and to test different CD adaptation methods. 

We also use different classification algorithm not yet explore by Worldline. In par- 

ticular, Random Forest have emerge a the best algorithm in many simulation and now 

the company have it in production. During the project we also investigate new way to 

create aggregate feature in order to include user behavior at the transaction level (see 

the transformation propose in Section 5.1.3). 

The unbalanced problem have never be theoretically study before in Worldline. The 

result of Chapter 4 show that, in case of fraud detection, the performance of a clas- 

sifier can be significantly improve when sample method be use to rebalance the 

two classes. Given the large imbalance ratio and number of transactions, undersam- 

pling should perhaps be favor w.r.t. oversampling techniques. At the same time 

it be important to calibrate the probability in order to provide an accurate rank of 

the transaction after sample the dataset (see Section 4.2). Additionally, the race 

strategy propose in Section 4.3 give Worldline a new tool for select efficiently the 

unbalanced method that best fit the data. 

Finally, in Chapter 6 we provide evidence that alert transaction can be very infor- 

mative for obtain accurate FDS. Currently, the FDS in production at Worldline be 

not able to trace if an historical transaction have be checked in a fraud investigation or 

not. In this setting, it be not possible to distinguish between feedback and non-feedback 

samples. As show in Section 6.5, high performance can be achieve by combine 

classifier separately train on feedback and delayed transactions. 

7.6 Concluding remark 

The Doctiris PhD project be an unique opportunity to work on real-world fraud detec- 

tion data that, because of it high sensitivity, be scarcely available. Moreover, this type of 

data be very interest because it combine several challenge such a class overlap, class 



Chapter 7. Conclusions and Future Perspectives 150 

imbalance and mislabeled sample among others. The collaboration with the industrial 

partner be particularly fruitful because in the company we have a supervisor, Dr. Olivier 

Caelen, who carefully guide our work. 

As show in Chapter 6, in real work condition we have constraint that define new 

challenges. For example, the limited number of transaction verify by investigator 

allows only few recent supervise samples, while in the literature most work assume 

to know the label of all transactions. Also, the figure of merit that be interest for a 

company may differ from standard accuracy measures. 

Typically, company be interested in more practical than theoretical results, e.g. “as 

long a the algorithm work well there be no need to question it design or implementa- 

tion”. On the contrary, the academic world be sometimes address complex problem 

that have few practical applications. We believe that both worlds, industry and univer- 

sity, should look at each other and exchange idea in order to have a much large impact 

on society. For all these reasons, we hope that the Doctiris initiative will be follow 

by many others and will pave the way of more collaboration between company and 

university in the Brussels region. 



Appendix A 

The unbalanced package 

This appendix present a new software package call unbalanced [24] available for 

the R language [25]. It implement some technique for unbalanced classification task 

present in Section 3.1.1 and provide a race strategy [227] to adaptively select the 

best method for a give dataset, classification algorithm and accuracy measure adopted. 

A.1 Methods for unbalanced classification 

The unbalanced package include some of the most well-known sample and distance- 

base method for unbalanced classification task. Within the family of sample meth- 

ods, we have function for random undersampling (ubUnder) and oversampling (ubOver) 

[91]. The package contains also a function call ubSMOTE that implement SMOTE [92]. 

Other distance-based method available in unbalanced be OSS [100] (ubOSS ), CNN [98] 

(ubCNN ), ENN [101] (ubENN ), NCL [89] (ubNCL) and Tomek Link [96] (ubTomek). All 

these method can be call by a wrapper function ubBalance that allows test all these 

strategy by simpling change the argument type. 

The package include the ubIonosphere datasets, which be a modification of the Ionosphere 

dataset contain in mlbench package. It have only numerical input variables, i.e. the first 

two variable be removed. The Class variable, originally take value bad and good, have 

be transform into a factor where 1 denotes the minority (bad) and 0 the majority 

class (good). This variable be our target and it be in the last column of the dataset. In the 

follow we will also call the minority class a positive and the majority a negative. 

For example, let’s apply oversampling to the Ionosphere dataset to have a balance 

dataset. 

151 



Appendix A. The unbalanced package 152 

library(unbalanced) 

data(ubIonosphere) 

n <- ncol(ubIonosphere) 

output <- ubIonosphere[ ,n] 

input <- ubIonosphere[ ,-n] 

set.seed(1234) 

# 1-option use ubOver function 

data <- ubOver(X=input, Y=output, k=0) 

# 2-option use ubBalance function 

#data <- ubBalance(X=input, Y=output, type="ubOver", k=0) 

#oversampled dataset 

overData <- data.frame(data$X, Class=data$Y) 

#check the frequency of the target variable after oversampling 

summary(overData$Class) 

## 0 1 

## 225 225 

In this case we replicate the minority class until we have a many positive a negative 

instances. Alternativelly, we can balance the dataset use undersampling (i.e. remove 

observation from the majority class): 

# use ubUnder function 

data <- ubUnder(X=input, Y=output, perc=50, method="percPos") 

#undersampled dataset 

underData <- data.frame(data$X, Class=data$Y) 

#check the frequency of the target variable after oversampling 

summary(underData$Class) 

## 0 1 

## 126 126 

Another well-know method for unbalanced distribution be SMOTE, which oversample the 

minority class by create new synthetic observations. Let’s compare the performance 



Appendix A. The unbalanced package 153 

of two randomForest classifiers, one train on the original unbalanced dataset and a 

second train on a dataset obtain after apply SMOTE. 

set.seed(1234) 

#keep half for training and half for test 

N <- nrow(ubIonosphere) 

N.tr <- floor(0.5*N) 

X.tr <- input[1:N.tr, ] 

Y.tr <- output[1:N.tr] 

X.ts <- input[(N.tr+1):N, ] 

Y.ts <- output[(N.tr+1):N] 

#use the original unbalanced training set to build a model 

unbalTrain <- data.frame(X.tr, Class=Y.tr) 

library(randomForest) 

model1 <- randomForest(Class ~ ., unbalTrain) 

#predict on the test set 

preds <- predict(model1, X.ts, type="class") 

confusionMatrix1 <- table(prediction=preds, actual=Y.ts) 

print(confusionMatrix1) 

## actual 

## prediction 0 1 

## 0 131 2 

## 1 6 37 

#rebalance the training set before building a model 

#balanced <- ubBalance(X=X.tr, Y=Y.tr, type="ubOver", k=0) 

balance <- ubBalance(X=X.tr, Y=Y.tr, type="ubSMOTE", percOver=250) 

balTrain <- data.frame(balanced$X, Class=balanced$Y) 

#use the balance training set 

model2 <- randomForest(Class ~ ., balTrain) 

#predict on the test set 

preds <- predict(model2, X.ts, type="class") 

confusionMatrix2 <- table(prediction=preds, actual=Y.ts) 

print(confusionMatrix2) 



Appendix A. The unbalanced package 154 

## actual 

## prediction 0 1 

## 0 128 0 

## 1 9 39 

#we can now correctly classify more minority class instance 

Using SMOTE we alter the original class distribution and we be able to increase the 

number of minority instance correctly classified. After smoting the dataset we have no 

false negatives, but a large number of false positives. In unbalanced classification, it 

often desire to correctly classify all minority instance (reducing the number of false 

negatives), because the cost of miss a positive instance (a false negative) be much 

high than the cost of miss a negative instance (a false positive). 

A.2 Racing for strategy selection 

The variety of approach available in the unbalanced package allows the user to test 

multiple unbalanced methods. In a real situation where we have no prior information 

about the data distribution, it be difficult to decide which unbalanced strategy to use. 

In this case test all alternative be not an option either because of the associate 

computational cost. 

As show in Section 4.3, a possible solution come from the adoption of the Racing 

algorithm. Racing be available in unbalanced with the ubRacing function and it im- 

plementation be a modification of the race function available in the race package. The 

function ubRacing compare the 8 unbalanced method (ubUnder, ubOver, ubSMOTE, 

ubOSS, ubCNN, ubENN, ubNCL, ubTomek) against the unbalanced distribution, so we 

have 9 candidate start the race. In the follow we will use a highly unbalanced 

dataset contain credit card transaction use in [28]. 

set.seed(1234) 

# load the dataset 

load(url("http://www.ulb.ac.be/di/map/adalpozz/data/creditcard.Rdata")) 

#configuration of the sample method use in the race 

ubConf <- list(type="ubUnder", percOver=200, percUnder=200, 

k=2, perc=50, method="percPos", w=NULL) 



Appendix A. The unbalanced package 155 

# Race with 10 tree in the Random Forest to speed up result 

result <- ubRacing(Class ~., creditcard, "randomForest", positive=1, 

metric="auc", ubConf=ubConf, ntree=10) 

## Racing for unbalanced method selection in 10 fold CV 

## Number of candidates...........................................9 

## Max number of fold in the CV.................................10 

## Max number of experiments....................................100 

## Statistical test...................................Friedman test 

## 

## Markers: 

## x No test be performed. 

## - The test be perform and 

## some candidate be discarded. 

## = The test be perform but 

## no candidate be discarded. 

## 

## +-+-----------+-----------+-----------+-----------+-----------+ 

## | | Fold| Alive| Best| Mean best| Exp so far| 

## +-+-----------+-----------+-----------+-----------+-----------+ 

## |x| 1| 9| 4| 0.9541| 9| 

## |=| 2| 9| 4| 0.954| 18| 

## |-| 3| 2| 4| 0.9591| 27| 

## |=| 4| 2| 4| 0.963| 29| 

## |=| 5| 2| 4| 0.9651| 31| 

## |-| 6| 1| 4| 0.9646| 33| 

## +-+-----------+-----------+-----------+-----------+-----------+ 

## Selected candidate: ubSMOTE metric: auc mean value: 0.9646 

The best method accord to the F-race be SMOTE. Please note that it be possible 

to change the type of statistical test use to remove candidate in the race with the 

argument stat.test. When we set stat.test = "no", no statistical test be perform and 

the race terminates when all the fold of the cross validation be explored. 



Appendix A. The unbalanced package 156 

A.3 Summary 

With the unbalanced package we have make available some of the most well-known 

method for unbalanced distribution. All these method can be call from ubBalance 

that be a wrapper to the method-specific functions. Depending on the type of dataset, 

classification algorithm and accuracy measure adopted, we may have different strategy 

that return the best accuracy. 

This consideration have lead u to adopt the race strategy where different candidate 

(unbalanced methods) be test simultaneously. This algorithm be implement in the 

ubRacing function which selects the best candidate without have to explore the whole 

dataset. 



Appendix B 

FDS software module 

This appendix present the software module of the FDS prototype present in Chap- 

ter 6. The software be divide in three main module (see Figure B.1): i) Model training, 

ii) Scoring and iii) Review. The follow section present the role of each module. These 

module be all implement in the R language [25]. 

Train_Model, 

Transac'ons) 
stream) 

Predic/on, Fraud)Score) CMT) 

Inves'gators)Feedbacks) 

Delayed) 
transac'ons) 

Model)training) Scoring) 

Review) 

Figure B.1: Software module of the FDS prototype present in Chapter 6. 

B.1 Model training 

This module be use to train predictive model that be capable of estimate the prob- 

ability of a transaction to be fraudulent. In particular, everyday it learns two models, 

one on delayed transaction Dt−δ and one on feedback Ft (see Section 6.3.2). The mod- 

ule implement two standard CD adaptation technique (see Section 6.3.1): i) a slide 

window classifier and ii) an ensemble of classifiers. Regardless of the CD adaptation 

strategy adopted, the module run at midnight so that the result predictive model 

(Ft and WDt or EDt ) be use to score transaction occur the next day (t + 1). The 
main function of this module be Train_Model which receives in input some supervise 

157 



Appendix B. FDS software module 158 

transaction (feedbacks or delayed samples) and return a predictive model built use 

the randomForest [222] package a explain in Section 6.3.3. 

B.2 Scoring 

In the score module all transaction arrive at day t+ 1 be score by the Prediction 

function. In the Prediction function, each transaction be first score by both the feedback 

classifier (with PFt(+|x)) and the delayed classifier (with PWDt (+|x) or PEDt (+|x)). Then, 
the final fraud score be compute use (6.3) or (6.4) depend on the CD adaptation 

strategy use in module B.1. This module be the most critical because transaction arrive 

a a continuous stream and have to be score in Near Real Time (see Section 2.2.1), i.e. 

the Prediction function receives in input transaction that have be authorize and have 

to score them within a little time span. Note that the feature vector of transaction 

enter this module contains already the aggregate feature present in Section 2.2.2. 

Aggregated feature be compute offline in a distinct module. 

B.3 Review 

In this module, transaction be make available to the investigator in a dashboard 

call Case Management Tool (CMT), where for each transaction they can see the fraud 

score return by the Prediction function, and the feature vector contain the original 

variable such a CARD_ID, the shop, currency, and also the aggregate variable. In 

the CMT transaction be sort accord to their fraud score so that investigator can 

review the most suspicious one at the time they access the CMT. After verification a 

transaction be flag a checked and it becomes a feedback. Transactions that be not 

review by the investigator remain unlabeled for δ day and then, if not report a 

fraudulent by the cardholders, be label a genuine. Alter δ days, transaction that 

have not be checked be use a delayed supervise sample in module B.1. 



Appendix C 

Bias and Variance of an estimator 

This appendix present two measure to ass the quality of an estimator, namely Bias 

and Variance. Then we show a Bias–Variance decomposition for the mean square error. 

Definition 1 (Bias of an estimator). An estimator θ̂ of θ have bias 

Bias[θ̂] = E[θ̂]− θ 

In particular, θ̂ be say to be unbiased if E[θ̂] = θ and bias otherwise. 

Definition 2 (Variance of an estimator). The variance of an estimator θ̂ be the variance 

of it sample distribution 

V ar[θ̂] = E[(θ̂ − E[θ̂])2] 

For any random variable x we can write 

V ar[x] = E[x2]− E[x]2 ⇐⇒ E[x2] = V ar[x] + E[x]2 (C.1) 

Since θ be deterministic E[θ] = θ and V ar[θ] = 0. Therefore, use (C.1) we can decom- 

pose the Mean Squared Error (MSE) in term of Bias and Variance of θ̂: 

MSE = E 
[ 
(θ − θ̂)2 

] 
= E[θ2 + θ̂ 

2 − 2θθ̂] (C.2) 
= E[θ2] + E[θ̂ 

2 
]− E[2θθ̂] (C.3) 

= V ar[θ] + E[θ]2 + V ar[θ̂] + E[θ̂]2 − 2θE[θ̂] (C.4) 
= 0 + V ar[θ̂] + (θ − E[θ̂])2 (C.5) 
= V ar[θ̂] + E[θ − θ̂]2 (C.6) 
= V ar[θ̂] +Bias[θ̂]2 (C.7) 

159 





Bibliography 

[1] D.J. Newman A. Asuncion. UCI machine learn repository, 2007. URL http: 

//archive.ics.uci.edu/ml/. 

[2] R.J. Bolton and D.J. Hand. Statistical fraud detection: A review. Statistical 

Science, page 235–249, 2002. 

[3] Piotr Juszczak, Niall M Adams, David J Hand, Christopher Whitrow, and David J 

Weston. Off-the-peg and bespoke classifier for fraud detection. Computational 

Statistics & Data Analysis, 52(9):4521–4532, 2008. 

[4] Sam Maes, Karl Tuyls, Bram Vanschoenwinkel, and Bernard Manderick. Credit 

card fraud detection use bayesian and neural networks. In Proceedings of the 1st 

international naiso congress on neuro fuzzy technologies, 2002. 

[5] Jon TS Quah and M Sriganesh. Real-time credit card fraud detection use com- 

putational intelligence. Expert Systems with Applications, 35(4):1721–1732, 2008. 

[6] Tej Paul Bhatla, Vikram Prabhu, and Amit Dua. Understanding credit card frauds. 

Cards business review, 1(6), 2003. 

[7] Christopher M Bishop et al. Pattern recognition and machine learning, volume 4. 

springer New York, 2006. 

[8] Linda Delamaire, HAH Abdou, and John Pointon. Credit card fraud and detection 

techniques: a review. Banks and Bank Systems, 4(2):57–68, 2009. 

[9] Raymond Anderson. The Credit Scoring Toolkit: Theory and Practice for Retail 

Credit Risk Management and Decision Automation. Oxford University Press, 2007. 

[10] Bart Baesens, Veronique Van Vlasselaer, and Wouter Verbeke. Fraud Analytics 

Using Descriptive, Predictive, and Social Network Techniques: A Guide to Data 

Science for Fraud Detection. John Wiley & Sons, 2015. 

[11] Richard J Bolton and David J Hand. Unsupervised profile method for fraud 

detection. Credit Scoring and Credit Control VII, page 235–255, 2001. 

161 

http://archive.ics.uci.edu/ml/ 
http://archive.ics.uci.edu/ml/ 


Bibliography 162 

[12] The nilson report, August 2013. URL http://www.nilsonreport.com/. [Issue 

1023 | Aug 2013]. 

[13] Lexis Nexis. True cost of fraud 2014 study. URL http://www.lexisnexis.com/ 

risk/insights/true-cost-fraud.aspx. 

[14] European Central Bank. Report on card fraud available: https://www.ecb. 

europa.eu/press/pr/date/2014/html/pr140225.en.html. 

[15] Cybersource. 2015 uk fraud report series: Part 2, 2015. URL http://www. 

cybersource.com/. [Online; access July-2015]. 

[16] Wikipedia. 3-d secure, 2015. URL http://en.wikipedia.org/wiki/3-D_Secure. 

[Online; access July-2015]. 

[17] Jose M Pavía, Ernesto J Veres-Ferrer, and Gabriel Foix-Escura. Credit card inci- 

dent and control systems. International Journal of Information Management, 32 

(6):501–503, 2012. 

[18] Andrea Dal Pozzolo, Olivier Caelen, Yann-Ael Le Borgne, Serge Waterschoot, and 

Gianluca Bontempi. Learned lesson in credit card fraud detection from a practi- 

tioner perspective. Expert Systems with Applications, 41(10):4915–4928, 2014. 

[19] Andrea Dal Pozzolo, Reid A. Johnson, Olivier Caelen, Serge Waterschoot, Nitesh V 

Chawla, and Gianluca Bontempi. Using hddt to avoid instance propagation in 

unbalanced and evolve data streams. In Neural Networks (IJCNN), 2014 Inter- 

national Joint Conference on, page 588–594. IEEE, 2014. 

[20] Andrea Dal Pozzolo, Giacomo Boracchi, Olivier Caelen, Cesare Alippi, and Gi- 

anluca Bontempi. Credit card fraud detection and concept-drift adaptation with 

delayed supervise information. In Neural Networks (IJCNN), 2015 International 

Joint Conference on. IEEE, 2015. 

[21] Haibo He and Edwardo A Garcia. Learning from imbalanced data. Knowledge and 

Data Engineering, IEEE Transactions on, 21(9):1263–1284, 2009. 

[22] G. Batista, A. Carvalho, and M. Monard. Applying one-sided selection to unbal- 

anced datasets. MICAI 2000: Advances in Artificial Intelligence, page 315–325, 

2000. 

[23] João Gama, Indrė Žliobaitė, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid 

Bouchachia. A survey on concept drift adaptation. ACM Computing Surveys 

(CSUR), 46(4):44, 2014. 

http://www.nilsonreport.com/ 
http://www.lexisnexis.com/risk/insights/true-cost-fraud.aspx 
http://www.lexisnexis.com/risk/insights/true-cost-fraud.aspx 
https://www.ecb.europa.eu/press/pr/date/2014/html/pr140225.en.html 
https://www.ecb.europa.eu/press/pr/date/2014/html/pr140225.en.html 
http://www.cybersource.com/ 
http://www.cybersource.com/ 
http://en.wikipedia.org/wiki/3-D_Secure 


Bibliography 163 

[24] Andrea Dal Pozzolo, Olivier Caelen, and Gianluca Bontempi. unbalanced: Racing 

For Unbalanced Methods Selection., 2015. URL http://CRAN.R-project.org/ 

package=unbalanced. R package version 2.0. 

[25] R Development Core Team. R: A Language and Environment for Statistical Com- 

puting. R Foundation for Statistical Computing, Vienna, Austria, 2011. URL 

http://www.R-project.org/. ISBN 3-900051-07-0. 

[26] M. Birattari, T. Stützle, L. Paquete, and K. Varrentrapp. A race algorithm for 

configure metaheuristics. In Proceedings of the genetic and evolutionary compu- 

tation conference, page 11–18, 2002. 

[27] Andrea Dal Pozzolo, Olivier Caelen, Serge Waterschoot, and Gianluca Bontempi. 

Racing for unbalanced method selection. In Proceedings of the 14th International 

Conference on Intelligent Data Engineering and Automated Learning. IDEAL, 

2013. 

[28] Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson, and Gianluca Bontempi. 

Calibrating probability with undersampling for unbalanced classification. In 2015 

IEEE Symposium on Computational Intelligence and Data Mining. IEEE, 2015. 

[29] Tom M Mitchell. Machine learning. 1997. Burr Ridge, IL: McGraw Hill, 45, 1997. 

[30] Vladimir Naumovich Vapnik and Vlamimir Vapnik. Statistical learn theory, 

volume 1. Wiley New York, 1998. 

[31] Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman, and 

R Tibshirani. The element of statistical learning, volume 2. Springer, 2009. 

[32] Lennart Ljung. System identification: Theory for the user. PTR Prentice Hall 

Information and System Sciences Series, 198, 1987. 

[33] Andrew R Webb. Statistical pattern recognition. John Wiley & Sons, 2003. 

[34] Richard O Duda, Peter E Hart, and David G Stork. Pattern classification. John 

Wiley & Sons, 2012. 

[35] C. Elkan. The foundation of cost-sensitive learning. In International Joint Con- 

ference on Artificial Intelligence, volume 17, page 973–978, 2001. 

[36] Trevor. Hastie, Robert. Tibshirani, and J Jerome H Friedman. The element of 

statistical learning, volume 1. Springer New York, 2001. 

[37] Pedro Domingos. A unified bias-variance decomposition for zero-one and square 

loss. AAAI/IAAI, 2000:564–569, 2000. 

http://CRAN.R-project.org/package=unbalanced 
http://CRAN.R-project.org/package=unbalanced 
http://www.R-project.org/ 


Bibliography 164 

[38] Robert C Holte. Very simple classification rule perform well on most commonly 

use datasets. Machine learning, 11(1):63–90, 1993. 

[39] Pedro Domingos and Michael Pazzani. On the optimality of the simple bayesian 

classifier under zero-one loss. Machine learning, 29(2-3):103–130, 1997. 

[40] Eric Bauer and Ron Kohavi. An empirical comparison of voting classification 

algorithms: Bagging, boosting, and variants. Machine learning, 36(1-2):105–139, 

1999. 

[41] Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996. 

[42] Robert E Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the 

margin: A new explanation for the effectiveness of voting methods. Annals of 

statistics, page 1651–1686, 1998. 

[43] Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001. 

[44] Anders Krogh, Jesper Vedelsby, et al. Neural network ensembles, cross validation, 

and active learning. Advances in neural information processing systems, 7:231–238, 

1995. 

[45] Hongyu Guo and Herna L Viktor. Learning from imbalanced data set with boost- 

ing and data generation: the databoost-im approach. ACM SIGKDD Explorations 

Newsletter, 6(1):30–39, 2004. 

[46] Gary M Weiss. Mining with rarity: a unify framework. ACM SIGKDD Explo- 

ration Newsletter, 6(1):7–19, 2004. 

[47] David L Olson and Dursun Delen. Advanced data mining techniques. Springer 

Science & Business Media, 2008. 

[48] I.H. Witten and E. Frank. Data Mining: Practical machine learn tool and 

techniques. Morgan Kaufmann, 2005. 

[49] Foster Provost. Machine learn from imbalanced data set 101. In Proceedings 

of the AAAI’2000 Workshop on Imbalanced Data Sets, 2000. 

[50] Chao Chen, Andy Liaw, and Leo Breiman. Using random forest to learn imbalanced 

data. University of California, Berkeley, 2004. 

[51] Nitesh V Chawla, David A Cieslak, Lawrence O Hall, and Ajay Joshi. Automati- 

cally counter imbalance and it empirical relationship to cost. Data Mining and 

Knowledge Discovery, 17(2):225–252, 2008. 



Bibliography 165 

[52] Gregory Ditzler, Robi Polikar, and Nitesh Chawla. An incremental learn algo- 

rithm for non-stationary environment and class imbalance. In Pattern Recognition 

(ICPR), 2010 20th International Conference on, page 2997–3000. IEEE, 2010. 

[53] Tom Fawcett. Roc graphs: Notes and practical consideration for researchers. 

Machine learning, 31:1–38, 2004. 

[54] Tom Fawcett. An introduction to roc analysis. Pattern recognition letters, 27(8): 

861–874, 2006. 

[55] Nitesh V Chawla. C4.5 and imbalanced data sets: investigate the effect of sam- 

pling method, probabilistic estimate, and decision tree structure. In Proceedings of 

the ICML, volume 3, 2003. 

[56] Nitesh V Chawla. Data mining for imbalanced datasets: An overview. In Data 

mining and knowledge discovery handbook, page 853–867. Springer, 2005. 

[57] Wei Liu, Sanjay Chawla, David A Cieslak, and Nitesh V Chawla. A robust decision 

tree algorithm for imbalanced data sets. In SDM, volume 10, page 766–777. SIAM, 

2010. 

[58] Jesse Davis and Mark Goadrich. The relationship between precision-recall and roc 

curves. In Proceedings of the 23rd international conference on Machine learning, 

page 233–240. ACM, 2006. 

[59] Ira Cohen and Moises Goldszmidt. Properties and benefit of calibrate classifiers. 

In Knowledge Discovery in Databases: PKDD 2004, page 125–136. Springer, 2004. 

[60] Glenn W Brier. Verification of forecast express in term of probability. Monthly 

weather review, 78(1):1–3, 1950. 

[61] Véronique Van Vlasselaer, Cristian Bravo, Olivier Caelen, Tina Eliassi-Rad, Le- 

man Akoglu, Monique Snoeck, and Bart Baesens. Apate: A novel approach for 

automate credit card transaction fraud detection use network-based extensions. 

Decision Support Systems, 2015. 

[62] M Krivko. A hybrid model for plastic card fraud detection systems. Expert Systems 

with Applications, 37(8):6070–6076, 2010. 

[63] Christopher Whitrow, David J Hand, Piotr Juszczak, D Weston, and Niall M 

Adams. Transaction aggregation a a strategy for credit card fraud detection. 

Data Mining and Knowledge Discovery, 18(1):30–55, 2009. 

[64] Siddhartha Bhattacharyya, Sanjeev Jha, Kurian Tharakunnel, and J Christopher 

Westland. Data mining for credit card fraud: A comparative study. Decision 

Support Systems, 50(3):602–613, 2011. 



Bibliography 166 

[65] Sanjeev Jha, Montserrat Guillen, and J Christopher Westland. Employing trans- 

action aggregation strategy to detect credit card fraud. Expert system with appli- 

cations, 39(16):12650–12657, 2012. 

[66] D.J. Hand. Measuring classifier performance: a coherent alternative to the area 

under the roc curve. Machine learning, 77(1):103–123, 2009. 

[67] Donald Bamber. The area above the ordinal dominance graph and the area below 

the receiver operating characteristic graph. Journal of mathematical psychology, 

12(4):387–415, 1975. 

[68] Yusuf Sahin, Serol Bulkan, and Ekrem Duman. A cost-sensitive decision tree 

approach for fraud detection. Expert Systems with Applications, 40(15):5916–5923, 

2013. 

[69] Nader Mahmoudi and Ekrem Duman. Detecting credit card fraud by modify 

fisher discriminant analysis. Expert Systems with Applications, 42(5):2510–2516, 

2015. 

[70] Alejandro Correa Bahnsen, Djamila Aouada, and Björn Ottersten. Example- 

dependent cost-sensitive decision trees. Expert Systems with Applications, 2015. 

[71] Alejandro Correa Bahnsen, Aleksandar Stojanovic, Djamila Aouada, and Bjorn 

Ottersten. Cost sensitive credit card fraud detection use bayes minimum risk. In 

Machine Learning and Applications (ICMLA), 2013 12th International Conference 

on, volume 1, page 333–338. IEEE, 2013. 

[72] Ekrem Duman and M Hamdi Ozcelik. Detecting credit card fraud by genetic 

algorithm and scatter search. Expert Systems with Applications, 38(10):13057– 

13063, 2011. 

[73] G. Fan and M. Zhu. Detection of rare item with target. Statistics and Its Interface, 

4:11–17, 2011. 

[74] Mu Zhu. Recall, precision and average precision. Department of Statistics and 

Actuarial Science, University of Waterloo, Waterloo, 2, 2004. 

[75] David J Hand and Martin J Crowder. Overcoming selectivity bias in evaluate 

new fraud detection system for revolve credit operations. International Journal 

of Forecasting, 28(1):216–223, 2012. 

[76] Nitesh V Chawla, Nathalie Japkowicz, and Aleksander Kotcz. Editorial: special 

issue on learn from imbalanced data sets. ACM SIGKDD Explorations Newslet- 

ter, 6(1):1–6, 2004. 



Bibliography 167 

[77] B. Zadrozny, J. Langford, and N. Abe. Cost-sensitive learn by cost- 

proportionate example weighting. In Data Mining, ICDM, page 435–442. IEEE, 

2003. 

[78] P. Domingos. Metacost: a general method for make classifier cost-sensitive. 

In Proceedings of the fifth ACM SIGKDD international conference on Knowledge 

discovery and data mining, page 155–164. ACM, 1999. 

[79] Nathalie Japkowicz. Concept-learning in the presence of between-class and within- 

class imbalances. In Advances in Artificial Intelligence, page 67–77. Springer, 

2001. 

[80] Taeho Jo and Nathalie Japkowicz. Class imbalance versus small disjuncts. ACM 

SIGKDD Explorations Newsletter, 6(1):40–49, 2004. 

[81] Gary M Weiss. Learning with rare case and small disjuncts. In ICML, page 

558–565, 1995. 

[82] Nathalie Japkowicz and Shaju Stephen. The class imbalance problem: A systematic 

study. Intelligent data analysis, 6(5):429–449, 2002. 

[83] Nathalie Japkowicz. Class imbalances: be we focus on the right issue. In 

Workshop on Learning from Imbalanced Data Sets II, volume 1723, page 63, 2003. 

[84] Ronaldo C Prati, Gustavo EAPA Batista, and Maria Carolina Monard. Class 

imbalance versus class overlapping: an analysis of a learn system behavior. In 

MICAI 2004: Advances in Artificial Intelligence, page 312–321. Springer, 2004. 

[85] J. Ross Quinlan. Induction of decision trees. Machine learning, 1(1):81–106, 1986. 

[86] Robert C Holte, Liane E Acker, and Bruce W Porter. Concept learn and the 

problem of small disjuncts. In Proceedings of the Eleventh International Joint 

Conference on Artificial Intelligence, volume 1, 1989. 

[87] Haibo He and Yunqian Ma. Imbalanced learning: foundations, algorithms, and 

applications. John Wiley & Sons, 2013. 

[88] Gary M Weiss and Foster Provost. The effect of class distribution on classifier 

learning: an empirical study. Rutgers Univ, 2001. 

[89] J. Laurikkala. Improving identification of difficult small class by balance class 

distribution. Artificial Intelligence in Medicine, page 63–66, 2001. 

[90] Andrew Estabrooks, Taeho Jo, and Nathalie Japkowicz. A multiple resampling 

method for learn from imbalanced data sets. Computational Intelligence, 20(1): 

18–36, 2004. 



Bibliography 168 

[91] C. Drummond and R.C. Holte. C4.5, class imbalance, and cost sensitivity: why 

under-sampling beat over-sampling. In Workshop on Learning from Imbalanced 

Datasets II, 2003. 

[92] NV Chawla, KW Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. Smote: 

synthetic minority over-sampling technique. Journal of Artificial Intelligence Re- 

search (JAIR), 16:321–357, 2002. 

[93] BX Wang and N Japkowicz. Imbalanced data set learn with synthetic samples. 

In Proc. IRIS Machine Learning Workshop, page 19, 2004. 

[94] Hui Han, Wen-Yuan Wang, and Bing-Huan Mao. Borderline-smote: a new over- 

sample method in imbalanced data set learning. In Advances in intelligent 

computing, page 878–887. Springer, 2005. 

[95] Haibo He, Yang Bai, Edwardo Garcia, Shutao Li, et al. Adasyn: Adaptive synthetic 

sample approach for imbalanced learning. In Neural Networks, 2008. IJCNN 

2008.(IEEE World Congress on Computational Intelligence). IEEE International 

Joint Conference on, page 1322–1328. IEEE, 2008. 

[96] I. Tomek. Two modification of cnn. IEEE Trans. Syst. Man Cybern., 6:769–772, 

1976. 

[97] S. Suman, K. Laddhad, and U. Deshmukh. Methods for handle highly skewed 

datasets. Part I-October, 3, 2005. 

[98] P. E. Hart. The condense near neighbor rule. IEEE Transactions on Informa- 

tion Theory, 1968. 

[99] D.R. Wilson and T.R. Martinez. Reduction technique for instance-based learn 

algorithms. Machine learning, 38(3):257–286, 2000. 

[100] Miroslav Kubat, Stan Matwin, et al. Addressing the curse of imbalanced training 

sets: one-sided selection. In ICML, volume 97, page 179–186. Nashville, USA, 

1997. 

[101] D.L. Wilson. Asymptotic property of near neighbor rule use edit data. 

Systems, Man and Cybernetics, (3):408–421, 1972. 

[102] J.R. Quinlan. C4.5: program for machine learning, volume 1. Morgan kaufmann, 

1993. 

[103] David A Cieslak and Nitesh V Chawla. Learning decision tree for unbalanced 

data. In Machine Learning and Knowledge Discovery in Databases, page 241–256. 

Springer, 2008. 



Bibliography 169 

[104] Sofia Visa and Anca Ralescu. Issues in mining imbalanced data sets-a review 

paper. In Proceedings of the sixteen midwest artificial intelligence and cognitive 

science conference, page 67–73. sn, 2005. 

[105] Inderjeet Mani and I Zhang. knn approach to unbalanced data distributions: a case 

study involve information extraction. In Proceedings of Workshop on Learning 

from Imbalanced Datasets, 2003. 

[106] Gustavo EAPA Batista, Ronaldo C Prati, and Maria Carolina Monard. A study 

of the behavior of several method for balance machine learn training data. 

ACM SIGKDD Explorations Newsletter, 6(1):20–29, 2004. 

[107] Rong Yan, Yan Liu, Rong Jin, and Alex Hauptmann. On predict rare class 

with svm ensemble in scene classification. In Acoustics, Speech, and Signal Pro- 

cessing, 2003. Proceedings.(ICASSP’03). 2003 IEEE International Conference on, 

volume 3, page III–21. IEEE, 2003. 

[108] Gang Wu and Edward Y Chang. Class-boundary alignment for imbalanced dataset 

learning. In ICML 2003 workshop on learn from imbalanced data set II, Wash- 

ington, DC, page 49–56, 2003. 

[109] Jérôme Callut and Pierre Dupont. F β support vector machines. In Neural Net- 

works, 2005. IJCNN’05. Proceedings. 2005 IEEE International Joint Conference 

on, volume 3, page 1443–1448. IEEE, 2005. 

[110] Xuchun Li, Lei Wang, and Eric Sung. Adaboost with svm-based component clas- 

sifiers. Engineering Applications of Artificial Intelligence, 21(5):785–795, 2008. 

[111] Wei Liu and Sanjay Chawla. Class confidence weight knn algorithm for im- 

balance data sets. In Advances in Knowledge Discovery and Data Mining, page 

345–356. Springer, 2011. 

[112] Bing Liu, Wynne Hsu, and Yiming Ma. Mining association rule with multiple min- 

imum supports. In Proceedings of the fifth ACM SIGKDD international conference 

on Knowledge discovery and data mining, page 337–341. ACM, 1999. 

[113] Florian Verhein and Sanjay Chawla. Using significant, positively associate and 

relatively class correlate rule for associative classification of imbalanced datasets. 

In Data Mining, 2007. ICDM 2007. Seventh IEEE International Conference on, 

page 679–684. IEEE, 2007. 

[114] Gary M Weiss. Foundations of imbalanced learning. H. He, & Y. Ma, Imbalanced 

Learning: Foundations, Algorithms, and Applications, page 13–41, 2013. 



Bibliography 170 

[115] Yoav Freund, Robert E Schapire, et al. Experiments with a new boost algorithm. 

In ICML, volume 96, page 148–156, 1996. 

[116] X.Y. Liu, J. Wu, and Z.H. Zhou. Exploratory undersampling for class-imbalance 

learning. Systems, Man, and Cybernetics, Part B: Cybernetics, 39(2):539–550, 

2009. 

[117] Shuo Wang, Ke Tang, and Xin Yao. Diversity exploration and negative correla- 

tion learn on imbalanced data sets. In Neural Networks, 2009. IJCNN 2009. 

International Joint Conference on, page 3259–3266. IEEE, 2009. 

[118] Fernando Vilariño, Panagiota Spyridonos, Jordi Vitrià, and Petia Radeva. Experi- 

ments with svm and stratify sample with an imbalanced problem: Detection of 

intestinal contractions. In Pattern Recognition and Image Analysis, page 783–791. 

Springer, 2005. 

[119] Pilsung Kang and Sungzoon Cho. Eus svms: Ensemble of under-sampled svms 

for data imbalance problems. In Neural Information Processing, page 837–846. 

Springer, 2006. 

[120] Yang Liu, Aijun An, and Xiangji Huang. Boosting prediction accuracy on imbal- 

anced datasets with svm ensembles. In Advances in Knowledge Discovery and Data 

Mining, page 107–118. Springer, 2006. 

[121] Benjamin X Wang and Nathalie Japkowicz. Boosting support vector machine for 

imbalanced data sets. Knowledge and Information Systems, 25(1):1–20, 2010. 

[122] N. Chawla, A. Lazarevic, L. Hall, and K. Bowyer. Smoteboost: Improving predic- 

tion of the minority class in boosting. Knowledge Discovery in Databases: PKDD 

2003, page 107–119, 2003. 

[123] Mahesh V Joshi, Vipin Kumar, and Ramesh C Agarwal. Evaluating boost 

algorithm to classify rare classes: Comparison and improvements. In Data Mining, 

2001. ICDM 2001, Proceedings IEEE International Conference on, page 257–264. 

IEEE, 2001. 

[124] David Mease, Abraham J Wyner, and Andreas Buja. Boosted classification tree 

and class probability/quantile estimation. The Journal of Machine Learning Re- 

search, 8:409–439, 2007. 

[125] Yanmin Sun, Mohamed S Kamel, Andrew KC Wong, and Yang Wang. Cost- 

sensitive boost for classification of imbalanced data. Pattern Recognition, 40 

(12):3358–3378, 2007. 



Bibliography 171 

[126] Wei Fan, Salvatore J Stolfo, Junxin Zhang, and Philip K Chan. Adacost: misclas- 

sification cost-sensitive boosting. In ICML, page 97–105, 1999. 

[127] Hamed Masnadi-Shirazi and Nuno Vasconcelos. Risk minimization, probability 

elicitation, and cost-sensitive svms. In ICML, page 759–766, 2010. 

[128] Matjaz Kukar, Igor Kononenko, et al. Cost-sensitive learn with neural networks. 

In ECAI, page 445–449, 1998. 

[129] Charles X Ling, Qiang Yang, Jianning Wang, and Shichao Zhang. Decision tree 

with minimal costs. In Proceedings of the twenty-first international conference on 

Machine learning, page 69. ACM, 2004. 

[130] Jeffrey P Bradford, Clayton Kunz, Ron Kohavi, Cliff Brunk, and Carla E Brodley. 

Pruning decision tree with misclassification costs. In Machine Learning: ECML- 

98, page 131–136. Springer, 1998. 

[131] C.X. Ling and V.S. Sheng. Cost-sensitive learn and the class imbalance problem. 

Encyclopedia of Machine Learning, 2008. 

[132] Marcus A Maloof, Pat Langley, Stephanie Sage, and T Binford. Learning to detect 

rooftop in aerial images. 1997. 

[133] Corinna Cortes, Mehryar Mohri, Michael Riley, and Afshin Rostamizadeh. Sam- 

ple selection bias correction theory. In Algorithmic learn theory, page 38–53. 

Springer, 2008. 

[134] Marco Saerens, Patrice Latinne, and Christine Decaestecker. Adjusting the output 

of a classifier to new a priori probabilities: a simple procedure. Neural computation, 

14(1):21–41, 2002. 

[135] Mark G Kelly, David J Hand, and Niall M Adams. The impact of change pop- 

ulations on classifier performance. In Proceedings of the fifth ACM SIGKDD in- 

ternational conference on Knowledge discovery and data mining, page 367–371. 

ACM, 1999. 

[136] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by 

weight the log-likelihood function. Journal of statistical planning and inference, 

90(2):227–244, 2000. 

[137] David J Hand et al. Classifier technology and the illusion of progress. Statistical 

science, 21(1):1–14, 2006. 

[138] Keisuke Yamazaki, Motoaki Kawanabe, Sumio Watanabe, Masashi Sugiyama, and 

Klaus-Robert Müller. Asymptotic bayesian generalization error when training and 



Bibliography 172 

test distribution be different. In Proceedings of the 24th international conference 

on Machine learning, page 1079–1086. ACM, 2007. 

[139] Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Müller. Covariate shift 

adaptation by importance weight cross validation. The Journal of Machine 

Learning Research, 8:985–1005, 2007. 

[140] James J Heckman. Sample selection bias a a specification error. Econometrica: 

Journal of the econometric society, page 153–161, 1979. 

[141] Jose G Moreno-Torres, Troy Raeder, Rocío Alaiz-Rodríguez, Nitesh V Chawla, 

and Francisco Herrera. A unify view on dataset shift in classification. Pattern 

Recognition, 45(1):521–530, 2012. 

[142] Bianca Zadrozny and Charles Elkan. Learning and make decision when cost 

and probability be both unknown. In Proceedings of the seventh ACM SIGKDD 

international conference on Knowledge discovery and data mining, page 204–213. 

ACM, 2001. 

[143] Bianca Zadrozny. Learning and evaluate classifier under sample selection bias. 

In Proceedings of the twenty-first international conference on Machine learning, 

page 114. ACM, 2004. 

[144] Wei Fan, Ian Davidson, Bianca Zadrozny, and Philip S Yu. An improve catego- 

rization of classifier’s sensitivity on sample selection bias. In Data Mining, Fifth 

IEEE International Conference on, page 4–pp. IEEE, 2005. 

[145] Miroslav Dudík, Steven J Phillips, and Robert E Schapire. Correcting sample 

selection bias in maximum entropy density estimation. In Advances in neural 

information processing systems, page 323–330, 2005. 

[146] Nitesh V Chawla and Grigoris I Karakoulas. Learning from label and unla- 

beled data: An empirical study across technique and domains. J. Artif. Intell. 

Res.(JAIR), 23:331–366, 2005. 

[147] Masashi Sugiyama. Learning under non-stationarity: Covariate shift adaptation 

by importance weighting. In Handbook of Computational Statistics, page 927–952. 

Springer, 2012. 

[148] T Ryan Hoens, Robi Polikar, and Nitesh V Chawla. Learning from stream data 

with concept drift and imbalance: an overview. Progress in Artificial Intelligence, 

1(1):89–101, 2012. 

[149] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D 

Lawrence. Dataset shift in machine learning. The MIT Press, 2009. 



Bibliography 173 

[150] C. Alippi, G. Boracchi, and M. Roveri. Just-in-time classifier for recurrent con- 

cepts. Neural Networks and Learning Systems, IEEE Transactions on, 24(4):620– 

634, April. ISSN 2162-237X. doi: 10.1109/TNNLS.2013.2239309. 

[151] Stephen Grossberg. Nonlinear neural networks: Principles, mechanisms, and ar- 

chitectures. Neural networks, 1(1):17–61, 1988. 

[152] Joao Gama, Pedro Medas, Gladys Castillo, and Pedro Rodrigues. Learning with 

drift detection. In Advances in artificial intelligence–SBIA 2004, page 286–295. 

Springer, 2004. 

[153] Kyosuke Nishida and Koichiro Yamauchi. Detecting concept drift use statistical 

testing. In Discovery Science, page 264–269. Springer, 2007. 

[154] Albert Bifet and Ricard Gavalda. Learning from time-changing data with adaptive 

windowing. In SDM, volume 7, page 2007. SIAM, 2007. 

[155] Cesare Alippi, Giacomo Boracchi, and Manuel Roveri. A just-in-time adaptive 

classification system base on the intersection of confidence interval rule. Neural 

Networks, 24(8):791–800, 2011. 

[156] Gregory Ditzler, Manuel Roveri, Cesare Alippi, and Robi Polikar. Learning in non- 

stationary environments: A survey. Computational Intelligence Magazine, IEEE, 

10(4):12–25, 2015. 

[157] Indrė Žliobaitė. Learning under concept drift: an overview. arXiv preprint 

arXiv:1010.4784, 2010. 

[158] Ryan Elwell and Robi Polikar. Incremental learn of concept drift in nonsta- 

tionary environments. Neural Networks, IEEE Transactions on, 22(10):1517–1531, 

2011. 

[159] Robi Polikar, L Upda, SS Upda, and Vasant Honavar. Learn++: An incremental 

learn algorithm for supervise neural networks. Systems, Man, and Cybernetics, 

Part C: Applications and Reviews, 31(4):497–508, 2001. 

[160] W Nick Street and YongSeog Kim. A stream ensemble algorithm (sea) for large- 

scale classification. In Proceedings of the seventh ACM SIGKDD international 

conference on Knowledge discovery and data mining, page 377–382. ACM, 2001. 

[161] Sheng Chen, Haibo He, Kang Li, and Sachi Desai. Musera: multiple selectively 

recursive approach towards imbalanced stream data mining. In Neural Networks 

(IJCNN), The 2010 International Joint Conference on, page 1–8. IEEE, 2010. 



Bibliography 174 

[162] J Zico Kolter and Marcus A Maloof. Dynamic weight majority: An ensemble 

method for drift concepts. The Journal of Machine Learning Research, 8:2755– 

2790, 2007. 

[163] Jeffrey C Schlimmer and Richard H Granger Jr. Incremental learn from noisy 

data. Machine learning, 1(3):317–354, 1986. 

[164] Gerhard Widmer and Miroslav Kubat. Learning in the presence of concept drift 

and hidden contexts. Machine learning, 23(1):69–101, 1996. 

[165] Ludmila I Kuncheva. Classifier ensemble for change environments. In Multiple 

classifier systems, page 1–15. Springer, 2004. 

[166] Charu C Aggarwal. Data streams: model and algorithms, volume 31. Springer 

Science & Business Media, 2007. 

[167] Pedro Domingos and Geoff Hulten. Mining high-speed data streams. In Proceedings 

of the sixth ACM SIGKDD international conference on Knowledge discovery and 

data mining, page 71–80. ACM, 2000. 

[168] Geoff Hulten, Laurie Spencer, and Pedro Domingos. Mining time-changing data 

streams. In Proceedings of the seventh ACM SIGKDD international conference on 

Knowledge discovery and data mining, page 97–106. ACM, 2001. 

[169] João Gama, Ricardo Fernandes, and Ricardo Rocha. Decision tree for mining 

data streams. Intelligent Data Analysis, 10(1):23–45, 2006. 

[170] Jing Gao, Wei Fan, Jiawei Han, and S Yu Philip. A general framework for mining 

concept-drifting data stream with skewed distributions. In SDM, 2007. 

[171] Jing Gao, Bolin Ding, Wei Fan, Jiawei Han, and Philip S Yu. Classifying data 

stream with skewed class distribution and concept drifts. Internet Computing, 

12(6):37–49, 2008. 

[172] Gregory Ditzler and Robi Polikar. An ensemble base incremental learn frame- 

work for concept drift and class imbalance. In Neural Networks (IJCNN), The 

2010 International Joint Conference on, page 1–8. IEEE, 2010. 

[173] Gregory Ditzler and Robi Polikar. Incremental learn of concept drift from 

stream imbalanced data. Knowledge and Data Engineering, IEEE Transactions 

on, 25(10):2283–2301, 2013. 

[174] Ryan Elwell and Robi Polikar. Incremental learn of variable rate concept drift. 

In Multiple Classifier Systems, page 142–151. Springer, 2009. 



Bibliography 175 

[175] Ryan N Lichtenwalter and Nitesh V Chawla. Adaptive method for classification 

in arbitrarily imbalanced and drift data streams. In New Frontiers in Applied 

Data Mining, page 53–75. Springer, 2010. 

[176] Sheng Chen and Haibo He. Towards incremental learn of nonstationary imbal- 

anced data stream: a multiple selectively recursive approach. Evolving Systems, 2 

(1):35–50, 2011. 

[177] Sheng Chen and Haibo He. Sera: selectively recursive approach towards nonsta- 

tionary imbalanced stream data mining. In Neural Networks, 2009. IJCNN 2009. 

International Joint Conference on, page 522–529. IEEE, 2009. 

[178] ShuoWang, Leandro L Minku, and Xin Yao. Online class imbalance learn and it 

application in fault detection. International Journal of Computational Intelligence 

and Applications, 12(04), 2013. 

[179] Nikunj C Oza. Online bagging and boosting. In Systems, man and cybernetics, 

volume 3, page 2340–2345. IEEE, 2005. 

[180] R. Brause, T. Langsdorf, and M. Hepp. Neural data mining for credit card fraud 

detection. In Tools with Artificial Intelligence, Proceedings, page 103–106. IEEE, 

1999. 

[181] P.K. Chan, W. Fan, A.L. Prodromidis, and S.J. Stolfo. Distributed data mining 

in credit card fraud detection. Intelligent Systems and their Applications, 14(6): 

67–74, 1999. 

[182] Dimitris K Tasoulis, Niall M Adams, and David J Hand. Unsupervised cluster 

in stream data. In ICDM Workshops, page 638–642, 2006. 

[183] F. Provost, T. Fawcett, et al. Analysis and visualization of classifier performance: 

Comparison under imprecise class and cost distributions. In Proceedings of the 

third international conference on knowledge discovery and data mining, page 43– 

48. Amer Assn for Artificial, 1997. 

[184] D.J. Weston, D.J. Hand, N.M. Adams, C. Whitrow, and P. Juszczak. Plastic 

card fraud detection use peer group analysis. Advances in Data Analysis and 

Classification, 2(1):45–62, 2008. 

[185] Agus Sudjianto, Sheela Nair, Ming Yuan, Aijun Zhang, Daniel Kern, and Fernando 

Cela-Díaz. Statistical method for fight financial crimes. Technometrics, 52(1), 

2010. 

[186] Naeem Siddiqi. Credit risk scorecards: develop and implement intelligent 

credit scoring, volume 3. Wiley. com, 2005. 



Bibliography 176 

[187] Tom Fawcett and Foster Provost. Adaptive fraud detection. Data mining and 

knowledge discovery, 1(3):291–316, 1997. 

[188] Corinna Cortes and Daryl Pregibon. Signature-based method for data streams. 

Data Mining and Knowledge Discovery, 5(3):167–182, 2001. 

[189] Haixun Wang, Wei Fan, Philip S Yu, and Jiawei Han. Mining concept-drifting 

data stream use ensemble classifiers. In Proceedings of the ninth ACM SIGKDD 

international conference on Knowledge discovery and data mining, page 226–235. 

ACM, 2003. 

[190] EWT Ngai, Yong Hu, YH Wong, Yijun Chen, and Xin Sun. The application of 

data mining technique in financial fraud detection: A classification framework and 

an academic review of literature. Decision Support Systems, 50(3):559–569, 2011. 

[191] Sushmito Ghosh and Douglas L Reilly. Credit card fraud detection with a neural- 

network. In System Sciences, 1994. Proceedings of the Twenty-Seventh Hawaii 

International Conference on, volume 3, page 621–630. IEEE, 1994. 

[192] Emin Aleskerov, Bernd Freisleben, and Bharat Rao. Cardwatch: A neural network 

base database mining system for credit card fraud detection. In Computational 

Intelligence for Financial Engineering (CIFEr), 1997., Proceedings of the IEEE/I- 

AFE 1997, page 220–226. IEEE, 1997. 

[193] J.R. Dorronsoro, F. Ginel, C. Sgnchez, and CS Cruz. Neural fraud detection in 

credit card operations. Neural Networks, 8(4):827–834, 1997. 

[194] D. Sánchez, MA Vila, L. Cerda, and JM Serrano. Association rule apply to 

credit card fraud detection. Expert Systems with Applications, 36(2):3630–3640, 

2009. 

[195] Tian-Shyug Lee, Chih-Chou Chiu, Yu-Chao Chou, and Chi-Jie Lu. Mining the 

customer credit use classification and regression tree and multivariate adaptive 

regression splines. Computational Statistics & Data Analysis, 50(4):1113–1130, 

2006. 

[196] Krishna M Gopinathan, Louis S Biafore, William M Ferguson, Michael A Lazarus, 

Anu K Pathria, and Allen Jost. Fraud detection use predictive modeling, Octo- 

ber 6 1998. US Patent 5,819,226. 

[197] B Fryer. Visa crack down on fraud. InformationWeek, 594:87, 1996. 

[198] P. Viola and M. Jones. Fast and robust classification use asymmetric adaboost 

and a detector cascade. Advances in Neural Information Processing System, 14, 

2001. 



Bibliography 177 

[199] K. Ting. An empirical study of metacost use boost algorithms. Machine 

Learning: ECML 2000, page 413–425, 2000. 

[200] G.K.J. Shawe-Taylor. Optimizing classifier for imbalanced training sets. Advances 

in Neural Information Processing Systems 11, 11:253, 1999. 

[201] Andrew Fast, Lisa Friedland, Marc Maier, Brian Taylor, David Jensen, Henry G 

Goldberg, and John Komoroske. Relational data pre-processing technique for 

improve security fraud detection. In Proceedings of the 13th ACM SIGKDD 

international conference on Knowledge discovery and data mining, page 941–949. 

ACM, 2007. 

[202] Yufeng Kou, Chang-Tien Lu, Sirirat Sirwongwattana, and Yo-Ping Huang. Survey 

of fraud detection techniques. In Networking, sense and control, 2004 IEEE 

international conference on, volume 2, page 749–754. IEEE, 2004. 

[203] Clifton Phua, Vincent Lee, Kate Smith, and Ross Gayler. A comprehensive survey 

of data mining-based fraud detection research. arXiv preprint arXiv:1009.6119, 

2010. 

[204] Vladimir Zaslavsky and Anna Strizhak. Credit card fraud detection use self- 

organize maps. Information and Security, 18:48, 2006. 

[205] Dominik Olszewski. Fraud detection use self-organizing map visualize the user 

profiles. Knowledge-Based Systems, 70:324–334, 2014. 

[206] F.E. Grubbs. Procedures for detect outlying observation in samples. Techno- 

metrics, 11(1):1–21, 1969. 

[207] V. Barnett and T. Lewis. Outliers in statistical data. Wiley Series in Probability 

and Mathematical Statistics. Applied Probability and Statistics, Chichester: Wiley, 

1984, 2nd ed., 1, 1984. 

[208] Longin Jan Latecki, Aleksandar Lazarevic, and Dragoljub Pokrajac. Outlier de- 

tection with kernel density functions. In Machine Learning and Data Mining in 

Pattern Recognition, page 61–75. Springer, 2007. 

[209] Feng Jiang, Yuefei Sui, and Cungen Cao. Outlier detection base on rough mem- 

bership function. In Rough Sets and Current Trends in Computing, page 388–397. 

Springer, 2006. 

[210] Michael H Cahill, Diane Lambert, Jost C Pinheiro, and Don X Sun. Detecting 

fraud in the real world. Computing Reviews, 45(7):447, 2004. 



Bibliography 178 

[211] Charu C Aggarwal and Philip S Yu. Outlier detection for high dimensional data. 

In ACM Sigmod Record, volume 30, page 37–46. ACM, 2001. 

[212] Zengyou He, Shengchun Deng, and Xiaofei Xu. An optimization model for outlier 

detection in categorical data. In Advances in Intelligent Computing, page 400–409. 

Springer, 2005. 

[213] Andrea Dal Pozzolo, Olivier Caelen, and Gianluca Bontempi. When be under- 

sample effective in unbalanced classification tasks? In Machine Learning and 

Knowledge Discovery in Databases. Springer, 2015. 

[214] Jerzy Stefanowski. Overlapping, rare example and class decomposition in learn 

classifier from imbalanced data. In Emerging Paradigms in Machine Learning, 

page 277–306. Springer, 2013. 

[215] Gustavo EAPA Batista, Ronaldo C Prati, and Maria C Monard. Balancing strate- 

gy and class overlapping. In Advances in Intelligent Data Analysis VI, page 

24–35. Springer, 2005. 

[216] Vicente García, Jose Sánchez, and Ramon Mollineda. An empirical study of the 

behavior of classifier on imbalanced and overlap data sets. In Progress in 

Pattern Recognition, Image Analysis and Applications, page 397–406. Springer, 

2007. 

[217] Vicente García, Ramón Alberto Mollineda, and José Salvador Sánchez. On the 

k-nn performance in a challenge scenario of imbalance and overlapping. Pattern 

Analysis and Applications, 11(3-4):269–280, 2008. 

[218] Jason Van Hulse and Taghi Khoshgoftaar. Knowledge discovery from imbalanced 

and noisy data. Data & Knowledge Engineering, 68(12):1513–1542, 2009. 

[219] D Anyfantis, M Karagiannopoulos, S Kotsiantis, and P Pintelas. Robustness 

of learn technique in handle class noise in imbalanced datasets. In Artifi- 

cial intelligence and innovation 2007: From theory to applications, page 21–28. 

Springer, 2007. 

[220] Damien Brain and Geoffrey I Webb. The need for low bias algorithm in classifi- 

cation learn from large data sets. In Principles of Data Mining and Knowledge 

Discovery, page 62–73. Springer, 2002. 

[221] HO Hartley and A Ross. Unbiased ratio estimators. 1954. 

[222] Andy Liaw and Matthew Wiener. Classification and regression by randomforest. 

R News, 2(3):18–22, 2002. URL http://CRAN.R-project.org/doc/Rnews/. 

http://CRAN.R-project.org/doc/Rnews/ 


Bibliography 179 

[223] Alexandros Karatzoglou, Alex Smola, Kurt Hornik, and Achim Zeileis. kernlab-an 

s4 package for kernel method in r. 2004. 

[224] Jarek Tuszynski. caTools: Tools: move window statistics, GIF, Base64, ROC 

AUC, etc., 2013. URL http://CRAN.R-project.org/package=caTools. R pack- 

age version 1.16. 

[225] David H Wolpert. The lack of a priori distinction between learn algorithms. 

Neural computation, 8(7):1341–1390, 1996. 

[226] David H Wolpert and William G Macready. No free lunch theorem for optimiza- 

tion. Evolutionary Computation, IEEE Transactions on, 1(1):67–82, 1997. 

[227] O. Maron and A.W. Moore. Hoeffding races: Accelerating model selection search 

for classification and function approximation. Robotics Institute, page 263, 1993. 

[228] David Meyer, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and 

Friedrich Leisch. e1071: Misc Functions of the Department of Statistics (e1071), 

TU Wien, 2012. URL http://CRAN.R-project.org/package=e1071. R package 

version 1.6-1. 

[229] Terry Therneau, Beth Atkinson, and Brian Ripley. rpart: Recursive Partitioning 

and Regression Trees, 2014. URL http://CRAN.R-project.org/package=rpart. 

R package version 4.1-5. 

[230] David A Cieslak, T Ryan Hoens, Nitesh V Chawla, and W Philip Kegelmeyer. 

Hellinger distance decision tree be robust and skew-insensitive. Data Mining and 

Knowledge Discovery, 24(1):136–158, 2012. 

[231] W. N. Venables and B. D. Ripley. Modern Applied Statistics with S. Springer, New 

York, fourth edition, 2002. URL http://www.stats.ox.ac.uk/pub/MASS4. ISBN 

0-387-95457-0. 

[232] Milton Friedman. The use of rank to avoid the assumption of normality implicit 

in the analysis of variance. Journal of the American Statistical Association, 32 

(200):675–701, 1937. 

[233] C Radhakrishna Rao. A review of canonical coordinate and an alternative to 

correspondence analysis use hellinger distance. Questiió: Quaderns d’Estadística, 

Sistemes, Informatica i Investigació Operativa, 19(1):23–63, 1995. 

[234] David A Cieslak and Nitesh V Chawla. Detecting fracture in classifier perfor- 

mance. In Data Mining, 2007. ICDM 2007. Seventh IEEE International Confer- 

ence on, page 123–132. IEEE, 2007. 

http://CRAN.R-project.org/package=caTools 
http://CRAN.R-project.org/package=e1071 
http://CRAN.R-project.org/package=rpart 
http://www.stats.ox.ac.uk/pub/MASS4 


Bibliography 180 

[235] Foster Provost and Pedro Domingos. Tree induction for probability-based ranking. 

Machine Learning, 52(3):199–215, 2003. 

[236] T Ryan Hoens, Nitesh V Chawla, and Robi Polikar. Heuristic updatable weight 

random subspace for non-stationary environments. In Data Mining (ICDM), 2011 

IEEE 11th International Conference on, page 241–250. IEEE, 2011. 

[237] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, 

and Ian H Witten. The weka data mining software: an update. ACM SIGKDD 

Explorations Newsletter, 11(1):10–18, 2009. 

[238] Albert Bifet, Geoff Holmes, Richard Kirkby, and Bernhard Pfahringer. Moa: Mas- 

sive online analysis. The Journal of Machine Learning Research, 99:1601–1604, 

2010. 

[239] Georg Krempl and Vera Hofer. Classification in presence of drift and latency. In 

Data Mining Workshops (ICDMW), 2011 IEEE 11th International Conference on, 

page 596–603. IEEE, 2011. 

[240] Wei Fan. Systematic data selection to mine concept-drifting data streams. In 

Proceedings of the tenth ACM SIGKDD international conference on Knowledge 

discovery and data mining, page 128–137. ACM, 2004. 

[241] Janez Demšar. Statistical comparison of classifier over multiple data sets. The 

Journal of Machine Learning Research, 7:1–30, 2006. 

[242] Wei Fan and Ian Davidson. On sample selection bias and it efficient correction via 

model average and unlabeled examples. In SDM, page 320–331. SIAM, 2007. 

[243] Torsten Hothorn, Peter Bühlmann, Sandrine Dudoit, Annette Molinaro, and 

Mark J Van Der Laan. Survival ensembles. Biostatistics, 7(3):355–373, 2006. 

[244] Hothorn Torsten, Hornik Kurt, and Achim Zeileis Carolin, Strobl and. party: A 

Laboratory for Recursive Partytioning., 2015. URL http://CRAN.R-project.org/ 

package=party. R package version 1.0-13. 

[245] Gilles Louppe. Understanding random forests: From theory to practice. arXiv 

preprint arXiv:1407.7502, 2014. 

[246] Burr Settles. Active learn literature survey. University of Wisconsin, Madison, 

52:55–66, 2010. 

[247] DJ Hand, C Whitrow, NM Adams, P Juszczak, and D Weston. Performance 

criterion for plastic card fraud detection tools. Journal of the Operational Research 

Society, 59(7):956–962, 2008. 

http://CRAN.R-project.org/package=party 
http://CRAN.R-project.org/package=party 


Bibliography 181 

[248] Bertrand Lebichot, Ilkka Kivimaki, Kevin Françoisse, and Marco Saerens. Semisu- 

pervised classification through the bag-of-paths group betweenness. Neural Net- 

work and Learning Systems, IEEE Transactions on, 25(6):1173–1186, 2014. 


Declaration of Authorship 
Abstract 
Résumé 
Acknowledgements 
List of Figures 
List of Tables 
List of Acronyms 
I Overview 
1 Introduction 
1.1 The problem of Fraud Detection 
1.2 The impact of fraud 
1.3 Credit Card Fraud Detection 
1.4 Challenges in Data Driven Fraud Detection Systems 
1.5 Contributions 
1.5.1 Understanding sample method 
1.5.2 Learning from evolve and unbalanced data stream 
1.5.3 Formalization of a real-world Fraud Detection System 
1.5.4 Software and Credit Card Fraud Detection Dataset 

1.6 Publications and research activity 
1.7 Financial support and project objective 
1.8 Outline 
1.9 Notation 

2 Preliminaries 
2.1 Machine Learning 
2.1.1 Formalization of supervise learn 
2.1.2 The problem of classification 
2.1.3 Bias-variance decomposition 
2.1.4 Evaluation of a classification problem 

2.2 Credit Card Fraud Detection 
2.2.1 Fraud Detection System work condition 
2.2.1.1 FDS Layers: 
2.2.1.2 Supervised Information 
2.2.1.3 System Update 

2.2.2 Features augmentation 
2.2.3 Accuracy measure of a Fraud Detection System 


3 State-of-the-art 
3.1 Techniques for unbalanced classification task 
3.1.1 Data level method 
3.1.2 Algorithm level method 

3.2 Learning with non-stationarity 
3.2.1 Sample Selection Bias 
3.2.2 Time evolve data 

3.3 Learning with evolve and unbalanced data stream 
3.4 Algorithmic solution for Fraud Detection 
3.4.1 Supervised Approaches 
3.4.2 Unsupervised Approaches 



II Contribution 
4 Techniques for unbalanced classification task 
4.1 When be undersampling effective in unbalanced classification tasks? 
4.1.1 The warp effect of undersampling on the posterior probability 
4.1.2 Warping and class separability 
4.1.3 The interaction between warp and variance of the estimator 
4.1.4 Experimental validation 
4.1.5 Discussion 

4.2 Using calibrate probability with undersampling 
4.2.1 Adjusting posterior probability to new prior 
4.2.2 Warping correction and classification threshold adjustment 
4.2.3 Experimental result 
4.2.4 Discussion 

4.3 Racing for sample method selection 
4.3.1 Racing for strategy selection 
4.3.2 Experimental result 
4.3.3 Discussion 

4.4 Conclusion 

5 Learning from evolve data stream with skewed distribution 
5.1 Learning strategy in credit card fraud detection 
5.1.1 Formalization of the learn problem 
5.1.2 Strategies for learn with unbalanced and evolve data stream 
5.1.3 Experimental assessment 
5.1.4 Discussion 

5.2 Using HDDT to avoid instance propagation 
5.2.1 Hellinger Distance Decision Trees 
5.2.2 Hellinger Distance a weight ensemble strategy 
5.2.3 Experimental assessment 
5.2.4 Discussion 

5.3 Conclusion 

6 A real-world Fraud Detection Systems: Concept Drift Adaptation with Alert-Feedback Interaction 
6.1 Realistic work condition 
6.2 Fraud Detection with Alert-Feedback Interaction 
6.3 Learning strategy 
6.3.1 Conventional Classification Approaches in FDS 
6.3.2 Separating delayed Supervised Samples from Feedbacks 
6.3.3 Two Specific FDSs base on Random Forest 

6.4 Selection bias and Alert-Feedback Interaction 
6.5 Experiments 
6.5.1 Separating feedback from delayed supervise sample 
6.5.2 Artificial dataset with Concept Drift 
6.5.3 Improving the performance of the feedback classifier 
6.5.4 Standard accuracy measure and classifier ignore AFI 
6.5.5 Adaptive aggregation 
6.5.6 Final strategy selection and classification model analysis 

6.6 Discussion 
6.7 Conclusion 

7 Conclusions and Future Perspectives 
7.1 Summary of contribution 
7.2 Learned lesson 
7.3 Open issue 
7.4 The Future: go towards Big Data solution 
7.5 Added value for the company 
7.6 Concluding remark 

A The unbalanced package 
A.1 Methods for unbalanced classification 
A.2 Racing for strategy selection 
A.3 Summary 

B FDS software module 
B.1 Model training 
B.2 Scoring 
B.3 Review 

C Bias and Variance of an estimator 
Bibliography 


