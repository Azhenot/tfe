


















































Proceedings of Machine Learning Research 81:1–15, 2018 Conference on Fairness, Accountability, and Transparency 

Gender Shades: Intersectional Accuracy Disparities in 
Commercial Gender Classification∗ 

Joy Buolamwini joyab@mit.edu 
MIT Media Lab 75 Amherst St. Cambridge, MA 02139 

Timnit Gebru timnit.gebru@microsoft.com 
Microsoft Research 641 Avenue of the Americas, New York, NY 10011 

Editors: Sorelle A. Friedler and Christo Wilson 

Abstract 
Recent study demonstrate that machine 
learn algorithm can discriminate base 
on class like race and gender. In this 
work, we present an approach to evaluate 
bias present in automate facial analysis al- 
gorithms and datasets with respect to phe- 
notypic subgroups. Using the dermatolo- 
gist approve Fitzpatrick Skin Type clas- 
sification system, we characterize the gen- 
der and skin type distribution of two facial 
analysis benchmarks, IJB-A and Adience. 
We find that these datasets be overwhelm- 
ingly compose of lighter-skinned subject 
(79.6% for IJB-A and 86.2% for Adience) 
and introduce a new facial analysis dataset 
which be balance by gender and skin type. 
We evaluate 3 commercial gender clas- 
sification system use our dataset and 
show that darker-skinned female be the 
most misclassified group (with error rate 
of up to 34.7%). The maximum error rate 
for lighter-skinned male be 0.8%. The 
substantial disparity in the accuracy of 
classify darker females, lighter females, 
darker males, and lighter male in gender 
classification system require urgent atten- 
tion if commercial company be to build 
genuinely fair, transparent and accountable 
facial analysis algorithms. 

Keywords: Computer Vision, Algorith- 
mic Audit, Gender Classification 

1. Introduction 

Artificial Intelligence (AI) be rapidly infiltrate 
every aspect of society. From help determine 

∗ Download our gender and skin type balance PPB 
dataset at gendershades.org 

who be hired, fired, grant a loan, or how long 
an individual spends in prison, decision that 
have traditionally be perform by human be 
rapidly make by algorithm (O’Neil, 2017; Citron 
and Pasquale, 2014). Even AI-based technology 
that be not specifically train to perform high- 
stake task (such a determine how long some- 
one spends in prison) can be use in a pipeline 
that performs such tasks. For example, while 
face recognition software by itself should not be 
train to determine the fate of an individual in 
the criminal justice system, it be very likely that 
such software be use to identify suspects. Thus, 
an error in the output of a face recognition algo- 
rithm use a input for other task can have se- 
rious consequences. For example, someone could 
be wrongfully accuse of a crime base on erro- 
neous but confident misidentification of the per- 
petrator from security video footage analysis. 

Many AI systems, e.g. face recognition tools, 
rely on machine learn algorithm that be 
train with label data. It have recently 
be show that algorithm train with bias 
data have result in algorithmic discrimination 
(Bolukbasi et al., 2016; Caliskan et al., 2017). 
Bolukbasi et al. even show that the popular 
word embed space, Word2Vec, encodes soci- 
etal gender biases. The author use Word2Vec 
to train an analogy generator that fill in miss- 
ing word in analogies. The analogy man be to 
computer programmer a woman be to “X” be 
complete with “homemaker”, conform to the 
stereotype that program be associate with 
men and homemaking with women. The bias 
in Word2Vec be thus likely to be propagate 
throughout any system that us this embedding. 

c© 2018 J. Buolamwini & T. Gebru. 

gendershades.org 


Gender Shades 

Although many work have study how to 
create fairer algorithms, and benchmarked dis- 
crimination in various context (Kilbertus et al., 
2017; Hardt et al., 2016b,a), only a handful of 
work have do this analysis for computer vi- 
sion. However, computer vision system with 
inferior performance across demographic can 
have serious implications. Esteva et al. show 
that simple convolutional neural network can be 
train to detect melanoma from images, with ac- 
curacy a high a expert (Esteva et al., 2017). 
However, without a dataset that have label for 
various skin characteristic such a color, thick- 
ness, and the amount of hair, one cannot measure 
the accuracy of such automate skin cancer de- 
tection system for individual with different skin 
types. Similar to the well document detrimen- 
tal effect of bias clinical trial (Popejoy and 
Fullerton, 2016; Melloni et al., 2010), bias sam- 
ples in AI for health care can result in treatment 
that do not work well for many segment of the 
population. 

In other contexts, a demographic group that 
be underrepresented in benchmark datasets can 
nonetheless be subject to frequent targeting. 
The use of automate face recognition by law 
enforcement provide such an example. At least 
117 million Americans be include in law en- 
forcement face recognition networks. A year- 
long research investigation across 100 police de- 
partments reveal that African-American indi- 
viduals be more likely to be stop by law 
enforcement and be subject to face recogni- 
tion search than individual of other ethnici- 
tie (Garvie et al., 2016). False positive and un- 
warrant search pose a threat to civil liberties. 
Some face recognition system have be show 
to misidentify people of color, women, and young 
people at high rate (Klare et al., 2012). Moni- 
toring phenotypic and demographic accuracy of 
these system a well a their use be necessary to 
protect citizens’ right and keep vendor and law 
enforcement accountable to the public. 

We take a step in this direction by make two 
contributions. First, our work advance gender 
classification benchmarking by introduce a new 
face dataset compose of 1270 unique individu- 
al that be more phenotypically balance on the 
basis of skin type than exist benchmarks. To 
our knowledge this be the first gender classifica- 
tion benchmark label by the Fitzpatrick (TB, 

1988) six-point skin type scale, allow u to 
benchmark the performance of gender classifica- 
tion algorithm by skin type. Second, this work 
introduces the first intersectional demographic 
and phenotypic evaluation of face-based gender 
classification accuracy. Instead of evaluate ac- 
curacy by gender or skin type alone, accuracy 
be also examine on 4 intersectional subgroups: 
darker females, darker males, lighter females, and 
lighter males. The 3 evaluate commercial gen- 
der classifier have the low accuracy on darker 
females. Since computer vision technology be be- 
ing utilized in high-stakes sector such a health- 
care and law enforcement, more work need to 
be do in benchmarking vision algorithm for 
various demographic and phenotypic groups. 

2. Related Work 

Automated Facial Analysis. Automated fa- 
cial image analysis describes a range of face per- 
ception task including, but not limited to, face 
detection (Zafeiriou et al., 2015; Mathias et al., 
2014; Bai and Ghanem, 2017), face classifica- 
tion (Reid et al., 2013; Levi and Hassner, 2015a; 
Rothe et al., 2016) and face recognition (Parkhi 
et al., 2015; Wen et al., 2016; Ranjan et al., 2017). 
Face recognition software be now built into most 
smart phone and company such a Google, 
IBM, Microsoft and Face++ have release com- 
mercial software that perform automate facial 
analysis (IBM; Microsoft; Face++; Google). 

A number of work have go further than 
solely perform task like face detection, recog- 
nition and classification that be easy for human 
to perform. For example, company such a Af- 
fectiva (Affectiva) and researcher in academia 
attempt to identify emotion from image of peo- 
ple’s face (Dehghan et al., 2017; Srinivasan et al., 
2016; Fabian Benitez-Quiroz et al., 2016). Some 
work have also use automate facial analysis 
to understand and help those with autism (Leo 
et al., 2015; Palestra et al., 2016). Controversial 
paper such a (Kosinski and Wang, 2017) claim 
to determine the sexuality of Caucasian male 
whose profile picture be on Facebook or date 
sites. And others such a (Wu and Zhang, 2016) 
and Israeli base company Faception (Faception) 
have developed software that purport to deter- 
mine an individual’s characteristic (e.g. propen- 
sity towards crime, IQ, terrorism) solely from 

2 



Gender Shades 

their faces. The client of such software include 
governments. An article by (Aguera Y Arcas et 
al., 2017) detail the danger and error propa- 
gate by some of these aforementioned works. 

Face detection and classification algorithm 
be also use by US-based law enforcement for 
surveillance and crime prevention purposes. In 
“The Perpetual Lineup”, Garvie and colleague 
provide an in-depth analysis of the unregulated 
police use of face recognition and call for rigorous 
standard of automate facial analysis, racial ac- 
curacy testing, and regularly inform the pub- 
lic about the use of such technology (Garvie 
et al., 2016). Past research have also show that 
the accuracy of face recognition system use 
by US-based law enforcement be systematically 
low for people label female, Black, or be- 
tween the age of 18—30 than for other demo- 
graphic cohort (Klare et al., 2012). The late 
gender classification report from the National In- 
stitute for Standards and Technology (NIST) also 
show that algorithm NIST evaluate perform 
bad for female-labeled face than male-labeled 
face (Ngan et al., 2015). 

The lack of datasets that be label by eth- 
nicity limit the generalizability of research ex- 
ploring the impact of ethnicity on gender classi- 
fication accuracy. While the NIST gender report 
explore the impact of ethnicity on gender classi- 
fication through the use of an ethnic proxy (coun- 
try of origin), none of the 10 location use in 
the study be in Africa or the Caribbean where 
there be significant Black populations. On the 
other hand, Farinella and Dugelay claimed that 
ethnicity have no effect on gender classification, 
but they use a binary ethnic categorization 
scheme: Caucasian and non-Caucasian (Farinella 
and Dugelay, 2012). To address the underrepre- 
sentation of people of African-descent in previ- 
ous studies, our work explores gender classifica- 
tion on African face to further scholarship on 
the impact of phenotype on gender classification. 

Benchmarks. Most large-scale attempt to 
collect visual face datasets rely on face de- 
tection algorithm to first detect face (Huang 
et al., 2007; Kemelmacher-Shlizerman et al., 
2016). Megaface, which to date be the large 
publicly available set of facial images, be com- 
pose utilize Head Hunter (Mathias et al., 
2014) to select one million image from the Yahoo 
Flicker 100M image dataset (Thomee et al., 2015; 

Kemelmacher-Shlizerman et al., 2016). Any sys- 
tematic error found in face detector will in- 
evitably affect the composition of the bench- 
mark. Some datasets collect in this manner 
have already be document to contain signif- 
icant demographic bias. For example, LFW, a 
dataset compose of celebrity face which have 
serve a a gold standard benchmark for face 
recognition, be estimate to be 77.5% male and 
83.5% White (Han and Jain, 2014). Although 
(Taigman et al., 2014)’s face recognition system 
recently report 97.35% accuracy on the LFW 
dataset, it performance be not broken down by 
race or gender. Given these skews in the LFW 
dataset, it be not clear that the high report ac- 
curacy be applicable to people who be not well 
represent in the LFW benchmark. In response 
to these limitations, Intelligence Advanced Re- 
search Projects Activity (IARPA) release the 
IJB-A dataset a the most geographically diverse 
set of collect face (Klare et al., 2015). In 
order to limit bias, no face detector be use 
to select image contain faces. In compari- 
son to face recognition, less work have be do 
to benchmark performance on gender classifica- 
tion. In 2015, the Adience gender and age classi- 
fication benchmark be release (Levi and Has- 
sner, 2015b). As of 2017, The National Insti- 
tute of Standards and Technology be start an- 
other challenge to spur improvement in face gen- 
der classification by expand on the 2014-15 
study. 

3. Intersectional Benchmark 

An evaluation of gender classification perfor- 
mance currently require reduce the construct 
of gender into define classes. In this work we use 
the sex label of “male” and “female” to define 
gender class since the evaluate benchmark 
and classification system use these binary labels. 
An intersectional evaluation further require a 
dataset represent the define gender with a 
range of phenotype that enable subgroup accu- 
racy analysis. To ass the suitability of exist- 
ing datasets for intersectional benchmarking, we 
provide skin type annotation for unique sub- 
jects within two select datasets, and compare 
the distribution of darker females, darker males, 
lighter females, and lighter males. Due to phe- 
notypic imbalance in exist benchmarks, we 

3 



Gender Shades 

Figure 1: Example image and average face from the new Pilot Parliaments Benchmark (PPB). As 
the example show, the image be constrain with relatively little variation in pose. The 
subject be compose of male and female parliamentarian from 6 countries. On average, 
Senegalese subject be the darkest skin while those from Finland and Iceland be the 
light skinned. 

create a new dataset with more balance skin 
type and gender representations. 

3.1. Rationale for Phenotypic Labeling 

Though demographic label for protect class 
like race and ethnicity have be use for per- 
form algorithmic audit (Friedler et al., 2016; 
Angwin et al., 2016) and assess dataset diver- 
sity (Han and Jain, 2014), phenotypic label be 
seldom use for these purposes. While race la- 
bel be suitable for assess potential algorith- 
mic discrimination in some form of data (e.g. 
those use to predict criminal recidivism rates), 
they face two key limitation when use on visual 
images. First, subjects’ phenotypic feature can 
vary widely within a racial or ethnic category. 
For example, the skin type of individual iden- 
tifying a Black in the US can represent many 
hues. Thus, facial analysis benchmark consist- 
ing of lighter-skinned Black individual would not 
adequately represent darker-skinned ones. Sec- 
ond, racial and ethnic category be not consis- 

tent across geographies: even within country 
these category change over time. 

Since race and ethnic label be unstable, we 
decide to use skin type a a more visually pre- 
cise label to measure dataset diversity. Skin type 
be one phenotypic attribute that can be use to 
more objectively characterize datasets along with 
eye and nose shapes. Furthermore, skin type be 
chosen a a phenotypic factor of interest because 
default camera setting be calibrate to expose 
lighter-skinned individual (Roth, 2009). Poorly 
expose image that result from sensor optimiza- 
tions for lighter-skinned subject or poor illumi- 
nation can prove challenge for automate facial 
analysis. By label face with skin type, we 
can increase our understand of performance 
on this important phenotypic attribute. 

3.2. Existing Benchmark Selection 
Rationale 

IJB-A be a US government benchmark release 
by the National Institute of Standards and Tech- 

4 



Gender Shades 

LightestDarkest 

180o 120o 60o 0o 60o 120o 180o 

60o 

0o 

30o 

60o 

30o 

Figure 2: The global distribution of skin color. Most Africans have darker skin while those from 
Nordic country be lighter-skinned. Image from (Encyclopedia Britannica) c©Copyright 
2012 Encyclopedia Britannica. 

nology (NIST) in 2015. We chose to evaluate this 
dataset give the government’s involvement and 
the explicit development of the benchmark to be 
geographically diverse (as mention in Sec. 2). 
At the time of assessment in April and May of 
2017, the dataset consist of 500 unique sub- 
jects who be public figures. One image of each 
unique subject be manually label with one of 
six Fitzpatrick skin type (TB, 1988). 

Adience be a gender classification benchmark 
release in 2014 and be select due to it re- 
cency and unconstrained nature. The Adience 
benchmark contains 2, 284 unique individual sub- 
jects. 2, 194 of those subject have reference im- 
age that be discernible enough to be label 
by skin type and gender. Like the IJB-A dataset, 
only one image of each subject be label for 
skin type. 

3.3. Creation of Pilot Parliaments 
Benchmark 

Preliminary analysis of the IJB-A and Adi- 
ence benchmark reveal overrepresentation of 
lighter males, underrepresentation of darker fe- 
males, and underrepresentation of darker indi- 
viduals in general. We developed the Pilot Par- 
liaments Benchmark (PPB) to achieve good in- 
tersectional representation on the basis of gender 
and skin type. PPB consists of 1270 individual 

from three African country (Rwanda, Senegal, 
South Africa) and three European country (Ice- 
land, Finland, Sweden) select for gender parity 
in the national parliaments. 

Property PPB IJB-A Adience 

Release Year 2017 2015 2014 
#Subjects 1270 500 2284 
Avg. IPD 63 pixel - - 
BBox Size 141 (avg) ≥36 - 
IM Width 160-590 - 816 
IM Height 213-886 - 816 

Table 1: Various image characteristic of the Pi- 
lot Parliaments Benchmark compare 
with prior datasets. #Subjects denotes 
the number of unique subjects, the aver- 
age bound box size be give in pixels, 
and IM stand for image. 

Figure 1 show example image from PPB a 
well a average face of male and female in 
each country represent in the datasets. We 
decide to use image of parliamentarian since 
they be public figure with know identity and 
photo available under non-restrictive license 
post on government websites. To add skin 

5 



Gender Shades 

type diversity to the dataset, we chose parlia- 
mentarians from African and European coun- 
tries. Fig. 2 show an approximate distribu- 
tion of average skin type around the world. As 
see in the map, African country typically have 
darker-skinned individual whereas Nordic coun- 
try tend to have lighter-skinned citizens. Col- 
onization and migration pattern nonetheless in- 
fluence the phenotypic distribution of skin type 
and not all Africans be darker-skinned. Simi- 
larly, not all citizen of Nordic country can be 
classify a lighter-skinned. 

The specific African and European country 
be select base on their rank for gen- 
der parity a assess by the Inter Parliamen- 
tary Union (Inter Parliamentary Union Rank- 
ing). Of all the country in the world, Rwanda 
have the high proportion of woman in parlia- 
ment. Nordic country be also well represent 
in the top 10 nations. Given the gender parity 
and prevalence of lighter skin in the region, Ice- 
land, Finland, and Sweden be chosen. To bal- 
ance for darker skin, the next two highest-ranking 
African nations, Senegal and South Africa, be 
also added. 

Table 1 compare image characteristic of PPB 
with IJB-A and Adience. PPB be highly con- 
strain since it be compose of official profile 
photo of parliamentarians. These profile photo 
be take under condition with cooperative sub- 
jects where pose be relatively fixed, illumination be 
constant, and expression be neutral or smiling. 
Conversely, the image in the IJB-A and Adi- 
ence benchmark be unconstrained and subject 
pose, illumination, and expression by construc- 
tion have more variation. 

3.4. Intersectional Labeling Methodology 

Skin Type Labels. We chose the Fitzpatrick 
six-point label system to determine skin type 
label give it scientific origins. Dermatologists 
use this scale a the gold standard for skin classi- 
fication and determine risk for skin cancer (TB, 
1988). 

The six-point Fitzpatrick classification system 
which label skin a Type I to Type VI be skewed 
towards lighter skin and have three category that 
can be apply to people perceive a White (Fig- 
ure 2). Yet when it come to fully represent 
the sepia spectrum that characterizes the rest of 

PPB 

IJB-A 

Adience 

0% 25% 50% 75% 100% 

30.323.325.021.3 

4.4 16.0 20.2 59.4 

7.4 6.4 44.6 41.6 %Darker Female 

%Darker Male 

%Lighter Female 

%Ligher Male 

Figure 3: The percentage of darker female, 
lighter female, darker male, and lighter 
male subject in PPB, IJB-A and Adi- 
ence. Only 4.4% of subject in Adience 
be darker-skinned and female in com- 
parison to 21.3% in PPB. 

the world, the categorization be fairly coarse. 
Nonetheless, the scale provide a scientifically 
base start point for audit algorithm and 
datasets by skin type. 

Gender Labels. All evaluate company 
provide a “gender classification” feature that 
us the binary sex label of female and male. 
This reductionist view of gender do not ade- 
quately capture the complexity of gender or ad- 
dress transgender identities. The company pro- 
vide no documentation to clarify if their gender 
classification system which provide sex label be 
classify gender identity or biological sex. To 
label the PPB data, we use female and male la- 
bel to indicate subject perceive a woman or 
men respectively. 

Labeling Process. For exist benchmarks, 
one author label each image with one of six 
Fitzpatrick skin type and provide gender an- 
notation for the IJB-A dataset. The Adience 
benchmark be already annotate for gender. 
These preliminary skin type annotation on ex- 
isting datasets be use to determine if a new 
benchmark be needed. 

More annotation resource be use to label 
PPB. For the new parliamentarian benchmark, 
3 annotator include the author provide gen- 
der and Fitzpatrick labels. A board-certified sur- 
gical dermatologist provide the definitive label 
for the Fitzpatrick skin type. Gender label be 
determine base on the name of the parliamen- 
tarian, gendered title, prefix such a Mr or Ms, 
and the appearance of the photo. 

6 



Gender Shades 

Set n F M Darker Lighter DF DM LF LM 

All Subjects 1270 44.6% 55.4% 46.4% 53.6% 21.3% 25.0% 23.3% 30.3% 

Africa 661 43.9% 56.1% 86.2% 13.8% 39.8% 46.4% 4.1% 9.7% 

South Africa 437 41.4% 58.6% 79.2% 20.8% 35.2% 43.9% 6.2% 14.6% 
Senegal 149 43.0% 57.0% 100.0% 0.0% 43.0% 57.0% 0.0% 0.0% 
Rwanda 75 60.0% 40.0% 100.0% 0.0% 60.0% 40.0% 0.0% 0.0% 

Europe 609 45.5% 54.5% 3.1% 96.9% 1.3% 1.8% 44.2% 52.7% 

Sweden 349 46.7% 53.3% 4.9% 95.1% 2.0% 2.9% 44.7% 50.4% 
Finland 197 42.6% 57.4% 1.0% 99.0% 0.5% 0.5% 42.1% 56.9% 
Iceland 63 47.6% 52.4% 0.0% 100.0% 0.0% 0.0% 47.6% 52.4% 

Table 2: Pilot Parliaments Benchmark decomposition by the total number of female subject de- 
note a F, total number of male subject (M), total number of darker and lighter subjects, 
a well a female darker/lighter (DF/LF) and male darker/lighter subject (DM/LM). The 
group composition be show for all unique subjects, Africa, Europe and the country in 
our dataset locate in each of these continents. 

Dataset Lighter (I,II,III) Darker (IV, V, VI) Total 

PPB 53.6% 681 46.4% 589 1270 
IJB-A 79.6% 398 20.4% 102 500 
Adience 86.2% 1892 13.8% 302 2194 

Table 3: The distribution of lighter and darker-skinned subject (according to the Fitzpatrick clas- 
sification system) in PPB, IJB-A, and Adience datasets. Adience have the most skewed 
distribution with 86.2% of the subject consist of lighter-skinned individual whereas 
PPB be more evenly distribute between lighter (53.6%) and darker (46.4%) subjects. 

3.5. Fitzpatrick Skin Type Comparison 

For the purpose of our analysis, lighter subject 
will refer to face with a Fitzpatrick skin type 
of I,II, or III. Darker subject will refer to face 
label with a Fitzpatrick skin type of IV,V, or 
VI. We intentionally choose country with ma- 
jority population at opposite end of the skin 
type scale to make the lighter/darker dichotomy 
more distinct. The skin type be aggregate to 
account for potential off-by-one error since the 
skin type be estimate use image instead of em- 
ploying a standard spectrophotometer and Fitz- 
patrick questionnaire. 

Table 2 present the gender, skin type, and in- 
tersectional gender by skin type composition of 
PPB. And Figure 3 compare the percentage of 
image from darker female, darker male, lighter 

female and lighter male subject from Adience, 
IJB-A, and PBB. PPB provide the most bal- 
anced representation of all four group whereas 
IJB-A have the least balance distribution. 

Darker female be the least represent in 
IJB-A (4.4%) and darker male be the least rep- 
resent in Adience (6.4%). Lighter male be the 
most represent unique subject in all datasets. 
IJB-A be compose of 59.4% unique lighter male 
whereas this percentage be reduce to 41.6% in 
Adience and 30.3% in PPB. As see in Table 3, 
Adience have the most skewed distribution by skin 
type. 

While all the datasets have more lighter- 
skin unique individuals, PPB be around half 
light at 53.6% whereas the proportion of lighter- 
skin unique subject in IJB-A and Adience 

7 



Gender Shades 

be 79.6% and 86.2% respectively. PPB provide 
substantially more darker-skinned unique sub- 
jects than IJB-A and Adience. Even though Adi- 
ence have 2194 label unique subjects, which be 
nearly twice that of the 1270 subject in PPB, 
it have 302 darker subjects, nearly half the 589 
darker subject in PPB. Overall, PPB have a more 
balance representation of lighter and darker 
subject a compare to the IJB-A and Adience 
datasets. 

4. Commercial Gender 
Classification Audit 

We evaluate 3 commercial gender classifiers. 
Overall, male subject be more accurately clas- 
sified than female subject replicate previous 
finding (Ngan et al., 2015), and lighter subject 
be more accurately classify than darker in- 
dividuals. An intersectional breakdown reveals 
that all classifier perform bad on darker fe- 
male subjects. 

4.1. Key Findings on Evaluated 
Classifiers 

• All classifier perform good on male face 
than female face (8.1% − 20.6% difference 
in error rate) 

• All classifier perform good on lighter face 
than darker face (11.8% − 19.2% difference 
in error rate) 

• All classifier perform bad on darker female 
face (20.8% − 34.7% error rate) 

• Microsoft and IBM classifier perform best 
on lighter male face (error rate of 0.0% and 
0.3% respectively) 

• Face++ classifier perform best on darker 
male face (0.7% error rate) 

• The maximum difference in error rate be- 
tween the best and bad classify group be 
34.4% 

4.2. Commercial Gender Classifier 
Selection: Microsoft, IBM, Face++ 

We focus on gender classifier sell in API bun- 
dle make available by Microsoft, IBM, and 

Face++ (Microsoft; IBM; Face++). Microsoft’s 
Cognitive Services Face API and IBM’s Wat- 
son Visual Recognition API be chosen since 
both company have make large investment in 
artificial intelligence, capture significant market 
share in the machine learn service domain, 
and provide public demonstration of their fa- 
cial analysis technology. At the time of evalua- 
tion, Google do not provide a publicly available 
gender classifier. Previous study have show 
that face recognition system developed in West- 
ern nation and those developed in Asian nation 
tend to perform good on their respective popu- 
lations (Phillips et al., 2011). Face++, a com- 
puter vision company headquarter in China 
with facial analysis technology previously inte- 
grate with some Lenovo computers, be thus 
chosen to see if this observation hold for gender 
classification. Like Microsoft and IBM, Face++ 
also provide a publicly available demonstration 
of their gender classification capability at the 
time of evaluation(April and May 2017). 

All of the company offer gender classifica- 
tion a a component of a set of proprietary facial 
analysis API service (Microsoft; IBM; Face++). 
The description of classification methodology 
lack detail and there be no mention of what 
training data be used. At the time of evaluation, 
Microsoft’s Face Detect service be described a 
use advanced statistical algorithm that “may 
not always be 100% precise” (Microsoft API Ref- 
erence). IBM Watson Visual Recognition and 
Face++ service be say to use deep learning- 
base algorithm (IBM API Reference; Face++ 
Terms of Service). None of the commercial gen- 
der classifier chosen for this analysis report 
performance metric on exist gender estima- 
tion benchmark in their provide documenta- 
tion. The Face++ term of use explicitly dis- 
claim any warranty of accuracy. Only IBM 
provide confidence score (between 0 and 1) for 
face-based gender classification labels. But it do 
not report how any metric like true positive rate 
(TPR) or false positive rate (FPR) be bal- 
anced. 

4.3. Evaluation Methodology 

In follow the gender classification evaluation 
precedent establish by the National Institute 
for Standards and Technology (NIST), we ass 

8 



Gender Shades 

Classifier Metric All F M Darker Lighter DF DM LF LM 

MSFT 

PPV(%) 93.7 89.3 97.4 87.1 99.3 79.2 94.0 98.3 100 
Error Rate(%) 6.3 10.7 2.6 12.9 0.7 20.8 6.0 1.7 0.0 

TPR (%) 93.7 96.5 91.7 87.1 99.3 92.1 83.7 100 98.7 
FPR (%) 6.3 8.3 3.5 12.9 0.7 16.3 7.9 1.3 0.0 

Face++ 

PPV(%) 90.0 78.7 99.3 83.5 95.3 65.5 99.3 94.0 99.2 
Error Rate(%) 10.0 21.3 0.7 16.5 4.7 34.5 0.7 6.0 0.8 

TPR (%) 90.0 98.9 85.1 83.5 95.3 98.8 76.6 98.9 92.9 
FPR (%) 10.0 14.9 1.1 16.5 4.7 23.4 1.2 7.1 1.1 

IBM 

PPV(%) 87.9 79.7 94.4 77.6 96.8 65.3 88.0 92.9 99.7 
Error Rate(%) 12.1 20.3 5.6 22.4 3.2 34.7 12.0 7.1 0.3 

TPR (%) 87.9 92.1 85.2 77.6 96.8 82.3 74.8 99.6 94.8 
FPR (%) 12.1 14.8 7.9 22.4 3.2 25.2 17.7 5.20 0.4 

Table 4: Gender classification performance a measure by the positive predictive value (PPV), error 
rate (1-PPV), true positive rate (TPR), and false positive rate (FPR) of the 3 evaluate 
commercial classifier on the PPB dataset. All classifier have the high error rate for 
darker-skinned female (ranging from 20.8% for Microsoft to 34.7% for IBM). 

Classifier Metric DF DM LF LM 

MSFT 

PPV(%) 76.2 100 100 100 
Error Rate(%) 23.8 0.0 0.0 0.0 

TPR(%) 100 84.2 100 100 
FPR(%) 15.8 0.0 0.0 0.0 

Face++ 

PPV(%) 64.0 99.5 100 100 
Error Rate(%) 36.0 0.5 0.0 0.0 

TPR(%) 99.0 77.8 100 96.9 
FPR(%) 22.2 1.03 3.08 0.0 

IBM 

PPV(%) 66.9 94.3 100 98.4 
Error Rate(%) 33.1 5.7 0.0 1.6 

TPR(%) 90.4 78.0 96.4 100 
FPR(%) 22.0 9.7 0.0 3.6 

Table 5: Gender classification performance a measure by the positive predictive value (PPV), error 
rate (1-PPV), true positive rate (TPR), and false positive rate (FPR) of the 3 evaluate 
commercial classifier on the South African subset of the PPB dataset. Results for South 
Africa follow the overall trend with the high error rate see on darker-skinned females. 

the overall classification accuracy, male classifi- 
cation accuracy, and female classification accu- 
racy a measure by the positive predictive value 
(PPV). Extending beyond the NIST Methodol- 
ogy we also evaluate the true positive rate, false 
positive rate, and error rate (1-PPV) of the fol- 

low groups: all subjects, male subjects, female 
subjects, lighter subjects, darker subjects, darker 
females, darker males, lighter females, and lighter 
males. See Table 2 in supplementary material 
for result disaggregated by gender and each Fitz- 
patrick Skin Type. 

9 



Gender Shades 

4.4. Audit Results 

Male and Female Error Rates 

To conduct a demographic performance analy- 
sis, the difference in male and female error rate 
for each gender classifier be compare first in 
aggregate (Table 4) and then for South Africa 
(Table 5). The NIST Evaluation of Automated 
Gender Classification Algorithms report reveal 
that gender classification performance on female 
face be 1.8% to 12.5% low than performance 
on male face for the nine evaluate algorithm 
(Ngan et al., 2015). The gender misclassifica- 
tion rate on the Pilot Parliaments Benchmark 
replicate this trend across all classifiers. The dif- 
ferences between female and male classification 
error rate range from 8.1% to 20.6%. The rela- 
tively high true positive rate for female indicate 
that when a face be predict to be female the es- 
timation be more likely to be correct than when 
a face be predict to be male. For the Microsoft 
and IBM classifiers, the false positive rate (FPR) 
for female be double or more than the FPR for 
males. The FPR for female be more than 13 
time that of male with the Face++ classifier. 

Darker and Lighter Error Rates 

To conduct a phenotypic performance analysis, 
the difference in darker and lighter skin type er- 
ror rate for each gender classifier be compare 
first in aggregate (Table 4) and then for South 
Africa (Table 5). All classifier perform good 
on lighter subject than darker subject in PPB. 
Microsoft achieves the best result with error rate 
of 12.9% on darker subject and 0.7% on lighter 
individuals. On darker subjects, IBM achieves 
the bad classification accuracy with an error 
rate of 22.4%. This rate be nearly 7 time high 
than the IBM error rate on lighter faces. 

Intersectional Error Rates 

To conduct an intersectional demographic and 
phenotypic analysis, the error rate for four inter- 
sectional group (darker females, darker males, 
lighter female and lighter males) be compare 
in aggregate and then for South Africa. 

Across the board, darker female account for 
the large proportion of misclassified subjects. 
Even though darker female make up 21.3% of 
the PPB benchmark, they constitute between 

61.0% to 72.4.1% of the classification error. 
Lighter male who make up 30.3% of the bench- 
mark contribute only 0.0% to 2.4% of the total 
error from these classifier (See Table 1 in sup- 
plementary materials). 

We present a deeper look at image from South 
Africa to see if difference in algorithmic per- 
formance be mainly due to image quality from 
each parliament. In PPB, the European parlia- 
mentary image tend to be of high resolution 
with less pose variation when compare to image 
from African parliaments. The South African 
parliament, however, have comparable image res- 
olution and have the large skin type spread of 
all the parliaments. Lighter subject makeup 
20.8% (n=91) of the images, and darker subject 
make up the remain 79.2% (n=346) of im- 
ages. Table 5 show that all algorithm perform 
bad on female and darker subject when com- 
par to their counterpart male and lighter sub- 
jects. The Microsoft gender classifier performs 
the best, with zero error on classify all male 
and lighter females. 

On the South African subset of the PPB bench- 
mark, all the error for Microsoft arises from mis- 
classify image of darker females. Table 5 also 
show that all classifier perform bad on darker 
females. Face++ be flawless on lighter male and 
lighter females. IBM performs best on lighter fe- 
male with 0.0% error rate. Examining classifica- 
tion performance on the South African subset of 
PPB reveals trend that closely match the algo- 
rithmic performance on the entire dataset. Thus, 
we conclude that variation in performance due 
to the image characteristic of each country do 
not fully account for the difference in misclassifi- 
cation rate between intersectional subgroups. In 
other words, the presence of more darker individ- 
uals be a good explanation for error rate than a 
deviation in how image of parliamentarian be 
compose and produced. However, darker skin 
alone may not be fully responsible for misclassi- 
fication. Instead, darker skin may be highly cor- 
related with facial geometry or gender display 
norm that be less represent in the training 
data of the evaluate classifiers. 

4.5. Analysis of Results 

The overall gender classification accuracy result 
show the obfuscate nature of single perfor- 

10 



Gender Shades 

●●●● 

● 

●●●● 
● 
●●●●● 

● 

●●●●●● 
● 

● 

● 
● 
● 

● 

● 

● 

●●●● ●● 
● 
● 
● 
●● 
●● 

● 

● 

● 

●●●●●●● 

● 

●●●●●●●●●●●●●●●● 

● 

● 

●● 

● 

●●● 
● 
● 
● 
●● 

●● 

● 
●● 
●●● 

● 

● 
● 

● 

●●● 

● 

●●●● 

● 

●●● 

● 

●●● 

● 
● 

● 

● 

● 

● 

● 

●●●● 

● 

● 

●● 

● 

●● 

● 

●● 
● 

●● 

● 

● 

● 

●● 

● 

● 

●● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

● 

●●●● 
●●●● 
●●●●● 
● 

● 

● 
● 

● 

●●●●●●●●●●● 
●●●●●●●●● 

● 

●●●●●●● 

● 

●●●●●●●●●●●●●● 
● 

●●●●●●●●●● 

● 

●●●●●●●●●●●●●● 
●● 

● 

● 

● 

●● 

0.00 

0.25 

0.50 

0.75 

1.00 

Darker Female Darker Male Lighter Female Lighter Male 
Group 

C 
on 

fid 
en 

ce 
S 

co 
re 

s 

Gender 
Female 

Male 

Figure 4: Gender classification confidence score 
from IBM (IBM). Scores be near 1 for 
lighter male and female subject while 
they range from ∼ 0.75 − 1 for darker 
females. 

mance metrics. Taken at face value, gender clas- 
sification accuracy range from 87.9% to 93.7% 
on the PPB dataset, suggest that these classi- 
fiers can be use for all population represent 
by the benchmark. A company might justify the 
market readiness of a classifier by present per- 
formance result in aggregate. Yet a gender and 
phenotypic breakdown of the result show that 
performance differs substantially for distinct sub- 
groups. Classification be 8.1% − 20.6% bad on 
female than male subject and 11.8% − 19.2% 
bad on darker than lighter subjects. 

Though helpful in see systematic error, gen- 
der and skin type analysis by themselves do not 
present the whole story. Is misclassification dis- 
tributed evenly amongst all females? Are there 
other factor at play? Likewise, be the misclassi- 
fication of darker skin uniform across gender? 

The intersectional error analysis that target 
gender classification performance on darker fe- 
male, lighter female, darker male, and lighter 
male subgroup provide more answers. Darker 
female have the high error rate for all gender 
classifier range from 20.8% − 34.7%. For Mi- 
crosoft and IBM classifier lighter male be the 
best classify group with 0.0% and 0.3% error 
rate respectively. Face++ classifies darker male 
best with an error rate of 0.7%. When examine 
the gap in lighter and darker skin classification, 
we see that even though darker female be most 
impacted, darker male be still more misclassi- 
fied than lighter male for IBM and Microsoft. 
The most improvement be need on darker fe- 
male specifically. More broadly, the error gap 
between male and female classification along with 
lighter and darker classification should be closed. 

4.6. Accuracy Metrics 

Microsoft and Face++ APIs solely output single 
label indicate whether the face be classify 
a female or male. IBM’s API output an ad- 
ditional number which indicates the confidence 
with which the classification be made. Fig- 
ure 4 plot the distribution of confidence value 
for each of the subgroup we evaluate (i.e. darker 
females, darker males, lighter female and lighter 
males). Numbers near 0 indicate low confidence 
whereas those close to 1 denote high confidence 
in classify gender. As show in the box plots, 
the API be most confident in classify lighter 
male and least confident in classify darker fe- 
males. 

While confidence value give user more in- 
formation, commercial classifier should provide 
additional metrics. All 3 evaluate APIs only 
provide gender classifications, they do not out- 
put probability associate with the likelihood 
of be a particular gender. This indicates that 
company be choose a threshold which deter- 
mine the classification: if the prediction proba- 
bility be great than this threshold, the image be 
determine to be that of a male (or female) sub- 
ject, and viceversa if the probability be less than 
this number. This do not give user the abil- 
ity to analyze true positive (TPR) and false posi- 
tive (FPR) rate for various subgroup if different 
threshold be to be chosen. The commercial 
classifier have picked threshold that result in 
specific TPR and FPR rate for each subgroup. 
And the FPR for some group can be much high 
than those for others. By have APIs that fail 
to provide the ability to adjust these thresholds, 
they be limit users’ ability to pick their own 
TPR/FPR trade-off. 

4.7. Data Quality and Sensors 

It be well establish that pose, illumination, and 
expression (PIE) can impact the accuracy of au- 
tomated facial analysis. Techniques to create ro- 
bust system that be invariant to pose, illumi- 
nation, expression, occlusions, and background 
have receive substantial attention in computer 
vision research (Kakadiaris et al., 2017; Ganguly 
et al., 2015; Ahmad Radzi et al., 2014). Illumi- 
nation be of particular importance when do an 
evaluation base on skin type. Default camera 
setting be often optimize to expose lighter skin 

11 



Gender Shades 

good than darker skin (Roth, 2009). Underex- 
pose or overexpose image that present signif- 
icant information loss can make accurate classi- 
fication challenging. 

With full awareness of the challenge that arise 
due to pose and illumination, we intentionally 
chose an optimistic sample of constrain image 
that be take from the parliamentarian web- 
sites. Each country have it peculiarities. Images 
from Rwanda and Senegal have more pose and 
illumination variation than image from other 
country (Figure 1). The Swedish parliamen- 
tarians all have photo that be take with a 
shadow on the face. The South African image 
have the most consistent pose and illumination. 
The South African subset be also compose of 
a substantial number of lighter and darker sub- 
jects. Given the diversity of the subset, the 
high image resolution, and the consistency of 
illumination and pose, our find that classi- 
fication accuracy varied by gender, skin type, 
and the intersection of gender with skin type do 
not appear to be confound by the quality of 
sensor readings. The disparity present with 
such a constrain dataset do suggest that error 
rate would be high on more challenge uncon- 
strain datasets. Future work should explore 
gender classification on an inclusive benchmark 
compose of unconstrained images. 

5. Conclusion 

We measure the accuracy of 3 commercial gen- 
der classification algorithm on the new Pilot 
Parliaments Benchmark which be balance by 
gender and skin type. We annotate the dataset 
with the Fitzpatrick skin classification system 
and test gender classification performance on 4 
subgroups: darker females, darker males, lighter 
female and lighter males. We found that all clas- 
sifiers perform best for lighter individual and 
male overall. The classifier perform bad 
for darker females. Further work be need to 
see if the substantial error rate gap on the ba- 
si of gender, skin type and intersectional sub- 
group reveal in this study of gender classifica- 
tion persist in other human-based computer vi- 
sion tasks. Future work should explore intersec- 
tional error analysis of facial detection, identifi- 
cation and verification. Intersectional phenotypic 
and demographic error analysis can help inform 

method to improve dataset composition, feature 
selection, and neural network architectures. 

Because algorithmic fairness be base on differ- 
ent contextual assumption and optimization for 
accuracy, this work aim to show why we need 
rigorous reporting on the performance metric on 
which algorithmic fairness debate center. The 
work focus on increase phenotypic and demo- 
graphic representation in face datasets and algo- 
rithmic evaluation. Inclusive benchmark datasets 
and subgroup accuracy report will be necessary 
to increase transparency and accountability in 
artificial intelligence. For human-centered com- 
puter vision, we define transparency a provide 
information on the demographic and phenotypic 
composition of training and benchmark datasets. 
We define accountability a reporting algorith- 
mic performance on demographic and pheno- 
typic subgroup and actively work to close 
performance gap where they arise. Algorith- 
mic transparency and accountability reach be- 
yond technical report and should include mech- 
anisms for consent and redress which we do not 
focus on here. Nonetheless, the finding from this 
work concern benchmark representation and 
intersectional audit provide empirical support 
for increase demographic and phenotypic trans- 
parency and accountability in artificial intelli- 
gence. 

Acknowledgments 

We thank board-certified surgical dermatologist 
Dr. Helen Raynham for provide the official 
Fitzpatrick annotation for the Pilot Parliaments 
Benchmark. 

References 

Face++ API. http://old.faceplusplus.com/ 
demo-detect/. Accessed: 2017-10-06. 

Face, Google APIs for Android, Google De- 
velopers. https://developers.google.com/ 
android/reference/com/google/android/ 

gms/vision/face/Face. Accessed: 2017-10- 
06. 

Watson Visual Recognition. http : 
//www.ibm.com/watson/services/visual- 
recognition/. Accessed: 2017-10-06. 

12 

http://old.faceplusplus.com/demo-detect/ 
http://old.faceplusplus.com/demo-detect/ 
https://developers.google.com/android/reference/com/google/android/gms/vision/face/Face 
https://developers.google.com/android/reference/com/google/android/gms/vision/face/Face 
https://developers.google.com/android/reference/com/google/android/gms/vision/face/Face 
https://www.ibm.com/watson/services/visual-recognition/ 
https://www.ibm.com/watson/services/visual-recognition/ 
https://www.ibm.com/watson/services/visual-recognition/ 


Gender Shades 

Microsoft Face API. http : / / 
www.microsoft.com / cognitive - service / 
en-us/faceapi. Accessed: 2017-10-06. 

Affectiva Emotion Recognition Software and 
Analysis. https://www.affectiva.com/. Ac- 
cessed: 2017-10-06. 

Physiognomys New Clothes. http : / / 
medium.com/@blaisea/physiognomys- new- 
clothes-f2d4b59fdd6a. Accessed: 2017-10- 
06. 

Face++ Terms of Use. a. Accessed: 2018-12-13. 

Faception, Facial Personality Analytics. https: 
//www.faception.com/, b. Accessed: 2017-10- 
06. 

Visual Recognition API Reference. Accessed: 
2018-12-13. 

How to Detect Faces in Image. Accessed: 2018- 
12-13. 

Proportion of seat held by woman in national 
parliaments. https://data.worldbank.org/ 
indicator/SG.GEN.PARL.ZS?year high desc= 
true. Accessed: 2017-10-06. 

Syafeeza Ahmad Radzi, Khalil-Hani Mohamad, 
Shan Sung Liew, and Rabia Bakhteri. Con- 
volutional neural network for face recognition 
with pose and illumination variation. Interna- 
tional Journal of Engineering and Technology 
(IJET), 6(1):44–57, 2014. 

Julia Angwin, Jeff Larson, Surya Mattu, and 
Lauren Kirchner. Machine bias: Theres soft- 
ware use across the country to predict future 
criminals. and it bias against blacks. ProP- 
ublica, May, 23, 2016. 

Yancheng Bai and Bernard Ghanem. Multi-scale 
fully convolutional network for face detection 
in the wild. In Computer Vision and Pattern 
Recognition Workshops (CVPRW), 2017 IEEE 
Conference on, page 2078–2087. IEEE, 2017. 

Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, 
Venkatesh Saligrama, and Adam T Kalai. Man 
be to computer programmer a woman be to 
homemaker? debiasing word embeddings. In 
D. D. Lee, M. Sugiyama, U. V. Luxburg, 
I. Guyon, and R. Garnett, editors, Advances 

in Neural Information Processing Systems 29, 
page 4349–4357. Curran Associates, Inc., 
2016. URL http://papers.nips.cc/paper/ 
6228- man- is- to- computer- programmer- 

as- woman- is- to- homemaker- debiasing- 

word-embeddings.pdf. 

Encyclopedia Britannica. Skin distribution map. 
https://media1.britannica.com/eb-media/ 
59/61759- 004- 9A507F1C.gif, 2012. Ac- 
cessed: 2017-12-17. 

Aylin Caliskan, Joanna J Bryson, and Arvind 
Narayanan. Semantics derive automatically 
from language corpus contain human-like bi- 
ases. Science, 356(6334):183–186, 2017. 

Danielle Keats Citron and Frank A Pasquale. 
The score society: due process for automate 
predictions. 2014. 

Afshin Dehghan, Enrique G Ortiz, Guang Shu, 
and Syed Zain Masood. Dager: Deep age, 
gender and emotion recognition use con- 
volutional neural network. arXiv preprint 
arXiv:1702.04280, 2017. 

Andre Esteva, Brett Kuprel, Roberto A Novoa, 
Justin Ko, Susan M Swetter, Helen M Blau, 
and Sebastian Thrun. Dermatologist-level clas- 
sification of skin cancer with deep neural net- 
works. Nature, 542(7639):115–118, 2017. 

C Fabian Benitez-Quiroz, Ramprakash Srini- 
vasan, and Aleix M Martinez. Emotionet: An 
accurate, real-time algorithm for the automatic 
annotation of a million facial expression in the 
wild. In Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition, 
page 5562–5570, 2016. 

Giovanna Farinella and Jean-Luc Dugelay. De- 
mographic classification: Do gender and eth- 
nicity affect each other? In Informatics, Elec- 
tronics & Vision (ICIEV), 2012 International 
Conference on, page 383–390. IEEE, 2012. 

Sorelle A Friedler, Carlos Scheidegger, and 
Suresh Venkatasubramanian. On the (im) 
possibility of fairness. arXiv preprint 
arXiv:1609.07236, 2016. 

13 

https://www.microsoft.com/cognitive-services/en-us/faceapi 
https://www.microsoft.com/cognitive-services/en-us/faceapi 
https://www.microsoft.com/cognitive-services/en-us/faceapi 
https://www.affectiva.com/ 
https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a 
https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a 
https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a 
https://www.faception.com/ 
https://www.faception.com/ 
https://data.worldbank.org/indicator/SG.GEN.PARL.ZS?year_high_desc=true 
https://data.worldbank.org/indicator/SG.GEN.PARL.ZS?year_high_desc=true 
https://data.worldbank.org/indicator/SG.GEN.PARL.ZS?year_high_desc=true 
http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf 
http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf 
http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf 
http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf 
https://media1.britannica.com/eb-media/59/61759-004-9A507F1C.gif 
https://media1.britannica.com/eb-media/59/61759-004-9A507F1C.gif 


Gender Shades 

Suranjan Ganguly, Debotosh Bhattacharjee, and 
Mita Nasipuri. Illumination, pose and occlu- 
sion invariant face recognition from range im- 
age use erfi model. International Journal of 
System Dynamics Applications (IJSDA), 4(2): 
1–20, 2015. 

Clare Garvie, Alvaro Bedoya, and Jonathan 
Frankle. The Perpetual Line-Up: Unregulated 
Police Face Recognition in America. George- 
town Law, Center on Privacy & Technology, 
2016. 

Hu Han and Anil K Jain. Age, gender and 
race estimation from unconstrained face im- 
ages. Dept. Comput. Sci. Eng., Michigan State 
Univ., East Lansing, MI, USA, MSU Tech. 
Rep.(MSU-CSE-14-5), 2014. 

Moritz Hardt, Eric Price, Nati Srebro, et al. 
Equality of opportunity in supervise learning. 
In Advances in Neural Information Processing 
Systems, page 3315–3323, 2016a. 

Moritz Hardt, Eric Price, Nati Srebro, et al. 
Equality of opportunity in supervise learning. 
In Advances in Neural Information Processing 
Systems, page 3315–3323, 2016b. 

Gary B Huang, Manu Ramesh, Tamara Berg, 
and Erik Learned-Miller. Labeled face in the 
wild: A database for study face recogni- 
tion in unconstrained environments. Technical 
report, Technical Report 07-49, University of 
Massachusetts, Amherst, 2007. 

Ioannis A Kakadiaris, George Toderici, Georgios 
Evangelopoulos, Georgios Passalis, Dat Chu, 
Xi Zhao, Shishir K Shah, and Theoharis Theo- 
haris. 3d-2d face recognition with pose and 
illumination normalization. Computer Vision 
and Image Understanding, 154:137–151, 2017. 

Ira Kemelmacher-Shlizerman, Steven M Seitz, 
Daniel Miller, and Evan Brossard. The 
megaface benchmark: 1 million face for recog- 
nition at scale. In Proceedings of the IEEE 
Conference on Computer Vision and Pattern 
Recognition, page 4873–4882, 2016. 

Niki Kilbertus, Mateo Rojas-Carulla, Giambat- 
tista Parascandolo, Moritz Hardt, Dominik 
Janzing, and Bernhard Schölkopf. Avoiding 
discrimination through causal reasoning. arXiv 
preprint arXiv:1706.02744, 2017. 

Brendan F Klare, Mark J Burge, Joshua C 
Klontz, Richard W Vorder Bruegge, and 
Anil K Jain. Face recognition performance: 
Role of demographic information. IEEE Trans- 
action on Information Forensics and Security, 
7(6):1789–1801, 2012. 

Brendan F Klare, Ben Klein, Emma Taborsky, 
Austin Blanton, Jordan Cheney, Kristen Allen, 
Patrick Grother, Alan Mah, and Anil K Jain. 
Pushing the frontier of unconstrained face de- 
tection and recognition: Iarpa janus bench- 
mark a. In Proceedings of the IEEE Conference 
on Computer Vision and Pattern Recognition, 
page 1931–1939, 2015. 

Michal Kosinski and Yilun Wang. Deep neural 
network be more accurate than human at 
detect sexual orientation from facial images. 
2017. 

Marco Leo, Marco Del Coco, Pierluigi Carcagni, 
Cosimo Distante, Massimo Bernava, Giovanni 
Pioggia, and Giuseppe Palestra. Automatic 
emotion recognition in robot-children interac- 
tion for asd treatment. In Proceedings of the 
IEEE International Conference on Computer 
Vision Workshops, page 145–153, 2015. 

Gil Levi and Tal Hassner. Age and gender classifi- 
cation use convolutional neural networks. In 
Proceedings of the IEEE Conference on Com- 
puter Vision and Pattern Recognition Work- 
shops, page 34–42, 2015a. 

Gil Levi and Tal Hassner. Age and gender classifi- 
cation use convolutional neural networks. In 
Proceedings of the IEEE Conference on Com- 
puter Vision and Pattern Recognition Work- 
shops, page 34–42, 2015b. 

Markus Mathias, Rodrigo Benenson, Marco Ped- 
ersoli, and Luc Van Gool. Face detection with- 
out bell and whistles. In European Conference 
on Computer Vision, page 720–735. Springer, 
2014. 

Chiara Melloni, Jeffrey S Berger, Tracy Y Wang, 
Funda Gunes, Amanda Stebbins, Karen S 
Pieper, Rowena J Dolor, Pamela S Douglas, 
Daniel B Mark, and L Kristin Newby. Repre- 
sentation of woman in randomize clinical tri- 
al of cardiovascular disease prevention. Circu- 

14 



Gender Shades 

lation: Cardiovascular Quality and Outcomes, 
3(2):135–142, 2010. 

Mei Ngan, Mei Ngan, and Patrick Grother. Face 
recognition vendor test (FRVT) performance of 
automate gender classification algorithms. US 
Department of Commerce, National Institute 
of Standards and Technology, 2015. 

Cathy O’Neil. Weapons of math destruction: 
How big data increase inequality and threat- 
en democracy. Broadway Books, 2017. 

Giuseppe Palestra, Giovanna Varni, Mohamed 
Chetouani, and Floriana Esposito. A multi- 
modal and multilevel system for robotics treat- 
ment of autism in children. In Proceedings of 
the International Workshop on Social Learn- 
ing and Multimodal Interaction for Designing 
Artificial Agents, page 3. ACM, 2016. 

Omkar M Parkhi, Andrea Vedaldi, Andrew Zis- 
serman, et al. Deep face recognition. In 
BMVC, volume 1, page 6, 2015. 

P Jonathon Phillips, Fang Jiang, Abhijit 
Narvekar, Julianne Ayyad, and Alice J 
O’Toole. An other-race effect for face recogni- 
tion algorithms. ACM Transactions on Applied 
Perception (TAP), 8(2):14, 2011. 

Alice B Popejoy and Stephanie M Fullerton. Ge- 
nomics be fail on diversity. Nature, 538 
(7624):161, 2016. 

Rajeev Ranjan, Swami Sankaranarayanan, Car- 
los D Castillo, and Rama Chellappa. An all-in- 
one convolutional neural network for face anal- 
ysis. In Automatic Face & Gesture Recogni- 
tion (FG 2017), 2017 12th IEEE International 
Conference on, page 17–24. IEEE, 2017. 

Daniel Reid, Sina Samangooei, Cunjian Chen, 
Mark Nixon, and Arun Ross. Soft biometrics 
for surveillance: an overview. Machine learn- 
ing: theory and applications. Elsevier, page 
327–352, 2013. 

Lorna Roth. Looking at shirley, the ultimate 
norm: Colour balance, image technologies, and 
cognitive equity. Canadian Journal of Commu- 
nication, 34(1):111, 2009. 

Rasmus Rothe, Radu Timofte, and Luc 
Van Gool. Deep expectation of real and ap- 
parent age from a single image without facial 
landmarks. International Journal of Computer 
Vision, page 1–14, 2016. 

Ramprakash Srinivasan, Julie D Golomb, and 
Aleix M Martinez. A neural basis of facial ac- 
tion recognition in humans. Journal of Neuro- 
science, 36(16):4434–4442, 2016. 

Yaniv Taigman, Ming Yang, Marc’Aurelio Ran- 
zato, and Lior Wolf. Deepface: Closing the 
gap to human-level performance in face veri- 
fication. In Proceedings of the IEEE confer- 
ence on computer vision and pattern recogni- 
tion, page 1701–1708, 2014. 

Fitzpatrick TB. The validity and practicality of 
sun-reactive skin type i through vi. Archives 
of Dermatology, 124(6):869–871, 1988. doi: 
10.1001 / archderm.1988.01670060015008. 
URL +http : / / dx.doi.org / 10.1001 / 
archderm.1988.01670060015008. 

Bart Thomee, David A Shamma, Gerald Fried- 
land, Benjamin Elizalde, Karl Ni, Douglas 
Poland, Damian Borth, and Li-Jia Li. The 
new data and new challenge in multimedia re- 
search. arXiv preprint arXiv:1503.01817, 1(8), 
2015. 

Yandong Wen, Kaipeng Zhang, Zhifeng Li, and 
Yu Qiao. A discriminative feature learn ap- 
proach for deep face recognition. In European 
Conference on Computer Vision, page 499– 
515. Springer, 2016. 

Xiaolin Wu and Xi Zhang. Automated infer- 
ence on criminality use face images. arXiv 
preprint arXiv:1611.04135, 2016. 

Stefanos Zafeiriou, Cha Zhang, and Zhengyou 
Zhang. A survey on face detection in the wild: 
past, present and future. Computer Vision and 
Image Understanding, 138:1–24, 2015. 

15 

+ http://dx.doi.org/10.1001/archderm.1988.01670060015008 
+ http://dx.doi.org/10.1001/archderm.1988.01670060015008 

Introduction 
Related Work 
Intersectional Benchmark 
Rationale for Phenotypic Labeling 
Existing Benchmark Selection Rationale 
Creation of Pilot Parliaments Benchmark 
Intersectional Labeling Methodology 
Fitzpatrick Skin Type Comparison 

Commercial Gender Classification Audit 
Key Findings on Evaluated Classifiers 
Commercial Gender Classifier Selection: Microsoft, IBM, Face++ 
Evaluation Methodology 
Audit Results 
Analysis of Results 
Accuracy Metrics 
Data Quality and Sensors 

Conclusion 

