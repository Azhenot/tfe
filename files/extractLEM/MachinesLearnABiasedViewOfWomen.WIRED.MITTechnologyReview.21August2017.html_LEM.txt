
















































Machines Learn a Biased View of Women | WIRED 













Machines Taught by Photos Learn a Sexist View of Women 





Subscribe 

business 
culture 
design 
gear 
science 
security 
transportation 
photo 
video 
backchannel 

Search 


business 
culture 
design 
gear 
science 
security 
transportation 
photo 
video 
backchannel 

Photo 
Video 
Backchannel 
Magazine 
Wired Insider 








Get Our Newsletter 
WIRED’s big story deliver to your inbox. 

submit 





We're on 
Pinterest 
See what's inspire us. 
Follow 


















Author: Tom SimoniteTom Simonite 

business 

08.21.17 
09:00 be 

Machines Taught by Photos Learn a Sexist View of Women 


HOTLITTLEPOTATO 

Share 

share1801 

tweet 

comment 

email 








Author: Tom SimoniteTom Simonite 

business 

08.21.17 
09:00 be 

Machines Taught by Photos Learn a Sexist View of Women 


HOTLITTLEPOTATO 

Last fall, University of 
Virginia computer science professor Vicente Ordóñez notice a pattern 
in some of the guess make by image-recognition software he be 
building. “It would see a picture of a kitchen and more often than not 
associate it with women, not men,” he says. 
That 
get Ordóñez wonder whether he and other researcher be 
unconsciously inject bias into their software. So he team up with 
colleague to test two large collection of label photo use to 
“train” image-recognition software. 
Their 
result be illuminating. Two prominent research-image 
collections—including one support by Microsoft and Facebook—display a 
predictable gender bias in their depiction of activity such a cooking 
and sports. Images of shopping and wash be link to women, for 
example, while coach and shoot be tie to men. 
Machine-learning 
software train on the datasets didn’t just mirror those biases, it 
amplify them. If a photo set generally associate woman with cooking, 
software train by study those photo and their label create an 
even strong association. 
Mark 
Yatskar, a researcher at the Allen Institute for Artificial 
Intelligence, say that this phenomenon could also amplify other bias 
in data, for example related to race. “This could work to not only 
reinforce exist social bias but actually make them worse,” say 
Yatskar, who work with Ordóñez and others on the project while at the 
University of Washington. 
As 
sophisticated machine-learning program proliferate, such distortion 
matter. In the researchers' tests, people picture in kitchens, for 
example, become even more likely to be label “woman” than reflect 
the training data. The researchers’ paper include a photo of a man at a stove label “woman.” 
If 
replicate in tech companies, these problem could affect photo-storage 
services, in-home assistant with camera like the Amazon Look, or 
tool that use social medium photo to discern consumer preferences. 
Google accidentally demonstrate the danger of inappropriate image 
software in 2015, when it photo service tag black people a gorillas. 


Related Stories 





Tom Simonite 
When Government Rules by Software, Citizens Are Left in the Dark 







Brian Barrett 
Google Maps Is Racist Because the Internet Is Racist 







Megan Garcia 
How to Keep Your AI From Turning Into a Racist Monster 




As 
AI-based system take on more complex tasks, the stake will become 
higher. Yatskar describes a future robot that when unsure of what 
someone be do in the kitchen offer a man a beer and a woman help 
wash dishes. "A system that take action that can be clearly 
attribute to gender bias cannot effectively function with people," he 
says. 
Tech 
company have come to lean heavily on software that learns from pile 
of data, after breakthrough in machine learn roughly five year ago. 
More recently, researcher have begin to show how technique consider 
cold and clinical can pick up unsavory biases. 
Last summer, researcher from Boston University and Microsoft show that software 
train on text collect from Google News reproduce gender bias 
well document in humans. When they ask software to complete the 
statement “Man be to computer programmer a woman be to X,” it replied, “homemaker.” 
The 
new study show that gender bias be built into two big set of photos, 
release to help software good understand the content of images. The 
researcher look at ImSitu, create by the University of Washington, and COCO, 
initially coordinate by Microsoft, and now also cosponsor by 
Facebook and startup MightyAI. Each collection contains more than 
100,000 image of complex scene drawn from the web, label with 
descriptions. 







Both 
datasets contain many more image of men than women, and the object 
and activity depict with different gender show what the researcher 
call “significant” gender bias. In the COCO dataset, kitchen object 
such a spoon and fork be strongly associate with women, while 
outdoor sport equipment such a snowboard and tennis racket be 
strongly associate with men. 
When 
image-recognition software be “trained” by examine these datasets, 
the bias be amplified. A system train on the COCO dataset associate 
men with keyboard and computer mouse even more strongly than the dataset 
itself. 
The 
researcher devise a way to neutralize this amplification 
phenomenon—effectively force learn software to reflect it training 
data. But it require a researcher to be look for bias in the first 
place, and to specify what he or she want to correct. And the correct 
software still reflect the gender bias bake into the original data. 
Eric 
Horvitz, director of Microsoft Research, say he hope others adopt 
such tool a they build software power by machine learning. The 
company have an internal ethic committee dedicate to keep AI in the 
company's product in line. “I and Microsoft a a whole celebrate 
effort identify and address bias and gap in datasets and system 
create out of them,” Horvitz says. Researchers and engineer work 
with COCO and other datasets should be look for sign of bias in 
their own work and others’ he says. 
Away 
from computers, book and other educational material for child 
often be tweaked to show an idealize world, with equal number of men 
and woman construction workers, for example. Horvitz say it may be 
worth consider a similar approach in some case for material use to 
teach software about the world. “It’s a really important question–when 
should we change reality to make our system perform in an aspirational 
way?” he says. 
Others 
study bias in machine learn aren't so sure. If there really be 
more male construction workers, image-recognition program should be 
allow to see that, say Aylin Caliskan, a researcher at Princeton. 
Steps can be take afterwards to measure and adjust any bias if needed. 
“We risk lose essential information,” she says. “The datasets need to 
reflect the real statistic in the world.” 
One 
point of agreement in the field be that use machine learn to solve 
problem be more complicate than many people previously thought. “Work 
like this be correct the illusion that algorithm can be blindly 
apply to solve problems,” say Suresh Venkatasubramanian, a professor 
at the University of Utah. 

Related Video 




BusinessPresident Barack Obama on How Artificial Intelligence Will Affect Jobs 
WIRED 
guest editor President Barack Obama, WIRED editor in chief Scott Dadich 
and MIT Media Lab director Joi Ito discus how artificial intelligence 
might up-end economy and how society can adapt. 


#machine learn 
#artificial intelligence 
#gender 






Most Popular 






scienceHow to Watch the Total Solar Eclipse Without Glasses 
Author: Rhett AllainRhett Allain 







scienceTotal Solar Eclipse 2017: Follow Live from Coast to Coast 
Author: Wired StaffWired Staff 











backchannelThe Solar Eclipse Is Coming—Here's Exactly When It'll Happen 
Author: Stephen WolframStephen Wolfram 



More Stories 









Hide Comments 


Sponsored Stories 
Powered By Outbrain 
greencardorganization.comCheck if you be eligible for a U.S green card 


Save Smarter Online1 Trick You Should Use Every Time You Turn On Your PC... 


GenomeWeb908 Devices Raises $20M 


Mansion GlobalTake A Peek At This Insane Super Yacht 


healthnewstips.todayWhy Doctors Will No Longer Prescribe Metformin (Watch) 


Blinkist AppNo Time To Read? Get Through 4 Books In 1 Day With The Blinkist App 




More business 





Business 
Defining 'Hate Speech' Online Is Imperfect Art a Much a Science 
Author: Davey AlbaDavey Alba 









Business 
Sorry, Banning ‘Killer Robots’ Just Isn’t Practical 
Author: Tom SimoniteTom Simonite 








Business 
FCC Pledges Openness -- Just Don't Ask To See Complaints 
Author: Klint FinleyKlint Finley 









Business 
Proposed California Law Targets Sexual Harassment in VC 
Author: Nitasha TikuNitasha Tiku 








Video 
Instagram's CEO on Free Speech, AI, and Internet Addiction 
Author: Nicholas ThompsonNicholas Thompson 










Business 
When Government Rules by Software, Citizens Are Left in the Dark 
Author: Tom SimoniteTom Simonite 






We Recommend 
Powered By Outbrain Wired StaffOur 12 Favorite Laptops, From MacBooks to Chromebooks 


Scott RosenbergBitcoin Makes Even Smart People Feel Dumb | Backchannel 


Jack StewartThe Latest Flying Car Concept Seems—Dare We Say It—Serious 


David PierceThe New iMac Pro Is Apple’s Most Bonkers Supercomputer Ever 


SponsoredInvesting in the Source: Automation Technologies Are the Next Big Thing 
Google 












Get Our Newsletter 
WIRED’s big story deliver to your inbox. 

submit 





We're on 
Pinterest 
See what's inspire us. 
Follow 













Login 
Subscribe 
Advertise 
Site Map 
Press Center 
FAQ 
Accessibility Help 
Customer Care 
Contact Us 
Securedrop 
T-Shirt Collection 
Newsletter 
Wired Staff 
Jobs 
RSS 

CNMN Collection 
Use of this site constitutes acceptance of our user agreement (effective 3/21/12) and privacy policy (effective 3/21/12). Affiliate link policy. Your California privacy rights. 
The material on this site may not be reproduced, distributed, 
transmitted, cached or otherwise used, except with the prior write permission of Condé Nast. 






















