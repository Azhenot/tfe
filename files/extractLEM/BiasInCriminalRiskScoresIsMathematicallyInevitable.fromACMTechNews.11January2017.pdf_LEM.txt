






































Bias in criminal risk score be mathematically inevitableÂ€|Â€New Orleans' Multicultural News Source | The Louisiana Weekly 


6J’aimeJ’aime Tweet 

StumbleUpon 

Wednesday, January 11th, 2017 

Judicials 

EDUCATION 
ENTERTAINMENT 
HEALTH 
OPINIONS 
Advertise 
Printing Service 
Shop 
Subscribe 
Videos 

Filed Under: National 

Bias in criminal risk score be mathematically inevitable 

9th January 2017 · 0 Comments 

By Julia Angwin and Jeff Larson 
ProPublica 

The racial bias that ProPublica found in a formula use by court and parole board to forecast future criminal behavior arises inevitably 
from the test’s design, accord to new research. 

The finding be described in scholarly paper publish or circulate over the past several months. Taken together, they represent the 
most far-reaching critique to date of the fairness of algorithm that seek to provide an objective measure of the likelihood a defendant will 

commit further crimes. 

Increasingly, criminal justice official be use similar risk prediction equation to inform their decision about bail, sentence and early 
release. 

ShareShare 1 

Bias in criminal risk score be mathematically inevitable | New Orleans' ... http://www.louisianaweekly.com/bias-in-criminal-risk-scores-is-mathema... 

1 sur 5 11-01-17 19:32 



The researcher found that the formula, and others like it, have be write in a way that guarantee black defendant will be 
inaccurately identify a future criminal more often than their white counterparts. 

The studies, by four group of scholar work independently, suggest the possibility that the widely use algorithm could be revise to 
reduce the number of Blacks who be unfairly categorize without sacrifice the ability to predict future crimes. 

The author of one of the paper say that her ongoing research suggests that this result could be achieve through a modest change in the 
work of the formula ProPublica studied, which be know a COMPAS. 

An article publish early this year by ProPublica (and be feature on the front page The Louisiana Weekly on June 6, 2016) focus 
attention on possible racial bias in the COMPAS algorithm. We collect the COMPAS score for more than 10,000 people arrest for 
crime in Florida’s Broward’s County and checked to see how many be charge with further crime within two years. 

When we look at the people who do not go on to be arrest for new crime but be dubbed high risk by the formula, we found a 
racial disparity. The data show that Black defendant be twice a likely to be incorrectly label a high risk than white defendants. 
Conversely, white defendant label low risk be far more likely to end up be charge with new offense than Blacks with 
comparably low COMPAS risk scores. 

Northpointe, the company that sell COMPAS, say in response that the test be racially neutral. To support that assertion, company 
official point to another of our findings, which be that the rate of accuracy for COMPAS score — about 60 percent — be the same 
for Black and white defendants. The company say it have devise the algorithm to achieve this goal. A test that be correct in equal 
proportion for all group cannot be biased, the company said. 

This question of how an algorithm could simultaneously be fair and unfair intrigue some of the nation’s top researcher at Stanford 
University, Cornell University, Harvard University, Carnegie Mellon University, University of Chicago and Google. 

The scholar set out to address this question: Since Blacks be re-arrested more often than whites, be it possible to create a formula that be 
equally predictive for all race without disparity in who suffers the harm of incorrect predictions? 

Working separately and use different methodologies, four group of scholar all reach the same conclusion. It’s not. 

Revealing their preliminary finding on a Washington Post blog, a group of Stanford researcher wrote: “It’s actually impossible for a risk 
score to satisfy both fairness criterion at the same time.” 

The problem, several say in interviews, arises from the characteristic that criminologist have use a the cornerstone for create fair 
algorithms, which be that formula must generate equally accurate forecast for all racial groups. 

The researcher found that an algorithm craft to achieve that goal, know a “predictive parity,” inevitably lead to disparity in what 
sort of people be incorrectly classify a high risk when two group have different arrest rates. 

“‘Predictive parity’ actually corresponds to ‘optimal discrimination,’” say Nathan Srebro, associate professor of computer science at the 
University of Chicago and the Toyota Technological Institute at Chicago. That’s because predictive parity result in a high proportion of 
Black defendant be wrongly rat a high-risk. 

Srebro’s research paper, “Equality of Opportunity in Supervised Learning,” be co-authored with Google research scientist Moritz Hardt 
and University of Texas at Austin computer science professor Eric Price in October. Their paper propose a definition of 
“nondiscrimination” that require the error rate between group be equalized. Otherwise, Srebro said, one group end up “paying the 
price for the uncertainty” of the algorithm. 

The need to look at the harm that arise when a test be inaccurate arises frequently in statistics, particularly in field like health care. 
When researcher weigh the merit of exam like mammograms, they want to know both how often they correctly detect breast cancer 
and how often they falsely indicate that patient have the disease. 

False finding be significant in medicine because they can cause patient to unnecessarily undergo painful procedure like breast 
biopsies. It’s entirely possible that a test could correctly identify most breast cancers, show what’s know a “positive predictive 
value,” and yet make so many mistake that it be view a unusable. 

When he first heard about the COMPAS debate, Jon Kleinberg, a computer science professor at Cornell University, hop he could figure 
out a way to reduce false finding while keep the positive predictive value intact. “We thought, can we fix it?” he said. 

But after he, his graduate student Manish Raghavan and Harvard economics professor Sendhil Mullainathan download and crunched 
ProPublica’s data, they realize that the problem be not resolvable. A risk score, they found, could either be equally predictive or 
equally wrong for all race — but not both. 

Bias in criminal risk score be mathematically inevitable | New Orleans' ... http://www.louisianaweekly.com/bias-in-criminal-risk-scores-is-mathema... 

2 sur 5 11-01-17 19:32 



The reason be the difference in the frequency with which Blacks and white be charge with new crimes. ““If you have two 
population that have unequal base rates,’’ Kleinberg said, “then you can’t satisfy both definition of fairness at the same time.” 

Kleinberg and his colleague go on to construct a mathematical proof that the two notion of fairness be incompatible. The paper, 
“Inherent Trade-Offs in the Fair Determination of Risk Scores” be post online in September. 

In the criminal justice context, false finding can have far-reaching effect on the life of people charge with crimes. Judges, prosecutor 
and parole board use the score to help decide whether defendant can be sent to rehab program instead of prison or be give shorter 
sentences. 

Defendants inaccurately class a “high risk’’ and deem more likely to be arrest in the future may be treat more harshly than be 
just or necessary, say Alexandra Chouldechova, Assistant Professor of Statistics & Public Policy at Carnegie Mellon University, who 
also study ProPublica’s COMPAS findings. 

Chouldechova say focus on outcome might be a good definition of fairness. To create equal outcomes, she said, “You would have to 
treat people differently.” Chouldechova’s paper, “Fair prediction with disparate impact: A study of bias in recidivism prediction 
instruments,” be post online in October. 

Chouldechova be continue to research way to improve the likelihood of equal outcomes. 

Using the Broward County data ProPublica make public, Chouldechova rearrange how the COMPAS score be interpret so that they 
be wrong equally often about Black and white defendants. 

This shift meant that the algorithm’s prediction of future criminal behavior be no longer the same for all races. Chouldechova say her 
revise formula be unchanged for white defendant (59 percent correct) while it predictive accuracy rise from 63 to 69 percent for 
Black defendants. 

Northpointe, the company that sell the COMPAS tool, say it have no comment on the critiques. And official in Broward County say 
they have make no change in how they use the COMPAS score in response to both ProPublica’s initial finding and the research paper 
that followed. 

This article originally publish in the January 9, 2017 print edition of The Louisiana Weekly newspaper. 

Readers Comments (0) 

You must be log in to post a comment. 

COMMUNITY CALENDAR 

UNCF Walk for Education reschedule for Nov. 2 
Mount Zion UMC celebrates it 142nd Anniversary this month 
21st Century Democracy: How Politics Will Change During the Next 100 Years 
Firehouse Ministries Multicultural Fest 
Ruth G. Carter 20th Annual Spring Concert 

View All Events 
PHOTO GALLERIES 
WEATHER 
POLLS 
GAMES 

Bias in criminal risk score be mathematically inevitable | New Orleans' ... http://www.louisianaweekly.com/bias-in-criminal-risk-scores-is-mathema... 

3 sur 5 11-01-17 19:32 


