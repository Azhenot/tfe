













































ETS R&D Connections: E-rater a a Quality Control on Human Scores (April 2005) 


Connections 
April 2005 



Listening. Learning. Leading. 

E-rater a a 
Quality Control on 
Human Scores 
William Monaghan and Brent Bridgeman 

Can natural language processing evaluate the 
quality of writing? 

Should computer replace human in analyze 
student essays? 

The answer depend on whom you ask. 

Opponents of automate essay evaluation 
system claim that computer lack the intrinsic 
human capacity to determine good write from 
bad. However, test organization see such 
capability a be a necessity to efficiently 
score essay test (Flam, 2004). A suitable 
compromise would be to have human reader 
score essay in tandem with an automate essay 
evaluation system, such a the ETS-developed 
e-rater®. The approach benefit those in the 
test industry by create less reliance on 
expensive reading and lessens the concern of 
critics, a human reader be an integral element 
in the system. 

The debate over the efficacy of use an essay 
format in test have a long history (Cooper, 1984). 
Testing programs, such a the Graduate Record 
Examinations® (GRE®) program, have come to 
recognize that essay can play an important role 
a indicator of student ability and have add 
essay section (Powers, Fowles, & Welsh, 1999). 
The Advanced Placement Program® have always 
utilized essays, and the College Board® have add 
an essay section to the SAT®. 

For those in the test industry, however, essay 
present a practical problem—how to efficiently 


develop, administer, and score test with essay 
sections. This paper focus on the score of 
essay and the role automate essay evaluation 
system can play in the process. 

Why Automated Essay Scoring 

ETS make it mark by standardize and then 
automate much of the test process. This be 
do out of necessity a much a for create 
system in which all test taker can demonstrate 
their proficiency in a common, fair way. Few 
reasonable option be available to administer 
test to million of student and complete the 
reporting of score in a timely manner. While 
ETS have focus primarily on multiple-choice 
test in these efforts, the organization have be a 
pioneer in way to use essay in such testing. 

When use essay for assessment purposes, 
ETS have found that have a single essay 
question or prompt and a single reader per essay 
do not produce reliable score (Breland, 
Bridgeman, & Fowles, 1999). The remedy be to 
have test taker write two essay if not more and 
to have at least two people read and rate each 
essay. Scoring cost for such a test be 
substantial. ETS have held annual massive 
reading for some of it test administration 
involve essays. This have meant bring 
together a small army of educator to a single 
location and have them read through and score 
essay after essay. Even move such a system 
online require hour of training and logistical 
support for each rater. Recruitment for each of 
these system alone can be a daunt task a 
qualify individual be relatively few, and they 
usually have press schedules. Compensation 
for the raters' time and possible travel be a huge 
expense that be pass along to test taker a 
addition to their registration fees. 

That be why the organization have invest in 
and developed automate essay evaluation 





Page 2 of 4 

capability such a e-rater. In the e-rater system, 
the computer be fed thousand of essay that 
human raters have scored. The essay range from 
those deem to be high-quality response to 
one see to be less than adequate. To score an 
essay, the system be set up to look for pattern 
that be evident in good essays. The system 
accomplishes this task in seconds. Studies show 
a high level of agreement between the score 
human raters assign to an essay and what e-rater 
award (Attali & Burstein, 2005). 

Text vs. Context 
Even with this high-level of agreement and 
e-rater's apparent efficiency, a number of people 
still object to the idea of automate essay 
evaluation. They argue, and rightly so, that such 
system can be fool by clever nonsense or the 
inclusion of well-constructed sentence that 
together make no sense at all. This assumes that a 
human reader, who would detect such cases, be 
not in the score model at all. The opposite fear 
be to have brilliant write construct in such a 
nonconformist manner that the machine assigns a 
poor score. Again, a reader should be an effective 
guard against such a situation. Of course, student 
seek instruction would have little to gain in 
use e-rater outside of it intend function. 

Another worry be that automate essay system 
might be less valid for use in the score of 
essay write by English language learners. Will 
a machine that be train on the write of native 
English speaker work in a situation where the 
majority of the test population doesn't speak 
English a a native language? Will system like 
e-rater have the same kind of validity in such 
instances? 

Bridgeman (2004) say that a possible solution be 
to use e-rater to check the score assign by 
human raters. By have e-rater run in the 
background, the score e-rater provide can be 
compare to the one assign by a single human 
rater. If there be no discrepancy, the score stands. 
If the score be discrepant, a second human 
reader receives the essay to see if a factor such a 
fatigue affected the score the first rater assign 
or if the essay have element that be unduly 
influence the automate system. In this system, 
the essay score would always be base solely on 
human raters. 

The approach allows test organization to 
streamline the essay evaluation process while still 
provide valid score reporting. 

Testing e-rater a Quality Control 

To test his model, Bridgeman (2004) turn to 
the GRE analytical write section, which have 
each test taker write two essays—one on an issue 
prompt and the other on an argument prompt. Jill 
Burstein, the lead scientist on the e-rater system 
and a computational linguist, developed e-rater 
score model for more than 100 prompt of 
each type (issue and argument). For the issue 
prompts, the e-rater score agree with the score 
assign by a human rater at the same rate that 
one human agree with another. For the argument 
prompts, agreement of e-rater and human raters 
be slightly lower, but still quite high. The 
correlation between the score assign by two 
human be .81, and the correlation of a human 
score and e-rater score be .76. 

To evaluate the effectiveness of use e-rater a 
an additional score or a a check on the score 
from one human rater per prompt, Bridgeman 
study 5,950 examinee who have take the GRE 
analytical write section twice. He use the final 
score base on at least four human rating (two 
for each prompt) from one administration a 
the criterion.1 This criterion provide an estimate 
of write ability that be totally independent of 
the estimate make from use e-rater either a 
an additional rater or a a check. The criterion 
be predict from score on a different 
administration that be base on two human 


1 A single score be report for the analytical write 

section. Each essay receives a score from two 
train reader use a 6-point holistic scale. In 
holistic scoring, reader be train to assign score 
on the basis of the overall quality of an essay in 
response to the assign task. If the two assign 
score differ by more than 1 point on the scale, the 
discrepancy be adjudicate by a third GRE reader. 
Otherwise, the score from the two reading of 
an essay be averaged. The final score be base on 
two essay (one a response to an issue prompt and 
the other a response to an argument prompt) that be 
then average and round up to the near half- 
point interval (e.g., 3.0, 3.5). 





Page 3 of 4 

per prompt, one human per prompt, one human 
with the e-rater check procedure result in a 
second human rating about 15% of the time, or 
one human plus e-rater. Results be summarize 
in Table 1. 

The high agreement, even high than two 
human reader per prompt, be found when the 
score assign from one human reader be 
combine with the e-rater score. But if test user 
be uncomfortable with have a score assign 
by a machine be part of a person’s score, the 
checked human approach result in agreement 
rate that be nearly a high. 

Summary 

Automated essay evaluation systems, such a 
e-rater, have a very high threshold to meet to 
gain people's full confidence a a valid score 
approach. This skepticism be healthy, and until 
these system reach a level of sophistication to 
make such concern unwarranted, educational 
measurement organization should be judicious 
in the use of these systems, especially in 
assessment that help in make high-stakes 
decisions, such a those use in admissions. 

However, automate essay evaluation system 
do have value if properly used. One such valid 
application, a this paper establishes, be a a 
quality control check on human rating essay 
prompts. To produce reliable score when use 
essay in assessment, multiple topic and 
multiple reader be necessary. Arranging for 
human reader be a time-consuming and costly 
task and one for which educational measurement 

organization can considerably lessen the burden 
with e-rater. The result described here show that 
the high reliability be obtain by combine 
a human reader's score with that generate by 
e-rater. 

ETS have and continue to explore other us for 
e-rater a it work to perfect the system. Even 
this seemingly limited usage of this capability 
can reap award by make essay score more 
efficient and less costly. Of course, test taker 
be the ultimate beneficiaries, a they will have 
another avenue besides multiple-choice test to 
demonstrate their true abilities. 

References 
Attali, Y., & Burstein, J. (2005). Automated essay score 

with e-rater v.2.0 (ETS RR-04-45). Princeton, NJ: ETS. 

Breland, H. M., Bridgeman, B., & Fowles, M. E. (1999). 
Writing assessment in admission to high education: 
Review and framework (College Board Research Rep. 
No. 99-03, GRE Board Research Rep. No. 96-12R, 
ETS RR-99-03). New York: College Entrance 
Examination Board. 

Bridgeman, B. (2004, December). E-rater a a quality 
control on human scorers. Presentation in the ETS 
Research Colloquium Series, Princeton, NJ. 

Cooper, P. L. (1984) The assessment of write ability: A 
review of research (GRE Board Research Report No. 
82-15R, ETS RR-84-12). Princeton, NJ: ETS. 

Flam, F. (2004, August 30). An apple for the computer. The 
Philadelphia Inquirer, p. D-01. 

Powers, D. E., Fowles, M. E., & Welsh, C. K. (1999). 
Further validation of a write assessment for graduate 
admission (GRE Board Research Rep. No. 96-13R, 
ETS RR-99-18). Princeton, NJ: ETS. 



Table 1 

Agreement When Criterion Is Analytical Writing Total From a Different Administration 

Readers per prompt Within ½ point Within 1 point 

2 human 76.6% 94.0% 

1 human 72.9% 92.5% 

Checked human 75.5% 93.9% 

1 human + e-rater 77.7% 94.2% 





Page 4 of 4 

How e-rater Works 

Earlier version of e-rater have some 50 features, 
and a subset of these feature would be select 
to score the particular set of essays. The newer 
version of e-rater us a fix set of about 
10 feature in seven category from which it 
derives the final score. 

Explanation of the seven score category 

• Grammar score – base on error such a those 
in subject-verb agreement among others 

• Mechanics score – derive from error in 
spell and other like error 

• Usage score – base on such error a article 
error and confuse word (an example would 
be an instance in which the essay writer us a 
word that although phonetically similar have a 
different meaning from the intend word; 
use "to" where it would have be proper to 
use "too") 

• Style score – base on instance of overly 
repeat word and the number of very long or 
very short sentence a well a other such 
feature 

• Lexical complexity score – drawn from 
information such a the level of vocabulary the 
essay writer us in the essay 











• Organization/development score – base on the 
identification of sentence that correspond to 
the background, thesis, main idea, support 
idea, and conclusion 

• Prompt-specific vocabulary usage score – 
derive from e-rater's evaluation of the word 
choice in an essay and the similarity to the 
word choice in sample of low- to high-quality 
essay write on the same topic 

In addition to these seven score categories, 
essay length also may be consider and 
weight in a control way. 


R&D Connections be publish by 

ETS Research & Development 
Educational Testing Service 
Rosedale Road, 19-T 
Princeton, NJ 08541-0001 

Send comment about this publication to the above address or via the 
Web at: 

http://www.ets.org/research/contact.html 

Copyright © 2005 by Educational Testing Service. All right reserved. 
Educational Testing Service be an Affirmative Action/Equal Opportunity 
Employer. 

Educational Testing Service, ETS, and the ETS logo, e-rater, Graduate 
Record Examinations, and GRE be register trademark of 
Educational Testing Service. 

College Board, Advanced Placement Program, and SAT be register 
trademark of the College Entrance Examination Board. 

L i s t e n i n g . 
L ea rn i n g . 
L e a d i n g . 


