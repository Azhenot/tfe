






































Can A.I. Be Taught to Explain Itself? 


Can A.I. Be Taught to Explain Itself? 

Cliff Kuang 

In September, Michal Kosinski publish a study that he fear 

might end his career. The Economist broke the news first, give it a 

self-consciously anodyne title: “Advances in A.I. Are Used to Spot Signs 

of Sexuality.” But the headline quickly grow more alarmed. By the next 

day, the Human Rights Campaign and Glaad, formerly know a the 

Gay and Lesbian Alliance Against Defamation, have label Kosinski’s 

work “dangerous” and “junk science.” (They claimed it have not be 

peer reviewed, though it had.) In the next week, the tech-news site The 

Verge have run an article that, while carefully reported, be nonetheless 

topped with a scorch headline: “The Invention of A.I. ‘Gaydar’ Could 

Be the Start of Something Much Worse.” 

Kosinski have make a career of warn others about the us and 

potential abuse of data. Four year ago, he be pursue a Ph.D. in 

psychology, hop to create good test for signature personality trait 

like introversion or openness to change. But he and a collaborator soon 

realize that Facebook might render personality test superfluous: 

Instead of ask if someone like poetry, you could just see if they 

“liked” Poetry Magazine. In 2014, they publish a study show that if 

give 200 of a user’s likes, they could predict that person’s personality- 

test answer good than their own romantic partner could. 

After get his Ph.D., Kosinski land a teach position at the 

Stanford Graduate School of Business and soon start look for new 

data set to investigate. One in particular stood out: faces. For decades, 

psychologist have be leery about associate personality trait with 

physical characteristics, because of the last taint of phrenology and 

eugenics; study face this way was, in essence, a taboo. But to 

understand what that taboo might reveal when questioned, Kosinski 

knew he couldn’t rely on a human judgment. 

Kosinski first mine 200,000 publicly post date profiles, complete 

with picture and information range from personality to political 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

1 sur 13 30-11-17 à 18:55 



views. Then he pour that data into an open-source facial-recognition 

algorithm — a so-called deep neural network, built by researcher at 

Oxford University — and ask it to find correlation between people’s 

face and the information in their profiles. The algorithm fail to turn 

up much, until, on a lark, Kosinski turn it attention to sexual 

orientation. The result almost defy belief. In previous research, the 

best any human have do at guess sexual orientation from a profile 

picture be about 60 percent — slightly good than a coin flip. Given 

five picture of a man, the deep neural net could predict his sexuality 

with a much a 91 percent accuracy. For women, that figure be low 

but still remarkable: 83 percent. 

Much like his early work, Kosinski’s finding raise question about 

privacy and the potential for discrimination in the digital age, 

suggest scenario in which good program and data set might be 

able to deduce anything from political leaning to criminality. But there 

be another question at the heart of Kosinski’s paper, a genuine 

mystery that go almost ignore amid all the medium response: How 

be the computer do what it did? What be it see that human 

could not? 

Photo illustration by Derek Brahney. Source photo: Howard Sochurek/The 

Life Picture Collection/Getty Images. 

It be Kosinski’s own research, but when he try to answer that 

question, he be reduce to a painstaking hunt for clues. At first, he 

try cover up or exaggerate part of faces, try to see how those 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

2 sur 13 30-11-17 à 18:55 



change would affect the machine’s predictions. Results be 

inconclusive. But Kosinski knew that women, in general, have big 

foreheads, thinner jaw and longer nose than men. So he have the 

computer spit out the 100 face it deem most likely to be gay or 

straight and average the proportion of each. It turn out that the 

face of gay men exhibit slightly more “feminine” proportions, on 

average, and that the converse be true for women. If this be accurate, 

it could support the idea that testosterone level — already know to 

mold facial feature — help mold sexuality a well. 

But it be impossible to say for sure. Other evidence seem to suggest 

that the algorithm might also be pick up on culturally driven traits, 

like straight men wear baseball hat more often. Or — crucially — 

they could have be pick up on element of the photo that human 

don’t even recognize. “Humans might have trouble detect these tiny 

footprint that border on the infinitesimal,” Kosinski says. “Computers 

can do that very easily.” 

It have become commonplace to hear that machines, arm with 

machine learning, can outperform human at decidedly human tasks, 

from play Go to play “Jeopardy!” We assume that be because 

computer simply have more data-crunching power than our soggy 

three-pound brains. Kosinski’s result suggest something stranger: 

that artificial intelligence often excel by develop whole new way of 

seeing, or even thinking, that be inscrutable to us. It’s a more profound 

version of what’s often call the “black box” problem — the inability to 

discern exactly what machine be do when they’re teach 

themselves novel skill — and it have become a central concern in 

artificial-intelligence research. In many arenas, A.I. method have 

advanced with startle speed; deep neural network can now detect 

certain kind of cancer a accurately a a human. But human doctor 

still have to make the decision — and they won’t trust an A.I. unless it 

can explain itself. 

This isn’t merely a theoretical concern. In 2018, the European Union 

will begin enforce a law require that any decision make by a 

machine be readily explainable, on penalty of fine that could cost 

company like Google and Facebook billion of dollars. The law be 

write to be powerful and broad and fails to define what constitutes a 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

3 sur 13 30-11-17 à 18:55 



satisfy explanation or how exactly those explanation be to be 

reached. It represent a rare case in which a law have manage to leap 

into a future that academic and tech company be just begin to 

devote concentrate effort to understanding. As researcher at Oxford 

dryly noted, the law “could require a complete overhaul of standard and 

widely use algorithmic techniques” — technique already permeate 

our everyday lives. 

Those technique can seem inescapably alien to our own way of 

thinking. Instead of certainty and cause, A.I. work off probability and 

correlation. And yet A.I. must nonetheless conform to the society we’ve 

built — one in which decision require explanations, whether in a court 

of law, in the way a business be run or in the advice our doctor give us. 

The disconnect between how we make decision and how machine 

make them, and the fact that machine be make more and more 

decision for us, have birth a new push for transparency and a field of 

research call explainable A.I., or X.A.I. Its goal be to make machine 

able to account for the thing they learn, in way that we can 

understand. But that goal, of course, raise the fundamental question of 

whether the world a machine see can be make to match our own. 

“Artificial intelligence” be a misnomer, an airy and evocative term 

that can be shade with whatever notion we might have about what 

“intelligence” be in the first place. Researchers today prefer the term 

“machine learning,” which good describes what make such algorithm 

powerful. Let’s say that a computer program be decide whether to give 

you a loan. It might start by compare the loan amount with your 

income; then it might look at your credit history, marital status or age; 

then it might consider any number of other data points. After 

exhaust this “decision tree” of possible variables, the computer will 

spit out a decision. If the program be built with only a few example 

to reason from, it probably wouldn’t be very accurate. But give 

million of case to consider, along with their various outcomes, a 

machine-learning algorithm could tweak itself — figure out when to, 

say, give more weight to age and less to income — until it be able to 

handle a range of novel situation and reliably predict how likely each 

loan be to default. 

Machine learn isn’t just one technique. It encompasses entire 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

4 sur 13 30-11-17 à 18:55 



family of them, from “boosted decision trees,” which allow an 

algorithm to change the weight it give to each data point, to 

“random forests,” which average together many thousand of randomly 

generate decision trees. The sheer proliferation of different techniques, 

none of them obviously good than the others, can leave researcher 

flummoxed over which one to choose. Many of the most powerful be 

bafflingly opaque; others evade understand because they involve an 

avalanche of statistical probability. It can be almost impossible to peek 

inside the box and see what, exactly, be happening. 

Rich Caruana, an academic who work at Microsoft Research, have spent 

almost his entire career in the shadow of this problem. When he be 

earn his Ph.D at Carnegie Mellon University in the 1990s, his thesis 

adviser ask him and a group of others to train a neural net — a 

forerunner of the deep neural net — to help evaluate risk for patient 

with pneumonia. Between 10 and 11 percent of case would be fatal; 

others would be less urgent, with some percentage of patient 

recover just fine without a great deal of medical attention. The 

problem be figure out which case be which — a high-stakes 

question in, say, an emergency room, where doctor have to make quick 

decision about what kind of care to offer. Of all the machine-learning 

technique student apply to this question, Caruana’s neural net be 

the most effective. But when someone on the staff of the University of 

Pittsburgh Medical Center ask him if they should start use his 

algorithm, “I say no,” Caruana recalls. “I say we don’t understand 

what it do inside. I say I be afraid.” 

The problem be in the algorithm’s design. Classical neural net focus 

only on whether the prediction they give be right or wrong, tweak 

and weigh and recombine all available morsel of data into a 

tangle web of inference that seem to get the job done. But some of 

these inference could be terrifically wrong. Caruana be particularly 

concerned by something another graduate student notice about the 

data they be handling: It seem to show that asthmatic with 

pneumonia fare good than the typical patient. This correlation be 

real, but the data masked it true cause. Asthmatic patient who 

contract pneumonia be immediately flag a dangerous cases; if they 

tend to fare better, it be because they get the best care the hospital 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

5 sur 13 30-11-17 à 18:55 



could offer. A dumb algorithm, look at this data, would have simply 

assume asthma meant a patient be likely to get good — and thus 

conclude that they be in less need of urgent care. 

“I knew I could probably fix the program for asthmatics,” Caruana says. 

“But what else do the neural net learn that be equally wrong? It 

couldn’t warn me about the unknown unknowns. That tension have 

bother me since the 1990s.” 

The story of asthmatic with pneumonia eventually become a legendary 

allegory in the machine-learning community. Today, Caruana be one of 

perhaps a few dozen researcher in the United States dedicate to 

find more transparent new approach to machine learning. For the 

last six years, he have be create a new model that combine a 

number of machine-learning techniques. The result be a accurate a his 

original neural network, and it can spit out chart that show how each 

individual variable — from asthma to age — be predictive of mortality 

risk, make it easy to see which one exhibit particularly unusual 

behavior. Immediately, asthmatic be reveal a a far outlier. Other 

strange truth surface, too: For example, risk for people age 100 go 

down suddenly. “If you make it to this round number of 100,” Caruana 

says, “it seem a if the doctor be saying, ‘Let’s try to get you 

another year,’ which might not happen if you’re 93.” 

Caruana may have brought clarity to his own project, but his solution 

only underscored the fact the explainability be a kaleidoscopic problem. 

The explanation a doctor need from a machine isn’t the same a the 

one a fighter pilot might need or the one an N.S.A. analyst sniff out a 

financial fraud might need. Different detail will matter, and different 

technical mean will be need for find them. You couldn’t, for 

example, simply use Caruana’s technique on facial data, because they 

don’t apply to image recognition. There may, in other words, eventually 

have to be a many approach to explainability a there be approach 

to machine learn itself. 

Three year ago, David Gunning, one of the most consequential 

people in the emerge discipline of X.A.I., attend a brainstorming 

session at a state university in North Carolina. The event have the title 

“Human-Centered Big Data,” and it be sponsor by a government- 

fund think tank call the Laboratory for Analytic Sciences. The idea 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

6 sur 13 30-11-17 à 18:55 



be to connect lead A.I. researcher with expert in data 

visualization and human-computer interaction to see what new tool 

they might invent to find pattern in huge set of data. There to judge 

the ideas, and act a hypothetical users, be analyst for the C.I.A., the 

N.S.A. and sundry other American intelligence agencies. 

The researcher in Gunning’s group step confidently up to the white 

board, show off new, more powerful way to draw prediction from a 

machine and then visualize them. But the intelligence analyst 

evaluate their pitches, a woman who couldn’t tell anyone in the room 

what she do or what tool she be using, wave it all away. Gunning 

remembers her a plainly dressed, middle-aged, typical of the countless 

government agent he have know who toil thanklessly in critical jobs. 

“None of this solves my problem,” she said. “I don’t need to be able to 

visualize another recommendation. If I’m go to sign off on a 

decision, I need to be able to justify it.” She be issue what amount 

to a broadside. It wasn’t just that a clever graph indicate the best 

choice wasn’t the same a explain why that choice be correct. The 

analyst be point to a legal and ethical motivation for explainability: 

Even if a machine make perfect decisions, a human would still have to 

take responsibility for them — and if the machine’s rationale be 

beyond reckoning, that could never happen. 

Gunning, a grandfatherly military man whose buzz cut have survive his 

stint a a civilian, be a program manager at the Defense Advanced 

Research Projects Agency. He work in Darpa’s shiny new midrise tower 

in downtown Alexandria, Va. — an office indistinguishable from the 

others nearby, except that the security guard out front will take away 

your cellphone and warn you that turn on the Wi-Fi on your laptop 

will make security personnel materialize within 30 seconds. Darpa 

manager like Gunning don’t have permanent jobs; the expectation be 

that they serve four-year “tours,” dedicate to funding cutting-edge 

research along a single line of inquiry. When he found himself at the 

brainstorming session, Gunning have recently complete his second tour 

a a sort of Johnny Appleseed for A.I.: Starting in the 1990s, he have 

found hundred of projects, from the first application of machine- 

learn technique to the internet, which presage the first search 

engines, to the project that eventually spun off a Siri, Apple’s voice- 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

7 sur 13 30-11-17 à 18:55 



control assistant. “I’m proud to be a dinosaur,” he say with a smile. 

As of now, most of the military’s practical application of such 

technology involve perform enormous calculation beyond the reach 

of human patience, like predict how to route supplies. But there be 

more ambitious application on the horizon. One recent research 

program try to use machine learn to sift through million of video 

clip and internet message in Yemen to detect cease-fire violations; if 

the machine do find something, it have to be able to describe what’s 

worth pay attention to. Another press need be for drone fly on 

self-directed mission to be able to explain their limitation so that the 

human command the drone know what the machine can — and 

cannot — be ask to do. Explainability have thus become a hurdle for a 

wealth of possible projects, and the Department of Defense have begin 

to turn it eye to the problem. 

After that brainstorming session, Gunning take the analyst’s story back 

to Darpa and soon sign up for his third tour. As he flew across the 

country meeting with computer scientist to help design an overall 

strategy for tackle the problem of X.A.I., what become clear be that 

the field need to collaborate more broadly and tackle grander 

problems. Computer science, have leapt beyond the bound of 

consider purely technical problems, have to look further afield — to 

experts, like cognitive scientists, who study the way human and 

machine interact. 

This represent a full circle for Gunning, who begin his career a a 

cognitive psychologist work on how to design good automate 

system for fighter pilots. Later, he begin work on what’s now call 

“old-fashioned A.I.” — so-called expert system in which machine be 

give voluminous list of rules, then tasked with draw conclusion by 

recombine those rules. None of those effort be particularly 

successful, because it be impossible to give the computer a set of rule 

long enough, or flexible enough, to approximate the power of human 

reasoning. A.I.’s current blossoming come only when researcher begin 

invent new technique for let machine find their own pattern 

in the data. 

Gunning’s X.A.I. initiative, which kick off this year, provide $75 

million in funding to 12 new research programs; by the power of the 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

8 sur 13 30-11-17 à 18:55 



purse strings, Gunning have refocus the energy of a significant part 

of the American A.I. research community. His hope be that by make 

these new A.I. method accountable to the demand of human 

psychology, they will become both more useful and more powerful. “The 

real secret be find a way to put label on the concept inside a deep 

neural net,” he says. If the concept inside can be labeled, then they can 

be use for reason — just like those expert system be suppose to 

do in A.I.’s first wave. 

Deep neural nets, which evolve from the kind of technique that 

Rich Caruana be experiment with in the 1990s, be now the class of 

machine learn that seem most opaque. Just like old-fashioned 

neural nets, deep neural network seek to draw a link between an input 

on one end (say, a picture from the internet) and an output on the other 

end (“This be a picture of a dog”). And just like those old neural nets, 

they consume all the example you might give them, form their own 

web of inference that can then be apply to picture they’ve never see 

before. Deep neural net remain a hotbed of research because they have 

produce some of the most breathtaking technological 

accomplishment of the last decade, from learn how to translate 

word with better-than-human accuracy to learn how to drive. 

To create a neural net that can reveal it inner workings, the researcher 

in Gunning’s portfolio be pursue a number of different paths. Some 

of these be technically ingenious — for example, design new kind of 

deep neural network make up of smaller, more easily understood 

modules, which can fit together like Legos to accomplish complex tasks. 

Others involve psychological insight: One team at Rutgers be design a 

deep neural network that, once it make a decision, can then sift 

through it data set to find the example that best demonstrates why it 

make that decision. (The idea be partly inspire by psychological study 

of real-life expert like firefighters, who don’t clock in for a shift 

thinking, These be the 12 rule for fight fires; when they see a fire 

before them, they compare it with one they’ve see before and act 

accordingly.) Perhaps the most ambitious of the dozen different project 

be those that seek to bolt new explanatory capability onto exist 

deep neural networks. Imagine give your pet dog the power of speech, 

so that it might finally explain what’s so interest about squirrels. Or, 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

9 sur 13 30-11-17 à 18:55 



a Trevor Darrell, a lead investigator on one of those teams, sum it up, 

“The solution to explainable A.I. be more A.I.” 

Five year ago, Darrell and some colleague have a novel idea for let 

an A.I. teach itself how to describe the content of a picture. First, they 

create two deep neural networks: one dedicate to image recognition 

and another to translate languages. Then they lash these two 

together and fed them thousand of image that have caption attach 

to them. As the first network learn to recognize the object in a 

picture, the second simply watch what be happen in the first, 

then learn to associate certain word with the activity it saw. Working 

together, the two network could identify the feature of each picture, 

then label them. Soon after, Darrell be present some different work 

to a group of computer scientist when someone in the audience raise 

a hand, complain that the technique he be describe would never 

be explainable. Darrell, without a second thought, said, Sure — but you 

could make it explainable by once again lash two deep neural 

network together, one to do the task and one to describe it. 

Darrell’s previous work have piggybacked on picture that be already 

captioned. What he be now propose be create a new data set and 

use it in a novel way. Let’s say you have thousand of video of baseball 

highlights. An image-recognition network could be train to spot the 

players, the ball and everything happen on the field, but it wouldn’t 

have the word to label what they were. But you might then create a new 

data set, in which volunteer have write sentence describe the 

content of every video. Once combined, the two network should then 

be able to answer query like “Show me all the double play involve 

the Boston Red Sox” — and could potentially show you what cues, like 

the logo on uniforms, it use to figure out who the Boston Red Sox are. 

Call it the Hamlet strategy: lending a deep neural network the power of 

internal monologue, so that it can narrate what’s go on inside. But do 

the concept that a network have taught itself align with the reality that 

human be describing, when, for example, narrate a baseball 

highlight? Is the network recognize the Boston Red Sox by their logo 

or by some other obscure signal, like “median facial-hair distribution,” 

that just happens to correlate with the Red Sox? Does it actually have 

the concept of “Boston Red Sox” or just some other strange thing that 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

10 sur 13 30-11-17 à 18:55 



only the computer understands? It’s an ontological question: Is the 

deep neural network really see a world that corresponds to our own? 

We human being seem to be obsess with black boxes: The high 

compliment we give to technology be that it feel like magic. When the 

working of a new technology be too obvious, too easy to explain, it can 

feel banal and uninteresting. But when I ask David Jensen — a 

professor at the University of Massachusetts at Amherst and one of the 

researcher be fund by Gunning — why X.A.I. have suddenly 

become a compelling topic for research, he sound almost soulful: “We 

want people to make inform decision about whether to trust 

autonomous systems,” he said. “If you don’t, you’re deprive people of 

the ability to be fully independent human beings.” 

A decade in the making, the European Union’s General Data 

Protection Regulation finally go into effect in May 2018. It’s a 

sprawling, many-tentacled piece of legislation whose opening line 

declare that the protection of personal data be a universal human right. 

Among it hundred of provisions, two seem aim squarely at where 

machine learn have already be deployed and how it’s likely to 

evolve. Google and Facebook be most directly threaten by Article 21, 

which affords anyone the right to opt out of personally tailor ads. The 

next article then confronts machine learn head on, limn a so- 

call right to explanation: E.U. citizen can contest “legal or similarly 

significant” decision make by algorithm and appeal for human 

intervention. Taken together, Articles 21 and 22 introduce the principle 

that people be owe agency and understand when they’re face by 

machine-made decisions. 

For many, this law seem frustratingly vague. Some legal scholar argue 

that it might be toothless in practice. Others claim that it will require 

the basic working of Facebook and Google to change, l they face 

penalty of 4 percent of their revenue. It remains to be see whether 

comply with the law will mean a heap of fine print and an extra 

check box bury in a pop-up window, some new kind of warning-label 

system mark every machine-made decision or much more profound 

changes. 

If Google be one of the company most endanger by this new scrutiny 

on A.I., it’s also the company with the great wherewithal to lead the 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

11 sur 13 30-11-17 à 18:55 



whole industry in solve the problem. Even among the company’s 

astonish roster of A.I. talent, one particular star be Chris Olah, who 

hold the title of research scientist — a title share by Google’s many ex- 

professor and Ph.D.s — without ever have complete more than a 

year of college. Olah have be work for the last couple of year on 

create new way to visualize the inner working of a deep neural 

network. You might recall when Google create a hallucinatory tool 

call Deep Dream, which produce psychedelic distortion when you 

fed it an image and which go viral when people use it to create 

hallucinatory mash-ups like a doll cover in a pattern of doll eye and 

a portrait of Vincent Van Gogh make up in place of bird beaks. Olah 

be one of many Google researcher on the team, lead by Alex 

Mordvintsev, that work on Deep Dream. It may have seem like a 

folly, but it be actually a technical steppingstone. 

Olah speaks faster and faster a he sink into an idea, and the word 

tumble out of him almost too quickly to follow a he explain what he 

found so excite about the work he be doing. “The truth is, it’s really 

beautiful. There’s some sense in which we don’t know what it mean to 

see. We don’t understand how human do it,” he told me, hand 

gesture furiously. “We want to understand something not just about 

neural net but something deeper about reality.” Olah’s hope be that 

deep neural network reflect something deeper about parse data — 

that insight glean from them might in turn shed light on how our 

brain work. 

Olah show me a sample of work he be prepare to publish with a 

set of collaborators, include Mordvintsev; it be make public this 

month. The tool they have developed be basically an ingenious way of 

test a deep neural network. First, it fed the network a random image 

of visual noise. Then it tweaked that image over and over again, work 

to figure out what excite each layer in the network the most. 

Eventually, that process would find the platonic ideal that each layer of 

the network be search for. Olah demonstrate with a network 

train to classify different breed of dogs. You could pick out a neuron 

from the topmost layer while it be analyze a picture of a golden 

retriever. You could see the ideal it be look for — in this case, a 

hallucinatory mash-up of floppy ear and a forlorn expression. The 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

12 sur 13 30-11-17 à 18:55 



network be indeed home in on higher-level trait that we could 

understand. 

Watching him use the tool, I realize that it be exactly what the 

psychologist Michal Kosinski need — a key to unlock what his deep 

neural network be see when it categorize profile picture a gay or 

straight. Kosinski’s most optimistic view of his research be that it 

represent a new kind of science in which machine could access 

truth that lay beyond human intuition. The problem be reduce what 

a computer knew into a single conclusion that a human could grasp and 

consider. He have painstakingly test his data set by hand and found 

evidence that the computer might be discover hormonal signal in 

facial structure. That evidence be still fragmentary. But with the tool 

that Olah show me, or one like it, Kosinski might have be able to 

pull back the curtain on how his mysterious A.I. be working. It would 

be a obvious and intuitive a a picture the computer have drawn on it 

own. 

Can A.I. Be Taught to Explain Itself? https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-expl... 

13 sur 13 30-11-17 à 18:55 


