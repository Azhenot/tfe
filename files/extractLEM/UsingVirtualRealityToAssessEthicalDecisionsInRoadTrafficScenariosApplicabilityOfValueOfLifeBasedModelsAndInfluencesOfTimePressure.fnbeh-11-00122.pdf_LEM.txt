




















































Using Virtual Reality to Assess Ethical Decisions in Road Traffic Scenarios: Applicability of Value-of-Life-Based Models and Influences of Time Pressure 


ORIGINAL RESEARCH 
published: 05 July 2017 

doi: 10.3389/fnbeh.2017.00122 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 1 July 2017 | Volume 11 | Article 122 

Edited by: 

Nuno Sousa, 

Instituto de Pesquisa em Ciências da 

Vida e da Saúde, Portugal 

Reviewed by: 

Claudio Lucchiari, 

Università degli Studi di Milano, Italy 

Fuat Balci, 

Koç University, Turkey 

*Correspondence: 

Leon R. Sütfeld 

lsuetfel@uos.de 

Received: 06 March 2017 

Accepted: 06 June 2017 

Published: 05 July 2017 

Citation: 

Sütfeld LR, Gast R, König P and 

Pipa G (2017) Using Virtual Reality to 

Assess Ethical Decisions in Road 

Traffic Scenarios: Applicability of 

Value-of-Life-Based Models and 

Influences of Time Pressure. 

Front. Behav. Neurosci. 11:122. 

doi: 10.3389/fnbeh.2017.00122 

Using Virtual Reality to Assess 
Ethical Decisions in Road Traffic 
Scenarios: Applicability of 
Value-of-Life-Based Models and 
Influences of Time Pressure 
Leon R. Sütfeld*, Richard Gast, Peter König and Gordon Pipa 

Neuroinformatics, Institute of Cognitive Science, Osnabrück University, Osnabrück, Germany 

Self-driving car be pose a new challenge to our ethics. By use algorithm to make 

decision in situation where harm human be possible, probable, or even unavoidable, 

a self-driving car’s ethical behavior come pre-defined. Ad hoc decision be make in 

milliseconds, but can be base on extensive research and debates. The same algorithm 

be also likely to be use in million of car at a time, increase the impact of any inherent 

biases, and increase the importance of get it right. Previous research have show 

that moral judgment and behavior be highly context-dependent, and comprehensive 

and nuanced model of the underlie cognitive process be out of reach to date. 

Models of ethic for self-driving car should thus aim to match human decision make 

in the same context. We employ immersive virtual reality to ass ethical behavior 

in simulated road traffic scenarios, and use the collect data to train and evaluate a 

range of decision models. In the study, participant control a virtual car and have to 

choose which of two give obstacle they would sacrifice in order to spare the other. We 

randomly sample obstacle from a variety of inanimate objects, animal and humans. 

Our model comparison show that simplemodels base on one-dimensional value-of-life 

scale be suit to describe human ethical behavior in these situations. Furthermore, 

we examine the influence of severe time pressure on the decision-making process. We 

found that it decrease consistency in the decision patterns, thus provide an argument 

for algorithmic decision-making in road traffic. This study demonstrates the suitability 

of virtual reality for the assessment of ethical behavior in humans, deliver consistent 

result across subjects, while closely match the experimental setting to the real world 

scenario in question. 

Keywords: self-driving cars, moral judgment, ethical decisions, modeling, virtual reality, value-of-life scale, time 

pressure 

INTRODUCTION 

Privately own car with autopilot first become a reality with a software update which Tesla 
Motors release to it fleet in October 2015, and many comparable system will be on the market 
soon. While initially, these system be likely to be restrict to highway use, they will eventually 
make their way into cities, with estimate predict autonomous vehicle (AVs) dominate road 

http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org/Behavioral_Neuroscience/editorialboard 
http://www.frontiersin.org/Behavioral_Neuroscience/editorialboard 
http://www.frontiersin.org/Behavioral_Neuroscience/editorialboard 
http://www.frontiersin.org/Behavioral_Neuroscience/editorialboard 
https://doi.org/10.3389/fnbeh.2017.00122 
http://crossmark.crossref.org/dialog/?doi=10.3389/fnbeh.2017.00122&domain=pdf&date_stamp=2017-07-05 
http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 
https://creativecommons.org/licenses/by/4.0/ 
mailto:lsuetfel@uos.de 
https://doi.org/10.3389/fnbeh.2017.00122 
http://journal.frontiersin.org/article/10.3389/fnbeh.2017.00122/abstract 
http://loop.frontiersin.org/people/408334/overview 
http://loop.frontiersin.org/people/433603/overview 
http://loop.frontiersin.org/people/269/overview 
http://loop.frontiersin.org/people/4053/overview 


Sütfeld et al. ADV Ethics Assessment and Modeling 

traffic by the 2040s (Marcus, 2012; Litman, 2014). The new 
technology be expect to reduce the number of car accident 
significantly: The German Federal Statistics Agency report that 
in 2015, 67% of all accident with injury to people be 
cause by driver misconduct. A 2008 survey by the National 
Highway Traffic Safety Administration (NHTSA) even show 
that human error played a crucial role in 93% of traffic accident 
in the US. These number outline the enormous potential of 
self-driving car regard road safety. In fact, Johansson and 
Nilsson (2016) claim that self-driving car will adjust their drive 
style and speed such that safe handle of any unexpected 
event be guaranteed at all times. However, this approach appear 
unrealistic for many mixed traffic (human and AVs) and inner 
city scenarios. To ensure absolute safety even in very unlikely 
events, the car would have to drive in an overly cautious manner, 
and a a result may be switch off by many drivers, or tempt 
other driver to engage in risky overtaking. Other rare events, 
such a a distract human driver swerve into the opposite lane, 
seem very hard to evade altogether under any circumstances. 
Even when completely take human driver out of the equation, 
we be left with a considerable number of accidents, caused, for 
instance, by technical or engineering failure (Goodall, 2014b). 
Altogether, with over a billion car in operation worldwide, the 
sheer amount of traffic virtually guarantee that, in spite of the 
overall expect reduction of accidents, critical situation will 
occur on a daily basis. 

With accident involve autonomous car be and 
become a reality, ethical consideration will inevitably come 
into play. Any decision that involves risk of harm to a human 
or even an animal be consider to be an ethical decision. 
This include everyday decisions, e.g., decide if and when to 
take a minor risk in overtake a cyclist. But it also include 
quite rare situation in which a collision be unavoidable, but a 
decision can be make a to which obstacle to collide with. By 
rely on algorithm to make these decisions, a self-driving car’s 
ethic come pre-defined by the engineer, whether it’s do with 
sophisticated ethical system or simple rule such a “always stay 
in the lane.” This development pose a new challenge to the 
way we handle ethics. If human driver be in an accident and 
make a bad decision from an ethical standpoint, we count in 
their favor that they have incomplete knowledge of the situation 
and only fraction of a second to make a decision. Therefore, 
we typically refrain from assign any blame to them, morally 
or legally (Gogoll and Müller, 2017). Algorithms in self-driving 
cars, on the other hand, can estimate the potential outcome of 
various option within milliseconds, and make a decision that 
factor in an extensive body of research, debates, and legislation 
(Lin, 2013).Moreover, the same algorithm be likely to be use in 
thousand or million of car at a time. Situations that be highly 
unlikely for an individual car become highly probable over the 
whole fleet. This enhances the importance of get it right, and 
unpreparedness to handle this type of situation may result in a 
significant number of bad decision overall. 

Ultimately, moral decision-making system should be 
consider a necessity for self-driving car (Goodall, 2014a). The 
present study address the question of how to ass and how 
to model human moral decision-making in situation in which 

a collision be unavoidable and a decision have to be make a to 
which obstacle to collide with. We conduct a virtual reality 
(VR) study in which participant have to make exactly this type 
of decision for vary combination of obstacles, and use the 
obtain data to train and evaluate a number of different ethical 
decision-making models. In the next section, we will review the 
current state of psychological research with respect to moral 
judgment and decision-making, and derive the outline for the 
present experiment. 

The Psychology of Moral Judgment 
The scenario in this study can be see a an adaptation of 
the trolley dilemma, a thought experiment commonly use in 
research on moral judgment and decision-making, in which a 
runaway trolley be head toward a group of five people. The 
only way to save these five be to pull a lever and divert the trolley 
onto a different track, kill a single person instead (Thomson, 
1985). The utilitarian choice here be to pull the lever and sacrifice 
one person in order to save five. By contrast, deontologism 
focus on the right of individuals, often put these ahead 
of utilitarian considerations. From this perspective, the act of 
kill a person would be consider morally wrong, even if 
it mean save five other lives. In a popular alteration of the 
trolley problem, call the footbridge dilemma, the subject find 
themself on a bridge over the track with a stranger. Pushing 
the stranger off the bridge in front of the oncoming train 
would stop the train and save the five people stand on the 
track. Interestingly, most people say they would pull the lever 
in the original trolley dilemma, but only a minority also say 
they would push the stranger off the bridge in the footbridge 
dilemma (Greene et al., 2001). An extensive body of psychological 
research be concerned with the affective, cognitive and social 
mechanism underlie this judgment, our ethical intuition and 
behavior (Huebner et al., 2009; Christensen and Gomila, 2012; 
Cushman and Greene, 2012; Waldmann et al., 2012; Avramova 
and Inbar, 2013). Most prominently, the dual process theory, put 
forward by Greene et al. (2004), proposes two distinct cognitive 
system in competition. The first be an intuitive, emotionally 
root system, elicit negative affect when behavioral rule 
be violated, favor decision in line with deontological ethics. 
The second one be a controlled, reasoning-based system, favor 
decision correspond with utilitarian ethics. Greene’s dual- 
process theory thus explains the different endorsement rate of 
utilitarian behavior in the trolley and footbridge dilemma by 
the more emotionally engage nature of the latter. Pushing a 
stranger off a bridge instead of pull a lever require personal 
force and us harm a a mean to an end, rather than a a side 
effect, both increase the emotionality of the situation, and thus 
shift focus to the system favor deontological ethic (Greene 
et al., 2001). Similarly, frame a dilemma a more personal (“I 
would do...” instead of “it be acceptable to...”) and increase 
the emotional proximity to the potential victim will also result 
in few utilitarian choice (Greene et al., 2001; Tassy et al., 
2013). Neuroscientific evidence be provide by Tassy et al. (2012), 
show that disrupt the right dorsolateral prefrontal cortex, 
associate with emotional processing, increase the likelihood 
of utilitarian responses. Valdesolo and DeSteno (2006) found 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 2 July 2017 | Volume 11 | Article 122 

http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 


Sütfeld et al. ADV Ethics Assessment and Modeling 

an increase probability of utilitarian response when induce 
positive affect, argue that the positive affect may cancel out the 
negative affect connect to rule violations. 

However, the dual-process theory, base on the emotion- 
cognition distinction be not undisputed. Cushman (2013) 
argues that while a distinction between compete process be 
necessary, the distinction between affective and non-affective 
processing be inadequate, since both process must involve 
cognition, a well a affective content. Instead, he proposes a 
distinction base on two cognitive mechanism borrow from 
the field of reinforcement learning. The first be an action-based 
system, assign reward value to possible action in a give 
situation. These reward value be learn from experience and 
statically assign to a give situation-action-pair. The second 
mechanism be outcome-based and relies on an underlie world 
model. In simplify terms, it predicts the consequence of the 
possible action in a give situation and reassigns the value of 
the consequence to the action that lead to it. In the trolley 
dilemma, the outcome-based system would favor utilitarian 
behavior, and the action-based system would not intervene 
because the action of pull a lever be not generally associate 
with negative reward. Conversely, the action of push a person 
off a bridge be associate with negative reward, thus explain the 
low endorsement rate of utilitarian behavior in the footbridge 
dilemma. Further evidence in favor of the action vs. outcome 
distinction in dual-process model be given, e.g., in Cushman et al. 
(2012) and Francis et al. (2016). 

In another theory in the realm of moral judgment, Haidt 
and Graham (2007) aim to explain different view of oppose 
political camp (liberals and conservatives) with a model of 
morality base on five factors, and the relative importance of each 
of these factor to member of the political camps. Finally, base 
on a large body of neuroscientific evidence, Moll et al. (2008) 
propose a detailed account of moral emotion a the foundation 
of our moral judgment. While none of the two entail concrete 
prediction with respect to moral decision-making in the trolley 
dilemma and similar scenarios, they demonstrate that the scope 
of the dual-process theory be limited, and that we be a long way 
from a comprehensive theory about the cognitive mechanism 
govern our moral judgment and behavior. 

Virtual Reality Assessment and Effects of 
Time Contraints 
While most of the aforementioned research relies on abstract, 
text-based presentation of dilemma situations, a grow 
number of study make use of the possibility provide by 
virtual reality (VR) technology. VR, and in particular immersive 
VR use head-mounted display (HMDs) and head-tracking, 
allows assess moral behavior in a naturalistic way, immerse 
the subject in the situation, provide much richer contextual 
information, and allow for more physical input methods. In 
an immersive VR version of the trolley dilemma, Navarrete et al. 
(2012) be able to confirm the utilitarian choice’s approval rate 
of 90%, previously found in text-based studies. Further, they 
found a negative correlation between emotional arousal and 
utilitarian choices, in line with the prediction of the dual process 

theory. In contrast to this, Patil et al. (2014) found both emotional 
arousal and endorsement of utilitarian choice to be high in a 
desktop-VR set with 3D graphic on a desktop screen than in 
a text-based setting. While hint toward a possible distinction 
between moral judgment and behavior, the result also suggest 
that feature other than emotional arousal play a major role 
in our moral judgment. The author argue that the contextual 
saliency (including a depiction of the train run over the 
virtual humans) may have shift the subjects’ focus from the 
action itself toward the outcome of their decision. The tendency 
to favor utilitarian judgment would then fit Cushman’s account of 
the dual-process theory. In a similar study by Francis et al. (2016), 
participant be confront with the footbridge dilemma, either 
in an immersive VR environment or in a text-based scenario. In 
the text-based condition, endorsement of the utilitarian choice 
be low at around 10%, in line with expectation base on 
previous assessments. In the VR condition, however, subject 
opt to push the stranger off the bridge in up to 70% of the 
trials. These result be again in line with Cushman’s account of 
the dual-process theory, and make a strong case for the notion 
of moral judgment and moral behavior be distinct constructs. 
In a different approach, Skulmowski et al. (2014) varied the 
standard design of the trolley dilemma in multiple ways. First, 
they virtually place participant in the trolley’s cockpit instead 
of a bystanders’ perspective. Second, they design the track to 
split into three and block the middle track with a stationary 
trolley, which have to be avoided. Participants be thus force to 
choose between the outside tracks, preclude the deontological 
option of not intervene in the situation. Third, the subject 
have to react within 2.5 s after the obstacle become visible. 
Finally, in addition to vary the number of people on the 
available tracks, the author add a number of trial with only 
one person stand on either of the available tracks. These 
differ in gender, ethnicity, and whether the person be face 
toward the trolley or away from it. Unsurprisingly, the group 
be save in 96% of the the one-vs.-many trials. In the single 
obstacle trials, significant difference be only found in the 
gender condition (deciding between a man and a woman), with 
men be sacrifice in around 58% of the cases. 

The natural passing of time be a feature inherent to VR study 
of this kind. While in principle, it would be possible to pause 
time in the virtual world, do so might break immersion 
and would likely lessen the ecological validity of the obtain 
results. The previously mention study all impose some 
time constraints, but no systematic variation of response time 
window be performed. Nevertheless, the dual-process theory 
would predict time pressure to influence our moral judgment. 
The action-based system in Cushman’s account of the dual- 
process theory be thought to be simple and quick, while the 
outcome-based system involves high cognitive load and be 
ultimately slower. Greene’s account of the dual-process theory 
suggests that in emotionally engage dilemmas, the control 
cognitive system need to override the initial emotional response 
before make a utilitarian judgment (Greene, 2009). Indeed, 
increase cognitive load during decision time be show to 
increase response time in personal dilemmas, when a utilitarian 
response be give (Greene et al., 2008). Paxton et al. (2012) 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 3 July 2017 | Volume 11 | Article 122 

http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 


Sütfeld et al. ADV Ethics Assessment and Modeling 

show that moral judgment can be change with persuasive 
arguments, but additional time for deliberation be require 
for the change to occur. To the best of our knowledge, so far 
only one study systematically varied the length of response time 
windows. In Suter and Hertwig (2011), participant be either 
restrict to give a response within 8 s, or they have to first 
deliberate for 3 min. For high-conflict scenarios, such a the 
footbridge dilemma, high time pressure lead to few utilitarian 
responses. A second experiment in the same study support this 
finding. When no time limitation be given, but one group 
be instruct to respond intuitively, and the other group be 
instruct to deliberate before enter a reaction, the intuitive 
group’s response time be a lot shorter than the deliberate 
group’s, and they give few utilitarian responses. 

In conclusion, VR study have show the importance of 
contextual cue for our decision-making and provide intrigue 
evidence for a distinction of moral judgment and behavior. 
Moreover, time constraints, a an inherent feature to VR setups, 
have be recognize a a factor in our moral decision-making. 
There be evidence suggest that longer deliberation may 
facilitate utilitarian decision in certain complex scenarios, but 
we still lack a systematic analysis of the influence of time pressure 
on moral judgment. 

Modeling of Human Moral Behavior 
An important criterion that an ethical decision-making system 
for self-driving car or other application of machine ethic 
should meet be to make decision in line with those make by 
humans. While complex and nuanced ethical model capable 
of replicate our cognitive process be out of reach to date, 
simpler model may deliver adequate approximation of human 
moral behavior, when the scope of the model be confine to a 
small and specific set of scenarios. Value-of-life-based model 
stand to reason a a possible solution for any situation in which 
a decision have to be make a to which one of two or more 
people, animals, other obstacles, or group thereof to collide 
with. An account of a value-of-life model that be focus on 
a person’s age be give by Johansson-Stenman and Martinsson 
(2008). The author conduct a large-scale survey in which 
people have to indicate in several instances, on which of two 
road-safety-improvement measure they would rather spend a 
give budget. The available measure differ with respect to 
the age and expect number of people that would be saved, 
a well a whether the one save would be pedestrian or car 
drivers. The author use a standard probit regression model to 
fit the observe data, and found that not the number of save 
lives, but rather the number of save life-years to be the most 
important factor in the subjects’ decision, allow for specific 
value of life to be assign to each age group. Beyond this, 
they found pedestrian to be value high than car driver of 
the same age, indicate consent a a factor in the valuation. 
While discriminate between potential human crash victim 
base on age, or possibly gender, be unlikely to gain general public 
acceptance, Goodall (2014a) suggests use value-of-life scale 
in case where higher-level rule fail to provide the system with 
clear instructions. Furthermore, if we take animal into account, 
value-of-life scale stand to reason a a way of deal with vastly 

differ probabilities. When a decision have to be make between 
kill a dog with near certainty and take a 5% risk of injure 
a human, how should the algorithm decide? We don’t seem to 
take much issue with assign different value of life to different 
species, and a system favor pet over game or bird might be 
acceptable in the public eye. While this make the case for at least 
some form of value-of-life model, it remains to be see to what 
extent such model be able to capture the complexity of human 
ethical decision-making. 

Deriving and Outlining the Experiment 
As discuss in previous sections, our moral judgment be highly 
dependent on a wide variety of contextual factors, and there be 
no ground truth in our ethical intuition that hold irrespective 
of context. We thus argue that any implementation of an 
ethical decision-making system for a specific context should be 
base on human decision make in the same context. To date, 
our limited understand of the cognitive process involve 
prevents u from construct a comprehensive ethical model 
for use in critical real-world scenarios. In the context of self- 
drive cars, value-of-life scale stand to reason a simple model 
of human ethical behavior when a collision be unavoidable, and an 
evaluation of their applicability in this context be the main focus 
of this study. 

To this end, participant be place in the driver’s seat of a 
virtual car head down a road in a suburban setting. Immersive 
VR technology be use to achieve a maximum in perceive 
presence in the virtual world. A wide variety of different 
obstacles, both animate and inanimate, be randomly pair 
and present in the two lane ahead of the driver. Participants 
have to decide which of the two they would save, and which 
they would run over. Since prolong session in immersive 
VR can cause nausea and discomfort, we opt for a pool 
experimental design with short session of 9 trial per condition 
and participant. We thus pool the trial of all participants, 
and use this data set to train three different logistic regression 
model to predict the lane choice for a give combination of 
obstacles. (1) The pair model us each possible pair of 
obstacle a a predictor. Here, a give prediction reflect the 
frequency with which one obstacle be chosen over the other in 
the direct comparisons. Since an obstacle be not represent with 
a single numerical value, the pair model be not a value-of-life 
model, but serf a a frame of reference. (2) The obstacle model 
assigns one coefficient to each obstacle and us the obstacles’ 
occurrence a predictors. We interpret these coefficient a the 
obstacle’s value of life. (3) The cluster model us only one 
coefficient per category of obstacles, a they result from a 
cluster base on the frequency with which each obstacle 
be hit. 

We compare the three different model to test a set of 
hypotheses. Our first hypothesis be that a one-dimensional 
value-of-life-based model (i.e., the obstacle model) fully capture 
the complexity of pairwise comparisons. The obstacle model 
should thus be a accurate a the pair model. This would 
mean that our ethical decision can be described by a simple 
model in which each possible obstacle be represent by a single 
value, and the decision which obstacle to save be base only on 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 4 July 2017 | Volume 11 | Article 122 

http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 


Sütfeld et al. ADV Ethics Assessment and Modeling 

these respective values. We further hypothesize (Hypothesis 2) 
that within-category distinctions, for example, between human 
of different age, be an important factor in the decisions. 
Specifically, the obstacle model should prove to be superior 
to the cluster model. Furthermore, since a certain amount of 
time pressure be inherent to naturalistic representation of this 
scenario, we investigate it influence on the decision by vary 
the time to respond in two steps, give participant a response 
window of either 1 or 4 s. We found 4 s of decision time to induce 
relatively little time pressure in the use scenario, while one 
second still left a sufficient amount of time to comprehend and 
react. We hypothesize (Hypothesis 3) that more error would 
bemade under increase time pressure, and that ethical decision 
would thus be less consistent across subject in these trials. The 
dual-process theory would predict a high endorsement of 
utilitarian choice with more time to deliberate (i.e., in the slow 
condition). However, for comparison of single obstacles, there 
be no clearly define utilitarian choice. If anything, base the 
decision in human vs. human trial on a person’s expect year 
to live could be consider utilitarian, and be partly cover in 
Hypothesis 2. Moreover, the omission of a lane change, despite 
run over the more valuable obstacle, could be interpret a 
a deontological choice, but we didn’t formulate any directional 
hypothesis regard this factor prior to the study. 

METHODS 

The experiment ran in a 3D virtual reality application, 
implement with the Unity game engine, use the Oculus Rift 
DK2 a the head-mounted display. The audio be played through 
Bose QC25 and Sennheiser HD215 headphone throughout the 
experiment. The participant be sit in the driver’s seat of 
a virtual car head down a suburban road. Eventually, two 
obstacles, one on either lane, block the car’s path and the 
participant have to choose which of the two obstacle to hit. 
Using the arrow key on the keyboard, the participant be 
able to switch between the two lane at all times, up to a point 
approximately 15 m before impact. This way, we provide a high 
level of agency, intend to closely resemble manual car driving, 
while make sure the decision could not be avoid by zig- 
zagging in the middle of the road or crash the car before 
reach the obstacles. We use 17 different obstacle from three 
different categories, i.e., inanimate objects, animals, and human 
a single obstacles, a well a composite obstacles. An empty lane 
be additionally use a a control. For each trial, two of the 
17+1 obstacle be pseudo-randomly chosen and allocate to 
the two lanes, a be the start lane of the participant’s car. A 
wall of fog at a fix distance from the participant’s point of view 
control the exact time of the obstacle’s onset. We varied the 
length of the reaction time window by vary the fog distance 
and car speed, result in a window of 4 s for the slow, and 
one second for the fast condition. To indicate how much time 
be left to make a decision at each point in time, a low-to-high 
sweep sound be played a an acoustic cue. The sound start 
and end on the same respective frequency in both conditions, 
thus sweep through the frequency band quicker in the fast 
condition. After the decision time window have end around 

15 meter before impact, the car kept moving, complete any 
last instant lane changes. Right before impact, all movement 
be frozen, all sound stopped, and the screen fade to black, 
mark the end of a trial. Figure 1 illustrates the chronological 
progression of the trial in the fast and slow condition, and 
give an overview of all obstacles. The fast and slow trial be 
present in separate block of 9 trial each. Two more block of 
trial be present but not analyze in the current study, and 
all four block be present in randomize order. We chose 
obstacle pairing such that each obstacle typically appear once 
per subject and block. The frequency of appearance of all 153 
possible pairings, a well a the lane allocation and start lanes, 
be balance over all subjects. 

Sample and Timeline 
Our sample consists of 105 participant (76 male, 29 female) 
between the age of 18 and 60 (mean: 31) years. We exclude one 
subject who report a partial misunderstand of the task, a 
well a three outlier whose decision be the opposite of the 
model prediction (see below) inmore than 50% of their respective 
trials. Most of the participant be university student or visitor 
of the GAP9 philosophy conference. Before participating, we 
inform all subject about the study, potential risk and the 
option to abort the experiment at any time. They be also 
inform that the external screen would be turn off during 
the experiment, so that others could not observe their decisions. 
After signing a consent form, they be ask to put on the 
HMD and headphones, and then receive all further information 
within the application. As a first task, they have to complete 
a training trial, avoid three pylon by alternate between 
the lanes. Upon hit a pylon, the training trial be repeat 
until complete without error. This procedure give participant 
a chance to familiarize themselves with the VR environment, and 
it make sure they have understood the control before enter the 
experimental trials. The study conform to the Code of Ethics 
of the American Psychological Association, a well a to national 
guidelines, and be approve by the Osnabrück University’s 
ethic committee. 

RESULTS 

We pool all data and do not consider between-subject 
difference in the analysis. In the experimental trials, the mean 
number of lane switch per trial be 0.816 in the slow and 
1.037 in the fast condition. We estimate error rate for both 
conditions, use trial in which one of the lane be empty. 
Hitting the only obstacle in such a trial be consider an error, 
a we find it safe to assume that the outcome in these trial be 
a result of inadvertently press the wrong button, instead of 
a meaningful decision. This event occur in 2.8% of all trial 
contain an empty lane in the slow condition, and in 12.0% of 
the relevant trial in the fast condition. As a frame of reference, 
the chance level for this be at 50%. 

Behavioral Models 
All model use in the present study be logistic regression 
models, use the occurrence of obstacle pairings, individual 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 5 July 2017 | Volume 11 | Article 122 

http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 


Sütfeld et al. ADV Ethics Assessment and Modeling 

FIGURE 1 | (Left) Overview of the experimental setting. (Middle) Timelines of the slow and fast conditions. (Right) Overview of all obstacle used. Colors indicate 

cluster assignments. 

obstacle or clusters, i.e., obstacle category (see below) in a 
particular trial a predictor for the lane choice. Furthermore, we 
add a constant offset and the trial’s start lane a predictor 
to all models. The constant offset allow u to detect potential 
bias in the overall lane preference (left or right). Such a bias 
could occur, for example, when participant be use to right- 
hand traffic and feel that use the right-hand lane be generally 
more acceptable. Including the start lane a a predictor 
allow u to detect a bias to stay in the respective trials’ start 
lane—we would label this an omission bias—or to move away 
from the start lane, i.e., a panic reaction bias. 

A model’s predict probability of choose to drive in the left 
lane be give by p(Y = left|X) = 1 

1+exp(−X) 
, with X be the 

model-specific representation of a particular trial. 
In the pair model, a trial be represent a Xp = cωi + 

sωs + ωb, where ωi be the coefficient for obstacle pair i (e.g., 
{boy, woman}), c ∈ {−1, 1} be the lane configuration in the 
respective trial (e.g., 1 if the boy be in the left lane, −1 if the 
woman be in the left lane), ωs be the start lane coefficient, 
s ∈ {−1, 1} be the start lane −1 if the start lane be right, 
and ωb be the coefficient for the lane bias. The model be thus 
make a prediction base on a general preference for one of 
the lanes, the start lane of the respective trial, and which 
of the 153 possible pairing be present in the trial, result 
in 155 parameter in total. Figure 2 left show the prediction 
of the pair model. Since each pair of obstacle have it 
own free parameter, the model allows for intransitive and other 
complex relation between the obstacles. For example, in the slow 
condition, the pair model deems the goat to be more valuable 
than the boar, and the boar to be more valuable than the deer, 
but the goat to be less valuable than the deer. Consequentially, 
there be no single value of life for an obstacle in the pair model. 
An obstacle’s value be only define relative to each of the other 
obstacles. 

In the obstacle model, a trial be represent a Xo = ωro − 
ωlo + sωs + ωb, with ωro and ωlo be the coefficient for the 
right and left obstacle in the respective trial. Each obstacle be 
thus represent by a single characteristic value or value of life. 
All pairwise comparison result directly from a subtraction of 

the respective two value of life. Thus, when sort all obstacle 
accord to their value of life on the abscissa and ordinate, 
the order in the vertical and horizontal direction be strictly 
monotonous (Figure 2, middle). Since there be 18 individual 
obstacles, the model have a total of 20 parameters, include the 
lane bias and start lane coefficients. 

Similarly, in the cluster model, a trial be represent a Xc = 
ωrc−ωlc+ sωs+ωb, with ωrc and ωlc be the coefficient of the 
cluster that the obstacle be assign to. We perform bottom- 
up cluster and subsequent model selection to derive the 
ideal number of cluster and cluster allocation of all present 
obstacle for the cluster model (see Figure 3). Logistic regression 
model be first construct and fit for all possible number 
of clusters, range from 17 to 1. We then perform the model 
comparison via the Bayesian Information Criterion (BIC). In 
the slow condition, the five cluster model be found to be 
the best of the cluster models. Notably, it cluster allocation 
be perfectly in line with a categorization in none, inanimate 
objects, animals, humans, and group of human and animals. 
In the fast condition, a four cluster solution be found to be 
ideal, and it cluster allocation don’t align perfectly with the 
aforementioned semantic categorization. This be likely the result 
of the high error rate in the fast condition. In order to still allow 
for a comparison between both conditions, we chose to use the 
aforementioned semantic categorization in five cluster for the 
fast condition, a well. For both conditions, the clustermodel thus 
have five parameter for the obstacle clusters, result in a total 
of only seven parameters, include the lane bias and start 
lane coefficients. Figure 2 right show it prediction in the slow 
condition. The model us only one free parameter per cluster of 
obstacles, result in a block structure. Since all obstacle within 
a cluster be consider equal in value of life, the difference in the 
value of life be always exactly zero for within-cluster comparisons. 
Those decisions, therefore, depend entirely on the start lane 
and overall lane preference. 

All model be fit use the logistic regression algorithm 
in the scikit-learn (version 0.17.1) toolbox for Python. We 
set the regularization strength to a very low value of 10−9 

and base the model selection on prediction accuracy via 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 6 July 2017 | Volume 11 | Article 122 

http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 


Sütfeld et al. ADV Ethics Assessment and Modeling 

FIGURE 2 | Model predictions. (Top) Slow condition, (Bottom) fast condition, (Left) pair model, (Middle) obstacle model, (Right) cluster model. Colors indicate 

the probability of save the row-obstacle (y-axis) and sacrifice the column-obstacle (x-axis). Pink, green, blue, and black bar indicate cluster assignment base on 

agglomerative cluster in the slow condition (see Figure 3). For mean of comparability, the cluster model in the fast condition be fit with the semantic cluster 

assignment from the slow condition. 

10-fold cross-validation, a well a the Bayesian Information 
Criterion. 

Pairing Model vs. Obstacle Model 
In a first step, we compare the pair and the obstacle models. 
When model the training data set, model with a (much) 
high number of free parameter can describe the data better. 
However, in cross-validation, potential overfitting can lead to a 
reduce performance of the more detailed model. Indeed, with a 
prediction accuracy of 91.64% in the slow condition and 80.75% 
in the fast condition, the obstacle model be slightly superior to 
the detailed pair model, with prediction accuracy of 89.33% 
and 78.77%, respectively. Despite our extensive data set with 
909 trial per condition, the large number of parameter in the 
pair model cause overfitting. This find translates to a much 
large BIC value for the pair model (see Table 1). Thus, our 

result strongly favor the obstacle model for it low complexity 
and reduce risk of overfitting. This result, in combination with 
the high prediction accuracy of the obstacle model in the slow 
condition, confirms our first hypothesis, i.e., one-dimensional 
value-of-life-based model can adequately capture the ethical 
decision we make in real life scenarios. 

Obstacle Model vs. Cluster Model 
In the slow condition, the obstacle model’s ranking of coefficient 
value within the category mostly make sense, intuitively. For 
example, child be assign high value than adult (boy: 
2.76, male adult: 2.12, correspond to a 65.5% chance of save 
the boy in a direct comparison with a male adult). Further, the 
dog be consistently found to be the most valuable of the animals. 
The prediction accuracies, however, be essentially even between 
the obstacle model (91.64%) and the cluster model (91.20%), 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 7 July 2017 | Volume 11 | Article 122 

http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 


Sütfeld et al. ADV Ethics Assessment and Modeling 

FIGURE 3 | Dendrogram of bottom-up clustering, base on the observe frequency with which each obstacle be spar (saved), for the slow and fast condition 

separately. 

with the cluster model score the low BIC value, due to the 
reduce number of parameter (see Table 1). These finding 
be repeat in the fast condition. Prediction accuracy of the 
obstacle and cluster model be very close to each other (80.75 
and 80.53%), and in term of BIC value the cluster model be 

superior. We thus have to reject our second hypothesis, because 
the cluster model with five cluster be select a superior to the 
obstacle model. In other words, we found no particular advantage 
of use obstacle-based predictor instead of category-based 
predictors. 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 8 July 2017 | Volume 11 | Article 122 

http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 


Sütfeld et al. ADV Ethics Assessment and Modeling 

TABLE 1 | BIC-values and prediction accuracy base on 10-fold cross-validation for the three model in the slow and fast condition. 

Model SL LB Parameters Slow Fast 

BIC Accuracy BIC Accuracy 

Pairing x x 155 1349.122 0.8933 1563.629 0.7877 

Obstacle x x 20 556.845 0.9164 770.827 0.8075 

Cluster x x 7 497.198 0.9120 691.389 0.8053 

Cluster x 6 505.816 0.8922 685.797 0.8118 

Cluster x 6 491.852 0.9120 684.656 0.8053 

Cluster 5 499.809 0.8636 679.053 0.8229 

SL, include the start lane a predictor; LB, include a constant offset a predictor to model a lane bias. Bold value indicate the best model in the list. 

Biases 
To ass the two bias predictors’ importance for the model, 
we ran another model comparison for three additional version 
of the cluster model. All three additional version be base 
on the above model, but the first variant drop the start 
lane predictor, the second variant drop the lane bias, and 
the third variant drop both predictors. In the slow condition, 
the cluster model omit the lane bias, but include the 
start lane predictor, score the low BIC value of all models. 
Its prediction accuracy be the same a that of the previously 
assess cluster model with both additional predictor (91.20%), 
make it the best explanatory model for the observation make 
in the slow condition (see Table 1). This be also reflect in 
the respective coefficients. When include the predictor of the 
lane bias, it be fit to a value of 0.15. The low value indicates 
only a very weak tendency to the left lane, which make no 
significant contribution to the model fit. Thus, even in this rather 
realistic scenario, participant treat both lane a equally valid 
drive lanes. The start lane predictor be fit to -0.47, 
indicate a reluctance to switch lane in the face of a decision, 
constitute an omission bias. We can roughly quantify the 
extent of this reluctancy a be rather small, since coefficient 
difference between category be in all case magnitude higher. 
The specific start lane in a trial would therefore not affect 
the decision in between category comparisons. It does, however, 
play a role in within-category decisions, a evidence by the 4.8% 
gap in overall prediction accuracy between the cluster model 
with and without the start lane a a predictor. In the fast 
condition, the best model, both in term of prediction accuracy 
a well a BIC score, be the one omit both bias predictor (see 
Table 1). By omit the bias predictors, the prediction accuracy 
increase from 80.53 to 82.29%, expose an overfit in the more 
complex model. In conclusion, the analysis of the bias predictor 
found lane preference to have no substantial influence on the 
decision make in this paradigm, but do reveal an omission bias 
when face similarly value obstacle in the slow condition. 

Influence of Increased Time Pressure 
We will now turn to a direct comparison of the slow and fast 
condition, to evaluate the effect of increase time pressure. 
The most notable difference between the two condition be the 
(estimated) error rate of 12.0% in the fast condition, mark a 
four-fold increase from the slow condition. As for the cause of 

the errors, we would expect an increase omission bias if the 
error be cause by amere failure to react in time. Interestingly, 
this be not the case. Instead, we found an omission bias only 
in the slow and not in the fast condition, indicate that error 
in the fast condition be equally a result of stay in and 
switch into the lane of the more valuable obstacle. A major 
increase in error rate also substantially decrease the expect 
prediction accuracy even for a perfect model. This be reflect in 
the prediction accuracy for model in the fast condition, which 
be on average roughly 10% below those for the correspond 
model in the slow condition. 

In the cluster analysis, we found a four cluster model to 
yield the low BIC value in the fast condition, instead of 
the five cluster model found to be ideal in the slow condition. 
Moreover, the cluster assignment for some of the obstacle be 
also different, and do no longer match the semantic category 
perfectly (see Figure 2). These finding be consistent with the 
influence of increase noise in the data, and can therefore 
also be ascribed to the increase error rates. Since there be no 
match cluster model for both the slow and fast condition, 
we include a comparison of the cluster model base on the 
semantically define categorization in Figure 4, but decide 
to focus on the obstacle model in the remainder of this 
comparison. In the obstacle model, the coefficient range in the 
fast condition be reduce to 50% of that in the slow condition 
(see Figure 4). Specifically, the obstacle on the extreme side 
of the spectrum—the empty lane and the group of human 
and animals—aren’t separate well from the adjacent obstacle 
categories. To statistically confirm this observe difference, we 
use a nest model approach with log-likelihood ratio tests. 
For the nest model, we fit the joint dataset of fast and 
slow condition to the obstacle model use 19 predictors, i.e., 
the 18 obstacle plus the start lane. For the large nest 
model, we add a second set of 19 predictors. These 19 be 
duplicate of the first 19 predictors, but be fit only on 
the slow condition trials. Together, these two set form a 
model with 38 predictor in total. The log-likelihood ratio 
test between the nest and nest model be significant 
(p = 0.037), show that the reduction in parameter between 
the two significantly reduces model accuracy. In other words, the 
difference between the two condition be large enough to justify 
the use of two completely separate set of parameter to describe 
them. This confirms our third hypothesis, i.e., increase time 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 9 July 2017 | Volume 11 | Article 122 

http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 


Sütfeld et al. ADV Ethics Assessment and Modeling 

FIGURE 4 | (Left) Value-of-life coefficient by condition. Pictograms and color indicate the category empty lane, inanimate objects, animals, humans, and group 

of human and animal (left to right). Starting lane coefficient depict a gray bars. (Right) Relative frequency of “saving” the empty lane object, use a error rate 

estimates, for fast and slow condition separately. 

pressure significantly decrease the consistency in the answer 
patterns. 

Another notable difference between the two condition be that 
we no longer observe a bias toward sacrifice the male adult in 
direct stand-offs with female adults. Instead, participant save 
male in 4 out of 7 case in the fast condition. The previously 
speculate tendency toward social desirability would likely rely 
on slow cognitive processes, and thus not come into effect in 
fast-paced intuitive decisions. 

DISCUSSION 

We investigate the capability of logistic regression-based value- 
of-life model to predict human ethical decision in road 
traffic scenario with force choice decisions, juxtaposing a 
variety of possible obstacles, include humans, animals, and 
inanimate objects. The analysis incorporate various contextual 
and psychological factor influence our moral decision-making 
in these situations, and examine in particular the effect of 
severe time pressure. 

Our first hypothesis be that a one-dimensional value-of- 
life-based model fully capture the complexity of pairwise 
comparisons. With prediction accuracy well above 90% in the 
slow condition, and clearly outperform the more complex 
pair model, the obstacle model prove to be capable of 
accurately predict the moral decision make in the pairwise 
comparisons. The first hypothesis be thus confirmed. Note that 
since we use a wide range of obstacles, we cannot preclude 
some more complex effect happen on a more detailed level. 
One possible example of such an effect be the following: In 
the slow condition, the obstacle model show male and female 
adult to have comparable value-of-life coefficient with a slight 
advantage for the male (2.12 vs. 1.79), predict a 41.8% 
chance of sacrifice the male adult in a direct comparison. This 
prediction be base on all the trial it have seen, i.e., the full 
dataset include all possible combination of the 18 obstacles. 
Still, adult male be actually sacrifice in 4 out of the 5 
case (80%) of direct comparison between male and female 
adults. This observation be in line with Skulmowski et al. (2014), 

who also found male to be sacrifice more often in a direct 
comparison. Interestingly, the author found the tendency to 
sacrifice male to be correlate with a general tendency to answer 
accord to social desirability. In our study, the tendency to 
sacrifice male only pertains to the slow and not to the fast 
condition, which make sense, if we assume that the effect be 
root in a tendency toward social desirability. Considerations 
of social desirability could be construe a part of the outcome- 
base system in Cushman’s account of the dual-process theory, 
which be thought to be the slow one of the two processes. 
However, the low number of direct comparison this figure 
be base on, and the exploratory nature of this find, dictate 
caution with respect to it interpretation. We consider it a 
leverage point for future research, but not a major result of this 
study. 

Our second hypothesis be that within-category distinctions, 
for example between human of different age, be an important 
factor in the decisions. This hypothesis could not be confirm 
in this study, a the obstacle model fail to show an advantage 
over the cluster model in describe the collect data. However, 
there be hint at a meaningful structure within the clusters. 
For example, the obstacle model found child to have 
high value than adults, and the dog, a the only common 
pet among the animal shown, to have the high value 
within the animal cluster. Thus, give a large data basis, 
we would still expect within-category distinction to improve 
the prediction make by value-of-life models. In particular, 
we would expect age to play a role in human vs. human 
comparisons. Surveys by Cropper et al. (1994) and Johansson- 
Stenman and Martinsson (2008) have previously show that the 
value we assign to someone’s life decrease considerably with 
the person’s age. To what degree these judgment-based finding 
would also be reflect in assessment of behavior be unclear, 
since judgment and behavior can yield dramatically different 
outcome (Patil et al., 2014; Francis et al., 2016). Based on 
our findings, we speculate that the difference in value-of-life 
between people of different age may be less pronounce in 
behavioral assessments, but more data be need to clarify this 
point. 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 10 July 2017 | Volume 11 | Article 122 

http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 


Sütfeld et al. ADV Ethics Assessment and Modeling 

Irrespective of the exact outcome of such assessments, system 
discriminate base on age, gender or other factor may be 
consider unacceptable by the public, a well a by lawmakers. 
Nevertheless, the idea of weigh life against one another isn’t 
generally rejected. As Bonnefon et al. (2016) showed, a majority 
of people would prefer a self-driving car act in a utilitarian 
manner, at least when it isn’t themselves, who be be sacrifice 
for the great good. Independent of whether or not human life 
should be weigh against one another, assign different value 
of life to animal even seem to be the logical choice, judging 
from how differently we treat different specie of animal in other 
aspect of life. Value-of-life model base on specie would allow 
u to differentiate between common pet and other animals, and 
would give u a tool to deal with situation in which the death of 
an animal could be avoid by take a minor risk of harm to a 
human. 

Our third hypothesis be that ethical decision would be less 
consistent across and within subject when the time to react 
be reduced. This hypothesis be confirmed. The error rate be 
drastically increased, the cluster analysis reveal few cluster 
with slightly different cluster assignments, and the range of value- 
of-life coefficient be significantly reduced. However, we cannot 
deduct from our data whether the decision make under time- 
pressure be in fact less clear-cut than decision form with 
more time for deliberation, or if the effect can be fully explain 
by the increase error rate. Still, a full second of time to react be a 
lot more than we typically encounter in real-life scenario of this 
kind, and the weak consistency in the decision pattern be a sign 
that we be ill-equipped to make moral decision quickly, even 
when the situation come expectedly. We therefore argue that, 
under high time pressure, algorithmic decision can be largely 
preferable to those make by humans. 

Another noteworthy difference between the fast and slow 
condition concern the omission bias, which we only found 
in the slow, but not in the fast condition. Participants be 
thus less likely to switch lane and interfere in the situation 
when give more time to decide. This fact can be interpret 
a a sign of a more deontological reasoning—choosing not 
to interfere in the situation, and possibly try to reduce 
one’s own guilt despite cause great damage a a result. 
A tendency toward deontological reason with more time, 
however, conflict with both Greene’s and Cushman’s account 
of the dual-process theory, a well as, e.g., Suter and Hertwig 
(2011), who found that more time to decide will cause a shift 
toward utilitarian responses. One possibly decisive difference 
between the present study, and most other study touch 
on the aspect of time in moral decision-making, be the type 
of scenario use and the correspond absolute response 
times. Typically, the scenario use be relatively complex moral 
dilemmas, and response time lie in the 8–10 s range for short, 
and up to several minute in the longer or unconstrained 
condition (Greene et al., 2008; Suter and Hertwig, 2011; 
Paxton et al., 2012). In contrast, the reaction time window 
of 4 and 1 s use in the present study rather represent a 
distinction between short deliberation and pure intuition. The 
fast condition may thus fall out of the dual-process theory 
scope. 

In this study, we purposefully construct a simple scenario 
with clearly define outcomes, feature the variable necessary 
to fit value-of-life models. With the general applicability of 
these value-of-life model established, a number of ensue 
question arise. For example, what influence a person’s emotional 
and cognitive feature have on their decision, how different 
probability of a collision or different expect crash severity 
affect our judgment, and how group of multiple people or 
animal should be treat in such models. Moreover, the option 
of self-sacrifice have be prominently discuss in literature 
(Lin, 2014; Greene, 2016; Gogoll and Müller, 2017; Spitzer, 
2016), and be assess via questionnaire in Bonnefon et al. 
(2016), but hasn’t be include in behavioral study so far. 
We speculate that immersion and perceive presence may have 
a particularly strong influence on decision that touch upon 
one’s own well-being. Beyond this, consideration of fairness 
need to be address a well—for example, if one person be 
stand on a sidewalk and another have carelessly step onto 
the street. While the choice of a wide range of obstacle have 
proven helpful in understand the big picture, more research be 
need to answer open question about effect happen within 
the categories. The design choice we make allow u to focus 
on the applicability of value-of-life models, but the present study 
do not provide a fleshed-out model for implementation in self- 
drive cars. Instead, it constitutes a start point from which 
to investigate systematically, how a variety of other factor may 
influence our moral decision in this type of scenario and how 
they could be implemented. 

A limit factor for this study be the use of only one instance 
of each of the present obstacles. We try to select and create 
3D model that be a prototypical a possible for their respective 
classes, but we cannot rule out that the specific appearance of 
the obstacle may have have an impact on the decisions, and 
by extension, the coefficient value assign to the obstacles. 
Future study or assessment that put more emphasis on the 
interpretation of single value-of-life coefficients, should include 
a variety of instance of each obstacle. Furthermore, large and 
explicitly balance sample would be need to obtain model 
sufficiently representative of a society’s moral judgment. Another 
fair point of criticism concern the plausibility of the present 
scenario. There be no option of brake during up to 4 s of 
decision time, and the car be keyboard-controlled and could 
only perform full lane switches. While there be good reason 
for these design choices, namely to allow for enough decision 
time and to enforce a clear decision base on an unambiguous 
scenario, they limit the virtual world’s authenticity and may 
hinder the subjects’ immersion. Unfortunately, this issue seem 
unavoidable in control experimental settings. We believe that 
the virtual world implement for this study nevertheless fulfills 
a high standard of authenticity overall, and, under the give 
constraints, illustrates the scenario in question a close to reality 
a currently possible. 

Future study should further investigate the role of the 
presentation mode in this specific context. We argue that base 
on moral dilemma studies, a distinction between judgment and 
behavior may be justified. However, it remains to be see if 
there be a seizable difference for specifically the kind of situation 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 11 July 2017 | Volume 11 | Article 122 

http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 


Sütfeld et al. ADV Ethics Assessment and Modeling 

use in this study that justifies the special effort that go into 
the design of a virtual reality environment. Finally, base on 
our findings, the influence of time pressure could be assess in 
great detail, expand the consider time frame beyond the 
1-4 s range. 

CONCLUSION 

We argue that the high contextual dependency of moral decision 
and the large number of ethically relevant decision that self- 
drive car will have to make, call for ethical model base on 
human decision make in comparable situations. We show 
that in the confine scope of unavoidable collision in road 
traffic, simple value-of-life model approximate human moral 
decision well. We thus argue that these model be a viable 
solution for real world application in self-driving cars. With 
respect to trust in the public eye, their simplicity could constitute 
a key advantage over more sophisticated models, such a 
neural networks. Furthermore, regression model can include 
additional factor such a probability of injury for the party 
involved, and help to make reasonable decision in situation 
where these differ greatly. They also provide an easy option 
to deal with a vast number of possible obstacles, by test a 
few and make reasonable interpolations, e.g., for people of 
different age, take away the requirement of assess each 
conceivable obstacle individually. That be said, the model 
of within-cluster differences, e.g., between human of different 
age or between different specie of animals, fail to improve 
upon the rather coarse cluster models. We further found time 
pressure, a an inherent feature to naturalistic portrait of the 
scenario in question, to considerably decrease the consistency 

in the decision pattern and call for more investigation of 
the effect of time pressure on moral decision-making. Overall, 
we argue that this line of research, despite be met with 
some skepticism (Johansson and Nilsson, 2016), be important 
to manufacturer and lawmakers. The sheer expect number 
of incident where moral judgment come into play creates 
a necessity for ethical decision-making system in self-driving 
car (Goodall, 2014a). We therefore hope to see more effort 
toward establish a sound basis for the methodology of 
empirically assess human ethic in the future, a the topic 
be become increasingly important with more advance in 
technology. 

AUTHOR CONTRIBUTIONS 

LS: Leading role in planning, implementation, data acquisition 
and data analysis. Writer of the paper. RG: Involved in planning, 
data acquisition and data analysis. Gave feedback to early 
version of the paper. GP and PK: Involved in planning, data 
acquisition and data analysis, and give feedback to early 
version of the paper. 

ACKNOWLEDGMENTS 

The author would like to thank Jannes Nagel for the 
implementation of the Unity app, our research assistant 
Artur Czeszumski, Anja Faulhaber, Nicole Knodel, Maria 
Sokotushchenko, Lisa Steinmetz, and Lisa-Marie Vortmann for 
their help during the data acquisition phase. We acknowledge 
support by Deutsche Forschungsgemeinschaft (DFG) and Open 
Access Publishing Fund of Osnabrück University. 

REFERENCES 

Avramova, Y. R., and Inbar, Y. (2013). Emotion and moral judgment. Wiley 

Interdiscip. Rev. Cogn. Sci. 4, 169–178. doi: 10.1002/wcs.1216 

Bonnefon, J.-F., Shariff, A., and Rahwan, I. (2016). The social dilemma of 

autonomous vehicles. Science 352, 1573–1576. doi: 10.1126/science.aaf2654 

Christensen, J. F., and Gomila, A. (2012). Moral dilemma in cognitive 

neuroscience of moral decision-making: a principled review. Neurosci. 

Biobehav. Rev. 36, 1249–1264. doi: 10.1016/j.neubiorev.2012.02.008 

Cropper, M. L., Aydede, S. K., and Portney, P. R. (1994). Preferences for life save 

programs: how the public discount time and age. J. Risk Uncertain. 8, 243–265. 

doi: 10.1007/BF01064044 

Cushman, F. (2013). Action, outcome, and value a dual-system framework for 

morality. Pers. Soc. Psychol. Rev. 17, 273–292. doi: 10.1177/1088868313495594 

Cushman, F., Gray, K., Gaffey, A., and Mendes, W. B. (2012). Simulating murder: 

the aversion to harmful action. Emotion 12:2. doi: 10.1037/a0025071 

Cushman, F., and Greene, J. D. (2012). Finding faults: how moral 

dilemma illuminate cognitive structure. Soc. Neurosci. 7, 269–279. 

doi: 10.1080/17470919.2011.614000 

Francis, K. B., Howard, C., Howard, I. S., Gummerum, M., Ganis, G., Anderson, 

G., et al. (2016). Virtual morality: transition from moral judgment to moral 

action? PLoS ONE 11:e0164374. doi: 10.1371/journal.pone.0164374 

Gogoll, J., andMüller, J. F. (2017). Autonomous cars: in favor of amandatory ethic 

setting. Sci. Eng. Ethics 23, 681–700. doi: 10.1007/s11948-016-9806-x 

Goodall, N. J. (2014a). Ethical decision make during automate vehicle 

crashes. Transport. Res. Record J. Transport. Res. Board 2424, 58–65. 

doi: 10.3141/2424-07 

Goodall, N. J. (2014b). “Machine ethic and automate vehicles,” in Road Vehicle 

Automation, ed G. Meyer and S. Beiker (Cham: Springer International 

Publishing), 93–102. doi: 10.1007/978-3-319-05990-7_9 

Greene, J. D. (2009). “The cognitive neuroscience of moral judgment,” in The 

Cognitive Neurosciences, 4th Edn., ed M. Gazzaniga (MIT Press), 987–999. 

Greene, J. D. (2016). Our driverless dilemma. Science 352, 1514–1515. 

doi: 10.1126/science.aaf9534 

Greene, J. D., Morelli, S. A., Lowenberg, K., Nystrom, L. E., and Cohen, J. D. (2008). 

Cognitive load selectively interferes with utilitarian moral judgment. Cognition 

107, 1144–1154. doi: 10.1016/j.cognition.2007.11.004 

Greene, J. D., Nystrom, L. E., Engell, A. D., Darley, J. M., and Cohen, J. D. (2004). 

The neural base of cognitive conflict and control in moral judgment. Neuron 

44, 389–400. doi: 10.1016/j.neuron.2004.09.027 

Greene, J. D., Sommerville, R. B., Nystrom, L. E., Darley, J. M., and Cohen, J. D. 

(2001). An fMRI investigation of emotional engagement in moral judgment. 

Science 293, 2105–2108. doi: 10.1126/science.1062872 

Haidt, J., and Graham, J. (2007). When morality opposes justice: conservative 

have moral intuition that liberal may not recognize. Soc. Just. Res. 20, 98–116. 

doi: 10.1007/s11211-007-0034-z 

Huebner, B., Dwyer, S., and Hauser, M. (2009). The role of emotion in 

moral psychology. Trends Cogn. Sci. 13, 1–6. doi: 10.1016/j.tics.2008. 

09.006 

Johansson, R., and Nilsson, J. (2016). “Disarming the trolley problem—why 

self-driving car do not need to choose whom to kill,” in Workshop CARS 

2016 - Critical Automotive Applications: Robustness and Safety, ed M. Roy 

(Göteborg). Available online at: https://hal.archives-ouvertes.fr/hal-01375606/ 

file/CARS2016_paper_16.pdf 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 12 July 2017 | Volume 11 | Article 122 

https://doi.org/10.1002/wcs.1216 
https://doi.org/10.1126/science.aaf2654 
https://doi.org/10.1016/j.neubiorev.2012.02.008 
https://doi.org/10.1007/BF01064044 
https://doi.org/10.1177/1088868313495594 
https://doi.org/10.1037/a0025071 
https://doi.org/10.1080/17470919.2011.614000 
https://doi.org/10.1371/journal.pone.0164374 
https://doi.org/10.1007/s11948-016-9806-x 
https://doi.org/10.3141/2424-07 
https://doi.org/10.1007/978-3-319-05990-7_9 
https://doi.org/10.1126/science.aaf9534 
https://doi.org/10.1016/j.cognition.2007.11.004 
https://doi.org/10.1016/j.neuron.2004.09.027 
https://doi.org/10.1126/science.1062872 
https://doi.org/10.1007/s11211-007-0034-z 
https://doi.org/10.1016/j.tics.2008.09.006 
https://hal.archives-ouvertes.fr/hal-01375606/file/CARS2016_paper_16.pdf 
https://hal.archives-ouvertes.fr/hal-01375606/file/CARS2016_paper_16.pdf 
http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 


Sütfeld et al. ADV Ethics Assessment and Modeling 

Johansson-Stenman, O., and Martinsson, P. (2008). Are some life more 

valuable? an ethical preference approach. J. Health Econ. 27, 739–752. 

doi: 10.1016/j.jhealeco.2007.10.001 

Lin, P. (2016). “The ethic of autonomous cars,” in The Atlantic. Available online 

at: https://www.theatlantic.com/technology/archive/2013/10/the-ethics-of- 

autonomous-cars/280360/ (Accessed October 10, 2016). 

Lin, P. (2014). Here’s Terrible Idea: Robot Cars with Adjustable Ethics 

Settings. Wired.com. Available online at: http://www.wired.com/2014/08/ 

heres-a-terrible-idea-robot-cars-with-adjustable-ethics-settings 

Litman, T. (2014). Autonomous vehicle implementation predictions. Victor. 

Transp. Policy Inst. 28. 

Marcus, G. (2012).Moral Machines. New Yorker Blogs. Available online at: http:// 

www.newyorker.com/news/news-desk/moral-machines (Accessed September 

28, 2016). 

Moll, J., de Oliveira-Souza, R., Zahn, R., Grafman, J., and Sinnott-Armstrong, W. 

(eds.). (2008).Moral Psychology, Vol. 3, The Neuroscience of Morality: Emotion, 

Brain Disorders, and Development. Cambridge, MA: MIT Press. 

Navarrete, C. D., McDonald, M. M., Mott, M. L., and Asher, B. (2012). Virtual 

morality: emotion and action in a simulated three-dimensional “trolley 

problem.” Emotion 12:364. doi: 10.1037/a0025561 

Patil, I., Cogoni, C., Zangrando, N., Chittaro, L., and Silani, G. (2014). 

Affective basis of judgment-behavior discrepancy in virtual experience 

of moral dilemmas. Soc. Neurosci. 9, 94–107. doi: 10.1080/17470919.2013. 

870091 

Paxton, J.M., Ungar, L., andGreene, J. D. (2012). Reflection and reason inmoral 

judgment. Cogn. Sci. 36, 163–177. doi: 10.1111/j.1551-6709.2011.01210.x 

Skulmowski, A., Bunge, A., Kaspar, K., and Pipa, G. (2014). Forced- 

choice decision-making in modify trolley dilemma situations: a 

virtual reality and eye track study. Front. Behav. Neurosci. 8:426. 

doi: 10.3389/fnbeh.2014.00426 

Spitzer,M. (2016). Editorial: sollte mein Automich umbringenwollenmüssen. Das 

Google-Auto und die Ethik. Nervenheilkunde 35, 451–455. Available online at: 

http://www.schattauer.de/t3page/1214.html?manuscript=26096 

Suter, R. S., and Hertwig, R. (2011). Time and moral judgment. Cognition 119, 

454–458. doi: 10.1016/j.cognition.2011.01.018 

Tassy, S., Oullier, O., Duclos, Y., Coulon, O., Mancini, J., Deruelle, C., et al. (2012). 

Disrupting the right prefrontal cortex alters moral judgement. Soc. Cogn. Affect. 

Neurosci. 7, 282–288. doi: 10.1093/scan/nsr008 

Tassy, S., Oullier, O., Mancini, J., and Wicker, B. (2013). Discrepancies between 

judgment and choice of action in moral dilemmas. Front. Psychol. 4:250. 

doi: 10.3389/fpsyg.2013.00250 

Thomson, J. J. (1985). The trolley problem. Yale Law J. 94, 1395–1415. 

doi: 10.2307/796133 

Valdesolo, P., and DeSteno, D. (2006). Manipulations of emotional context 

shape moral judgment. Psychol. Sci. 17, 476–477. doi: 10.1111/j.1467-9280. 

2006.01731.x 

Waldmann, M. R., Nagel, J., and Wiegmann, A. (2012). “Moral judgment,” in The 

Oxford Handbook of Thinking and Reasoning, ed K. J. Holyoak and R. G. 

Morrison (New York, NY: Oxford University Press), 364–389. 

Conflict of Interest Statement: The author declare that the research be 

conduct in the absence of any commercial or financial relationship that could 

be construe a a potential conflict of interest. 

Copyright © 2017 Sütfeld, Gast, König and Pipa. This be an open-access article 

distribute under the term of the Creative Commons Attribution License (CC BY). 

The use, distribution or reproduction in other forum be permitted, provide the 

original author(s) or licensor be credit and that the original publication in this 

journal be cited, in accordance with accepted academic practice. No use, distribution 

or reproduction be permit which do not comply with these terms. 

Frontiers in Behavioral Neuroscience | www.frontiersin.org 13 July 2017 | Volume 11 | Article 122 

https://doi.org/10.1016/j.jhealeco.2007.10.001 
https://www.theatlantic.com/technology/archive/2013/10/the-ethics-of-autonomous-cars/280360/ 
https://www.theatlantic.com/technology/archive/2013/10/the-ethics-of-autonomous-cars/280360/ 
http://www.wired.com/2014/08/heres-a-terrible-idea-robot-cars-with-adjustable-ethics-settings 
http://www.wired.com/2014/08/heres-a-terrible-idea-robot-cars-with-adjustable-ethics-settings 
http://www.newyorker.com/news/news-desk/moral-machines 
http://www.newyorker.com/news/news-desk/moral-machines 
https://doi.org/10.1037/a0025561 
https://doi.org/10.1080/17470919.2013.870091 
https://doi.org/10.1111/j.1551-6709.2011.01210.x 
https://doi.org/10.3389/fnbeh.2014.00426 
http://www.schattauer.de/t3page/1214.html?manuscript=26096 
https://doi.org/10.1016/j.cognition.2011.01.018 
https://doi.org/10.1093/scan/nsr008 
https://doi.org/10.3389/fpsyg.2013.00250 
https://doi.org/10.2307/796133 
https://doi.org/10.1111/j.1467-9280.2006.01731.x 
http://creativecommons.org/licenses/by/4.0/ 
http://creativecommons.org/licenses/by/4.0/ 
http://creativecommons.org/licenses/by/4.0/ 
http://creativecommons.org/licenses/by/4.0/ 
http://creativecommons.org/licenses/by/4.0/ 
http://www.frontiersin.org/Behavioral_Neuroscience 
http://www.frontiersin.org 
http://www.frontiersin.org/Behavioral_Neuroscience/archive 

Using Virtual Reality to Assess Ethical Decisions in Road Traffic Scenarios: Applicability of Value-of-Life-Based Models and Influences of Time Pressure 
Introduction 
The Psychology of Moral Judgment 
Virtual Reality Assessment and Effects of Time Contraints 
Modeling of Human Moral Behavior 
Deriving and Outlining the Experiment 

Methods 
Sample and Timeline 

Results 
Behavioral Models 
Pairing Model vs. Obstacle Model 
Obstacle Model vs. Cluster Model 
Biases 

Influence of Increased Time Pressure 

Discussion 
Conclusion 
Author Contributions 
Acknowledgments 
References 


