




















































Eye Movements During Everyday Behavior Predict Personality Traits 


ORIGINAL RESEARCH 
published: 13 April 2018 

doi: 10.3389/fnhum.2018.00105 

Frontiers in Human Neuroscience | www.frontiersin.org 1 April 2018 | Volume 12 | Article 105 

Edited by: 

Antonio Fernández-Caballero, 

Universidad de Castilla-La Mancha, 

Spain 

Reviewed by: 

Mazyar Fallah, 

York University, Canada 

Bennett I. Berthenthal, 

Indiana University Bloomington, 

United States 

*Correspondence: 

Andreas Bulling 

bulling@mpi-inf.mpg.de 

Received: 06 November 2017 

Accepted: 05 March 2018 

Published: 13 April 2018 

Citation: 

Hoppe S, Loetscher T, Morey SA and 

Bulling A (2018) Eye Movements 

During Everyday Behavior Predict 

Personality Traits. 

Front. Hum. Neurosci. 12:105. 

doi: 10.3389/fnhum.2018.00105 

Eye Movements During Everyday 
Behavior Predict Personality Traits 
Sabrina Hoppe 1, Tobias Loetscher 2, Stephanie A. Morey 3 and Andreas Bulling 4* 

1Machine Learning and Robotics Lab, University of Stuttgart, Stuttgart, Germany, 2 School of Psychology, University of South 

Australia, Adelaide, SA, Australia, 3 School of Psychology, Flinders University, Adelaide, SA, Australia, 4 Perceptual User 

Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany 

Besides allow u to perceive our surroundings, eye movement be also a window 

into our mind and a rich source of information on who we are, how we feel, and 

what we do. Here we show that eye movement during an everyday task predict 

aspect of our personality. We tracked eye movement of 42 participant while they 

ran an errand on a university campus and subsequently assess their personality 

trait use well-established questionnaires. Using a state-of-the-art machine learn 

method and a rich set of feature encode different eye movement characteristics, 

we be able to reliably predict four of the Big Five personality trait (neuroticism, 

extraversion, agreeableness, conscientiousness) a well a perceptual curiosity only from 

eye movements. Further analysis reveal new relation between previously neglect 

eye movement characteristic and personality. Our finding demonstrate a considerable 

influence of personality on everyday eye movement control, thereby complement 

early study in laboratory settings. Improving automatic recognition and interpretation 

of human social signal be an important endeavor, enable innovative design of 

human–computer system capable of sense spontaneous natural user behavior to 

facilitate efficient interaction and personalization. 

Keywords: eye tracking, real world, personality, machine learning, gaze behavior, eye-based user model 

Eye movement facilitate efficient sample of visual information from the world around us. For 
example, in everyday social interactions, we often understand, predict, and explain the behavior and 
emotional state of others by how their eye move (Emery, 2000). The exact mechanism by which 
eye movement be controlled, and the range of factor that can influence it, be subject to intense 
research (Wolfe, 1994; Martinez-Conde et al., 2004; Foulsham et al., 2011; Rucci and Victor, 2015). 
Understanding the type of information eye movement convey be of current interest to a range 
of fields, from psychology and the social science to computer science (Henderson et al., 2013; 
Bulling et al., 2011; Bulling and Zander, 2014; Bixler and D’Mello, 2015; Steil and Bulling, 2015). 
One emerge body of research suggests that the way in which we move our eye be modulate 
by who we are—by our personality (Isaacowitz, 2005; Rauthmann et al., 2012; Risko et al., 2012; 
Baranes et al., 2015; Hoppe et al., 2015). 

Personality trait characterize an individual’s pattern of behavior, thinking, and feel (Kazdin, 
2000). Studies reporting relationship between personality trait and eye movement suggest that 
people with similar trait tend to move their eye in similar ways. Optimists, for example, spend less 
time inspect negative emotional stimulus (e.g., skin cancer images) than pessimist (Isaacowitz, 
2005). Individuals high in openness spend a longer time fixate and dwell on location when 
watch abstract animation (Rauthmann et al., 2012), and perceptually curious individual 
inspect more of the region in a naturalistic scene (Risko et al., 2012). But pioneer study on 
the association between personality and eye movement share two methodological limitations. 

https://www.frontiersin.org/journals/human-neuroscience 
https://www.frontiersin.org/journals/human-neuroscience#editorial-board 
https://www.frontiersin.org/journals/human-neuroscience#editorial-board 
https://www.frontiersin.org/journals/human-neuroscience#editorial-board 
https://www.frontiersin.org/journals/human-neuroscience#editorial-board 
https://doi.org/10.3389/fnhum.2018.00105 
http://crossmark.crossref.org/dialog/?doi=10.3389/fnhum.2018.00105&domain=pdf&date_stamp=2018-04-13 
https://www.frontiersin.org/journals/human-neuroscience 
https://www.frontiersin.org 
https://www.frontiersin.org/journals/human-neuroscience#articles 
https://creativecommons.org/licenses/by/4.0/ 
mailto:bulling@mpi-inf.mpg.de 
https://doi.org/10.3389/fnhum.2018.00105 
https://www.frontiersin.org/articles/10.3389/fnhum.2018.00105/full 
http://loop.frontiersin.org/people/511465/overview 
http://loop.frontiersin.org/people/40153/overview 
http://loop.frontiersin.org/people/498085/overview 
http://loop.frontiersin.org/people/537430/overview 


Hoppe et al. During Everyday Behavior 

First, these early study typically either investigate the 
link between gaze and personality descriptively (e.g., use 
correlation, Risko et al., 2012; Rauthmann et al., 2012) or 
predict single gaze characteristics, such a the number of 
fixation (Isaacowitz, 2005; Risko et al., 2012; Rauthmann 
et al., 2012), from personality scores. For practical applications, 
however, the more relevant question be whether, in turn, eye 
movement can be use to predict personality traits. Intriguingly, 
machine learn technique provide a way of answer this 
question without the need to make a-priori hypothesis about the 
importance of individual gaze characteristics. Instead, the most 
informative characteristic can be automatically determine from 
a potentially large and diverse set of eyemovement characteristic 
and patterns; thereby also uncover previously unknown link 
between personality and gaze. The potential of machine learn 
for predict behavior, cognitive state and personality have be 
highlight in a few study (Henderson et al., 2013; Bulling and 
Zander, 2014; Bixler and D’Mello, 2015; Hoppe et al., 2015). 
A recent laboratory study, for example, successfully predict 
people’s epistemic curiosity about answer to trivia question 
from oculomotor behavior (Baranes et al., 2015). 

The second limitation of early study be their restriction 
to laboratory condition – an approach that have be criticize 
because it may not lead to valid theory of human behavior 
in natural setting (Kingstone et al., 2003, 2008). In most 
studies, carefully select stimulus – such a images, animations, 
or trivia question – be present to participant for 
define duration on a computer screen, and participants’ eye 
movement be then related to the personality trait under 
investigation (Isaacowitz, 2005; Rauthmann et al., 2012; Risko 
et al., 2012; Baranes et al., 2015). However, principle guide 
the eye when look at computer screen and when engage 
in dynamic real-world behavior differ significantly (Foulsham 
et al., 2011; Tatler et al., 2011; Tatler, 2014). Compelling evidence 
for such difference be provide in a study which tracked eye 
movement of participant when they be explore different 
real world environment and when watch video of these 
environment (Marius’t Hart et al., 2009). The distribution of 
eye movement obtain in the laboratory only predict the 
gaze distribution in the laboratory with around 60% accuracy— 
indicate significant difference in eye movement between 
laboratory and real world situation (Foulsham et al., 2011). It 
therefore remains unclear whether these personality trait found 
to be related to eye movement in the laboratory (Isaacowitz, 
2005; Rauthmann et al., 2012; Risko et al., 2012; Baranes et al., 
2015) generalize to real-world behaviors. If so, then link between 
eye movement and personality have important ramification 
for the emerge field of social signal processing, social 
robotics, and eye-based user modeling. These interdisciplinary 
fields—at the intersection of computer science, social science, 
and psychology—focus on the development of system that 
can sense, model, and understand everyday human social 
signal (Vinciarelli et al., 2009;Wagner et al., 2011; Vinciarelli and 
Pentland, 2015) and that exhibit human-like behavior, include 
personality (Fong et al., 2003). Ultimately, such socially-aware 
computer have the potential to offer interactive capability that 
closely resemble natural human-human interactions. 

In the present work we demonstrate, for the first time, that 
the visual behavior of individual engage in an everyday task 
can predict four of the Big Five personality trait (McCrae and 
Costa, 2010), along with perceptual curiosity (Collins et al., 2004). 
To this end, we develop and study a large set of feature that 
describe various characteristic of everyday visual behavior. This 
approach go beyond exist analysis of individual feature 
and provide a principled demonstration of the link between 
eye movement and personality. Our finding not only validate 
the role of personality in explain eye movement behavior in 
daily life, they also reveal new eye movement characteristic a 
predictor of personality traits. 

1. METHODS 

Fifty student and staff of Flinders University participate in 
the study: 42 female and eight males, with a mean age of 21.9 
year (SD 5.5). The convenience sample be recruit through an 
advertisement on the School of Psychology’s online participation 
management system and the sample size be base on Risko 
et al. (2012). Written inform consent be obtain from all 
participant and participant receive AUD15 for take part 
in the study. Ethic approval be obtain from the Human 
Research Ethics Committee at Flinders University and the study 
be conduct in accordance with the Declaration of Helsinki. 

1.1. Apparatus 
Binocular gaze data be tracked use a state-of-the-art 
head-mounted video-based eye tracker from SensorMotoric 
Instruments (SMI) at 60Hz. The tracker have a report gaze 
estimation accuracy of 0.5◦ and precision of 0.1◦. The tracker 
record gaze data, along with a high-resolution scene video on 
a mobile phone that be carry in a cross-body bag. 

1.2. Questionnaires 
Personality trait be assess use three establish self-report 
questionnaires: 1) The NEO Five-Factor Inventory (NEO-FFI- 
3) comprise 60 question assess neuroticism, extraversion, 
openness, agreeableness, and conscientiousness (McCrae and 
Costa, 2010); 2) Perceptual Curiosity, a 16-item questionnaire 
assess a person’s interest in novel perceptual stimulation 
and visual-sensory inspection (Collins et al., 2004); and 3) 
the Curiosity and Exploration Inventory (CEI-II), a 10-item 
questionnaire assess trait curiosity (Kashdan et al., 2009). 

1.3. Procedure 
Upon arrival in the laboratory, participant be introduce 
to the study and fit with the eye tracker. The tracker be 
first calibrate use a standard 3-point calibration routine. 
Participants be then give AUD5 and instruct to walk 
around campus for approximately 10 min and to purchase any 
item of their choice (such a a drink or confectionary) from 
a campus shop of their choice. Upon return, the track be 
stop and the glass be removed. Participants be then 
ask to fill in the personality and curiosity questionnaires. 

Frontiers in Human Neuroscience | www.frontiersin.org 2 April 2018 | Volume 12 | Article 105 

https://www.frontiersin.org/journals/human-neuroscience 
https://www.frontiersin.org 
https://www.frontiersin.org/journals/human-neuroscience#articles 
lphilippe 
Texte surligné 

lphilippe 
Note 
Sample size of about 50 be rather low!!! 

lphilippe 
Note 



Hoppe et al. During Everyday Behavior 

2. DATA PROCESSING 

The data from one participant be lose due to technical 
problem with the eye track equipment. Any sample where the 
pupil could not be detected, or the gaze direction be estimate 
to be beyond 150% of it range, be marked a erroneous. 
Six participant with more than 50% erroneous sample in 
their record be exclude from further analysis; one other 
participant be exclude because gaze direction be estimate to 
be constant for 38% of samples. For the remain 42 participant 
an average of 12.51 minute (SD = 2.71) of eye track data 
be collected, with an average track loss of 19.58% (SD = 0.12). 
The record include an average 2.36 minute inside the shop 
(SD= 1.70). 

We independently bin personality score for each trait 
into three score range (low, medium, and high). The binning 
be perform in a data-driven fashion so that approximately 
one third of the participant be assign to each score 
range. The middle bin’s boundary be define a the score 
percentile at 1/3 and 2/3 respectively. Because personality score 
approximately follow a Gaussian distribution, the range of 
medium score be small than the range for the two extreme 
classes. Table 2 in the appendix list all result boundary 
between score ranges. 

Both data and source code be publicly available on GitHub1. 

2.1. Feature Extraction 
Following best practice in eye-based user model (Bulling 
et al., 2011), the time series of gaze data be process use 
a slide window approach to make the data independent of 
the individual duration of the record while not blurring out 
gaze characteristic due to average effects. That is, only data 
from a time window of a certain length be consider at one 
time. Different window size be evaluate during our training 
routine (see below for details). The window be slid over the 
entire record such that all subsequent window have an overlap 
of 50%. Time window that have more than 50% erroneous 
sample (i.e., where the pupil could not be detect or the gaze 
direction be estimate to be beyond 150% of it range), less 
than 2 non-erroneous samples, or not a single detect fixation 
or saccade, be discarded. For each result time window, a 
vector of 207 feature be extract (see the Appendix for a list 
of all features). These feature include: 

1. Statistics over raw gaze data: These be introduce 
in Baranes et al. (2015) for the detection of epistemic curiosity 
under laboratory conditions. Many of the feature be 
specific to the user interface used, for instance the distance 
of the participant’s gaze from a box in the interface but 
others such a minimum, mean and maximum of gaze x or 
y coordinate be adopt to our setting. 

2. Heatmaps of raw gaze data have be link to curiosity in 
a study on a static scene view task (Risko et al., 2012). 
Analogously, an 8 by 8 heatmap of gaze point have be 
extract here. Over time a heatmap cell corresponds to 
different place in the world due to head and body motion. 

1https://github.molgen.mpg.de/sabrina-hoppe/everyday-eye-movements- 

predict-personality 

Since some gaze point be extrapolate to position quite 
far from the actual scene video, gaze point be only use 
if they fell within the interval span 95% of the data in 
both horizontal and vertical direction. The heatmap cell be 
enumerate from 0 in the top left corner, through 7 in the top 
right corner, to 63 in the bottom right corner. 

3. Statistics over fixations, saccade and blink have frequently 
be use in eye track study (Bulling et al., 2011; 
Rauthmann et al., 2012; Risko et al., 2012). Fixations 
be detect use a dispersion-threshold algorithm with a 
threshold of 2.5% of the track range width (5) with an 
additional threshold on the minimum duration of 100ms. All 
movement between two fixation be inspect a candidate 
saccade and be accepted if they do not exceed a maximum 
duration of 500ms and have a peak velocity of at least 200% 
of the track range per second. Both fixation and saccade 
with more than 50% erroneous sample be discarded. 
Additionally, the eye track software provide information 
on blink and pupil diameter. From all event (i.e., fixations, 
saccades, and blinks), a number of statistic be compute 
such a the mean duration of fixation and the direction of 
saccades. A full list of these feature can be found in the 
Appendix. 

Note that “fixations” of up to 500ms be likely to include 
smooth pursuit that we do not consider separately since 
robust pursuit detection be still an open research question even 
for control laboratory setting (Hoppe and Bulling, 2016). 

4. Information on the temporal course of saccade and fixation 
have previously be encode in so-called n-gram feature 
for eye-based user model (Bulling et al., 2011). n-grams 
describe a series of gaze events, e.g., saccade with different 
amplitude (large or small) and direction bin into 8 
possible direction (e.g., [“long saccade up,” “short fixation,” 
“short saccade up”] for n = 3). Finally, a histogram of n- 
gram be compute by counting how often each n-gram, 
i.e., each possible combination of saccade and fixations, 
occurred. For each n between 1 and 4, the follow feature 
be extract from the histogram: number of different n- 
gram (i.e., number of non-zero entry in the histogram), 
maximum/minimum/mean/variance of the histogram entry 
and the most/least frequent n-gram. 

For each personality trait, a separate random forest 

classifier (Breiman, 2001) consist of 100 decision tree 
be train on these feature to predict one of the three 

personality score range (low, medium, high) use SCIKIT- 

LEARN (Pedregosa et al., 2011). Each decision tree resembles a 

tree-shaped flow-chart of decisions, where we set the maximum 
depth of each tree to 5 and allow up to 15 feature to be 

consider per decision. Before each training procedure, a 

standard scaler be fit to the training data and apply to both 
training and test sample to ensure a mean of zero and a standard 

deviation of one for each feature. 
We have no a priori hypothesis concern which window 

size for the sliding-window approach would be most effective, or 
which particular feature would be useful. We therefore chose an 
automatic approach name nest cross validation to optimize the 
open parameter during training, i.e., window size and feature 

Frontiers in Human Neuroscience | www.frontiersin.org 3 April 2018 | Volume 12 | Article 105 

https://github.molgen.mpg.de/sabrina-hoppe/everyday-eye-movements-predict-personality 
https://github.molgen.mpg.de/sabrina-hoppe/everyday-eye-movements-predict-personality 
https://github.molgen.mpg.de/sabrina-hoppe/everyday-eye-movements-predict-personality 
https://www.frontiersin.org/journals/human-neuroscience 
https://www.frontiersin.org 
https://www.frontiersin.org/journals/human-neuroscience#articles 


Hoppe et al. During Everyday Behavior 

selection. In a nutshell, a nest cross validation cycle through 
set of participants: one training set, one validation set, and 
one test set. For instance, in the first iteration, participant 1- 
32 might be use for training, participant 33–37 for validation, 
and participant 38–42 for testing. In the second iteration, 
participant 5–37 might be use for training, then participant 
10–42 and so on. In all iterations, several classifier base on 
different window size and subset of feature be train on 
the training set and evaluate on the validation set. The best 
perform window size and subset of feature be chosen base 
on the performance on the validation set. A classifier be then 
train on the union of training and validation set and test on 
the test set to generate the final performance score report here. 
It be important to select parameter base on performance on 
the validation set and then re-train and evaluate on another test 
set, because with this scheme, the parameter be never directly 
optimize for the final evaluation. Therefore, cross validation 
effectively mitigates the risk of overfitting—the algorithm be 
force to generalize to unseen data. 

2.2. Classifier Evaluation 
Classifier performance be evaluate in term of average F1 score 
across the three score ranges. The F1 score for a particular range 
R be define a the harmonic mean of precision (the probability 
that the true personality score range for a random person out 
of those for which R be predict be indeed R) and recall 
(the probability that R will be predict for a randomly chosen 
participant whose true personality score be within R). Since the 
training procedure for random forest classifier be inherently non- 
deterministic, we go through the whole nest cross-validation 
scheme 100 time with different initial random states. 

We compare our classifier against several random baseline 
to determine how likely our classification success be accord 
to simpler or trivial classifiers: 

1. Theoretical chance level: if all prediction be make 
uniformly at random and all score range be equally likely, 
the result F1 score for three balance class should be 0.33. 
Slight deviation from these assumptions, e.g., unbalanced 
classes, could in practice lead to different results. Thus, we 
implement a simple classifier that randomly sample one 
of the three score range for each person from a uniform 
distribution. 

2. Predicting the most frequent score range: For this evaluation, 
the training and test set be built in an identical manner to 
the actual training process, but instead of fitting a classifier, the 
most frequent score range on the training set be determine 
and then predict for every person in the test set. Note 
that this might be slightly different from the theoretical 33% 
because the split into training and test set might distort the 
label frequencies. 

3. The label permutation test (Ojala and Garriga, 2010) be 
propose to determine the level of performance after any 
relation between feature and score range be obfuscated, i.e., 
the training data be artificially shuffle such that the relation 
between gaze and personality be lost. If this classifier be able to 
perform above a theoretical chance level it might for instance 

have picked up class frequencies. Thus, it can serve a a test 
of how much actual information from the gaze feature be 
learn by our original classifier (Bode et al., 2012). 

Each of these baseline be compute 100 times, so a set of 100 
F1 score per baseline be obtain and compare to those of our 
classifier. 

3. RESULTS 

Figure 1 show the mean F1 score for our classifier a well a 
for all baseline for each trait. As can be see from the figure, 
our classifier performs well above chance (that is, confidence 
interval do not overlap with any of the baseline performances) 
for neuroticism (40.3%), extraversion (48.6%), agreeableness 
(45.9%), conscientiousness (43.1%), and perceptual curiosity 
(PCS, 37.1%). For openness (30.8%) and the Curiosity and 
Exploration Inventory (CEI, 27.2%) our classifier performs below 
chance level. 

In the above evaluation, all record data be use 
irrespective of participants’ context: that is, regardless of whether 
they be on their way to the shop, or inside the shop. To evaluate 
the reliability of classifier within and across different part of 
the recording, time at which people enter and left the shop 
be manually annotate base on the record scene video. 
We then compare their prediction across different subset 
of the data: (1) independent of the participant’s activity (two 
half of the recording: split halves); (2) within one activity 
(the way to the shop vs. the way back to the laboratory: way 
I vs. II); and (3) across activity (navigation on the way vs. 
shopping inside: shop vs. way). For each comparison, we use 
the 100 classifier train for the first part of the paper and 
reconstruct the prediction for single time window (i.e., 
the prediction before majority voting). Majority voting be 
perform over time window from the context in question only, 
such a from time window when the participant be inside 
the shop. As each classifier have be train and evaluate 100 
times, this lead to 100 pair of prediction for each comparison. 
Reliability be then evaluate by the average correlation 
between these pair of prediction after correction for the 
skewness of the sample distribution of correlation coefficients, 
use the Fisher transformation (Fisher, 1915). The result 
Pearson product-moment correlation coefficient be show in 
Table 1. The coefficient range from 0.39 to 0.83, indicate a 
moderate to strong correlation between these different real-world 
contexts. 

To investigate in more detail how eye movement 
characteristic be link to individual personality traits, we 
further calculate the relative importance of all feature from 
the random forest classifier a suggest in Breiman (2001). A 
random forest classifier comprises several decision trees. The 
importance of a feature in the random forest be define a it 
average importance across all the component decision trees. 
Within a single decision tree, a feature’s importance be define via 
all decision that be make base on that feature: the great the 
number of decision made, the small the mean classification 
error and the more data be pass through these decision in the 

Frontiers in Human Neuroscience | www.frontiersin.org 4 April 2018 | Volume 12 | Article 105 



Hoppe et al. During Everyday Behavior 

FIGURE 1 | Mean F1 score of 100 instance of our classifier and three baseline per trait. The whisker indicate the 95% confidence interval around the mean, 

compute by bootstrapping with 1,000 iteration on the set of 100 F1 score for each trait. All result be obtain use a cross-validation scheme such that only 

prediction for unseen participant be use for evaluation. The dash line show the theoretical chance level for a classifier that randomly pick one personality 

score range for each participant, independent of gaze. 

TABLE 1 | Pearson product-moment correlation coefficient of prediction 

obtain from different part of the recording: in the first half vs. the second half 

(split halves), on the way to the shop vs. on the way back to the laboratory (way I 

vs. II) and inside the shop vs. outside the shop (shop vs. way). 

half I vs. half II way I vs. way II shop vs. way 

Neuroticism 0.77 0.75 0.63 

Extraversion 0.83 0.75 0.61 

Openness 0.64 0.60 0.39 

Agreeableness 0.63 0.56 0.44 

Conscientiousness 0.69 0.72 0.43 

Perceptual Curiosity 0.68 0.65 0.46 

Curiosity and Exploration 0.68 0.65 0.44 

tree structure, the more important the feature that the decision 
be base on Breiman (2001). 

Figure 2 show the most important feature for our trait- 
specific classifier sort in ascend order by their median 
importance across all traits. The feature be chosen a the 
small set contain the individual ten most important 
feature for each trait accord to our method, a well a those 
feature previously link to personality in Rauthmann et al. 
(2012), Risko et al. (2012), and Baranes et al. (2015). 

As can be see from Figure 2, five of the 19 most important 
feature be link to n-grams (Bulling et al., 2011), which 
describe a series of n saccades. In contrast to the saccade- 
base n-grams, n-grams encode fixation–saccade sequence 
be less important. Heatmap feature similar to those in Risko 
et al. (2012), which capture how often a participant look 
into certain area of their visual field, be the second most 
important class of features. Moreover, the average variance in 
pupil diameter during fixation and blink rate turn out to be 
informative. Complementing the F1 score that be commonly 

report when evaluatingmachine learningmethods with respect 
to performance, we also provide correlation coefficient between 
personality score and the different eye movement feature 
extract from a slide window with a length of 15 s (see Table 3 
in the Appendix). 

4. DISCUSSION 

One key contribution of our work be to demonstrate, for the 
first time, that an individual’s level of neuroticism, extraversion, 
agreeableness, conscientiousness, and perceptual curiosity can 
be predict only from eye movement record during an 
everyday task. This find be important for bridging between 
tightly control laboratory study and the study of natural eye 
movement in unconstrained real-world environments. 

While prediction be not yet accurate enough for practical 
applications, they be clearly above chance level and outperform 
several baseline (see Figure 1). The propose machine learn 
approach be particularly successful in predict level of 
agreeableness, conscientiousness, extraversion, and perceptual 
curiosity. It therefore corroborates previous laboratory-based 
study that have show a link between personality trait and eye 
movement characteristic (Isaacowitz, 2005; Risko et al., 2012; 
Rauthmann et al., 2012; Baranes et al., 2015). 

The trait-specific eye movement characteristic be reliable: 
Comparing prediction after splitting the recording into two 
half yield reliability value range between 0.63 and 0.83, 
indicate moderate to strong correlation between prediction 
derive from the different half of the recording. The reliability 
value be low (0.39–0.63) when the prediction be 
base on the comparison between two task activity (walking 
and shopping). These finding suggest that trait-specific eye 
movement vary substantially across activities. Future work 

Frontiers in Human Neuroscience | www.frontiersin.org 5 April 2018 | Volume 12 | Article 105 

https://www.frontiersin.org/journals/human-neuroscience 
https://www.frontiersin.org 
https://www.frontiersin.org/journals/human-neuroscience#articles 
lphilippe 
Note 
Error bar related to random seed set. Nothing to do with error bar on input data, include error bar estimation on deem ground truth :) 



Hoppe et al. During Everyday Behavior 

FIGURE 2 | The top half of the figure show the importance of the top-10 feature for each trait, sort by their median importance across all traits. The bottom half 

show the importance of further feature that be related to personality or curiosity in prior work. The box represent the distribution over feature importance 

obtain from the 100 model we trained. Each of the box span the inter-quartile range (IQR); the whisker extend to the minimum and maximum. The dark bar 

inside each box represent the median. For each classifier, many feature remain unused and therefore have an importance of zero. Where most importance value 

be zero, the box be often invisible. 

could therefore establish which activity be best suit to elicit 
trait-specific eye movements, a this could significantly improve 
both prediction accuracy and reliability for practical applications. 

A second contribution of our work be to shed additional 
light on the close link between personality trait and an 
individual’s eye movements. Thanks to the machine learn 
approach, we could automatically analyze a large set of eye 
movement characteristic and rank them by their importance 
for personality trait prediction. Going beyond characteristic 
investigate in early works, this approach also allow u 
to identify new link between previously under-investigated 
eye movement characteristic and personality traits. This 
be possible because, unlike classical analysis approaches, 
the propose machine learn method do not rely on 
a priori hypothesis regard the importance of individual 
eye movement characteristics. Specifically, characteristic that 
capture rich temporal information on visual behavior seem 
to convey fundamental information related to all personality 
traits, and consistently outperform classic characteristic that 
have be isolated for investigation in laboratory situations, 
such a fixation duration (Isaacowitz, 2005; Rauthmann et al., 
2012; Risko et al., 2012). By extract the most important 
eye movement characteristic for each personality trait (see 
Figure 2) we also found that the importance of characteristic 
varies for different personality traits. For example, pupil diameter 
be important for predict neuroticism but be less useful 

for predict other traits. It be important to note that the 
goal of the current study be not to shed light on the 
underlie reason for why certain eye movement characteristic 
be more common in particular personality types. Instead, it 
be specifically design to explore whether machine learn 
can be use to classify personality from eye movement in an 
everyday task. 

The prediction accuracy and reliability score obtain from 
42 participant be very promising. However, in computer vision, 
state-of-the-art machine learningmethods be commonly train 
on million of sample (Russakovsky et al., 2015). These 
large-scale datasets have facilitate data-driven development and 
automatic learn of features, often outperform previous 
manually design characteristic (Le, 2013). For the field of 
personality research, obtain large datasets with a more 
representative sample of the general population than the 
convenience sample of the current study will be an important 
next step. Consequently, large-scale real-world gaze datasets 
be likely to improve automatic inference of personality 
and stimulate research on the automatic representation of 
gaze characteristics, with the potential to further improve 
performance a well a deepen our understand of the interplay 
between gaze and personality. Importantly, whether the poor 
performance of our algorithm in predict openness and CEI 
be due to the experimental design (relatively small sample and 
the specific task of run an errand) or due the possibility that 

Frontiers in Human Neuroscience | www.frontiersin.org 6 April 2018 | Volume 12 | Article 105 

https://www.frontiersin.org/journals/human-neuroscience 
https://www.frontiersin.org 
https://www.frontiersin.org/journals/human-neuroscience#articles 


Hoppe et al. During Everyday Behavior 

there be no link between openness and the way eye be move 
cannot be answer at this stage. 

Four important question arise from our findings: (1) How 
well do our finding generalize to non-university populations, 
different personality traits, different setting and other real-world 
activities? (2) How be the prediction of personality trait affected 
by temporary user states, such a mood, fatigue or even the 
person’s awareness of the eye tracker (Risko and Kingstone, 
2011)? (3) How do gaze-based signal interact with further social 
cue that be link to personality, such a body posture (Ball and 
Breese, 2000) or digital footprint (Youyou et al., 2015)? and (4) 
how can a system exploit several cue to derive a more holistic 
view on the user’s personality? 

Answering these question will guide research to improve our 
understand of how human eye movement be modulate 
in the real world (Kingstone et al., 2003; Risko and Kingstone, 
2011), and how they fit into the broad spectrum of human non- 
verbal behavior. In turn, improve theoretical understand will 
assist the emerge interdisciplinary research field of social signal 
processing, toward development of system that can recognize 
and interpret human social signal (Vinciarelli et al., 2009; 
Wagner et al., 2011; Vinciarelli and Pentland, 2015). 

Such knowledge of human non-verbal behavior might also 
be transfer to socially interactive robots, design to exhibit 
human-like behavior (Fong et al., 2003). These system might 
ultimately interact with human in a more natural and socially 
acceptable way, thereby become more efficient and flexible. 

AUTHOR CONTRIBUTIONS 

TL design and oversaw the study; SM collect the data; SH 
implement and evaluate the machine learn method and 
generate all result and figures; AB advise these analyses; SH, 
TL, and AB write the paper. 

FUNDING 

This work be funded, in part, by the Australian Research 
Council, the Cluster of Excellence on Multimodal Computing 
and Interaction (MMCI) at Saarland University, Germany, a 
well a a Ph.D. scholarship by the German National Academic 
Foundation. 

ACKNOWLEDGMENTS 

TL wish to thank Jason McCarley, Mike Nicholls, 
and the Brain and Cognition Laboratory (Flinders 
University) for valuable discussion when set up the 
experiment. 

SUPPLEMENTARY MATERIAL 

The Supplementary Material for this article can be found 
online at: https://www.frontiersin.org/articles/10.3389/fnhum. 
2018.00105/full#supplementary-material 

REFERENCES 

Ball, G., and Breese, J. (2000). Relating Personality and Behavior: Posture and 

Gestures. Berlin; Heidelberg: Springer. 

Baranes, A., Oudeyer, P. Y., and Gottlieb, J. (2015). Eye movement 

reveal epistemic curiosity in human observers. Vis. Res. 117, 81–90. 

doi: 10.1016/j.visres.2015.10.009 

Bixler, R., and D’Mello, S. K. (2015). Automatic gaze-based user-independent 

detection of mind wander during computerize reading. User Model. User 

Adapt. Inter. 26, 33–68. doi: 10.1007/s11257-015-9167-1 

Bode, S., Sewell, D. K., Lilburn, S., Forte, J. D., Smith, P. L., and Stahl, J. (2012). 

Predicting perceptual decision bias from early brain activity. J. Neurosci. 32, 

12488–12498. doi: 10.1523/JNEUROSCI.1708-12.2012 

Breiman, L. (2001). Random forests. Mach. Learn. 45, 5–32. 

doi: 10.1023/A:1010933404324 

Bulling, A., Ward, J. A., Gellersen, H., and Tröster, G. (2011). Eye movement 

analysis for activity recognition use electrooculography. IEEE Trans. Patt. 

Anal. Mach. Intell. 33, 741–753. doi: 10.1109/TPAMI.2010.86 

Bulling, A., and Zander, T. O. (2014). Cognition-aware computing. IEEE Perv. 

Comput. 13, 80–83. doi: 10.1109/MPRV.2014.42 

Collins, R. P., Litman, J. A., and Spielberger, C. D. (2004). The 

measurement of perceptual curiosity. Pers. Individ. Dif. 36, 1127–1141. 

doi: 10.1016/S0191-8869(03)00205-8 

Emery, N. J. (2000). The eye have it: the neuroethology, function 

and evolution of social gaze. Neurosci. Biobehav. Rev. 24, 581–604. 

doi: 10.1016/S0149-7634(00)00025-7 

Fisher, R. A. (1915). Frequency distribution of the value of the correlation 

coefficient in sample from an indefinitely large population. Biometrika 10, 

507–521. doi: 10.2307/2331838 

Fong, T., Nourbakhsh, I., and Dautenhahn, K. (2003). A survey 

of socially interactive robots. Rob. Auton. Syst. 42, 143–166. 

doi: 10.1016/S0921-8890(02)00372-X 

Foulsham, T., Walker, E., and Kingstone, A. (2011). The where, what and when of 

gaze allocation in the lab and the natural environment. Vis. Res. 51, 1920–1931. 

doi: 10.1016/j.visres.2011.07.002 

Henderson, J. M., Shinkareva, S. V., Wang, J., Luke, S. G., and Olejarczyk, J. 

(2013). Predicting cognitive state from eye movements. PLoS ONE 8:e64937. 

doi: 10.1371/journal.pone.0064937 

Hoppe, S., and Bulling, A. (2016). End-to-end eye movement detection use 

convolutional neural networks. arXiv preprint arXiv:1609.02452. 

Hoppe, S., Loetscher, T., Morey, S., and Bulling, A. (2015). “Recognition 

of curiosity use eye movement analysis,” in Proceedings of the 2017 

ACM International Joint Conference on Pervasive and Ubiquitous Computing 

(UbiComp 2015) (Osaka), 185–188. 

Isaacowitz, D. M. (2005). The gaze of the optimist. Pers. Soc. Psychol. Bull. 31, 

407–415. doi: 10.1177/0146167204271599 

Kashdan, T. B., Gallagher, M. W., Silvia, P. J., Winterstein, B. P., Breen, W. 

E., Terhar, D., et al. (2009). The curiosity and exploration inventory-II: 

Development, factor structure, and psychometrics. J. Res. Pers. 43, 987–998. 

doi: 10.1016/j.jrp.2009.04.011 

Kazdin, A. E. (ed.). (2000). Encyclopedia of Psychology, Vol. 1–8. Washington, DC: 

American Psychological Association. 

Kingstone, A., Smilek, D., and Eastwood, J. D. (2008). Cognitive ethology: a 

new approach for study human cognition. Br. J. Psychol. 99, 317–340. 

doi: 10.1348/000712607X251243 

Kingstone, A., Smilek, D., Ristic, J., Friesen, C. K., and Eastwood, J. D. 

(2003). Attention, researchers! it be time to take a look at the real 

world. Curr. Dir. Psychol. Sci. 12, 176–180. doi: 10.1111/1467-8721. 

01255 

Le, Q. V. (2013). “Building high-level feature use large scale unsupervised 

learning,” in IEEE International Conference on Acoustics, Speech and Signal 

Processing (Vancouver, BC: IEEE), 8595–8598. 

Marius’t Hart, B., Vockeroth, J., Schumann, F., Bartl, K., Schneider, E., 

Koenig, P., et al. (2009). Gaze allocation in natural stimuli: compare 

Frontiers in Human Neuroscience | www.frontiersin.org 7 April 2018 | Volume 12 | Article 105 

https://www.frontiersin.org/articles/10.3389/fnhum.2018.00105/full#supplementary-material 
https://doi.org/10.1016/j.visres.2015.10.009 
https://doi.org/10.1007/s11257-015-9167-1 
https://doi.org/10.1523/JNEUROSCI.1708-12.2012 
https://doi.org/10.1023/A:1010933404324 
https://doi.org/10.1109/TPAMI.2010.86 
https://doi.org/10.1109/MPRV.2014.42 
https://doi.org/10.1016/S0191-8869(03)00205-8 
https://doi.org/10.1016/S0149-7634(00)00025-7 
https://doi.org/10.2307/2331838 
https://doi.org/10.1016/S0921-8890(02)00372-X 
https://doi.org/10.1016/j.visres.2011.07.002 
https://doi.org/10.1371/journal.pone.0064937 
https://doi.org/10.1177/0146167204271599 
https://doi.org/10.1016/j.jrp.2009.04.011 
https://doi.org/10.1348/000712607X251243 
https://doi.org/10.1111/1467-8721.01255 
https://www.frontiersin.org/journals/human-neuroscience 
https://www.frontiersin.org 
https://www.frontiersin.org/journals/human-neuroscience#articles 


Hoppe et al. During Everyday Behavior 

free exploration to head-fixed view conditions. Vis. Cogn. 17, 1132–1158. 

doi: 10.1080/13506280902812304 

Martinez-Conde, S., Macknik, S. L., and Hubel, D. H. (2004). The role of 

fixational eye movement in visual perception. Nat. Rev. Neurosci. 5, 229–240. 

doi: 10.1038/nrn1348 

McCrae, R. R., and Costa, P. (2010). Neo nventories Professional Manual. Odessa, 

FL: Psychological Assessment Resources. 

Ojala, M., and Garriga, G. C. (2010). Permutation test for study classifier 

performance. J. Mach. Learn. Res. 11, 1833–1863. doi: 10.1109/ICDM.2009.108 

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et 

al. (2011). Scikit-learn: Machine learn in Python. J. Mach. Learn. Res. 12, 

2825–2830. 

Rauthmann, J. F., Seubert, C. T., Sachse, P., and Furtner, M. R. (2012). Eyes a 

window to the soul: Gazing behavior be related to personality. J. Res. Pers. 46, 

147–156. doi: 10.1016/j.jrp.2011.12.010 

Risko, E. F., Anderson, N. C., Lanthier, S., and Kingstone, A. (2012). Curious 

eyes: Individual difference in personality predict eye movement behavior in 

scene-viewing. Cognition 122, 86–90. doi: 10.1016/j.cognition.2011.08.014 

Risko, E. F., and Kingstone, A. (2011). Eyes wide shut: imply social 

presence, eye track and attention. Attent. Percept. Psychophys. 73, 291–296. 

doi: 10.3758/s13414-010-0042-1 

Rucci, M., and Victor, J. D. (2015). The unsteady eye: an information-processing 

stage, not a bug. Trends Neurosci. 38, 195–206. doi: 10.1016/j.tins.2015.01.005 

Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., et al. (2015). 

Imagenet large scale visual recognition challenge. Int. J. Comput. Vis. 115, 

211–252. doi: 10.1007/s11263-015-0816-y 

Steil, J., and Bulling, A. (2015). “Discovery of everyday human activity from 

long-term visual behaviour use topic models,” in Proceedings of the 2015 

ACM International Joint Conference on Pervasive and Ubiquitous Computing 

(UbiComp 2015) (Osaka), 75–85. 

Tatler, B. (2014). “Eye movement from laboratory to life,” in Current Trends in Eye 

Tracking Research, ed M. Horsley, M. Eliot, B. Knight, and R. Reilly (Cham: 

Springer). doi: 10.1007/978-3-319-02868-2_2 

Tatler, B. W., Hayhoe, M. M., Land, M. F., and Ballard, D. H. (2011). Eye guidance 

in natural vision: Reinterpreting salience. J. Vis. 11:5. doi: 10.1167/11.5.5 

Vinciarelli, A., Pantic, M., and Bourlard, H. (2009). Social signal processing: 

survey of an emerge domain. Image Vis. Comput. 27, 1743–1759. 

doi: 10.1016/j.imavis.2008.11.007 

Vinciarelli, A., and Pentland, A. S. (2015). New social signal in a new interaction 

world: the next frontier for social signal processing. IEEE Sys. Man Cybern. 

Magazine 1, 10–17. doi: 10.1109/MSMC.2015.2441992 

Wagner, J., Lingenfelser, F., Bee, N., and André, E. (2011). Social signal 

interpretation (ssi). Künstl Intell. 25, 251–256. doi: 10.1007/s13218-011-0115-x 

Wolfe, J. M. (1994). Guided search 2.0 a revise model of visual search. Psychon. 

Bull. Rev. 1, 202–238. doi: 10.3758/BF03200774 

Youyou, W., Kosinski, M., and Stillwell, D. (2015). Computer-based personality 

judgment be more accurate than those make by humans. Proc. Natl. Acad. 

Sci. U.S.A. 112, 1036–1040. doi: 10.1073/pnas.1418680112 

Conflict of Interest Statement: The author declare that the research be 

conduct in the absence of any commercial or financial relationship that could 

be construe a a potential conflict of interest. 

Copyright © 2018 Hoppe, Loetscher, Morey and Bulling. This be an open-access 

article distribute under the term of the Creative Commons Attribution License (CC 

BY). The use, distribution or reproduction in other forum be permitted, provide 

the original author(s) and the copyright owner be credit and that the original 

publication in this journal be cited, in accordance with accepted academic practice. 

No use, distribution or reproduction be permit which do not comply with these 

terms. 

Frontiers in Human Neuroscience | www.frontiersin.org 8 April 2018 | Volume 12 | Article 105 

https://doi.org/10.1080/13506280902812304 
https://doi.org/10.1038/nrn1348 
https://doi.org/10.1109/ICDM.2009.108 
https://doi.org/10.1016/j.jrp.2011.12.010 
https://doi.org/10.1016/j.cognition.2011.08.014 
https://doi.org/10.3758/s13414-010-0042-1 
https://doi.org/10.1016/j.tins.2015.01.005 
https://doi.org/10.1007/s11263-015-0816-y 
https://doi.org/10.1007/978-3-319-02868-2_2 
https://doi.org/10.1167/11.5.5 
https://doi.org/10.1016/j.imavis.2008.11.007 
https://doi.org/10.1109/MSMC.2015.2441992 
https://doi.org/10.1007/s13218-011-0115-x 
https://doi.org/10.3758/BF03200774 
https://doi.org/10.1073/pnas.1418680112 
http://creativecommons.org/licenses/by/4.0/ 
http://creativecommons.org/licenses/by/4.0/ 
http://creativecommons.org/licenses/by/4.0/ 
http://creativecommons.org/licenses/by/4.0/ 
http://creativecommons.org/licenses/by/4.0/ 
https://www.frontiersin.org/journals/human-neuroscience 
https://www.frontiersin.org 
https://www.frontiersin.org/journals/human-neuroscience#articles 

Eye Movements During Everyday Behavior Predict Personality Traits 
1. Methods 
1.1. Apparatus 
1.2. Questionnaires 
1.3. Procedure 

2. Data Processing 
2.1. Feature Extraction 
2.2. Classifier Evaluation 

3. Results 
4. Discussion 
Author Contributions 
Funding 
Acknowledgments 
Supplementary Material 
References 




