


















































Membership Inference Attacks Against 
Machine Learning Models 

Reza Shokri 
Cornell Tech 

shokri@cornell.edu 

Marco Stronati⇤ 
INRIA 

marco@stronati.org 

Congzheng Song 
Cornell 

cs2296@cornell.edu 

Vitaly Shmatikov 
Cornell Tech 

shmat@cs.cornell.edu 

Abstract—We quantitatively investigate how machine learn 
model leak information about the individual data record on 
which they be trained. We focus on the basic membership 
inference attack: give a data record and black-box access to 
a model, determine if the record be in the model’s training 
dataset. To perform membership inference against a target model, 
we make adversarial use of machine learn and train our own 
inference model to recognize difference in the target model’s 
prediction on the input that it train on versus the input 
that it do not train on. 

We empirically evaluate our inference technique on classi- 
fication model train by commercial “machine learn a a 
service” provider such a Google and Amazon. Using realistic 
datasets and classification tasks, include a hospital discharge 
dataset whose membership be sensitive from the privacy perspec- 
tive, we show that these model can be vulnerable to membership 
inference attacks. We then investigate the factor that influence 
this leakage and evaluate mitigation strategies. 

I. INTRODUCTION 

Machine learn be the foundation of popular Internet 
service such a image and speech recognition and natural lan- 
guage translation. Many company also use machine learn 
internally, to improve marketing and advertising, recommend 
product and service to users, or good understand the data 
generate by their operations. In all of these scenarios, ac- 
tivities of individual users—their purchase and preferences, 
health data, online and offline transactions, photo they take, 
command they speak into their mobile phones, location they 
travel to—are use a the training data. 

Internet giant such a Google and Amazon be already 
offering “machine learn a a service.” Any customer in 
possession of a dataset and a data classification task can upload 
this dataset to the service and pay it to construct a model. 
The service then make the model available to the customer, 
typically a a black-box API. For example, a mobile-app maker 
can use such a service to analyze users’ activity and query 
the result model inside the app to promote in-app purchase 
to user when they be most likely to respond. Some machine- 
learn service also let data owner expose their model to 
external user for query or even sell them. 
Our contributions. We focus on the fundamental question 
know a membership inference: give a machine learn 
model and a record, determine whether this record be use a 

⇤This research be perform while the author be at Cornell Tech. 

part of the model’s training dataset or not. We investigate this 
question in the most difficult setting, where the adversary’s 
access to the model be limited to black-box query that 
return the model’s output on a give input. In summary, 
we quantify membership information leakage through the 
prediction output of machine learn models. 

To answer the membership inference question, we turn 
machine learn against itself and train an attack model 
whose purpose be to distinguish the target model’s behavior 
on the training input from it behavior on the input that it 
do not encounter during training. In other words, we turn the 
membership inference problem into a classification problem. 

Attacking black-box model such a those built by com- 
mercial “machine learn a a service” provider require 
more sophistication than attack white-box model whose 
structure and parameter be know to the adversary. To 
construct our attack models, we invent a shadow training 
technique. First, we create multiple “shadow models” that 
imitate the behavior of the target model, but for which we 
know the training datasets and thus the ground truth about 
membership in these datasets. We then train the attack model 
on the label input and output of the shadow models. 

We developed several effective method to generate training 
data for the shadow models. The first method us black-box 
access to the target model to synthesize this data. The second 
method us statistic about the population from which the 
target’s training dataset be drawn. The third method assumes 
that the adversary have access to a potentially noisy version 
of the target’s training dataset. The first method do not 
assume any prior knowledge about the distribution of the target 
model’s training data, while the second and third method 
allow the attacker to query the target model only once before 
infer whether a give record be in it training dataset. 

Our inference technique be generic and not base on any 
particular dataset or model type. We evaluate them against 
neural networks, a well a black-box model train use 
Amazon ML and Google Prediction API. All of our experi- 
ments on Amazon’s and Google’s platform be do without 
know the learn algorithm use by these services, nor 
the architecture of the result models, since Amazon and 
Google don’t reveal this information to the customers. For our 
evaluation, we use realistic classification task and standard 
model-training procedure on concrete datasets of images, 
retail purchases, location traces, and hospital inpatient stays. In 



addition to demonstrate that membership inference attack 
be successful, we quantify how their success relates to the 
classification task and the standard metric of overfitting. 

Inferring information about the model’s training dataset 
should not be confuse with technique such a model in- 
version that use a model’s output on a hidden input to infer 
something about this input [17] or to extract feature that 
characterize one of the model’s class [16]. As explain 
in [27] and Section IX, model inversion do not produce an 
actual member of the model’s training dataset, nor, give a 
record, do it infer whether this record be in the training 
dataset. By contrast, the membership inference problem we 
study in this paper be essentially the same a the well-known 
problem of identify the presence of an individual’s data in a 
mixed pool give some statistic about the pool [3], [15], [21], 
[29]. In our case, however, the goal be to infer membership 
give a black-box API to a model of unknown structure, a 
oppose to explicit statistics. 

Our experimental result show that model create use 
machine-learning-as-a-service platform can leak a lot of in- 
formation about their training datasets. For multi-class clas- 
sification model train on 10,000-record retail transaction 
datasets use Google’s and Amazon’s service in default 
configurations, our membership inference achieves median 
accuracy of 94% and 74%, respectively. Even if we make 
no prior assumption about the distribution of the target 
model’s training data and use fully synthetic data for our 
shadow models, the accuracy of membership inference against 
Google-trained model be 90%. Our result for the Texas 
hospital discharge dataset (over 70% accuracy) indicate that 
membership inference can present a risk to health-care datasets 
if these datasets be use to train machine learn model 
and access to the result model be open to the public. 
Membership in such datasets be highly sensitive. 

We discus the root cause that make these attack possi- 
ble and quantitatively compare mitigation strategy such a 
limit the model’s prediction to top k classes, decrease 
the precision of the prediction vector, increase it entropy, 
or use regularization while training the model. 

In summary, this paper demonstrates and quantifies the 
problem of machine learn model leak information 
about their training datasets. To create our attack models, we 
developed a new shadow learn technique that work with 
minimal knowledge about the target model and it training 
dataset. Finally, we quantify how the leakage of membership 
information be related to model overfitting. 

II. MACHINE LEARNING BACKGROUND 
Machine learn algorithm help u good understand and 

analyze complex data. When the model be create use 
unsupervised training, the objective be to extract useful feature 
from the unlabeled data and build a model that explains it 
hidden structure. When the model be create use supervise 
training, which be the focus of this paper, the training record 
(as input of the model) be assign label or score (as 
output of the model). The goal be to learn the relationship 

between the data and the label and construct a model that can 
generalize to data record beyond the training set [19]. Model- 
training algorithm aim to minimize the model’s prediction er- 
ror on the training dataset and thus may overfit to this dataset, 
produce model that perform good on the training input 
than on the input drawn from the same population but not 
use during the training. Many regularization technique have 
be propose to prevent model from become overfitted 
to their training datasets while minimize their prediction 
error [19]. 

Supervised training be often use for classification and other 
prediction tasks. For example, a retailer may train a model 
that predicts a customer’s shopping style in order to offer her 
suitable incentives, while a medical researcher may train a 
model to predict which treatment be most likely to succeed 
give a patient’s clinical symptom or genetic makeup. 
Machine learn a a service. Major Internet company 
now offer machine learn a a service on their cloud 
platforms. Examples include Google Prediction API,1 Amazon 
Machine Learning (Amazon ML),2 Microsoft Azure Machine 
Learning (Azure ML),3 and BigML.4 

These platform provide simple APIs for upload the data 
and for training and query models, thus make machine 
learn technology available to any customer. For example, 
a developer may create an app that gather data from users, 
uploads it into the cloud platform to train a model (or update 
an exist model with new data), and then us the model’s 
prediction inside the app to improve it feature or good 
interact with the users. Some platform even envision data 
holder training a model and then share it with others 
through the platform’s API for profit.5 

The detail of the model and the training algorithm be 
hidden from the data owners. The type of the model may be 
chosen by the service adaptively, depend on the data and 
perhaps accuracy on validation subsets. Service provider do 
not warn customer about the consequence of overfitting and 
provide little or no control over regularization. For example, 
Google Prediction API hide all details, while Amazon ML 
provide only a very limited set of pre-defined option (L1- or 
L2-norm regularization). The model cannot be download 
and be access only through the service’s API. Service 
provider derive revenue mainly by charge customer for 
query through this API. Therefore, we treat “machine learn- 
ing a a service” a a black box. All inference attack we 
demonstrate in this paper be perform entirely through the 
services’ standard APIs. 

III. PRIVACY IN MACHINE LEARNING 

Before deal with inference attacks, we need to define 
what privacy mean in the context of machine learn or, 

1https://cloud.google.com/prediction 
2https://aws.amazon.com/machine-learning 
3https://studio.azureml.net 
4https://bigml.com 
5https://cloud.google.com/prediction/docs/gallery 

2 



alternatively, what it mean for a machine learn model to 
breach privacy. 

A. Inference about member of the population 
A plausible notion of privacy, know in statistical disclosure 

control a the “Dalenius desideratum,” state that the model 
should reveal no more about the input to which it be apply 
than would have be know about this input without apply 
the model. This cannot be achieve by any useful model [14]. 

A related notion of privacy appear in prior work on model 
inversion [17]: a privacy breach occurs if an adversary can 
use the model’s output to infer the value of unintended 
(sensitive) attribute use a input to the model. As observe 
in [27], it may not be possible to prevent this “breach” if 
the model be base on statistical fact about the population. 
For example, suppose that training the model have uncovered 
a high correlation between a person’s externally observable 
phenotype feature and their genetic predisposition to a certain 
disease. This correlation be now a publicly know scientific 
fact that allows anyone to infer information about the person’s 
genome after observe that person. 

Critically, this correlation applies to all member of a give 
population. Therefore, the model breach “privacy” not just of 
the people whose data be use to create the model, but also of 
other people from the same population, even those whose data 
be not use and whose identity may not even be know to 
the model’s creator (i.e., this be “spooky action at a distance”). 
Valid model generalize, i.e., they make accurate prediction 
on input that be not part of their training datasets. This 
mean that the creator of a generalizable model cannot do 
anything to protect “privacy” a define above because the 
correlation on which the model be based—and the inference 
that these correlation enable—hold for the entire population, 
regardless of how the training sample be chosen or how the 
model be create from this sample. 

B. Inference about member of the training dataset 
To bypass the difficulty inherent in define and protect 

privacy of the entire population, we focus on protect privacy 
of the individual whose data be use to train the model. This 
motivation be closely related to the original goal of differential 
privacy [13]. 

Of course, member of the training dataset be member 
of the population, too. We investigate what the model reveals 
about them beyond what it reveals about an arbitrary member 
of the population. Our ultimate goal be to measure the mem- 
bership risk that a person incurs if they allow their data to be 
use to train a model. 

The basic attack in this set be membership inference, 
i.e., determine whether a give data record be part of the 
model’s training dataset or not. When a record be fully know 
to the adversary, learn that it be use to train a particular 
model be an indication of information leakage through the 
model. In some cases, it can directly lead to a privacy breach. 
For example, know that a certain patient’s clinical record 
be use to train a model associate with a disease (e.g, to 

determine the appropriate medicine dosage or to discover the 
genetic basis of the disease) can reveal that the patient have this 
disease. 

We investigate the membership inference problem in the 
black-box scenario where the adversary can only supply input 
to the model and receive the model’s output(s). In some 
situations, the model be available to the adversary indirectly. 
For example, an app developer may use a machine-learning 
service to construct a model from the data collect by the app 
and have the app make API call to the result model. In this 
case, the adversary would supply input to the app (rather than 
directly to the model) and receive the app’s output (which be 
base on the model’s outputs). The detail of internal model 
usage vary significantly from app to app. For simplicity and 
generality, we will assume that the adversary directly supply 
input to and receives output from the black-box model. 

IV. PROBLEM STATEMENT 
Consider a set of label data record sample from some 

population and partition into classes. We assume that a 
machine learn algorithm be use to train a classification 
model that capture the relationship between the content of 
the data record and their labels. 

For any input data record, the model output the prediction 
vector of probabilities, one per class, that the record belongs 
to a certain class. We will also refer to these probability 
a confidence values. The class with the high confidence 
value be select a the predict label for the data record. 
The accuracy of the model be evaluate by measure how it 
generalizes beyond it training set and predicts the label of 
other data record from the same population. 

We assume that the attacker have query access to the model 
and can obtain the model’s prediction vector on any data 
record. The attacker know the format of the input and 
output of the model, include their number and the range of 
value they can take. We also assume that the attacker either 
(1) know the type and architecture of the machine learn 
model, a well a the training algorithm, or (2) have black-box 
access to a machine learn oracle (e.g., a “machine learn 
a a service” platform) that be use to train the model. In 
the latter case, the attacker do not know a priori the model’s 
structure or meta-parameters. 

The attacker may have some background knowledge about 
the population from which the target model’s training dataset 
be drawn. For example, he may have independently drawn 
sample from the population, disjoint from the target model’s 
training dataset. Alternatively, the attacker may know some 
general statistic about the population, for example, the 
marginal distribution of feature values. 

The set for our inference attack be a follows. The 
attacker be give a data record and black-box query access 
to the target model. The attack succeed if the attacker can 
correctly determine whether this data record be part of the 
model’s training dataset or not. The standard metric for attack 
accuracy be precision (what fraction of record infer a 
member be indeed member of the training dataset) and 

3 



(data record, class label) Target Model 

Attack Model 

data 2 training set ? 

predict(data) 

label 

prediction 

Fig. 1: Membership inference attack in the black-box setting. The 
attacker query the target model with a data record and obtains 
the model’s prediction on that record. The prediction be a vector of 
probabilities, one per class, that the record belongs to a certain class. 
This prediction vector, along with the label of the target record, be 
pass to the attack model, which infers whether the record be in 
or out of the target model’s training dataset. 

ML API 

Private Training Set Target Model 

Shadow Training Set 1 

Shadow Model 1 

Shadow Training Set 2 

Shadow Model 2 

. 
. 
. 

. 
. 
. 

Shadow Training Set k 
Shadow Model k 

train() 

train() 

train() 

train() 

Fig. 2: Training shadow model use the same machine learn 
platform a be use to train the target model. The training datasets 
of the target and shadow model have the same format but be disjoint. 
The training datasets of the shadow model may overlap. All models’ 
internal parameter be train independently. 

recall (what fraction of the training dataset’s member be 
correctly infer a member by the attacker). 

V. MEMBERSHIP INFERENCE 

A. Overview of the attack 

Our membership inference attack exploit the observation 
that machine learn model often behave differently on the 
data that they be train on versus the data that they “see” 
for the first time. Overfitting be a common reason but not the 
only one (see Section VII). The objective of the attacker be to 
construct an attack model that can recognize such difference 
in the target model’s behavior and use them to distinguish 
member from non-members of the target model’s training 
dataset base solely on the target model’s output. 

Our attack model be a collection of models, one for each 
output class of the target model. This increase accuracy of the 

attack because the target model produce different distribution 
over it output class depend on the input’s true class. 

To train our attack model, we build multiple “shadow” 
model intend to behave similarly to the target model. In 
contrast to the target model, we know the ground truth for each 
shadow model, i.e., whether a give record be in it training 
dataset or not. Therefore, we can use supervise training on 
the input and the correspond output (each label “in” or 
“out”) of the shadow model to teach the attack model how to 
distinguish the shadow models’ output on member of their 
training datasets from their output on non-members. 

Formally, let f 
target 

() be the target model, and let Dtrain 
target 

be it private training dataset which contains label data 
record (x{i}, y{i}) 

target 

. A data record x{i} 
target 

be the input to 
the model, and y{i} 

target 

be the true label that can take value 
from a set of class of size c 

target 

. The output of the target 
model be a probability vector of size c 

target 

. The element of 
this vector be in [0, 1] and sum up to 1. 

Let f 
attack 

() be the attack model. Its input x 
attack 

be com- 
pose of a correctly label record and a prediction vector 
of size c 

target 

. Since the goal of the attack be decisional 
membership inference, the attack model be a binary classifier 
with two output classes, “in” and “out.” 

Figure 1 illustrates our end-to-end attack process. For a 
label record (x, y), we use the target model to compute 
the prediction vector y = f 

target 

(x). The distribution of y 
(classification confidence values) depends heavily on the true 
class of x. This be why we pas the true label y of x in 
addition to the model’s prediction vector y to the attack 
model. Given how the probability in y be distribute around 
y, the attack model computes the membership probability 
Pr{(x, y) 2 Dtrain 

target 

}, i.e., the probability that ((x, y),y) 
belongs to the “in” class or, equivalently, that x be in the 
training dataset of f 

target 

(). 
The main challenge be how to train the attack model to 

distinguish member from non-members of the target model’s 
training dataset when the attacker have no information about the 
internal parameter of the target model and only limited query 
access to it through the public API. To solve this conundrum, 
we developed a shadow training technique that let u train 
the attack model on proxy target for which we do know the 
training dataset and can thus perform supervise training. 

B. Shadow model 
The attacker creates k shadow model f i 

shadow 

(). Each 
shadow model i be train on a dataset Dtrain 

shadow 

i of the same 
format a and distribute similarly to the target model’s train- 
ing dataset. These shadow training datasets can be generate 
use one of method described in Section V-C. We assume 
that the datasets use for training the shadow model be 
disjoint from the private dataset use to train the target model 
(8i,Dtrain 

shadow 

i \ Dtrain 
target 

= ;). This be the bad case for the 
attacker; the attack will perform even good if the training 
datasets happen to overlap. 

The shadow model must be train in a similar way to 
the target model. This be easy if the target’s training algorithm 

4 



Algorithm 1 Data synthesis use the target model 
1: procedure SYNTHESIZE(class : c) 
2: x RANDRECORD(.) . initialize a record randomly 
3: y⇤ 

c 

0 
4: j 0 
5: k k 

max 

6: for iteration = 1 · · · iter 
max 

do 
7: y f 

target 

(x) . query the target model 
8: if y 

c 

� y⇤ 
c 

then . accept the record 
9: if y 

c 

> conf 

min 

and c = argmax(y) then 
10: if rand() < y 

c 

then . sample 
11: return x . synthetic data 
12: end if 
13: end if 
14: x⇤ x 
15: y⇤ 

c 

y 
c 

16: j 0 
17: else 
18: j j + 1 
19: if j > rej 

max 

then . many consecutive reject 
20: k max(k 

min 

, dk/2e) 
21: j 0 
22: end if 
23: end if 
24: x RANDRECORD(x⇤, k) . randomize k feature 
25: end for 
26: return ? . fail to synthesize 
27: end procedure 

(e.g., neural networks, SVM, logistic regression) and model 
structure (e.g., the wiring of a neural network) be known. 
Machine learn a a service be more challenging. Here the 
type and structure of the target model be not known, but 
the attacker can use exactly the same service (e.g., Google 
Prediction API) to train the shadow model a be use to 
train the target model—see Figure 2. 

The more shadow models, the more accurate the attack 
model will be. As described in Section V-D, the attack model 
be train to recognize difference in shadow models’ behavior 
when these model operate on input from their own training 
datasets versus input they do not encounter during training. 
Therefore, more shadow model provide more training fodder 
for the attack model. 

C. Generating training data for shadow model 

To train shadow models, the attacker need training data 
that be distribute similarly to the target model’s training data. 
We developed several method for generate such data. 

Model-based synthesis. If the attacker do not have real 
training data nor any statistic about it distribution, he can 
generate synthetic training data for the shadow model use 
the target model itself. The intuition be that record that be 
classify by the target model with high confidence should 

be statistically similar to the target’s training dataset and thus 
provide good fodder for shadow models. 

The synthesis process run in two phases: (1) search, use 
a hill-climbing algorithm, the space of possible data record 
to find input that be classify by the target model with high 
confidence; (2) sample synthetic data from these records. After 
this process synthesizes a record, the attacker can repeat it until 
the training dataset for shadow model be full. 

See Algorithm 1 for the pseudocode of our synthesis 
procedure. First, fix class c for which the attacker want to 
generate synthetic data. The first phase be an iterative process. 
Start by randomly initialize a data record x. Assuming that 
the attacker know only the syntactic format of data records, 
sample the value for each feature uniformly at random from 
among all possible value of that feature. In each iteration, 
propose a new record. A propose record be accepted only 
if it increase the hill-climbing objective: the probability of 
be classify by the target model a class c. 

Each iteration involves propose a new candidate record by 
change k randomly select feature of the late accepted 
record x⇤. This be do by flip binary feature or resam- 
pling new value for feature of other types. We initialize k to 
k 

max 

and divide it by 2 when rej 
max 

subsequent proposal 
be rejected. This control the diameter of search around the 
accepted record in order to propose a new record. We set the 
minimum value of k to k 

min 

. This control the speed of the 
search for new record with a potentially high classification 
probability y 

c 

. 
The second, sample phase start when the target model’s 

probability y 
c 

that the propose data record be classify a 
belonging to class c be large than the probability for all 
other class and also large than a threshold conf 

min 

. This 
ensures that the predict label for the record be c, and that the 
target model be sufficiently confident in it label prediction. We 
select such record for the synthetic dataset with probability y⇤ 

c 

and, if selection fails, repeat until a record be selected. 
This synthesis procedure work only if the adversary can 

efficiently explore the space of possible input and discover 
input that be classify by the target model with high confi- 
dence. For example, it may not work if the input be high- 
resolution image and the target model performs a complex 
image classification task. 

Statistics-based synthesis. The attacker may have some statis- 
tical information about the population from which the target 
model’s training data be drawn. For example, the attacker 
may have prior knowledge of the marginal distribution of 
different features. In our experiments, we generate synthetic 
training record for the shadow model by independently 
sample the value of each feature from it own marginal 
distribution. The result attack model be very effective. 

Noisy real data. The attacker may have access to some data 
that be similar to the target model’s training data and can be 
consider a a “noisy” version thereof. In our experiment 
with location datasets, we simulate this by flip the (bi- 
nary) value of 10% or 20% randomly select features, then 

5 



(data record, class label) predict(data) (prediction, class label, “in” / “out”) 

Shadow Training Set 1 

Shadow Test Set 1 

Shadow Model 1 “in” Prediction Set 1 

“out” Prediction Set 1 

·· 
· 

·· 
· 

·· 
· 

Shadow Training Set k 

Shadow Test Set k 

Shadow Model k “in” Prediction Set k 

“out” Prediction Set k 

Attack Training Set 

Attack Model 

train() 

Fig. 3: Training the attack model on the input and output of the shadow models. For all record in the training dataset of a shadow model, 
we query the model and obtain the output. These output vector be label “in” and add to the attack model’s training dataset. We also 
query the shadow model with a test dataset disjoint from it training dataset. The output on this set be label “out” and also add to the 
attack model’s training dataset. Having construct a dataset that reflect the black-box behavior of the shadow model on their training and 
test datasets, we train a collection of c 

target 

attack models, one per each output class of the target model. 

training our shadow model on the result noisy dataset. 
This scenario model the case where the training data for the 
target and shadow model be not sample from exactly the 
same population, or else sample in a non-uniform way. 

D. Training the attack model 

The main idea behind our shadow training technique be that 
similar model train on relatively similar data record use 
the same service behave in a similar way. This observation be 
empirically borne out by our experiment in the rest of this 
paper. Our result show that learn how to infer membership 
in shadow models’ training datasets (for which we know the 
ground truth and can easily compute the cost function during 
supervise training) produce an attack model that successfully 
infers membership in the target model’s training dataset, too. 

We query each shadow model with it own training dataset 
and with a disjoint test set of the same size. The output on 
the training dataset be label “in,” the rest be label “out.” 
Now, the attacker have a dataset of records, the correspond 
output of the shadow models, and the in/out labels. The 
objective of the attack model be to infer the label from the 
record and correspond outputs. 

Figure 3 show how to train the attack model. For all 
(x, y) 2 Dtrain 

shadow 

i , compute the prediction vector y = 
f 

i 

shadow 

(x) and add the record (y,y, in) to the attack training 
set Dtrain 

attack 

. Let Dtest 
shadow 

i be a set of record disjoint from the 
training set of the ith shadow model. Then, 8(x, y) 2 Dtest 

shadow 

i 

compute the prediction vector y = f i 
shadow 

(x) and add the 
record (y,y, out) to the attack training set Dtrain 

attack 

. Finally, 
split Dtrain 

attack 

into c 
target 

partitions, each associate with a 
different class label. For each label y, train a separate model 
that, give y, predicts the in or out membership status for x. 

If we use model-based synthesis from Section V-C, all of 
the raw training data for the attack model be drawn from 
the record that be classify by the target model with high 
confidence. This be true, however, both for the record use in 
the shadow models’ training datasets and for the test record 
left out of these datasets. Therefore, it be not the case that 
the attack model simply learns to recognize input that be 
classify with high confidence. Instead, it learns to perform 
a much subtler task: how to distinguish between the training 
input classify with high confidence and other, non-training 
input that be also classify with high confidence. 

In effect, we convert the problem of recognize the com- 
plex relationship between member of the training dataset and 
the model’s output into a binary classification problem. Binary 
classification be a standard machine learn task, thus we can 
use any state-of-the-art machine learn framework or service 
to build the attack model. Our approach be independent of the 
specific method use for attack model training. For example, 
in Section VI we construct the attack model use neural 
network and also use the same black-box Google Prediction 
API that we be attacking, in which case we have no control 
over the model structure, model parameters, or training meta- 
parameters—but still obtain a work attack model. 

VI. EVALUATION 

We first describe the datasets that we use for evaluation, 
follow by the description of the target model and our exper- 
imental setup. We then present the result of our membership 
inference attack in several setting and study in detail how and 
why the attack work against different datasets and machine 
learn platforms. 

6 



A. Data 

CIFAR. CIFAR-10 and CIFAR-100 be benchmark datasets 
use to evaluate image recognition algorithm [24]. CIFAR-10 
be compose of 32⇥32 color image in 10 classes, with 6, 000 
image per class. In total, there be 50, 000 training image 
and 10, 000 test images. CIFAR-100 have the same format a 
CIFAR-10, but it have 100 class contain 600 image each. 
There be 500 training image and 100 test image per 
class. We use different fraction of this dataset in our attack 
experiment to show the effect of the training dataset size on 
the accuracy of the attack. 
Purchases. Our purchase dataset be base on Kaggle’s “ac- 
quire value shoppers” challenge dataset that contains shop- 
ping history for several thousand individuals.6 The purpose 
of the challenge be to design accurate coupon promotion 
strategies. Each user record contains his or her transaction 
over a year. The transaction include many field such a 
product name, store chain, quantity, and date of purchase. 

For our experiments, we derive a simplify purchase 
dataset (with 197, 324 records), where each record consists 
of 600 binary features. Each feature corresponds to a product 
and represent whether the user have purchase it or not. To 
design our classification tasks, we first cluster the record 
into multiple classes, each represent a different purchase 
style. In our experiments, we use 5 different classification 
task with a different number of class {2, 10, 20, 50, 100}. 
The classification task be to predict the purchase style of a 
user give the 600-feature vector. We use 10, 000 randomly 
select record from the purchase dataset to train the target 
model. The rest of the dataset contributes to the test set and 
(if necessary) the training set of the shadow models. 
Locations. We create a location dataset from the publicly 
available set of mobile users’ location “check-ins” in the 
Foursquare social network, restrict to the Bangkok area 
and collect from April 2012 to September 2013 [36].7 The 
check-in dataset contains 11, 592 user and 119, 744 locations, 
for a total of 1, 136, 481 check-ins. We filter out user with 
few than 25 check-in and venue with few than 100 visits, 
which left u with 5, 010 user profiles. For each location venue, 
we have the geographical position a well a it location type 
(e.g., Indian restaurant, fast food, etc.). The total number of 
location type be 128. We partition the Bangkok map into area 
of size 0.5km ⇥ 0.5km, yield 318 region for which we 
have at least one user check-in. 

Each record in the result dataset have 446 binary features, 
represent whether the user visit a certain region or 
location type, i.e., the user’s semantic and geographical profile. 
The classification task be similar to the purchase dataset. We 
cluster the location dataset into 30 classes, each represent 
a different geosocial type. The classification task be to predict 
the user’s geosocial type give his or her record. We use 1, 600 
randomly select record to train the target model. The rest 

6https://kaggle.com/c/acquire-valued-shoppers-challenge/data 
7https://sites.google.com/site/yangdingqi/home/foursquare-dataset 

of the dataset contributes to the test set and (if necessary) the 
training set of the shadow models. 
Texas hospital stays. This dataset be base on the Hospital 
Discharge Data public use file with information about inpa- 
tients stay in several health facilities,8 release by the Texas 
Department of State Health Services from 2006 to 2009. Each 
record contains four main group of attributes: the external 
cause of injury (e.g., suicide, drug misuse), the diagnosis 
(e.g., schizophrenia, illegal abortion), the procedure the pa- 
tient underwent (e.g., surgery) and some generic information 
such a the gender, age, race, hospital id, and length of stay. 

Our classification task be to predict the patient’s main proce- 
dure base on the attribute other than secondary procedures. 
We focus on the 100 most frequent procedures. The result 
dataset have 67, 330 record and 6, 170 binary features. We use 
10, 000 randomly select record to train the target model. 

Note that our experiment do not involve re-identification 
of know individual and fully comply with the data use 
agreement for the original Public Use Data File. 
MNIST. This be a dataset of 70, 000 handwritten digit 
format a 32 ⇥ 32 image and normalize so that the 
digit be locate at the center of the image.9 We use 10, 000 
randomly select image to train the target model. 
UCI Adult (Census Income). This dataset include 48, 842 
record with 14 attribute such a age, gender, education, 
marital status, occupation, work hours, and native country. 
The (binary) classification task be to predict if a person make 
over $50K a year base on the census attributes.10 We use 
10, 000 randomly select record to train the target model. 

B. Target model 

We evaluate our inference attack on three type of target 
models: two construct by cloud-based “machine learn a 
a service” platform and one we implement locally. In all 
cases, our attack treat the model a black boxes. For the 
cloud services, we do not know the type or structure of the 
model they create, nor the value of the hyper-parameters 
use during the training process. 
Machine learn a a service. The first cloud-based machine 
learn service in our study be Google Prediction API. With 
this service, the user uploads a dataset and obtains an API 
for query the result model. There be no configuration 
parameter that can be change by the user. 

The other cloud service be Amazon ML. The user cannot 
choose the type of the model but can control a few meta- 
parameters. In our experiments, we varied the maximum num- 
ber of pass over the training data and L2 regularization 
amount. The former determines the number of training epoch 
and control the convergence of model training; it default 
value be 10. The latter tune how much regularization be per- 
form on the model parameter in order to avoid overfitting. 

8https://www.dshs.texas.gov/THCIC/Hospitals/Download.shtm 
9http://yann.lecun.com/exdb/mnist 
10http://archive.ics.uci.edu/ml/datasets/Adult 

7 



0.5 

0.6 

0.7 

0.8 

0.9 

1 

1 2 3 4 5 6 7 8 9 10 

Pr 
ec 

be 
io 

n 

Classes 

CIFAR-10, CNN, Membership Inference Attack 

2500 
5000 

10000 
15000 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 2000 4000 6000 8000 10000 12000 14000 

Pr 
ec 

be 
io 

n 

Training Set Size 

CIFAR-10, CNN, Membership Inference Attack 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 5000 10000 15000 20000 25000 30000 

Pr 
ec 

be 
io 

n 

Training Set Size 

CIFAR-100, CNN, Membership Inference Attack 

Fig. 4: Precision of the membership inference attack against neural network train on CIFAR datasets. The graph show precision for 
different class while vary the size of the training datasets. The median value be connect across different training set sizes. The 
median precision (from the small dataset size to largest) be 0.78, 0.74, 0.72, 0.71 for CIFAR-10 and 1, 1, 0.98, 0.97 for CIFAR-100. Recall 
be almost 1 for both datasets. The figure on the left show the per-class precision (for CIFAR-10). Random guess accuracy be 0.5. 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

Accuracy 

Purchase Dataset, Amazon (10,1e-6), Membership Inference Attack 

Precision 
Recall 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

Accuracy 

Purchase Dataset, Amazon (100,1e-4), Membership Inference Attack 

Precision 
Recall 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

Accuracy 

Purchase Dataset, Google, Membership Inference Attack 

Precision 
Recall 

Fig. 5: Empirical CDF of the precision and recall of the membership inference attack against different class of the model train use 
Amazon ML (in two different configurations) and Google Prediction API on 10, 000 purchase records. 50, 75, 90-percentile of precision be 
0.74, 0.79, 0.84 on Amazon (10, 1e � 6), 0.84, 0.88, 0.91 on Amazon (100, 1e � 4), and 0.94, 0.97, 1 on Google, respectively. Recall be 
close to 1. 

We use the platform in two configurations: the default set 
(10, 1e� 6) and (100, 1e� 4). 

Neural networks. Neural network have become a very 
popular approach to large-scale machine learning. We use 
Torch7 and it nn packages,11 a deep-learning library that have 
be use and extend by major Internet company such a 
Facebook.12 

On CIFAR datasets, we train a standard convolutional neural 
network (CNN) with two convolution and max pool layer 
plus a fully connect layer of size 128 and a SoftMax layer. 
We use Tanh a the activation function. We set the learn 
rate to 0.001, the learn rate decay to 1e � 07, and the 
maximum epoch of training to 100. 

On the purchase dataset (see Section VI-A), we train a fully 
connect neural network with one hidden layer of size 128 
and a SoftMax layer. We use Tanh a the activation function. 
We set the learn rate to 0.001, the learn rate decay to 
1e� 07, and the maximum epoch of training to 200. 

11https://github.com/torch/nn 
12https://github.com/facebook/fblualib 

C. Experimental setup 

The training set and the test set of each target and shadow 
model be randomly select from the respective datasets, have 
the same size, and be disjoint. There be no overlap between the 
datasets of the target model and those of the shadow models, 
but the datasets use for different shadow model can overlap 
with each other. 

We set the training set size to 10, 000 for the purchase 
dataset a well a the Texas hospital-stay dataset, Adult dataset 
and the MNIST dataset. We set it to 1, 200 for the location 
dataset. We vary the size of the training set for the CIFAR 
datasets, to measure the difference in the attack accuracy. 
For the CIFAR-10 dataset, we choose 2, 500; 5, 000; 10, 000; 
and 15, 000. For the CIFAR-100 dataset, we choose 4, 600; 
10, 520; 19, 920; and 29, 540. 

The experiment on the CIFAR datasets be run lo- 
cally, against our own models, so we can vary the model’s 
configuration and measure the impact on the attack accu- 
racy. The experiment on the other datasets (purchases with 
{2, 10, 20, 50, 100} classes, Texas hospital stays, locations, 
Adult, and MNIST) be run against model train use 
either Google or Amazon services, where we have no visibility 

8 



0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

Accuracy 

Texas Dataset, Google, Membership Inference Attack 

Precision 
Recall 

Fig. 6: Precision and recall of the membership inference attack against 
the classification model train use Google Prediction API on the 
Texas hospital-stay dataset. 

into their choice of the model type and structure and little 
control over the training process (see Section VI-B). 

For the purchase dataset, we built target model on all plat- 
form (Google, Amazon, local neural networks) employ the 
same training dataset, thus enable u to compare the leakage 
from different models. We use similar training architecture 
for the attack model across different platforms: either a fully 
connect neural network with one hidden layer of size 64 
with ReLU (rectifier linear units) activation function and a 
SoftMax layer, or a Google-trained black-box model. 

We set the number of shadow model to 100 for the CIFAR 
datasets, 20 for the purchase dataset, 10 for the Texas hospital- 
stay dataset, 60 for the location dataset, 50 for the MNIST 
dataset, and 20 for the Adult dataset. Increasing the number 
of shadow model would increase the accuracy of the attack 
but also it cost. 

D. Accuracy of the attack 

The attacker’s goal be to determine whether a give record 
be part of the target model’s training dataset. We evaluate 
this attack by execute it on randomly reshuffle record from 
the target’s training and test datasets. In our attack evaluation, 
we use set of the same size (i.e, equal number of member 
and non-members) in order to maximize the uncertainty of 
inference, thus the baseline accuracy be 0.5. 

We evaluate the attack use the standard precision and 
recall metrics. Precision be the fraction of the record infer 
a member of the training dataset that be indeed members. 
Recall measure coverage of the attack, i.e., the fraction of 
the training record that the attacker can correctly infer a 
members. Most measurement be report per class because 
the accuracy of the attack can vary considerably for different 
classes. This be due to the difference in size and composition 
of the training data belonging to each class and highly depends 
on the dataset. 

The test accuracy of our target neural-network model with 
the large training datasets (15, 000 and 29, 540 records, 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

Accuracy 

Purchase Dataset, Membership Inference Attack 

Google 
Amazon (100,1e-4) 
Amazon (10,1e-6) 

Neural Network 

Fig. 7: Precision of the membership inference attack against model 
train on the same datasets but use different platforms. The attack 
model be a neural network. 

respectively) be 0.6 and 0.2 for CIFAR-10 and CIFAR-100, 
respectively. The accuracy be low, indicate that the model 
be heavily overfitted on their training sets. Figure 4 show 
the result of the membership inference attack against the 
CIFAR models. For both CIFAR-10 and CIFAR-100, the 
attack performs much good than the baseline, with CIFAR- 
100 especially vulnerable. 

Table I show the training and test accuracy of the model 
construct use different machine learn platform for the 
purchase dataset with 100 classes. Large gap between training 
and test accuracy indicate overfitting. Larger test accuracy 
indicates good generalizability and high predictive power. 

Figure 5 show the result of the membership inference 
attack against the black-box model train by Google’s and 
Amazon’s machine learn platforms. Figure 7 compare 
precision of the attack against these model with the attack 
against a neural-network model train on the same data. Mod- 
el train use Google Prediction API exhibit the big 
leakage. 

For the Texas hospital-stay dataset, we evaluate our attack 
against a Google-trained model. The training accuracy of the 
target model be 0.66 and it test accuracy be 0.51. Figure 6 
show the accuracy of membership inference. Precision be 
mostly above 0.6, and for half of the classes, it be above 0.7. 
Precision be above 0.85 for more than 20 classes. 

For the location dataset, we evaluate our attack against 
a Google-trained model. The training accuracy of the target 
model be 1 and it test accuracy be 0.66. Figure 8 show the 
accuracy of membership inference. Precision be between 0.6 
and 0.8, with an almost constant recall of 1. 

E. Effect of the shadow training data 

Figure 8 report precision of the attack train on the 
shadow model whose training datasets be noisy version of 
the real data (disjoint from the target model’s training dataset 
but sample from the same population). Precision drop a the 
amount of noise increases, but the attack still outperforms the 

9 



ML Platform Training Test 
Google 0.999 0.656 
Amazon (10,1e-6) 0.941 0.468 
Amazon (100,1e-4) 1.00 0.504 
Neural network 0.830 0.670 

TABLE I: Training and test accuracy of the model construct use 
different ML-as-a-service platform on the purchase dataset (with 100 
classes). 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

Precision 

Location Dataset, Google, Membership Inference Attack 

Real Data 
Noisy Data 10% 
Noisy Data 20% 

Fig. 8: Empirical CDF of the precision of the membership inference 
attack against the Google-trained model for the location dataset. 
Results be show for the shadow model train on real data and for 
the shadow model train on noisy data with 10% and 20% noise 
(i.e., x% of feature be replace with random values). Precision of 
the attack over all class be 0.678 (real data), 0.666 (data with 10% 
noise), and 0.613 (data with 20% noise). The correspond recall 
of the attack be 0.98, 0.99, and 1.00, respectively. 

baseline and, even with 10% of the feature in the shadows’ 
training data replace by random values, match the original 
attack. This demonstrates that our attack be robust even 
if the attacker’s assumption about the distribution of the 
target model’s training data be not very accurate. 

Figure 9 report precision of the attack when the attacker 
have no real data (not even noisy) for training his shadow mod- 
els. Instead, we use the marginal distribution of individual 
feature to generate 187, 300 synthetic purchase records, then 
train 20 shadow model on these records. 

We also generate 30, 000 synthetic record use the 
model-based approach present in Algorithm 1. In our ex- 
periments with the purchase dataset where record have 600 
binary features, we initialize k to k 

max 

= 128 and divide it 
by 2 when rej 

max 

= 10 subsequent proposal be rejected. 
We set it minimum value k 

min 

= 4. In the sample phase, 
we set the minimum confidence threshold conf 

min 

to 0.2. 
For our final set of sample records, the target model’s 

confidence in classify the record be 0.24 on average (just 
a bit over our threshold conf 

min 

= 0.2). On average, each 
synthetic record need 156 query (of propose records) 
during our hill-climbing two-phase process (see Section V-C). 
We train 8 shadow model on this data. 

Figure 9 compare precision of the attack when shadow 
model be train on real data versus shadow model train 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.2 0.4 0.6 0.8 1 

C 
um 

ul 
at 

iv 
e 

Fr 
ac 

tio 
n 

of 
C 

la 
s 

e 

Precision 

Purchase Dataset, Google, Membership Inference Attack 

Real Data 
Marginal-Based Synthetic 

Model-Based Synthetic 

Fig. 9: Empirical CDF of the precision of the membership inference 
attack against the Google-trained model for the purchase dataset. 
Results be show for different way of generate training data for 
the shadow model (real, synthetic generate from the target model, 
synthetic generate from marginal statistics). Precision of the attack 
over all class be 0.935 (real data), 0.795 (marginal-based synthetic 
data), and 0.896 (model-based synthetic data). The correspond 
recall of the attack be 0.994, 0.991, and 0.526, respectively. 

on synthetic data. The overall precision be 0.935 on real data 
compare to 0.795 for marginal-based synthetic and 0.895 
for model-based synthetics. The accuracy of the attack use 
marginal-based synthetic data be noticeably reduce versus real 
data, but be nevertheless very high for most classes. The attack 
use model-based synthetic data exhibit dual behavior. For 
most class it precision be high and close to the attack 
that use real data for shadow training, but for a few class 
precision be very low (less than 0.1). 

The reason for the attack’s low precision on some class 
be that the target classifier cannot confidently model the dis- 
tribution of data record belonging to these classes—because 
it have not see enough examples. These class be under- 
represent in the target model’s training dataset. For example, 
each of the class where the attack have less than 0.1 precision 
contributes under 0.6% of the target model’s training dataset. 
Some of these class have few than 30 training record (out 
of 10, 000). This make it very difficult for our algorithm to 
synthesize representative of these class when search the 
high-dimensional space of possible records. 

For the majority of the target model’s classes, our attack 
achieves high precision. This demonstrates that a membership 
inference attack can be train with only black-box access 
to the target model, without any prior knowledge about 
the distribution of the target model’s training data if the 
attacker can efficiently generate input that be classify by 
the target model with high confidence. 

F. Effect of the number of class and training data per class 

The number of output class of the target model contributes 
to how much the model leaks. The more classes, the more 
signal about the internal state of the model be available to 
the attacker. This be one of the reason why the result in Fig. 4 

10 



0.5 

0.6 

0.7 

0.8 

0.9 

1 

2 10 20 50 100 

At 
ta 

ck 
P 

re 
ci 

si 
on 

Number of Classes 

Purchase Dataset, Google, Membership Inference Attack 

Fig. 10: Precision of the membership inference attack against differ- 
ent purchase classification model train on the Google platform. 
The boxplots show the distribution of precision over different classi- 
fication task (with a different number of classes). 

be good for CIFAR-100 than for CIFAR-10. The CIFAR- 
100 model be also more overfitted to it training dataset. For 
the same number of training record per class, the attack 
performs good against CIFAR-100 than against CIFAR-10. 
For example, compare CIFAR-10 when the size of the training 
dataset be 2, 000 with CIFAR-100 when the size of the training 
dataset be 20, 000. The average number of data record per 
class be 200 in both cases, but the attack accuracy be much 
good (close to 1) for CIFAR-100. 

To quantify the effect that the number of class have 
on the accuracy of the attack, we train target model 
use Google Prediction API on the purchase dataset with 
{2, 10, 20, 50, 100} classes. Figure 10 show the distribution 
of attack precision for each model. Models with few class 
leak less information about their training inputs. As the 
number of class increases, the model need to extract more 
distinctive feature from the data to be able to classify input 
with high accuracy. Informally, model with more output 
class need to remember more about their training data, thus 
they leak more information. 

Figure 11 show the relationship between the amount of 
training data per class and the accuracy of membership infer- 
ence. This relationship be more complex, but, in general, the 
more data in the training dataset be associate with a give 
class, the low the attack precision for that class. 

Table II show the precision of membership inference 
against Google-trained models. For the MNIST dataset, the 
training accuracy of the target model be 0.984 and it test 
accuracy be 0.928. The overall precision of the membership 
inference attack be 0.517, which be just slightly above random 
guessing. The lack of randomness in the training data for each 
class and the small number of class contribute to the failure 
of the attack. 

For the Adult dataset, the training accuracy of the target 
model be 0.848 and it test accuracy be 0.842. The overall 
precision of the attack be 0.503, which be equivalent to random 

Dataset Training Testing Attack 
Accuracy Accuracy Precision 

Adult 0.848 0.842 0.503 
MNIST 0.984 0.928 0.517 
Location 1.000 0.673 0.678 
Purchase (2) 0.999 0.984 0.505 
Purchase (10) 0.999 0.866 0.550 
Purchase (20) 1.000 0.781 0.590 
Purchase (50) 1.000 0.693 0.860 
Purchase (100) 0.999 0.659 0.935 
TX hospital stay 0.668 0.517 0.657 

TABLE II: Accuracy of the Google-trained model and the corre- 
sponding attack precision. 

guessing. There could be two reason for why membership 
inference fails against this model. First, the model be not 
overfitted (its test and train accuracy be almost the same). 
Second, the model be a binary classifier, which mean that the 
attacker have to distinguish member from non-members by 
observe the behavior of the model on essentially 1 signal, 
since the two output be complement of each other. This 
be not enough for our attack to extract useful membership 
information from the model. 

G. Effect of overfitting 
The more overfitted a model, the more it leaks—but only 

for model of the same type. For example, the Amazon- 
train (100, 1e�4) model that, accord to Table I, be more 
overfitted leak more than the Amazon-trained (10, 1e � 6) 
model. However, they both leak less than the Google-trained 
model, even though the Google model be less overfitted than 
one of the Amazon model and have a much good predictive 
power (and thus generalizability) than both Amazon models. 
Therefore, overfitting be not the only factor that cause 
a model to be vulnerable to membership inference. The 
structure and type of the model also contribute to the problem. 

In Figure 11, we look deeper into the factor that contribute 
to attack accuracy per class, include how overfitted the 
model be and what fraction of the training data belongs to each 
class. The (train-test) accuracy gap be the difference between 
the accuracy of the target model on it training and test data. 
Similar metric be use in the literature to measure how 
overfitted a model be [18]. We compute this metric for each 
class. Bigger gap indicate that the model be overfitted on it 
training data for that class. The plot show that, a expected, 
big (train-test) accuracy gap be associate with high 
precision of membership inference. 

VII. WHY OUR ATTACKS WORK 
Table II show the relationship between the accuracy of 

our membership inference attack and the (train-test) gap of 
the target models. Figure 12 also illustrates how the target 
models’ output distinguish member of their training datasets 
from the non-members. This be the information that our attack 
exploits. 

Specifically, we look at how accurately the model predicts 
the correct label a well a it prediction uncertainty. The ac- 
curacy for class i be the probability that the model classifies an 

11 



0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 

At 
ta 

ck 
P 

re 
ci 

si 
on 

Fraction of the Training Set for a Class 

Purchase Dataset, 10-100 Classes, Google, Membership Inference Attack 

10 Classes 
20 Classes 
50 Classes 

100 Classes 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 

At 
ta 

ck 
P 

re 
ci 

si 
on 

Target Model (Train-Test) Accuracy Gap 

Purchase Dataset, 10-100 Classes, Google, Membership Inference Attack 

10 Classes 
20 Classes 
50 Classes 

100 Classes 
0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 

Ta 
rg 

et 
M 

od 
el 

(T 
ra 

in 
-T 

e 
t) 

Ac 
cu 

ra 
cy 

G 
ap 

Fraction of the Training Set For a Class 

Purchase Dataset, 10-100 Classes, Google, Membership Inference Attack 

10 Classes 
20 Classes 
50 Classes 

100 Classes 

Fig. 11: Relationship between the precision of the membership inference attack on a class and the (train-test) accuracy gap of the target 
model, a well a the fraction of the training dataset that belongs to this class. Each point represent the value for one class. The (train-test) 
accuracy gap be a metric for generalization error [18] and an indicator of how overfitted the target model is. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Prediction Uncertainty 

Purchase Dataset, 10 Classes, Google, Membership Inference Attack 

Members 
Non-members 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Prediction Uncertainty 

Purchase Dataset, 20 Classes, Google, Membership Inference Attack 

Members 
Non-members 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Prediction Uncertainty 

Purchase Dataset, 100 Classes, Google, Membership Inference Attack 

Members 
Non-members 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Prediction Accuracy 

Purchase Dataset, 10 Classes, Google, Membership Inference Attack 

Members 
Non-members 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Prediction Accuracy 

Purchase Dataset, 20 Classes, Google, Membership Inference Attack 

Members 
Non-members 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 
Prediction Accuracy 

Purchase Dataset, 100 Classes, Google, Membership Inference Attack 

Members 
Non-members 

Fig. 12: Classification uncertainty (top row) and prediction accuracy (bottom row) of the target model for the member of it training dataset 
vs. non-members, visualize for several sample classes. The difference between the member and non-member output distribution be among 
the factor that our attack exploit to infer membership. The accuracy of our attack be high for the model where the two distribution be 
more distinguishable (See Table II). 

input with label i a i. Prediction uncertainty be the normalize 
entropy of the model’s prediction vector: �1 

log(n) 

P 
i 

p 

i 

log(p 

i 

), 
where p 

i 

be the probability that the input belongs to class i, 
and n be the number of classes. The plot show that there 
be an observable difference between the output (both accuracy 
and uncertainty) of the model on the member input versus the 
non-member input in the case where our attack be successful. 

Success of membership inference be directly related to the 
(1) generalizability of the target model and (2) diversity of it 
training data. If the model overfits and do not generalize well 
to input beyond it training data, or if the training data be not 
representative, the model leak information about it training 
inputs. We quantify this relationship in Fig. 11. From the 
machine learn perspective, overfitting be harmful because 
it produce model that lack predictive power. In this paper, 
we show another harm of overfitting: the leakage of sensitive 
information about the training data. 

As we explain in Section VI, overfitting be not the only 
reason why our inference attack work. Different machine 
learn models, due to their different structures, “remember” 
different amount of information about their training datasets. 
This lead to different amount of information leakage even if 
the model be overfitted to the same degree (see Table I). 

VIII. MITIGATION 

As explain in Section VII, overfitting be an important 
(but not the only) reason why machine learn model leak 
information about their training datasets. Of course, overfitting 
be a canonical problem in machine learn because it limit 
the predictive power and generalizability of models. This 
mean that instead of the usual tradeoff between utility and 
privacy, machine learn research and privacy research have 
similar objective in this case. Regularization technique such 
a dropout [31] can help defeat overfitting and also strengthen 

12 



privacy guarantee in neural network [23]. Regularization be 
also use for objective perturbation in differentially private 
machine learn [9]. 

(Ideal) well-regularized model should not leak much infor- 
mation about their training data, and our attack can serve a 
a metric to quantify this. Also, model with a trivial structure 
(e.g., XOR of some input features) generalize to the entire 
universe and do not leak information. 

If the training process be differentially private [12], the 
probability of produce a give model from a training dataset 
that include a particular record be close to the probability of 
produce the same model when this record be not included. 
Differentially private model are, by construction, secure 
against membership inference attack of the kind developed in 
this paper because our attack operate solely on the output of 
the model, without any auxiliary information. One obstacle be 
that differentially private model may significantly reduce the 
model’s prediction accuracy for small ✏ values. In Section IX, 
we survey some of the related work in this area. 

In the case of machine learn a a service, platform 
operator such a Google and Amazon have significant re- 
sponsibility to the user of their services. In their current 
form, these service simply accept the data, produce a model 
of unknown type and structure, and return an opaque API to 
this model that data owner use a they see fit, without any 
understand that by do so, they may be leak out their 
data. Machine learn service do not inform their customer 
about the risk of overfitting or the harm that may result 
from model train on inadequate datasets (for example, with 
unrepresentative record or too few representative for certain 
classes). 

Instead, when adaptively choose a model for a customer- 
supply dataset, service such a Google Prediction API and 
Amazon ML should take into account not only the accuracy of 
the model but also the risk that it will leak information about 
it training data. Furthermore, they need to explicitly warn 
customer about this risk and provide more visibility into the 
model and the method that can be use to reduce this leakage. 
Our inference attack can be use a metric to quantify 
leakage from a specific model, and also to measure the 
effectiveness of future privacy protection technique deployed 
by machine-learning services. 

A. Mitigation strategy 
We quantitatively evaluate several defense against mem- 

bership inference. 
Restrict the prediction vector to top k classes. When the 
number of class be large, many class may have very small 
probability in the model’s prediction vector. The model will 
still be useful if it only output the probability of the most 
likely k classes. To implement this, we add a filter to the last 
layer of the model. The small k is, the less information the 
model leaks. In the extreme case, the model return only the 
label of the most likely class without reporting it probability. 
Coarsen precision of the prediction vector. To implement 
this, we round the classification probability in the prediction 

Purchase dataset Testing Attack Attack Attack 
Accuracy Total Accuracy Precision Recall 

No Mitigation 0.66 0.92 0.87 1.00 
Top k = 3 0.66 0.92 0.87 0.99 
Top k = 1 0.66 0.89 0.83 1.00 
Top k = 1 label 0.66 0.66 0.60 0.99 
Rounding d = 3 0.66 0.92 0.87 0.99 
Rounding d = 1 0.66 0.89 0.83 1.00 
Temperature t = 5 0.66 0.88 0.86 0.93 
Temperature t = 20 0.66 0.84 0.83 0.86 
L2 � = 1e� 4 0.68 0.87 0.81 0.96 
L2 � = 1e� 3 0.72 0.77 0.73 0.86 
L2 � = 1e� 2 0.63 0.53 0.54 0.52 

Hospital dataset Testing Attack Attack Attack 
Accuracy Total Accuracy Precision Recall 

No Mitigation 0.55 0.83 0.77 0.95 
Top k = 3 0.55 0.83 0.77 0.95 
Top k = 1 0.55 0.82 0.76 0.95 
Top k = 1 label 0.55 0.73 0.67 0.93 
Rounding d = 3 0.55 0.83 0.77 0.95 
Rounding d = 1 0.55 0.81 0.75 0.96 
Temperature t = 5 0.55 0.79 0.77 0.83 
Temperature t = 20 0.55 0.76 0.76 0.76 
L2 � = 1e� 4 0.56 0.80 0.74 0.92 
L2 � = 5e� 4 0.57 0.73 0.69 0.86 
L2 � = 1e� 3 0.56 0.66 0.64 0.73 
L2 � = 5e� 3 0.35 0.52 0.52 0.53 

TABLE III: The accuracy of the target model with different mitiga- 
tion technique on the purchase and Texas hospital-stay datasets (both 
with 100 classes), a well a total accuracy, precision, and recall of 
the membership inference attack. The relative reduction in the metric 
for the attack show the effectiveness of the mitigation strategy. 

vector down to d float point digits. The small d is, the 
less information the model leaks. 
Increase entropy of the prediction vector. One of the signal 
that membership inference exploit be the difference between 
the prediction entropy of the target model on it training input 
versus other inputs. As a mitigation technique for neural- 
network models, we can modify (or add) the softmax layer and 
increase it normalize temperature t > 0. The softmax layer 
convert the logits compute for each class into probabilities. 
For the logits vector z, the ith output of the softmax function 
with temperature t be e 

zi/t 

P 
j e 

zj/t 
. This technique, also use 

in knowledge distillation and information transfer between 
model [20], would increase the entropy of the prediction 
vector. Note that for a very large temperature, the output 
becomes almost uniform and independent of it input, thus 
leak no information. 
Use regularization. Regularization technique be use to 
overcome overfitting in machine learning. We use L 

2 

-norm 
standard regularization that penalizes large parameter by 
add � 

P 
i 

✓ 

2 

i 

to the model’s loss function, where ✓ 
i 

s be 
model’s parameters. We implement this technique with various 
value for the regularization factor �. The large � is, the 
strong the effect of regularization during the training. 

B. Evaluation of mitigation strategy 

To evaluate the effectiveness of different mitigation strate- 
gies, we implement all of them in locally train mod- 

13 



el over which we have full control. The inference attack, 
however, still assumes only black-box access to the result 
models. The baseline model for these experiment be a neural 
network with one hidden layer with 256 unit (for the purchase 
dataset) and 1,000 unit (for the Texas hospital-stay dataset). 
We use Tanh a the activation function. 

Table III show the result of our evaluation. It compare 
different mitigation strategy base on how they degrade the 
accuracy of our attack relative to the attack on a model 
that do not use any mitigation. The mitigation strategy 
that we implement do not impose any cost on the target 
model’s prediction accuracy, and in the case of regularization, 
the target model’s prediction accuracy increase a expected. 
Note that more regularization (by increase � even further) 
would potentially result in a significant reduction of the target 
model’s test accuracy, even if it foil membership inference. 
This be show in the table for � = 1e � 2 on the purchase 
dataset, and for � = 5e�3 on the Texas hospital stay dataset. 

Overall, our attack be robust against these mitigation strate- 
gies. Filtering out low-probability class from the predic- 
tion vector and limit the vector to the top 1 or 3 most 
likely class do not foil the attack. Even restrict the 
prediction vector to a single label (most likely class), 
which be the absolute minimum a model must output to 
remain useful, be not enough to fully prevent membership 
inference. Our attack can still exploit the mislabeling behavior 
of the target model because member and non-members of 
the training dataset be mislabeled differently (assigned to 
different wrong classes). If the prediction vector contains 
probability in addition to the labels, the model leak even 
more information that can be use for membership inference. 

Some of the mitigation method be not suitable for 
machine-learning-as-service APIs use by general application 
and services. Regularization, however, appear to be neces- 
sary and useful. As mention above, it (1) generalizes the 
model and improves it predictive power and (2) decrease 
the model’s information leakage about it training dataset. 
However, regularization need to be deployed carefully to 
avoid damage the model’s performance on the test datasets. 

IX. RELATED WORK 

Attacks on statistical and machine learn models. In [2], 
knowledge of the parameter of SVM and HMM model be 
use to infer general statistical information about the training 
dataset, for example, whether record of a particular race be 
use during training. By contrast, our inference attack work 
in a black-box setting, without any knowledge of the model’s 
parameters, and infer information about specific record in the 
training dataset, a oppose to general statistics. 

Homer et al. [21] developed a technique, which be further 
study in [3], [15], for infer the presence of a particular 
genome in a dataset, base on compare the publish statis- 
tic about this dataset (in particular, minor allele frequencies) 
to the distribution of these statistic in the general population. 
By contrast, our inference attack target train machine 
learn models, not explicit statistics. 

Fig. 13: Images produce by model inversion on a train CIFAR-10 
model. Top: airplane, automobile, bird, cat, deer. Bottom: dog, frog, 
horse, ship, truck. The image do not correspond to any specific 
image from the training dataset, be not human-recognizable, and at 
best (e.g., the truck class image) be vaguely similar to the average 
image of all object in a give class. 

Other attack on machine learn include [7], where the 
adversary exploit change in the output of a collaborative 
recommender system to infer input that cause these changes. 
These attack exploit temporal behavior specific to the recom- 
mender system base on collaborative filtering. 

Model inversion. Model inversion [16], [17] us the output 
of a model apply to a hidden input to infer certain feature 
of this input. See [27] for a detailed analysis of this attack and 
an explanation of why it do not necessarily entail a privacy 
breach. For example, in the specific case of pharmacogenetics 
analyze in [17], the model capture the correlation between 
the patient’s genotype and the dosage of a certain medicine. 
This correlation be a valid scientific fact that hold for all 
patients, regardless of whether they be include in the 
model’s training dataset or not. It be not possible to prevent 
disclosure due to population statistic [14]. 

In general, model inversion cannot tell whether a particular 
record be use a part of the model’s training dataset. Given 
a record and a model, model inversion work exactly the same 
way when the record be use to train the model and when 
it be not used. In the case of pharmacogenetics [17], model 
inversion produce almost identical result for member and 
non-members. Due to the overfitting of the model, the result 
be a little (4%) more accurate for the members, but this 
accuracy can only be measure in retrospect, if the adversary 
already know the ground truth (i.e., which record be indeed 
member of the model’s training dataset). By contrast, our goal 
be to construct a decision procedure that distinguishes member 
from non-members. 

Model inversion have also be apply to face recognition 
model [16]. In this scenario, the model’s output be set to 1 
for class i and 0 for the rest, and model inversion be use to 
construct an input that produce these outputs. This input be 
not an actual member of the training dataset but simply an 
average of the feature that “characterize” the class. 

In the face recognition scenario—and only in this specific 
scenario—each output class of the model be associate with a 
single person. All training image for this class be different 
photo of that person, thus model inversion construct an 
artificial image that be an average of these photos. Because 
they all depict the same person, this average be recognizable 

14 



(by a human) a that person. Critically, model inversion do 
not produce any specific image from the training dataset, which 
be the definition of membership inference. 

If the image in a class be diverse (e.g., if the class contains 
multiple individual or many different objects), the result of 
model inversion a use in [16] be semantically meaningless 
and not recognizable a any specific image from the training 
dataset. To illustrate this, we ran model inversion against 
a convolutional neural network13 train on the CIFAR-10 
dataset, which be a standard benchmark for object recognition 
models. Each class include different image of a single type 
of object (e.g., an airplane). Figure 13 show the image 
“reconstructed” by model inversion. As expected, they do not 
depict any recognizable object, let alone an image from the 
training dataset. We expect similar result for other models, 
too. For the pharmacogenetics model mention above, this 
form of model inversion produce an average of different 
patients’ genomes. For the model that classifies location trace 
into geosocial profile (see Section VI-A), it produce an 
average of the location trace of different people. In both 
cases, the result of model inversion be not associate with 
any specific individual or specific training input. 

In summary, model inversion produce the average of the 
feature that at best can characterize an entire output class. 
It do not (1) construct a specific member of the training 
dataset, nor (2) give an input and a model, determines if this 
specific input be use to train the model. 
Model extraction. Model extraction attack [32] aim to 
extract the parameter of a model train on private data. 
The attacker’s goal be to construct a model whose predictive 
performance on validation data be similar to the target model. 

Model extraction can be a step stone for infer 
information about the model’s training dataset. In [32], this be 
illustrate for a specific type of model call kernel logistic 
regression (KLR) [38]. In KLR models, the kernel function 
include a tiny fraction of the training data (so call “import 
points”) directly into the model. Since import point be 
parameter of the model, extract them result in the leakage 
of that particular part of the data. This result be very specific 
to KLR and do not extend to other type of model since 
they do not explicitly store training data in their parameters. 

Even for KLR models, leakage be not quantify other than 
via visual similarity of a few chosen import point and “the 
closest (in L1 norm) extract representers” on the MNIST 
dataset of handwritten digits. In MNIST, all member of a 
class be very similar (e.g., all member of the first class be 
different way of write digit “1”). Thus, any extract digit 
must be similar to all image in it class, whether this digit 
be in the training set or not. 
Privacy-preserving machine learning. Existing literature on 
privacy protection in machine learn focus mostly on how 
to learn without direct access to the training data. Secure 
multiparty computation (SMC) have be use for learn 
decision tree [26], linear regression function [11], Naive 

13https://github.com/Lasagne/Recipes/blob/master/modelzoo/cifar10 nin.py 

Bayes classifier [33], and k-means cluster [22]. The goal 
be to limit information leakage during training. The training 
algorithm be the same a in the non-privacy-preserving case, 
thus the result model be a vulnerable to inference attack 
a any conventionally train model. This also hold for the 
model train by compute on encrypt data [4], [6], [35]. 

Differential privacy [12] have be apply to linear and 
logistic regression [8], [37], support vector machine [28], risk 
minimization [5], [9], [34], deep learn [1], [30], learn 
an unknown probability distribution over a discrete population 
from random sample [10], and release hyper-parameters 
and classifier accuracy [25]. By definition, differentially pri- 
vate model limit the success probability of membership 
inference attack base solely on the model, which include 
the attack described in this paper. 

X. CONCLUSIONS 

We have designed, implemented, and evaluate the first 
membership inference attack against machine learn models, 
notably black-box model train in the cloud use Google 
Prediction API and Amazon ML. Our attack be a general, 
quantitative approach to understand how machine learn 
model leak information about their training datasets. When 
choose the type of the model to train or a machine learn 
service to use, our attack can be use a one of the selection 
metrics. 

Our key technical innovation be the shadow training tech- 
nique that train an attack model to distinguish the target 
model’s output on member versus non-members of it train- 
ing dataset. We demonstrate that shadow model use in this 
attack can be effectively create use synthetic or noisy data. 
In the case of synthetic data generate from the target model 
itself, the attack do not require any prior knowledge about 
the distribution of the target model’s training data. 

Membership in hospital-stay and other health-care datasets 
be sensitive from the privacy perspective. Therefore, our result 
have substantial practical privacy implications. 
Acknowledgments. Thanks to Adam Smith for explain 
differential privacy and the state of the art in membership 
inference attack base on explicit statistics. 

This work be support by the NSF grant 1409442 and a 
Google Research Award. 

REFERENCES 

[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Tal- 
war, and L. Zhang, “Deep learn with differential privacy,” in CCS, 
2016. 

[2] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and 
G. Felici, “Hacking smart machine with smarter ones: How to extract 
meaningful data from machine learn classifiers,” International Jour- 
nal of Security and Networks, vol. 10, no. 3, pp. 137–150, 2015. 

[3] M. Backes, P. Berrang, M. Humbert, and P. Manoharan, “Membership 
privacy in MicroRNA-based studies,” in CCS, 2016. 

[4] M. Barni, P. Failla, R. Lazzeretti, A. Sadeghi, and T. Schneider, “Privacy- 
preserve ECG classification with branching program and neural 
networks,” Trans. Info. Forensics and Security, vol. 6, no. 2, pp. 452– 
468, 2011. 

[5] R. Bassily, A. Smith, and A. Thakurta, “Private empirical risk minimiza- 
tion: Efficient algorithm and tight error bounds,” in FOCS, 2014. 

15 



[6] J. Bos, K. Lauter, and M. Naehrig, “Private predictive analysis on 
encrypt medical data,” J. Biomed. Informatics, vol. 50, pp. 234–243, 
2014. 

[7] J. Calandrino, A. Kilzer, A. Narayanan, E. Felten, and V. Shmatikov, 
““You might also like:” Privacy risk of collaborative filtering,” in S&P, 
2011. 

[8] K. Chaudhuri and C. Monteleoni, “Privacy-preserving logistic regres- 
sion,” in NIPS, 2009. 

[9] K. Chaudhuri, C. Monteleoni, and A. Sarwate, “Differentially private 
empirical risk minimization,” JMLR, vol. 12, pp. 1069–1109, 2011. 

[10] I. Diakonikolas, M. Hardt, and L. Schmidt, “Differentially private 
learn of structure discrete distributions,” in NIPS, 2015. 

[11] W. Du, Y. Han, and S. Chen, “Privacy-preserving multivariate statistical 
analysis: Linear regression and classification.” in SDM, 2004. 

[12] C. Dwork, “Differential privacy,” in Encyclopedia of Cryptography and 
Security. Springer, 2011, pp. 338–340. 

[13] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise to 
sensitivity in private data analysis,” in TCC, 2006. 

[14] C. Dwork and M. Naor, “On the difficulty of disclosure prevention in 
statistical database or the case for differential privacy,” J. Privacy and 
Confidentiality, vol. 2, no. 1, pp. 93–107, 2010. 

[15] C. Dwork, A. Smith, T. Steinke, J. Ullman, and S. Vadhan, “Robust 
traceability from trace amounts,” in FOCS, 2015. 

[16] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attack that 
exploit confidence information and basic countermeasures,” in CCS, 
2015. 

[17] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart, 
“Privacy in pharmacogenetics: An end-to-end case study of personalize 
Warfarin dosing,” in USENIX Security, 2014. 

[18] M. Hardt, B. Recht, and Y. Singer, “Train faster, generalize better: 
Stability of stochastic gradient descent,” in ICML, 2016. 

[19] T. Hastie, R. Tibshirani, J. Friedman, and J. Franklin, “The element 
of statistical learning: Data mining, inference and prediction,” The 
Mathematical Intelligencer, vol. 27, no. 2, pp. 83–85, 2005. 

[20] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural 
network,” arXiv:1503.02531, 2015. 

[21] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, 
J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig, 
“Resolving individual contribute trace amount of DNA to highly 
complex mixture use high-density SNP genotyping microarrays,” 
PLoS Genetics, vol. 4, no. 8, 2008. 

[22] G. Jagannathan and R. Wright, “Privacy-preserving distribute k-means 
cluster over arbitrarily partition data,” in KDD, 2005. 

[23] P. Jain, V. Kulkarni, A. Thakurta, and O. Williams, “To drop or not 
to drop: Robustness, consistency and differential privacy property of 
dropout,” arXiv:1503.02031, 2015. 

[24] A. Krizhevsky, “Learning multiple layer of feature from tiny images,” 
Master’s thesis, University of Toronto, 2009. 

[25] M. J. Kusner, J. R. Gardner, R. Garnett, and K. Q. Weinberger, 
“Differentially private Bayesian optimization,” in ICML, 2015. 

[26] Y. Lindell and B. Pinkas, “Privacy preserve data mining,” in CRYPTO, 
2000. 

[27] F. McSherry, “Statistical inference consider harmful,” 
https://github.com/frankmcsherry/blog/blob/master/posts/2016-06- 
14.md, 2016. 

[28] B. Rubinstein, P. Bartlett, L. Huang, and N. Taft, “Learning in a large 
function space: Privacy-preserving mechanism for SVM learning,” J. 
Privacy and Confidentiality, vol. 4, no. 1, p. 4, 2012. 

[29] S. Sankararaman, G. Obozinski, M. I. Jordan, and E. Halperin, “Ge- 
nomic privacy and limit of individual detection in a pool,” Nature 
Genetics, vol. 41, no. 9, pp. 965–967, 2009. 

[30] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in CCS, 
2015. 

[31] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut- 
dinov, “Dropout: A simple way to prevent neural network from over- 
fitting,” JMLR, vol. 15, no. 1, pp. 1929–1958, 2014. 

[32] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing 
machine learn model via prediction APIs,” in USENIX Security, 
2016. 

[33] J. Vaidya, M. Kantarcıoğlu, and C. Clifton, “Privacy-preserving Naive 
Bayes classification,” VLDB, vol. 17, no. 4, pp. 879–898, 2008. 

[34] M. Wainwright, M. Jordan, and J. Duchi, “Privacy aware learning,” in 
NIPS, 2012. 

[35] P. Xie, M. Bilenko, T. Finley, R. Gilad-Bachrach, K. Lauter, and 
M. Naehrig, “Crypto-nets: Neural network over encrypt data,” 
arXiv:1412.6181, 2014. 

[36] D. Yang, D. Zhang, and B. Qu, “Participatory cultural mapping base on 
collective behavior data in location-based social networks,” ACM TIST, 
vol. 7, no. 3, p. 30, 2016. 

[37] J. Zhang, Z. Zhang, X. Xiao, Y. Yang, and M. Winslett, “Functional 
mechanism: Regression analysis under differential privacy,” VLDB, 
vol. 5, no. 11, pp. 1364–1375, 2012. 

[38] J. Zhu and T. Hastie, “Kernel logistic regression and the import vector 
machine,” in NIPS, 2001. 

16 


