



































Addressing Bias in Machine Learning Algorithms: 
A Pilot Study on Emotion Recognition for 

Intelligent Systems 
Ayanna Howard1*, Cha Zhang2, Eric Horvitz2 
1School of Electrical and Computer Engineering 

Georgia Institute of Technology 
Atlanta, GA∗ 


2Microsoft Research 

Redmond, WA 


Abstract: Recently, there have be an explosion of cloud-based service that enable developer to include a spectrum of recognition 
services, such a emotion recognition, in their applications. The recognition of emotion be a challenge problem, and research have 
be do on building classifier to recognize emotion in the open world. Often, learn emotion model be train on data set 
that may not sufficiently represent a target population of interest. For example, many of these on-line service have focus on 
training and test use a majority representation of adult and thus be tune to the dynamic of mature faces. For application 
design to serve an old or young age demographic, use the output from these pre-defined model may result in low 
performance rate than when use a specialized classifier. Similar challenge with bias in performance arise in other situation 
where datasets in these large-scale on-line service have a non-representative ratio of the desire class of interest. We consider the 
challenge of provide application developer with the power to utilize pre-constructed cloud-based service in their application 
while still ensure satisfactory performance for their unique workload of cases. We focus on bias in emotion recognition a a 
representative scenario to evaluate an approach to improve recognition rate when an on-line pre-trained classifier be use for 
recognition of a class that may have a minority representation in the training set. We discus a hierarchical classification approach 
to address this challenge and show that the average recognition rate associate with the most difficult emotion for the minority class 
increase by 41.5% and the overall recognition rate for all class increase by 17.3% when use this approach. 

I.! INTRODUCTION 
Poor representation of people of different age and skin 

color in training data can lead to performance problem and 
bias for real-world classification task involve the visual 
attribute of people—such a detect facial expression or 
pose. These performance problem and bias be directly 
correlate with the problem of class-imbalance in the datasets 
use to train these machine learn algorithms. There have 
be a number of effort that have try to resolve this issue 
with training on imbalanced datasets [1], include use 
different form of re-sampling [2], adjust the decision 
threshold [3], or a mixture-of-experts approach which 
combine the result of many classifier [4]. The difficulty 
with imbalance be recognize why and when imbalanced data 
set be problematic. It can be difficult to distinguish between 
data associate with a low incidence class and noisy data when 
training a classifier. This be especially problematic when 
building model use cloud-based service that utilize 
training data drawn from readily available sources, such a 
photo crawl from the web. Tens-of-thousands of case 
might be download for such training, and the result 
datasets may be dominate by high prevalence age and skin 
color categories. Such broad scan of data lead to three 
challenge for application developers, include machine 
learn practitioner and roboticists. The first challenge be 


* Research perform while Visiting Researcher at Microsoft Research 

that, by use a cloud-based service, application developer 
typically cannot directly influence it behavior since they do 
not have access to the services’ internal processes. The second 
challenge be that the category which be derive from a 
representative quantity of majority data (and thus have 
likelihood representative of real-world data streams) may 
lead to incorrect outcome when a person in a minority age 
and skin color category us the system. In this instance, the 
learn converges base on theoretically acceptable 
outcome but it real-world outcome may not be socially 
acceptable. The third challenge deal with the amplification of 
the problem when the data source be or be almost completely 
influence by the dominant class, which may not be 
representative of the world culture, thus lead to the 
perpetuation of bias beliefs. Although the derivation of 
these problem be different, these situation can bias the 
classification results, especially when design learn 
algorithm for emotion recognition in intelligent systems. 

Emotion recognition be a grow area of interest, from 
interpret mood for effective caregiver robot to 
recognize a child’s anxiety level for therapeutic robots. 
Emotion recognition be the process of identify human 
emotion, typically via facial expressions, linguistics, voice, or 
even body gestures. Most machine learn algorithm for 
emotion recognition use image to track facial expression in 



order to identify basic emotion such a Happy, Angry, Fear, 
or Surprise. In fact, in a review of online emotion recognition 
APIs [5], over 50% of the intelligent software package 
available use facial expression to recognize emotions. 

Generalization of these algorithm for optimize 
performance in the open world be typically achieve by 
training on large set of unconstrained data collect ‘in the 
wild’. The term ‘in the wild’ mean image have be 
capture under natural condition with vary parameter 
such a environment and scenes, diverse illumination 
conditions, head pose and with occlusions. Unfortunately, 
most of these image sets, by the nature of their collection 
process, will have small collection of low incidence classes, 
such a those associate with a young or old age 
demographic, or with an ethnic minority. For example, in the 
yearly Emotion Recognition in the Wild (EmotiW) challenge 
[6], researcher be provide a dataset to benchmark the 
performance of their method on in-the-wild data. The dataset 
represent the typical expression class of Angry, Disgust, 
Fear, Happiness, Neutral, Sadness and Surprise, but focus 
primarily on scene with adults. 

In this paper, we examine the bias issue found in learn 
algorithm for intelligent system by focus on the emotion 
recognition problem. We first present baseline outcome for a 
cloud-based emotion recognition algorithm apply to image 
associate with a minority class, in this instance, children’s 
facial expressions. We then present a hierarchical approach 
that combine output from the cloud-based emotion 
recognition algorithm with a specialized learner, and show 
that this methodology can increase overall recognition result 
by 17.3%. We also verify that this hierarchical algorithm, 
when apply to an additional corpus of test data, show 
similar improvement in the recognition rate. This additional 
corpus be use to ass the performance of the hierarchical 
approach in generalize to new unseen images. We conclude 
by discuss future work need to address the problem of 
bias stem from poor representation of people in a large 
training set. 

II.! RELATED WORK 
Although there be a number of research effort focus 

on child that incorporate the recognition of emotion to 
enable their functionality, most have not do a systematic 
analysis of accuracy with respect to their emotion recognition 
algorithms. For example, with socially-interactive robots, a 
number of research robot such a Darwin [7], the iCub [8], 
Avatar [9], and the GRACE robot [10], use emotion to 
engage child in therapy or learning. Their analysis though 
be base on overall performance on child engagement and not 
on emotion recognition. 

Of those research effort that have focus on emotion 
recognition performance [11], very few have focus on 
children. In [12], researcher use their own video data 
collection of child to develop method to analyze change 
in facial expression of 50 child age 3 to 9, with a focus on 
problem solve scenarios. Their accuracy measure be not 
directly base on emotion but rather center on Facial 
Action Units, which, when blend together, can be use to 
represent emotions. In their application, they developed 

separate linear support vector machine to detect the presence 
or absence of one of 19 facial actions. The training set 
consist of 10000 image frame and the test set consist 
of 200 randomly sample frame from this set, result in a 
recognition rate for the facial action unit that range from 
61% to 100% depend on the unit. In [13], an approach for 
learn children’s affective state be present use three 
different type of neural network structures, namely a multi- 
stage radial basis function neural network, a probabilistic 
neural network, and a multi-class classification support vector 
machine (SVM). Using the Dartmouth Database of Children 
Faces [14], they subdivide image of child age 5 to 9 into 
a training set of 1040 image and a test set of 242 images. 
The training set be cluster into three affective classes: 
positive (Happy, Pleased, Surprised), negative (Disgust, Sad, 
Angry) and Neutral. They achieve a maximum overall 
recognition rate of 85% on the untrained facial test image 
use the multi-class classification SVM. In [15], a method 
be discuss for automatic recognition of facial expression 
for children. They validate their methodology on the full 
Dartmouth Database of Children Faces of 1280 image and 
their 8 emotion classes. The full image set be use for both 
training and test use a support vector machine, a C4.5 
decision tree, random forest and the multi-layer perceptron 
method. The SVM achieve the maximum overall recognition 
rate of 79%. They then test on the NIMH child emotional 
face picture set (NIMH-ChEFS) database [16] with 482 
image to ass the generalization accuracy of the classifier 
on new unseen image and achieve a maximum overall 
recognition rate of 68.4% when use the SVM that be 
train on the Dartmouth dataset. 

These represent several effort that have focus on the 
explicit evaluation of emotion recognition algorithm a 
apply to the domain of children’s emotional cues. In the next 
section, we discus baseline result derive from use a 
cloud-based classifier on publically available datasets of 
children’s facial expressions. 

III.! BASELINE RESULTS 
Recently, several cloud-based service have offer 

program library that enable developer to include 
emotion recognition capability in their image application 
[17]. These include, among others, Google’s Vision API 
(https://cloud.google.com/vision) and the Microsoft Emotion 
API (https://www.microsoft.com/cognitive-services), a 
component of Microsoft’s Cognitive Services. These cloud- 
base emotion recognition algorithm optimize their 
performance in the open world by training on large set of 
unconstrained data set collect ‘in the wild.’ To establish a 
baseline on the capability of learn and inference for a 
minority class, we evaluate the emotion recognition result 
associate with children’s facial expression use the 
Microsoft Emotion API, a deep learn neural network [18]. 
The emotion detect use the Microsoft Emotion API be 
Angry, Contempt, Disgust, Fear, Happy, Neutral, Sad, and 
Surprise. 

We select four datasets of children’s face that be 
publically available for research purpose and with publish 




Neutral Contempt Surprise 


Sad Disgust Fear 


Fear Angry Happy 

Fig. 1.! Example stimulus of child associate with the facial expression 
databases: Top: The Radboud Faces Database, Middle: The Dartmouth 
Database of Children’s Faces, Bottom: NIMH-ChEFS Database 

human inter-rater reliability measure associate with the 
emotion labels. The four datasets be the NIMH Child 
Emotional Faces Picture Set (NIMH-ChEFS) [16], the 
Dartmouth Database of Children’s Faces [14], the Radboud 
Faces Database [19], and the Child Emotions Picture Set 
(CEPS) [20] (Figure 1). The NIMH Child Emotional Faces 
Picture Set (NIMH-ChEFS) contains 534 image of child 
range in age from 10 to 17 year old with a mean age of 13.6 
year old. The picture set include 39 girl and 20 boy 
cover 5 emotion (Fear, Angry, Happy, Sad and Neutral). 
We select an inter-rater reliability value for inclusion in our 
evaluation at 75% of the raters correctly identify the 
intend emotion, which exclude 52 picture from the 
original set leave a final set of 482 pictures. Using inter-rater 
reliability for inclusion provide a way of quantify the 
degree of agreement between two or more coders. We select 
75% a this be the benchmark establish for have good 
agreement among raters when rating among 5-7 categories. 
The Dartmouth Database of Children’s Faces contains 1280 
image of child range in age from 5 to 16 year old with 
a mean age of 9.72 year old. The picture set include 40 girl 
and 40 boy cover 7 emotion (Neutral, Happy, Sad, 
Angry, Fear, Surprise, and Disgust). For evaluation, we 
select the inter-rater reliability cut-off value for inclusion at 
75%, which exclude 370 picture from the original set 
leave a final set of 910 pictures. The Radboud Faces 
Database (RaFD) include 240 image cover 8 emotion 
(Neutral, Angry, Sad, Fear, Disgust, Surprise, Happy, and 
Contempt). There be 4 boy and 6 girl in the dataset. Based 
on a 75% inclusion criteria, we exclude 57 picture from the 
original set leave a final set of 183 pictures. Lastly be the 

Child Emotions Picture Set (CEPS), which contains 273 
image of children, range in age from 6 to 11 year old with 
a mean age of 8.9 year old. The dataset include 9 girl and 8 
boy cover 7 emotion (Happy, Sad, Angry, Disgust, Fear, 
Surprise, and Neutral). Since we do not have access to the 
individual inter-rater reliability value associate with this 
dataset, we use the researcher’s inclusion critera, which 
result in a final set of 225 images. 

We seek to characterize the performance of exist 
machine learn algorithm on case from these distinct data 
set to develop an understand of how well the deep learn 
neural network (i.e. the Microsoft Emotion API) performs on 
recognize children’s emotional states. 

To evaluate the performance of the deep learn 
algorithm, we parse each of the final image set into the 
Emotion API and tabulate the recognition results. Table I 
show the performance for each of the datasets. 

TABLE I. ! DEEP LEARNING RECOGNITION RATES ACROSS THE 
DIFFERENT STIMULI SETS (IN %): (FE)AR, (AN)GRY, (HA)PPY, (SA)D, 
(NE)UTRAL, (SU)RPRISED, (DI)SGUST, (CO)NTEMPT 

Fe An Di Ha Ne Sa Su Co 

NIMH-ChEFS 13 43 100 100 48 

Dartmouth 25 35 55 100 99 64 91 
Radboud 33 54 100 100 100 95 100 50 

CEPS 5 50 10 95 92 52 81 



With respect to the overall recognition rates, which 

incorporate the variable data dimension of the image set 
(Table II), the overall emotion recognition rate be 62% for 
the NIMH-ChEFS dataset, 81% for the Dartmouth dataset, 
83% for the Radboud dataset, and 61% for the CEPS dataset. 
If we compare these result with specialized learn 
algorithm that have be train specifically on children’s 
facial image and a discuss in the related work section, we 
see that the deep learn algorithm, base on training in the 
wild, have result that be comparable to the specialized result 
that use comparatively small training set but with an 
emphasis on a minority class (Table III). Yet, if we look at the 
overall emotion recognition rate for adults, the rate should 
be closer to 88% [30]. Our goal therefore be to capitalize on 
the power of the cloud-based emotion recognition algorithm 
while improve the overall recognition rate. 

TABLE II. ! NUMBER OF IMAGES ASSOCIATED WITH EACH STIMULI 
SET: (FE)AR, (AN)GRY, (HA)PPY, (SA)D, (NE)UTRAL, (SU)RPRISED, 

(DI)SGUST, (CO)NTEMPT 

Fe An Di Ha Ne Sa Su Co 
NIMH-ChEFS 102 94 108 98 80 

Dartmouth 20 83 101 302 145 135 124 
Radboud 21 26 25 30 24 20 29 8 

CEPS 20 30 31 56 24 33 31 





TABLE III. ! DEEP LEARNING ‘IN THE WILD’ APPROACH VERSUS 
SPECIALIZED LEARNING METHODOLOGIES 

Dartmouth Database 
with emotion grouped into positive, 
negative, and neutral affective state 

Albu [13] 85% 
Emotion API 84% 
Dartmouth Database NIMH-ChEFS 
Khan [15] 79% 68% 
Emotion API 81% 62% 

III.! METHODOLOGY AND RESULTS 
When we examine the result from the deep learn 

neural network, we note that Fear have a significantly low 
recognition rate than the other emotion across all of the 
different datasets. If we look at the confusion matrix 
associate with Fear (Table IV), we also note that Fear be most 
often confuse with Surprise. Facial expression of Fear and 
Disgust have repeatedly be found in the emotion 
recognition literature to be less well recognize than those of 
other basic emotion [21]. In fact, it have be show that 
Surprise be the most frequent error make when try to 
recognize expression of Fear in child [22]. If we look at 
the basic facial action unit that make up these two 
expressions, it becomes obvious why this confusion occurs. 
Facial Action Units (AUs) represent the 44 anatomically 
distinct muscular activity that activate when change occur 
in an individual’s facial appearance (Table V). Based on 
comprehensive comparative human studies, Ekman and 
Friesen [23, 24] have label these muscle movement and 
identify those believe to be associate with emotional 
expression (Table VI) [25]. When examine the facial action 
unit associate with Fear and Surprise (Table V), we confirm 
that Surprise be actually a subset of Fear (i.e. Surprise ⊆ Fear); 
and over 60% of the AUs in Fear be also found in the set of 
Surprise AUs. As such, Surprise becomes easy to recognize 
than Fear a a default since there be less distinctive cue to 
identify. 

TABLE IV. ! CONFUSION MATRIX ASSOCIATED WITH DEEP LEARNING 
RESULTS 

Surprise Neutral Happy Sad Fear 
Fear 

NIMH- 
ChEFS 

74 
(83.7%) 

11 
(10.2%) 

0 3 13 
(6.1%) 

Dartmouth 11 (55%) 
0 3 

(15%) 
1 

(5%) 
5 

(25%) 

Radboud 13 (61.9%) 
0 0 1 

(4.8%) 
7 

(33.3%) 

CEPS 9 (45%) 
4 

(20%) 
3 

(15%) 
3 

(15%) 
1 

(5%) 
Surprise 

Dartmouth 113 (91.1%) 
3 

(2.4%) 
8 

(6.5%) 
0 0 

Radboud 29 (100%) 
0 0 0 0 

CEPS 25 (80.6%) 
3 

(9.7%) 
3 

(9.7%) 
0 0 

TABLE V. ! FACIAL ACTION UNITS INVOLVED IN EMOTION STIMULI 

Emotion AUs associate with Emotion 
Angry 4, 5 and/or 7, 22, 23, 24 
Fear 1, 2, 4, 5, 7, 20, 25 or 26 

Surprise 1, 2, 5, 25 or 26 


TABLE VI. ! FACIAL ACTION UNITS AND FACIAL FEATURE IMAGES 
FROM THE CHILDREN FACIAL EXPRESSION DATASETS 

Action 
Unit Description 

Facial Feature 
Image 

1 Inner Brow Raiser 

2 Outer Brow Raiser 


4 Brow Lowerer 


5 Upper Lid Raiser 


7 Lid Tightener 


20 Lip Stretcher 

22 Lip Funneler 


23 Lip Tightener 


24 Lip Pressor 


25 Lips Apart 

26 Jaw Drop 


Thus, a a first step in illustrate a process for improve 
the recognition rate of a generalize machine learn 
algorithm, we focus on improve the overall recognition rate 
by improve recognition of the Fear emotion. Figure 2 
depicts the overall algorithmic flow of the approach. Given a 
facial image, facial landmark be extract and use to 
compute a number of anthropometric features. The 
anthropometric feature be then fed into two Support Vector 
Machines (SVMs) for binary classification, one to distinguish 
between Fear and Surprise with an explicit bias toward Fear 
and one to distinguish between Surprise and Not-Surprise 
with a balance bias. The design of this construct be to increase 
the bias toward the minority class (in the first SVM) while 
ensure that the recognition of the majority class be not 
drastically reduce (in the second SVM). We train the SVMs 
on 50% of the data from three of the datasets of children’s 
face and evaluate the result on all four datasets, include 
the remain untrained dataset. 




Fig. 2.! Feature-based learn approach for emotion recognition of 
children’s facial expression 

A. Extraction of Anthropometric Features 
Most method that focus on develop image-based age 

classification method typically use feature associate with 
face anthropometry [26]. A face anthropometric model be 
base on measurement of size and proportion of the human 
face, i.e. human face ratios. Although age classification be not 
our direct target application, it do provide some measure for 
distinguish between age group (i.e. child versus adults) 
base on facial features. We thus utilize various human face 
ratio a the input into our specialized learn algorithm. In 
[27], it be show that four feature distance be sufficient to 
represent the mathematical relationship among all of the 
various landmark for age classification. Since we be 
interested in emotion classification, we compute all principal 
ratios, namely: Width of Left Eye, Height of Left Eye, Length 
of the Nose Bridge, Distance between the Nose Tip and Chin, 
Width of Mouth, Height of Open Mouth, and Offset Distance 
between the Inner and Outer Corner of the Eyebrow. These 
anthropometric feature be compute base on extract face 
landmarks, a show in Figure 3, and Equations (1)-(7). 

"#$%ℎ'()*(%+,* = |+,*)*(%/00*12 − +,*)*(%45%*12| 
(1) 

6'7*81#$9*)*09%ℎ = |6'7*:''%:#9ℎ%2 − 
6'7*:''%)*(%2| (2) 

6'7*;#<%'=ℎ#0 = |=ℎ#0>'7#%#'0?@ − ?6'7*;#<@| (3) 

A*#9ℎ%'()*(%+,* = |+,*)*(%8'%%'B@ − +,*)*(%;'<@| 
(4) 


1 ! https://www.projectoxford.ai/face 

"#$%ℎ'(C'5%ℎ = |C'5%ℎ:#9ℎ%2 − C'5%ℎ)*(%2| (5) 

A*#9ℎ%'(4<*0C'5%ℎ = |D<<*1)#<8'%%'B@ − 
D0$*1)#<;'<@| (6) 

+,*81'EA*#9ℎ%4((7*% = |+,*F1'E)*(%/00*1@ − 

??+,*F1'E)*(%45%*1@|?? (7) 

Where x and y represent the pixel location in (x,y) screen 
coordinate and ChinPosition be estimate a the pixel 
coordinate associate with the bottom center of the Face 
Rectangle provide by the Face API, which indicates where 
in the image a face be located. Once computed, all ratio be 
normalize base on the calculate width and height of the 
Face Rectangle. 

















Fig. 3.! Face Landmarks extract use the Face API developed by 
Microsoft Oxford Project1 

Once computed, these feature be use to train two SVMs 
for emotion classification. 

B. SVMs for Classification of Fear versus Surprised 
Given that there be a bias for Surprise versus Fear 

associate with children’s facial expressions, our first task 
be to develop a specialized classifier that bias the result 
toward the minority class, in this case, the Fear class. Support 
Vector Machines (SVMs) be supervise learn model that 
can be use for classification of class associate with a set 
of training example [28]. For our application, we want to 
good differentiate between the Fear minority class and the 
Surprise majority class. Thus, any facial expression that be 
classify (correctly or incorrectly) a Surprised by the deep 
learn algorithm, we want to re-evaluate with a specialized 
learner. To enable this process, all emotion label a 
Surprise be fed to the first-level SVM and reclassify into 
one of two classes: Fear or Surprise. We thus design the first- 
level SVM to learn the mapping: 

G ↦ I, where J ∊ :L, , ∊ ±1 , 0 = 7 (8) 

In this case, x represent the vector contain 
anthropometric features, y=1 represent the Surprise class, 
and y=-1 represent the Fear class. 



For our application, we train the first-level SVM on 
50% of the feature vector classify a Fear or Surprise by the 
deep learn algorithm and extract from the Radboud 
Faces Database, the Dartmouth Database of Children’s Faces 
and the NIMH-ChEFS Database. We do not train on the 
Child Emotions Picture Set a we wish to ass the 
capability of the new algorithm when face with unseen 
facial characteristics. We then bias the class decision 
threshold of the first-level SVM by select the minimum 
threshold value that maximize the true positive rate 
associate with Fear. This, by default, increase the false 
positive rate of Fear while potentially reduce the true 
positive rate associate with Surprise. Thus, after parse the 
image through the first-level SVM, the minority class have a 
significantly high recognition rate but the majority class 
recognition rate, on average, be reduce a show in Table VII. 
The high recognition rate for the minority class be valid 
even for the CEPS database which contains data that be not 
in the training set of of the SVM classifiers. All recognition 
rate incorporate the variable data dimension of the image 
set (Table II). 

TABLE VII. ! EMOTION RECOGNITION RATES AFTER TRAINING: ML – 
DEEP LEARNING ALGORITHM, SVM – FIRST-LEVEL SUPPORT VECTOR 

MACHINE 

Fear Surprise Change in 
Overall 

Rec. Rate ML ML+ SVM ML 
ML+ 
SVM 

NIMH-ChEFS 13% 47% 34% 
Dartmouth 25% 91% 91% 79% -1.2% 
Radboud 33% 77% 100% 100% 31.8% 

CEPS 5% 70% 81% 68% 17.6% 


The goal of the second-level SVM be to increase the 
recognition rate of the majority class to pre-bias level while 
still keep the recognition rate associate with the minority 
class high than it original recognition rate. From Table V, 
we note that Angry and Fear have more Action Units in 
common than Angry and Surprise. Thus, to reduce the effect 
of the Action Unit overlap between Surprise and Fear, we 
train the second-level SVM on the recognition of two 
primary class – (Fear ∨ Angry) and Surprise. We then 
associate the derive anthropometric feature vector from 
each image to one of two classes: Surprised and Not- 
Surprised, where Not-Surprised represent the union of Fear 
and Angry. In this case, for the mapping: G ↦ I, where J ∊ 
:L, , ∊ ±1 , 0 = 7, y=1 be associate with the Surprise class, 
and y=-1 be associate with the Fear or Angry class. In 
practice, only those feature vector that be classify a Fear 
by the first-level SVM be process by the second-level 
SVM. This approach result in an average increase in the 
recognition of Fear by 41.5% and an increase in the overall 
recognition rate by 17.3%. As Table VIII shows, the overall 
recognition rate increase after parse through the second- 
level SVM, even though the recognition rate for the minority 
class fall to a low value than in the first-level SVM. Of 
special interest, we note that, although recognition of Fear have 
increase greatly for the Dartmouth database, the recognition 

rate for Surprise be slightly low than the original Surprise 
recognition rate, even after parse through the second-level 
SVM. Of all the datasets, the Dartmouth dataset have a large 
imbalance between Surprise and Fear, in fact this dataset have 
6x more image belonging to Surprise than Fear (Table II). As 
such, this result be not surprising a the balance that we be 
try to achieve in our approach be to ensure that the minority 
class have an increase in benefit with respect to recognition 
rates, while ensure that the reduction in benefit to the 
majority class be not major. If the motivation is, instead, to 
ensure that the recognition rate for the minority class be 
maximized, the only requirement be to ignore the second-level 
SVM and utilize only the output result from parse 
through the first-level SVM. 

In the next section, we make some interest observation 
about these result and provide discussion on way to 
generalize this approach to the broad class of generalize 
learn algorithms. 

TABLE VIII. ! EMOTION RECOGNITION RATES AFTER TRAINING: ML – 
DEEP LEARNING ALGORITHM, SVM – SECOND-LEVEL SUPPORT VECTOR 

MACHINE 

Fear Surprise Change in 
Overall 

Rec. Rate ML ML+ SVM ML 
ML+ 
SVM 

NIMH-ChEFS 13% 47% 34% 
Dartmouth 25% 70% 91% 83% -0.6% 
Radboud 33% 71% 100% 100% 32.8% 

CEPS 5% 55% 81% 81% 19.6% 


IV.! DISCUSSION 
We conclude this paper with a discussion on the present 

result and highlight some area for future effort that could 
address the limitation associate with building classifier 
when there be imbalanced representation in their training sets. 

Recently, there have be an upsurge of attention give to 
generalize machine learn algorithm and the practice of 
inequality and discrimination that be potentially be built 
into them [29]. We know that imbalance exist and thus, our 
goal in this paper be to present an approach that enables u to 
capitalize on the power of generalize learn algorithms, 
while incorporate a process that allows u to tune those 
result for different target demographics. Bias in machine 
learn algorithm will occur anytime there be a large 
majority class couple with other minority class have 
low incidence rates, such a those associate with a young 
or old age demographic, or an ethnic minority. The 
challenge be to develop a process for ensure the overall 
positive result of the generalize learn approach be 
maintained, while also increase the outcome associate 
with any minority classes. In this paper, we address this issue 
by develop a hierarchical approach that couple the result 
from the generalize learn algorithm with result from a 
specialized learner. Although we focus on the issue of emotion 
recognition for intelligent systems, and address emotion 



recognition associate with children’s facial expressions, this 
concept can be apply to similar classification applications. 
The step involve be (1) identify the set(s) of minority 
classes, (2) develop specialized learner that address the 
minority class via special focus on the class, and (3) 
develop a specialized learner that combine signal from 
both the minority and majority class models. 

As show in the results, if at any point, we determine that 
it be more important to have a maximum outcome rate 
associate with the minority class, regardless of the outcome 
rate associate with the majority class, only step (1) and (2) 
be necessary. That question on the inclusiveness of a 
classifier touch on ethic of equity in the performance of 
algorithms. 

Although the present approach show validity in 
address the issue of bias, there be still a number of thread 
that need to be investigated. Future work in this domain 
include validate the approach with a focus on a different 
minority class, validate the approach with a focus on a 
different classification problem, and validate the approach 
with different generalize machine learn algorithms. We 
will also target improve the classification rate of both Fear 
and Disgust, since both of these expression be hard to detect, 
and would provide further evidence of the impact of this 
methodology. We hope that this work will contribute to 
raise the sensitivity to the potential challenge in the 
performance and bias of classifier when make inference 
about people of different age and skin colors. There be 
opportunity for additional research to identify and address 
these challenges. 

V.! REFERENCES 
[1]! Kotsiantis, S., Kanellopoulos, D. and Pintelas, P. “Handling 

imbalanced datasets: A review,” GESTS International Transactions on 
Computer Science and Engineering, Vol. 30, pp. 25-36, 2006. 

[2]! Chawla, N.V., Hall, L. O., Bowyer, K. W. and Kegelmeyer, W. P., 
“SMOTE: Synthetic Minority Oversampling Technique,” Journal of 
Artificial Intelligence Research, Vol. 16, pp. 321–357, 2002. 

[3]! Joshi, M. V., Kumar, V. and Agarwal, R.C. “Evaluating boost 
algorithm to classify rare cases: comparison and improvements,” In 
First IEEE International Conference on Data Mining, pp. 257-264, 
2001. 

[4]! Provost, F. and Fawcett, T. “Robust classification for imprecise 
environments,” Machine Learning, Vol. 42, pp. 203-231, 2001. 

[5]! Doerrfeld, B. “20+ Emotion Recognition APIs That Will Leave You 
Impressed, and Concerned,” http://nordicapis.com/20-emotion- 
recognition-apis-that-will-leave-you-impressed-and-concerned, 2015. 

[6]! Dhall, A., Ramana Murthy O.V., Goecke, R., Joshi J. and Gedeon, T. 
“Video and Image base Emotion Recognition Challenges in the Wild: 
EmotiW 2015,” ACM International Conference on Multimodal 
Interaction (ICMI), 2015. 

[7]! Brown, L. and Howard A. “Gestural Behavioral Implementation on a 
Humanoid Robotic Platform for Effective Social Interaction,” IEEE 
Int. Symp. on Robot and Human Interactive Communication (RO- 
MAN), pp. 471 – 476, 2014. 

[8]! Metta, G., Sandini, G., Vernon, D. Natale, L. and Nori, F. “The iCub 
humanoid robot: an open platform for research in embody cognition,” 
8th workshop on performance metric for intelligent systems, pp. 50- 
56, 2008. 

[9]! Cloutier, P., Park, H.W., MacCalla, J. and Howard, A. “It’s All in the 
Eyes: Designing Facial Expressions for an Interactive Robot Therapy 
Coach for Children,” 8th Cambridge Workshop on Universal Access 
and Assistive Technology, Cambridge, UK, 2016. 

[10]! Simmons R., et al. “GRACE: An Autonomous Robot for the AAAI 
Robot Challenge,” AI Magazine, Vol. 24(2), pp. 51-72, 2003. 

[11]! Sankur, B., Ulukaya, S. and Çeliktutan, O. “A Comparative Study of 
Face Landmarking Techniques,” EURASIP J. Image and Video 
Processing, Vol. 13, 2013. 

[12]! Littleworth, G., Bartlett, M.S., Salamanca, L.P. and Reilly, J. 
“Automated Measurement of Children’s Facial Expressions during 
Problem Solving Tasks,” IEEE Int. Conference on Automatic Face and 
Gesture Recognition, pp. 30–35, 2011. 

[13]! Albu, F., Hagiescu, D., Vladutu, L. and Puica, M. “Neural network 
approach for children's emotion recognition in intelligent learn 
applications,” in Proc. of EDULEARN 2015, Barcelona, Spain, pp. 
3229-3239, 2015. 

[14]! Dalrymple, KA, Gomez, J, and Duchaine, B. “The Dartmouth Database 
of Children’s Faces: Acquisition and Validation of a New Face 
Stimulus Set,” Urgesi C, ed.PLoS ONE. 2013. Vol. 8(11), 2013. 

[15]! Khan, R.A., Meyer, A. and Bouakaz, S. “Automatic Affect Analysis: 
From Children to Adults,” International Symposium on Visual 
Computing, ISVC 2015, pp. 304-313, 2015. 

[16]! Egger, H.L., Pine, D.S., Nelson, E., et al. “The NIMH Child Emotional 
Faces Picture Set (NIMH-ChEFS): A new set of children’s facial 
emotion stimuli,” International Journal of Methods in Psychiatric 
Research, Vol. 20(3), pp. 145-156, 2011. 

[17]! Schmidt, A. “Cloud-Based AI for Pervasive Applications,” IEEE 
Pervasive Computing, Vol. 15(1), pp. 14-18, 2016. 

[18]! Barsoum, E., Zhang, C., Canton Ferrer, C. and Zhang, Z. “Training 
Deep Networks for Facial Expression Recognition with Crowd- 
Sourced Label Distribution,” ACM International Conference on 
Multimodal Interaction (ICMI), Tokyo, Japan, 2016. 

[19]! Langner, O., Dotsch, R., Bijlstra, G., Wigboldus, D.H.J., et. al. 
“Presentation and validation of the Radboud Faces 
Database,” Cognition & Emotion, Vol. 4(8), pp. 1377—1388, 2010. 

[20]! Romani-Sponchiado, A., Sanvicente-Vieira, B., Mottin, C., Hertzog- 
Fonini, D., Arteche, A. “Child Emotions Picture Set (CEPS): 
Development of a database of children’s emotional expressions,” 
Psychology & Neuroscience, Vol. 8(4), pp. 467-478, 2015. 

[21]! Gagnon, M., Gosselin P., Hudon-ven der Buhs, I., Larocque, K., 
Milliard, K. “Children’s recognition and discrimination of Fear and 
Disgust facial expressions,” Journal of Nonverbal Behavior, Vol. 
34(1), pp. 27–42, 2010. 

[22]! Gosselin, P., Roberge, P. and Lavalle´e, M. C. “The development of 
the recognition of facial emotional expression comprise in the human 
repertoire,” Enfance, Vol. 4, pp. 379–396, 1995. 

[23]! Ekman, P. and Friesen, W. Facial action cod system: A technique 
for the measurement of facial movement. Palo Alto, Ca.: Consulting 
Psychologists Press, 1978. 

[24]! Friesen, W. and Ekman, P. EMFACS-7: Emotional Facial Action 
Coding System. Unpublished manual, University of California, 
California, 1983. 

[25]! Matsumoto, D. and Ekman, P. “Facial expression analysis,” 
Scholarpedia, Vol. 3(5), pp. 4237, 2008. 

[26]! Grd, P. “Two-dimensional face image classification for distinguish 
child from adult base on anthropometry,” Thesis submit to 
University of Zagreb, 2015. 

[27]! Alom M.Z., Piao, M-L., Islam M.S., Kim, N. and Park, J-H. 
“Optimized Facial Features-based Age Classification,” World 
Academy of Science. Engineering and Technology Conference, Vol. 
6, pp. 319–324, 2012. 

[28]! Joachims, T. “Making Large-Scale SVM Learning Practical. Advances 
in Kernel Methods - Support Vector Learning,” B. Schölkopf and C. 
Burges and A. Smola (ed.), MIT-Press, 1999. 

[29]! Crawford, K. “Artificial Intelligence’s White Guy Problem,” New 
York Times – Opinion, http://www.nytimes.com/2016/06/26/ 
opinion/sunday/artificial-intelligences-white-guy-problem.html, 2016. 

[30]! Brodny, G., Kołakowska, A., Landowska, A., Szwoch, M., Szwoch, 
W. and Wróbel, M. “Comparison of select off-the-shelf solution for 
emotion recognition base on facial expressions,” 9th Int. Conf. on 
Human System Interactions (HSI), pp. 397-404, 2016. 


