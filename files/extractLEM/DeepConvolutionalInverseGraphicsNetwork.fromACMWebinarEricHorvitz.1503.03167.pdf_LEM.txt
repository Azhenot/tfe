


















































Deep Convolutional Inverse Graphics Network 

Tejas D. Kulkarni*1, Will Whitney*2, Pushmeet Kohli3, Joshua B. Tenenbaum4 
1,2,4Computer Science and Artificial Intelligence Laboratory, MIT 

1,4Brain and Cognitive Sciences, MIT 
3Microsoft Research Cambridge, UK 

1tejask@mit.edu 2wwhitney@mit.edu 3pkohli@microsoft.com 4jbt@mit.edu 
* First two author contribute equally to this work. 

Abstract 

This paper present the Deep Convolution Inverse Graphics Network (DC-IGN), 
a model that learns an interpretable representation of images. This representation 
be disentangle with respect to transformation such a out-of-plane rotation and 
light variations. The DC-IGN model be compose of multiple layer of convo- 
lution and de-convolution operator and be train use the Stochastic Gradient 
Variational Bayes (SGVB) algorithm [11]. We propose a training procedure to 
encourage neuron in the graphic code layer to represent a specific transforma- 
tion (e.g. pose or light). Given a single input image, our model can generate new 
image of the same object with variation in pose and lighting. We present qual- 
itative and quantitative result of the model’s efficacy at learn a 3D render 
engine. 

1 Introduction 

Deep learn have lead to remarkable breakthrough in automatically learn hierarchical repre- 
sentations from images. Models such a Convolutional Neural Networks (CNNs) [14], Restricted 
Boltzmann Machine-based generative model [8, 22], and Auto-encoders [2, 26] have be success- 
fully apply to produce multiple layer of increasingly abstract visual representations. However, 
there be relatively little work on characterize the optimal representation of the data. While Cohen 
et al. [4] have consider this problem by propose a theoretical framework to learn irreducible 
representation have both invariance and equivariances, come up with the best representation 
for any give task be an open question. 

Various work [3, 4, 7] have be do on the theory and practice of representation learning, and from 
this work a consistent set of desideratum for representation have emerged: invariance, meaningfulness 
of representations, abstraction, and disentanglement. In particular, Bengio et al. [3] propose that a 
disentangle representation be one for which change in the encode data be sparse over real-world 
transformations; that is, change in only a few latents at a time should be able to represent sequence 
which be likely to happen in the real world. 

The “vision a inverse graphics” model suggests a representation for image which provide these 
features. Computer graphic consists of a function to go from compact description of scene (the 
graphic code) to images, and this graphic code be typically disentangle to allow for render 
scene with fine-grained control over transformation such a object location, pose, lighting, texture, 
and shape. This encode be design to easily and interpretably represent sequence of real data 
so that common transformation may be compactly represent in software code; this criterion be 
almost identical to that of Bengio et al., and graphic code conveniently align with the property 
of an ideal representation. 

1 

ar 
X 

iv 
:1 

50 
3. 

03 
16 

7v 
4 

[ 
c 

.C 
V 

] 
2 

2 
Ju 

n 
20 

15 



observe 
image 

Filters = 96 
kernel size (KS) = 5 

150x150 

Convolution + Pooling 

graphic code 

x 

Q(zi|x) 

Filters = 64 
KS = 5 

Filters = 32 
KS = 5 

7200pose 
light 

shape 

.... 
Filters = 32 

KS = 7 
Filters = 64 

KS = 7 
Filters = 96 

KS = 7 

P (x|z) 

Encoder 
(De-rendering) 

Decoder 
(Renderer) 

Unpooling (Nearest Neighbor) + 
Convolution 

{µ200,⌃200} 

Figure 1: Model Architecture: Deep Convolutional Inverse Graphics Network (DC-IGN) have an 
encoder and a decoder. We follow the variational autoencoder [11] architecture with variations. The 
encoder consists of several layer of convolution follow by max-pooling and the decoder have 
several layer of unpooling (upsampling use near neighbors) follow by convolution. (a) Dur- 
ing training, data x be pass through the encoder to produce the posterior approximation Q(zi|x), 
where zi consists of scene latent variable such a pose, light, texture or shape. In order to learn 
parameter in DC-IGN, gradient be back-propagated use stochastic gradient descent use the 
follow variational object function: −log(P (x|zi)) + KL(Q(zi|x)||P (zi)) for every zi. We can 
force DC-IGN to learn a disentangle representation by show mini-batches with a set of inactive 
and active transformation (e.g. face rotating, light sweep in some direction etc). (b) During test, 
data x can be pass through the encoder to get latents zi. Images can be re-rendered to different 
viewpoints, light conditions, shape variation etc by just manipulate the appropriate graphic 
code group (zi), which be how one would manipulate an off-the-shelf 3D graphic engine. 

Recent work in inverse graphic [10, 17, 16, 12] follow a general strategy of define a probabilis- 
tic or deterministic model with latent parameters, then use an inference or optimization algorithm 
to find the most appropriate set of latent parameter give the observations. Recently, Tieleman et 
al. [24] move beyond this two-stage pipeline by use a generic encoder network and a domain- 
specific decoder network to approximate a 2D render function. However, none of these ap- 
proaches have be show to automatically produce a semantically-interpretable graphic code and 
to learn a 3D render engine to reproduce images. 

In this paper, we present an approach for learn interpretable graphic code for complex trans- 
formation such a out-of-plane rotation and light variations. Given a set of images, we use 
a hybrid encoder-decoder model to learn a representation that be disentangle with respect to var- 
ious transformation such a object out-of-plane rotation and light variations. To achieve this, 
we employ a deep direct graphical model with many layer of convolution and de-convolution 
operator that be train use the Stochastic Gradient Variational Bayes (SGVB) algorithm [11]. 

We propose a training procedure to encourage each group of neuron in the graphic code layer to 
distinctly represent a specific transformation. To learn a disentangle representation, we train use 
data where each mini-batch have a set of active and inactive transformations, but we do not provide 
target value a in supervise learning; the objective function remains reconstruction quality. For 
example, a nod face would have the 3D elevation transformation active but it shape, texture 
and other affine transformation would be inactive. We exploit this type of training data to force 
chosen neuron in the graphic code layer to specifically represent active transformations, thereby 
automatically create a disentangle representation. Given a single face image, our model can re- 
generate the input image with a different pose and lighting. We present qualitative and quantitative 
result of the model’s efficacy at learn a 3D render engine. 

2 



2 Related Work 

As mention before, a number of generative model have be propose in the literature to obtain 
abstract visual representations. Unlike most RBM-based model [8, 22, 15], our approach be train 
use back-propagation with objective function consist of data reconstruction and the variational 
bound. 

Relatively recently, Kingma et al. [11] propose the SGVB algorithm to learn generative model 
with continuous latent variables. In this work, a feed-forward neural network (encoder) be use to 
approximate the posterior distribution and a decoder network serf to enable stochastic reconstruc- 
tion of observations. In order to handle fine-grained geometry of faces, we work with relatively 
large scale image (150 × 150 pixels). Our approach extends and applies the SGVB algorithm to 
jointly train and utilize many layer of convolution and de-convolution operator for the encoder and 
decoder network respectively. The decoder network be a function that transform a compact graphic 
code ( 200 dimensions) to a 150× 150 image. We propose use unpooling (nearest neighbor sam- 
pling) follow by convolution to handle the massive increase in dimensionality with a manageable 
number of parameters. 

Recently, [6] propose use CNNs to generate image give object-specific parameter in a super- 
vised setting. As their approach require ground-truth label for the graphic code layer, it cannot be 
directly apply to image interpretation tasks. Our work be similar to Ranzato et al. [21], whose work 
be amongst the first to use a generic encoder-decoder architecture for feature learning. However, 
in comparison to our proposal their model be train layer-wise, the intermediate representation 
be not disentangle like a graphic code, and their approach do not use the variational auto- 
encoder loss to approximate the posterior distribution. Our work be also similar in spirit to [23], but 
in comparison our model do not assume a Lambertian reflectance model and implicitly construct 
the 3D representations. Another piece of related work be Desjardins et al. [5], who use a spike and 
slab prior to factorize representation in a generative deep network. 

In comparison to exist approaches, it be important to note that our encoder network produce the 
interpretable and disentangle representation necessary to learn a meaningful 3D graphic engine. 
A number of inverse-graphics inspire method have recently be propose in the literature [10, 
17, 16]. However, most such method rely on hand-crafted render engines. The exception to 
this be work by Hinton et al. [9] and Tieleman [24] on transform autoencoders which use a 
domain-specific decoder to reconstruct input images. Our work be similar in spirit to these work but 
have some key differences: (a) It us a very generic convolutional architecture in the encoder and 
decoder network to enable efficient learn on large datasets and image sizes; (b) it can handle 
single static frame a oppose to pair of image require in [9]; and (c) it be generative. 

3 Model 

As show in Figure 1, the basic structure of the Deep Convolutional Inverse Graphics Network 
(DC-IGN) consists of two parts: an encoder network which capture distribution over graphic 
code Z give data x and a decoder network which learns a conditional distribution to produce 
an approximation x̂ give Z. Z can be a disentangle representation contain a factor set of 
latent variable zi ∈ Z such a pose, light and shape. This be important in learn a meaningful 
approximation of a 3D graphic engine and help tease apart the generalization capability of the 
model with respect to different type of transformations. 

Let u denote the encoder output of DC-IGN to be ye = encoder(x). The encoder output be use to 
parametrize the variational approximation Q(zi|ye), where Q be chosen to be a multivariate normal 
distribution. There be two reason for use this parametrization: (1) Gradients of sample with 
respect to parameter θ of Q can be easily obtain use the reparametrization trick propose in 
[11], and (2) Various statistical shape model train on 3D scanner data such a face have the same 
multivariate normal latent distribution [20]. Given that model parameter We connect ye and zi, the 
distribution parameter θ = (µzi ,Σzi) and latents Z can then be express as: 

µzi = We ∗ ye (1) 
Σzi = diag(exp(We ∗ ye)) (2) 
∀i, zi ∼ N (µzi ,Σzi) (3) 

3 



𝜙1 
𝛼1 

𝛼 

𝜙L 
1 

𝜙L 

z[4,n]z = z3z2z1 

𝜙corresponds to 

Output 

first sample in batch x1 

from encoder to encoder 

intrinsic property (shape, texture, etc) 

same a output for x1 

z[4,n]z3z2z1 

late sample in batch xi z[4,n]z3z2z1 

unique for each 
xi in batch 

zero error signal 
for clamped output 

zero error signal 
for clamped output 

error signal 
from decoder 

∇zki = z 
k 
i - mean z 

k 

Backpropagation 

z[4,n]z3z2z1 

z[4,n]z3z2z1 

Backpropagation with 
invariance target 

z[4,n]z3z2z1 

k ∈ batch 

k ∈ batch 

Caption: Training on a minibatch in which only 𝜙, the azimuth angle of the face, 
changes. 
During the forward step, the output from each component z_k != z_1 of the 
encoder be force to be the same for each sample in the batch. This reflect the fact 
that the generate variable of the image which correspond to the desire value of 
these latents be unchanged throughout the batch. By hold these output 
constant throughout the batch, z_1 be force to explain all the variance within the 
batch, i.e. the full range of change to the image cause by change 𝜙. 

During the backward step, backpropagation of gradient happens only through the latent 
z_1, with gradient for z_k != z_1 set to zero. This corresponds with the clamped 
output from those latents throughout the batch. 

Caption: In order to directly enforce invariance of the latents correspond to 
property of the image which do not change within a give batch, we calculate 
gradient for the z_k != z_1 which move them towards the mean of each 
invariant latent over the batch. This be equivalent to regularize the 
latents z_{[2,n]} by the L2 norm of (zk - mean zk). 

Figure 2: Structure of the representation vector. φ be the azimuth of the face, α be the elevation of 
the face with respect to the camera, and φL be the azimuth of the light source. 

We present a novel training procedure which allows network to be train to have disentangle and 
interpretable representations. 

3.1 Training with Specific Transformations 

The main goal of this work be to learn a representation of the data which consists of disentangle and 
semantically interpretable latent variables. We would like only a small subset of the latent variable 
to change for sequence of input correspond to real-world events. 

One natural choice of target representation for information about scene be that already design for 
use in graphic engines. If we can deconstruct a face image by splitting it into variable for pose, 
light, and shape, we can trivially represent the same transformation that these variable be use for 
in graphic applications. Figure 2 depicts the representation which we will attempt to learn. 

To achieve this goal, we perform a training procedure which directly target this definition of disen- 
tanglement. We organize our data into mini-batches correspond to change in only a single scene 
variable (azimuth angle, elevation angle, azimuth angle of the light source); these be transforma- 
tions which might occur in the real world. We will term these the extrinsic variables, and they be 
represent by the component z1,2,3 of the encoding. 

We also generate mini-batches in which the three extrinsic scene variable be held fix but all 
other property of the face change. That is, these batch consist of many different face under the 
same view condition and pose. These intrinsic property of the model, which describe identity, 
shape, expression, etc., be represent by the remainder of the latent variable z[4,200]. These mini- 
batch vary intrinsic property be intersperse stochastically with those vary the extrinsic 
properties. 

We train this representation use SGVB, but we make some key adjustment to the output of the 
encoder and the gradient which train it. The procedure (Figure 3) be a follows. 

1. Select at random a latent variable ztrain which we wish to correspond to one of {azimuth 
angle, elevation angle, azimuth of light source, intrinsic properties}. 

2. Select at random a mini-batch in which that only that variable changes. 

3. Show the network each example in the minibatch and capture it latent representation for 
that example zk. 

4. Calculate the average of those representation vector over the entire batch. 

5. Before put the encoder’s output into the decoder, replace the value zi 6= ztrain with 
their average over the entire batch. These output be “clamped”. 

6. Calculate reconstruction error and backpropagate a per SGVB in the decoder. 

7. Replace the gradient for the latents zi 6= ztrain (the clamped neurons) with their difference 
from the mean (see Section 3.2). The gradient at ztrain be pass through unchanged. 

8. Continue backpropagation through the encoder use the modify gradient. 

Since the intrinsic representation be much higher-dimensional than the extrinsic ones, it require 
more training. Accordingly we select the type of batch to use in a ratio of about 1:1:1:10, az- 
imuth:elevation:lighting:intrinsic; we arrive at this ratio after extensive testing, and it work well 
for both of our datasets. 

4 



Forward Backward 

Encoder 

Decoder 

out = mean zk 
k ∈ batchi i 

grad = zk mean zk 
k ∈ batchi i iz[4,n]z3z2z1 

out1 = z1 

grad1 = ∇z1 

∇out1 

Encoder 

Decoder 

clamped 

unclamped 

Figure 3: Training on a minibatch in which only φ, the azimuth angle of the face, changes. 
During the forward step, the output from each component zi 6= z1 of the encoder be alter to be the 
same for each sample in the batch. This reflect the fact that the generate variable of the image 
(e.g. the identity of the face) which correspond to the desire value of these latents be unchanged 
throughout the batch. By hold these output constant throughout the batch, the single neuron z1 be 
force to explain all the variance within the batch, i.e. the full range of change to the image cause 
by change φ. During the backward step z1 be the only neuron which receives a gradient signal from 
the attempt reconstruction, and all zi 6= z1 receive a signal which nudge them to be closer to their 
respective average over the batch. During the complete training process, after this batch, another 
batch be select at random; it likewise contains variation of only one of φ, α, φL, intrinsic; all 
neuron which do not correspond to the select latent be clamped; and the training proceeds. 

(a) 

Original Reconstuction Pose (Elevation) varied 

(b) 

Original Reconstuction Pose (Azimuth) varied 

Figure 4: Manipulating pose variables: Qualitative result show the generalization capability 
of the learn DC-IGN decoder to rerender a single input image with different pose directions. (a) 
We change the latent zelevation smoothly from -15 to 15, leave all 199 other latents unchanged. 
(b) We change the latent zazimuth smoothly from -15 to 15, leave all 199 other latents unchanged. 

This training procedure work to train both the encoder and decoder to represent certain property 
of the data in a specific neuron. By clamp the output of all but one of the neurons, we force the 
decoder to recreate all the variation in that batch use only the change in that one neuron’s value. 
By clamp the gradients, we train the encoder to put all the information about the variation in the 
batch into one output neuron. 

This training method lead to network whose latent variable have a strong equivariance with the 
correspond generate parameters, a show in Figure 6. This allows the value of the true gener- 
ating parameter (e.g. the true angle of the face) to be trivially extract from the encoder. 

5 



(a) 

Original Reconstuction Light direction varied 

(b) 

Figure 5: (a) Manipulating light variables: Qualitative result show the generalization capabil- 
ity of the learnt DC-IGN decoder to render original static image with different light directions. The 
latent neuron zlight be change to random value but all other latents be clamped. (b) Entangled 
versus disentangle representations. Top: Original reconstruction (left) and transform (right) 
use a normally-trained network. Bottom: The same transformation use the DC-IGN. 

3.2 Invariance Targeting 

By training with only one transformation at a time, we be encourage certain neuron to contain 
specific information; this be equivariance. But we also wish to explicitly discourage them from 
have other information; that is, we want them to be invariant to other transformations. Since our 
mini-batches of training data consist of only one transformation per batch, then this goal corresponds 
to have all but one of the output neuron of the encoder give the same output for every image in 
the batch. 

To encourage this property of the DC-IGN, we train all the neuron which correspond to the inactive 
transformation with an error gradient equal to their difference from the mean. It be simplest to think 
about this gradient a act on the set of subvectors zinactive from the encoder for each input in 
the batch. Each of these zinactive’s will be point to a close-together but not identical point in a 
high-dimensional space; the invariance training signal will push them all closer together. We don’t 
care where they are; the network can represent the face show in this batch however it likes. We 
only care that the network always represent it a still be the same face, no matter which way it’s 
facing. This regularize force need to be scale to be much small than the true training signal, 
otherwise it can overwhelm the reconstruction goal. Empirically, a factor of 1/100 work well. 

4 Experiments 

We train our model on about 12,000 batch of face generate from a 3D face model obtain 
from Paysan et al. [20], where each batch consists of 20 face with random variation on face 
identity variable (shape/texture), pose, or lighting. We use the rmsprop [25] learn algorithm 
during training and set the meta learn rate to be equal to 0.0005, the momentum decay to be 0.1 
and weight decay to be 0.01. 

To ensure that these technique work on other type of data, we also train network to perform 
reconstruction on image of widely varied 3D chair from many perspective derive from the Pascal 
Visual Object Classes dataset a extract by Aubry et al. [18, 1]. This task test the ability of the 
DC-IGN to learn a render function for a dataset with high variation between the element of the 
set; the chair vary from office chair to wicker to modern designs, and viewpoint span 360 degree 
and two elevations. These network be train with the same method and parameter a the one 
above. 

6 



(a) 
2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 

Ground Truth 
30 

20 

10 

0 

10 

20 

30 

In 
fe 

rr 
ed 

Pose (Azimuth) 

(b) 
0.4 0.2 0.0 0.2 0.4 

Ground Truth 
20 

15 

10 

5 

0 

5 

10 

15 

20 

In 
fe 

rr 
ed 

Pose (Elevation) 

(c) 
100 50 0 50 100 

Ground Truth 
15 

10 

5 

0 

5 

10 

15 

In 
fe 

rr 
ed 

Light 

Figure 6: Generalization of decoder to render image in novel viewpoint and light condi- 
tions: We generate several datasets by vary light, azimuth and elevation, and test the invari- 
ance property of DC-IGN’s representation Z. We show quantitative performance on three network 
configuration a described in section 4.1. (a,b,c) All DC-IGN encoder network reasonably predicts 
transformation from static test images. Interestingly, a see in (a), the encoder network seem to 
have learnt a switch node to separately process azimuth on left and right profile side of the face. 

4.1 3D Face Dataset 

The decoder network learns an approximate render engine a show in Figures (4,5). Given a 
static test image, the encoder network produce the latents Z depict scene variable such a light, 
pose, shape etc. Similar to an off-the-shelf render engine, we can independently control these to 
generate new image with the decoder. For example, a show in Figure 5, give the original test 
image, we can vary the light of an image by keep all the other latents constant and vary 
zlight. It be perhaps surprising that the fully-trained decoder network be able to function a a 3D 
render engine. 

We also quantitatively illustrate the network’s ability to represent pose and light on a smooth linear 
manifold a show in Figure 6, which directly demonstrates our training algorithm’s ability to disen- 
tangle complex transformations. In these plots, the infer and ground-truth transformation value 
be plot for a random subset of the test set. Interestingly, a show in Figure 6(a), the encoder 
network’s representation of azimuth have a discontinuity at 0◦ (facing straight forward). 

4.1.1 Comparison with Entangled Representations 

To explore how much of a difference the DC-IGN training procedure makes, we compare the novel- 
view reconstruction performance of network with entangle representation (baseline) versus disen- 
tangle representation (DC-IGN). The baseline network with entangle representation be identical 
in every way to the DC-IGN, but be train with SGVB without use the training procedure we 
propose in this paper. As in Figure 4, we feed each network a single input image, then attempt to use 
the decoder to re-render this image at different azimuth angles. To do this, we first must figure out 
which latent of the entangle representation most closely corresponds to the azimuth. This we do 
rather simply. First, we encode all image in an azimuth-varied batch use the baseline’s encoder. 
Then we calculate the variance of each of the latents over this batch. The latent with the large 
variance be then the one most closely associate with the azimuth of the face, and we will call it 
zazimuth. Once that be found, the latent zazimuth be varied for both the model to render a novel 
view of the face give a single image of that face. Figure 5 show that explicit disentanglement be 
critical for novel-view reconstruction. 

4.2 Chair Dataset 

We perform a similar set of experiment on the 3D chair dataset described above. This dataset 
contains still image render from 3D CAD model of 1357 different chairs, each model skin 
with the photographic texture of the real chair. Each of these model be render in 60 different 
poses; at each of two elevations, there be 30 image take from 360 degree around the model. We 
use approximately 1200 of these chair in the training set and the remain 150 in the test set; a 
such, the network have never see the chair in the test set from any angle, so the test explore the 
network ability to generalize to arbitrary chairs. We resize the image to 150 × 150 pixel and 
make them grayscale to match our face dataset. 

7 



(a) 

(b) 

Figure 7: Manipulating rotation: Each row be generate by encode the input image (leftmost) 
with the encoder, then change the value of a single latent and put this modify encode 
through the decoder. The network have never see these chair before at any orientation. (a) Some 
positive examples. Note that the DC-IGN be make a conjecture about any component of the chair 
it cannot see; in particular, it guess that the chair in the top row have arms, because it can’t see that 
it doesn’t. (b) Examples in which the network extrapolates to new viewpoint less accurately. 

We train these network with the azimuth (flat rotation) of the chair a a disentangle variable 
represent by a single node z1; all other variation between image be undifferentiated and repre- 
sented by z[2,200]. The DC-IGN network succeed in achieve a mean-squared error (MSE) of 
reconstruction of 2.7722× 10−4 on the test set. Each image have grayscale value in the range [0, 1] 
and be 150× 150 pixels. 
In Figure 7 we have include example of the network ability to re-render previously-unseen chair 
at different angle give a single image. For some chair it be able to render fairly smooth transitions, 
show the chair at many intermediate poses, while for others it seem to only capture a sort of 
keyframes representation, only have distinct output for a few angles. Interestingly, the task of 
rotate a chair see only from one angle require speculation about unseen components; the chair 
might have arms, or not; a curve seat or a flat one; etc. 

5 Discussion 

We have show that it be possible to train a deep convolutional inverse graphic network with in- 
terpretable graphic code layer representation from static images. By utilize a deep convolution 
and de-convolution architecture within a variational autoencoder formulation, our model can be 
train end-to-end use back-propagation on the stochastic variational objective function [11]. We 
propose a training procedure to force the network to learn disentangle and interpretable repre- 
sentations. Using 3D face analysis a a work example, we have demonstrate the invariant and 
equivariant characteristic of the learn representations. 

To scale our approach to handle more complex scenes, it will likely be important to experiment with 
deeper architecture in order to handle large number of object category within a single network 
architecture. It be also very appeal to design a spatio-temporal base convolutional architecture 
to utilize motion in order to handle complicate object transformations. Furthermore, the current 

8 



formulation of SGVB be restrict to continuous latent variables. However, real-world visual scene 
contain unknown number of object that move in and out of frame. Therefore, it might be necessary 
to extend this formulation to handle discrete distribution [13] or extend the model to a recurrent 
setting. The decoder network in our model can also be replace by a domain-specific decoder [19] 
for fine-grained model-based inference. We hope that our work motivates further research into 
automatically learn interpretable representation use variant of our model. 

Acknowledgements 

We thank Thomas Vetter for give u access to the Basel face model. T. Kulkarni be graciously 
support by the Leventhal Fellowship. We would like to thank Ilker Yildrim, Max Kleiman-Weiner, 
Karthik Rajagopal and Geoffrey Hinton for helpful feedback and discussions. 

References 
[1] M. Aubry, D. Maturana, A. Efros, B. Russell, and J. Sivic. Seeing 3d chairs: exemplar part-based 2d-3d 

alignment use a large dataset of cad models. In CVPR, 2014. 

[2] Y. Bengio. Learning deep architecture for ai. Foundations and trend R© in Machine Learning, 2(1):1– 
127, 2009. 

[3] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern 
Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798–1828, 2013. 

[4] T. Cohen and M. Welling. Learning the irreducible representation of commutative lie groups. arXiv 
preprint arXiv:1402.4437, 2014. 

[5] G. Desjardins, A. Courville, and Y. Bengio. Disentangling factor of variation via generative entangling. 
arXiv preprint arXiv:1210.5474, 2012. 

[6] A. Dosovitskiy, J. Springenberg, and T. Brox. Learning to generate chair with convolutional neural 
networks. arXiv:1411.5928, 2015. 

[7] I. Goodfellow, H. Lee, Q. V. Le, A. Saxe, and A. Y. Ng. Measuring invariance in deep networks. In 
Advances in neural information processing systems, page 646–654, 2009. 

[8] G. Hinton, S. Osindero, and Y.-W. Teh. A fast learn algorithm for deep belief nets. Neural computation, 
18(7):1527–1554, 2006. 

[9] G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In Artificial Neural Networks 
and Machine Learning–ICANN 2011, page 44–51. Springer, 2011. 

[10] V. Jampani, S. Nowozin, M. Loper, and P. V. Gehler. The inform sampler: A discriminative approach 
to bayesian inference in generative computer vision models. arXiv preprint arXiv:1402.0859, 2014. 

[11] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 

[12] T. D. Kulkarni, V. K. Mansinghka, P. Kohli, and J. B. Tenenbaum. Inverse graphic with probabilistic cad 
models. arXiv preprint arXiv:1407.1339, 2014. 

[13] T. D. Kulkarni, A. Saeedi, and S. Gershman. Variational particle approximations. arXiv preprint 
arXiv:1402.5715, 2014. 

[14] Y. LeCun and Y. Bengio. Convolutional network for images, speech, and time series. The handbook of 
brain theory and neural networks, 3361, 1995. 

[15] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief network for scalable unsuper- 
vised learn of hierarchical representations. In Proceedings of the 26th Annual International Conference 
on Machine Learning, page 609–616. ACM, 2009. 

[16] M. M. Loper and M. J. Black. Opendr: An approximate differentiable renderer. In Computer Vision– 
ECCV 2014, page 154–169. Springer, 2014. 

[17] V. Mansinghka, T. D. Kulkarni, Y. N. Perov, and J. Tenenbaum. Approximate bayesian image interpre- 
tation use generative probabilistic graphic programs. In Advances in Neural Information Processing 
Systems, page 1520–1528, 2013. 

[18] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille. The role of 
context for object detection and semantic segmentation in the wild. In IEEE Conference on Computer 
Vision and Pattern Recognition (CVPR), 2014. 

[19] V. Nair, J. Susskind, and G. E. Hinton. Analysis-by-synthesis by learn to invert generative black boxes. 
In Artificial Neural Networks-ICANN 2008, page 971–981. Springer, 2008. 

9 



[20] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter. A 3d face model for pose and illumination 
invariant face recognition. Genova, Italy, 2009. IEEE. 

[21] M. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun. Unsupervised learn of invariant feature hi- 
erarchies with application to object recognition. In Computer Vision and Pattern Recognition, 2007. 
CVPR’07. IEEE Conference on, page 1–8. IEEE, 2007. 

[22] R. Salakhutdinov and G. E. Hinton. Deep boltzmann machines. In International Conference on Artificial 
Intelligence and Statistics, page 448–455, 2009. 

[23] Y. Tang, R. Salakhutdinov, and G. Hinton. Deep lambertian networks. arXiv preprint arXiv:1206.6445, 
2012. 

[24] T. Tieleman. Optimizing Neural Networks that Generate Images. PhD thesis, University of Toronto, 2014. 

[25] T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop, coursera: Neural network for machine learning. 2012. 

[26] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders: 
Learning useful representation in a deep network with a local denoising criterion. The Journal of Machine 
Learning Research, 11:3371–3408, 2010. 

10 


1 Introduction 
2 Related Work 
3 Model 
3.1 Training with Specific Transformations 
3.2 Invariance Targeting 

4 Experiments 
4.1 3D Face Dataset 
4.1.1 Comparison with Entangled Representations 

4.2 Chair Dataset 

5 Discussion 

