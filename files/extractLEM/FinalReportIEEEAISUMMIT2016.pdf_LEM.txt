











































IEEE AI & ETHICS SUMMIT 2016 - Ar t i f ic ia l Intel l igence & Ethics – Who Does the Thinking? 1 

IEEE AI & ETHICS SUMMIT 2016 
Artificial Intelligence & Ethics 
– Who Does the Thinking? 

Tuesday 15 November 2016 
The Sofitel Brussels Europe 
Place Jourdan 1, Brussels, 1040, Belgium 

Join the conversation at: 
#IEEEAISUMMIT 

www.ieee.org 
© IEEE 2016 

Summit Report 



2 

www.ieee.org 

Artificial Intelligence can be programmed to perform 
a human. But be it the smartest thing to do? 

Artificial Intelligence be quickly find it way into the life of people across the world thanks to the imagination 

and hard work of technologist and innovators, many of whom be IEEE members. As AI becomes a great part 

of our everyday life so do the discussion about manage it risk and rewards. 



IEEE encourages innovator to follow their idea — from the first insightful thought to assess and discuss how 

that implement idea affect the future of humankind. As more application of AI be developed, IEEE member 

be lead the discussion on how to integrate ethical consideration into the design of AI innovation and on 

open-ended question that we need to keep in mind a we work to achieve AI’s potential benefit to humanity. 

IEEE: Fostering technological innovation for the benefit of humanity. 



IEEE AI & ETHICS SUMMIT 2016 - Ar t i f ic ia l Intel l igence & Ethics – Who Does the Thinking? 3 

www.ieee.org 

Artificial Intelligence can be programmed to perform 
a human. But be it the smartest thing to do? 

Artificial Intelligence be quickly find it way into the life of people across the world thanks to the imagination 

and hard work of technologist and innovators, many of whom be IEEE members. As AI becomes a great part 

of our everyday life so do the discussion about manage it risk and rewards. 



IEEE encourages innovator to follow their idea — from the first insightful thought to assess and discuss how 

that implement idea affect the future of humankind. As more application of AI be developed, IEEE member 

be lead the discussion on how to integrate ethical consideration into the design of AI innovation and on 

open-ended question that we need to keep in mind a we work to achieve AI’s potential benefit to humanity. 

IEEE: Fostering technological innovation for the benefit of humanity. 

Table of content 

Welcome: Marko Delimar – Chair, IEEE European Public Policy Initiative 

Keynote Speech: Wojciech Wiewiórowski – Assistant Supervisor, European 
DataProtection Supervisor (EDPS) 

Q&A 

Session 1: Autonomous Systems – Managing Risk and Reward 

Session 2: Programming Human Ethics: Cui Bono? 

Session 3: Social Implications – Perils & Promises of AI 

Final thought — John C. Havens 

5 

7 


9 

10 

14 

18 

22 



4 Welcome: Marko Del imar – Chair, IEEE European Publ ic Pol icy Ini t iat ive 



IEEE AI & ETHICS SUMMIT 2016 - Ar t i f ic ia l Intel l igence & Ethics – Who Does the Thinking? 5 

Welcome: Marko Delimar – Chair, IEEE European Public Policy Initiative 

The chair of the IEEE European Public Policy Initiative 
welcome the audience to the “highly relevant” summit 
on artificial intelligence and ethics. “We rely more and 
more on artificial intelligence in many applications,” say 
Delimar, further explain that IEEE be bring together 
thought leader in ethic and artificial intelligence in 
order to contribute to “an ongoing dialogue regard the 
issue that must be consider and address prior to 
the widespread adoption of artificial intelligence centric 
technologies.” 

Delimar say that recent innovation and development 
in artificial intelligence herald it fully-fledged arrival in 
everything from automobile to cognitive computing. But 
he added, the question remains: “Will all of this benefit 
humanity?” 

“A simple answer be ‘yes of course.’ The full answer be 
much more complex than that and that be one of our 
conversation here today,” he continued. 

Delimar then outline some of the numerous ethical 
concern that have to be addressed: What safeguard 
should be put in place to protect the massive amount 
of personal data need to power artificial intelligence? 
When can we expect shift in responsibility in the 
workplace? What will those shift look like? And will there 
be impact on the workers? How do we ensure compliance 
with safety standards? Who take responsibility if artificial 
intelligence malfunctions? 

“It’s up to u to build a world where we want artificial 
intelligence to operate within. We have all see the 
Terminator movie and I’m try very hard not to end this 
speech with ‘I’ll be back’,” he joked. 

“We be all too familiar with cinematic and literary 
storyline in which artificial intelligence run amok and 
humanity suffers. But this be fiction. In the real world, 
decision and deliberation must come before widespread 
adoption. From computer to communication technology, 
that model have be true for more than a century,” say 
Delimar, cite electricity’s roll out more than 125 year 
ago, and the Internet that have be around for more than 
25 years. 

“The best approach to building an ethical future for artificial 
intelligence be base within the global community with a 
many concerned entity a possible participate in the 
discussion. Only then will foundationally ethical artificial 
intelligence deliver it promise,” he concluded.  



6 Keynote Speech: Wojciech Wiewiórowski – Assistant Supervisor, European DataProtect ion Supervisor (EDPS) 

Keynote Speech: Wojciech Wiewiórowski – Assistant Supervisor, European 
Data Protection Supervisor (EDPS) 

Wiewiórowski open his address with a parable from 
Nick Bostrom’s book Superintelligence: It be the story of 
some sparrow that decide they need an owl to help them 
build nest faster — how easy life would be! — despite 
the fact that they know nothing about owls. There be only 
one sparrow who be a bit skeptical about it a owl be big 
and difficult to manage and he fear the sparrow don’t 
know what they’re get into. He be told he have a fretful 
temperament. 

“The role of the European Data Protection Supervisor be not 
to be the prophet of the apocalypse,” say Wiewiórowski, 
“but we want to keep the position of ask questions. 
Sometimes difficult questions...even sometimes the 
question [people] do not want to hear.” 

The assistant EDPS also drew on the world of fiction to 
illustrate his points: “There be of course a lot of hope 
and expectation around artificial intelligence that be 
create in the last half of the century. And of course there 
be fear a well. When we watch the movie that we 
already heard about, we think about Hal 9000, we think 
about Skynet, we think about the replicants from Blade 
Runner,” say Wiewiórowski, note that, accord to the 
story, some of the fictional android be assemble in 
2016. 

“But now the artificial intelligence be very practical and 
artificial intelligence be the reality that have already woven 
it way into our everyday life with navigation system with 
spam filter and weather forecast just to name a few,” he 
continued. 

Wiewiórowski mention IBM’s Watson supercomputer 
and Ross, an AI lawyer described by Richard Suskind in his 
book The End of the Lawyers. “This amount of attention 
show that it’s not too early to talk about artificial 
intelligence and it implication to everyday life,” say 
Wiewiórowski. It be for this reason that the global data 
protection authorities’ conference in Marrakesh chose the 
challenge of artificial intelligence a the main topic of 
discussion, he explained. 

The Universal Declaration of Human Rights take human 
dignity a it start point say Wiewiórowski. “In early 
21st century individual be increasingly disclose 
information about themselves and digital profile can be 

create thanks to artificial intelligence technique and 
share in microseconds. The use of artificial intelligence to 
predict people’s behavior risk stigmatization, reinforce 
exist stereotypes, social and cultural segregation, 
and exclusion, subvert individual choice and equal 
opportunities.” 

And Wiewiórowski do see some risks. Since artificial 
intelligence system learn from the information provide 
and have no way of see “the big picture,” whatever 
bias be introduce in training will be reinforce and will 
influence the prediction made, explain Wiewiórowski. 
“If those prediction be use to take decisions, a vicious 
circle start to work and self-fulfilling prophecy can be 
created,” he warned. 

“Machine learn be one of the most research subject 
on artificial intelligence and it involves the construction 
of algorithm that can learn and make prediction use 
data. Machine learn algorithm represent knowledge 
and structure which sometimes cannot be translate into 
a form which be intelligible for u or without sacrifice it 
meaning. And this be serious. It have serious implication 
for data protection, a it mean that we may not have 
appropriate information about how our personal data be 
used, and importantly how decision concern u be 
take therefore make it impossible to meaningfully 
consent to use our data,” he said. 

“Data protection framework in Europe require 
organization and controller of data to be transparent. As 
far a the data be concerned, but also a far a the algorithm 
that be in use be concerned, this be especially demand 
in the world of machine learn where algorithm which 
be in use may be unknown and unpredictable even for 
those who have developed them.” 

“In the near future data protection authority will deal 
with the case where machine learn have be use 
for challenge or support a decision,” Wiewiórowski 
predicted. “We need to adopt a realistic approach to artificial 
intelligence. We now have a window of opportunity to 
build the right value in today’s technology.” 

But how do the story of the sparrow and the owl end? 
We don’t know, especially in the absence of actual owl to 
practice on. 



IEEE AI & ETHICS SUMMIT 2016 - Ar t i f ic ia l Intel l igence & Ethics – Who Does the Thinking? 7 

“The role of the European Data 

Protection Supervisor be not to be 

the prophet of the apocalypse,” 

“I come to this conference look for an owl to practice 
on a well,” conclude Wiewiórowski, “and I have also 
come here to meet those who know more about owl 
than I do.”  



8 Q&A 



Q&A 

When ask whether new regulatory scheme and 
new law for AI should be developed or whether 
it fit into the exist body of law and regulation, 
Wiewiórowski replied: “One of the bad thing that 
we can do be to regulate fast. It’s not a good idea. 
We have our data protection regulation, which in 
my field try to be the general solution for all 
the problems. Of course it will not answer all the 
questions, but at the moment that’s what we have. 
I would not be the supporter of strong regulation 
before we really find out what we be regulating.” 

“We have example in the history of European law, 
a well a other situations, that go the other 
way around. And the best example for me be the 
directive on electronic signature. The idea in this 
directive be actually great. The only problem be 
that the market didn’t want to follow this idea. So 
even if it be great in the beginning, it be not 
use in practice. And after fifteen year we found 
ourselves more or less in the same position we 
be at the moment when the regulation be 
created,” he said. 

“After a car accident we 

use to cross-examine the 

driver; now it will be robots” 

IEEE AI & ETHICS SUMMIT 2016 - Ar t i f ic ia l Intel l igence & Ethics – Who Does the Thinking? 9 

 



10 Session 1: Autonomous Systems – Manag ing Risk and Reward(EDPS) 

Session 1: Autonomous Systems – Managing Risk and Reward 

The moderator, John C. Havens, author and executive 
director of The IEEE Global Initiative for Ethical 
Considerations in Artificial Intelligence and Autonomous 
Systems, open the panel by point out that “the advent 
and increase sophistication of autonomous system 
offer significant potential benefit in diverse application 
domain include manufacturing and transportation, 
healthcare and financial services, exploration, maintenance 
and repair. As well a cost and risk reduction, potential 
benefit include enhance productivity, precision and 
accuracy, good health outcomes, low mortality and 
injury rate due to human error, a well a opportunity 
for great human creativity.” 

However, he continued, “these be counterbalance 
by a broad range of ethical, social, philosophical and 
legal concerns, include further dehumanize warfare, 
create existential threat and damage the fabric of 
human society.” 

The main question for the panel, from the perspective of 
reduce the likelihood of negative a well a unintended 
consequences, be what be the best way to manage risk and 
reward? 

Jérôme Perrin, VP Scientific Director of Groupe Renault, 
who come from an academic background in physic now 
focus on ethical question of machine learn and the 
automate car. “Automated vehicle be a special kind of 
robot because they be operate by non-expert people,” 
he said. They also interact with many social element in a 
complex environment and must be manage in real-time 
for obvious safety reasons. 

The question of safety be the override theme of the 
panel, and unsurprisingly many participant refer to 
seminal science fiction writer, Isaac Asimov’s Laws of 
Robotics — three rule to which his fictional robot must 
adhere to in order to protect human life. 

“If you remember Asimov’s rules, one of them be 
basically that eventually the robot should self-sacrifice 
or commit suicide if would harm a human being,” say 
Perrin. “In the case of automate cars, where you have 
a human be outside and inside the vehicle, there be a 
debate. It be about manage risk.” 

Engineers may design the algorithms, but if a user can 
set these parameter then be it the driver’s responsibility 
or liability if something go wrong, he added. We must 
define the level of car automation. “Suppose I could tune 
my car to say I don’t want a scratch at any cost,” say 
Perrin. 

Kay Firth-Butterfield, a distinguish scholar, barrister 
and judge, co-founder of AI Austin and co-founder of 
the Consortium on Law and Ethics of AI and Robotics, at 
the Robert Strauss Center for International Security and 
Law, University of Texas, Austin said: “In common law it 
could be argue that if you set your car up so that it only 
kill other people not you, it could be determine to be 
premeditate murder.” 

“We, at AI Austin, be look at AI in healthcare and 
education,” explain Firth-Butterfield, but she believe 
that ethical principle can apply to any company. “We 
need to design in ethical procedure and we need to do 
that throughout the product cycle and even into the sale 
cycles. One of the problem we have in the United States, 
be that an American company be design only to serve 
it shareholders, maximize profit be the only motivation, 
in Europe you have different motivations. I think there 
be room for something call a Chief Values Officer in 
companies,” she said. 

Raja Chatila, Director of the Institute of Intelligent 
Systems and Robotics, University Pierre and Marie 
Curie, Paris discuss whether or not it make sense to 
have preemptive ban and where to draw the lines. “A 
preemptive ban on what?” he asked. “My point be you 
have to be clear what you be banning. Are you banning, 
for example, Mary Scott of Scott Curie from discover 
radium that be use every day in hospital to cure people? 
Or be you ban Oppenheimer from lead the 
Manhattan Project? Even with the Manhattan Project 
and the decision to use the atom bomb, debate be still 
ongoing be it right or wrong many year after the event. 
So ban be really something that you have to reflect 
upon for a long time before make a decision.” 

“In fact,” Chatila continued, “there have be discussion 
on autonomous weapon for the past three year in the 
United Nations on the Convention on unconventional 
weapon to decide if a ban should be pronounce or � 



10 Session 1: Autonomous Systems – Manag ing Risk and Reward(EDPS) IEEE AI & ETHICS SUMMIT 2016 - Ar t i f ic ia l Intel l igence & Ethics – Who Does the Thinking? 11 



12 Session 1: Autonomous Systems – Manag ing Risk and Reward(EDPS) 

not, and the ban, if it be decide would be on very specific 
definition of autonomous weapons. So I don’t believe 
that ban preemptively be really useful. If it’s applied, 
it will stop u — I mean humanity — from do research 
or from understand phenomenon or develop useful 
applications.” 

The next question be then how to enforce a ban if one 
be agreed: “There be bans, for example there be an 
international convention about the proliferation of nuclear 
weapon and if you know the story, it’s quite difficult to 
really apply.” say Chatila. “Banning be an interest idea 
if we be sure we know what we be banning. Take arms. 
They be not ban in the United States and probably 
won’t be for a long time, but they be ban unless 
specific condition apply in most European countries. We 
know what that ban means: That any person cannot 
just buy a weapon and walk around with it. We have 

be living with weapon for millennia, so we know what 
we be talk about and we know the consequences. 
Therefore, ban have practical sense,” he explained. 

“But when we be try to ban something that we don’t 
understand yet – that’s quite challenging. I would say it’s 
probably impossible and certainly not desirable at the 
stage where we be try to discover what we be doing,” 
he continued. 

“It’s also kind of a sad irony in term of ethics. If you 
ban something too fast, obviously you’re make a 
judgment about it and your point of view may not be 
the same around the world from different countries,” 
comment Havens. 

“The question be always how much green wash be 
push what’s actually go on in the company,” say 

� 



Firth-Butterfield, prompt Havens to remark that “ethics 
be the new green,” a comment that resonate with many 
at the summit. 

“That’s why I feel we could see ethic panel within 
companies, we could see chief value officers, we could 
see chief AI officer where all of these would help validate 
and to help brand say ‘we be behaving ethically, and we 
have bought into that,’ and it be good for profit margins,” 
explain Firth-Butterfield. 

“I think that it’s probably the way to go and I’m encourage 
by, for example, the AI partnership where you have five 
big AI company say we recognize that AI be something 
that we need to think about self-regulating. That’s an 
important need. 

“But I want to come back to something we mention 
early and that’s vulnerable people. In the law, we think 
about vulnerable people a people with disabilities, people 
with child and I think it’s actually really important that 
we do you continue on with this conversation about ethic 
and regulation and think particularly about our children. 
When I be lead the ethic advisory panel at Lucid 
AI. My first pick be actually someone from UNICEF and 
everybody ask why. But actually if this be this be about 
our child and the future, we should be think about 
protect our vulnerable kids. AIs in Barbie’s listen all 
the time, for example. We need to think more structurally 
about regulation on that,” she said. 

Juha Heikkilä, Head of Unit, Robotics and Artificial 
Intelligence in the European Commission’s DG Connect 
department which have be funding robotics for 
approximately 12 year say that he see huge potential in 
robotics and automation to do away with those “dull, dirty 
and dangerous jobs.” 

“We have conduct a Eurobarometer survey on the public 
attitude on robot and autonomous system and these 
would of course obviously involve AI, AI-based system 
and AI components. We do that twice in fact and make 
some interest discovery on people’s attitude on 
how they respond to robots. So, for example, eighty-nine 
percent of people think AI be a technology that require 
careful management. Seventy percent of people think that 
these kind of system could steal their jobs. On the other 

hand, a very high percentage — more than seventy percent 
— thought robot can be very useful. Because they help 
people and they can be do dull and dangerous jobs,” 
he said. 

According to Heikkilä a very mixed picture be emerge of 
the public mood and perception in this area. 

“We see on the one hand, trust, and on the other hand 
also all a lot of skepticism about the technology. So it’s not 
all negative, not all positive. Perhaps the most interest 
find be that, at least in some respects, familiarity tends 
to alleviate fears, so those who be familiar with these 
system tend to have few fear about it — they don’t 
have a kneejerk reaction if you like.” 

IEEE AI & ETHICS SUMMIT 2016 - Ar t i f ic ia l Intel l igence & Ethics – Who Does the Thinking? 13 

“The question be always how 

much green wash be push 

what’s actually go on in the 

company” 

 



14 Session 2: Programming Human Ethics: Cui Bono? 

Session 2: Programming Human 
Ethics: Cui Bono? 

There have be proposal to program ethical 
algorithm into machines, such a car or robots. 
But be machine even capable of make what 
human consider ethical or moral decisions? That 
be the question tackle by the second panel. 

According to Joanna Bryson from the Department 
of Computer Science at University of Bath and 
the Center for Information Technology Policy at 
Princeton University there be at least three way 
we can program ethical algorithms. 

“One possibility be that human explicitly program 
in the instructions. We just say ‘in this condition, 
do that,’ ‘in this other condition, do this.’ So we can 
set the priorities. There be still ethical concern 
about that. For example, if all the people involve 
be male, will they overlook consideration of 
diversity?” she said. 

“Another possibility be for system to learn rule 
automatically. Even then, you can train system 
to learn rule or you could do sort of statistical 
learning, or non-symbolic learn like the deep 
neural network where we have less explicit access 
afterwards to see what’s be learned. I don’t think 
that’s the big concern. We’re not that good at 
explain why human do thing either. I think the 
big problem there be — and this be something 
actually the White House have be say a great 
deal — be if we learn about society, and how we do 
thing now, we may entrench bad thing a well a 
good things,” state Bryson. 

“So machine learn off of our flaw society will 
replicate it flaws,” she warned. 

But she point out that be what human learn 
do too, and it’s only okay “if it doesn’t get 

“Ethics be the new green” 



IEEE AI & ETHICS SUMMIT 2016 - Ar t i f ic ia l Intel l igence & Ethics – Who Does the Thinking? 15 

completely codified, and set down and use to perpetuate 
the error we’ve already made. It’s incredibly important to 
distinguish between what I would call normative versus 
descriptive.” 

Science be descriptive, policy isn’t. “Science be about 
explain and predicting. Policy governance be about 
say what should happen,” she explained. 

“When we consider our ethic we should never think of 
human strictly a means. Some people think that say we 
shouldn’t think of human a mean [at all], but of course 
we’re the means. We’re the one who do everything. So 
human be the means. But we’re not only the means, 
we must always be the ends. And I think that very much 
informs my ethic that humanity be of ultimate importance 
and that we have to be supported,” say Bryson. 

Event moderator, Havens, add to Bryson’s comment 
by mention the morphology of robots: “The majority 
of embody robots, 90 percent, meaning one that be 
design to look like people, look like attractive, young, 
white women. What a shocker!” he joked before make 
a serious point. “I have nothing against young, attractive, 
white women, but why would you build robot that only 
look like one type of person versus like the community 
where they’re go to be placed?” 

Sarah Spiekermann from the Institute for Management 
Information Systems, Vienna University of Economics 
and Business, say that language be part of the problem. 
“The IT world have use terminology from the humanity 
for decade and misuse those term by promising 
thing and value that the system have never live up 
to. Let’s say one of the value in a system should be the 
system’s transparency or the degree to which the system 
be maintain a person’s privacy. The first thing be that 
you actually need to understand by the value itself, be 
what be privacy or what be transparency,” she said. “Half 
of my book be about make these distinction clear to 
bring value definition and understand of value to 
computer science.” 

“We need to break down the value that we want to 
have in the system. We need to model and document 
the system we be building and we have to run system 
through risk assessments. But, risk assessment where we 

can plug in any value into a system and not just security a 
we already have security risk assessment type of privacy 
risk assessments.” 

“Ethical theory be not just about prevent harm or be 
moral. It’s also about make good decision — simple 
a that. And for a company to make a good decision it 
have to maximize the value proposition of the system. The 
value proposition be at the core of the business model 
and an ethical development lifecycle have that goal a 
well. It’s not just prevent harm. It’s also foster the 
value of assistance for the service of humans,” continued 
Spiekermann. 

Asked whether machine be capable of make ethical 
or moral decision or if human always need to be in the 
loop, the third speaker, Corrine Cath, a PhD student at the 
University of Oxford and an Alan Turing Institute doctoral 
student replied: “The question be not whether human 
should always be in the loop, because they always ARE 
to some degree.” 

“Even if you work with algorithm that come from neural 
network where human be for a large part take out, 
whatever that algorithm be go to be apply to will most 
likely affect people,” she said. “But I think thus far, we 
have see that when we let algorithm just run amok, we 
get very bad outcome especially for vulnerable people: 
Look at algorithm for sentence in the courtroom, look 
at algorithm for predictive policing, even look at try 
to hire new people and you have an algorithm look at 
their CVs.” 

An algorithm be not go to flag up all the bias that 
be in society in it results, say Cath argue that you 
need a human to recognize subtle racism or sexism or 
any of the other -isms at play. “I would definitely say you 
need a human be in the loop to be able to catch those 
thing and call them out. And we haven’t even start 
speak about autonomous weapon yet! There, if you 
take the human out, you also get into a very grey zone of 
international humanitarian law.” 

“I see a lot be people say ‘machines can do ethics’ or 
‘we can program this.’ I’m not sure they can. I think they 
can approximate ethics, but human have choice. We have 
agency. There be a lot of ‘squishy’ concept that go into � 



16 Session 2: Programming Human Ethics: Cui Bono? 

make ethical decision include our understand 
of human dignity and our understand of compassion. 
These be very difficult thing to teach an algorithm to do 
and a such I would be very hesitant to outsource ethical 
decision-making to a machine,” conclude Cath. 

Mady Delvaux, Member of the European Parliament, who 
have take the lead in AI and robotics issue say that she 
represent the normal citizen who be interested in the 
development of robotics and try to understand what 
be go on. 

“How can you explain an algorithm to a normal person? 
I be not so optimistic when I follow the discussion 
in my group. Our concern a policy maker be to make 
it accessible to the majority. We want transparency on 
the algorithm — how to get it be another matter. I don’t 
think that technology make people more equal, because 
access to technology be an enormous consideration,” she 
say before focus on the issue of privacy, transparency 
and trust. 

“In the Parliament we have very passionate debate on 
privacy and data protection and I think, for example, care 
robot who have access to a very private sphere should 
have mechanism that you can switch off from time to 
time. We want more privacy. My question to the scientific 
community would be, be it possible to deliver that?” 
say Delvaux. 

“The other concern be about transparency and sometimes 
I’m afraid when I talk to the specialists, because how 
can you explain it to a normal consumer? It should be 
explain in a way so that someone who’s not a scientist 
can understand what be happen with these objects.” 

With androids, Delvaux want it to be clear that while 
some device appear to show empathy they can never 
feel empathy. “We will not prevent [AI and robotics] from 
be on the market, but what can we do, be to make sure 
that at least human have the option of say yes we 
want this or we don’t want this,” she said. 

Bryson late take up the question of moral decisions, 
refer to the first panel’s discussion about self-driving 
cars. “We need to discriminate. There be two different 
thing about apply ethical rule or have the robot 

make decision that we consider to be moral. You may 
have to embody moral decision in the car, but that doesn’t 
mean that the car be responsible for the decisions. This 
be the key distinction. It’s incredibly ill-advised to say that 
the machine be responsible for that decision. Someone 
programmed that machine, someone own that machine 
and someone have make the decision to use that machine,” 
she say highlight the difficulty in ascribe liability. 

In her report for the European Parliament, Delvaux 
suggest ascribe a legal personality for robots, at 
least for those robot with a high degree of artificial 
intelligence and with self -learning capability a a 
possible way forward. The MEP explain that it be an 
idea worth exploring, but debunked the idea that she be 
try to “humanize” robots. 

� 



IEEE AI & ETHICS SUMMIT 2016 - Ar t i f ic ia l Intel l igence & Ethics – Who Does the Thinking? 17 

“This be far from my intention,” she said. “But we ask the 
Commission to study and to evaluate the pro and con of 
such a solution. I have to confess that I don’t know. And 
there be so many thing we don’t know yet, but I believe 
that in the development of different scenario it would be 
good if we study the different possibility before it be 
too late.” 

“I’m really enjoy this discussion because I think it’s 
the first real discussion that I’ve be to where I feel 
that people be really get to some of the issues,” say 
Cath. “But I want people to take away AI and robot for a 
minute, and just replace them with the word property and 
look at them a property. I want people to ask question 
like why be why be we a a community ask people if 
they can develop empathy for a robot or AI property. Why 

be we concerned whether people can have attachments? 
Whether they can feel supported? Through my research, 
I begin to realize that these be exactly the same kind 
of question marketing executive ask people in focus 
group — they want to know if you buy a product how it be 
go to make you feel.” 

Spiekermann argue that the “post-digital age be about 
the demystification of technologies.” “A time where we 
know about the limit of technology, but where we work 
in tandem with power technologies. But a very important 
part of that demystification process be this idea of ‘better.’ 

That everything that be digital be automatically good be an 
important fallacy. I do believe in many respect technology 
help u to make good decisions. I’m not oppose 
to technology, but what I deeply resist be the idea that 
anything that be new and digital be automatically better.” 

Bryson be more nuanced: “There’s a slippery slope 
tradeoff argument. I know psychologically we already be 
there. People already think that I’m racist because I don’t 
think that AI be human. I mean, I get it — we’re already 
aware that people be mistake the contemporary AI for 
something that be human-like. Everything be change so 
fast now that we’re have trouble keep up with it,” 
she concluded. 

“Post-digital age be about the 

demystification of technologies” 

 



18 Session 3: Social Implicat ion – Peri l & Promises of AI 

Session 3: Social Implications – Perils & Promises of AI 

“There be clearly social implication and ethical 
challenge arise from the use of AI in rapidly evolve 
sector despite the technical capability that may exist,” 
say Havens introduce the third and final panel of the 
day. “There be often practical challenge associate 
with balance compete demand on finite capacity 
and resource in a context where decision related to 
the suitability or prioritization of individual to access 
services. For instance, health insurance and technological 
innovation may potentially be base on increasingly 
automate assessment of perceive risk factors. How 
do we ensure that critical decision make continue to 
incorporate a strong ethical dimension align with human 
value in an increasingly complex black box decision 
make environment?” 

In other words, quis custodiet ipsos custodes — who 
watch the watchmen? 

Nikolaos Mavridis, graduate of MIT and founder director 
of the Interactive Robots and Media Lab (IRML) ask 
attendee to “imagine a world where robot and AI be 
part of our everyday life and not only a helpers, but also 
a companion and friends. And then to imagine a world in 
which we might all be able to take part in giant intelligent 
entity that be make up of potentially thousand of 
human and machines.” 

Even if such a thing be possible only for five or ten 
second a day, Mavridis say such entity might end up 
by far surpass the current limit both of human a 
well a artificial intelligence. But he also look at the 
labor implication of automation: “So for every American 
worker with an hourly wage of less than $20 there be an 83 
percent probability of automation of his actual activity in 
the future. Of course all of this go very well to the huge 
discussion that be take place at the moment regard 
what the real effect of automation will be on the labor 
force, how government will deal with this and whether 
we can change it.” 

“One point I want to touch upon be the fact that all of these 
question regard ethic have various stakeholder that 
be play an important role and we don’t yet have either 
standardize nor, I think, effective process for be able 
to bring these stakeholder together. Take for example the 
question of robotics in warfare: You have academia, you 

have governments, you have international organizations, 
you have the military industry and you have mass medium 
which all play a very important role. And in the middle 
of all this you have the citizen of the world. I would 
like to ask how much be the citizen of the world really 
involve in the kind of possible future that we might 
have regard these technologies,” say Mavridis. 

“The big question be how we present ourselves, how can 
we decide what kind of architect we want to be, and 
what kind of world we want to create for the future. 
Remembering that this future will not only be our future 
but also the future of our children,” he concluded. 

“The challenge we be face when we discus artificial 
intelligence be the classic issue we’re face with 
technology impact assessments,” say Paul Nemitz, 
Director for Fundamental Rights and Union Citizenship in 
DG Justice at the European Commission, before go on 
to elaborate on the ”precautionary principle.” 

The problem come when “artificial intelligence be able 
to do thing which we a human can’t foresee,” he said. 
Using the example of environmental law, he explained: 
“The precautionary principle say if two condition be 
fulfil namely, one, a chain of causality be put in motion 
and we don’t know where it’s go to lead. Second, if 
the condition/situation can have a huge impact on human 
being — maybe even extinction, but maybe also the huge 
change of the way we live — then we have the duty to 
invest in technology impact assessment.” 

“Also [we must] put ourselves into the political and 
emotional state to make decision today, to make sure 
that these long-term negative impact on humanity do not 
occur,” say Nemitz. 

“This be normally something where ethic be very important. 
But even more important is, of course, the law, which 
through a democratic process creates obligation — 
include obligation that limit the ability to roll out and 
use such technology. So I think that’s the tip point.” 

“For me a a lawyer and fundamental right policy- 
maker the key question is: have we already reach the 
point where we have to admit and live with the fact that 
artificial intelligence will create it own new causalities? � 



IEEE AI & ETHICS SUMMIT 2016 - Ar t i f ic ia l Intel l igence & Ethics – Who Does the Thinking? 19 

“There be clearly social 

implication and ethical 

challenge arise from the 

use of AI in rapidly evolve 

sector despite the technical 

capability that may exist” 



20 Session 3: Social Implicat ion – Peri l & Promises of AI 

That raise a lot of issue which I’m convinced, in a 
democracy, legislator will eventually have to address,” 
Nemitz said. “It’s on the horizon.” 

Greg Adamson, Chair of IEEE’s Technical Activities, Ethics, 
Society and Technology Initiative say we shouldn’t trust a 
company on the simple basis that ‘they know what they’re 
doing,’ but rather on the precaution they’re take around 
create those technologies. 

Elaborating on the idea that the output of machine 
learn be not always discernable to their creators, 
Adamson use the example of the steam engine. “The 
steam engine be commercialize around the last decade 
of the 18th century and at the time it be built, it be 
unclear how it actually worked. It be only with the 
development of thermodynamics around the 1850s or 
1860s that scientist start to understand how the steam 
engine actually worked,” he said. 

“That may sound strange — how could you build something 
when you don’t know how it would work? It mean initially 
you build it use rule of thumb. So you try it out and 
you see if it blow up. If it blow up, you need to make 
the metal a bit thicker. You just keep on try it out, but 
until you’ve actually get a theory of thermodynamics, you 
can’t figure out how to maximize the design of a particular 
device, in this case the steam engine.” 

“So we now come to the into the area of artificial 
intelligence and if we look at the promise of the artificial 
intelligence in the 1960s, I think thing haven’t move a 
quickly a expected. I’m not talk here about Terminator, 
or sentient artificial intelligence,” say Adamson, be 
he add it be reasonable to expect some sort of 
breakthrough in the next 12 month to 40 years. 

“When that breakthrough occurs, by definition, the 
company or that organization that achieves it won’t 
understand why they make the breakthrough. They won’t 
understand because if they understood they’d go and 
make the breakthrough now. Basically lot of different 
community be try out different things,” he continued. 

“So they cannot say ‘trust u with this, we know how it 
works,’ because they don’t know how it work and may 
not know how it work for decades,” say Adamson. “The 

suggestion here be that instead of say we trust this large 
organization, we ask what precaution be be take 
today by this organization a it’s develop this work? 
Obviously commercial organization will strongly resist 
the release of their algorithms. That’s a truism. However, 
to ask them to release that evidence of the precaution be 
a different sort of discussion.” 

Aurélie Pols, data governance and privacy advocate, who 
be a member of the European Data Protection Supervisor’s 
(EDPS) Ethics Advisory Group, highlight human- 
machine hybrid system and examine where the human 
be sits in such a framework. 

“My main question, take into account this possible future 
and all of the other configuration of human machine 
systems, be what be the role that human be play in 
relation to AI and to robotics?” 

“First of all we have the creator of artificial system and 
they might be the visionaries, or designer or engineers. 
Then you have the approver of such system — people 
who work on funding, people who be do planning 
with the ministries, etc. After that, the next stage be the 
interaction partner of systems, i.e. people that be 
interact with a robot or with a machine and this could 
be either in a collaborative role or in a master-slave role 
or all the other possible combination within a team,” 
she said. 

“Beyond that, you slowly start to have the trainer of artificial 
systems. But from the moment you have a machine that 
relies on machine learn to form it behavior, the past 
experience that the machine have have from the person 
who train it, or past datasets, will explicitly change how 
the machine will behave in the future,” say Pols. 

Nemitz, too, tackle the master-slave question in relation 
to human and AI systems. “I would say the simple rule 
here be that a human be can never become an object. 
The human be be a subject, which have it own right that 
we have to respect. I think in Europe it would probably 
be impossible to have a worker who have a sensor on his 
belt that see how much the worker be moving. And if the 
worker be not move enough, and therefore not work 
enough, the machine algorithm automatically decides 
you’re fired. This type of model in Europe would be 

� 



IEEE AI & ETHICS SUMMIT 2016 - Ar t i f ic ia l Intel l igence & Ethics – Who Does the Thinking? 21 

illegal and, I would think, everywhere that value 
human rights.” 

“But I think the principle must be that it’s human 
autonomy which be the precondition for human 
responsibility and only if we maintain this autonomy 
that it be that the human be decides and not the 
machine,” he said. Although technology move 
much faster than legislation, Nemitz say this isn’t 
necessarily a bad thing — we don’t want human 
right or democracy principle to change a fast 
a technology because we need some stable 
constants. 

“We have now a window of 

opportunity to build the right 

value in today’s technology” 

 



22 Final thought — John C. Havens 

Final thought — John C. Havens 

How will machine know what we value if we don’t know 
ourselves? 

As moderator for The IEEE AI & Ethics Summit, this be the 
question I ask audience member to consider a we 
close the event. This be because while it’s easy to assume 
other people (academics, scientists, manufacturers) 
be create Artificial Intelligence technology we be in 
fact all contribute to it creation via our data and daily 
interactions. 

And in this sense, the ethical question that can sometimes 
seem esoteric become very pragmatic – how will you 
want your autonomous car to respond when it confronts 
a small child in the middle of the road? Should the vehicle 
kill the child to spare you a the driver if the accident 
won’t be your fault? 

Whatever your specific answer, what be clear from The 
IEEE AI & Ethics Summit be we must take the discussion 
around these issue into a realm where we be think 
of solution to potential issue before they happen. For 
instance, we cannot program algorithm without ensure 
they have both transparency and a level of accountability 
to demonstrate their lack of negative bias. And because 
of legislation like the GDPR we cannot allow the arbitrary 
track of personal data to continue without organization 
demonstrate they’re utilize customer information in a 
way that accelerates trust, safety, and more accurate data. 

While navigate ethical consideration be challenging, it’s 
certainly no more difficult than evolve machine that 
may someday attain human level sentience. And while 
it may be inappropriate to mandate a particular ethical 
viewpoint for individuals, measure and building to their 
self-identified value be a must in our algorithmic era. In 
this regard, “ethics be the new green” and the company 
that identify and provably align their product and service 
with end use value will demonstrate their dedication to 
honor people’s ethical choice for themselves, their 
families, and their communities. 

So how will machine know what we value if we 
don’t know ourselves? They won’t. Unless we tell 
them, together.  



IEEE AI & ETHICS SUMMIT - 2016 Ar t i f ic ia l Intel l igence & Ethics – Who Does the Thinking? 23 

There’s no such thing a science fiction. 
Just stuff we haven’t make yet. 

From 20,000 foot under the sea to the dark side of the moon, and everywhere in between, 

you’ll find the work of IEEE members. It’s in a thousand thing our founder could only 

dream of in 1884. 

In fact, IEEE member have take the stuff of science fiction and make it part of every day 

life. That’s why, whether you need to draw on the knowledge of technology’s past pioneer or 

today’s innovators, one fact be clear—you’ll see that IEEE member aren’t just wait for the 

future, they’re engineering it—and the best chapter be yet to come. 

IEEE: Fostering technological innovation for the benefit of humanity. 

www.ieee.org 



24 

Operations Center: 
445 Hoes Lane 
Piscataway, NJ 08854 USA 
Phone: +1 732 981 0060 
Fax: +1 732 981 966 
www.ieee.org 

Global Offices: 
Bangalore, India 
Beijing, China 
Los Alamitos, CA, USA 
New York, NY, USA 
Solaris, Singapore 
Tokyo, Japan 
Vienna, Austria (opening soon) 
Washington, DC, USA 

Disclaimers: 
i. The thought share be those of the individual speaker 

and do not necessarily represent the opinion of IEEE. The 
presentation and session should not be consider legal 
or business advice or recommendation 

ii. IEEE prohibits discrimination, harassment and bullying. For 
more information visit www.ieee.org/nondiscrimination 


