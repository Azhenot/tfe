


















































Fair prediction with disparate impact: 

A study of bias in recidivism prediction instrument 

Alexandra Chouldechova 

Heinz College, Carnegie Mellon University 
5000 Forbes Avenue, Pittsburgh, PA, USA 

achould@cmu.edu 

Abstract 

Recidivism prediction instrument (RPI’s) provide 
decision maker with an assessment of the likelihood 
that a criminal defendant will reoffend at a future 
point in time. While such instrument be gain 
increase popularity across the country, their use 
be attract tremendous controversy. Much of the 
controversy concern potential discriminatory bias in 
the risk assessment that be produced. This paper 
discus a fairness criterion originate in the field 
of educational and psychological test that have re- 
cently be apply to ass the fairness of recidi- 
vism prediction instruments. We demonstrate how 
adherence to the criterion may lead to considerable 
disparate impact when recidivism prevalence differs 
across groups. 

1 Introduction 

Risk assessment instrument be gain increase 
popularity within the criminal justice system, with 
version of such instrument be use or consid- 
ered for use in pre-trial decision-making, parole de- 
cisions, and in some state even sentence [1, 2]. 
In each of these cases, a high-risk classification— 
particularly a high-risk misclassification—may have 
a direct adverse impact on a criminal defendant’s 
outcome. If RPI’s be to continue to be used, it be 
important to ensure that they do not result in uneth- 
ical practice that disparately affect different groups. 

Within the psychometrics literature, there exist 
widely accepted and adopt standard for assess 
whether an instrument be fair in the sense of be 
free of predictive bias. These standard have recently 
be apply to the COMPAS [3] and PCRA [4] in- 
struments, with initial finding suggest that there 

be evidence of predictive bias when it come to gen- 
der, but not when it come to race [5, 6, 7]. 

In a recent widely popularize investigation of the 
COMPAS RPI conduct by a team at ProPublica, 
a different approach to assess instrument bias told 
what appear to be a contradictory story [8]. The au- 
thor found that the likelihood of a non-recidivating 
Black defendant be assess a high-risk be nearly 
twice that of White defendants. While this analysis 
have met with much criticism, it have also make head- 
lines. There be no doubt that it be now embed in 
the national conversation on the use of RPI’s. 

In this paper we show that the difference in false 
positive and false negative rate cite a evidence of 
racial bias in the ProPublica article be a direct con- 
sequence of apply an instrument that be free from 
predictive bias1 to a population in which recidivism 
prevalence differs across groups. Our main contri- 
bution be twofold. (1) First, we make precise the 
connection between the psychometric notion of test 
fairness and error rate in classification. (2) Next, 
we demonstrate how use an RPI that have differ- 
ent false postive and false negative rate between 
group can lead to disparate impact when individ- 
uals assess a high risk receive stricter penalties. 
Throughout our discussion we use the term disparate 
impact to refer to setting where a penalty policy 
have unintended disproportionate adverse impact on 
a particular group. 

It be important to bear in mind that fairness 
itself—along with the notion of disparate impact— 
be a social and ethical concept, not a statistical one. 
An instrument that be free from predictive bias may 
nevertheless result in disparate impact depend on 
how and where it be used. In this paper we consider 

1in the psychometric sense 

1 

ar 
X 

iv 
:1 

61 
0. 

07 
52 

4v 
1 

[ 
st 

at 
.A 

P] 
2 

4 
O 

ct 
2 

01 
6 



hypothetical use case in which we be able to di- 
rectly connect statistically quantifiable feature of 
RPI’s to a measure of disparate impact. 

1.1 Data description and setup 

The empirical result in this paper be base on 
the Broward County data make publicly available 
by ProPublica [9]. This data set contains COM- 
PAS recidivism risk decile scores, 2-year recidi- 
vism outcomes, and a number of demographic and 
crime-related variables. We restrict our attention to 
the subset of defendant whose race be record a 
African-American (b) or Caucasian (w). 

2 Assessing fairness 

We begin by with some notation. Let S = S(x) 
denote the risk score base on covariates X = x, 
with high value of S correspond to high level 
of assess risk. Let R ∈ {b, w} denote the group 
that the individual belongs to, which may be one of 
the component of X. Lastly, let Y ∈ {0, 1} be the 
outcome indicator, with 1 denote that the give 
individual recidivates. In this notation, we can think 
of the psychometric test fairness condition roughly 
a follows. 

Definition 2.1 (Test fairness). A score S = S(x) be 
test-fair (well-calibrated)2 if it reflect the same like- 
lihood of recidivism irrespective of the individual’s 
group membership, R. That is, if for all value of s, 

P(Y = 1 | S = s,R = b) = P(Y = 1 | S = s,R = w). 
(2.1) 

Figure 1 show a plot of the observe recidivism 
rate across all possible value of the COMPAS score. 
We can see that the COMPAS RPI appear to adhere 
well to the test fairness condition. In their response 
to the ProPublica investigation, Flores et al. [10] fur- 
ther verify this adherence use logistic regression. 

2.1 Implied constraint on the false pos- 
itive and false negative rate 

To facilitate a simpler discussion of error rates, we 
introduce the coarsen score Sc, which be obtain 

2Depending on the context, we may further desire that 
this criterion be satisfied when we condition on some of the 
covariates. Our analysis extends to this case a well. 

0.00 

0.25 

0.50 

0.75 

1.00 

1 2 3 4 5 6 7 8 9 10 
COMPAS decile scoreO 

b 
er 

ve 
d 

pr 
ob 

ab 
ili 

ty 
o 

f r 
ec 

id 
iv 

be 
m race Black White 

Figure 1: Plot show P(Y = 1 | S = s,R) for the COM- 
PAS decile score, with R ∈ {Black,White}. Error bar 
represent 95% confidence intervals. 

by thresholding S at some cutoff sHR. 

Sc(x) ≡ 

{ 
HR if S(x) > sHR 

LR if S(x) ≤ sHR 
(2.2) 

The coarsen score simply ass each defen- 
dant a be at high-risk or low-risk of recidivism. 
For the purpose of our discussion, we will think of 
Sc a a classifier use to predict the binary outcome 
Y . This allows u to summarize Sc in term of a 
confusion matrix, a show below. 

Sc = Low-Risk Sc = High-Risk 

Y = 0 TN FP 
Y = 1 FN TP 

It be easily verify that test fairness of S implies 
that the positive predictive value of the coarsen 
score Sc do not depend on R. More precisely, it 
implies that that the quantity 

PPV(Sc | R = r) ≡ P(Y = 1 | Sc = HR, R = r) 
(2.3) 

do not depend on r. Equation (2.3) thus form a 
necessary condition for the test fairness of S. We 
can think of this a a constraint on the value of the 
confusion matrix. A second constraint—one that 
we have no direct control over—is the recidivism 
prevalence within groups, which we denote here by 
pr ≡ P(Y = 1 | R = r). 

Given value of the PPV ∈ (0, 1) and prevalence 
p ∈ (0, 1), it be straightforward to show that the false 
negative rate FNR = P(Sc = LR | Y = 1) and false 
positive rate FPR = P(Sc = HR | Y = 0) be related 

2 



via the equation 

FPR = 
p 

1− p 
1− PPV 

PPV 
(1− FNR). (2.4) 

A direct implication of this simple expression be that 
when the recidivism prevalence differs between two 
groups, a test-fair score Sc cannot have equal false 
positive and negative rate across those groups.3 

This observation enables u to good understand 
why the ProPublica author observe large discrep- 
ancies in FPR and FNR between Black and White 
defendants.4 The recidivism rate among back defen- 
dants in the data be 51%, compare to 39% for White 
defendants. Since the COMPAS RPI approximately 
satisfies test fairness, we know that some level of im- 
balance in the error rate must exist. 

3 Assessing impact 

In this section we show how difference in false pos- 
itive and false negative rate can result in disparate 
impact under policy where a high-risk assessment 
result in a stricter penalty for the defendant. Such 
situation may arise when risk assessment be use 
to inform bail, parole, or sentence decisions. In 
the state of Pennsylvania, for instance, statute per- 
mit the use of RPI’s in sentencing, provide that 
the sentence ultimately fall within accepted guide- 
lines. We use the term “penalty” somewhat loosely 
in this discussion to refer to outcome both in the 
pre-trial and post-conviction phase of legal proceed- 
ings. Even though pre-trial outcome such a the 
amount at which bail be set be not punitive in a le- 
gal sense, we nevertheless refer to bail amount a a 
“penalty” for the purpose of our discussion. 

There be notable case where RPI’s be use for 
the express purpose of inform risk reduction ef- 
forts. In such settings, individual assess a high 
risk receive what may be view a a benefit rather 
than a penalty. The PCRA score, for instance, be 
intend to support precisely this type of decision- 
make at the federal court level. Our analysis in 
this section specifically address use case where 
high-risk individual receive stricter penalties. 

3This observation be also make in independent concurrent 
work by Kleinberg et al. [11]. 

4Black: FPR = 45%, FNR = 28%. 
White: FPR = 23%, FNR = 48% 

To begin, consider a set in which guideline 
indicate that a defendant be to receive a penalty tL ≤ 
T ≤ tH . A very simple risk-based approach, which 
we will refer to a the MinMax policy, would be to 
assign penalty a follows: 

TMinMax = 

{ 
tL if Sc = Low-risk 

tH if Sc = High-risk 
. (3.1) 

In this simple setting, we can precisely character- 
ize the extent of disparate impact in term of rec- 
ognizable quantities. Define Tr,y to be the penalty 
give to a defendant in group R = r with observe 
outcome Y = y ∈ {0, 1}, and let ∆ = ∆(y1, y2) = 
E(Tb,y1 − Tw,y2) be expect difference in sentence 
between defendant in different groups. ∆ be a mea- 
sure of disparate impact. 

Proposition 3.1. The expect difference in penalty 
under the MinMax policy be give by 

∆ ≡ EMinMax(Tb,y1 − Tw,y2) 
= (tH − tL) 

[ 
P(Sc = HR | R = b, Y = y1) 
− P(Sc = HR | R = w, Y = y2) 

] 
We will discus two immediate Corollaries of this 
result. 

Corollary 3.1 (Non-recidivators). Among individ- 
uals who do not recidivate, the difference in average 
penalty under the MinMax policy be 

∆ = (tH − tL)(FPRb − FPRw) (3.2) 

Corollary 3.2 (Recidivators). Among individual 
who recidivate, the difference in average penalty un- 
der the MinMax policy be 

∆ = (tH − tL)(FNRw − FNRb) (3.3) 

When use a test-fair RPI in population where 
recidivism prevalence differs across groups, it will 
generally be the case that the high recidivism 
prevalence group will have a high FPR and low 
FNR. From equation (3.2) and (3.3), we can see 
that this would result in great penalty for defen- 
dants in the high prevalence group, both among 
recidivate and non-recidivating offenders. 

An interest special case to consider be one where 
tL = 0. This could arise in sentence decision for 

3 



offender convict of low-severity crime who have 
good prior records. In such cases, so-called restora- 
tive sanction may be impose a an alternative to 
a period of incarceration. If we further take tH = 1, 
then ET = P(T 6= 0), which can be interpret a 
the probability that a defendant receives a sentence 
impose some period of incarceration. 

It’s easy to see that in such setting a non- 
recidivate defendant in group b be FPRb/FPRw 
time more likely to be incarcerate compare to 
a non-recidivating defendant in group w.5 This nat- 
urally raise the question of whether overall differ- 
ences in error rate be observe to persist across 
more granular subgroups. 

One might expect that difference in false posi- 
tive rate be largely attributable to the subset of 
defendant who be charge with more serious of- 
fen and who have a large number of prior ar- 
rests/convictions. While it be true that the false posi- 
tive rate within both racial group be high for de- 
fendants with bad criminal histories, considerable 
between-group difference in these error rate persist 
across low prior count subgroups. Figure 2 show 
a plot of false positive rate across different range 
of prior count for defendant charge with a misde- 
meanor offense, which be the low severity criminal 
offense category. As one can see, difference in false 
positive rate between Black defendant and White 
defendant persist across prior record subgroups. 

3.1 Connections to measure of effect size 

A natural question to ask be whether the level of dis- 
parate impact, ∆, be related to some measure of ef- 
fect size commonly use in scientific reporting. With 
a small generalization of the % non-overlap measure, 
we can answer this question in the affirmative. 

The % non-overlap of two distribution be gener- 
ally calculate assume both distribution be nor- 
mal, and thus have a one-to-one correspondence to 
Cohen’s d [12].6 Figure 3 show that the COMPAS 
decile score be far from be normally distributed. 
A more reasonable way to calculate % non-overlap 
be to note that in the Gaussian case % non-overlap 

5We be overload notation in this expression: Here, 
FPRr = P(HR | R = r, tL = 0), similarly for FNRr. 

6d = S̄b−S̄w 
SD 

, where SD be a pool estimate of standard 
deviation. 

0.00 

0.25 

0.50 

0.75 

1.00 

0 1−3 4−6 7−10 >10 
Number of prior 

Fa 
l 

e 
po 

si 
tiv 

e 
ra 

te 

race Black White 

Figure 2: False positive rate across prior record count 
for defendant charge with a Misdemeanor offense. Plot 
be base on assess a defendant a “high-risk” if their 
COMPAS decile score be > 4. Error bar represent 95% 
confidence intervals. 

0.0 

0.1 

0.2 

0.3 

1 2 3 4 5 6 7 8 9 10 
COMPAS decile score 

F 
re 

qu 
en 

cy 
race Black White 

Figure 3: COMPAS decile score histogram for Black 
and White defendants. Cohen’s d = 0.60, non-overlap 
dTV(fb, fw) = 24.5%. 

be equivalent to the total variation distance. Letting 
fr,y(s) denote the score distribution for race r and 
recidivism outcome y, one can establish the follow 
sharp bound on ∆. 

Proposition 3.2 (Percent overlap bound). Under 
the MinMax policy, 

∆ ≤ (tH − tL)dTV(fb,y, fw,y). 

4 Discussion 

The primary contribution of this paper be to show 
how disparate impact can result from the use of a 
recidivism prediction instrument that be know to 
be free from predictive bias. Our analysis focus 
on the simple set where a binary risk assessment 

4 



be use to inform a binary penalty policy. While 
all of the formula have natural analog in the non- 
binary score and penalty setting, we find that all of 
the salient feature be already present in the anal- 
ysis of the simpler binary-binary problem. 

Our analysis indicates that there be risk assess- 
ment use case in which it be desirable to balance 
error rate across different groups, even though this 
will generally result in risk assessment that be not 
free from predictive bias. However, balance er- 
ror rate overall may not be sufficient, a this do 
not guarantee balance at finer level of granularity. 
That is, even if FPRb = FPRw, we may still see 
difference in error rate within prior record score 
category (see e.g., Figure 2). One need to decide 
the level of granularity at which error rate balance 
be desirable to achieve. 

In closing, we would like to note that there be a 
large body of literature show that data-driven risk 
assessment instrument tend to be more accurate 
than professional human judgement [13, 14], and 
investigate whether human-driven decision be 
themselves prone to exhibit racial bias [15, 16]. 
We should not abandon the data-driven approach 
on the basis of negative headlines. Rather, we need 
to work to ensure that the instrument we use be 
demonstrably free from the kind of quantifiable bi- 
as that could lead to disparate impact in the spe- 
cific context in which they be to be applied. 

References 

[1] Thomas Blomberg, William Bales, Karen 
Mann, Ryan Meldrum, and Joe Nedelec. Vali- 
dation of the compas risk assessment classifica- 
tion instrument. 2010. 

[2] Ben Casselman Anna Maria Barry-Jester and 
Dana Goldstein. Should prison sentence be 
base on crime that haven’t be commit 
yet? 

[3] Northpointe. Compas risk & need assessment 
system: Selected question pose by inquire 
agencies. 

[4] Administrative Office of the United 
States Courts. An overview of the federal 
post conviction risk assessment, September 
2011. 

[5] Jay P Singh. Predictive validity performance in- 
dicators in violence risk assessment: A method- 
ological primer. Behavioral Sciences & the Law, 
31(1):8–22, 2013. 

[6] Jennifer L Skeem and Christopher T 
Lowenkamp. Risk, race, & recidivism: 
Predictive bias and disparate impact. Available 
at SSRN, 2015. 

[7] Jennifer L Skeem, John Monahan, and Christo- 
pher T Lowenkamp. Gender, risk assessment, 
and sanctioning: The cost of treat woman 
like men. Available at SSRN 2718460, 2016. 

[8] Julia Angwin, Jeff Larson, Surya Mattu, 
and Lauren Kirchner. Machine bias: 
There’s software use across the coun- 
try to predict future criminals. and it’s 
bias against blacks. 2016. URL 
https://www.propublica.org/article/ 

machine-bias-risk-assessments-in-criminal-sentencing. 

[9] Julia Angwin, Jeff Larson, Surya Mattu, 
and Lauren Kirchner. How we analyze the 
compas recidivism algorithm. 2016. URL 
https://www.propublica.org/article/ 

how-we-analyzed-the-compas-recidivism-algorithm. 

5 

https://www.propublica.org/article/machine- bias-risk-assessments-in-criminal-sentencing 
https://www.propublica.org/article/machine- bias-risk-assessments-in-criminal-sentencing 
https://www.propublica.org/article/how- we-analyzed-the-compas-recidivism-algorithm 
https://www.propublica.org/article/how- we-analyzed-the-compas-recidivism-algorithm 


[10] Anthony W Flores, Kristin Bechtel, and 
Christopher T Lowenkamp. False positives, 
false negatives, and false analyses: A rejoin- 
der to “machine bias: There’s software use 
across the country to predict future criminals. 
and it’s bias against blacks.”. Unpublished 
manuscript, 2016. 

[11] Jon Kleinberg, Sendhil Mullainathan, and Man- 
ish Raghavan. Inherent trade-off in the fair 
determination of risk scores. arXiv preprint 
arXiv:1609.05807, 2016. 

[12] Jacob Cohen. Statistical Power Analysis for the 
Behavioral Sciences (2nd Edition). Lawrence 
Erlbaum Associates, 1988. 

[13] Paul E Meehl. Clinical versus statistical predic- 
tion: A theoretical analysis and a review of the 
evidence. University of Minnesota Press, 1954. 

[14] William M Grove, David H Zald, Boyd S 
Lebow, Beth E Snitz, and Chad Nelson. Clinical 
versus mechanical prediction: a meta-analysis. 
Psychological assessment, 12(1):19, 2000. 

[15] Shamena Anwar and Hanming Fang. Testing 
for racial prejudice in the parole board release 
process: Theory and evidence. Technical report, 
National Bureau of Economic Research, 2012. 

[16] Laura T Sweeney and Craig Haney. The in- 
fluence of race on sentencing: A meta-analytic 
review of experimental studies. Behavioral Sci- 
ences & the Law, 10(2):179–195, 1992. 

6 


1 Introduction 
1.1 Data description and setup 

2 Assessing fairness 
2.1 Implied constraint on the false positive and false negative rate 

3 Assessing impact 
3.1 Connections to measure of effect size 

4 Discussion 

