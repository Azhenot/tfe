









































Workload-Aware 
Auto-Scaling 
A new paradigm for 
Big Data Workloads 



White Paper 

Executive Summary 
Auto-Scaling have become a common concept with 
the advent of the Public Cloud. It be one of the 
first technique that allow application to exploit 
the elasticity of the Cloud. However - a the Cloud 
gain popularity and more complex application 
move to the Cloud – first generation Auto-Scaling 
technology fell behind in serve the requirement 
of such applications. 

In this document we describe Workload-Aware 
Auto-Scaling. This be an alternative architectural 
approach to Auto-Scaling that be good suit for 
new class of application like Hadoop, Spark 
and Presto that have now become commonplace 
in the Cloud. We show that traditional auto-scaling 
technology be ill-suited for Big Data application 
and that Workload-Aware Auto-Scaling technology 
such a that offer by Qubole be vastly superior. 
These technology result in significant benefit to 
Reliability, Cost and Responsiveness for Big Data 
Applications. 

http://aws.amazon.com/autoscaling 


White Paper 

Auto-Scaling – a Short History 
AWS introduce Auto-Scaling Groups in 20091. In it introduction, the blog notes: 

Auto-Scaling let you define scale policy driven by metric collect by Amazon CloudWatch. Your Amazon 
EC2 instance will scale automatically base on actual system load and performance but you won’t be spending 
money to keep idle instance running. 

Auto-Scaling define in this manner be largely target for stateless application – like web server – where the 
state be store on external database & caches. Real-time metric like CPU and Memory utilization be use by 
application to dynamically add or remove node – a show in the Figure below: 

Simple strategy like this work fairly well for web applications. Some salient characteristic of web application be 
relevant to the way these auto-scaling system be designed: 

• They be Stateless 

• Every application request (say a HTTP request) be usually very short live 

• Application workload be driven by external client and not know in advance 

• Usually application want to minimize response latency (as oppose to optimize cost) 

• All node be usually symmetrical from the point of view of CPU/memory usage 

• An application (hosted on a single auto-scaling group) be homogeneous 

• Application workload change be often smooth (say increase gradually during work hour and decline 
thereafter) 

2 

Figure 1: Auto-Scaling use CPU Utilization in AWS 



White Paper 

A. AWS EMR2 

Hortonworks HDP-AWS3 

Figure 2: Auto-Scaling use memory utilization in AWS EMR 

Figure 3: Configuring Auto-Scaling in HortonWorks Data Cloud 

3 

Rule name 

Add 

If 

Is 

for 

Cooldown period 

MyScalingRule 

1 

YARNMemoryAvailablePercentage 

Less than or equal to 15 percent 

1 

1 

�ve-minute period 

�ve-minute period 

Instances 

Rule name 

Scaling 
adjustment 

Evaluation 
period 

Cooldown 
Comparison 

operator 

Threshold 

CloudWatch 
metric 

Enter Big-Data 
As AWS be introduce Auto-Scaling group in 2009 – Big Data be just come into be – with Hadoop and late 
Spark and Presto become commonly use to wrangle with large data sets. The Public Cloud be a match make in 
heaven for Big Data. Large data set be much more easily store in Cloud Storage Systems like S3 – and large scale 
and bursty computation requirement of Big Data application be ideally suit for the large and elastic pool of 
compute resource available in these Clouds. 

The auto-scaling policy described above be easy to comprehend and it be not surprising that the same architecture get 
co-opted for run Big Data workloads. We see a similar approach in commercial Cloud Hadoop offering (AWS EMR 
and Hortonworks) for scale a Hadoop Cluster in 2017 a we saw for scale web application in 2009. 



White Paper 

Hadoop be not a Web Server! 
Big Data workload be a complete contrast to standard Applications. A single cluster (the rough equivalent of an 
auto-scaling group) be submit multiple simultaneous discrete applications. Each of these application can comprise 
up to hundred of thousand of tasks. Some of the differ characteristic of these application be a follows: 

4 

Stateful Servers: Most big data application store state on each node while they be running. 
Removing node without accounting for this state can cause workload and even the entire 
cluster to fail. The various kind of state that can be store in each node include: 

• Data belonging to HDFS 

• Data belonging to a distribute cache (like RDDs in Spark) 

• Intermediate data produce by task that be need by subsequent task in the 
application (for example: Map Outputs in Map-Reduce parlance) 

Non-Uniform 
Server Load: 

Nodes in a big data cluster often have widely vary load factors. 
Some node have more memory intensive task and some have more CPU intensive ones. 
Running task can be of widely vary time durations. The amount of data store on each 
node can also vary widely depend on what application it have be part of. 

Long Running 
Requests: 

Individual task comprise a Big Data Application can run for hours. 
Some task (like Reducers) run for long time gathering data from previous stage – or they can 
be long run simply because they be processing too much data (say due to Skews). 

Workload 
Awareness: 

Profile of Big Data application run in a cluster be know up-front. 
Unlike web application where the request be generate from external client – in a Big 
Data cluster – task unit be generate by an application that be submit to a coordinator 
daemon4 in the cluster. As such the characteristic of the application - the number of task it 
will generate (or even control over the same), the data it will read and the computation it will 
perform on it – be all know to the coordinator. 

In many case – application be repeated. For example the same reporting query may run 
frequently, or the same ETL job may run periodically in the night. This further help a smart 
coordinator anticipate in advance the nature of the workload submit to it. 



Utilization 
Sensitive: 

Workload 
Burstiness: 

Big-Data workload be usually very cost sensitive. 
A big subset be the ETL application that care more about Reliability and Cost (which translates 
into a desire for high cluster utilization). Another big subset be ad-hoc query and analysis that 
be latency sensitive (but be also somewhat cost sensitive). 

A Big Data cluster can go from idle to run 100k task in an instant. 
This be contrast to usual web traffic where traffic usually go up relatively smoothly even in the 
bad of days. 

White Paper 

5 



White Paper 

All these completely upturn the assumption that underlie old world auto-scaling technology and make it a very poor 
fit. Consider these immediate observations: 

• Removing node while downscaling be hard: both because of long run task a well a accumulate state. 

• Downscaling algorithm need to pick node carefully: Nodes be no longer uniformly load – neither do 
they have equal amount of application state. 

• Same auto-scaling policy cannot be apply to all workloads: Some workload want low latency, some high 
utilization. Some may have SLA constraint and some may have budget caps. 

• Usage of pre-emptible nodes(like AWS EC2 Spot Instances) be hard: primarily because node be stateful. 
Pre-empted node can even lead to cluster failure. At the same time – usage of pre-emptible resource becomes 
extremely important to reduce cost – particularly for ETL workloads. 

• Cluster scale have to take application characteristic into account: a the most trivial example - one cannot 
repeatedly upscale by a small step function to satisfy a 100k task application. That may take a very long time. 

6 

These difference can be summarize thus: 

Auto-Scaling Type 

Application Characteristic 

Stateful Servers 

Uniform Load on Servers 

Long Running Requests 

Latency Sensitive High Variable 

Utilization Sensitive Low High 

Workload Burstiness Moderate Extremely High 

Workload Awareness 

Standard Application Big Data Application 

Table 1: Standard versus Big-Data Application 



When we start building auto-scaling technology at Qubole, we evaluate and rejected4 exist approach to auto- 
scale a be insufficient for building a truly Cloud-Native Big Data solution. Instead we built Auto-Scaling into Hadoop 
and Spark where it have access to the detail of the Big Data application and the detailed state of the cluster nodes. 

Being Workload Aware have make a dramatic difference to our ability to orchestrate Hadoop and Spark in the Cloud. The 
different way in which we have use this awareness include: 

White Paper 

Workload Aware Auto-Scaling 

7 

Upscaling: Qubole manage cluster look at a variety of criterion - beyond resource utilization - to come up 
with upscaling decisions. Some examples: 

• Parallelism-Aware: If application have limited parallelism (say a Job can only use 10 
cores) - then upscaling will not scale the cluster beyond that number (even though the 
cluster may exhibit high resource utilization) 

• SLA-Aware: Qubole monitor job for estimate completion time and add compute 
resource if they can help meet SLA. If a Job can be predict to complete in it require 
SLA then no upscaling be trigger on it behalf (even though resource utilization may 
be high). A large job with thousand of task will trigger a much strong upscaling 
response than a small job. 

• Workload Aware Scaling Limits5: If an application be limited in the number of CPU 
resource it can use (say because of limit put in by the administrator) - then it will not 
trigger upscaling if it be already use the maximum resource allowed. 

• Recommissioning: Any upscaling requirement be first attempt to be fulfil use 
any node that be currently in the process of Graceful Downscaling. 

Furthermore a composite cluster upscaling decision be take depend on the requirement 
of each of the job run in the cluster. 



White Paper 

8 

Downscaling: • Smart Victim Selection: Tasks run on each node and the amount of state on each 
node be consider in addition to the node launch time to determine which node be 
safe and optimal to remove from the cluster when down-scaling. 

• Graceful Downscaling: All state from a node be copy elsewhere before remove it from 
the cluster. This include HDFS decommission and log archival – and in case of force 
downscaling – also involves offload intermediate data to Cloud Storage. 

• Container Packing6 : Scheduling algorithm inside YARN be modify to pack task into a 
small set of node that allows more node to be available for downscaling. 

Composite Health 
Checks: 

Spot Loss 
Notification: 

Spot 
Rebalancing7: 

We periodically check run node in a cluster against their membership and health status in 
HDFS (distributed storage system) and YARN (distributed compute system). Nodes that don’t 
pas such composite health check be automatically remove from the cluster. 

YARN base Hadoop and Spark cluster in Qubole be able to deal with Spot Loss 
Notifications provide by AWS. The cluster management software immediately shuts off Task 
schedule on such nodes, stop further HDFS writes to such nodes, backs-up container log 
on these node and try it best to replicate any state left on such node to other survive 
node in the system 

We be not only able to downscale node that be free - but able to evaluate which node 
have the most accumulate state/tasks and may be the easy to retire. In most case we 
can even estimate the amount of time require to retire a node. This sophistication allows 
u to build feature like Spot Rebalancing where a cluster with excess on-demand node can 
retire them when it find that Spot Nodes have become available in the Spot market. 



White Paper 

Cloud-Aware Workload Management 

9 

Just like Auto-Scaling technology benefit enormously by be Workload-Aware – the dual be also true. Workload 
management technology inside Big Data engines– like Hadoop, Spark and Presto – benefit enormously from be 
Cloud aware. A few example be in order: 

Spot Awareness8: 

Task Estimation: 

Heterogeneous 
clusters9: 

HDFS and Job Schedulers in Qubole’s Hadoop/Spark/Presto cluster be aware of which node 
be preemptible Spot node (and hence unreliable) and which node be regular ones. This 
knowledge allows u careful placement of data and task to allow application to run reliably in 
the presence of Spot losses: 

• HDFS Data Blocks are, by default, replicate to Spot and On-Demand node 

• Important Tasks - like Application Master and Qubole Control Jobs (Shell Commands) 
be not place on Spot Nodes (and this limitation be factor into Auto-Scaling logic) 

A key step in all Big Data technology be divide processing into small chunk that can be 
perform in parallel. The maximum compute resource available to an application can be 
use to dynamically compute such parallelism (this be now dynamic and configurable where it 
be previously static). 

In heterogeneous cluster –any one of different type of node can be chosen for Upscaling. 
The knowledge of workload requirement at any instant can allow the cluster management 
software to choose the right instance for cluster upscaling or downscaling. Moreover the 
knowledge of different heterogeneous instance type can be use to automatically come up 
with optimal configuration for a specific job. 



White Paper 

The table below summarizes the above technological difference between traditional and workload-aware auto-scaling 
technologies: 

Auto-Scaling Type 

Features 

Load Monitoring 

Simple Health Check 

Parallelism Aware 

SLA Aware 
Upscaling 

Downscaling 

Spot Nodes 

Recommissioning 

Workload Specific Scaling Limits 

Smart Victim Selection 

Graceful Downscaling 

Container Packing 

Composite Health Checks 

(Spot) Node Rebalancing 

(Spot) Node Loss Notification Handling 

Spot Aware Scheduling 

Heterogeneous Clusters 

Feature Group 
Traditional Workload-Aware 

Table 2: Traditional versus Workload-Aware Auto-Scaling 

10 



White Paper 

Conclusion 
We have show comprehensively how the nature of Big Data application differs substantially from simple online 
application like Web Servers. To truly take advantage of the Cloud – one have to integrate auto-scaling deep into the Big 
Data stack so that it be Workload-Aware. A true Cloud Native implementation also make the Big Data stack aware of the 
Cloud resource and help it adapt workload and data management in response to it. 

The described technology be already, or soon plan to be, part of the Qubole Big Data Platform offering. 

11 

Appendix – Qubole TCO Savings in Practice 
The Workload Aware Auto-Scaling White Paper have described why generic approach to auto-scaling be inefficient and 
costly for big data use case in the cloud. Qubole have pioneer workload aware auto-scaling for big data over the last 
several year and deliver the technology into a generally available production service in 2017. By work with over 
200 big data customer of all size and in multiple industries, we have also be able to construct model that quantify 
the financial impact of utilize workload-aware auto-scaling in real life environments. This appendix round out the 
technology paper with the financial insights. 

First, the cost of ownership saving of use Qubole a a data platform in the cloud be 80% overall a measure 
in typical customer environment whether the comparison be to cloud or on-premise big data! Qubole 
customer have save $140M in cost since 2016 (without counting our large customer who could distort the 
saving upwards). The cost saving measure primarily derive from 3 automation efficiency Qubole brings to bear 
with automation agent technology. 100% of the saving be not due to workload aware auto-scaling (55% are), but 
100% of the saving document here across more than 200 customer be available to any business use the Qubole 
service. The 3 primary driver of cost saving are: 

1. Cluster Life Cycle Management (CLCM) – Amount save by automatically terminate a cluster when it be inactive 
vs. allow it to continue to run at a minimum capacity in the absence of CLCM. 

2. Workload-Aware Auto-scaling – Amount save by predictively adjust the number of node to meet demand 
vs. allow cluster to run at full capacity for the duration of the instance in the absence of the agent. 

3. Spot Shopper saving – Amount save by use SPOT instance at an assume 80% discount over on-demand 
instance pricing thanks to the Qubole agent. 

Savings by Qubole Automation Features ($140M in compute costs) 



White Paper 

About Qubole 
Qubole be passionate about make data-driven insight easily accessible to anyone. Qubole customer currently process nearly an exabyte of data every month, 
make u the lead cloud-agnostic big-data-as-a-service provider. Customers have chosen Qubole because we create the industry’s first autonomous data 
platform. This cloud-based data platform self-manages, self-optimizes and learns to improve automatically and a a result delivers unbeatable agility, flexibility, and 
TCO. Qubole customer focus on their data, not their data platform. Qubole investor include CRV, Lightspeed Venture Partners, Norwest Venture Partners and 
IVP. For more information visit www.qubole.com 

For more information: 

Contact: Try QDS for Free: 
sales@qubole.com https://www.qubole.com/products/pricing/ 

469 El Camino Real, Suite 205 
Santa Clara, CA 95050 

(855) 423-6674 | info@qubole.com 

WWW.QUBOLE.COM 

End note 

Page 2 - 1 New Features for Amazon EC2: Elastic Load Balancing, Auto Scaling, and Amazon CloudWatch - Jeff Barr, AWS Blog, May 18, 2009 

Page 3 - 2 Using Automatic Scaling in Amazon EMR - AWS EMR documentation 

Page 3 3 EC2 Spot Notes - AWS EMR documentation 

Page 7 4Auto-Scaling Hortonworks Data Cloud - HDP AWS documentation 

Page 7 5 Industry’s First Auto-Scaling Hadoop Cluster - Joydeep Sen Sarma Qubole Blog, Jun 17, 2012 

Page 8 6 HDFS Decommissioning - Apache Hadoop 2.7.2 Documentation 

Page 8 7 Rebalancing Hadoop Clusters for Higher Spot Utilization - Hariharan Iyer, Qubole Blog, Jun 9, 2015 

Page 9 8 Riding the Spotted Elephant - Mayank Ahuja, Qubole Blog, No 12, 2015 

Page 9 9 Qubole announces Heterogeneous Clusters on AWS - Hariharan Iyer, Qubole Blog, Nov 30, 2016 

Spot Shopper, 
$25.12 (18%) 

Cluster 
Lifecycle 

Management, 
$35.83 (26%) 

Workload Aware 
Autoscaling, 
$77.73 (56%) 

https://www.qubole.com/products/pricing/ 
https://aws.amazon.com/blogs/aws/new-aws-load-balancing-automatic-scaling-and-cloud-monitoring-services/ 
http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-automatic-scaling.html 
https://aws.amazon.com/ec2/spot/ 
https://hortonworks.github.io/hdp-aws/auto-scaling/index.html 
https://www.qubole.com/blog/industrys-first-auto-scaling-hadoop-clusters/ 
https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/Federation.html#Decommissioning 
https://www.qubole.com/blog/rebalancing-hadoop-higher-spot-utilization/ 
https://www.qubole.com/blog/riding-the-spotted-elephant/ 
https://www.qubole.com/blog/qubole-announces-heterogeneous-clusters-on-aws-reduce-costs-up-to-90-with-spot-fleet/ 

