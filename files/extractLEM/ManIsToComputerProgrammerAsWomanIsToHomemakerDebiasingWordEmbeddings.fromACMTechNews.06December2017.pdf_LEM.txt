






































































Man be to Computer Programmer a Woman be to Homemaker? Debiasing Word Embeddings 


Man be to Computer Programmer a Woman be to 
Homemaker? Debiasing Word Embeddings 

Tolga Bolukbasi1, Kai-Wei Chang2, James Zou2, Venkatesh Saligrama1,2, Adam Kalai2 
1Boston University, 8 Saint Mary’s Street, Boston, MA 

2Microsoft Research New England, 1 Memorial Drive, Cambridge, MA 
tolgab@bu.edu, kw@kwchang.net, jamesyzou@gmail.com, srv@bu.edu, adam.kalai@microsoft.com 

Abstract 
The blind application of machine learn run the risk of amplify bias present 
in data. Such a danger be face u with word embedding, a popular framework to 
represent text data a vector which have be use in many machine learn and 
natural language processing tasks. We show that even word embeddings train on 
Google News article exhibit female/male gender stereotype to a disturb extent. 
This raise concern because their widespread use, a we describe, often tends to 
amplify these biases. Geometrically, gender bias be first show to be capture by 
a direction in the word embedding. Second, gender neutral word be show to 
be linearly separable from gender definition word in the word embedding. Using 
these properties, we provide a methodology for modify an embed to remove 
gender stereotypes, such a the association between the word receptionist and 
female, while maintain desire association such a between the word queen 
and female. Using crowd-worker evaluation a well a standard benchmarks, we 
empirically demonstrate that our algorithm significantly reduce gender bias in 
embeddings while preserve the it useful property such a the ability to cluster 
related concept and to solve analogy tasks. The result embeddings can be use 
in application without amplify gender bias. 

1 Introduction 
Research on word embeddings have drawn significant interest in machine learn and natural language 
processing. There have be hundred of paper write about word embeddings and their applications, 
from Web search [22] to parse Curriculum Vitae [12]. However, none of these paper have 
recognize how blatantly sexist the embeddings be and hence risk introduce bias of various 
type into real-world systems. 

A word embedding, train on word co-occurrence in text corpora, represent each word (or common 
phrase) w a a d-dimensional word vector ~w 2 Rd. It serf a a dictionary of sort for computer 
program that would like to use word meaning. First, word with similar semantic meaning tend to 
have vector that be close together. Second, the vector difference between word in embeddings 
have be show to represent relationship between word [27, 21]. For example give an analogy 
puzzle, “man be to king a woman be to x” (denoted a man:king :: woman:x), simple arithmetic of 
the embed vector find that x=queen be the best answer because ��!man �����!woman ⇡ ��!king ����!queen. 
Similarly, x=Japan be return for Paris:France :: Tokyo:x. It be surprising that a simple vector 
arithmetic can simultaneously capture a variety of relationships. It have also excite practitioner 
because such a tool could be useful across application involve natural language. Indeed, they 
be be study and use in a variety of downstream application (e.g., document rank [22], 
sentiment analysis [14], and question retrieval [17]). 

However, the embeddings also pinpoint sexism implicit in text. For instance, it be also the case that: 
��!man �����!woman ⇡ ���������������!computer programmer ��������!homemaker. 

In other words, the same system that solve the above reasonable analogy will offensively answer 
“man be to computer programmer a woman be to x” with x=homemaker. Similarly, it output that a 

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. 



Extreme she 
1. homemaker 
2. nurse 
3. receptionist 
4. librarian 
5. socialite 
6. hairdresser 
7. nanny 
8. bookkeeper 
9. stylist 
10. housekeeper 

Extreme he 
1. maestro 
2. skipper 
3. protege 
4. philosopher 
5. captain 
6. architect 
7. financier 
8. warrior 
9. broadcaster 
10. magician 

Gender stereotype she-he analogy 
sewing-carpentry register nurse-physician housewife-shopkeeper 
nurse-surgeon interior designer-architect softball-baseball 
blond-burly feminism-conservatism cosmetics-pharmaceuticals 
giggle-chuckle vocalist-guitarist petite-lanky 
sassy-snappy diva-superstar charming-affable 
volleyball-football cupcakes-pizzas lovely-brilliant 

Gender appropriate she-he analogy 
queen-king sister-brother mother-father 
waitress-waiter ovarian cancer-prostate cancer convent-monastery 

Figure 1: Left The most extreme occupation a project on to the she�he gender direction on 
w2vNEWS. Occupations such a businesswoman, where gender be suggest by the orthography, 
be excluded. Right Automatically generate analogy for the pair she-he use the procedure 
described in text. Each automatically generate analogy be evaluate by 10 crowd-workers to whether 
or not it reflect gender stereotype. 

father be to a doctor a a mother be to a nurse. The primary embed study in this paper be the 
popular publicly-available word2vec [19, 20] 300 dimensional embed train on a corpus of 
Google News text consist of 3 million English words, which we refer to here a the w2vNEWS. 
One might have hop that the Google News embed would exhibit little gender bias because 
many of it author be professional journalists. We also analyze other publicly available embeddings 
train via other algorithm and find similar bias (Appendix B). 

In this paper, we quantitatively demonstrate that word-embeddings contain bias in their geometry 
that reflect gender stereotype present in broader society.1 Due to their wide-spread usage a basic 
features, word embeddings not only reflect such stereotype but can also amplify them. This pose a 
significant risk and challenge for machine learn and it applications. The analogy generate from 
these embeddings spell out the bias implicit in the data on which they be trained. Hence, word 
embeddings may serve a a mean to extract implicit gender association from a large text corpus 
similar to how Implicit Association Tests [11] detect automatic gender association possess by 
people, which often do not align with self reports. 

To quantify bias, we will compare a word vector to the vector of a pair of gender-specific words. For 
instance, the fact that ���!nurse be close to ����!woman be not in itself necessarily biased(it be also somewhat 
close to ��!man – all be humans), but the fact that these distance be unequal suggests bias. To make 
this rigorous, consider the distinction between gender specific word that be associate with a gender 
by definition, and the remain gender neutral words. Standard example of gender specific word 
include brother, sister, businessman and businesswoman. We will use the gender specific word to 
learn a gender subspace in the embedding, and our debiasing algorithm remove the bias only from 
the gender neutral word while respect the definition of these gender specific words. 

We propose approach to reduce gender bias in the word embed while preserve the useful 
property of the embedding. Surprisingly, not only do the embed capture bias, but it also 
contains sufficient information to reduce this bias.We will leverage the fact that there exists a low 
dimensional subspace in the embed that empirically capture much of the gender bias. 

2 Related work and Preliminary 
Gender bias and stereotype in English. It be important to quantify and understand bias in language 
a such bias can reinforce the psychological status of different group [28]. Gender bias in language 
have be study over a number of decade in a variety of context (see, e.g., [13]) and we only 
highlight some of the finding here. Biases differ across people though commonality can be detected. 
Implicit Association Tests [11] have uncovered gender-word bias that people do not self-report and 
may not even be aware of. Common bias link female term with liberal art and family and male 
term with science and career [23]. Bias be see in word morphology, i.e., the fact that word such a 

1 Stereotypes be bias that be widely held among a group of people. We show that the bias in the word 
embed be in fact closely align with social conception of gender stereotype, a evaluate by U.S.-based 
crowd worker on Amazon’s Mechanical Turk. The crowd agree that the bias reflect both in the location of 
vector (e.g. 

���! 
doctor closer to ��!man than to ����!woman) a well a in analogy (e.g., he:coward :: she:whore.) exhibit 

common gender stereotypes. 

2 



actor are, by default, associate with the dominant class [15], and female version of these words, 
e.g., actress, be marked. There be also an imbalance in the number of word with F-M with various 
associations. For instance, while there be more word refer to males, there be many more word 
that sexualize female than male [30]. Consistent bias have be study within online context 
and specifically related to the context we study such a online news (e.g., [26]), Web search (e.g., 
[16]), and Wikipedia (e.g., [34]). 

Bias within algorithms. A number of online system have be show to exhibit various biases, 
such a racial discrimination and gender bias in the ad present to user [31, 4]. A recent study 
found that algorithm use to predict repeat offender exhibit indirect racial bias [1]. Different 
demographic and geographic group also use different dialect and word-choices in social medium 
[6]. An implication of this effect be that language use by minority group might not be able to be 
process by natural language tool that be train on “standard” data-sets. Biases in the curation of 
machine learn data-sets have explore in [32, 3]. 

Independent from our work, Schmidt [29] identify the bias present in word embeddings and 
propose debiasing by entirely remove multiple gender dimensions, one for each gender pair. His 
goal and approach, similar but simpler than ours, be to entirely remove gender from the embedding. 
There be also an intense research agenda focus on improve the quality of word embeddings from 
different angle (e.g., [18, 25, 35, 7]), and the difficulty of evaluate embed quality (as compare 
to supervise learning) parallel the difficulty of define bias in an embedding. 

Within machine learning, a body of notable work have focus on “fair” binary classification in 
particular. A definition of fairness base on legal tradition be present by Barocas and Selbst [2]. 
Approaches to modify classification algorithm to define and achieve various notion of fairness 
have be described in a number of works, see, e.g., [2, 5, 8] and a recent survey [36]. The prior 
work on algorithmic fairness be largely for supervise learning. Fair classification be define base 
on the fact that algorithm be classify a set of individual use a set of feature with a 
distinguish sensitive feature. In word embeddings, there be no clear individual and no a priori 
define classification problem. However, similar issue arise, such a direct and indirect bias [24]. 

Word embedding. An embed consists of a unit vector ~w 2 Rd, with k~wk = 1, for each word 
(or term) w 2 W . We assume there be a set of gender neutral word N ⇢ W , such a flight attendant 
or shoes, which, by definition, be not specific to any gender. We denote the size of a set S by |S|. We 
also assume we be give a set of F-M gender pair P ⇢ W ⇥W , such a she-he or mother-father 
whose definition differ mainly in gender. Section 5 discus how N and P can be found within 
the embed itself, but until then we take them a given. As be common, similarity between two 
vector u and v can be measure by their cosine similarity : cos(u, v) = u·vkukkvk . This normalize 
similarity between vector u and v be the cosine of the angle between the two vectors. Since word 
be normalize cos(~w1, ~w2) = ~w1 · ~w2.2 

Unless otherwise stated, the embed we refer to be the aforementioned w2vNEWS embedding, a 
d = 300-dimensional word2vec [19, 20] embedding, which have proven to be immensely useful since 
it be high quality, publicly available, and easy to incorporate into any application. In particular, we 
download the pre-trained embed on the Google News corpus,3 and normalize each word to 
unit length a be common. Starting with the 50,000 most frequent words, we select only lower-case 
word and phrase consist of few than 20 lower-case character (words with upper-case letters, 
digits, or punctuation be discarded). After this filtering, 26,377 word remained. While we focus 
on w2vNEWS, we show late that gender stereotype be also present in other embed data-sets. 
Crowd experiments.4 Two type of experiment be performed: one where we solicit word 
from the crowd (to see if the embed bias contain those of the crowd) and one where we 
solicit rating on word or analogy generate from our embed (to see if the crowd’s bias 
contain those from the embedding). These two type of experiment be analogous to experiment 
perform in rating result in information retrieval to evaluate precision and recall. When we speak 
of the majority of 10 crowd judgments, we mean those annotation make by 5 or more independent 
workers. The Appendix contains the questionnaire that be give to the crowd-workers. 

2We will abuse terminology and refer to the embed of a word and the word interchangeably. For example, 
the statement cat be more similar to dog than to cow mean �!cat ·�!dog � �!cat ·��!cow. 

3 
https://code.google.com/archive/p/word2vec/ 

4All human experiment be perform on the Amazon Mechanical Turk platform. We select for 
U.S.-based worker to maintain homogeneity and reproducibility to the extent possible with crowdsourcing. 

3 



3 Geometry of Gender and Bias in Word Embeddings 
Our first task be to understand the bias present in the word-embedding (i.e. which word be closer 
to she than to he, etc.) and the extent to which these geometric bias agree with human notion of 
gender stereotypes. We use two simple method to approach this problem: 1) evaluate whether the 
embed have stereotype on occupation word and 2) evaluate whether the embed produce 
analogy that be judged to reflect stereotype by humans. The exploratory analysis of this section 
will motivate the more rigorous metric use in the next two sections. 

Occupational stereotypes. Figure 1 list the occupation that be closest to she and to he in the 
w2vNEWS embeddings. We ask the crowdworkers to evaluate whether an occupation be consider 
female-stereotypic, male-stereotypic, or neutral. The projection of the occupation word onto the she- 
he axis be strongly correlate with the stereotypicality estimate of these word (Spearman ⇢ = 0.51), 
suggest that the geometric bias of embed vector be align with crowd judgment. We 
project each of the occupation onto the she-he direction in the w2vNEWS embed a well a a 
different embed generate by the GloVe algorithm on a web-crawl corpus [25]. The result be 
highly consistent (Appendix Figure 6), suggest that gender stereotype be prevalent across different 
embeddings and be not an artifact of the particular training corpus or methodology of word2vec. 

Analogies exhibit stereotypes. Analogies be a useful way to both evaluate the quality of a word 
embed and also it stereotypes. We first briefly describe how the embed generate analogy 
and then discus how we use analogy to quantify gender stereotype in the embedding. A more 
detailed discussion of our algorithm and prior analogy solver be give in Appendix C. 

In the standard analogy tasks, we be give three words, for example he, she, king, and look for the 
4th word to solve he to king be a she to x. Here we modify the analogy task so that give two words, 
e.g. he, she, we want to generate a pair of words, x and y, such that he to x a she to y be a good 
analogy. This modification allows u to systematically generate pair of word that the embed 
believe it analogous to he, she (or any other pair of seed words). The input into our analogy generator 
be a seed pair of word (a, b) determine a seed direction ~a �~b correspond to the normalize 
difference between the two seed words. In the task below, we use (a, b) = (she, he). We then score 
all pair of word x, y by the follow metric: 

S(a,b)(x, y) = co 
⇣ 
~a�~b, ~x� ~y 

⌘ 
if k~x� ~yk  �, 0 else (1) 

where � be a threshold for similarity. The intuition of the score metric be that we want a good 
analogy pair to be close to parallel to the seed direction while the two word be not too far apart in 
order to be semantically coherent. The parameter � set the threshold for semantic similarity. In all 
the experiments, we take � = 1 a we find that this choice often work well in practice. Since all 
embeddings be normalized, this threshold corresponds to an angle  ⇡/3, indicate that the two 
word be closer to each other than they be to the origin. In practice, it mean that the two word 
form the analogy be significantly closer together than two random embed vectors. Given the 
embed and seed words, we output the top analogous pair with the large positive S(a,b) scores. 
To reduce redundancy, we do not output multiple analogy share the same word x. 

We employ U.S. base crowd-workers to evaluate the analogy output by the aforementioned 
algorithm. For each analogy, we ask the worker two yes/no questions: (a) whether the pair 
make sense a an analogy, and (b) whether it reflect a gender stereotype. Overall, 72 out of 150 
analogy be rat a gender-appropriate by five or more out of 10 crowd-workers, and 29 analogy 
be rat a exhibit gender stereotype by five or more crowd-workers (Figure 4). Examples of 
analogy generate from w2vNEWS be show at Figure 1. The full list be in Appendix J. 

Identifying the gender subspace. Next, we study the bias present in the embed geometrically, 
identify the gender direction and quantify the bias independent of the extent to which it be 
align with the crowd bias. Language use be “messy” and therefore individual word pair do not 
always behave a expected. For instance, the word man have several different usages: it may be use 
a an exclamation a in oh man! or to refer to people of either gender or a a verb, e.g., man the 
station. To more robustly estimate bias, we shall aggregate across multiple pair comparisons. By 
combine several directions, such a 

�! 
she ��!he and ����!woman ���!man, we identify a gender direction 

g 2 Rd that largely capture gender in the embedding. This direction help u to quantify direct and 
indirect bias in word and associations. 

In English a in many languages, there be numerous gender pair terms, and for each we can 
consider the difference between their embeddings. Before look at the data, one might imagine 

4 



def. stereo. 
�! 
she��!he 92% 89% 
�! 
her��!his 84% 87% 

����!woman���!man 90% 83% 
���! 
Mary���!John 75% 87% 

����! 
herself�����!himself 93% 89% 

�����! 
daughter��!son 93% 91% 
����! 
mother����!father 91% 85% 

�! 
gal��!guy 85% 85% 
�! 
girl��!boy 90% 86% 

����! 
female���!male 84% 75% 

RG WS analogy 

Before 62.3 54.5 57.0 
Hard-debiased 62.4 54.1 57.0 
Soft-debiased 62.4 54.2 56.8 

Figure 2: Left: Ten word pair to define gender, along with agreement with set of definitional 
and stereotypical word solicit from the crowd. The accuracy be show for the correspond 
gender classifier base on which word be closer to a target word, e.g., the she-he classifier predicts a 
word be female if it be closer to she than he. Middle: The bar plot show the percentage of variance 
explain in the PCA of the 10 pair of gender words. The top component explains significantly more 
variance than any other; the correspond percentage for random word show a more gradual decay 
(Figure create by average over 1,000 draw of ten random unit vector in 300 dimensions). Right: 
The table show performance of the original w2vNEWS embed (“before”) and the debiased 
w2vNEWS on standard evaluation metric measure coherence and analogy-solving abilities: RG 
[27], WS [10], MSR-analogy [21]. Higher be better. The result show that the performance do not 
degrade after debiasing. Note that we use a subset of vocabulary in the experiments. Therefore, the 
performance be low than the previously publish results. See Appendix for full results. 

that they all have roughly the same vector differences, a in the follow caricature: 
��������! 
grandmother =��! 

wise+ 
�! 
gal, 

�������! 
grandfather = 

��! 
wise+�!guy,��������!grandmother��������!grandfather = �!gal��!guy = g However, gender 

pair difference be not parallel in practice, for multiple reasons. First, there be different bias 
associate with with different gender pairs. Second be polysemy, a mentioned, which in this case 
occurs due to the other use of grandfather a in to grandfather a regulation. Finally, randomness in 
the word count in any finite sample will also lead to differences. Figure 2 illustrates ten possible 
gender pairs, 

� 
(x 

i 

, y 

i 

) 

10 
i=1 

. 

To identify the gender subspace, we take the ten gender pair difference vector and compute it 
principal component (PCs). As Figure 2 shows, there be a single direction that explains the majority 
of variance in these vectors. The first eigenvalue be significantly large than the rest. Note that, 
from the randomness in a finite sample of ten noisy vectors, one expect a decrease in eigenvalues. 
However, a also illustrate in 2, the decrease one observes due to random sample be much more 
gradual and uniform. Therefore we hypothesize that the top PC, denote by the unit vector g, capture 
the gender subspace. In general, the gender subspace could be high dimensional and all of our 
analysis and algorithm (described below) work with general subspaces. 

Direct bias. To measure direct bias, we first identify word that should be gender-neutral for the 
application in question. How to generate this set of gender-neutral word be described in Section 5. 
Given the gender neutral words, denote by N , and the gender direction learn from above, g, we 
define the direct gender bias of an embed to be 1|N | 

P 
w2N |cos(~w, g)| 

c, where c be a parameter 
that determines how strict do we want to in measure bias. If c be 0, then |cos(~w � g)|c = 0 
only if ~w have no overlap with g and otherwise it be 1. Such strict measurement of bias might be 
desirable in setting such a the college admission example from the Introduction, where it would 
be unacceptable for the embed to introduce a slight preference for one candidate over another 
by gender. A more gradual bias would be set c = 1. The presentation we have chosen favor 
simplicity – it would be natural to extend our definition to weight word by frequency. For example, 
in w2vNEWS, if we take N to be the set of 327 occupations, then DirectBias1 = 0.08, which 
confirms that many occupation word have substantial component along the gender direction. 

4 Debiasing algorithm 
The debiasing algorithm be define in term of set of word rather than just pairs, for generality, so 
that we can consider other bias such a racial or religious biases. We also assume that we have a set 
of word to neutralize, which can come from a list or from the embed a described in Section 5. 
(In many case it may be easy to list the gender specific word not to neutralize a this set can be 
much smaller.) 

5 



bias 

okay 

he 

Figure 3: Selected word project along two axes: x be a projection onto the difference between 
the embeddings of the word he and she, and y be a direction learn in the embed that capture 
gender neutrality, with gender neutral word above the line and gender specific word below the line. 
Our hard debiasing algorithm remove the gender pair association for gender neutral words. In this 
figure, the word above the horizontal line would all be collapse to the vertical line. 
The first step, call Identify gender subspace, be to identify a direction (or, more generally, a 
subspace) of the embed that capture the bias. For the second step, we define two options: 
Neutralize and Equalize or Soften. Neutralize ensures that gender neutral word be zero in the 
gender subspace. Equalize perfectly equalizes set of word outside the subspace and thereby 
enforces the property that any neutral word be equidistant to all word in each equality set. For 
instance, if {grandmother, grandfather} and {guy, gal} be two equality sets, then after equalization 
babysit would be equidistant to grandmother and grandfather and also equidistant to gal and guy, 
but presumably closer to the grandparent and further from the gal and guy. This be suitable for 
application where one do not want any such pair to display any bias with respect to neutral words. 

The disadvantage of Equalize be that it remove certain distinction that be valuable in certain 
applications. For instance, one may wish a language model to assign a high probability to the phrase 
to grandfather a regulation) than to grandmother a regulation since grandfather have a meaning that 
grandmother do not – equalize the two remove this distinction. The Soften algorithm reduces 
the difference between these set while maintain a much similarity to the original embed a 
possible, with a parameter that control this trade-off. 

To define the algorithms, it will be convenient to introduce some further notation. A subspace B be 
define by k orthogonal unit vector B = {b1, . . . , bk} ⇢ Rd. In the case k = 1, the subspace be 
simply a direction. We denote the projection of a vector v onto B by, v 

B 

= 

P 
k 

j=1(v · bj)bj . This 
also mean that v � v 

B 

be the projection onto the orthogonal subspace. 

Step 1: Identify gender subspace. Inputs: word set W , define set D1, D2, . . . , Dn ⇢ W 
a well a embed 

� 
~w 2 Rd 


w2W and integer parameter k � 1. Let µi := 

P 
w2Di ~w/|Di| 

be the mean of the define sets. Let the bias subspace B be the first k row of SVD(C) where 
C := 

P 
n 

i=1 

P 
w2Di(~w � µi) 

T 

(~w � µ 
i 

) 

� 
|D 

i 

|. 
Step 2a: Hard de-biasing (neutralize and equalize). Additional inputs: word to neutralize 
N ✓ W , family of equality set E = {E1, E2, . . . , Em} where each Ei ✓ W . For each word 
w 2 N , let ~w be re-embedded to ~w := (~w � ~w 

B 

) 

� 
k~w � ~w 

B 

k. For each set E 2 E , let 

µ := 

P 
w2E w/|E| and ⌫ := µ� µB . For each w 2 E, ~w := ⌫ + 

p 
1� k⌫k2 ~wB�µBk~wB�µBk . Finally, 

output the subspace B and the new embed 
� 
~w 2 Rd 


w2W . 

Equalize equates each set of word outside of B to their simple average ⌫ and then adjusts vector 
so that they be unit length. It be perhaps easy to understand by think separately of the two 
component ~w 

B 

and ~w?B = ~w� ~wB . The latter ~w?B be all simply equate to their average. Within 
B, they be center (moved to mean 0) and then scale so that each ~w be unit length. To motivate 
why we center, beyond the fact that it be common in machine learning, consider the bias direction 
be the gender direction (k = 1) and a gender pair such a E = {male, female}. As discussed, it 

6 



Figure 4: Number of stereotypical (Left) and appropriate (Right) analogy generate by word 
embeddings before and after debiasing. 

so happens that both word be positive (female) in the gender direction, though female have a great 
projection. One can only speculate a to why this be the case, e.g., perhaps the frequency of text 
such a male nurse or male escort or she be assault by the male. However, because female have a 
great gender component, after center the two will be symmetrically balance across the origin. 
If instead, we simply scale each vector’s component in the bias direciton without centering, male 
and female would have exactly the same embed and we would lose analogy such a father:male 
:: mother:female. We note that Neutralizing and Equalizing completely remove pair bias. 

Observation 1. After Steps 1 and 2a, for any gender neutral word w any equality set E, and any two 
word e1, e2 2 E, ~w·~e1 = w·~e2 and k~w�~e1k = k~w�~e2k. Furthermore, if E = 

� 
{x, y}|(x, y) 2 P 



be the set of pair define PairBias, then PairBias = 0. 

Step 2b: Soft bias correction. Overloading the notation, we let W 2 Rd⇥|vocab| denote the matrix 
of all embed vector and N denote the matrix of the embed vector correspond to gender 
neutral words. W and N be learn from some corpus and be input to the algorithm. The 
desire debiasing transformation T 2 Rd⇥d be a linear transformation that seek to preserve pairwise 
inner product between all the word vector while minimize the projection of the gender neutral 
word onto the gender subspace. This can be formalize a min 

T 

k(TW )T (TW ) � WTWk2 
F 

+ 

�k(TN)T (TB)k2 
F 

, where B be the gender subspace learn in Step 1 and � be a tune parameter 
that balance the objective of preserve the original embed inner product with the goal of 
reduce gender bias. For � large, T would remove the projection onto B from all the vector in N , 
which corresponds exactly to Step 2a. In the experiment, we use � = 0.2. The optimization problem 
be a semi-definite program and can be solve efficiently. The output embed be normalize to have 
unit length, ˆW = {Tw/kTwk2, w 2 W}. 

5 Determining gender neutral word 
For practical purposes, since there be many few gender specific words, it be more efficient to 
enumerate the set of gender specific word S and take the gender neutral word to be the compliment, 
N = W \ S. Using dictionary definitions, we derive a subset S0 of 218 word out of the word in 
w2vNEWS. Recall that this embed be a subset of 26,377 word out of the full 3 million word 
in the embedding, a described in Section 2. This base list S0 be give in Appendix F. Note that the 
choice of word be subjective and ideally should be customize to the application at hand. 

We generalize this list to the entire 3 million word in the Google News embed use a linear 
classifier, result in the set S of 6,449 gender-specific words. More specifically, we train a linear 
Support Vector Machine (SVM) with regularization parameter of C = 1.0. We then ran this classifier 
on the remain words, take S = S0 [ S1, where S1 be the word label a gender specific by 
our classifier among the word in the entire embed that be not in the 26,377 word of w2vNEWS. 
Using 10-fold cross-validation to evaluate the accuracy, we find an F -score of .627± .102. 
Figure 3 illustrates the result of the classifier for separate gender-specific word from gender- 
neutral words. To make the figure legible, we show a subset of the words. The x-axis correspond to 
projection of word onto the 

�! 
she ��!he direction and the y-axis corresponds to the distance from the 

decision boundary of the train SVM. 

7 



6 Debiasing result 
We evaluate our debiasing algorithm to ensure that they preserve the desirable property of the 
original embed while reduce both direct and indirect gender biases. First we use the same 
analogy generation task a before: for both the hard-debiased and the soft-debiased embeddings, 
we automatically generate pair of word that be analogous to she-he and ask crowd-workers 
to evaluate whether these pair reflect gender stereotypes. Figure 4 show the results. On the initial 
w2vNEWS embedding, 19% of the top 150 analogy be judged a show gender stereotype 
by a majority of the ten workers. After apply our hard debiasing algorithm, only 6% of the new 
embed be judged a stereotypical. 

As an example, consider the analogy puzzle, he to doctor be a she to X . The original embed 
return X = nurse while the hard-debiased embed find X = physician. Moreover the hard- 
debiasing algorithm preserve gender appropriate analogy such a she to ovarian cancer be a he 
to prostate cancer. This demonstrates that the hard-debiasing have effectively reduce the gender 
stereotype in the word embedding. Figure 4 also show that the number of appropriate analogy 
remains similar a in the original embed after execute hard-debiasing. This demonstrates that 
that the quality of the embeddings be preserved. The detail result be in Appendix J. Soft-debiasing 
be less effective in remove gender bias. To further confirms the quality of embeddings after 
debiasing, we test the debiased embed on several standard benchmark that measure whether 
related word have similar embeddings a well a how well the embed performs in analogy tasks. 
Appendix Table 2 show the result on the original and the new embeddings and the transformation 
do not negatively impact the performance. In Appendix A, we show how our algorithm also reduces 
indirect gender bias. 

7 Discussion 
Word embeddings help u further our understand of bias in language. We find a single direction 
that largely capture gender, that help u capture association between gender neutral word and 
gender a well a indirect inequality. The projection of gender neutral word on this direction enables 
u to quantify their degree of female- or male-bias. 

To reduce the bias in an embedding, we change the embeddings of gender neutral words, by remove 
their gender associations. For instance, nurse be move to to be equally male and female in the 
direction g. In addition, we find that gender-specific word have additional bias beyond g. For 
instance, grandmother and grandfather be both closer to wisdom than gal and guy are, which do not 
reflect a gender difference. On the other hand, the fact that babysit be so much closer to grandmother 
than grandfather (more than for other gender pairs) be a gender bias specific to grandmother. By 
equate grandmother and grandfather outside of gender, and since we’ve remove g from babysit, 
both grandmother and grandfather and equally close to babysit after debiasing. By retain the 
gender component for gender-specific words, we maintain analogy such a she:grandmother 
:: he:grandfather. Through empirical evaluations, we show that our hard-debiasing algorithm 
significantly reduces both direct and indirect gender bias while preserve the utility of the embedding. 
We have also developed a soft-embedding algorithm which balance reduce bias with preserve 
the original distances, and could be appropriate in specific settings. 

One perspective on bias in word embeddings be that it merely reflect bias in society, and therefore 
one should attempt to debias society rather than word embeddings. However, by reduce the bias in 
today’s computer system (or at least not amplify the bias), which be increasingly reliant on word 
embeddings, in a small way debiased word embeddings can hopefully contribute to reduce gender 
bias in society. At the very least, machine learn should not be use to inadvertently amplify these 
biases, a we have see can naturally happen. 

In specific applications, one might argue that gender bias in the embed (e.g. computer 
programmer be closer to he) could capture useful statistic and that, in these special cases, the original 
bias embeddings could be used. However give the potential risk of have machine learn 
algorithm that amplify gender stereotype and discriminations, we recommend that we should err on 
the side of neutrality and use the debiased embeddings provide here a much a possible. 

Acknowledgments. The author thank Tarleton Gillespie and Nancy Baym for numerous helpful 
discussions.5 

5 This material be base upon work support in part by NSF Grants CNS-1330008, CCF-1527618, by ONR 
Grant 50202168, NGA Grant HM1582-09-1-0037 and DHS 2013-ST-061-ED0001 

8 



References 
[1] J. Angwin, J. Larson, S. Mattu, and L. Kirchner. Machine bias: There’s software use across the country 

to predict future criminals. and it’s bias against blacks., 2016. 
[2] S. Barocas and A. D. Selbst. Big data’s disparate impact. Available at SSRN 2477899, 2014. 
[3] E. Beigman and B. B. Klebanov. Learning with annotation noise. In ACL, 2009. 
[4] A. Datta, M. C. Tschantz, and A. Datta. Automated experiment on ad privacy settings. Proceedings on 

Privacy Enhancing Technologies, 2015. 
[5] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel. Fairness through awareness. In Innovations in 

Theoretical Computer Science Conference, 2012. 
[6] J. Eisenstein, B. O’Connor, N. A. Smith, and E. P. Xing. Diffusion of lexical change in social media. PLoS 

ONE, page 1–13, 2014. 
[7] M. Faruqui, J. Dodge, S. K. Jauhar, C. Dyer, E. Hovy, and N. A. Smith. Retrofitting word vector to 

semantic lexicons. In NAACL, 2015. 
[8] M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian. Certifying and 

remove disparate impact. In KDD, 2015. 
[9] C. Fellbaum, editor. Wordnet: An Electronic Lexical Database. The MIT Press, Cambridge, MA, 1998. 

[10] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. Placing search 
in context: The concept revisited. In WWW. ACM, 2001. 

[11] A. G. Greenwald, D. E. McGhee, and J. L. Schwartz. Measuring individual difference in implicit cognition: 
the implicit association test. Journal of personality and social psychology, 74(6):1464, 1998. 

[12] C. Hansen, M. Tosik, G. Goossen, C. Li, L. Bayeva, F. Berbain, and M. Rotaru. How to get the best word 
vector for resume parsing. In SNN Adaptive Intelligence / Symposium: Machine Learning 2015, Nijmegen. 

[13] J. Holmes and M. Meyerhoff. The handbook of language and gender, volume 25. John Wiley & Sons, 
2008. 

[14] O. İrsoy and C. Cardie. Deep recursive neural network for compositionality in language. In NIPS. 2014. 
[15] R. Jakobson, L. R. Waugh, and M. Monville-Burston. On language. Harvard Univ Pr, 1990. 
[16] M. Kay, C. Matuszek, and S. A. Munson. Unequal representation and gender stereotype in image search 

result for occupations. In Human Factors in Computing Systems. ACM, 2015. 
[17] T. Lei, H. Joshi, R. Barzilay, T. Jaakkola, A. M. Katerina Tymoshenko, and L. Marquez. Semi-supervised 

question retrieval with gate convolutions. In NAACL. 2016. 
[18] O. Levy and Y. Goldberg. Linguistic regularity in sparse and explicit word representations. In CoNLL, 

2014. 
[19] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representation in vector space. 

In ICLR, 2013. 
[20] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representation of word and 

phrase and their compositionality. In NIPS. 
[21] T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic regularity in continuous space word representations. In 

HLT-NAACL, page 746–751, 2013. 
[22] E. Nalisnick, B. Mitra, N. Craswell, and R. Caruana. Improving document rank with dual word 

embeddings. In www, April 2016. 
[23] B. A. Nosek, M. Banaji, and A. G. Greenwald. Harvesting implicit group attitude and belief from a 

demonstration web site. Group Dynamics: Theory, Research, and Practice, 6(1):101, 2002. 
[24] D. Pedreshi, S. Ruggieri, and F. Turini. Discrimination-aware data mining. In SIGKDD, page 560–568. 

ACM, 2008. 
[25] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vector for word representation. In EMNLP, 

2014. 
[26] K. Ross and C. Carter. Women and news: A long and wind road. Media, Culture & Society, 33(8):1148– 

1165, 2011. 
[27] H. Rubenstein and J. B. Goodenough. Contextual correlate of synonymy. Communications of the ACM, 

8(10):627–633, 1965. 
[28] E. Sapir. Selected writing of Edward Sapir in language, culture and personality, volume 342. Univ of 

California Press, 1985. 
[29] B. Schmidt. Rejecting the gender binary: a vector-space operation. http://bookworm.benschmidt. 

org/posts/2015-10-30-rejecting-the-gender-binary.html, 2015. 
[30] J. P. Stanley. Paradigmatic woman: The prostitute. Papers in language variation, page 303–321, 1977. 
[31] L. Sweeney. Discrimination in online ad delivery. Queue, 11(3):10, 2013. 
[32] A. Torralba and A. Efros. Unbiased look at dataset bias. In CVPR, 2012. 
[33] P. D. Turney. Domain and function: A dual-space model of semantic relation and compositions. Journal 

of Artificial Intelligence Research, page 533–585, 2012. 
[34] C. Wagner, D. Garcia, M. Jadidi, and M. Strohmaier. It’s a man’s wikipedia? assess gender inequality 

in an online encyclopedia. In Ninth International AAAI Conference on Web and Social Media, 2015. 
[35] D. Yogatama, M. Faruqui, C. Dyer, and N. A. Smith. Learning word representation with hierarchical 

sparse coding. In ICML, 2015. 
[36] I. Zliobaite. A survey on measure indirect discrimination in machine learning. arXiv preprint 

arXiv:1511.00148, 2015. 

9 


