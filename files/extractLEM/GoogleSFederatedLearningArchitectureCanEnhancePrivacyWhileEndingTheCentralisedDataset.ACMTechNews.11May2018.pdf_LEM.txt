






































Googleâ•Žs Federated Learning Architecture Can Enhance Privacy While Ending The Centralised Dataset 


Google’s Federated Learning Architecture Can Enhance Privacy 
While Ending The Centralised Dataset 

Traditionally, datasets be a huge part of the machine learn pipeline. With the advent of deep learn techniques, the amount 
of high quality, well-labelled data be paramount to the success of the machine learn project. The standard way of execute 
machine learn also need the data to be on the datacentre or the machine where the model be be trained. But now, 
engineer at Google have come up with a new secure and robust cloud infrastructure architecture for processing data call 
Federated Learning. It have be create for model which be train from user interaction on mobile devices. 

A significant amount of research have be do on enable the efficient distribute training of neural networks. We can take 
several approach to distribute the training of deep learn networks. 

Model parallelism: Here, different machine in the distribute system be responsible for the computation in different 
part of a single network 
Data parallelism: Here different machine have a complete copy of the model; each machine simply get a different portion 
of the data, and result from each be somehow combined. 

Federated Learning : The Architecture 

Federated Learning take advantage of mobile phone to collaborate and learn a share prediction model. This be do while all 
the training data stay on the device and be not sent over to the cloud. This decouple help u to execute machine learn on 
the device without the the need to store the data in the cloud. This be different from the use case such a the Mobile Vision API 
and On-Device Smart Reply where prediction be do on device; in federate learn the model training happens on the device 
a well. 

Advertisement 

Google’s Federated Learning Architecture Can Enhance Privacy While E... https://analyticsindiamag.com/googles-federated-learning-architecture-c... 

1 sur 2 18-05-18 à 19:26 



The phone personalises the model locally, base on your usage (A). Many users’ update be aggregate (B) to form a 
consensus change (C) to the share model, after which the procedure be repeated. 

Here how it works: The mobile device downloads the current model, improves it by learn from data on your phone, and then 
make a summary of the knowledge it have learn from the data a a small focus update. Only this update to the model be sent 
to the cloud, use encrypt communication, where it be immediately average with other user update to improve the share 
model. All the training data remains on your device, and no individual update be store in the cloud. 

This kind of architecture result in smarter models, low latency, and less power consumption; all this while ensure complete 
privacy of user data. The architecture also sends an update to the share model, but the improve model on the phone can now 
be use immediately, and this result in a very powerful user experience. 

Making Federated Learning Possible 

The system of Federated Learning be already be test in Gboard on Android, the very popular Google Keyboard. Whenever 
Gboard show a suggest query, the mobile device locally store information about the current context and whether you use 
the suggestion. Federated Learning process that history on-device to suggest improvement to the next iteration of Gboard’s 
query suggestion model. 

The research team at Google have to overcome many algorithmic and research challenge to make federate learn possible. 
Optimisation algorithm like Stochastic Gradient Descent (SGD) which be typically use in many machine learn system run 
on a large dataset. Most of the time these datasets be partition homogeneously across server in the cloud. Many highly 
iterative algorithm require low-latency, high-throughput connection to the training data. But in this particular setting, the data 
be distribute across million of mobile and cellular device in a highly heterogeneous fashion. In addition, these device have 
significantly higher-latency, lower-throughput connection and be only intermittently available for training. 

To put such a system in deployment to million heterogenous phone run Gboard require a fairly advanced technology 
stack. On-device training us a minimise version of TensorFlow. Upload speed be typically much slow than download 
speeds, the researcher also developed a novel way to reduce upload communication cost up to another 100x by compress 
update use random rotation and quantisation. 

The Future Of Federated Learning 

The researcher still feel that the work be only the beginning. They say in their official blog, “Our work have only scratch the 
surface of what be possible. Federated Learning can’t solve all machine learn problem (for example, learn to recognise 
different dog breed by training on carefully label examples), and for many other model the necessary training data be already 
store in the cloud (like training spam filter for Gmail).“ 

The application of Federated Learning require that machine learn practitioner to use new tool and a new way of look at 
the problem. Model development, training, and evaluation with no direct access to or label of raw data, with communication 
cost a a limit factor. The researcher believe that the user benefit of Federated Learning make tackle the technical 
challenge worthwhile. 

Provide your comment below 

comment 

Google’s Federated Learning Architecture Can Enhance Privacy While E... https://analyticsindiamag.com/googles-federated-learning-architecture-c... 

2 sur 2 18-05-18 à 19:26 


