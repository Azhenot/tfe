










































Interpretable classification model for recidivism prediction 


© 2016 Royal Statistical Society 0964–1998/17/180689 

J. R. Statist. Soc. A (2017) 
180, Part 3, pp. 689–722 

Interpretable classification model for recidivism 
prediction 

Jiaming Zeng, Berk Ustun and Cynthia Rudin 

Massachusetts Institute of Technology, Cambridge, USA 

[Received March 2015. Final revision June 2016] 

Summary. We investigate a long-debated question, which be how to create predictive model of 
recidivism that be sufficiently accurate, transparent and interpretable to use for decision making. 
This question be complicate a these model be use to support different decisions, from 
sentencing, to determine release on probation to allocate preventative social services. Each 
case might have an objective other than classification accuracy, such a a desire true positive 
rate TPR or false positive rate FPR. Each (TPR, FPR) pair be a point on the receiver operator 
characteristic (ROC) curve. We use popular machine learn method to create model along 
the full ROC curve on a wide range of recidivism prediction problems. We show that many 
method (support vector machines, stochastic gradient boost and ridge regression) produce 
equally accurate model along the full ROC curve. However, method that be design for 
interpretability (classification and regression tree and C5.0) cannot be tune to produce model 
that be accurate and/or interpretable. To handle this shortcoming, we use a recent method 
call supersparse linear integer model to produce accurate, transparent and interpretable 
score system along the full ROC curve. These score system can be use for decision 
make for many different use cases, since they be just a accurate a the most powerful black 
box machine learn model for many applications, but completely transparent, and highly 
interpretable. 

Keywords: Binary classification; Interpretability; Machine learning; Recidivism; Scoring 
system 

1. Introduction 

Forecasting have be use for criminology application since the 1920s (Borden, 1928; Burgess, 
1928) when various factor derive from age, race, prior offence history, employment, grade 
and neighbourhood background be use to estimate success of parole. Many thing have 
change since then, include the fact that we have developed machine learn method that 
can produce accurate predictive model and have collect large high dimensional data set on 
which to apply them. 

Prediction of recidivism be still extremely important. In the USA, for example, a minority of 
individual commit the majority of the crime (Wolfgang, 1987): these be the ‘power few’ of 
Sherman (2007) on which we should focus our efforts. We want to ensure that public resource 
be direct effectively, be they correctional facility or preventative social services. Milgram 
(2014) recently discuss the critical importance of accurately predict whether an individual 
who be release on bail pose a risk to public safety, point out that high risk individual be 
be release 50% of the time whereas low risk individual be be release less often than 
they should be. Her observation be in line with long-standing work on clinical versus actuarial 

Address for correspondence: Jiaming Zeng, Department of Mathematics, Massachusetts Institute of Technology, 
355 Massachusetts Avenue, Cambridge, MA 02139, USA. 
E-mail: jiaming@alum.mit.edu 



690 J. Zeng, B. Ustun and C. Rudin 

judgement, which show that humans, on their own, be not a good at risk assessment a 
statistical model (Dawes et al., 1989; Grove and Meehl, 1996). This be the reason that several 
US state have mandate the use of predictive model for sentence decision (Pew Center of 
the States, Public Safety Performance Project, 2011; Wroblewski, 2014). 

There have be some controversy about whether sophisticated machine learn method 
(such a random forests; see for example Breiman (2001a), Berk et al. (2009) and Ritter (2013)) 
be necessary to produce accurate predictive model of recidivism, or if traditional approach 
such a logistic regression or linear discriminant analysis would suffice (see, for example, Tol- 
lenaar and van der Heijden (2013), Berk and Bleich (2013) and Bushway (2013)). Random 
forest may produce accurate predictive models, but these model effectively operate a black 
boxes, which make it difficult to understand how the input variable be produce a predict 
outcome. If a simpler, more transparent, but equally accurate predictive model could be de- 
veloped, it would be more usable and defensible for many decision-making applications. There 
be a precedent for use such model in criminology (Steinhart, 2006; Andrade, 2009); Ridge- 
way (2013) argue that a ‘decent transparent model that be actually use will outperform a 
sophisticated system that predicts good but sits on a shelf ’. This discussion be capture nicely 
by Bushway (2013), who contrast the work of Berk and Bleich (2013) and Tollenaar and 
van der Heijden (2013). Berk and Bleich (2013) claimed that we need sophisticated machine 
learn method because of their substantial benefit in accuracy, whereas Tollenaar and van 
der Heijden (2013) claimed that ‘modern statistical, data mining and machine learn model 
provide no real advantage over logistic regression and LDA’, assume that human have do 
appropriate preprocessing. In this work, we argue that the answer to the question be far more 
subtle than a simple yes or no. 

In particular, the answer depends on how the model will be use for decision making. For 
each use case (e.g. sentencing, parole decision and policy interventions), we might need a 
decision point at a different level of true positive rate TPR and false positive rate FPR (see 
also Ritter (2013)). Each (TPR, FPR) pair be a point on the receiver operator characteristic 
(ROC) curve. To determine whether one method be good than another, we must consider the 
appropriate point along the ROC curve for decision making. As we show, for a wide range 
of recidivism prediction problems, many machine learn method (support vector machine 
(SVMs) or random forests) produce equally accurate predictive model along the ROC curve. 
However, there be trade-off between accuracy, transparency and interpretability: method 
that be design to yield transparent model (classification and regression tree (CART); C5.0) 
cannot be tune to produce a accurate model along the ROC curve and do not always yield 
model that be interpretable. This be not to say that interpretable model for prediction of 
recidivism do not exist. The fact that many machine learn method produce model with 
similar level of predictive accuracy indicates that there be a large class of approximately equally 
accurate predictive model (called the ‘Rashomon’ effect by Breiman (2001b)). In this case, there 
may be interpretable model that also attain the same level of accuracy. Finding model that be 
accurate and interpretable, however, be computationally challenging. 

In this paper, we explore whether such accurate yet interpretable model exist and how to find 
them. For this, we use a new machine learn method know a a supersparse linear integer 
model (SLIM) (Ustun and Rudin, 2015) to learn score system from data. Scoring system 
have be use for many criminal justice application because they let user make quick predic- 
tions by adding, subtract and multiply a few small number (see, for example, Hoffman 
and Adelberg (1980), US Sentencing Commission (1987) and Pennsylvania Commission on 
Sentencing (2012)). In contrast with exist tools, which have be built by use heuristic 
approach (see, for example, Gottfredson and Snyder (2005)), the model that be built by 

lphilippe 
Texte surligné 

lphilippe 
Texte surligné 



Recidivism Prediction 691 

SLIM be fully optimize for accuracy and sparsity, and can handle additional constraint (e.g. 
bound on the false positive rate and monotonicity property for the coefficients). We use SLIM 
to produce a set of simple score system at different decision point across the full ROC curve 
and provide a comparison with other popular machine learn methods. Our finding show 
that the SLIM score system be often just a accurate a the most powerful black box machine 
learn models, but transparent and highly interpretable. 

1.1. Structure 
The remainder of this paper be structure a follows. In Section 1.2, we discus related work. 
In Section 2, we describe how we derive six recidivism prediction problems. In Section 3, 
we provide a brief overview of SLIM and describe several new technique that can reduce 
the computation that be require to produce score systems. In Section 4, we compare the 
accuracy and interpretability of model produce by the nine machine learn method on the 
six recidivism prediction problems. We include additional result that be related to the accuracy 
and interpretability of model from different method in Appendix A. An on-line supplement 
with additional appendix can be found at http://arxiv.org/abs/1503.07810. 

1.2. Related work 
Predictive model for recidivism have be in widespread use in many country and many area 
of the criminal justice system since the early 1920s (see, for example, Borden (1928), Burgess 
(1928) and Tibbitts (1931)). The use of these tool have be spur on by continued research 
into the superiority of actuarial judgement (Dawes et al., 1989; Grove and Meehl, 1996) a well 
a a desire to use limited public resource efficiently (Clements, 1996; Simon, 2005; McCord, 
1978, 2003). In the USA, federal guideline currently mandate the use of a predictive recidivism 
measure know a the criminal history category for sentence (US Sentencing Commission, 
1987). Besides the USA, country that currently use risk assessment tool include Canada 
(Hanson and Thornton, 2003), the Netherlands (Tollenaar and van der Heijden, 2013) and the 
UK (Howard et al., 2009). Applications of these tool can be see in evidence-based sentence 
(Hoffman, 1994), correction and prison administration (Belfrage et al., 2000), inform release 
on parole (Pew Center of the States, Public Safety Performance Project, 2011), determine the 
level of supervision during parole (Barnes and Hyatt, 2012; Ritter, 2013), determine appro- 
priate sanction for violation of parole (Turner et al., 2009), and target policy intervention 
(Lowenkamp and Latessa, 2004). 

Our paper focus on binary classification model to predict general recidivism (i.e. recidivism 
of any type of crime) a well a crime-specific recidivism (i.e. recidivism for drug, general violence, 
domestic violence, sexual violence and fatal violence offences). Risk assessment tool for general 
recidivism include the salient factor score (Hoffman and Adelberg, 1980; Hoffman, 1994), the 
offender group reconviction scale (Copas and Marshall, 1998; Maden et al., 2006; Howard 
et al., 2009), the statistical information of recidivism scale (Nafekh and Motiuk, 2002) and 
the ‘Level of service/case management inventory’ (Andrews and Bonta, 2000). Crime-specific 
application include risk assessment tool for domestic violence (see, for example, the spousal 
abuse risk assessment of Kropp and Hart (2000)), sexual violence (see, for example, Hanson 
and Thornton (2003) and Langton et al. (2007)) and general violence (see, for example, the 
‘Historical clinical and risk management’ tool of Webster (1997), or the ‘Structured assessment 
of violence risk in youth’ tool of Borum (2006)). 

The score system that we present in this paper be design to mimic the form of risk score 
that be currently use throughout the criminal justice system—i.e. linear classification model 



692 J. Zeng, B. Ustun and C. Rudin 

that require user only to add, subtract and multiply a few small number to make a prediction 
(Ustun and Rudin, 2015). These tool be unique in that they allow user to make quick predic- 
tions by hand, without a computer, calculator or nomogram (which be a visualization tool for 
more difficult calculations). Current example of such tool include the salient factor score (Hoff- 
man and Adelberg, 1980), the criminal history category (US Sentencing Commission, 1987) and 
the offence gravity score (Pennsylvania Commission on Sentencing, 2012). Our approach aim 
to produce score system that be fully optimize for accuracy and sparsity without any post- 
processing. In contrast, current tool be produce through heuristic approach that primarily 
involve logistic regression with some ad hoc post-processing to ensure that the model be sparse 
and use integer coefficient (see, for example, the method that be described in Gottfredson 
and Snyder (2005)). 

Our score system differ from exist tool in that they directly output a predict outcome 
(i.e. prisoner i will lapse into crime) a oppose to a predict probability of the outcome (i.e. 
the predict probability that prisoner i will lapse into crime be 90%). The predict probability 
from exist tool be typically convert into an outcome by impose a threshold (i.e. classify a 
prisoner a ‘high risk’ if the predict probability of arrest be great than 70%). In practice, user 
arbitrarily pick several threshold to translate predict probability into an ordinal outcome 
(e.g. prisoner i be ‘low risk’, if the predict probability be less than 30%, ‘medium risk’ if the 
predict probability be less than 60% and ‘high risk’ otherwise). These arbitrary threshold 
make it difficult, if not impossible, to ass the predictive accuracy of the tool effectively 
(Hannah-Moffat, 2013). Netter (2007), for instance, mention that ‘the possibility of make 
a prediction error (false positive or false negative) use a risk tool be probable, but not easily 
determined’. In contrast with exist tools, the score system let user ass accuracy in a 
straightforward way (i.e. through the true positive rate and true negative rate). Further, our 
approach have the advantage that it can yield a score system that optimizes the class-based 
accuracy at a particular decision point (i.e. produce the model that maximizes the true positive 
rate, give a false positive rate of at most 30%). 

Our work be related to a stream of research that have aim to leverage new method for 
predictive model in criminology. In contrast with our work, much of the research to date 
have focus on improve predictive accuracy by training powerful black box model such a 
random forest (Breiman, 2001a) and stochastic gradient boost (SGB) (Friedman, 2002). 
Random forest (Breiman, 2001a), in particular, have be use for several criminological ap- 
plications, include predict homicide offender recidivism (Neuilly et al., 2011), predict 
serious misconduct among incarcerate prisoner (Berk et al., 2006), forecasting potential mur- 
ders for criminal on probation or parole (Berk et al., 2009), forecasting domestic violence and 
help to inform court decision at arraignment (Berk and Sorenson, 2014). We note that not 
all study in black box model use (Berk et al., 2005), for instance, help the Los Angeles Sher- 
iff ’s Department to develop a simple and practical screener to forecast domestic violence by 
use decision trees. More recently Goel et al. (2016) developed a simple score system to help 
the New York Police Department to address stop and frisk by first run logistic regression, 
and then round the coefficients. 

The program that be use to analyse the data can be obtain from 

http://wileyonlinelibrary.com/journal/rss-datasets 

2. Data and prediction problem 

Each problem be a binary classification problem with N = 33796 prisoner and P = 48 input 
variables. The goal be to predict whether a prisoner will be arrest for a certain type of crime 



Recidivism Prediction 693 

within 3 year of be release from prison. In what follows, we describe how we create each 
prediction problem. 

2.1. Database detail 
We derive the recidivism prediction problem in our paper from the ‘Recidivism of prisoner 
release in 1994’ database, assemble by the US Department of Justice, Bureau of Justice 
Statistics (2014). It be the large publicly available database on prisoner recidivism in the USA. 
The study tracked 38624 prisoner for 3 year follow their release from prison in 1994. These 
prisoner be randomly sample from the population of all prisoner release from 15 US 
state (Arizona, California, Delaware, Florida, Illinois, Maryland, Michigan, Minnesota, New 
Jersey, New York, North Carolina, Ohio, Oregon, Texas and Virginia). The population sample 
account for roughly two-thirds of all prisoner who be release from prison in the USA in 
1994. Other study that use this database include Bhati and Piquero (2007), Bhati (2007) and 
Zhang et al. (2009). 

The database be compose of 38624 row and 6427 columns, where each row represent a 
prisoner and each column represent a feature (i.e. a field of information for a give prisoner). 
The 6427 column consist of 91 field that be record before or during release from prison in 
1994 (e.g. date of birth and effective sentence length) and 64 field that be repeatedly record 
for up to 99 different arrest in the 3-year follow-up period (for example, if a prisoner be 
rearrested three time within 3 years, three record cycle would be recorded). The information 
for each prisoner be source from record of arrest and prosecution sheet that be kept by state 
law enforcement agency and/or the Federal Bureau of Investigation. A detailed descriptive 
analysis of the database be carry out by statistician at the US Bureau of Justice Statistics 
(Langan and Levin, 2002). This study restrict it attention to 33796 of the 38624 prisoner 
to exclude extraordinary or unrepresentative release cases. To be select for the analysis of 
Langan and Levin (2002), a prisoner have to be alive during the 3-year follow-up period and 
have to have be release from prison in 1994 for an original sentence that be at least 1 year 
or longer. Prisoners with certain release types—release to custody, detainer or warrant, absent 
without leave, escape, transfer, administrative release and release on appeal—were excluded. To 
mirror the approach of Langan and Levin (2002), we restrict our attention to the same subset 
of prisoners. 

This data set have some serious flaw which we point out below. To begin, many important 
factor that could be use to predict recidivism be missing, and many include factor be 
sufficiently noisy to be exclude from our preliminary experiments. The information about 
level of education be extremely minimal; we do not even know whether each prisoner attend 
college or complete high school. The information about course in prison be only an indicator 
of whether the inmate take any education or vocation course at all. Also, there be no family 
history for each prisoner (e.g. foster care) and no record of visitor while in prison (e.g. indicator 
of care family member or friends). There be no information about re-entry programme or 
employment history. Although some of these factor exist, such a drug or alcohol treatment 
and in-prison vocational programmes, the data be highly incomplete and therefore have be 
exclude from our analysis. For example, for drug treatment, less than 14% of the prisoner have 
a valid entry. The rest be ‘unknown’. To include a many prisoner a possible, we chose to 
exclude factor with extremely sparse information. 

2.2. Deriving input variable 
We provide a summary of the P = 48 input variable that be derive from the database in 



694 J. Zeng, B. Ustun and C. Rudin 

Table 1. Overview of input variable for all prediction problems† 

Input variable P(xij =1) Definition 

female 0.06 Prisoner i be female 
prior alcohol abuse 0.20 Prisoner i have a history of alcohol abuse 
prior drug abuse 0.16 Prisoner i have a history of drug abuse 
age at release�17 0.00 Prisoner i be �17 year old at release in 1994 
age at release 18 to 24 0.19 Prisoner i be 18–24 year old at release in 1994 
age at release 25 to 29 0.21 Prisoner i be 25–29 year old at release in 1994 
age at release 30 to 39 0.38 Prisoner i be 30–39 year old at release in 1994 
age at release�40 0.21 Prisoner i be �40 year old at release in 1994 
release unconditional 0.11 Prisoner i be release at expiration of sentence 
release conditional 0.87 Prisoner i be release on parole or probation 
time served�6mo 0.23 Prisoner i serve �6 month 
time serve 7 to 12mo 0.20 Prisoner i serve 7–12 month 
time serve 13 to 24mo 0.23 Prisoner i serve 13–24 month 
time serve 25 to 60mo 0.25 Prisoner i serve 25–60 month 
time served�61mo 0.10 Prisoner i serve �61 month 
infraction in prison 0.24 Prisoner i have a record of misconduct in prison 
age 1st arrest�17 0.14 Prisoner i be �17 year old at 1st arrest 
age 1st arrest 18 to 24 0.61 Prisoner i be 18–24 year old at 1st arrest 
age 1st arrest 25 to 29 0.10 Prisoner i be 25–29 year old at 1st arrest 
age 1st arrest 30 to 39 0.09 Prisoner i be 30–39 year old at 1st arrest 
age 1st arrest�40 0.04 Prisoner i be �40 year old at 1st arrest 
age 1st confinement�17 0.03 Prisoner i be �17 year old at 1st confinement 
age 1st confinement 18 to 24 0.46 Prisoner i be 18–24 year old at 1st confinement 
age 1st confinement 25 to 29 0.18 Prisoner i be 25–29 year old at 1st confinement 
age 1st confinement 30 to 39 0.21 Prisoner i be 30–39 year old at 1st confinement 
age 1st confinement�40 0.12 Prisoner i be �40 year old at 1st confinement 
prior arrest for drug 0.47 Prisoner i be once arrest for a drug offence 
prior arrest for property 0.67 Prisoner i be once arrest for a property 

offence 
prior arrest for public order 0.62 Prisoner i be once arrest for a public order 

offence 
prior arrest for general violence 0.52 Prisoner i be once arrest for general violence 
prior arrest for domestic violence 0.04 Prisoner i be once arrest for domestic violence 
prior arrest for sexual violence 0.03 Prisoner i be once arrest for sexual violence 
prior arrest for fatal violence 0.01 Prisoner i be once arrest for fatal violence 
prior arrest for multiple type 0.77 Prisoner i be once arrest for multiple type of 

crime 
prior arrest for felony 0.84 Prisoner i be once arrest for a felony 
prior arrest for misdemeanor 0.49 Prisoner i be once arrest for a misdemeanour 
prior arrest for local ordinance 0.01 Prisoner i be once arrest for local ordinance 
prior arrest with firearm involve 0.09 Prisoner i be once arrest for an incident 

involve firearm 
prior arrest with child involve 0.17 Prisoner i be once arrest for an incident 

involve child 
no prior arrest 0.12 Prisoner i have no prior arrest 
prior arrests�1 0.88 Prisoner i have at least 1 prior arrest 
prior arrests�2 0.78 Prisoner i have at least 2 prior arrest 
prior arrests�5 0.60 Prisoner i have at least 5 prior arrest 
multiple prior prison time 0.43 Prisoner i have be to prison multiple time 
any prior jail time 0.47 Prisoner i have be to prison at least once 
multiple prior jail time 0.29 Prisoner i have be to prison multiple time 
any prior probation or fine 0.42 Prisoner i have be on probation or paid a 

fine at least once 
multiple prior probation or fine 0.22 Prisoner i have be on probation or paid a 

fine multiple time 

†Each variable be a binary rule of the form xij ∈{0, 1}. We list condition require for xij =1 under the definition 
column. 



Recidivism Prediction 695 

Table 2. Overview of recidivism prediction problems† 

Prediction problem P(yi =1)(%) Outcome variable 

arrest 59.0 yi =1 if prisoner i be arrest for any offence within 3 year of release 
from prison 

drug 20.0 yi =1 if prisoner i be arrest for a drug-related offence (e.g. possession 
or trafficking) within 3 year of release from prison 

general violence 19.1 yi =1 if prisoner i be arrest for a violent offence (e.g. robbery or 
aggravate assault) within 3 year of release from prison 

domestic violence 3.5 yi =1 if prisoner i be arrest for domestic violence within 3 year of 
release from prison 

sexual violence 3.0 yi =1 if prisoner i be arrest for sexual violence within 3 year of 
release from prison 

fatal violence 0.7 yi =1 if prisoner i be arrest for murder or manslaughter within 3 
year of release from prison 

†The percentage P.yi =1/ do not add up to 100% because a prisoner could be arrest for multiple type of crime 
at one time (e.g. both drug and public order offences) and could also be arrest multiple time over the 3-year 
follow-up period. 

Table 1. We encode each input variable a a binary rule of the form xij ∈ {0, 1}, j = 1, : : : , P , 
where xij = 1 if condition j hold true about prisoner i. This allows a linear model to encode 
non-linear function of the original variables. We refer to input variable in the text by use 
italic (e.g. female). All prediction problem in Table 2 and all machine learn method in 
Table 4 in Section 4.1.4 use these same input variables. 

The final set of input variable be representative of well-known risk factor for recidivism 
(Bushway and Piehl, 2007; Crow, 2008) and have be use in risk assessment tool since 1928 
(see, for example, Borden (1928), Hinojosa et al. (2005), Berk et al. (2006) and Baradaran 
(2013)). They include 

(a) information about prison release in 1994 (e.g. time serve , age at release and infraction 
in prison), 

(b) information from past arrests, sentence and conviction (e.g. prior arrests�1 and any 
prior jail time), 

(c) history of substance abuse (e.g. alcohol abuse) and 
(d) gender (e.g. female). 

(The prior arrest variable do not count the original crime for which they be release from 
prison in 1994; thus, about 12% of the prisoner have no prior arrest = 1 even though they 
be arrest at least once.) These input variable be advantageous because 

(a) the information be easily accessible to law enforcement official (all the above information 
can be found in state record of arrest and prosecution sheets) and 

(b) they do not include socio-economic factor such a race, which would directly eliminate 
the potential to use these tool in application such a sentencing. 

We note that encode the input variable a binary value present many advantages. They 
produce model that be easy to understand (removing the wide range that be present by 
continuous variables), and they avoid potential confusion stem from coefficient of nor- 
malized input (for instance, after undo the normalization for normalize coefficients, a small 
coefficient might be highly influential if it applies to a variable take large values). Binariza- 
tion be especially useful for SLIM a we can fit SLIM model by solve a slightly easy discrete 



696 J. Zeng, B. Ustun and C. Rudin 

Table 3. Table of conditional probability for all input variable (rows) and prediction problem (columns)† 

Input variable Probabilities for the follow prediction problems: 

arrest drug general domestic sexual fatal 
violence violence violence violence 

female 0.54 0.21 0.11 0.02 0.01 0.0005 
prior alcohol abuse 0.58 0.18 0.20 0.04 0.03 0.01 
prior drug abuse 0.61 0.23 0.21 0.03 0.03 0.004 
age at release�17 0.84 0.35 0.31 0.01 0.01 0.04 
age at release 18 to 24 0.71 0.24 0.25 0.04 0.03 0.01 
age at release 25 to 29 0.66 0.23 0.21 0.04 0.03 0.01 
age at release 30 to 39 0.59 0.20 0.17 0.04 0.03 0.01 
age at release�40 0.41 0.12 0.09 0.02 0.03 0.003 
release unconditional 0.65 0.20 0.23 0.06 0.04 0.01 
release conditional 0.58 0.20 0.17 0.03 0.03 0.01 
time served�6mo 0.67 0.27 0.19 0.04 0.03 0.01 
time serve 7 to 12mo 0.63 0.22 0.19 0.04 0.03 0.01 
time serve 13 to 24mo 0.59 0.20 0.17 0.04 0.03 0.01 
time serve 25 to 60mo 0.53 0.16 0.17 0.03 0.03 0.01 
time served�61mo 0.48 0.11 0.15 0.02 0.04 0.004 
infraction in prison 0.65 0.19 0.20 0.01 0.04 0.01 
age 1st arrest�17 0.73 0.27 0.27 0.04 0.04 0.01 
age 1st arrest 18 to 24 0.64 0.22 0.20 0.04 0.03 0.01 
age 1st arrest 25 to 29 0.47 0.14 0.10 0.02 0.02 0.005 
age 1st arrest 30 to 39 0.34 0.10 0.06 0.02 0.02 0.003 
age 1st arrest�40 0.21 0.05 0.03 0.01 0.02 0.002 
age 1st confinement�17 0.78 0.28 0.29 0.04 0.04 0.02 
age 1st confinement 18 to 24 0.68 0.24 0.23 0.05 0.04 0.01 
age 1st confinement 25 to 29 0.60 0.20 0.17 0.03 0.03 0.005 
age 1st confinement 30 to 39 0.50 0.16 0.12 0.03 0.02 0.003 
age 1st confinement�40 0.34 0.09 0.07 0.01 0.02 0.002 
prior arrest for drug 0.68 0.32 0.21 0.04 0.02 0.01 
prior arrest for property 0.67 0.24 0.22 0.04 0.03 0.01 
prior arrest for public order 0.65 0.24 0.22 0.04 0.03 0.01 
prior arrest for general violence 0.67 0.25 0.26 0.05 0.04 0.01 
prior arrest for domestic violence 0.66 0.21 0.27 0.13 0.04 0.01 
prior arrest for sexual violence 0.49 0.13 0.16 0.04 0.06 0.01 
prior arrest for fatal violence 0.54 0.19 0.21 0.04 0.03 0.01 
prior arrest for multiple crime type 0.64 0.23 0.21 0.04 0.03 0.01 
prior arrest for felony 0.60 0.21 0.19 0.04 0.03 0.01 
prior arrest for misdemeanor 0.69 0.26 0.24 0.06 0.03 0.01 
prior arrest for local ordinance 0.91 0.29 0.43 0.15 0.05 0.02 
prior arrest with firearm involve 0.70 0.30 0.27 0.06 0.03 0.01 
prior arrest with child involve 0.48 0.13 0.14 0.03 0.06 0.01 
no prior arrest 0.32 0.07 0.08 0.02 0.02 0.003 
prior arrest�1 0.63 0.22 0.19 0.04 0.03 0.01 
prior arrest�2 0.66 0.23 0.20 0.04 0.03 0.01 
prior arrest�5 0.70 0.25 0.22 0.04 0.03 0.01 
multiple prior prison time 0.65 0.23 0.19 0.03 0.03 0.01 
any prior jail time 0.69 0.25 0.21 0.04 0.03 0.01 
multiple prior jail time 0.73 0.27 0.22 0.04 0.03 0.01 
any prior probation or fine 0.67 0.24 0.20 0.04 0.03 0.01 
multiple prior probation or fine 0.71 0.27 0.22 0.05 0.03 0.01 

†Each cell represent the conditional probability P.y=1|x=1/ where x be the input variable that be specify in the 
row and y be the outcome variable for the prediction problem specify in the column. 



Recidivism Prediction 697 

optimization problem when the data contain only binary input variable (as discuss in Section 
3.3). In appendix E of the on-line supplement, we explore the change in predictive accuracy if 
continuous variable be include and show that the change in performance be minor for most 
methods. There be some exceptions; for example, the CART and C5.0T method experienced 
an improvement of 4:6% for drug and SVM radial basis function (RBF) experienced a 7:7% 
improvement for fatal violence. Yet, even for these methods, no clear improvement be see 
across all problems. 

2.3. Deriving outcome variable 
We create a total of six recidivism prediction problem by encode a binary outcome variable 
yi ∈ {−1, 1} such that yi = 1 if a prisoner be arrest for a particular type of crime within 3 
year after be release from prison. For clarity, we refer to each prediction problem in the 
text by use Courier fount (e.g. arrest). We provide detail on each recidivism prediction 
problem in Table 2. These include an arrest for any crime (arrest), an arrest for a drug- 
related offence (drug) or an arrest for a certain type of violent offence (general violence, 
domestic violence, sexual violence and fatal violence). 

In the data set, all crime type can be broken down into small subcategories (e.g. fatal 
violence can be broken into six subcategories such a murder and vehicular man 
laughter). We chose to use the broader crime category for conciseness and clarity. Indeed, the 
study by Langan and Levin (2002) also split crime into the same major categories. We note that 
the outcome of violent offence be mutually exclusive, a different type of violence be treat 
differently within the US legal system. In other words, yi = 1 for general violence do 
not necessarily imply that yi = 1 for domestic violence, sexual violence or fatal 
violence). 

2.4. Relationships between input and output variable 
Table 3 list the conditional probability P.y=1|xj =1/ between the outcome variable y and each 
input variable xj for all prediction problems. Using Table 3, we can identify strong association 
between the input and output for each prediction problem. These association can help to 
uncover insight into each problem and also help to validate predictive model in Section 4.4 
qualitatively. 

Consider, for instance, the arrest problem. Here, we can see that prisoner who be release 
from prison at a late age be less likely to be arrest (as the probability for arrest decrease 
monotonically a age at release increases). This also appear to be so for prisoner who be 
first confine (i.e. sent to prison) at an old age (see, for example, age of first confinement). 
In addition, we can also see that prisoner with more prior arrest have a high likelihood of 
be arrest (as the probability for arrest increase monotonically with prior arrest). 

Similar insight can be make for crime-specific prediction problems. In drug, for instance, 
we see that prisoner who be previously arrest for a drug-related offence be more likely to 
be rearrested for a drug-related offence (32%) than those who be previously arrest for any 
other type of offence. Likewise, look at domestic violence, we see that the prisoner 
with the great probability of be arrest for a domestic violence crime be those with a 
history of domestic violence (13%). 

3. Supersparse linear integer model 

SLIM be a new machine learn method for create score systems—i.e. binary classification 



698 J. Zeng, B. Ustun and C. Rudin 

model that require user only to add, subtract and multiply a few small number to make 
a prediction (Ustun and Rudin, 2015). Scoring system be widely use because they allow 
user to make quick predictions, without the use of a computer, and without extensive training 
in statistics. These model be also useful because their high degree of sparsity and integer 
coefficient let user easily gauge the influence of multiple input variable on the predict 
outcome (see Section 4.4 for an example). In what follows, we provide a brief overview of SLIM 
and provide several new technique to reduce the computation for problem with binary input 
variables. 

3.1. Framework and optimization problem 
SLIM score system be linear classification model of the form 

ŷi = 

⎧⎪⎪⎨ 
⎪⎪⎩ 

1 if 
P∑ 

j=1 
λjxij >λ0, 

−1 if 
P∑ 

j=1 
λjxij �λ0: 

Here, λ1, : : : , λP represent the coefficient (i.e. the ‘points’ for the input variable j = 1, : : : , P) 
and λ0 represent an intercept (i.e. the ‘threshold score’ that must be surpass to predict ŷi =1). 

The value of the coefficient be determine from data by solve a discrete optimization 
problem that have the form 

min 
λ 

1 
N 

N∑ 
i=1 

1.yi �= ŷi/+C0 
P∑ 

j=1 
1.λj �=0/+ � 

P∑ 
j=1 

|λj| 

such that .λ0, λ1, : : : , λP/∈L: .1/ 
Here, the objective directly minimizes the error rate .1=N/ΣNi=1 1.yi �= ŷi/ and directly penalizes 
the number of non-zero term ΣPj=1 1.λj �=0/. The constraint restrict coefficient to a finite set 
such a L={−10, : : : , 10}P+1. Optionally, one could include additional operational constraint 
on the accuracy and interpretability of the score system desired. 

The objective include a tiny penalty on the absolute value of the coefficient to restrict co- 
efficients to coprime value without affect accuracy or sparsity. To illustrate the use of this 
penalty, consider a classifier such a ŷ= sgn.x1 +x2/. If SLIM minimize only the misclassifica- 
tion rate and the number of term (the first two term of the objective), then ŷ = sgn.2x1 +2x2/ 
would have the same objective value a ŷ = sgn.x1 +x2/ because it make the same prediction 
and have the same number of non-zero coefficients. Since coefficient be restrict to a discrete 
set, we use this tiny penalty on the absolute value of these coefficient so that SLIM chooses the 
classifier with the small (coprime) coefficients, ŷ = sgn.x1 +x2/. 

The C0-parameter represent the maximum accuracy that SLIM be willing to sacrifice to 
remove a feature from the optimal score system. If, for instance, C0 be set within the range 
.1=N, 2=N/, we would sacrifice the accuracy of one observation to have a model with one few 
feature. Given C0, we can set the l1-penalty parameter � to any value 

0 < �< 
min.1=N, C0/ 

max 
{λj}j∈L 

P∑ 
j=1 

|λj| 

so that it do not affect the accuracy or sparsity of the optimal classifier but only induces the 
coefficient to be coprime for the feature that be selected. 



Recidivism Prediction 699 

SLIM differs from traditional machine learn method because it directly optimizes 
accuracy and sparsity without make approximation that other method make for scalability 
(e.g. control for accuracy use convex surrogate loss functions). By avoid these 
approximations, SLIM sacrifice the ability to fit a model in second or in a way that scale 
to extremely large data sets. In return, however, it gain the ability to fit model that be highly 
customizable, since one could directly encode a wide range of operational constraint in it 
integer program (IP) formulation. In this paper, we primarily make use of a simple con- 
straint to limit the number of non-zero coefficients; however, it be also natural to incorporate 
constraint on class-specific accuracy, structural sparsity and prediction (see Ustun and Rudin 
(2015)). 

In this paper we train the follow version of SLIM, which be different from problem (1) 
in that it include class weight and have specific constraint on the coefficients: 

min 
λ 

W+ 

N 

∑ 
i∈I+ 

1.yi �= ŷi/+ 
W− 

N 

∑ 
i∈I− 

1.yi �= ŷi/+C0 
P∑ 

j=1 
1.λj �=0/+ � 

P∑ 
j=1 

|λj| 

such that 
P∑ 

j=1 
1.λj �=0/�8, 

λj ∈{−10, : : : , 10} forj =1, : : : , P , 
λ0 ∈{−100, : : : , 100}: 

.2/ 

In this formulation, the constraint restrict each coefficient λj to an integer between −10 and 10, 
the threshold λ0 to an integer between −100 and 100, the number of non-zero coefficient to at 
most 8 (i.e. within the range of cognitive entity that human could handle, a per Miller (1956)). 
The parameter W+ and W− be class-based weight that control the accuracy on positive and 
negative examples. We typically choose value of W+ and W− such that W+ +W− =2, so that 
we recover an error minimize formulation by set W+ = W− = 1. The C0-parameter be 
set to a sufficiently small value so that SLIM would not sacrifice accuracy for sparsity: give 
W+ and W−, we can set C0 to any value 

0 <C0 < min{W−, W+}=.NP/ 

to ensure this condition. The �-parameter be set to a sufficiently small value so that SLIM 
would produce a model with coprime coefficient without affect accuracy or sparsity: give 
W+, W− and C0, we can set � to any value 0 < �<C0= max ΣPj=1|λj| to ensure this condition. 

3.2. General supersparse linear integer model integer program formulation 
Training a SLIM score system require solve an IP problem by use a solver such a 
CPLEX, Gurobi or Com-OR branch and cut. In general, we use the follow IP formulation 
to recover the solution to the optimization problem (2): 

min 
λ,z,Φ,α,β 

1 
N 

N∑ 
i=1 

zi + 
P∑ 

j=1 
Φj 

such that Mizi �γ − 
P∑ 

j=0 
yiλjxi,j i=1, : : : , N .error on i/, .3a/ 

Φj =C0αj + �βj j =1, : : : , P .penalty for coefficient j/, .3b/ 
−Λjαj �λj �Λjαj j =1, : : : , P .l0-norm/, .3c/ 



700 J. Zeng, B. Ustun and C. Rudin 

−βj �λj �βj j =1, : : : , P .l1-norm/, .3d/ 
λj ∈Z∩ [−Λj, Λj] j =0, : : : , P .coefficient set/, 

zi ∈{0, 1} i=1, : : : , N .loss variables/, 
Φj ∈R+ j =1, : : : , P .penalty variables/, 

αj ∈{0, 1} j =1, : : : , P .l0-variables/, 
βj ∈R+ j =1, : : : , P .l1-variables/: 

The constraint in expression (3a) compute the error rate by set the loss variable zi = 
1.yiλ 

Txi � 0/ to 1 if a linear classifier with coefficient λ misclassifies example i (or be close 
to misclassifying it, depend on the margin γ). This be a big M constraint for the error rate 
that depends on scalar parameter γ and Mi (see, for example, Rubin (2009)). The value of Mi 
represent the maximum score when example i be misclassified and can be set a Mi =maxλ∈L.γ − 
yiλ 

Txi/, which be easy to compute since L be finite. The value of γ represent the margin, and 
the objective be penalize when point be either incorrectly classify or within γ of the decision 
boundary. How close a point be to the decision boundary (or whether it be misclassified) be 
determine by yiλTxi. When the feature be binary, and since the coefficient be integers, γ 
can naturally be set to any value between 0 and 1. (In other cases, we can set γ =0:5 for instance, 
which make an implicit assumption on the value of the features.) The constraint in expression 
(3b) set the total penalty for each coefficient to Φj =C0αj + �βj, where αj :=1.λj �=0/ be define 
by big M constraint in expression (3c), and βj :=|λj| be define by the constraint in expression 
(3d). We denote the large absolute value of each coefficient a Λj :=maxλj∈Lj |λj|. 

Restricting coefficient to a finite set result in significant practical benefit for the SLIM IP 
formulation, especially in comparison with other IP formulation that minimize the 0–1-loss 
and/or penalize the l0-norm. Without the restriction of λ to a bound set, we would not have 
a natural choice for the big M constant, which mean that the user chooses one that be very 
large, lead to a less efficient formulation (see, for example, Wolsey (1998)). For SLIM, the 
big M constant that be use to compute the 0–1-loss in constraint (3a) be bound a 
Mi � maxλ∈L.γ − yiλTxi/, and the big M constant that be use to compute the l0-norm in 
constraint (3c) be bound a Λj �maxλj∈Lj |λj|. Bounding these constant lead to a tighter 
linear program relaxation, which narrow the integrality gap, and improves the ability of 
commercial IP solver to obtain a proof of optimality more quickly. 

3.3. Improved superparse linear integer model integer program formulation 
The follow formulation provide a tighter relaxation of the IP which reduces computation. 
It relies on the fact that, when the input variable be binary, we be likely to obtain repeat 
feature value among observations. 

min 
λ,z,Φ,α,β 

W+ 

N 

∑ 
s∈S 

nszs + W 
− 

N 

∑ 
t∈T 

ntzt + 
P∑ 

j=1 
Φj 

such that Mszs �1− 
P∑ 

j=0 
λjxs,j s∈S .error on s/, .4a/ 

Mtzt � 
P∑ 

j=0 
λjxt,j t ∈T .error on t/, .4b/ 



Recidivism Prediction 701 

1= z + zt ∀ s, t : x =xt , y =−yt .conflicting labels/, .4c/ 
Φj =C0αj + �βj j =1, : : : , P .penalty for coefficientj/, .4d/ 

−Λjαj �λj �Λjαj j =1, : : : , P .l0-norm/ .4e/ 
−βj �λj �βj j =1, : : : , P .l1-norm/, .4f/ 

λj ∈Z∩ [−Λj, Λj] j =0, : : : , P .coefficient set/, 
zs, zt ∈{0, 1} s∈S, t ∈T .loss variables/, 
Φj ∈R+ j =1, : : : , P .penalty variables/, 

αj ∈{0, 1} j =1, : : : , P .l0-variables/, 
βj ∈R+ j =1, : : : , P .l1-variables/: 

The main difference between this formulation and that in expression (3) be that we compute the 
error rate of the classifier by use loss constraint that be express in term of the number of 
distinct point in the data set. Here, the set S represent the set of distinct point with positive 
labels, and the set T represent the set of distinct point with negative examples. The parameter 
n (and nt) count the number of time that a point of type s (or t) be found in the original data 
set so that Σsns =ΣNi=11.yi =1/, Σtnt =ΣNi=11.yi =−1/ and N =Σs n +Σt nt . 

The main computational benefit of this formulation be because 

(a) we can reduce the number of loss constraint by counting the number of repeat row in 
the data set and 

(b) we can directly encode a low bound on the error rate by counting the number of point 
s and t with identical feature but opposite label (i.e. x =xt but y =−yt). 

Here benefit (a) reduces the size of the problem that we pas to an IP solver, and benefit (b) 
produce a much strong low bound on the 0–1-loss (in comparison with the linear program- 
ming relaxation), which speed up the progress of branch-and-bound type algorithms. It would 
be possible to use this formulation on a data set without binary input variables, though it would 
not necessarily be effective because it could be much less likely for a data set to contain repeat 
row in such a setting. 

Another subtle benefit of this formulation be that the margin for the negative point be 0 
whereas the margin for the positive point be 1. This mean that, for positive points, we have a 
correct prediction if and only if the score be 1 or greater. For negative points, we have a correct 
prediction if and only if the score be 0 or less. This provide a slight computational advantage 
since the negative point do not need to have score below −1 to be correctly classified, which 
reduces the size of the big M parameter and the coefficient set. For instance, say that we would 
like to produce a linear model that encodes ‘predict rearrest unless a1 or a2 be true’. Using the 
previous formulation with the margin of γ ∈ .0, 1/ on both positive and negatives, the optimal 
SLIM classifier would be ‘rearrest = sgn.1−2a1 −2a2/’. In contrast, the margin of the current 
formulation be ‘rearrest = sgn.1 − a1 − a2/’, which us small coefficient and produce a 
slightly simpler model. 

3.4. Active set polish 
On large data sets, IP solver may take a long time to produce an optimal solution or to provide 
user with a certificate of optimality. Here, we present a polish procedure that can be use to 



702 J. Zeng, B. Ustun and C. Rudin 

improve the quality of solution locally. For a fix set of features, this procedure optimizes the 
value of coefficients. 

The polish procedure take a input a feasible set of coefficient from the SLIM IP λfeasible 

and return a polished set of coefficient λpolished by solve a simpler IP formulation show 
in expression (5). The polish IP optimizes only the coefficient of feature that belong to the 
active set of λfeasible, i.e. the set of feature with non-zero coefficient A :={j :λfeasiblej �=0}. The 
coefficient for feature that do not belong to the active set be fix to 0 so that λj =0 for j �∈A. 
In this way, the optimization no longer involves feature selection, and the formulation becomes 
much easy to solve. 

min 
λ,z,Φ,α,β 

W+ 

N 

∑ 
s∈S 

nszs + W 
− 

N 

∑ 
t∈T 

ntzt .5a/ 

such that Mszs �1− 
∑ 

j∈A 
λjxs,j s∈S .error on s/, .5b/ 

Mtzt � 
∑ 

j∈A 
λjxt,j t ∈T .error on t/, .5c/ 

1= z + zt ∀ s, t : x =xt , y =−yt .conflicting labels/, .5d/ 

λj ∈Z∩ [−Λj, Λj] j ∈A .coefficient set/, 

zs, zt ∈{0, 1} s∈S, t ∈T .loss variables/: 
The polish IP formulation be especially fast to solve to optimality for classification problem 
with binary input variable because this limit the number of loss constraints. Say for instance 
that we wish to polish a set of coefficient with only five non-zero variables; then there be at most 
|{−1, 1}|× |{0, 1}5|=64 possible unique data points, and thus the same number of possible loss 
constraints. 

In our experiment in Section 4, we use the polish procedure on all the feasible solution 
that we find from the early formulation. In all cases, we can solve the polish IP problem to 
optimality within a few second (i.e. an optimality gap of 0.0%). 

4. Experimental result 

In this section, we compare the accuracy and interpretability of recidivism prediction model 
from SLIM to model from eight other popular classification methods. In Section 4.1, we explain 
the experimental set-up that be use for all the methods. In Section 4.2, we compare the 
predictive accuracy of the method with the area under the curve value AUC and ROC curves. 
In Sections 4.3 and 4.4, we evaluate the interpretability of the models. Finally, in Section 4.5, 
we present the score system that be generate by SLIM. 

4.1. Methodology 
In what follow we discus cost-sensitive classification for imbalanced problem and provide an 
overview of techniques. 

4.1.1. Evaluating predictive accuracy for imbalanced problem 
The majority of classification problem that we consider be imbalanced , where the data contain 



Recidivism Prediction 703 

a relatively small number of example from one class and a relatively large number of example 
from the other. 

Imbalanced problem necessitate change in the way that we evaluate the performance of 
classification models. Consider, for instance, a heavily imbalanced problem such a fatal 
violence where only P.yi = 1/ = 0:7% of individual be arrest within 3 year of be 
release from prison. In this case, a method that maximizes overall classification accuracy be 
likely to produce a trivial model that predicts that no one will be arrest for fatal offences— 
a result that be not surprising give that the trivial model be 99.3% accurate on the overall 
population. Unfortunately, this model will never be able to identify individual who will be 
arrest for a fatal offence, and therefore it will be 0% accurate on the population of interest. 

To provide a measure of classification model performance on imbalanced problems, we ass 
the accuracy of a model on the positive and negative class separately. In our experiments, we 
report the class-based accuracy of each model by use the true positive rate TPR, which reflect 
the accuracy on the positive class, and the false positive rate FPR, which reflect the error rate 
on the negative class. For a give classification model, we compute these quantity a 

TPR= 1 
N+ 

∑ 
i∈I+ 

1.ŷi =1/, 

FPR= 1 
N− 

∑ 
i∈I− 

1.ŷi =1/, 

where ŷi denotes the predict outcome for example i, N 
+ denotes the number of example in 

the positive class I+ = {i : yi = 1} and N− denotes the number of example from the negative 
class I− ={i : yi =−1}. Ideally, a classification model should have high TPR and low FPR (i.e. 
TPR close to 1 and FPR = 0). 

Most classification method can be adapt to yield a model that be more accurate on the 
positive class, but only if we be willing to sacrifice some accuracy on example from the negative 
class, and vice versa. To illustrate the trade-off of classification accuracy between positive and 
negative classes, we plot all model that be produce by a give method a point on an ROC 
curve, which plot TPR on the vertical axis and FPR on the horizontal axis. Having construct 
an ROC curve, we then ass the overall performance of each method by calculate the area 
under the ROC curve, AUC. (We note that AUC be a summary statistic that be frequently misuse 
in the context of classification problems. It be true that a method with AUC = 1 always produce 
model that be more accurate than a method with AUC = 0. Other than this simple case, 
however, it be not possible to state that a method with high AUC always produce model that 
be more accurate than a method with low AUC.) A detailed discussion of ROC analysis in 
recidivism prediction can be found in the work of Maloof (2003). 

4.1.2. Fitting model over the full receiver operating characteristic curve by use a cost- 
sensitive approach 
Different application require predictive model at different point of the ROC curve. Models 
for sentencing, for example, need low FPR to avoid predict that a low risk individual will 
reoffend. Models for screening, however, need high TPR to capture a many high risk individual 
a possible. In our experiments, we use a cost-sensitive approach to produce classification model 
at different point of the ROC curve (see, for example, Berk (2010, 2011)). This approach involves 
control the accuracy on the positive and negative class by tune the misclassification cost 
for example in each class. In what follows, we denote the misclassification cost on example 
from the positive and negative class a W+ and W− respectively. As we increase W+, the cost 



704 J. Zeng, B. Ustun and C. Rudin 

of make a mistake on a positive example increases, and we expect to obtain a model that 
classifies the positive example more accurately (i.e. with high TPR). We choose W+ and W− 
so W+ +W− =2. Thus, when W+ =2, we obtain a trivial model that predicts ŷi =1 and attains 
TPR = 1. When W+ =0, we obtain a trivial model that predicts ŷi =−1 that attains FPR = 0. 

4.1.3. Choice of classification method 
We compare SLIM score system with model produce by eight popular classification 
methods, include those previously use for recidivism prediction (see Section 1.2) or those 
that ranked among the ‘top 10 algorithm in data mining’ (Wu et al., 2008). In choose these 
methods, we restrict our attention to method that have publicly available software package 
and allow user to specify misclassification cost for positive and negative classes. Our final 
choice of method include the follow methods. 

(a) C5.0 tree and C5.0 rules: C5.0 be an update version of the popular C4.5 algorithm 
(Quinlan, 2014; Kuhn and Johnson, 2013) that can create decision tree and rule sets. 

(b) CART : the CART method be a popular method to create decision tree through recursive 
partition of the input variable (Breiman et al., 1984). 

(c) L1- and L2-penalized logistic regression be variant of logistic regression that penalize 
the coefficient to prevent overfitting (Friedman et al., 2010). L1-penalized method be 
typically use to create linear model that be sparse (Tibshirani, 1996; Hesterberg et al., 
2008). The L2-regularized method be call ‘ridge’ regression and be not generally 
sparse. 

(d) Random forest be a popular black box method that make prediction by use a large 
ensemble of weak classification trees. The method be originally developed by Breiman 
(2001a) but be widely use for prediction of recidivism (see, for example, Berk et al. (2009) 
and Ritter (2013)). 

(e) SVMs be a popular black box method for non-parametric linear classification. The RBF 
kernel let the method handle classification problem where the decision boundary may 
be non-linear (see, for example, Cristianini and Shawe-Taylor (2000) and Berk and Bleich 
(2014)). 

(f) SGB be a popular black box method that creates prediction model in the form of an 
ensemble of weaker prediction model (Friedman, 2001; Freund and Schapire, 1997). 

4.1.4. Details on experimental design, parameter tune and computation 
We summarize the methods, software and setting that we use in our experiment in Table 4. 

For each of the six recidivism prediction problem and each of the nine methods, we con- 
structed ROC curve by run the algorithm with 19 value of W+. The value of W+ 
be chosen to produce model across the full ROC curves. By default, we chose value of 
W+ ∈{0:1, 0:2, : : : , 1:9} and set W− =2−W+. These value of W+ be inappropriate for prob- 
lem with a significant class imbalance a all method produce trivial models. Thus, for sig- 
nificantly imbalanced problems, such a domestic violence and sexual violence, we 
use value of W+ ∈ {1:815, 1:820, : : : , 1:995}. For fatal violence, which be extremely 
imbalanced, we use W+ ∈{1:975, 1:976, : : : , 1:995}. 

This set-up require u to produce a total of 1026 recidivism prediction model (six recidivism 
problem time nine method time 19 imbalance ratios). Each of the 1026 model be built 
on a training set and their performance be assess out of sample. In particular, a third of 
the data be reserve a the test set. The remain two-thirds of the data be the training set. 
During training, we use fivefold nest cross-validation (CV) for parameter tuning. Explicitly, 



Recidivism Prediction 705 

Table 4. Methods, software and free parameter use to train model for all six recidivism prediction 
problems† 

Method Software Free parameter and setting 

CART decision tree rpart (Therneau et al., 2012) minSplit ∈ .3, 5, 10, 15, 20/× CP ∈ .0:0001, 
0:001, 0:01/ 

C5.0T decision tree c50 (Kuhn et al., 2012) Default setting 
C5.0R decision rule c50 (Kuhn et al., 2012) Default setting 
Logistic regression (lasso) 

(L1-penalty) 
glmnet (Friedman et al., 2010) 100 value of L1-penalty chosen by glmnet 

Logistic (ridge) regression 
(L2-penalty) 

glmnet (Friedman et al., 2010) 100 value of L2-penalty chosen by glmnet 

Random forest randomForest (Liaw and Wiener, 
2002) 

sampsize ∈ .0:632N, 0:4N, 0:2N/× nodesize 
∈ .1, 5, 10, 20/ with unbounded tree depth 

SVMs (radial basis kernel) e1071 (Meyer et al., 2012) C ∈ .0:01, 0:1, 1, 10/× 
γ ∈ .1=.10P/, 1=.5P/, 1=.2P/, 1=P , 2=P , 5=P , 
10=P/ 

SGB (Adaboost) gbm (Ridgeway, 2006) shrinkage ∈ .0:001, 0:01, 0:1/ × 
interaction.depth ∈ .1, 2, 3, 4/× ntrees ∈ 
.100, 500, 1500, 3000/ 

SLIM score system CPLEX 12.6 (Ustun, 2016) C0 and � set to find the most accurate model 
with � 8 coefficient where λ0 ∈{−100, : : : , 
100} and λj ∈{−10, : : : , 10} 

†We ran each method for 19 value of W+ and all combination of free parameter list in the table. For each 
value of W+, we select the model that minimize the mean weight fivefold CV error. The value of W+ be 
problem specific (see Section 4.1.4 for details). 

the training data be split into five folds, and one of those be reserve a the validation fold. 
The validation fold be rotate to select free parameter values, and a final model be train 
on the full training set (two-thirds) with the select parameter value and it performance be 
assess on the test set (a third). The fold be generate once to allow for comparison across 
method and prediction problems. The parameter be chosen during nest CV to minimize 
the mean weight fivefold CV error on the training set. Having obtain a set of 19 different 
model for each method and each problem, we then construct an ROC curve for that method 
on that problem by plot the test TPR and test FPR of the 19 final models. 

We train all baseline method by use publicly available package in R 3.2.2 (R Core Team, 
2015) without impose any time constraints. In comparison, we train SLIM by solve IP 
problem with the CPLEX 12.6 application program interface in MATLAB 2013a. We solve 
each IP through the follow procedure: we train the solver on the formulation in Section 
3.3 for a total of 4 h on a local compute cluster with 2.7-GHz central processor units. Each 
time we solve an IP problem we kept 500 feasible solution and polished them by use the 
formulation in Section 3.4. We then use the same nest CV procedure a the other method 
to tune the number of term in the final model. Polishing all 500 solution take less than 1 min 
of compute time. Thus, the total number of optimization problem that we solve be 500 
polish IP problem time (five fold plus one final model) time six problem time 19 value 
of W+ =342000 IP problems. 

4.2. Observations on predictive accuracy 
We show ROC curve for all method and prediction problem in Fig. 1 and summarize the test 
AUC of each method in Table 5. Tables with the training and fivefold CV validation AUCs for 
all method be include in Appendix A. 



706 J. Zeng, B. Ustun and C. Rudin 

0% 

20% 

40% 

60% 

80% 

100% 

0% 20% 40% 60% 80% 100% 
Test FPR 

Te 
st 

T 
P 

R 

0% 

20% 

40% 

60% 

80% 

100% 

0% 20% 40% 60% 80% 100% 
Test FPR 

Te 
st 

T 
P 

R 

0% 

20% 

40% 

60% 

80% 

100% 

0% 20% 40% 60% 80% 100% 
Test FPR 

Te 
st 

T 
P 

R 

0% 

20% 

40% 

60% 

80% 

100% 

0% 20% 40% 60% 80% 100% 
Test FPR 

Te 
st 

T 
P 

R 

0% 

20% 

40% 

60% 

80% 

100% 

0% 20% 40% 60% 80% 100% 
Test FPR 

Te 
st 

T 
P 

R 

0% 

20% 

40% 

60% 

80% 

100% 

0% 20% 40% 60% 80% 100% 
Test FPR 

Te 
st 

T 
P 

R 

●● ● ● ● ● ● ● ● ● Boosting C5.0R C5.0T CART Lasso Ridge RF SLIM SVM RBF 

(a) (b) 

(c) (d) 

(e) (f) 

Fig. 1. ROC curve for general recidivism-related prediction problem with test data (all model per- 
form similarly except for the C5.0R, C5.0T and CART models): (a) arrest problem; (b) drug problem; 
(c) general violence problem; (d) domestic violence problem; (e) sexual violence problem; 
(f) data violence problem 



Recidivism Prediction 707 

Table 5. Test AUC for all method on all prediction problems† 

Prediction problem Results for the follow methods: 

Lasso Ridge C5.0R C5.0T CART Random SVM SGB SLIM 
regression forest RBFs 

arrest 0.72 0.73 0.72 0.72 0.68 0.73 0.72 0.73 0.72 
drug 0.74 0.74 0.63 0.63 0.59 0.75 0.73 0.75 0.74 
general violence 0.72 0.72 0.56 0.57 0.56 0.71 0.70 0.72 0.71 
domestic violence 0.77 0.77 0.50 0.50 0.53 0.64 0.77 0.78 0.76 
sexual violence 0.72 0.72 0.50 0.50 0.51 0.54 0.69 0.70 0.70 
fatal violence 0.67 0.68 0.50 0.50 0.50 0.50 0.69 0.70 0.62 

†Each cell contains the test AUC. 

We make the follow important observations, which we believe carry over to a large class 
of problem beyond prediction of recidivism. 

(a) All method do well on the general recidivism prediction problem arrest. In this case, 
we observe only small difference in predictive accuracy of different methods: all method 
other than CART attain a test AUC above 0.72; the high test AUC of 0.73 be achieve 
by SGB, ridge regression and random forests. This multiplicity of good model reflect 
the Rashomon effect of Breiman (2001a). 

(b) Major difference between method appear in their performance on imbalanced pre- 
diction problems. We expect different method to respond differently to change in the 
misclassification cost and therefore train each method over a large range of possible 
misclassification costs. Even so, it be difficult (if not impossible) to tune certain method 
to produce model at certain point of the ROC curve (see, for example, problem with 
significant imbalance, such a fatal violence). 

(c) the SVM RBFs, SGB, the lasso and ridge regression could produce accurate model at dif- 
ferent point on the ROC curve for most problems. SGB usually achieve the high AUC 
on most problem (e.g. arrest, drug, general violence, domestic violence 
and fatal violence). The lasso, ridge regression and the SVM RBFs often produce 
comparable AUCs. We find that these method respond well to cost-sensitive tuning, but 
it be difficult to tune the misclassification cost for highly imbalanced problems, such a 
fatal violence, to obtain model at specific point on the ROC curve. 

(d) the C5.0T, C5.0R and CART method could not produce accurate model at different 
point on the ROC curve on any imbalanced problems. We found that these method 
do not respond well to cost-sensitive tuning. The issue becomes markedly more severe 
a problem become more imbalanced. For drug and general violence, for in- 
stance, these method could not produce model with high TPR. For fatal violence, 
sexual violence and domestic violence, these method almost always pro- 
duced trivial model that predict y = −1 (resulting in AUCs of 0.5). This result may 
be attribute to the greedy nature of the algorithm that be use to fit the trees, a 
oppose to the use of tree model in general. The issue be unlikely to be software re- 
lated a it affect both C5.0 and CART method and have be observe by others (see, 
for example, Goh and Rudin (2014)). This problem might not occur if tree be good 
optimized. 



708 J. Zeng, B. Ustun and C. Rudin 

0% 

20% 

40% 

60% 

80% 

100% 

0% 20% 40% 60% 80% 100% 
Mean Score 

Fr 
ac 

tio 
n 

of 
T 

ru 
e 

Po 
si 

tiv 
e 

Lasso Ridge SLIM 

Fig. 2. Risk calibration plot for arrest base on test data: we compare three model chosen at a similar 
decision point, with test FPR � 50%; although it be not a risk assessment tool, we see that SLIM be well 
calibrate 

(e) In general, SLIM produce model that be close to or on the efficient frontier of the 
ROC curve, despite be restrict to a relatively small class of simple linear model (at 
most eight non-zero coefficient from −10 to 10). Even on highly imbalanced problem 
such a domestic violence and sexual violence, it responds well to change in 
misclassification cost (as expected, by nature of it formulation). 

In addition to predictive accuracy, we also examine the risk calibration of the models. Fig. 2 
show the risk calibration for arrest, construct by use the binning method from Zadrozny 
and Elkan (2002). We include calibration plot for all other problem in an extend version 
of appendix B of the on-line supplement. We see that SLIM be well calibrated, even though 
there be no reason why it should be; it be a decision-making tool, not a risk assessment tool. 
For arrest, the lasso and ridge regression be well calibrated; however, they lose this quality 
once we consider only sparse model (see appendix D of the on-line supplement). This property 
would also be lose if the lasso and ridge regression coefficient be rounded. 

4.3. Trade-offs between accuracy and interpretability 
In Appendix C, we show that the baseline method cannot maintain the same level of accuracy 
a they have in Section 4.2 when their model size be constrained. For the lasso, ridge regression 
and SLIM, model size be define a the number of feature in the model. For the CART and C5.0 
methods, model size be the number of leaf or rules. In fact, we find that the only method that 
can consistently produce accurate model along the full ROC curve and also have the potential 
for interpretability be SLIM and the (non-sparse) lasso. 

Tree and rule-based method such a the CART, C5.0T and C5.0R method be gener- 
ally unable to produce model that attain high degree of accuracy. Worse, even for balance 



Recidivism Prediction 709 

Table 6. SLIM score system for arrest† 

Predict arrest for any offence if score > 1 
1, age at release 18 to 24 2 point · · · 
2, prior arrests�5 2 point +· · · 
3, prior arrest for misdemeanor 1 point +· · · 
4, no prior arrest −1 point +· · · 
5, age at release�40 −1 point +· · · 
Add point from row 1–5 Score =· · · 

†This model have a test TPR/FPR of 76.6%/44.5%, and a mean 
fivefold CV TPR/FPR of 78.3%/46.5%. 

Table 7. Lasso model for arrest, with coefficient round 
to two significant digits† 

Predict arrest for any offence if score > 0:31 
1, prior arrests�5 0.63 point · · · 
2, age 1st confinement 18 to 24 0.15 point +· · · 
3, prior arrest for property 0.09 point +· · · 
4, prior arrest for misdemeanor 0.05 point +· · · 
5, age at release�40 −0.20 point +· · · 
Add point from row 1–5 Score =· · · 

†This model have a test TPR/FPR of 70.9%/43.8%, and a mean 
fivefold CV TPR/FPR of 72.2%/44.0%. 

problem such a arrest, where these method do produce accurate models, the model be 
complicate and use a very large number of rule or leaf (similar behaviour for C5.0T or 
C5.0R be also observe by, for instance, Lim et al. (2000)). As we show in Appendix C, it be 
not reasonably possible to obtain a C5.0R, C5.0T or CART model with at most eight rule or 
eight leaf for almost every prediction problem. 

4.4. On the interpretability of equally accurate transparent model 
To ass the interpretability of different models, we provide a comparison of predictive model 
produce by the SLIM, lasso and CART method for the arrest problem in Tables 6 and 7 and 
Fig. 3. This set-up provide a nice basis for comparison a all three method produce model at 
roughly the same decision point, and with the same degree of sparsity. For this comparison, we 
consider any transparent model with at most eight coefficient (the lasso), eight rule (C5.0R) 
or eight leaf (C5.0T and CART) and have a test FPR of below 50%. We report the model with 
the minimum weight test error. Here, neither C5.0R nor C5.0T could produce an acceptable 
model with at most eight rule or eight leaves, so only model from the SLIM, CART and lasso 
method could be displayed. As described before, it be rare for the lasso and CART method to 
produce model with a similar degree of accuracy to SLIM when the model size be constrained. 
We make the follow observations. 

(a) All three model attain similar level of predictive accuracy. Test TPR-values range 
between 70% and 79% and test FPR-values range between 43% and 48%. There may 
not be a classification model that can attain substantially high accuracy. 



710 J. Zeng, B. Ustun and C. Rudin 

Fig. 3. CART model for arrest: this model have a test TPR/FPR of 79.1%/47.9%, and a mean fivefold CV 
TPR/FPR of 79.9%/48.5% 

Table 8. SLIM score system for drug† 

Predict arrest for drug offence if score > 7 
1, prior arrest for drug 9 point · · · 
2, age at release 18 to 24 5 point +· · · 
3, age at release 25 to 29 3 point +· · · 
4, prior arrest for multiple type of crime 2 point +· · · 
5, prior arrest for property 1 point +· · · 
6, age at release 30 to 39 −1 point +· · · 
7, no prior arrest −6 point +· · · 
Add point from row 1–7 Score =· · · 

†This model have a test TPR/FPR of 85.7%/51.1%, and a mean fivefold 
CV TPR/FPR of 82.3%/49.7%. 

(b) The SLIM model us five input variable and small integer coefficient (see, for example, 
Table 6). There be a natural rule-based interpretation. In this case, the model implies that, 
if the prisoner be young (age at release of 18 to 24) or have a history of arrest (prior 
arrests�5), he be highly likely to be rearrested. In contrast, if he be relatively old (age 
at release�40) or have no history of arrest (no prior arrests), he be unlikely to commit 
another crime. 

(c) The CART model also allows user to make prediction without a calculator. In com- 
parison with the SLIM model, however, the hierarchical structure of the CART model 
make it difficult to gauge the relationship of each input variable on the predict out- 
come. Consider, for instance, the relationship between age at release and the outcome. 
In this case, user be immediately aware that there be an effect, a the model branch 
on the variable age at release�40 and age at release 18 to 24. However, the effect 
be difficult to comprehend since it depends on prior arrest for misdemeanour: if prior 
arrests�5 = 1 and age at release 18 to 24 = 1 then the model predicts ŷ=1; if prior 
arrests�5 = 0 and age at release�40 = 0 then ŷ = 1; however, if prior arrests�5 = 0 
and age at release�40 = 1 then ŷ =1 only if prior arrest for misdemeanor = 1. Such 
issue do not affect linear model such a SLIM and the lasso, where user can immedi- 
ately gauge the direction and strength of the relationship between an input variable and 
the predict outcome by the size and sign of a coefficient. The literature on interpretabil- 
ity in machine learn indicates that interpretability be domain specific; there be some 
domain where logical model be prefer over linear models, and vice versa (e.g. Freitas 
(2014)). 



Recidivism Prediction 711 

Table 9. SLIM score system for general violence† 

Predict arrest for general violence offence if score > 7 
1, prior arrest for general violence 8 point · · · 
2, prior arrest for misdemeanor 5 point +· · · 
3, infraction in prison 3 point +· · · 
4, prior arrest for local ord 3 point +· · · 
5, prior arrest for property 2 point +· · · 
6, prior arrest for fatal violence 2 point +· · · 
7, prior arrest with firearm involve 1 point +· · · 
8, age at release�40 −7 point +· · · 
Add point from row 1–8 Score =· · · 

†This model have a test TPR/FPR of 76.7%/45.4%, and a mean 
fivefold CV TPR/FPR of 76.8%/47.6%. 

Table 10. SLIM score system for domestic violence† 

Predict arrest for domestic violence offence if score > 3 
1, prior arrest for misdemeanor 4 point · · · 
2, prior arrest for felony 3 point +· · · 
3, prior arrest for domestic violence 2 point +· · · 
4, age 1st confinement 18 to 24 1 point +· · · 
5, infraction in prison −5 point +· · · 
Add point from row 1–5 Score =· · · 

†This model have a test TPR/FPR of 85.5%/46.0%, and a mean 
fivefold CV TPR/FPR of 81.4%/48.0%. 

Table 11. SLIM score system for sexual violence† 

Predict arrest for sexual violence offence if score > 2 
1, prior arrest for sexual violence 3 point · · · 
2, prior arrests� 5 1 point +· · · 
3, multiple prior jail time 1 point +· · · 
4, prior arrest for multiple type of −1 point +· · · 

crime 
5, no prior arrest −2 point +· · · 
Add point from row 1–5 Score =· · · 

†This model have a test TPR/FPR of 44.3%/17.7%, and a mean 
fivefold CV TPR/FPR of 43.7%/19.9%. 

4.5. Scoring system for recidivism prediction 
We show a SLIM score system for each of the prediction problem that we consider in Tables 
8–12. The model be chosen at specific decision points, with the constraint that the fivefold CV 
FPR� 50% except for sexual violence, which be chosen at fivefold CV FPR� 20%. The 
model that be present here may be suitable for screen tasks. To obtain a model that be 
suitable for sentencing, a point on the ROC curve with a much high TPR would be needed. 
We note that these model generalize well from the data set, which be evident by the close match 
between test TPR/FPR (Table 5) and training TPR/FPR (Table 13 in Appendix A). 



712 J. Zeng, B. Ustun and C. Rudin 

Table 12. SLIM score system for fatal violence† 

Predict arrest for fatal violence offence if score > 4 
1, age 1st confinement�17 5 point · · · 
2, prior arrest with firearm involve 3 point +· · · 
3, age 1st confinement 18 to 24 2 point +· · · 
4, prior arrest for felony 2 point +· · · 
5, age at release 18 to 24 1 point +· · · 
6, prior arrest for drug 1 point +· · · 
Add point from row 1–6 Score =· · · 

†This model have a test TPR/FPR of 55.4%/35.5%, and a mean 
fivefold CV TPR/FPR of 64.2%/42.4%. 

Many of these model exhibit the same ‘rule-like’ tendency a discuss in Section 4.4. For 
example, the model for drug in Table 8 predicts that a person will be arrest for a drug- 
related offence if he or she have ever have any prior drug offences. Similarly, the model for 
sexual violence in Table 11 effectively state that a person will be rearrested for a sexual 
offence if and only if he or she have a prior history of sexual crimes. For completeness, we include 
comparison with other model in Appendix B. 

5. Discussion 

Our paper merges two perspective on model recidivism: the first be to obtain accurate 
predictive model by use the most powerful machine learn tool that be available, and the 
second be to create model that be easy to use and understand. 

We use a set of feature that be commonly accessible to police officer and judges, and 
compare the ability of various machine learn method to produce model at different deci- 
sion point across the ROC curve. Our result suggest that it be possible for traditional methods, 
such a ridge regression, to perform just a well a more modern methods, such a SGB—a 
find that be in line with the work of Tollenaar and van der Heijden (2013) and Yang et al. 
(2010). Further, we found that even simple model may perform surprisingly well, even when 
they be fitting from a heavily constrain space—a find that be in line with work on the 
surprising performance of simple model (see, for example, Dawes (1979) and Holte (1993, 
2006)). 

Our study show that there may be major advantage of use SLIM for prediction of recidi- 
vism, a it can dependably produce a simple score system that be accurate and interpretable on 
any decision point along the ROC curve. Interpretability be crucial for many of the high stake 
application where recidivism prediction model be be used. In such applications, it be not 
enough for the decision maker to know what input variable be be use to train the model, or 
how individual input variable be related to the outcome; decision maker should know how the 
model combine all the input variable to generate it predictions, and whether this mechanism 
aligns with their ethical values. SLIM not only show this mechanism but also accommodates 
constraint that be design to align the prediction model with the ethical value of the decision 
maker. 

In comparison with current machine learn methods, the main drawback of run SLIM 
be increase computation involve in solve an IP problem. For this, we propose two new 
technique to reduce computation involve in training high quality SLIM score systems: 



Recidivism Prediction 713 

(a) a polish procedure that improves the quality of feasible solution that be found by an 
IP solver and 

(b) an IP formulation that make it easy for an IP solver to provide a certificate of optimality. 

In our experiments, the time that be require to train SLIM be ultimately comparable with the 
time that be require to train random forest or SGB. However, it be still significant compare 
with the time that be require for other method such a the CART, C5.0 method and penalize 
logistic regression. In theory, the computation that be require to find an optimal solution to the 
SLIM integer programme be ‘NP hard’, meaning that the run time increase exponentially with 
the number of features. In practice, the run time depends on several factors, such a the number of 
samples, the number of dimensions, the underlie ease of the classification and how the data be 
encoded. Since most criminological problem cannot by nature involve massive data set (since 
each observation be a person), and since computer speed of solve million of instruction per 
second also increase exponentially, it be possible that mathematical program technique 
like SLIM be well suit to criminological problem that be substantially large and more 
complex than the problem that be discuss in this work. 

Acknowledgement 

Jiaming Zeng and Berk Ustun contribute equally to this work. 

Appendix A: Additional result on predictive accuracy 

To supplement the experimental result in Section 4.2, we include the training and fivefold CV results. 
Table 13 show the training AUC performance for all method on all prediction problems, and Table 14 
show the fivefold CV AUC performance for all methods. A table of test AUCs for all method on all 
prediction problem can be found in Table 5. 

Appendix B: Model-based comparison 

In Section 4, we include a comparison of transparent model produce for the arrest problem. Here, 
we include a similar comparison for all other recidivism prediction problems. 

The model and calibration plot that be show here correspond to the best model that we produce 
by use the lasso and ridge regression (i.e. the model that be plot a point in Fig. 1). We omit 
CART result and C5.0 model be show because all model that be produce be either trivial or 

Table 13. Training AUC for all method on all regression prediction problem 

Prediction problem Results for the follow methods: 

Lasso Ridge C5.0R C5.0T CART Random SVM SGB SLIM 
regression forest RBFs 

arrest 0.73 0.73 0.73 0.73 0.81 0.73 0.87 0.75 0.72 
drug 0.74 0.73 0.65 0.66 0.76 0.73 0.85 0.77 0.73 
general violence 0.71 0.71 0.58 0.59 0.77 0.71 0.84 0.74 0.71 
domestic violence 0.77 0.77 0.50 0.50 0.75 0.64 0.88 0.81 0.76 
sexual violence 0.71 0.71 0.50 0.50 0.84 0.55 0.86 0.77 0.71 
fatal violence 0.75 0.74 0.50 0.50 0.50 0.51 0.90 0.84 0.73 



714 J. Zeng, B. Ustun and C. Rudin 

Ta 
b 

le 
14 

. 
F 

iv 
ef 

ol 
d 

C 
V 

A 
U 

C 
fo 

r 
al 

lm 
et 

ho 
d 

on 
al 

lp 
re 

di 
ct 

io 
n 

pr 
ob 

le 
m 

s† 

P 
re 

di 
ct 

io 
n 

pr 
ob 

le 
m 

R 
e 

ul 
t 

fo 
r 

th 
e 

fo 
llo 

w 
in 

g 
m 

et 
ho 

d 
: 

L 
a 

so 
R 

id 
ge 

C 
5. 

0R 
C 

5. 
0T 

C 
A 

R 
T 

R 
an 

do 
m 

S 
V 

M 
S 

G 
B 

S 
L 

IM 
re 

gr 
e 

si 
on 

fo 
re 

st 
s 

R 
B 

F 

a 
r 

r 
e 

s 
t 

0. 
72 

0. 
73 

0. 
71 

0. 
71 

0. 
67 

0. 
73 

0. 
71 

0. 
73 

0. 
72 

0. 
72 

–0 
.7 

4 
0. 

72 
–0 

.7 
4 

0. 
71 

–0 
.7 

3 
0. 

70 
–0 

.7 
2 

0. 
66 

–0 
.6 

9 
0. 

72 
–0 

.7 
4 

0. 
70 

–0 
.7 

2 
0. 

72 
–0 

.7 
4 

0. 
71 

–0 
.7 

3 
d 

r 
u 

g 
0. 

73 
0. 

73 
0. 

62 
0. 

62 
0. 

59 
0. 

73 
0. 

72 
0. 

74 
0. 

72 
0. 

72 
–0 

.7 
4 

0. 
71 

–0 
.7 

4 
0. 

61 
–0 

.6 
4 

0. 
61 

–0 
.6 

4 
0. 

58 
–0 

.6 
0 

0. 
72 

–0 
.7 

4 
0. 

71 
–0 

.7 
3 

0. 
72 

–0 
.7 

4 
0. 

71 
–0 

.7 
3 

g 
e 

n 
e 

r 
a 

l 
v 

i 
o 

l 
e 

n 
c 

e 
0. 

71 
0. 

71 
0. 

56 
0. 

57 
0. 

56 
0. 

70 
0. 

69 
0. 

71 
0. 

70 
0. 

70 
–0 

.7 
1 

0. 
70 

–0 
.7 

1 
0. 

55 
–0 

.5 
7 

0. 
55 

–0 
.5 

9 
0. 

55 
–0 

.5 
8 

0. 
69 

–0 
.7 

1 
0. 

69 
–0 

.7 
0 

0. 
70 

–0 
.7 

1 
0. 

69 
–0 

.7 
1 

d 
o 

m 
e 

s 
t 

i 
c 

v 
i 

o 
l 

e 
n 

c 
e 

0. 
76 

0. 
76 

0. 
50 

0. 
50 

0. 
53 

0. 
63 

0. 
76 

0. 
77 

0. 
75 

0. 
75 

–0 
.7 

9 
0. 

75 
–0 

.7 
8 

0. 
50 

–0 
.5 

0 
0. 

50 
–0 

.5 
0 

0. 
51 

–0 
.5 

4 
0. 

59 
–0 

.6 
6 

0. 
74 

–0 
.7 

8 
0. 

75 
–0 

.7 
9 

0. 
72 

–0 
.7 

8 
s 

e 
x 

u 
a 

l 
v 

i 
o 

l 
e 

n 
c 

e 
0. 

70 
0. 

69 
0. 

50 
0. 

50 
0. 

51 
0. 

54 
0. 

67 
0. 

68 
0. 

68 
0. 

68 
–0 

.7 
4 

0. 
66 

–0 
.7 

4 
0. 

50 
–0 

.5 
0 

0. 
50 

–0 
.5 

0 
0. 

50 
–0 

.5 
1 

0. 
53 

–0 
.5 

5 
0. 

63 
–0 

.7 
0 

0. 
65 

–0 
.7 

2 
0. 

66 
–0 

.7 
2 

f 
a 

t 
a 

l 
v 

i 
o 

l 
e 

n 
c 

e 
0. 

66 
0. 

67 
0. 

50 
0. 

50 
0. 

50 
0. 

51 
0. 

67 
0. 

67 
0. 

65 
0. 

59 
–0 

.7 
4 

0. 
62 

–0 
.7 

5 
0. 

50 
–0 

.5 
0 

0. 
50 

–0 
.5 

0 
0. 

50 
–0 

.5 
2 

0. 
50 

–0 
.5 

3 
0. 

63 
–0 

.7 
3 

0. 
61 

–0 
.7 

4 
0. 

61 
–0 

.6 
9 

†W 
e 

re 
po 

rt 
th 

e 
fiv 

ef 
ol 

d 
C 

V 
m 

ea 
n 

va 
lid 

at 
io 

n 
A 

U 
C 

.T 
he 

ra 
ng 

e 
un 

de 
rn 

ea 
th 

ea 
ch 

ce 
ll 

re 
pr 

e 
en 

t 
th 

e 
fiv 

ef 
ol 

d 
C 

V 
m 

in 
im 

um 
an 

d 
m 

ax 
im 

um 
. 



Recidivism Prediction 715 

contain too many leaf to be printed. For any give problem, the model operate at similar decision 
point (TPR) and be constrain to the same FPR-criteria a in Section 4.5. 

The calibration plot will appear to be flat for problem with significant class imbalance. Typically, a 
well-calibrated classifier on a problem without class imbalance should fall on the x = y line. However, 
because the y-axis be define a P{y=1|s.x/= s}, where s be the predict score of a model, the slope of the 
graph will be less than P.y=1/ by definition. Therefore, for a highly imbalanced problem such a fatal 
violence, where P.y =1/=0:7%, the plot will be flat. 

B.1. drug 
This be the SLIM model for drug. This model have a test TPR/FPR of 85.7%/51.1%, and a mean fivefold 
CV TPR/FPR of 82.3%/49.7%: 

9.00 prior arrest for drug 
+ 3:00 prior arrest for multiple type of crime 
− 1:00 age at release 30 to 39 

+ 5:00 age at release 18 to 24 
+ 1:00 prior arrest for property 
− 7:00 

+ 4.00 age at release 25 to 29 
− 6:00 no prior arrest 

This be the best lasso model for drug. This model have a test TPR/FPR of 82.0%/45.9%, and a mean 
fivefold CV TPR/FPR of 81.2 %/45.9%: 

1.14 prior arrest for drug 
+ 0.19 prior arrest for other violence 
+ 0.16 age at release 18 to 24 
+ 0.12 prior arrest for public order 
+ 0.06 age 1st arrest�17 
+ 0.03 multiple prior prison time 
− 0:25 prior arrest for sexual 
− 0.11 prior arrest with child involve 
− 1:11×10−03 time served�61mo 
+ 0.27 prior arrest for property 

+ 0.18 prior arrest for multiple type of 
crime 

+ 0.14 prior arrests�5 
+ 0.10 prior arrest with firearm involve 
+ 0.04 multiple prior jail time 
+ 0.03 any prior prb or fine 
− 0:23 age at release 30 to 39 
− 0:08 alcohol abuse 
− 1:01 
+ 0.26 time served�6mo 

+ 0.17 prior arrest for 
misdemeanor 

+ 0.13 age 1st confinement 18 
to 24 

+ 0.08 any prior jail time 
+ 0.04 drug abuse 
− 0:62 age at release�40 
− 0:12 time serve 25 to 60mo 
− 0:07 age 1st confinement�40 

This be the best ridge regression model for drug. This model have a test TPR/FPR of 84.0%/48.2%, and 
a mean fivefold CV TPR/FPR of 83.1%/48.4%: 

0.91 prior arrest for drug 
+ 0.21 prior arrest for multiple type of 

crime 
+ 0.17 prior arrest for other violence 
+ 0.13 prior arrest with firearm involve 
+ 0.11 prior arrest for public order 
+ 0.08 any prior jail time 
+ 0.06 multiple prior prison time 
+ 0.05 prior arrests�2 
+ 0.02 prior arrests�1 
+ 2:52×10−03 prior arrest for felony 
− 0:33 age at release�40 
− 0:16 prior arrest with child involve 
− 0:13 time served�61mo 
− 0:05 age 1st arrest�40 
− 0:03 age 1st arrest 30 to 39 
− 4:71×10−03 prior arrest for local ord 
− 1:09 

+ 0.25 time served�6mo 
+ 0.20 prior arrest for property 
+ 0.17 age 1st confinement 18 to 24 
+ 0.12 age at release 25 to 29 
+ 0.09 age 1st arrest�17 
+ 0.07 multiple prior jail time 
+ 0.06 release unconditonal 
+ 0.04 time serve 7 to 12mo 
+ 0.01 age 1st confinement 25 to 29 
+ 1:76×10−03 age 1st arrest 18 to 24 
− 0:25 prior arrest for sexual 
− 0:15 time serve 25 to 60mo 
− 0:10 prior arrest for domestic 

violence 
− 0:04 female 
− 0:02 age 1st confinement 30 to 39 
− 4:45×10−03 time serve 13 to 24mo 
+ 0.24 age at release 18 to 24 

+ 0.17 prior arrest for 
misdemeanor 

+ 0.14 prior arrests�5 
+ 0.11 drug abuse 
+ 0.08 age 1st confinement�17 
+ 0.07 age at release�17 
+ 0.05 any prior prb or fine 
+ 0.04 multiple prior prb or fine 
+ 0.01 release conditonal 
+ 9:58×10−04 prior arrest for 

fatal violence 
− 0:19 age 1st confinement�40 
− 0:14 alcohol abuse 
− 0:09 age at release 30 to 39 
− 0:04 infraction in prison 
− 0:02 no prior arrest 
− 2:23×10−03 age 1st arrest 25 

to 29 

B.2. general violence 
This be the SLIM model for general violence. This model have a test TPR/FPR of 76.7%/45.4%, and 
a mean fivefold CV TPR/FPR of 76.8%/47.6%: 

8 prior arrest for other violence 
+ 3 prior arrest for local ord 
+ prior arrest with firearm involve 

+ 5 prior arrest for misdemeanor 
+ 2 prior arrest for property 
− 7 age at release�40 

+ 3 infraction in prison 
+ 2 prior arrest for fatal violence 
− 7 



716 J. Zeng, B. Ustun and C. Rudin 

This be the best lasso model for general violence. This model have a test TPR/FPR of 79.7%/45.5%, 
and a mean fivefold CV TPR/FPR of 77.3%/45.7%: 

0.90 prior arrest for other violence 
+ 0.28 age at release 18 to 24 
+ 0.20 release unconditonal 
+ 0.14 prior arrest for fatal violence 
+ 0.10 prior arrests�5 
+ 0.09 infraction in prison 
+ 2:89×10−03 prior arrest for drug 
− 0:27 age at release 30 to 39 
− 0:05 age 1st arrest�40 
− 1:19 

+ 0.35 prior arrest for property 
+ 0.24 prior arrest for public order 
+ 0.17 age 1st confinement 18 to 24 
+ 0.14 age 1st confinement�17 
+ 0.10 prior arrest with firearm 

involve 
+ 0.04 time served�6mo 
− 0:72 age at release�40 
− 0:15 prior arrest with child involve 
− 0:01 time serve 25 to 60mo 

+ 0.28 prior arrest for misdemeanor 
+ 0.20 age 1st arrest�17 
+ 0.16 alcohol abuse 
+ 0.10 prior arrest for felony 
+ 0.10 age 1st arrest 18 to 24 
+ 0.03 time serve 7 to 12mo 
− 0:41 female 
− 0:07 age 1st confinement�40 
− 1:84×10−03 age 1st confinement 

30 to 39 

This be the best ridge regression model for general violence. This model have a test TPR/FPR of 
81.4%/48.1%, and a mean fivefold CV TPR/FPR of 80.0%/48.5%: 

0.62 prior arrest for other violence 
+ 0.23 prior arrest for misdemeanor 
+ 0.17 age 1st arrest�17 
+ 0.13 prior arrests�5 
+ 0.11 age 1st confinement�17 
+ 0.10 prior arrest for fatal violence 
+ 0.07 prior arrest for domestic violence 
+ 0.05 prior arrest for local ord 
+ 0.03 prior arrests�2 
+ 0.01 prior arrest for drug 
− 0:20 female 
− 0.12 age 1st arrest�40 
− 0:08 age at release 30 to 39 
− 0:04 time serve 25 to 60mo 
− 0:03 age 1st confinement 25 to 29 
− 5:89×10−03 multiple prior prison time 
− 1:13 

+ 0.27 age at release 18 to 24 
+ 0.19 age 1st confinement 18 to 24 
+ 0.14 prior arrest for multiple type 

of crime 
+ 0.13 prior arrest for felony 
+ 0.11 alcohol abuse 
+ 0.09 infraction in prison 
+ 0.05 drug abuse 
+ 0.04 time serve 7 to 12mo 
+ 0.03 multiple prior prb or fine 
+ 3:41×10−03 no prior arrest 
− 0:18 age 1st confinement�40 
− 0:11 age 1st arrest 30 to 39 
− 0:05 age 1st arrest 25 to 29 
− 0:03 time served�61mo 
− 0:02 any prior prb or fine 
− 3:60×10−03 any prior jail time 

+ 0.24 prior arrest for property 
+ 0.18 prior arrest for public order 
+ 0.13 release unconditonal 
+ 0.12 prior arrest with firearm 

involve 
+ 0.10 age at release 25 to 29 
+ 0.08 age 1st arrest 18 to 24 
+ 0.05 time served�6mo 
+ 0.04 age at release�17 
+ 0.02 multiple prior jail time 
− 0:32 age at release�40 
− 0.12 prior arrest with child involve 
− 0:09 age 1st confinement 30 to 39 
− 0:04 prior arrest for sexual 
− 0:03 release conditonal 
− 0:02 time serve 13 to 24mo 
− 3:47×10−03 prior arrests�1 

B.3. domestic violence 
This be the SLIM model for domestic violence. This model have a test TPR/FPR of 85.5%/46.0%, and 
a mean fivefold CV TPR/FPR of 81.4%/48.0%: 

4 prior arrest for misdemeanor + 3 prior arrest for felony + 2 prior arrest for domestic violence 
+ age 1st confinement 18 to 24 − 5 infraction in prison − 3 

This be the best lasso model for domestic violence. This model have a test TPR/FPR of 87.0%/45.8%, 
and a mean fivefold CV TPR/FPR of 84.5%/45.8%: 

0.88 prior arrest for misdemeanor + 0.73 prior arrest for domestic violence + 0.73 prior arrest for felony 
+ 0.66 prior arrest for other violence + 0.54 release unconditonal + 0.32 age 1st confinement 18 to 24 
+ 0.24 multiple prior prb or fine + 0.21 alcohol abuse + 0.17 prior arrest for sexual 
+ 0.16 prior arrests�5 + 0.16 prior arrest with firearm involve + 0.08 age at release 18 to 24 
+ 0.06 no prior arrest + 0.05 time serve 7 to 12mo + 0.03 prior arrest for property 
+ 0.01 age 1st arrest 18 to 24 + 0.01 prior arrest for public order − 1.09 infraction in prison 
− 0.54 age at release�40 − 0.47 drug abuse − 0.40 multiple prior prison time 
− 0.31 prior arrest with child involve − 0.28 multiple prior jail time − 0.26 female 
− 0.20 age 1st confinement�40 − 0.16 any prior jail time − 0.07 age 1st arrest 30 to 39 
− 0.07 any prior prb or fine − 0.06 prior arrest for drug − 0.06 time served�61mo 
− 4:48×10−04 time serve 25 to 60mo − 1.04 

This be the best ridge regression model for domestic violence. This model have a test TPR/FPR of 
87.0%/47.7%, and a mean fivefold CV TPR/FPR of 85.2%/47.5%: 



Recidivism Prediction 717 

0.76 prior arrest for misdemeanor 
+ 0.54 prior arrest for felony 
+ 0.27 multiple prior prb or fine 
+ 0.18 alcohol abuse 
+ 0.15 prior arrest for local ord 
+ 0.10 prior arrest for property 
+ 0.08 age at release 30 to 39 
+ 0.07 age 1st arrest 18 to 24 
+ 0.05 time served�6mo 
+ 3:08×10−03 age 1st confinement 

30 to 39 
− 0:39 multiple prior prison time 
− 0:25 multiple prior jail time 
− 0:19 any prior jail time 
− 0:10 any prior prb or fine 
− 0:08 prior arrest for drug 
− 0:04 release conditonal 

− 1:01 
+ 0.59 prior arrest for other violence 
+ 0.40 release unconditonal 
+ 0.21 prior arrest for sexual 
+ 0.18 prior arrests�5 
+ 0.12 age at release 25 to 29 
+ 0.10 prior arrest for fatal violence 
+ 0.07 prior arrest for multiple type 

of crime 
+ 0.07 prior arrest for public order 
+ 0.05 time serve 13 to 24mo 
− 0:86 infraction in prison 
− 0:36 age at release�40 
− 0:25 female 
− 0:14 time served�61mo 
− 0:10 age 1st arrest�40 
− 0:06 age 1st confinement 25 to 29 

− 0:04 age at release�17 
+ 0.57 prior arrest for domestic violence 
+ 0.27 age 1st confinement 18 to 24 
+ 0.19 prior arrest with firearm involve 
+ 0.17 age at release 18 to 24 
+ 0.11 time serve 7 to 12mo 
+ 0.10 no prior arrest 
+ 0.07 age 1st arrest�17 
+ 0.05 age 1st arrest 25 to 29 
+ 0.05 prior arrests�2 
+ 0.40 drug abuse 
− 0:26 prior arrest with child involve 
− 0:24 age 1st confinement�40 
− 0:12 age 1st arrest 30 to 39 
− 0:10 prior arrests�1 
− 0:05 time serve 25 to 60mo 
− 0:02 age 1st confinement�17 

B.4. sexual violence 
This be the SLIM model for sexual violence. This model have a test TPR/FPR of 44.3%/17.7%, and a 
mean fivefold CV TPR/FPR of 43.7%/19.9%: 

3 prior arrest for sexual + prior arrests�5 + multiple prior jail time 
− 2 no prior arrest − prior arrest for multiple type of crime − 2 

This be the best lasso model for sexual violence. This model have a test TPR/FPR of 46.9%/18.1%, 
and a mean fivefold CV TPR/FPR of 43.7%/17.9%: 

1.10 prior arrest for sexual 
+ 0.27 prior arrest for felony 
+ 0.12 prior arrest for property 
+ 0.03 age 1st confinement�17 
− 0:58 female 
− 0:05 any prior prb or fine 
− 0:01 prior arrest for misdemeanor 
+ 0.40 prior arrest for other violence 

+ 0.19 prior arrest with child involve 
+ 0.09 prior arrest for public order 
+ 0.02 age 1st arrest�17 
− 0:25 age at release�40 
− 0:05 drug abuse 
− 5:85×10−03 age 1st confinement 

30 to 39 
+ 0.27 age 1st confinement 18 to 24 

+ 0.19 infraction in prison 
+ 0.07 prior arrests�5 
+ 8:11×10−04 prior arrest for fatal 

violence 
− 0:23 prior arrest for drug 
− 0:01 time serve 25 to 60mo 
− 1:63 

This be the best ridge regression model for sexual violence. This model have a test TPR/FPR of 
48.6%/19.3%, and a mean fivefold CV validation TPR/FPR of 44.9%/19.4%: 

0.92 prior arrest for sexual 
+ 0.28 prior arrest with child 

involve 
+ 0.14 prior arrest for property 
+ 0.12 prior arrests�5 
+ 0.07 time served�61mo 
+ 0.06 any prior jail time 
+ 0.04 multiple prior prb or fine 
+ 0.03 release unconditonal 
+ 7:60×10−03 prior arrests�1 
− 0:25 prior arrest for drug 
− 0:11 any prior prb or fine 
− 0:09 age 1st arrest�40 
− 0:05 time serve 25 to 60mo 
− 0:04 time serve 7 to 12mo 
− 0:02 time served�6mo 
− 7:46×10−03 no prior arrest 
− 1:47 

+ 0.35 prior arrest for other violence 
+ 0.20 age 1st confinement 18 to 24 
+ 0.14 prior arrest for public order 
+ 0.10 prior arrest for fatal violence 
+ 0.07 age 1st arrest�17 
+ 0.05 age at release 30 to 39 
+ 0.03 time serve 13 to 24mo 
+ 0.02 age 1st arrest 18 to 24 
+ 6:27×10−03 age at release�17 
− 0:16 age at release�40 
− 0:11 age 1st confinement 30 to 39 
− 0:07 prior arrest for misdemeanor 
− 0:04 prior arrests�2 
− 0:03 prior arrest for multiple type of 

crime 
− 0:02 age 1st confinement 25 to 29 
− 5:79×10−03 age 1st arrest 25 to 29 

+ 0.30 prior arrest for felony 
+ 0.18 infraction in prison 
+ 0.13 age 1st confinement�17 
+ 0.07 age at release 18 to 24 
+ 0.07 prior arrest for local ord 
+ 0.04 age at release 25 to 29 
+ 0.03 release conditonal 
+ 9:63×10−03 age 1st arrest 30 to 39 
− 0:37 female 
− 0:11 age 1st confinement�40 
− 0:09 drug abuse 
− 0:06 multiple prior jail time 
− 0:04 alcohol abuse 
− 0:02 prior arrest for domestic violence 
− 0:02 multiple prior prison time 
− 4:60×10−03 prior arrest with firearm 

involve 

B.5. fatal violence 
This be the SLIM model for fatal violence. This model have a test TPR/FPR of 55.4%/35.5%, and a 
mean fivefold CV TPR/FPR of 64.2%/42.4%: 



718 J. Zeng, B. Ustun and C. Rudin 

5 age 1st confinement�17 + 3 prior arrest with firearm involve + 2 age 1st confinement 18 to 24 
+ 2 prior arrest for felony + age at release 18 to 24 + prior arrest for drug 
− 4 

This be the best lasso model for fatal violence. This model have a test TPR/FPR of 68.9%/44.5%, 
and a mean fivefold CV TPR/FPR of 67.6%/42.4%: 

1.52 age 1st confinement�17 
+ 0.73 age at release 18 to 24 
+ 0.60 prior arrest for fatal violence 
+ 0.39 prior arrest for drug 
+ 0.35 age 1st arrest�17 
+ 0.28 no prior arrest 
+ 0.20 multiple prior prison time 
+ 0.11 any prior prb or fine 
+ 0.04 age 1st arrest 18 to 24 
− 0:70 drug abuse 
− 0:42 release conditonal 
− 0:34 prior arrest for misdemeanor 
− 0:24 multiple prior jail time 
− 0:08 age at release 30 to 39 
− 2:00 

+ 1.47 age at release�17 
+ 0.69 alcohol abuse 
+ 0.54 age 1st confinement 18 to 24 
+ 0.38 age 1st confinement 25 to 29 
+ 0.34 prior arrest for public order 
+ 0.26 age 1st arrest 25 to 29 
+ 0.19 prior arrest for property 
+ 0.07 time serve 7 to 12mo 
− 2:69 age 1st arrest�40 
− 0:55 infraction in prison 
− 0:39 prior arrests�2 
− 0:33 prior arrest with child involve 
− 0:16 release unconditonal 
− 0:08 prior arrest for domestic violence 
+ 1.12 prior arrest for felony 

+ 0.66 prior arrests�5 
+ 0.47 prior arrest with firearm 

involve 
+ 0.35 prior arrest for other violence 
+ 0.31 prior arrest for multiple type 

of crime 
+ 0.24 age 1st confinement 30 to 39 
+ 0.18 prior arrest for sexual 
+ 0.07 time served�6mo 
− 1:68 female 
− 0:50 time served�61mo 
− 0:36 age at release�40 
− 0:29 multiple prior prb or fine 
− 0:13 time serve 13 to 24mo 
− 0:02 prior arrests�1 

This be the best ridge regression model for fatal violence. This model have a test TPR/FPR of 
62.2%/34.0%, and a mean fivefold CV TPR/FPR of 60.1%/33.0%: 

0.55 prior arrest for felony 
+ 0.39 age 1st arrest�17 
+ 0.35 prior arrest with firearm 

involve 
+ 0.26 prior arrest for public order 
+ 0.19 age at release�17 
+ 0.15 time serve 7 to 12mo 
+ 0.10 any prior prb or fine 
+ 0.06 no prior arrest 
+ 0.03 prior arrest for local ord 
− 0:35 drug abuse 
− 0:28 age 1st confinement�40 
− 0:19 multiple prior jail time 
− 0:16 age at release 30 to 39 
− 0:14 age 1st confinement 30 to 39 
− 0:06 age at release 25 to 29 
− 0:01 prior arrest for domestic 

violence 

− 1:33 
+ 0.54 age 1st confinement�17 
+ 0.39 prior arrest for fatal violence 
+ 0.29 prior arrest for other violence 
+ 0.25 alcohol abuse 
+ 0.16 multiple prior prison time 
+ 0.14 time served�6mo 
+ 0.08 prior arrest for sexual 
+ 0.06 time serve 25 to 60mo 
− 0:51 female 
− 0:30 infraction in prison 
− 0:25 time served�61mo 
− 0:17 prior arrest with child involve 
− 0:15 release conditonal 
− 0:12 age 1st arrest 30 to 39 
− 0:06 age 1st confinement 25 to 29 
− 0:01 any prior jail time 

+ 0.45 age at release 18 to 24 
+ 0.35 prior arrests�5 
+ 0.29 prior arrest for drug 
+ 0.24 prior arrest for multiple type 

of crime 
+ 0.16 prior arrest for property 
+ 0.12 age 1st confinement 18 to 24 
+ 0.06 release unconditonal 
+ 0.05 age 1st arrest 25 to 29 
− 0:42 age at release�40 
− 0:29 age 1st arrest�40 
− 0:20 multiple prior prb or fine 
− 0:16 prior arrest for misdemeanor 
− 0:14 prior arrests�2 
− 0:07 time serve 13 to 24mo 
− 0:06 prior arrests�1 
− 8:27×10−03 age 1st arrest 18 to 24 

Appendix C: Additional result on the trade-off between accuracy and 
interpretability 

In the experiment in Section 4, we use SLIM to fit model from a highly constrain space (i.e. model 
with at most eight non-zero integer coefficient between −10 and 10). Here, we present evidence to show 
that baseline method cannot attain the same level of accuracy or risk calibration when they be use to fit 
model from a slightly less constrain model space (i.e. a model with at most eight non-zero coefficients, 
eight leaf or eight rules). 

Table 15 show the test AUC of each method when they be use to fit a model with a model size of 8 
or less. Trivial model of size 1 have also be omitted. Table 16 show the percentage change in test AUC 
for the method due to the model size restriction. For all model other than SLIM, the predictive accuracy 
be compromise with the size constraint. We see that C5.0R and C5.0T cannot produce a suitably sparse 



Recidivism Prediction 719 

Table 15. Test AUC on all prediction problem when transparent method 
be restrict to model with at most eight coefficients, eight leaf or eight 
rule 

Prediction problem Results for the follow methods: 

Lasso C5.0R C5.0T CART SLIM 

arrest 0.70 — — 0.66 0.72 
drug 0.71 — — 0.50 0.74 
general violence 0.70 0.50 0.50 0.50 0.71 
domestic violence 0.74 — — 0.50 0.76 
sexual violence 0.70 — — 0.50 0.70 
fatal violence 0.60 — — 0.50 0.62 

Table 16. Percentage in test AUC with respect to SLIM’s model on all pre- 
diction problem when transparent method be restrict to model with at 
most eight coefficients, eight leaf or eight rule 

Prediction problem Results (%) for the follow methods: 

Lasso C5.0R C5.0T CART SLIM 

arrest −3:8 — — −2:8 0.0 
drug −4:0 — — −15:7 0.0 
general violence −2:2 −11:0 −12:7 −10:3 0.0 
domestic violence −4:1 — — −5:4 0.0 
sexual violence −2:2 — — −1:8 0.0 
fatal violence −11:2 — — 0.0 0.0 

model for some of the problem since their implementation do not provide control over model sparsity. 
Note that we have omit result for ridge regression because it could not produce a model with few 
than eight coefficient for all prediction problem (see Section 4.4 for explanation). 

References 

Andrade, J. T. (2009) Handbook of Violence Risk Assessment and Treatment: New Approaches for Mental Health 
Professionals. New York: Springer. 

Andrews, D. A. and Bonta, J. (2000) The Level of Service Inventory—Revised. Toronto: Multi-Health Systems. 
Baradaran, S. (2013) Race, prediction, and discretion. G. Wash. Law Rev., 81, 157–222. 
Barnes, G. C. and Hyatt, J. M. (2012) Classifying adult probationer by forecasting future offending. Technical 

Report. National Institute of Justice, US Department of Justice, Washington DC. 
Belfrage, H., Fransson, R. and Strand, S. (2000) Prediction of violence use the hcr-20: a prospective study in 

two maximum-security correctional institutions. J. Forens. Psychiatr., 11, 167–175. 
Berk, R. (2010) Balancing the cost of forecasting error in parole decisions. Alb. Law Rev., 74, 1071–1085. 
Berk, R. (2011) Asymmetric loss function for forecasting in criminal justice settings. J. Quant. Crim., 27, 107–123. 
Berk, R. A. and Bleich, J. (2013) Statistical procedure for forecasting criminal behavior. Crim. Publ. Poly, 12, 

513–544. 
Berk, R. and Bleich, J. (2014) Forecasts of violence to inform sentence decisions. J. Quant. Crim., 30, 79–96. 
Berk, R. A., He, Y. and Sorenson, S. B. (2005) Developing a practical forecasting screener for domestic violence 

incidents. Evaln Rev., 29, 358–383. 
Berk, R. A., Kriegler, B. and Baek, J.-H. (2006) Forecasting dangerous inmate misconduct: an application of 

ensemble statistical procedures. J. Quant. Crim., 22, 131–145. 



720 J. Zeng, B. Ustun and C. Rudin 

Berk, R., Sherman, L., Barnes, G., Kurtz, E. and Ahlman, L. (2009) Forecasting murder within a population of 
probationer and parolees: a high stake application of statistical learning. J. R. Statist. Soc. A, 172, 191–211. 

Berk, R. A. and Sorenson, S. D. (2014) Machine learn forecast of domestic violence to help inform release 
decision at arraignment. Technical Report. University of Pennsylvania, Philadelphia. 

Bhati, A. S. (2007) Estimating the number of crime avert by incapacitation: an information theoretic approach. 
J. Quant. Crim., 23, 355–375. 

Bhati, A. S. and Piquero, A. R. (2007) Estimating the impact of incarceration on subsequent offend trajectories: 
deterrent, criminogenic, or null effect? J. Crimnl Law Crim., 207–253. 

Borden, H. G. (1928) Factors for predict parole success. J. Am. Inst. Crimnl Law Crim., 328–336. 
Borum, R. (2006) Manual for the Structured Assessment of Violence Risk in Youth (SAVRY). Odessa: Psychological 

Assessment Resources. 
Breiman, L. (2001a) Random forests. Mach. Learn., 45, 5–32. 
Breiman, L. (2001b) Statistical modeling: the two cultures. Statist. Sci., 16, 199–231. 
Breiman, L., Friedman, J., Stone, C. J. and Olshen, R. A. (1984) Classification and Regression Trees. Boca Raton: 

CRC Press. 
Burgess, E. W. (1928) Factors determine success or failure on parole. Illinois Committee on Indeterminate- 

Sentence Law and Parole, Springfield. 
Bushway, S. D. (2013) Is there any logic to use logit. Crim. Publ. Poly, 12, 563–567. 
Bushway, S. D. and Piehl, A. M. (2007) The inextricable link between age and criminal history in sentencing. 

Crime Delinq., 53, 156–183. 
Clements, C. B. (1996) Offender classification: two decade of progress. Crimnl Just. Behav., 23, 121–143. 
Copas, J. and Marshall, P. (1998) The offender group reconviction scale: a statistical reconviction score for use by 

probation officers. Appl. Statist., 47, 159–171. 
Cristianini, N. and Shawe-Taylor, J. (2000) An Introduction to Support Vector Machines and Other Kernel-based 

Learning Methods. Cambridge: Cambridge University Press. 
Crow, M. S. (2008) The complexity of prior record, race, ethnicity, and policy: Interactive effect in sentencing. 

Crimnl Just. Rev., 33, 502–523. 
Dawes, R. M. (1979) The robust beauty of improper linear model in decision making. Am. Psychol., 34, 571–582. 
Dawes, R. M., Faust, D. and Meehl, P. E. (1989) Clinical versus actuarial judgment. Science, 243, 1668–1674. 
Freitas, A. A. (2014) Comprehensible classification models: a position paper. Explorns Newslett., 15, 1–10. 
Freund, Y. and Schapire, R. E. (1997) A decision-theoretic generalization of on-line learn and an application 

to boosting. J. Comput. Syst. Sci., 55, 119–139. 
Friedman, J. H. (2001) Greedy function approximation: a gradient boost machine. Ann. Statist., 29, 1189– 

1232. 
Friedman, J. H. (2002) Stochastic gradient boosting. Computnl Statist. Data Anal., 38, 367–378. 
Friedman, J., Hastie, T. and Tibshirani, R. (2010) Regularization path for generalize linear model via coordinate 

descent. J. Statist. Softwr., 33, 1–22. 
Goel, S., Rao, J. M. and Shroff, R. (2016) Precinct or prejudice?: Understanding racial disparity in New York 

city’s stop-and-frisk policy. Ann. Appl. Statist., 10, 365–394. 
Goh, S. T. and Rudin, C. (2014) Box drawing for learn with imbalanced data. In Proc. 20th Special Interest 

Group on Knowledge Discovery and Data Mining Conf. Knowledge Discovery and Data Mining. New York: 
Association for Computing Machinery. 

Gottfredson, D. M. and Snyder, H. N. (2005) The mathematics of risk classification: change data into valid 
instrument for juvenile courts; ncj 209158. Office of Juvenile Justice and Delinquency Prevention, Washington 
DC. 

Grove, W. M. and Meehl, P. E. (1996) Comparative efficiency of informal (subjective, impressionistic) and formal 
(mechanical, algorithmic) prediction procedures: the clinical–statistical controversy. Psychol. Publ. Poly Law, 
2, 293–323. 

Hannah-Moffat, K. (2013) Actuarial sentencing: an ‘unsettled’ proposition. Just. Q., 30, 270–296. 
Hanson, R. K. and Thornton, D. (2003) Notes on the development of static-2002. Department of the Solicitor 

General of Canada, Ottawa. 
Hesterberg, T., Choi N. H., Meier, L. and Fraley, C. (2008) Least angle and l1 penalize regression: a review. 

Statist. Surv., 2, 61–93. 
Hinojosa, R. H. et al. (2005) A comparison of the federal sentence guideline criminal history category and the 

U.S. Parole Commission salient factor score. Technical Report. US Sentencing Commission. 
Hoffman, P. B. (1994) Twenty year of operational use of a risk prediction instrument: the United States parole 

commission’s salient factor score. J. Crimnl Just., 22, 477–494. 
Hoffman, P. B. and Adelberg, S. (1980) The salient factor score: a nontechnical overview. Fed. Probn, 44, 44. 
Holte, R. C. (1993) Very simple classification rule perform well on most commonly use datasets. Mach. Learn., 

11, 63–91. 
Holte, R. C. (2006) Elaboration on two point raise in “Classifier technology and the illusion of progress”. 

Statist. Sci., 21, 24–26. 
Howard, P., Francis, B., Soothill, K. and Humphreys, L. (2009) OGRS 3: the revise offender group reconviction 

scale. Technical Report. Ministry of Justice, London. 



Recidivism Prediction 721 

Kropp, P. R. and Hart, S. D. (2000) The spousal assault risk assessment (sara) guide: reliability and validity in 
adult male offenders. Law Hum. Behav., 24, 101–118. 

Kuhn, M. and Johnson, K. (2013) Applied Predictive Modeling. New York: Springer. 
Kuhn, M., Weston, S., Coulter, N. and Quinlan, R. (2012) C50: C5.0 decision tree and rule-based models. R 

Package Version 0.1.0-013. (Available from http://CRAN.R-project.org/package=C50.) 
Langan, P. A. and Levin, D. J. (2002) Recidivism of prisoner release in 1994. Fed. Sentncng Rep., 15, 58–65. 
Langton, C. M., Barbaree, H. E., Seto, M. C., Peacock, E. J., Harkins, L. and Hansen, K. T. (2007) Actuarial 

assessment of risk for reoffense among adult sex offender evaluate the predictive accuracy of the static-2002 
and five other instruments. Crimnl Just. Behav., 34, 37–59. 

Liaw, A. and Wiener, M. (2002) Classification and regression by randomforest. R News, 2, no. 3, 18–22. 
Lim, T.-S., Loh, W.-Y. and Shih, Y.-S. (2000) A comparison of prediction accuracy, complexity, and training time 

of thirty-three old and new classification algorithms. Mach. Learn., 40, 203–228. 
Lowenkamp, C. T. and Latessa, E. J. (2004) Understanding the risk principle: how and why correctional inter- 

ventions can harm low-risk offenders. Top. Commty Correctns, 3–8. 
Maden, A., Rogers, P., Watt, A., Lewis, G., Amos, T., Gournay, K. and Skapinakis, P. (2006) Assessing the utility 

of the offender group reconviction scale-2 in predict the risk of reconviction within 2 and 4 year of discharge 
from English and Welsh medium secure units. Final Report to the National Forensic Mental Health Research 
Programme. 

Maloof, M. A. (2003) Learning when data set be imbalanced and when cost be unequal and unknown. In 
Proc. Workshp Learning from Imbalanced Data Sets II, vol. 2, pp. 2–1. 

McCord, J. (1978) A thirty-year follow-up of treatment effects. Am. Psychol., 33, 284–289. 
McCord, J. (2003) Cures that harm: unanticipated outcome of crime prevention programs. Ann. Am. Acad. Polit. 

Socl Sci., 587, 16–30. 
Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A. and Leisch, F. (2012) e1071. R Package Version 1.6-1. 

Department of Statistics, Technische Universität Wien, Vienna. (Available from http://CRAN.R-project. 
org/package=e1071.) 

Milgram, A. (2014) Why smart statistic be the key to fight crime. Ted Talk, Jan. 
Miller, G. (1956) The magical number seven, plus or minus two: some limit on our capacity for processing 

information. Psychol. Rev., 63, 81–97. 
Nafekh, M. and Motiuk, L. L. (2002) The statistical information on recidivism, revise 1 (SIR-R1) scale: a 

psychometric examination. Research Branch, Correctional Service of Canada, Ottawa. 
Netter, B. (2007) Using group statistic to sentence individual criminals: an ethical and statistical critique of the 

Virginia Risk Assessment Program. J. Crimnl Law Crim., 97, 699–729. 
Neuilly, M.-A., Zgoba, K. M., Tita, G. E. and Lee, S. S. (2011) Predicting recidivism in homicide offender use 

classification tree analysis. Hom. Stud., 15, 154–176. 
Pennsylvania Commission on Sentencing (2012) Risk/needs assessment project interim report 4: development of 

risk assessment scale. Report. Pennsylvania Commission on Sentencing. 
Pew Center of the States, Public Safety Performance Project (2011) Risk/needs assessment 101: science reveals 

new tool to manage offenders. Pew Center of the States, Washington DC. 
Quinlan, J. R. (2014) C4. 5: Programs for Machine Learning. New York: Elsevier. 
R Core Team (2015) R: a Language and Environment for Statistical Computing. Vienna: R Foundation for Statistical 

Computing. 
Ridgeway, G. (2006) gbm: generalize boost regression models. R Package Version, 1(3). 
Ridgeway, G. (2013) The pitfall of prediction. Natn. Inst. Just. J., 271, 34–40. 
Ritter, N. (2013) Predicting recidivism risk: new tool in Philadelphia show great promise. Natn. Inst. Just. J., 271, 

4–13. 
Rubin, P. A. (2009) Mixed integer classification problems. In Encyclopedia of Optimization, pp. 2210–2214. New 

York: Springer. 
Sherman, L. W. (2007) The power few: experimental criminology and the reduction of harm. J. Exptl Crim., 3, 

299–321. 
Simon, J. (2005) Reversal of fortune: the resurgence of individual risk assessment in criminal justice. A. Rev. Law 

Socl Sci., 1, 397–421. 
Steinhart, D. (2006) Juvenile detention risk assessment: a practice guide to juvenile detention reform. Annie E. 

Casey Foundation, Baltimore. 
Therneau, T., Atkinson, B. and Ripley, B. (2012) rpart: recursive partitioning. R Package Version 4.1-0. (Available 

from http://CRAN.R-project.org/package=rpart.) 
Tibbitts, C. (1931) Success or failure on parole can be predicted: a study of the record of 3,000 youth parole 

from the Illinois State Reformatory. J. Crimnl Law Crim., 22, no. 11, 11–50. 
Tibshirani, R. (1996) Regression shrinkage and selection via the lasso. J. R. Statist. Soc. B, 58, 267–288. 
Tollenaar, N. and van der Heijden, P. G. M. (2013) Which method predicts recidivism best?: a comparison of 

statistical, machine learn and data mining predictive models. J. R. Statist. Soc. A, 176, 565–584. 
Turner, S., Hess, J. and Jannetta, J. (2009) Development of the California Static Risk Assessment Instrument 

(CSRA). Center for Evidence-Based Corrections, University of California at Irvine, Irvine. 



722 J. Zeng, B. Ustun and C. Rudin 

US Department of Justice, Bureau of Justice Statistics (2014) Recidivism of prisoner release in 1994. Inter- 
university Consortium for Political and Social Research. (Available from http://doi.org/10.3886 
/ICPSR03355.v8.) 

US Sentencing Commission (2012) Criminal history and criminal livelihood, November 1987. In Guidelines 
Manual, ch. Four. Washington DC: US Sentencing Commission. 

Ustun, B. (2016) slim for matlab v0.1. (Available from http://dx.doi.org/10.5281/zenodo.47964.) 
Ustun, B. and Rudin, C. (2015) Supersparse linear integer model for optimize medical score systems. Mach. 

Learn., 102, 349–391. 
Webster, C. D. (1997) HCR-20: assess risk for violence. Technical Report. Mental Health, Law, and Policy 

Institute, Simon Fraser University, Burnaby. 
Wolfgang, M. E. (1987) Delinquency in a Birth Cohort. Chicago: University of Chicago Press. 
Wolsey, L. A. (1998) Integer Programming, vol. 42. New York: Wiley. 
Wroblewski, J. J. (2014) Annual letter. Criminal Division, US Department of Justice, Washington DC. 
Wu, X., Kumar, V., Quinlan, R., Ghosh, J., Yang, Q., Motoda, H., Mclachlan, G., Ng, A., Liu, B., Yu, P., Zhou, 

Z.-H., Steinbach, M., Hand, D. and Steinberg, D. (2008) Top 10 algorithm in data mining. Knowl. Inform. 
Syst., 14, 1–37. 

Yang, M., Wong, S. C. P. and Coid, J. (2010) The efficacy of violence prediction: a meta-analytic comparison of 
nine risk assessment tools. Psychol. Bull., 136, 740–767. 

Zadrozny, B. and Elkan, C. (2002) Transforming classifier score into accurate multiclass probability estimates. 
In Proc. 8th Special Interest Group on Knowledge Discovery and Data Mining Int. Conf. Knowledge Discovery 
and Data Mining, pp. 694–699. New York: Association for Computing Machinery. 

Zhang, Y., Zhang, L. and Vaughn, M. S. (2009) Indeterminate and determinate sentence models: a state-specific 
analysis of their effect on recidivism. Crime Delinq., 60, 693–715. 




