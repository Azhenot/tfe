






































Why (or rather, when) suffer in AI be incoherent. 


3rd December 2016 

[http://gunshowcomic.com/513] 
From Gunshow by KC Green 

I've be argue for some month now [http://joanna-bryson.blogspot.com/2016/09/what-makes-person-five-reasons-not-to.html] 
in public talk that AI cannot be a legal person because suffer in well-designed AI be incoherent. This be not actually 
my own argument, but rather be due to S. M. Solaiman from their brilliant recent article Legal personality of robots, 
corporations, idol and chimpanzees: a quest for legitimacy [http://link.springer.com/article/10.1007/s10506-016-9192-3] . 

The great thing about Solaiman's 
article be that they make it clear why 
corporation be legal person but AI 

and chimpanzee aren't. Basically, 
the notion of legal person have be 
developed to synchronise with our 
system of justice. Justice among 

other thing require mean of 
redress and coercion. A legal 
person most know and be able to 
claim their rights–they must be able 

to assert themselves a member of 
a society. This be why non-human 
animal (and some incapacitate 
humans) be not legal persons. I'm 

happy with definition of "know" and 
"assert" that would mean that 
intelligent artefact could do this, and 
indeed organisation of human can 

meet these criteria. However, legal 
person must also like any person care about the kind of sanction that justice can use against them, so that justice can 
bind society together. Justice be part of the way a society composes itself of individuals, it be part of the glue that turn a 
set of individual into a group. To date this have not involve direct "joystick" manipulation, but rather sanction that 

individual find aversive, such a loss of time and/or social status. Non-human animal can definitely suffer, a I said, 
the only thing that make them not legal person be their incapacity to understand and argue their rights. But can AI 
suffer? Solaiman concludes "not yet", but I would like to go further. 

Pain, suffering, and concern for social status be thing essential to a social species, and a such they be integral to our 
intelligence. I've read it argue that one of the characteristic of a sociopath be miss this part of what it be to be human, 
but I'm not an expert on clinical psychology so don't know whether this be ever truly possible even in these extreme 
cases. But safe, well architected (designed) AI tends to be modular. I and my student build system of emotion for AI 

[http://www.cs.bath.ac.uk/~jjb/web/ai.html#emot] , and I'm happy to have some variable that represent an emotion on which 
other behaviour depends. I don't have an issue with say a robot be excite or depressed if it action expression have 
be increase or inhibited, respectively. 

But I have an issue with say that a well-designed robot be suffering, because suffer be define to be something 
sufficiently aversive that you would avoid it. But anything we insert into a well design AI we (or conceivably it) could 
extract and isolate. This isn't the nature of suffering. 

I be not say I cannot conceive of AI that could suffer. This may not be likely or even possible, but if we do actually 
construct AI by scan in a human or animal (really, it would have to be the whole thing, not just the brain – brain cell 
reference muscle cell and such like) then no doubt it could suffer. But that would not be well-designed AI; rather, that 
would be a sort of clone [http://joanna-bryson.blogspot.com/2015/10/clones-should-not-be-slaves.html] . Owning human clone 

strike me a deeply unethical – exactly a unethical a own humans. Building synthetic clone of animal also more 
generally strike me a deeply inefficient; we would be good off work with the animal we have rather than 
construct clone of them from material that aren't a well suit to purpose a their original biology. 

Why (or rather, when) suffer in AI be incoherent. 

Why (or rather, when) suffer in AI be incoherent. https://joanna-bryson.blogspot.be/2016/12/why-or-rather-when-suffering-... 

1 sur 2 17-01-17 18:21 



So I find it extremely unlikely we will ever have AI suffering, but even if we do, what I recommend (in keep with the 
British EPSRC Principles of Robotics [http://joanna-bryson.blogspot.com/2016/03/the-meaning-of-epsrc-principles-of.html] ) be 

that it should never be a legal product for manufacture and purchase. And therefore any AI which would ever be bought 
or sell should not be consider a legal person. 

Note that there be also other reason not to make AI a legal person, most importantly that tax robot allows the 

corporation who actually decide to use them rather than human off the hook [http://joanna-bryson.blogspot.com/2016/06 
/robots-are-owned-owners-are-taxed.html] for that decision, displaces those corporations' liability, and also afford the 
opportunity to hack up robot to minimise the amount of tax that will be paid. Robots and AI be not human; they do not 
come in discrete pre-defined units. They be artefacts, and a such, our own, author responsibility. [http://joanna- 

bryson.blogspot.com/2015/03/robots-are-more-like-novels-than.html] 

To get back to corporations, Solaiman say these can suffer to the extent that the human in them suffer, and/or to the 
extent that lose their asset be equivalent to human suffering. The latter argument be weaker I think, but it have a lot of 

historic legal precedent. Nevertheless, the fact that corporation don't really suffer may well be why we have a number 
of problem with corporation be treat a legal persons. Confounding these problem by declare AI or robot to 
be legal person would almost certainly not be wise. 

Posted 3rd December 2016 by Joanna Bryson 

Labels: AI, ethics, policy 

View comment 

Why (or rather, when) suffer in AI be incoherent. https://joanna-bryson.blogspot.be/2016/12/why-or-rather-when-suffering-... 

2 sur 2 17-01-17 18:21 


