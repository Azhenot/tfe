






































Neural network everywhere | MIT News 


Neural network everywhere | MIT 
News 

Most recent advance in artificial-intelligence system such a speech- 

or face-recognition program have come courtesy of neural networks, 

densely interconnect mesh of simple information processor that 

learn to perform task by analyze huge set of training data. 

But neural net be large, and their computation be energy intensive, 

so they’re not very practical for handheld devices. Most smartphone 

apps that rely on neural net simply upload data to internet servers, 

which process it and send the result back to the phone. 

Now, MIT researcher have developed a special-purpose chip that 

increase the speed of neural-network computation by three to seven 

time over it predecessors, while reduce power consumption 94 to 95 

percent. That could make it practical to run neural network locally on 

smartphones or even to embed them in household appliances. 

“The general processor model be that there be a memory in some part of 

the chip, and there be a processor in another part of the chip, and you 

move the data back and forth between them when you do these 

computations,” say Avishek Biswas, an MIT graduate student in 

electrical engineering and computer science, who lead the new chip’s 

development. 

“Since these machine-learning algorithm need so many computations, 

this transfer back and forth of data be the dominant portion of the 

energy consumption. But the computation these algorithm do can be 

simplify to one specific operation, call the dot product. Our 

approach was, can we implement this dot-product functionality inside 

the memory so that you don’t need to transfer this data back and forth?” 

Biswas and his thesis advisor, Anantha Chandrakasan, dean of MIT’s 

School of Engineering and the Vannevar Bush Professor of Electrical 

Engineering and Computer Science, describe the new chip in a paper 

that Biswas be present this week at the International Solid State 

Circuits Conference. 

Neural network everywhere | MIT News http://news.mit.edu/2018/chip-neural-networks-battery-powered-devices... 

1 sur 3 16-02-18 à 19:21 



Back to analog 

Neural network be typically arrange into layers. A single processing 

node in one layer of the network will generally receive data from several 

node in the layer below and pas data to several node in the layer 

above. Each connection between node have it own “weight,” which 

indicates how large a role the output of one node will play in the 

computation perform by the next. Training the network be a matter of 

set those weights. 

A node receive data from multiple node in the layer below will 

multiply each input by the weight of the correspond connection and 

sum the results. That operation — the summation of multiplication — 

be the definition of a dot product. If the dot product exceeds some 

threshold value, the node will transmit it to node in the next layer, over 

connection with their own weights. 

A neural net be an abstraction: The “nodes” be just weight store in a 

computer’s memory. Calculating a dot product usually involves fetch 

a weight from memory, fetch the associate data item, multiply 

the two, store the result somewhere, and then repeat the operation 

for every input to a node. Given that a neural net will have thousand or 

even million of nodes, that’s a lot of data to move around. 

But that sequence of operation be just a digital approximation of what 

happens in the brain, where signal travel along multiple neuron 

meet at a “synapse,” or a gap between bundle of neurons. The neurons’ 

fire rate and the electrochemical signal that cross the synapse 

correspond to the data value and weights. The MIT researchers’ new 

chip improves efficiency by replicate the brain more faithfully. 

In the chip, a node’s input value be convert into electrical voltage 

and then multiply by the appropriate weights. Summing the product 

be simply a matter of combine the voltages. Only the combine 

voltage be convert back into a digital representation and store for 

further processing. 

The chip can thus calculate dot product for multiple node — 16 at a 

time, in the prototype — in a single step, instead of shuttle between a 

processor and memory for every computation. 

Neural network everywhere | MIT News http://news.mit.edu/2018/chip-neural-networks-battery-powered-devices... 

2 sur 3 16-02-18 à 19:21 



All or nothing 

One of the key to the system be that all the weight be either 1 or -1. 

That mean that they can be implement within the memory itself a 

simple switch that either close a circuit or leave it open. Recent 

theoretical work suggests that neural net train with only two weight 

should lose little accuracy — somewhere between 1 and 2 percent. 

Biswas and Chandrakasan’s research bear that prediction out. In 

experiments, they ran the full implementation of a neural network on a 

conventional computer and the binary-weight equivalent on their chip. 

Their chip’s result be generally within 2 to 3 percent of the 

conventional network’s. 

"This be a promising real-world demonstration of SRAM-based in- 

memory analog compute for deep-learning applications,” say Dario 

Gil, vice president of artificial intelligence at IBM. "The result show 

impressive specification for the energy-efficient implementation of 

convolution operation with memory arrays. It certainly will open the 

possibility to employ more complex convolutional neural network for 

image and video classification in IoT [the internet of things] in the 

future." 

Neural network everywhere | MIT News http://news.mit.edu/2018/chip-neural-networks-battery-powered-devices... 

3 sur 3 16-02-18 à 19:21 


