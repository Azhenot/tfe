


















































Proceedings of Machine Learning for Healthcare 2017 JMLR W&C Track Volume 68 

Clinical Intervention Prediction and Understanding with Deep Neural 
Networks 

Harini Suresh 1 HSURESH@MIT.EDU 

Nathan Hunt 1 NHUNT@MIT.EDU 

Alistair Johnson 2 AEWJ@MIT.EDU 

Leo Anthony Celi 2 LCELI@MIT.EDU 

Peter Szolovits 1 PSZ@MIT.EDU 

Marzyeh Ghassemi 1 MGHASSEM@MIT.EDU 

1 Computer Science and Artificial Intelligence Lab, MIT, Cambridge, MA 
2 Laboratory for Computational Physiology, MIT, Cambridge, MA 

Abstract 

Real-time prediction of clinical intervention remains a challenge within intensive care unit (ICUs). 
This task be complicate by data source that be sparse, noisy, heterogeneous and outcome that be 
imbalanced. In this work, we integrate data across many ICU source — vitals, labs, notes, demo- 
graphic — and focus on learn rich representation of this data to predict onset and wean of 
multiple invasive interventions. In particular, we compare both long short-term memory network 
(LSTM) and convolutional neural network (CNN) for prediction of five intervention tasks: in- 
vasive ventilation, non-invasive ventilation, vasopressors, colloid boluses, and crystalloid boluses. 
Our prediction be do in a forward-facing manner after a six hour gap time to support clinically 
actionable planning. We achieve state-of-the-art result on these predictive task use deep archi- 
tectures. Further, we explore the use of feature occlusion to interpret LSTM models, and compare 
this to the interpretability gain from examine input that maximally activate CNN outputs. We 
show that our model be able to significantly outperform baseline for intervention prediction, and 
provide insight into model learning. 

1. Introduction 

As Intensive Care Units (ICUs) play an increase role in acute healthcare delivery (Vincent, 2013), 
clinician must anticipate patients’ care need in a fast-paced, data-overloaded setting. The sec- 
ondary analysis of healthcare data be a critical step toward improve modern healthcare, a it af- 
ford the study of care in real care setting and patient populations. The widespread availability of 
electronic healthcare data (Charles et al., 2013; Jamoom E and E, 2016) allows new investigation 
into evidence-based decision support, where we can learn when patient need a give intervention. 

c©2017. 



Continuous, forward-facing event prediction be particularly important in the ICU set where we 
want to account for evolve clinical needs. 

In this work, we focus on predict the onset and wean of interventions. The efficacy of 
clinical intervention can vary drastically among patients, and unnecessarily administer an inter- 
vention can be harmful and expensive. We target intervention that span a wide severity of need 
in critical care: invasive ventilation, non-invasive (NI) ventilation, vasopressors, colloid boluses, 
and crystalloid boluses. Mechanical ventilation be commonly use for breathing assistance, but have 
many potential complication (Yang and Tobin) and small change in ventilation setting can have 
large impact in patient outcome (Tobin, 2006). Vasopressors be a common ICU medication, but 
there be no robust evidence of improve outcome from their use (Müllner et al., 2004), and some 
evidence they may be harmful (D’Aragon et al., 2015). Fluid bolus be use to improve cardio- 
vascular function and organ perfusion. There be two bolus types: crystalloid and colloid. Both be 
often consider a less aggressive alternative to vasopressors, but there be no multi-center trial 
study whether fluid bolus therapy should be give to critically ill patients, only study try to 
distinguish which type of fluid should be give (Malbrain et al., 2014). 

Capturing complex relationship across disparate data type be key for predictive performance 
in our tasks. To this end, we take advantage of the success of deep learn model in capture 
rich representation of data with little hand-engineering by domain experts. We use long short-term 
memory network (LSTM) (Hochreiter and Schmidhuber, 1997), which have be show to effec- 
tively model complicate dependency in timeseries data (Bengio et al., 1994) and have achieve 
state-of-the-art result in many different applications: e.g. machine translation (Hermann et al., 
2015), dialogue system (Chorowski et al., 2015) and image caption (Xu et al., 2015). They be 
well-suited to our model task because patient symptom may exhibit important temporal depen- 
dencies. We compare the LSTM model to a convolutional neural network (CNN) architecture that 
have previously be explore for longitudinal laboratory data (Razavian et al., 2016). We train one 
model per intervention which predicts all outcome for that intervention give any patient record. 
In do so, we: 

1. Achieve state-of-the-art prediction result in forward-facing, hourly prediction of clinical in- 
terventions (onset, weaning, or continuity). 

2. Demonstrate how feature occlusion can be use in the LSTM model to show which data 
modality and feature be most important in different predictive tasks. This be an important 
step in make model more interpretable by physicians. 

3. Further aid in model interpretability by highlight patient trajectory that lead to the most 
and least confident prediction across outcome and feature in a CNN model. 

2. Background and Related Work 

Clinical decision-making often happens in setting of limited knowledge and high uncertainty; for 
example, only 10 of the 72 ICU intervention evaluate in randomize control trial (RCTs) be 
associate with improve outcome (Ospina-Tascón et al., 2008). Secondary analysis of electronic 
health record (EHR) aim to gain insight from healthcare data previously collect for the primary 
purpose of facilitate patient care. 

Recent study have apply recurrent neural network (RNNs) to model sequential EHR 
data to tag ICU signal with billing code label (Che et al., 2016; Lipton et al., 2015; Choi et al., 
2015), and to identify the impact of different drug for diabetes (Krishnan et al., 2015). Razavian 
et al. (2016) compare CNNs to LSTMs for longitudinal outcome prediction on billing code use 
lab tests. With regard to interpretability, Choi et al. (2016) use temporal attention to identify im- 



portant feature in early diagnostic prediction of chronic disease from time-ordered billing codes. 
Others have focus on use representation of clinical note (Ghassemi et al., 2014) or patient 
physiological signal to predict mortality (Ghassemi et al., 2015). 

Previous work on intervention in ICU population have often either focus on a single out- 
come or use data from specialized cohorts. Such model with vasopressor a a predictive target 
have achieve AUCs of 0.79 in patient receive fluid resuscitation (Fialho et al., 2013), 0.85 in 
septic shock patient (Salgado et al., 2016), and 0.88 for onset after a 4 hour gap and 0.71 for wean- 
ing, only train on patient who do receive a vasopressor (Wu et al., 2016). However, we train 
our model on general ICU population in order to make them more applicable. In the most recent 
prior work on interventions, also on a general ICU population, the best AUC performance be 
0.67 (ventilation), 0.78 (vasopressor) for vasopressor onset prediction after a 4 hour gap (Ghassemi 
et al., 2017). These be lower to 0.66 and 0.74 with a longer prediction gap time of 8 hours. 

3. Data and Preprocessing 

See Figure 1 for an overall description of data flow. 

3.1 Data Source 

We use data from the Multiparameter Intelligent Monitoring in Intensive Care (MIMIC-III v1.4) 
database (Johnson et al., 2016). MIMIC III be publicly available, and contains over 58,000 hospital 
admission from approximately 38,600 adults. We consider patient 15 and old who have ICU 
stay of 12 – 240 hour and consider each patient’s first ICU stay only. This yield 34,148 unique 
ICU stays. 

3.2 Data Extraction and Preprocessing 

For each patient, we extract 5 static variable (such a gender and age), 29 time-varying vitals and 
lab (such a oxygen saturation and blood urea nitrogen), and all available, de-identified clinical 
note for each patient a timeseries across their entire stay (see Table 3 in the Appendix for a 
complete listing of variables). 

Static variable be replicate across all timesteps for each patient. Vital and lab measurement 
be give timestamps that be round to the near hour. If an hour have multiple measurement for 
a signal, those measurement be averaged. 

3.3 Representation of Notes and Vitals 

Clinical narrative note be transform to a 50-dimensional vector of topic proportion for each 
note use Latent Dirichlet Allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). These vector 
be replicate forward and aggregate through time (Ghassemi et al., 2014). For example, if a 
patient have a note A record at hour 3 and a note B at hour 7, hour 3–6 would contain the topic 
distribution from A, while hour 7 onward would contain the aggregate topic distribution from A 
and B combined. 

We compare raw physiological data to physiological words, where we categorize the vitals data 
by first convert each value into a z-score base on the population mean and standard deviation 
for that variable, and then round this score to the near integer and cap it to be between -4 
and 4. Each z-score value then becomes it own column, which explicitly allows for a representation 
of missingness (e.g., all column for a particular variable zeroed) that do not require imputation 
(Figure 7 in Appendix B) (Wu et al., 2016). 



Figure 1: Data preprocessing and feature extraction with numerical measurement and lab values, 
clinical note and static demographics. 

Figure 2: Given data from a fixed-length (6 hour) slide window, model predict the status of 
intervention in a prediction window (4 hours) after a gap time (6 hours). Windows slide along the 
entire patient record, create multiple example from each record. 

The physiological variables, topic distribution, and static variable for each patient be concate- 
nated into a single feature vector per patient, per hour (Esteban et al., 2016). The intervention state 
of each patient (a binary value indicate whether or not they be on the intervention of interest at 
each timestep) and the time of day for each timestep (an integer from 0 to 23 represent the hour) 
be also add to this feature vector. Using the time of day a a feature make it easy for the model 
to capture circadian rhythm that may be present in, e.g., the vitals data. 

3.4 Prediction Task 

We split each patient’s record into 6 hour chunk use a slide window and make a prediction 
for a window of 4 hour after a gap time of 6 hour (Figure 2). When predict ventilation, non- 
invasive ventilation, or vasopressors, the model classifies the prediction window a one of four 
possible outcomes: 1) onset, 2) wean, 3) continue on an intervention, or 4) continue to stay off 
an intervention. In this representation, a label of 0 indicates “off” an intervention and a label of 1 
indicates “on”. Therefore, a prediction window be an onset if there be a transition from a label of 0 
to 1 for the patient during that window; wean be the opposite: a transition from 1 to 0. A window 
be classify a “stay on” if the label for the entire window be 1 or “stay off” if 0. When predict 
colloid or crystalloid boluses, we classify the prediction window into one of two classes: 1) onset, 



Onset Weaning Stay Off Stay On 
Ventilation 0.005 0.017 0.798 0.18 
Vasopressor 0.008 0.016 0.862 0.114 

Non-invasive Ventilation 0.024 0.035 0.695 0.246 
Colloid Bolus 0.003 - - - 

Crystalloid Bol 0.022 - - - 

Table 1: Proportion of each intervention class. Note that colloid and crystalloid bolus be not 
administer for specific durations, and thus have only a single class (onset). 

or 2) no Onset, since these intervention be not administer for on-going duration of time. After 
splitting the patient record into fixed-length chunks, there be 1,154,101 examples. Table 1 list the 
proportion of each class for each intervention. 

4. Methods 

4.1 Long Short-Term Memory Network (LSTM) 

Having see the input sequence x1 . . . xt of a give example, we predict ŷt, a probability distribution 
over the outcomes, with target outcome yt: 

h1 . . . ht = LSTM(x1 . . . xt) (1) 
ŷt = softmax(Wyht + by) (2) 

where xi ∈ RV ,Wy ∈ RNC×L2 , ht ∈ RL2 , by ∈ RNC where V be the dimensionality of the input 
(number of variables), NC be the number of class we predict, and L2 be the second hidden layer 
size. Figure 3a show a model schematic, and more model detail be provide in Appendix C. 

4.2 Convolution Neural Network (CNN) 

We employ a similar CNN architecture to Razavian et al. (2016), except that we do not initially 
convolve the feature into an intermediate representation. We represent feature a channel and 
perform 1D temporal convolutions, rather than treat the input a a 2D image. Our architecture 
consists of temporal convolution at three different temporal granularity with 64 filter each. The 
dimension of the filter be 1× i, where i ∈ {3, 4, 5}. 

We pad the input such that the output from the convolutional layer be the same size, and we 
use a stride of 1. Each convolution be follow by a max pool layer with a pool size of 3. The 
output from all three temporal granularity be concatenate and flattened, and follow by 2 fully 
connect layer with dropout in between and a softmax over the output (Figure 3b). 

4.3 Experimental Settings 

We use a train/validation/test split of 70/10/20 and stratify the split base on outcome. For the 
LSTM, we use dropout with a keep probability of 0.8 during training (only on stack layers), and 
L2 regularization with lambda = 0.0001. We use 2 hidden LSTM layer of 512 node each. For 
the CNN, we use dropout between fully-connected layer with a keep probability of 0.5. We use 
a weight loss function during optimization to account for class imbalances. All parameter be 
determine use cross-validation with the validation set. We implement all model in Tensor- 
Flow version 1.0.1 use the Adam optimizer on mini-batches of 128 examples. We determine when 



(a) The LSTM consists of two hidden layer with 512 
node each. We sequentially feed in each hour’s data. 
At the end of the example window, we use the final 
hidden state to predict the output. 

(b) The CNN architecture performs temporal convolution 
at 3 different granularity (3, 4, and 5 hours), max-pools 
and combine the outputs, and run this through 2 fully 
connect layer to arrive at the prediction. 

Figure 3: Schematics of the a) LSTM and b) CNN model architectures. 

to stop training with early stop base on AUC (area under the receiver operating characteristic 
curve) performance on the validation set. 

4.4 Evaluation 

We evaluate our result base on per-class AUCs a well a aggregate macro AUCs. If there be 
K class each with a per-class AUC of AUCk then the macro AUC be define a the average of 
the per-class AUCS, AUCmacro = 1K 

∑ 
k AUCk. We use the macro AUC a an aggregate score 

because it weight the AUCs of all class equally, regardless of class size (Manning et al., 2008). 
This be important because of the large class imbalance present in the data. 

We use L2 regularize logistic regression (LR) a a baseline for comparison with the neural 
network (Pedregosa et al., 2011). The same input be use for the LR model a for the numerical 
LSTM and CNN (imputed time window of data) but the timesteps be concatenate into a single 
input vector. 

4.5 Interpretibility 

4.5.1 LSTM FEATURE-LEVEL OCCLUSION 

Because of the additional time dependency of recurrent neural networks, get feature-level 
interpretability from LSTMs be notoriously difficult. To achieve this, we borrow an idea from image 
recognition to help understand how the LSTM us different feature of the patients. Zeiler and 
Fergus (2013) use occlusion to understand how model process images: they remove a region of the 
image (by set all value in that region to 0) and compare the model’s prediction of this occlude 
image with the original prediction. A large shift in the prediction implies that the occlude region 
contains important information for the correct prediction. With our LSTM model, we mask feature 
one by one from the patient (replacing the give feature with random noise drawn from the same 
distribution by bootstrapping). We then compare the predictive ability of the model with and without 
each feature; when this difference be large, then the model be rely heavily on that feature to 
make the prediction. 

Note that examine feature interaction would require a more complex analysis to occlude 
all pairs, triples, etc., but would not necessarily demonstrate the direction or exact nature of the 
interaction. 



4.5.2 CNN FILTER/ACTIVATION VISUALIZATION 

We get interpretability from the CNN model in two ways. First, in order to understand how the 
CNN be use the patient data to predict certain tasks, we find and compare the top 10 real example 
that our model predicts be most and least likely to have a specific outcome. As our gap time be 6 
hours, this mean that the model predicts high probability of onset of the give task 6 hour after 
the end of the identify trajectories. 

Second, we generate “hallucinations” from the model which maximize the predict probability 
for a give task (Erhan et al., 2009). This be do by create an objective function that maximizes 
the activation of a specific output node, and backpropagating gradient back to the input image, 
adjust the image so that it maximally activates the output node. 

5. Results 

We found deep architecture achieve state-of-the-art prediction result for our intervention tasks, 
compare to both our baseline a well a other work predict intervention onset and wean 
(Ghassemi et al., 2017; Wu et al., 2016). The AUCs for each of our five intervention type and 4 
prediction task be show for all model in Table 2. All model use 6 hour chuck of “raw” data 
which have either be transform to a 0-1 range (normalized and mean imputed), or discretized 
into physiological word (described in section 3.3). 

5.1 Physiological Words Improve Predictive Task Performance With High Class Imbalance 

We observe a significantly increase AUC for some intervention when use physiological word 
— specifically for ventilation onset (from 0.61 to 0.75) and colloid bolus onset (from 0.52 to 0.72), 
which have the low proportion of onset example (Table 1). This may be because physiological 
word have a smooth effect. Since we round the z-score for each value to the near integer, if 
a patient have a heart rate of 87 at one hour and then 89 at the next, those value will probably be 
represent a the same word. This effect may make the model invariant to small fluctuation in 
the patient’s data and more resilient to overfitting small classes. In addition, the physiological word 
representation have an explicit encode for miss data. This be in contrast to the raw data that have 
be forward-filled and mean-imputed, introduce noise and make it difficult for the model to 
know how confident to be in the measurement it be give (Che et al., 2016). 

5.2 Feature-Level Occlusions Identify Important Per-Class Features 

We be able to interpret the LSTM’s prediction use feature occlusion (Section 4.5.1). We note 
that vitals, labs, topic and static data be important for different intervention (Figure 4). Table 5 in 
Appendix D have a complete listing of the most probable word for each topic mentioned. 

For mechanical ventilation, the top five most important feature – pH, sodium, lactate, hemoglobin, 
and potassium – be consistent for wean and onset . This be sensible, because all be important 
lab value use to ass a patient’s physiological stability, and ventilation be an aggressive interven- 
tion. However, ventilation onset additionally place importance on a patient’s Glasgow Coma Score 
(GCS) and Topic 4 (assessing patient consciousness), likely because patient sedation be a critical 
part of mechanical ventilation. We also note that the scale of AUC difference between ventilation 
onset and wean be the large observe (up to 0.30 for wean and 0.12 for onset). 

In vasopressor onset prediction, physiological variable such a potassium and hematocrit be 
consistently important, which agrees with clinical assessment of cardiovascular state (Bassi et al., 
2013). Similarly, Topic 3 (noting many physiological values) be also important for both onset and 



Intervention Type 
Task Model VENT NI-VENT VASO COL BOL CRYS BOL 

O 
n 

et 
A 

U 
C 

Baseline 0.60 0.66 0.43 0.65 0.67 
LSTM Raw 0.61 0.75 0.77 0.52 0.70 

LSTM Words 0.75 0.76 0.76 0.72 0.71 
CNN 0.62 0.73 0.77 0.70 0.69 

W 
ea 

n 
A 

U 
C 

Baseline 0.83 0.71 0.74 - - 
LSTM Raw 0.90 0.80 0.91 - - 

LSTM Words 0.90 0.81 0.91 - - 
CNN 0.91 0.80 0.91 - - 

St 
ay 

O 
n 

A 
U 

C 

Baseline 0.50 0.79 0.55 - - 
LSTM Raw 0.96 0.86 0.96 - - 

LSTM Words 0.97 0.86 0.95 - - 
CNN 0.96 0.86 0.96 - - 

St 
ay 

O 
ff 

A 
U 

C 

Baseline 0.94 0.71 0.93 - - 
LSTM Raw 0.95 0.86 0.96 - - 

LSTM Words 0.97 0.86 0.95 - - 
CNN 0.95 0.86 0.96 - - 

M 
ac 

ro 
A 

U 
C 

Baseline 0.72 0.72 0.66 - - 
LSTM Raw 0.86 0.82 0.90 - - 

LSTM Words 0.90 0.82 0.89 - - 
CNN 0.86 0.81 0.90 - - 

Table 2: Comparison of model performance on five target interventions. Models that perform 
best for a give (intervention, task) pair be bolded. 

weaning. Note that the overall difference in AUC for onset range up to 0.16, but there be no signifi- 
cant decrease in AUC for wean (< 0.02). This be consistent with previous work that demonstrate 
wean to be a more difficult task in general for vasopressor (Wu et al., 2016). We also note that 
wean prediction place importance on time of day. As note by Wu et al. (2016), this could be a 
side-effect of patient be left on intervention longer than necessary. 

For non-invasive ventilation onset and wean the learn topic be more important than phys- 
iological variables. This may mean that the need for less severe intervention can only be detect 
from clinical insight derive in notes. Similar to vasopressors, we note that onset AUCs vary more 
than wean AUCs (0.14 v 0.01), and that time of day be important for weaning. 

For crystalloid and colloid bolus onsets, topic be all but one of the five most important feature 
for detection. Colloid bolus in general have more AUC variance for the topic feature (0.14 vs. 
0.05), which be likely due to the large class imbalance compare to crystalloids. 

5.3 Convolutional Filters Target Short-term Trajectories 

We be able to understand the CNN by examine maximally activate patient trajectory (Section 
4.5.2). Figure 5 show the mean with standard deviation error bar for four of the most differen- 
tiated feature of the 10 real patient trajectory that be the high and low activate for each 
task. The trend suggest that patient who will require ventilation in the future have high diastolic 
blood pressure, respiratory rate, and heart rate, and low oxygen saturation – possibly correspond- 



(a) (b) (c) (d) 

(e) (f) (g) (h) 

Figure 4: We be able to make interpretable prediction use an LSTM and occlude specific 
features. These figure display the eight feature that cause the large decrease in prediction AUC 
for each intervention task. In general, physiological data be more important for the more invasive 
intervention – mechanical ventilation (4a, 4b) and vasopressor (4c, 4d) – while clinical note topic 
be more important for less invasive task – non-invasive ventilation (4e, 4f) and fluid bolus (4g, 
4h). Note that all wean task except for ventilation have significantly less AUC variance. 

ing to patient who be hyperventilating. For vasopressor onsets, we see a decrease systolic blood 
pressure, heart rate and oxygen saturation rate. These could either indicate alter peripheral perfu- 
sion or stress hyperglycemia. Topic 3, which be important for vasopressor onset use occlusion 
(Figure 4), be also increased. 

Non-invasive ventilation onset be associate with decrease creatinine, phosphate, oxygen satu- 
ration and blood urea nitrogen, potentially indicate neuromuscular respiratory failure. For colloid 
and crystalloid boluses, we note general indicator of physiological decline, a bolus be give for 
a wide range of conditions. 

Hallucinations for vasopressor and ventilation onset be show in Figure 6. While the model 
be not train with any physiological priors, it identifies blood pressure drop a be maximally 
activate for vasopressor onset, and respiratory rate decline for ventilation onset. This suggests that 
it be able to learn physiologically-relevant factor that be important for intervention prediction. The 
hallucination give u more insight into underlie property of the network and what it be look 
for. However, since these trajectory be make to maximize the output of the model, they do not 
necessarily correspond to physiologically plausible trajectories. 

6. Conclusion 

In this work, we target forward-facing prediction of ICU intervention cover multiple phys- 
iological organ systems. To our knowledge, our model be the first to use deep neural network to 



Figure 5: Trajectories of the 10 maximally and minimally activate example for onset of each of 
the interventions. These be the six hour trajectory that occur before another six hour gap time 
precede the onset. 

Figure 6: Trajectories generate by adjust input to maximally activate a specific output node of 
the CNN. 

predict both onset and wean of intervention use all available modality of ICU data. In these 
tasks, deep learn method beat state-of-the-art AUCs report in prior work for intervention pre- 
diction tasks. This be sensible give that prior work have focus on single target with small 
datasets (Wu et al., 2016) or unsupervised representation prior to supervise training (Ghassemi 
et al., 2017). We also note that LSTM model over physiological word input significantly improve 
performance on the two intervention task with the low incidence rate — possibly because this 
representation encodes important information about what be “normal” for each physiological value, 
or be more robust to missingness in the physiological data. 

Importantly, we be able to demonstrate interpretability for both models. In the LSTMs, we 
examine feature importance use occlusion, and found that physiological data be important in 
more invasive tasks, while clinical note topic be more important for less invasive interventions. 
This could indicate that there be more clinical discretion at play for less invasive tasks. We also 
found that all wean task save ventilation have less AUC variance, which could indicate that these 
decision be also make with a large amount of clinical judgment. 

The temporal convolution in our CNN filter over the multi-channel input learnt interest 
and clinically-relevant trend in real patient trajectories, and these be further mimicked in the 
hallucination generate by the network. As in prior work (Razavian et al., 2016), we found that 
RNNs often have similar or improve performance a compare to CNNs. 



Acknowledgements 

This research be fund in part by the Intel Science and Technology Center for Big Data, the 
National Library of Medicine Biomedical Informatics Research Training grant 2T15 LM007092-22, 
NIH National Institute of Biomedical Imaging and Bioengineering (NIBIB) grant R01-EB017205, 
and NIH National Human Genome Research Institute (NHGRI) grant U54-HG007963. 

References 

Estevão Bassi, Marcelo Park, and Luciano Cesar Pontes Azevedo. Therapeutic strategy for high- 
dose vasopressor-dependent shock. Critical care research and practice, 2013, 2013. 

Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependency with gradient 
descent be difficult. IEEE transaction on neural networks, 5(2):157–166, 1994. 

D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. JMLR, 3(5):993–1022, 2003. 

Dustin Charles, Meghan Gabriel, and Michael F Furukawa. Adoption of electronic health record 
system among u non-federal acute care hospitals: 2008-2012. ONC data brief, 9:1–9, 2013. 

Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent neu- 
ral network for multivariate time series with miss values. arXiv preprint arXiv:1606.01865, 
2016. 

Edward Choi, Mohammad Taha Bahadori, and Jimeng Sun. Doctor AI: predict clinical event 
via recurrent neural networks. CoRR, abs/1511.05942, 2015. URL http://arxiv.org/ 
abs/1511.05942. 

Edward Choi, Mohammad Taha Bahadori, Jimeng Sun, Joshua Kulas, Andy Schuetz, and Walter 
Stewart. Retain: An interpretable predictive model for healthcare use reverse time attention 
mechanism. In Advances in Neural Information Processing Systems, page 3504–3512, 2016. 

Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. 
Attention-based model for speech recognition. In Advances in Neural Information Processing 
Systems, page 577–585, 2015. 

Frederick D’Aragon, Emilie P Belley-Cote, Maureen O Meade, François Lauzier, Neill KJ Ad- 
hikari, Matthias Briel, Manoj Lalu, Salmaan Kanji, Pierre Asfar, Alexis F Turgeon, et al. Blood 
pressure target for vasopressor therapy: A systematic review. Shock, 43(6):530–539, 2015. 

Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer 
feature of a deep network. Technical report, University of Montreal, 2009. 

Cristóbal Esteban, Oliver Staeck, Stephan Baier, Yinchong Yang, and Volker Tresp. Predicting 
clinical event by combine static and dynamic information use recurrent neural networks. In 
Healthcare Informatics (ICHI), 2016 IEEE International Conference on, page 93–101. IEEE, 
2016. 

AS Fialho, LA Celi, F Cismondi, SM Vieira, SR Reti, JM Sousa, SN Finkelstein, et al. Disease- 
base model to predict fluid response in intensive care units. Methods Inf Med, 52(6):494–502, 
2013. 

http://arxiv.org/abs/1511.05942 
http://arxiv.org/abs/1511.05942 


Marzyeh Ghassemi, Tristan Naumann, Finale Doshi-Velez, Nicole Brimmer, Rohit Joshi, Anna 
Rumshisky, and Peter Szolovits. Unfolding physiological state: Mortality model in intensive 
care units. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge 
discovery and data mining, page 75–84. ACM, 2014. 

Marzyeh Ghassemi, Marco AF Pimentel, Tristan Naumann, Thomas Brennan, David A Clifton, 
Peter Szolovits, and Mengling Feng. A multivariate timeseries model approach to severity 
of illness assessment and forecasting in icu with sparse, heterogeneous clinical data. In Proc. 
Twenty-Ninth AAAI Conf. on Artificial Intelligence, 2015. 

Marzyeh Ghassemi, Mike Wu, Michael Hughes, and Finale Doshi-Velez. Predicting intervention 
onset in the icu with switch state space models. In Proceedings of the AMIA Summit on 
Clinical Research Informatics (CRI), volume 2017. American Medical Informatics Association, 
2017. 

T. Griffiths and M. Steyvers. Finding scientific topics. In PNAS, volume 101, page 5228–5235, 
2004. 

Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa 
Suleyman, and Phil Blunsom. Teaching machine to read and comprehend. In Advances in 
Neural Information Processing Systems, page 1693–1701, 2015. 

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 
1735–1780, 1997. 

Yang N Jamoom E and Hing E. Office-based physician electronic health record adoption. Office of 
the National Coordinator for Health Information Technology, 2016. 

Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad 
Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. MIMIC-III, 
a freely accessible critical care database. Scientific data, 3, 2016. 

Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint 
arXiv:1511.05121, 2015. 

Zachary C Lipton, David C Kale, Charles Elkan, and Randall Wetzell. Learning to diagnose with 
lstm recurrent neural networks. arXiv preprint arXiv:1511.03677, 2015. 

ML Malbrain, Paul E Marik, Ine Witters, Colin Cordemans, Andrew W Kirkpatrick, Derek J 
Roberts, and Niels Van Regenmortel. Fluid overload, de-resuscitation, and outcome in critically 
ill or injured patients: a systematic review with suggestion for clinical practice. Anaesthesiol 
Intensive Ther, 46(5):361–80, 2014. 

Christopher Manning, Prabhakar Raghavan, and Hinrich Schtze. Introduction to Information Re- 
trieval. Cambridge University Press, 2008. 

Marcus Müllner, Bernhard Urbanek, Christof Havel, Heidrun Losert, Gunnar Gamper, and Harald 
Herkner. Vasopressors for shock. The Cochrane Library, 2004. 

Gustavo A Ospina-Tascón, Gustavo Luiz Büchele, and Jean-Louis Vincent. Multicenter, random- 
ized, control trial evaluate mortality in intensive care: Doomed to fail? Critical care 
medicine, 36(4):1311–1322, 2008. 



F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten- 
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, 
and E. Duchesnay. Scikit-learn: Machine learn in Python. Journal of Machine Learning 
Research, 12:2825–2830, 2011. 

Narges Razavian, Jake Marcus, and David Sontag. Multi-task prediction of disease onset from 
longitudinal lab tests. In JMLR (Journal of Machine Learning Research): MLHC Conference 
Proceedings, 2016. 

Cátia M Salgado, Susana M Vieira, Luı́s F Mendonça, Stan Finkelstein, and João MC Sousa. En- 
semble fuzzy model in personalize medicine: Application to vasopressor administration. En- 
gineering Applications of Artificial Intelligence, 49:141–148, 2016. 

Martin J Tobin. Principles and practice of mechanical ventilation, 2006. 

Jean-Louis Vincent. Critical care-where have we be and where be we going? Critical Care, 17 
(Suppl 1):S2, 2013. 

Mike Wu, Marzyeh Ghassemi, Mengling Feng, Leo A Celi, Peter Szolovits, and Finale Doshi-Velez. 
Understanding vasopressor intervention and weaning: Risk prediction in a public heterogeneous 
clinical time series database. Journal of the American Medical Informatics Association, page 
ocw138, 2016. 

Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan Salakhutdinov, 
Richard S Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation 
with visual attention. In ICML, volume 14, page 77–81, 2015. 

Karl L Yang and Martin J Tobin. A prospective study of index predict the outcome of trial of 
wean from mechanical ventilation. New England Journal of Medicine, 324. 

Matthew D. Zeiler and Rob Fergus. Visualizing and understand convolutional networks. CoRR, 
abs/1311.2901, 2013. URL http://arxiv.org/abs/1311.2901. 

http://arxiv.org/abs/1311.2901 


Appendix 

Appendix A. Dataset Statistics 

Table 3: Variables 

Static Variables Gender Age Ethnicity 
ICU Admission Type 

Vitals and Labs Anion gap Bicarbonate blood pH 
Blood urea nitrogen Chloride Creatinine 

Diastolic blood pressure Fraction inspire oxygen Glascow coma scale total 
Glucose Heart rate Hematocrit 

Hemoglobin INR* Lactate 
Magnesium Mean blood pressure Oxygen saturation 

Partial thromboplastin time Phosphate Platelets 
Potassium Prothrombin time Respiratory rate 

Sodium Systolic blood pressure Temperature 
Weight White blood cell count 

*International normalize ratio of the prothrombin time 

Table 4: Dataset Statistics 

Train Test Total 
Patients 27,318 6,830 34,148 
Notes 564,652 140,089 703,877 
Elective Admission 4,536 1,158 5,694 
Urgent Admission 746 188 934 
Emergency Admission 22,036 5,484 27,520 
Mean Age 63.9 64.1 63.9 
Black/African American 1,921 512 2,433 
Hispanic/Latino 702 166 868 
White 19,424 4,786 24,210 
CCU (coronary care unit) 4,156 993 5,149 
CSRU (cardiac surgery recovery) 5,625 1,408 7,033 
MICU (medical ICU) 9,580 2,494 12,074 
SICU (surgical ICU) 4,384 1,074 5,458 
TSICU (trauma SICU) 3,573 861 4,434 
Female 11,918 2,924 14,842 
Male 15,400 3,906 19,306 
ICU Mortalities 1,741 439 2,180 
In-hospital Mortalities 2,569 642 3,211 
30 Day Mortalities 2,605 656 3,216 
90 Day Mortalities 2,835 722 3,557 
Vasopressor Usage 8,347 2,069 10,416 
Ventilator Usage 11,096 2,732 13,828 



Appendix B. Physiological Word Generation 

See Figure 7. 

Figure 7: Converting data from continuous timeseries format to discrete “physiological words.” The 
numeric value be first z-scored and rounded, and then each z-score be make into it own category. 
On the right, glucose -2 indicates the presence of a glucose value that be 2 standard deviation 
below the mean. A row contain all zero for a give variable indicates that the value for that 
variable be miss at the timestep. 

Appendix C. LSTM Model Details 

LSTM performs the follow update equation for a single layer, give it previous hidden state 
and the new input: 

ft = σ(Wf [ht−1, xt] + bf ) (3) 
it = σ(Wi[ht−1, xt] + bi) (4) 

c̃t = tanh(Wc[ht−1, xt] + bc) (5) 
ct = ft � ct−1 + it � c̃t (6) 

ot = σ(Wo[ht−1, xt] + bo) (7) 
ht = ot � tanh(ct) (8) 

where Wf ,Wi,Wc,Wo ∈ RL1×(L1+V ), bf , bi, bc, bo ∈ RL1 be learn parameters, and ft, it, c̃t, 
ct, ot, ht ∈ RL1 . In these equations, σ stand for an element-wise application of the sigmoid (logis- 
tic) function, and � be an element-wise product. This be generalize to multiple layer by provide 
ht from the previous layer in place of the input. 

We calculate classification loss use categorical cross-entropy, which set the loss for predic- 
tions for N example over M class as: 

L(ŷ1 . . . ŷN ) = − 
1 

N 

N∑ 
i=1 

M∑ 
j=1 

yij log ŷij 

where ŷij be the probability our model predicts for example i be in class j, and yij be the true 
value. 



Appendix D. Generated Topics 

Table 5: Most probable word in the topic most important for intervention predictions. 

Topic Top Ten Words Possible Topic 
Topic 1 pt care resp vent respiratory secretion remains intubate 

abg plan psv b support setting cont place change note 
wean rsbi coarse cpap continue peep suction clear extu- 
bat rr mask wean 

Respiratory fail- 
ure/infection 

Topic 2 family pt ni care patient dnr stitle dr home daughter support 
team meeting wife son comfort note social doctor sw dni 
know time status hospital contact pt’s work plan lastname 

Discussion of end- 
of-life care 

Topic 3 hr resp gi pt cont gu neuro b cv id note abd soft bp today 
stool social note progress clear remains nursing skin urine 
sat foley npn yellow stable l 

Multiple physio- 
logical change 

Topic 4 pain pt assessment response action plan control continue 
give dilaudid monitor chronic acute morphine iv po prn 
patient pca hr med bp drain cont nausea order relief sbp 
pericardial ass 

Assessments of 
patient responsive- 
ness 

Topic 10 pt intubate vent propofol sedation sedate fentanyl peep 
tube verse secretion abg wean remains continue ett suc- 
tioned plan p increase extubation setting ac sound min 
cpap sputum respiratory hr ogt 

Continued need for 
ventilation 

Topic 38 ml dl mg pm meq assess icu ul total medication sys- 
tems review pulse lab balance comment code hour rr min 
respiratory rhythm prophylaxis admission allergy blood 
urine mmhg status dose 

Many lab test 

Topic 48 ed pt patient transfer hospital pain admit denies ad- 
mission day nausea receive ago present micu show 
vomit past report history give blood bp old year ar- 
rival know osh diarrhea unit 

Emergency ad- 
mission/transfer 
patient 


Introduction 
Background and Related Work 
Data and Preprocessing 
Data Source 
Data Extraction and Preprocessing 
Representation of Notes and Vitals 
Prediction Task 

Methods 
Long Short-Term Memory Network (LSTM) 
Convolution Neural Network (CNN) 
Experimental Settings 
Evaluation 
Interpretibility 
LSTM Feature-Level Occlusion 
CNN Filter/Activation Visualization 


Results 
Physiological Words Improve Predictive Task Performance With High Class Imbalance 
Feature-Level Occlusions Identify Important Per-Class Features 
Convolutional Filters Target Short-term Trajectories 

Conclusion 
Dataset Statistics 
Physiological Word Generation 
LSTM Model Details 
Generated Topics 


