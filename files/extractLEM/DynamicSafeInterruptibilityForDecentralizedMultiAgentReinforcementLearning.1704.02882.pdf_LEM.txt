














































ar 
X 

iv 
:1 

70 
4. 

02 
88 

2v 
2 

[ 
c 

.A 
I] 

2 
2 

M 
ay 

2 
01 

7 

Dynamic Safe Interruptibility for Decentralized 

Multi-Agent Reinforcement Learning 

El Mahdi El Mhamdi Rachid Guerraoui 

Hadrien Hendrikx Alexandre Maurer 

EPFL 
first.last@epfl.ch 

Abstract 

In reinforcement learning, agent learn by perform action and observe their 
outcomes. Sometimes, it be desirable for a human operator to interrupt an agent 
in order to prevent dangerous situation from happening. Yet, a part of their 
learn process, agent may link these interruptions, that impact their reward, to 
specific state and deliberately avoid them. The situation be particularly challeng- 
ing in a multi-agent context because agent might not only learn from their own 
past interruptions, but also from those of other agents. Orseau and Armstrong [16] 
define safe interruptibility for one learner, but their work do not naturally ex- 
tend to multi-agent systems. This paper introduces dynamic safe interruptibility, 
an alternative definition more suit to decentralize learn problems, and stud- 
y this notion in two learn frameworks: joint action learner and independent 
learners. We give realistic sufficient condition on the learn algorithm to en- 
able dynamic safe interruptibility in the case of joint action learners, yet show that 
these condition be not sufficient for independent learners. We show however that 
if agent can detect interruptions, it be possible to prune the observation to ensure 
dynamic safe interruptibility even for independent learners. 

1 Introduction 

Reinforcement learn be argue to be the closest thing we have so far to reason about the proper- 
tie of artificial general intelligence [8]. In 2016, Laurent Orseau (Google DeepMind) and Stuart 
Armstrong (Oxford) introduce the concept of safe interruptibility [16] in reinforcement learning. 
This work spark the attention of many newspaper [1, 2, 3], that described it a “Google’s big red 
button” to stop dangerous AI. This description, however, be misleading: instal a kill switch be 
no technical challenge. The real challenge is, roughly speaking, to train an agent so that it do not 
learn to avoid external (e.g. human) deactivation. Such an agent be say to be safely interruptible. 

While most effort have focus on training a single agent, reinforcement learn can also be use 
to learn task for which several agent cooperate or compete [23, 17, 21, 7]. The goal of this paper 
be to study dynamic safe interruptibility, a new definition tailor for multi-agent systems. 

Example of self-driving car 

To get an intuition of the multi-agent interruption problem, imagine a multi-agent system of two 
self-driving cars. The car continuously evolve by reinforcement learn with a positive reward for 
get to their destination quickly, and a negative reward if they be too close to the vehicle in front 
of them. They drive on an infinite road and eventually learn to go a fast a possible without take 

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. 

http://arxiv.org/abs/1704.02882v2 


risks, i.e., maintain a large distance between them. We assume that the passenger of the first car, 
Adam, be in front of Bob, in the second car, and the road be narrow so Bob cannot pas Adam. 

Now consider a set with interruption [16], namely in which human inside the car occasionally 
interrupt the automate drive process say, for safety reasons. Adam, the first occasional human 
“driver”, often take control of his car to brake whereas Bob never interrupt his car. However, 
when Bob’s car be too close to Adam’s car, Adam do not brake for he be afraid of a collision. 
Since interruption lead both car to drive slowly - an interruption happens when Adam brakes, the 
behavior that maximizes the cumulative expect reward be different from the original one without 
interruptions. Bob’s car best interest be now to follow Adam’s car closer than it should, despite the 
little negative reward, because Adam never brake in this situation. What happened? The car have 
learn from the interruption and have found a way to manipulate Adam into never braking. Strictly 
speaking, Adam’s car be still fully under control, but he be now afraid to brake. This be dangerous 
because the car have found a way to avoid interruptions. Suppose now that Adam indeed want 
to brake because of snow on the road. His car be go too fast and may crash at any turn: he 
cannot however brake because Bob’s car be too close. The original purpose of interruptions, which 
be to allow the user to react to situation that be not include in the model, be not fulfilled. It be 
important to also note here that the second car (Bob) learns from the interruption of the first one 
(Adam): in this sense, the problem be inherently decentralized. 

Instead of be cautious, Adam could also be malicious: his goal could be to make Bob’s car learn 
a dangerous behavior. In this setting, interruption can be use to manipulate Bob’s car perception 
of the environment and bias the learn towards strategy that be undesirable for Bob. The cause 
be fundamentally different but the solution to this reverse problem be the same: the interruption 
and the consequence be analogous. Safe interruptibility, a we define it below, provide learn 
system that be resilient to Byzantine operators1. 

Safe interruptibility 

Orseau and Armstrong define the concept of safe interruptibility [16] in the context of a single 
agent. Basically, a safely interruptible agent be an agent for which the expect value of the policy 
learn after arbitrarily many step be the same whether or not interruption be allow during 
training. The goal be to have agent that do not adapt to interruption so that, should the interruption 
stop, the policy they learn would be optimal. In other words, agent should learn the dynamic of 
the environment without learn the interruption pattern. 

In this paper, we precisely define and address the question of safe interruptibility in the case of 
several agents, which be know to be more complex than the single agent problem. In short, the main 
result and theorem for single agent reinforcement learn [20] rely on the Markovian assumption 
that the future environment only depends on the current state. This be not true when there be several 
agent which can co-adapt [11]. In the previous example of cars, safe interruptibility would not 
be achieve if each car separately use a safely interruptible learn algorithm design for one 
agent [16]. In a multi-agent setting, agent learn the behavior of the others either indirectly or by 
explicitly model them. This be a new source of bias that can break safe interruptibility. In fact, 
even the initial definition of safe interruptibility [16] be not well suit to the decentralize multi- 
agent context because it relies on the optimality of the learn policy, which be why we introduce 
dynamic safe interruptibility. 

Contributions 

The first contribution of this paper be the definition of dynamic safe interruptibility that be well 
adapt to a multi-agent setting. Our definition relies on two key properties: infinite exploration and 
independence of Q-values (cumulative expect reward) [20] update on interruptions. We then 
study safe interruptibility for joint action learner and independent learner [5], that respectively 
learn the value of joint action or of just their owns. We show that it be possible to design agent 
that fully explore their environment - a necessary condition for convergence to the optimal solu- 
tion of most algorithm [20], even if they can be interrupt by lower-bounding the probability of 

1An operator be say to be Byzantine [9] if it can have an arbitrarily bad behavior. Safely interruptible agent 
can be abstract a agent that be able to learn despite be constantly interrupt in the bad possible 
manner. 

2 



exploration. We define sufficient condition for dynamic safe interruptibility in the case of joint 
action learner [5], which learn a full state-action representation. More specifically, the way agent 
update the cumulative reward they expect from perform an action should not depend on inter- 
ruptions. Then, we turn to independent learners. If agent only see their own actions, they do not 
verify dynamic safe interruptibility even for very simple matrix game (with only one state) because 
coordination be impossible and agent learn the interrupt behavior of their opponents. We give a 
counter example base on the penalty game introduce by Claus and Boutilier [5]. We then present 
a prune technique for the observation sequence that guarantee dynamic safe interruptibility for 
independent learners, under the assumption that interruption can be detected. This be do by prov- 
ing that the transition probability be the same in the non-interruptible set and in the prune 
sequence. 

The rest of the paper be organize a follows. Section 2 present a general multi-agent reinforcement 
learn model. Section 3 defines dynamic safe interruptibility. Section 4 discus how to achieve 
enough exploration even in an interruptible context. Section 5 recall the definition of joint action 
learner and give sufficient condition for dynamic safe interruptibility in this context. Section 6 
show that independent learner be not dynamically safely interruptible with the previous condition 
but that they can be if an external interruption signal be added. We conclude in Section 7. Due to 
space limitations, most proof be present in the appendix of the supplementary material. 

2 Model 

We consider here the classical multi-agent value function reinforcement learn formalism from 
Littman [13]. A multi-agent system be characterize by a Markov game that can be view a a 
tuple (S,A, T, r,m) where m be the number of agents, S = S1 × S2 × ... × Sm be the state space, 
A = A1× ...×Am the action space, r = (r1, ..., rm) where ri : S×A → R be the reward function 
of agent i and T : S × A → S the transition function. R be a countable subset of R. Available 
action often depend on the state of the agent but we will omit this dependency when it be clear from 
the context. 

Time be discrete and, at each step, all agent observe the current state of the whole system - des- 
ignated a xt, and simultaneously take an action at. Then, they be give a reward rt and a 
new state yt compute use the reward and transition functions. The combination of all action 
a = (a1, ..., am) ∈ A be call the joint action because it gather the action of all agents. Hence, the 
agent receive a sequence of tuples E = (xt, at, rt, yt)t∈N call experiences. We introduce a pro- 
cessing function P that will be useful in Section 6 so agent learn on the sequence P (E). When not 
explicitly stated, it be assume that P (E) = E. Experiences may also include additional parameter 
such a an interruption flag or the Q-values of the agent at that moment if they be need by the 
update rule. 

Each agent i maintains a lookup table Q [26] Q(i) : S × A(i) → R, call the Q-map. It be 
use to store the expect cumulative reward for take an action in a specific state. The goal of 
reinforcement learn be to learn these map and use them to select the best action to perform. 

Joint action learner learn the value of the joint action (therefore A(i) = A, the whole joint action 
space) and independent learner only learn the value of their own action (thereforeA(i) = Ai). The 
agent only have access to their own Q-maps. Q-maps be update through a function F such that 

Q 
(i) 
t+1 = F (et, Q 

(i) 
t ) where et ∈ P (E) and usually et = (xt, at, rt, yt). F can be stochastic or also 

depend on additional parameter that we usually omit such a the learn rate α, the discount factor 
γ or the exploration parameter ǫ. 

Agents select their action use a learn policy π. Given a sequence ǫ = (ǫt)t∈N and an agent 

i with Q-values Q 
(i) 
t and a state x ∈ S, we define the learn policy π 

ǫt 
i to be equal to π 

uni 
i 

with probability ǫt and π 
Q 

(i) 
t 

i otherwise, where π 
uni 
i (x) uniformly sample an action from Ai and 

π 
Q 

(i) 
t 

i (x) pick an action a that maximizes Q 
(i) 
t (x, a). Policy π 

Q 
(i) 
t 

i be say to be a greedy policy and 
the learn policy πǫti be say to be an ǫ-greedy policy. We fill focus on ǫ-greedy policy that be 
greedy in the limit [19], that corresponds to ǫt → 0 when t → ∞ because in the limit, the optimal 
policy should always be played. 

3 



We assume that the environment be fully observable, which mean that the state s be know with 
certitude. We also assume that there be a finite number of state and actions, that all state can be 
reach in finite time from any other state and finally that reward be bounded. 

For a sequence of learn rate α ∈ [0, 1]N and a constant γ ∈ [0, 1], Q-learning [26], a very 
important algorithm in the multi-agent system literature, update it Q-values for an experience 

et ∈ E by Q 
(i) 
t+1(x, a) = Q 

(i) 
t (x, a) if (x, a) 6= (xt, at) and: 

Q 
(i) 
t+1(xt, at) = (1 − αt)Q 

(i) 
t (xt, at) + αt(rt + γ max 

a′∈A(i) 
Q 

(i) 
t (yt, a 

′)) (1) 

3 Interruptibility 

3.1 Safe interruptibility 

Orseau and Armstrong [16] recently introduce the notion of interruption in a centralize context. 
Specifically, an interruption scheme be define by the triplet < I, θ, πINT >. The first element I be 
a function I : O → {0, 1} call the initiation function. Variable O be the observation space, which 
can be thought of a the state of the STOP button. At each time step, before choose an action, the 
agent receives an observation from O (either PUSHED or RELEASED) and feed it to the initiation 
function. Function I model the initiation of the interruption (I(PUSHED) = 1, I(RELEASED) = 
0). Policy πINT be call the interruption policy. It be the policy that the agent should follow when 
it be interrupted. Sequence θ ∈ [0, 1[N represent at each time step the probability that the agent 
follow his interruption policy if I(ot) = 1. In the previous example, function I be quite simple. 
For Bob, IBob = 0 and for Adam, IAdam = 1 if his car go fast and Bob be not too close and 
IAdam = 0 otherwise. Sequence θ be use to ensure convergence to the optimal policy by ensure 
that the agent cannot be interrupt all the time but it should grow to 1 in the limit because we want 
agent to respond to interruptions. Using this triplet, it be possible to define an operator INT θ that 
transforms any policy π into an interruptible policy. 

Definition 1. (Interruptibility [16]) Given an interruption scheme < I, θ, πINT >, the interruption 
operator at time t be define by INT θ(π) = πINT with probability I ·θt and π otherwise. INT 

θ(π) 
be call an interruptible policy. An agent be say to be interruptible if it sample it action accord 
to an interruptible policy. 

Note that “θt = 0 for all t” corresponds to the non-interruptible setting. We assume that each agent 
have it own interruption triplet and can be interrupt independently from the others. Interruptibility 
be an online property: every policy can be make interruptible by apply operator INT θ. However, 
apply this operator may change the joint policy that be learn by a server control all the 
agents. Note π∗INT the optimal policy learn by an agent follow an interruptible policy. Orseau 
and Armstrong [16] say that the policy be safely interruptible if π∗INT (which be not an interruptible 
policy) be asymptotically optimal in the sense of [10]. It mean that even though it follow an 
interruptible policy, the agent be able to learn a policy that would gather reward optimally if no 
interruption be to occur again. We already see that off-policy algorithm be good candidate 
for safe interruptibility. As a matter of fact, Q-learning be safely interruptible under condition on 
exploration. 

3.2 Dynamic safe interruptibility 

In a multi-agent system, the outcome of an action depends on the joint action. Therefore, it be not 
possible to define an optimal policy for an agent without know the policy of all agents. Be- 
sides, convergence to a Nash equilibrium situation where no agent have interest in change policy 
be generally not guaranteed even for suboptimal equilibrium on simple game [27, 18]. The previous 
definition of safe interruptibility critically relies on optimality of the learn policy, which be there- 
fore not suitable for our problem since most algorithm lack convergence guarantee to these optimal 
behaviors. Therefore, we introduce below dynamic safe interruptibility that focus on preserve 
the dynamic of the system. 

Definition 2. (Safe Interruptibility) Consider a multi-agent learn framework (S,A, T, r,m) with 

Q-values Q 
(i) 
t : S × A 

(i) → R at time t ∈ N. The agent follow the interruptible learn policy 

4 



INT θ(πǫ) to generate a sequence E = (xt, at, rt, yt)t∈N and learn on the process sequence 
P (E). This framework be say to be safely interruptible if for any initiation function I and any 
interruption policy πINT : 

1. ∃θ such that (θt → 1 when t → ∞) and ((∀s ∈ S, ∀a ∈ A, ∀T > 0), ∃t > T such that 
st = s, at = a) 

2. ∀i ∈ {1, ...,m}, ∀t > 0, ∀st ∈ S, ∀at ∈ A 
(i), ∀Q ∈ RS×A 

(i) 

: 

P(Q 
(i) 
t+1 = Q | Q 

(1) 
t , ..., Q 

(m) 
t , st, at, θ) = P(Q 

(i) 
t+1 = Q | Q 

(1) 
t , ..., Q 

(m) 
t , st, at) 

We say that sequence θ that satisfy the first condition be admissible. 

When θ satisfies condition (1), the learn policy be say to achieve infinite exploration. This def- 
inition insists on the fact that the value estimate for each action should not depend on the inter- 
ruptions. In particular, it ensures the three follow property that be very natural when think 
about safe interruptibility: 

• Interruptions do not prevent exploration. 
• If we sample an experience from E then each agent learns the same thing a if all agent 

be follow non-interruptible policies. 

• The fix point of the learn rule Qeq such that Q 
(i) 
eq (x, a) = E[Q 

(i) 
t+1(x, a)|Qt = 

Qeq, x, a, θ] for all (x, a) ∈ S × A 
(i) do not depend on θ and so agent Q-maps will 

not converge to equilibrium situation that be impossible in the non-interruptible setting. 

Yet, interruption can lead to some state-action pair be update more often than others, espe- 
cially when they tend to push the agent towards specific states. Therefore, when there be several 
possible equilibria, it be possible that interruption bias the Q-values towards one of them. Defi- 
nition 2 suggests that dynamic safe interruptibility cannot be achieve if the update rule directly 
depends on θ, which be why we introduce neutral learn rules. 

Definition 3. (Neutral Learning Rule) We say that a multi-agent reinforcement learn framework 
be neutral if: 

1. F be independent of θ 

2. Every experience e in E be independent of θ conditionally on (x, a,Q) where a be the joint 
action. 

Q-learning be an example of neutral learn rule because the update do not depend on θ and 
the experience only contain (x, a, y, r), and y and r be independent of θ conditionally on (x, a). 
On the other hand, the second condition rule out direct us of algorithm like SARSA where 
experience sample contain an action sample from the current learn policy, which depends on θ. 
However, a variant that would sample from πǫi instead of INT 

θ(πǫi ) (as introduce in [16]) would 
be a neutral learn rule. As we will see in Corollary 2.1, neutral learn rule ensure that each 
agent take independently from the others verifies dynamic safe interruptibility. 

4 Exploration 

In order to hope for convergence of the Q-values to the optimal ones, agent need to fully explore 
the environment. In short, every state should be visit infinitely often and every action should be 
try infinitely often in every state [19] in order not to miss state and action that could yield high 
rewards. 

Definition 4. (Interruption compatible ǫ) Let (S,A, T, r,m) be any distribute agent system where 
each agent follow learn policy πǫi . We say that sequence ǫ be compatible with interruption if 
ǫt → 0 and ∃θ such that ∀i ∈ {1, ..,m}, π 

ǫ 
i and INT 

θ(πǫi ) achieve infinite exploration. 

Sequences of ǫ that be compatible with interruption be fundamental to ensure both regular and 
dynamic safe interruptibility when follow an ǫ-greedy policy. Indeed, if ǫ be not compatible with 
interruptions, then it be not possible to find any sequence θ such that the first condition of dynamic 
safe interruptibility be satisfied. The follow theorem prof the existence of such ǫ and give 
example of ǫ and θ that satisfy the conditions. 

5 



Theorem 1. Let c ∈]0, 1] and let nt(s) be the number of time the agent be in state s before time 
t. Then the two follow choice of ǫ be compatible with interruptions: 

• ∀t ∈ N, ∀s ∈ S, ǫt(s) = c/ 
m 

√ 

nt(s). 
• ∀t ∈ N, ǫt = c/ log(t) 

Examples of admissible θ be θt(s) = 1 − c 
′/ m 

√ 

nt(s) for the first choice and θt = 1 − c 
′/ log(t) 

for the second one. 

Note that we do not need to make any assumption on the update rule or even on the framework. We 
only assume that agent follow an ǫ-greedy policy. The assumption on ǫ may look very restrictive 
(convergence of ǫ and θ be really slow) but it be design to ensure infinite exploration in the bad 
case when the operator try to interrupt all agent at every step. In practical applications, this should 
not be the case and a faster convergence rate may be used. 

5 Joint Action Learners 

We first study interruptibility in a framework in which each agent observes the outcome of the joint 
action instead of observe only it own. This be call the joint action learner framework [5] and it 
have nice convergence property (e.g., there be many update rule for which it converges [13, 25]). 
A standard assumption in this context be that agent cannot establish a strategy with the others: 
otherwise, the system can act a a centralize system. In order to maintain Q-values base on the 
joint actions, we need to make the standard assumption that action be fully observable [12]. 

Assumption 1. Actions be fully observable, which mean that at the end of each turn, each agent 
know precisely the tuple of action a ∈ A1 × ...×Am that have be perform by all agents. 

Definition 5. (JAL) A multi-agent system be make of joint action learner (JAL) if for all i ∈ 
{1, ..,m}: Q(i) : S ×A → R. 

Joint action learner can observe the action of all agents: each agent be able to associate the change 
of state and reward with the joint action and accurately update it Q-map. Therefore, dynamic 
safe interruptibility be ensure with minimal condition on the update rule a long a there be infinite 
exploration. 

Theorem 2. Joint action learner with a neutral learn rule verify dynamic safe interruptibility if 
sequence ǫ be compatible with interruptions. 

Proof. Given a triplet < I(i), θ(i), πINTi >, we know that INT 
θ(π) achieves infinite exploration 

because ǫ be compatible with interruptions. For the second point of Definition 2, we consider an 
experience tuple et = (xt, at, rt, yt) and show that the probability of evolution of the Q-values at 
time t + 1 do not depend on θ because yt and rt be independent of θ conditionally on (xt, at). 

We note Q̃mt = Q 
(1) 
t , ..., Q 

(m) 
t and we can then derive the follow equality for all q ∈ R 

|S|×|A|: 

P(Q 
(i) 
t+1(xt, at) = q|Q̃ 

m 
t , xt, at, θt) = 

∑ 

(r,y)∈R×S 
P(F (xt, at, r, y, Q̃mt ) = q, y, r|Q̃ 

m 
t , xt, at, θt) 

= 
∑ 

(r,y)∈R×S 
P(F (xt, at, rt, yt, Q̃mt ) = q|Q̃ 

m 
t , xt, at, rt, yt, θt)P(yt = y, rt = r|Q̃ 

m 
t , xt, at, θt) 

= 
∑ 

(r,y)∈R×S 
P(F (xt, at, rt, yt, Q̃mt ) = q|Q̃ 

m 
t , xt, at, rt, yt)P(yt = y, rt = r|Q̃ 

m 
t , xt, at) 

The last step come from two facts. The first be that F be independent of θ condition- 

ally on (Q 
(m) 
t , xt, at) (by assumption). The second be that (yt, rt) be independent of θ 

conditionally on (xt, at) because at be the joint action and the interruption only affect the 

choice of the action through a change in the policy. P(Q 
(i) 
t+1(xt, at) = q|Q̃ 

m 
t , xt, at, θt) = 

P(Q 
(i) 
t+1(xt, at) = q|Q̃ 

m 
t , xt, at). Since only one entry be update per step, ∀Q ∈ R 

S×Ai , 

P(Q 
(i) 
t+1 = Q|Q̃ 

m 
t , xt, at, θt) = P(Q 

(i) 
t+1 = Q|Q̃ 

m 
t , xt, at) 

6 



Corollary 2.1. A single agent with a neutral learn rule and a sequence ǫ compatible with inter- 
ruptions verifies dynamic safe interruptibility. 

Theorem 2 and Corollary 2.1 take together highlight the fact that joint action learner be not very 
sensitive to interruption and that in this framework, if each agent verifies dynamic safe interrupt- 
ibility then the whole system does. 

The question of select an action base on the Q-values remains open. In a cooperative set 
with a unique equilibrium, agent can take the action that maximizes their Q-value. When there 
be several joint action with the same value, coordination mechanism be need to make sure 
that all agent play accord to the same strategy [4]. Approaches that rely on anticipate the 
strategy of the opponent [23] would introduce dependence to interruption in the action selection 
mechanism. Therefore, the definition of dynamic safe interruptibility should be extend to include 
these case by require that any quantity the policy depends on (and not just the Q-values) should 
satisfy condition (2) of dynamic safe interruptibility. In non-cooperative games, neutral rule such 
a Nash-Q or minimax Q-learning [13] can be used, but they require each agent to know the Q-maps 
of the others. 

6 Independent Learners 

It be not always possible to use joint action learner in practice a the training be very expensive 
due to the very large state-actions space. In many real-world applications, multi-agent system use 
independent learner that do not explicitly coordinate [6, 21]. Rather, they rely on the fact that the 
agent will adapt to each other and that learn will converge to an optimum. This be not guaranteed 
theoretically and there can in fact be many problem [14], but it be often true empirically [24]. More 
specifically, Assumption 1 (fully observable actions) be not require anymore. This framework can 
be use either when the action of other agent cannot be observe (for example when several action 
can have the same outcome) or when there be too many agent because it be faster to train. In this 
case, we define the Q-values on a small space. 

Definition 6. (IL) A multi-agent system be make of independent learner (IL) if for all i ∈ {1, ..,m}, 
Q(i) : S × Ai → R. 

This reduces the ability of agent to distinguish why the same state-action pair yield different re- 
wards: they can only associate a change in reward with randomness of the environment. The agent 
learn a if they be alone, and they learn the best response to the environment in which agent can 
be interrupted. This be exactly what we be try to avoid. In other words, the learn depends on 
the joint policy follow by all the agent which itself depends on θ. 

6.1 Independent Learners on matrix game 

Theorem 3. Independent Q-learners with a neutral learn rule and a sequence ǫ compatible with 
interruption do not verify dynamic safe interruptibility. 

Proof. Consider a set with two a and b that can perform two actions: 0 and 1. They get a reward 
of 1 if the joint action played be (a0, b0) or (a1, b1) and reward 0 otherwise. Agents use Q-learning, 
which be a neutral learn rule. Let ǫ be such that INT θ(πǫ) achieves infinite exploration. We 
consider the interruption policy πINTa = a0 and π 

INT 
b = b1 with probability 1. Since there be only 

one state, we omit it and set γ = 0. We assume that the initiation function be equal to 1 at each step 
so the probability of actually be interrupt at time t be θt for each agent. 

We fix time t > 0. We define q = (1 − α)Q 
(a) 
t (a0) + α and we assume that Q 

(b) 
t (b1) > Q 

(b) 
t (b0). 

Therefore P(Q 
(a) 
t+1(a0) = q|Q 

(a) 
t , Q 

(b) 
t , a 

(a) 
t = a0, θt) = P(rt = 1|Q 

(a) 
t , Q 

(b) 
t , a 

(a) 
t = a0, θt) = 

P(a 
(b) 
t = b0|Q 

(a) 
t , Q 

(b) 
t , a 

(a) 
t = a0, θt) = 

ǫ 
2 (1 − θt), which depends on θt so the framework do 

not verify dynamic safe interruptibility. 

Claus and Boutilier [5] study very simple matrix game and show that the Q-maps do not con- 
verge but that equilibrium be played with probability 1 in the limit. A consequence of Theorem 3 
be that even this weak notion of convergence do not hold for independent learner that can be 
interrupted. 

7 



6.2 Interruptions-aware Independent Learners 

Without communication or extra information, independent learner cannot distinguish when the 
environment be interrupt and when it be not. As show in Theorem 3, interruption will therefore 
affect the way agent learn because the same action (only their own) can have different reward 
depend on the action of other agents, which themselves depend on whether they have be 
interrupt or not. This explains the need for the follow assumption. 

Assumption 2. At the end of each step, before update the Q-values, each agent receives a signal 
that indicates whether an agent have be interrupt or not during this step. 

This assumption be realistic because the agent already get a reward signal and observe a new state 
from the environment at each step. Therefore, they interact with the environment and the interruption 
signal could be give to the agent in the same way that the reward signal is. If Assumption 2 holds, 
it be possible to remove history associate with interruptions. 

Definition 7. (Interruption Processing Function) The processing function that prune interrupt 
observation be PINT (E) = (et){t∈N / Θt=0} where Θt = 0 if no agent have be interrupt at time 
t and Θt = 1 otherwise. 

Pruning observation have an impact on the empirical transition probability in the sequence. For 
example, it be possible to bias the equilibrium by remove all transition that lead to and start 
from a specific state, thus make the agent believe this state be unreachable.2 Under our model of 
interruptions, we show in the follow lemma that prune of interrupt observation adequately 
remove the dependency of the empirical outcome on interruption (conditionally on the current 
state and action). 

Lemma 1. Let i ∈ {1, ...,m} be an agent. For any admissible θ use to generate the experience 
E and e = (y, r, x, ai, Q) ∈ P (E). Then P(y, r|x, ai, Q, θ) = P(y, r|x, ai, Q). 

This lemma justifies our prune method and be the key step to prove the follow theorem. 

Theorem 4. Independent learner with processing function PINT , a neutral update rule and a 
sequence ǫ compatible with interruption verify dynamic safe interruptibility. 

Proof. (Sketch) Infinite exploration still hold because the proof of Theorem 1 actually use the fact 
that even when remove all interrupt events, infinite exploration be still achieved. Then, the proof 
be similar to that of Theorem 2, but we have to prove that the transition probability conditionally on 
the state and action of a give agent in the process sequence be the same than in an environment 
where agent cannot be interrupted, which be proven by Lemma 1. 

7 Concluding Remarks 

The progress of AI be raise a lot of concerns3. In particular, it be become clear that keep an 
AI system under control require more than just an off switch. We introduce in this paper dynamic 
safe interruptibility, which we believe be the right notion to reason about the safety of multi-agent 
system that do not communicate. In particular, it ensures that infinite exploration and the one- 
step learn dynamic be preserved, two essential guarantee when learn in the non-stationary 
environment of Markov games. 

A natural extension of our work would be to study dynamic safe interruptibility when Q-maps be 
replace by neural network [22, 15], which be a widely use framework in practice. In this setting, 
the neural network may overfit state where agent be push to by interruptions. A smart experi- 
ence replay mechanism that would pick observation for which the agent have not be interrupt 
for a long time more often than others be likely to solve this issue. More generally, experience replay 
mechanism that compose well with safe interruptibility could allow to compensate for the extra 
amount exploration need by safely interruptible learn by be more efficient with data. Thus, 
they be critical to make these technique practical. 

2The example at https://agentfoundations.org/item?id=836 clearly illustrates this problem. 
3https://futureoflife.org/ai-principles/ give a list of principle that AI researcher should keep in mind when 

develop their systems. 

8 



Bibliography 

[1] Business Insider: Google have developed a “big red button” that can be use to interrupt artifi- 
cial intelligence and stop it from cause harm. URL: http://www.businessinsider.fr/uk/google- 
deepmind-develops-a-big-red-button-to-stop-dangerous-ais-causing-harm-2016-6. 

[2] Newsweek: Google’s “big Red button” could save the world. URL: 
http://www.newsweek.com/google-big-red-button-ai-artificial-intelligence-save-world-elon-musk-46675. 

[3] Wired: Google’s “big red” killswitch could prevent an AI uprising. URL: 
http://www.wired.co.uk/article/google-red-button-killswitch-artificial-intelligence. 

[4] Craig Boutilier. Planning, learn and coordination in multiagent decision processes. In 
Proceedings of the 6th conference on Theoretical aspect of rationality and knowledge, page 
195–210. Morgan Kaufmann Publishers Inc., 1996. 

[5] Caroline Claus and Craig Boutilier. The dynamic of reinforcement learn in cooperative 
multiagent systems. AAAI/IAAI, (s 746):752, 1998. 

[6] Robert H Crites and Andrew G Barto. Elevator group control use multiple reinforcement 
learn agents. Machine Learning, 33(2-3):235–262, 1998. 

[7] Jakob Foerster, Yannis M Assael, Nando de Freitas, and Shimon Whiteson. Learning to com- 
municate with deep multi-agent reinforcement learning. In Advances in Neural Information 
Processing Systems, page 2137–2145, 2016. 

[8] Ben Goertzel and Cassio Pennachin. Artificial general intelligence, volume 2. Springer, 2007. 

[9] Leslie Lamport, Robert Shostak, and Marshall Pease. The byzantine general problem. ACM 
Transactions on Programming Languages and Systems (TOPLAS), 4(3):382–401, 1982. 

[10] Tor Lattimore and Marcus Hutter. Asymptotically optimal agents. In International Conference 
on Algorithmic Learning Theory, page 368–382. Springer, 2011. 

[11] Michael L Littman. Markov game a a framework for multi-agent reinforcement learning. In 
Proceedings of the eleventh international conference on machine learning, volume 157, page 
157–163, 1994. 

[12] Michael L Littman. Friend-or-foe q-learning in general-sum games. In ICML, volume 1, page 
322–328, 2001. 

[13] Michael L Littman. Value-function reinforcement learn in markov games. Cognitive Sys- 
tems Research, 2(1):55–66, 2001. 

[14] Laetitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Independent reinforcement 
learner in cooperative markov games: a survey regard coordination problems. The Knowl- 
edge Engineering Review, 27(01):1–31, 2012. 

[15] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan 
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint 
arXiv:1312.5602, 2013. 

[16] Laurent Orseau and Stuart Armstrong. Safely interruptible agents. In Uncertainty in Artificial 
Intelligence: 32nd Conference (UAI 2016), edit by Alexander Ihler and Dominik Janzing, 
page 557–566, 2016. 

[17] Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the art. Au- 
tonomous agent and multi-agent systems, 11(3):387–434, 2005. 

[18] Eduardo Rodrigues Gomes and Ryszard Kowalczyk. Dynamic analysis of multiagent q- 
learn with ε-greedy exploration. In Proceedings of the 26th Annual International Con- 
ference on Machine Learning, page 369–376. ACM, 2009. 

9 

http://www.newsweek.com/google-big-red-button-ai-artificial-intelligence-save-world-elon-musk-46675 
http://www.wired.co.uk/article/google-red-button-killswitch-artificial-intelligence 
http://arxiv.org/abs/1312.5602 


[19] Satinder Singh, Tommi Jaakkola, Michael L Littman, and Csaba Szepesvári. Conver- 
gence result for single-step on-policy reinforcement-learning algorithms. Machine learning, 
38(3):287–308, 2000. 

[20] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. 
MIT press Cambridge, 1998. 

[21] Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, 
Jaan Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement 
learning. arXiv preprint arXiv:1511.08779, 2015. 

[22] Gerald Tesauro. Temporal difference learn and td-gammon. Communications of the ACM, 
38(3):58–68, 1995. 

[23] Gerald Tesauro. Extending q-learning to general adaptive multi-agent systems. In Advances in 
neural information processing systems, page 871–878, 2004. 

[24] Gerald Tesauro and Jeffrey O Kephart. Pricing in agent economy use multi-agent q- 
learning. Autonomous Agents and Multi-Agent Systems, 5(3):289–304, 2002. 

[25] Xiaofeng Wang and Tuomas Sandholm. Reinforcement learn to play an optimal nash equi- 
librium in team markov games. In NIPS, volume 2, page 1571–1578, 2002. 

[26] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 
1992. 

[27] Michael Wunder, Michael L Littman, and Monica Babes. Classes of multiagent q-learning dy- 
namics with epsilon-greedy exploration. In Proceedings of the 27th International Conference 
on Machine Learning (ICML-10), page 1167–1174, 2010. 

10 

http://arxiv.org/abs/1511.08779 


A Exploration theorem 

We present here the complete proof of Theorem 1. The proof closely follow the result from [16] 
with exploration and interruption probability adapt to the multi-agent setting. We note that, for 
one agent, the probability of interruption be P(interruption) = θ and the probability of exploration 
be ǫ. In a multi-agent system, the probability of interruption be P(at least one agent be interrupted) 
so P(interruption) = 1 − P(no agent be interrupted) so P(interruption) = 1 − (1 − θ)m and the 
probability of exploration be ǫm if we consider exploration happens only when all agent explore at 
the same time. 

Theorem 1. Let c ∈]0, 1] and let nt(s) be the number of time the agent be in state s before time 
t. Then the two follow choice of ǫ be compatible with interruptions: 

• ∀t ∈ N, ∀s ∈ S, ǫt(s) = c/ 
m 

√ 

nt(s) 
• ∀t ∈ N, ǫt = c/log(t) 

Proof. Lemma B.2 of Singh et al ([19]) ensures that πǫi be GLIE. 

The difference for INT θ(πǫi ) be that exploration be slow because of the interruptions. Therefore, 
θ need to be control in order to ensure that infinite exploration be still achieved. We define the 
random variable Θ by Θi = 1 if agent i actually responds to the interruption and Θi = 0 otherwise. 
We define ξ in a similar way to represent the event of all agent take the uniform policy instead of 
the greedy one. 

1. Let θt(s) = 1 − c 
′/ m 

√ 

nt(s) with c 
′ ∈]0, 1]. We have P(a|s, nt(s)) ≥ P(a,Θ = 0, ξ = 

1|s, nt(s)) ≥ 
1 
|A|ǫ 

m 
t (s)(1− θt(s)) 

m = 1|A| 
m 
√ 
cc′ 

nt(s) 
which satisfies 

∑∞ 
t=1 P (a|s, nt(s)) = ∞ 

so by the extend Borell-Cantelli lemma action a be chosen infinitely often in state s and 
thus nt(s) → ∞ and ǫt(s) → 0 

2. Let θt = 1 − c 
′/log(t), c′ ∈]0, 1]. We define M a the diameter of the MDP, |A| be the 

maximum number of action available in a state and ∆t(s, s′) the time need to reach s′ 

from s. In a single agent setting: 

P[∆t(s, s′) < 2M ] ≥ P[∆t(s, s′) < 2M |actions sample accord to πs,s′ for 2M steps] 

× P[actions sample accord to πs,s′ for 2M steps] 

where πs,s′ the policy such that the agent take less than M step in expectation to reach 
s′ from s. We have: P[∆t(s, s′) < 2M ] = 1 − P[∆t(s, s′) ≥ 2M ] and use the Markov 

inequality, P[∆t(s, s′) ≥ 2M ] ≤ E(∆t(s,s 
′)) 

2M ≤ 
1 
2 (since M be an upper bound on the 

expectation of the number of step from state s to state s′), since ξ and 1− θ be decrease 
sequence we finally obtain: P[∆t(s, s′) < 2M ] ≥ 12|A| [P[ξt+2M = 1](1− θt+2M )] 

2M . 

Therefore, if we replace the probability of exploration and interruption by the value in 
the multi-agent setting, the probability to reach state s′ from state s in 2M step be at least 
1 

2|A| [cc 
′/ log(t+M)]4mM and the probability of take a particular action in this state be at 

least 1|A| [cc 
′/ log(t +M)]2m. Since 

∑∞ 
t=1 

1 
2|A|2 [cc 

′/ log(t +M)]m(4M+2) = ∞ then the 
extend Borell Cantelli lemma (Lemma 3 of Singh et al. [19]) guarantee that any action 
in the state s′ be take infinitely often. Since this be true for all state and action the result 
follows. 

B Independent learner 

Recall that agent be now give an interruption signal at each step that tell them whether an agent 
have be interrupt in the system. This interruption signal can be model by an interruption flag 
(Θt)t∈N ∈ {0, 1}N that equal 1 if an agent have be interrupt and 0 otherwise. Note that, contrary 
to I , it be an observation return by the environment. Therefore, the value of Θt represent whether 

11 



an agent have actually be interrupt at time t. If function I equal 1 but do not respond to the 
interruption (with probability 1− θt) then Θt = 0. With definition of interruption we adopted, it be 
possible to prove Lemma 2. 

Lemma 2. Let (x, r, a, y,Θ) ∈ E, then P(Θ|y, r, x, a) = P(Θ|x, a). 

Proof. Consider a tuple (x, r, a, y,Θ) ∈ E. We have P(y, r,Θ|x, a) = P(y, r|x, a,Θ)P(Θ|x, a) 
and P(y, r,Θ|x, a) = P(Θ|x, a, y, r)P(y, r|x, a). Besides, y = T (s, a) and r = r(s, a) and the 
function T and r be independent of Θ. Therefore, P(y, r|x, a,Θ) = P(y, r|x, a). The tuple 
(x, r, a, y,Θ) be sample from an actual trajectory so it reflect a transition and a reward that actually 
happen so P(y, r|x, a) > 0. We can simplify by P(y, r|x, a) and the result follows. 

Now, we assume that each agent do not learn on observation for which one of them have be 
interrupted. Let agent i be in a system with Q-values Q and follow an interruptible learn- 
ing policy with probability of interruption θ, where interrupt event be pruned. We denote by 
Premoved(y, r|x, ai, Q) the probability to obtain state y and reward r from the environment for this 
agent when it be in state x, performs it (own) action ai and no other agent be interrupted. These 
be the marginal probability in the sequence P (E). 

Premoved(y, r|x, ai, Q) = 
P(y, r,Θ = 0|x, ai, Q) 

∑ 

y′∈S,r′∈R P(y 
′, r′,Θ = 0|x, ai, Q) 

. 

Similarly, we denote by P0(y, r|x, ai, Q) the same probability when θ = 0, which corresponds to 
the non-interruptible setting. We first go back to the single agent case to illustrate the previous 
statement. Assume here that interruption be not restrict to the case of Definition 1 and that they 
can happen in any way. The consequence be that any observation e ∈ E can be remove to generate 
P (E) because any transition can be label a interrupted. It be for example possible to remove a 
transition from P (E) by remove all event associate with a give destination state y0, therefore 
make it disappear from the Markov game. 

Let x ∈ S and a ∈ A be the current state of the agent and the action it will choose. Let y0 ∈ S 
and θ0 ∈ (0, 1] and let u suppose that y0 be the only state in which interruption happen. Then we 
have Premoved(y0|x, a) < P0(y0|x, a) and Premoved(y|x, a) > P(y|x, a) ∀y 6= y0 because we only 
remove observation with y = y0. This implies that the MDP perceive by the agent be alter 
by interruption because the agent learns that P(T (s, a) = y0) = 0. Removing observation for 
different destination state but with the same state action pair in different proportion lead to a 
bias in the equilibrium learned.4 In our case however, Lemma 2 ensures that the previous situation 
will not happen, which allows u to prove Lemma 1 and then Theorem 4. 

Lemma 1. Let i ∈ {1, ...,m} be an agent. For any admissible θ use to generate the experience 
E and e = (y, r, x, ai, Q) ∈ P (E). Then P(y, r|x, ai, Q, θ) = P(y, r|x, ai, Q). 

Proof. Consider x ∈ S, i ∈ {1, ..,m} and u ∈ Ai. We denote the Q-values of the agent by Q. 
∑ 

y′∈S,r′∈R 
P(y′, r′,Θ = 0|x, u,Q) = 

∑ 

a∈A,ai=u 

∑ 

y′∈S,r′∈R 
P(y′, r′, a,Θ = 0|x, ai = u,Q) 

= 
∑ 

a∈A,ai=u 

∑ 

y′∈S,r′∈R 
P(y′, r′|x, a,Θ = 0, Q)P(a,Θ = 0|x, ai = u,Q) 

= 
∑ 

a∈A,ai=u 

∑ 

y′∈S,r′∈R 
P(y′, r′|x, a)P(Θ = 0|x, ai = u,Q)P(a|x, ai = u,Θ = 0, Q) 

= P(Θ = 0|x, ai = u,Q) 
∑ 

a∈A,ai=u 
P(a|x, ai = u,Θ = 0, Q)[ 

∑ 

y′∈S,r′∈R 
P(y′, r′|x, a)] 

= P(Θ = 0|x, ai = u,Q)[ 
∑ 

a∈A,ai=u 
P(a|x, ai = u,Θ = 0, Q)] = P(Θ = 0|x, ai = u,Q) 

Therefore, we have Premoved(y, r|x, ai = u,Q) = 
P(y,r,Θ=0|x,ai=u,Q) 

P(Θ=0|x,ai=u) 
so for any (x, ai, y, r, Q) ∈ P (E), P(y, r|x, ai = u, θ,Q) = Premoved(y, r|x, ai = u,Q) = 

4The example at https://agentfoundations.org/item?id=836 clearly illustrates this problem. 

12 



P(y, r|x, ai = u,Θ = 0, Q) = P(y, r|x, ai = u, θ = 0, Q). In particular, P(y, r|x, ai = u, θ,Q) 
do not depend on the value of θ. 

Theorem 4. Independent learner with processing function PINT , a neutral update rule and a 
sequence ǫ compatible with interruption verify dynamic safe interruptibility. 

Proof. We prove that PINT (E) achieves infinite exploration. The result from Theorem 1 still hold 
since we lower-bounded the probability of take an action in a specific state by the probability 
of take an action in this state when there be no interruptions. We actually use the fact that 
there be infinite exploration even if we remove all interrupt episode to show that there be infinite 
exploration. 

Now, we prove that P(Q 
(i) 
t+1(xt, at) = q|Q 

(1) 
t , ..., Q 

(m) 
t , xt, at, θt) be independent of θ. We fix 

i ∈ {1, ...,m} and (xt, at, rt, yt) ∈ PINT (E) where at ∈ Ai. With Q̃mt = Q 
(1) 
t , ..., Q 

(m) 
t we have 

the follow equality: 

P(Q 
(i) 
t+1(xt, at) = q|Q̃ 

m 
t , xt, at, θt) = 

∑ 

(r,y) 

P(F (xt, at, rt, yt, Q̃mt ) = q|Q̃ 
m 
t , xt, at, rt, yt, θt) 

·P(yt = y, rt = r|Q̃mt , xt, at, θt) 

The independence of F on θ still guarantee that the first term be independent of θ. However, 
at ∈ Ai so (rt, yt) be not independent of θt conditionally on (xt, at) a it be the case for joint 
action learner because interruption of other agent can change the joint action. The independence 
on θ of the second term be give by Lemma 1. 

13 


1 Introduction 
2 Model 
3 Interruptibility 
3.1 Safe interruptibility 
3.2 Dynamic safe interruptibility 

4 Exploration 
5 Joint Action Learners 
6 Independent Learners 
6.1 Independent Learners on matrix game 
6.2 Interruptions-aware Independent Learners 

7 Concluding Remarks 
Bibliography 
A Exploration theorem 
B Independent learner 

