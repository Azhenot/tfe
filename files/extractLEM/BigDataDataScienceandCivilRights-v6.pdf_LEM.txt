









































Microsoft Word - BigDataDataScienceandCivilRights FINAL.docx 


1 


Big Data, Data Science, and Civil Rights 


Solon Barocas, Elizabeth Bradley, Vasant Honavar, and Foster Provost 


Abstract 
Advances in data analytics bring with them civil right implications. Data-driven and 
algorithmic decision make increasingly determine how business target 
advertisement to consumers, how police department monitor individual or groups, how 
bank decide who get a loan and who do not, how employer hire, how college and 
university make admission and financial aid decisions, and much more. As data-driven 
decision increasingly affect every corner of our lives, there be an urgent need to ensure 
they do not become instrument of discrimination, barrier to equality, threat to social 
justice, and source of unfairness. In this paper, we argue for a concrete research agenda 
aim at address these concerns, comprise five area of emphasis: (i) Determining if 
model and model procedure exhibit objectionable bias; (ii) Building awareness of 
fairness into machine learn methods; (iii) Improving the transparency and control of 
data- and model-driven decision making; (iv) Looking beyond the algorithm(s) for 
source of bias and unfairness—in the myriad human decision make during the problem 
formulation and model process; and (v) Supporting the cross-disciplinary scholarship 
necessary to do all of that well. 



Over the past several years, government, academia, and the private sector have increasingly recognize 
that the use of big data and data science in more and more decision have important implication for civil 
rights, from racial discrimination to income equality to social justice. We have see many fruitful 
meeting and discussions, some of which be summarize briefly in an appendix below and have 
inform this report. However, a coherent research agenda for address these topic be only begin 
to emerge. 

The need for such an agenda be critical and timely. Big data and data science have begin to profoundly 
affect decision make because the modern world be more broadly instrument to gather data—from 
financial transactions, mobile phone calls, web and app interactions, emails, chats, Facebook posts, 
Tweets, cars, Fitbits, and on and on. Increasingly sophisticated algorithm can extract pattern from that 
data, enable important advance in science, medicine, and commerce. As described in a recent 60 
Minutes segment, for instance, IBM's Watson have help doctor identify treatment strategy for cancer.1 
Xerox now cedes hire decision for it 48,700 call-center job to software, cut attrition by a fifth.2 
And if you use the web, you have receive advertisement target base on fine-grained detail of your 
online behavior. 

Along with improve science and commerce come important civil right implications. For example, data 
analytics tool can capture and instantiate decision-making pattern that be implicitly discriminatory— 
and can do so unintentionally, simply from distil the data. Implicit discrimination by algorithm 
require our attention because such data-driven method be deployed in many of our most crucial social 
institutions. Risk assessment tools, for instance, be increasingly common in the criminal justice system, 
inform critical decision like pre-trial detention, bond amounts, sentence lengths, and parole. Last year, 
ProPublica complete a study of a risk assessment tool employ in a number of courtroom across that 


1“Artificial Intelligence,” 60 Minutes, October 9, 2016, http://www.cbsnews.com/videos/artificial-intelligence/. 
2 Walker, Joseph. “Meet the New Boss: Big Data: Companies Trade in Hunch-Based Hiring for Computer Modeling,” The Wall 
Street Journal, September 20, 2012, http://online.wsj.com/articles/SB10000872396390443890304578006252019616768. 



2 

nation that be equally accurate in predict whether black and white defendant will recidivate, but be far 
more likely to assign a high risk score to black defendant who do not go on to reoffend. White 
defendant who do go on to commit a crime when release were, in turn, more likely to be mislabeled a 
have low risk. The study establish that even when a model be equally accurate in make prediction 
about member of different racial groups, the false positive and false negative rate might differ between 
groups. In this case, the cost of false positive (unwarranted incarceration) disproportionately fell on one 
group.3 

What’s more, practitioner seldom provide explanation of the reason for decision make by such 
systems, give no view of why you get turn down for a job or flag a a terrorist. As Cathy O'Neil 
writes in Weapons of Math Destruction, “The model be use today be opaque, unregulated, and 
uncontestable, even when they’re wrong.”4 There be some momentum in the research community to fix 
this,5 but the issue be complex. There be the reason why a particular model make a decision—for 
example, you be deny credit because you’ve only be at your current job for two months, and you 
transact with merchant that defaulter frequent. And there be deeper reasons: why be these particular 
“attributes” deem by the system to be important evidence of default? Do we want to use this sort of 
evidence for this sort of decision making? Are the model codifying, and thereby reinforcing, the effect 
of prior unfairness? Are the model simply incorrect, due to unrecognized bias in the data? For 
example, a Kate Crawford and Ryan Calo note, “a 2015 study show that a machine-learning technique 
use to predict which hospital patient would develop pneumonia complication work well in most 
situations. But it make one serious error: it instruct doctor to send patient with asthma home even 
though such people be in a high-risk category.6 Because the hospital automatically sent patient with 
asthma to intensive care, these people be rarely on the ‘required further care’ record on which the 
system be trained.”7 

Year by year, the sophistication of these data-driven algorithm increases, and we already cannot escape 
their effects. Over the next decade, the data that feed these algorithm will become more pervasive and 
more personal. Progress toward address issue of civil right and fairness will be make only if 
incentive be put in place to bypass the considerable roadblock to success: (1) most computer scientist 
do not have a deep understand of issue of fairness and civil rights, and they thus be not traditional or 
natural issue for computer scientist to address; (2) traditional civil-rights scholar generally lack the 
sophisticated understand of big data and data science need to make substantive progress; (3) the key 
question for which answer be need be not broadly accepted a be important research problems. 

Determining if model learn from data exhibit objectionable bias 
Establishing whether a model discriminates on the basis of race, gender, age, or other legally protect or 
otherwise sensitive characteristic might seem like a straightforward task: do the model include any of 
these features? If not, one might quickly conclude that it decision cannot exhibit any bias. 
Unfortunately, there be a number of problem that might nevertheless result in a bias model, even if 
the model do not consider these feature explicitly. Existing scholarly work have address the notion 
that other feature can act a surrogate for explicitly sensitive characteristics, most famously location of 


3 Angwin, Julia, Jeff Larson, Surya Mattu, and Lauren Kirchner, “Machine Bias,” ProPublica, May 23, 2016, 
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing. 
4 O’Neil, Cathy, Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy, New York: 
Crown, 2016. 
5 Fairness, Accountability, and Transparency in Machine Learning, http://www.fatml.org. 
6 Caruana, Rich, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad, "Intelligible model for healthcare: 
Predicting pneumonia risk and hospital 30-day readmission," Proceedings of the 21th ACM SIGKDD International Conference 
on Knowledge Discovery and Data Mining, pp. 1721-1730. ACM, 2015. 
7 Crawford, Kate, and Ryan Calo, "There be a blind spot in AI research," Nature 538, no. 7625, 2016. 



3 

residence act a a surrogate for race. However, there be even more complex reason one might end up 
with a bias model, despite conscious effort to the contrary. 

The selection of the data use to build the models—the training data—is an important source of potential 
bias. Non-representative sample of the population will often lead to model that exhibit systematic 
errors. Such sample bias be easy to overlook and sometimes impossible to fully recognize; worse, 
standard validation method that depend on hold-out data drawn from the same sample will fail to reveal 
them. Even representative samples—or datasets that capture the entire population of interest—can fail to 
ensure that model perform equally well for different part of the population. When minority group do 
not follow the same pattern of behavior a the majority group, machine learn may struggle to model 
the behavior of the minority a effectively a the majority because there will be proportionally few 
example of the minority behavior from which to learn. Under these conditions, the dominant group in 
society may well enjoy relatively high accuracy rates. Training data may also encode prior prejudicial 
or bias assessments. A model train on historical hire data could easily lead to future hire 
decision that simply replicate the discrimination at work in previous human decision-making upon which 
such model hop to improve. Tainted training example might wrongly instruct the machine to see 
feature that actually predict success on the job a indicator of poor performance. 

Any bias exhibit by such model would be unintentional, but no less pernicious than decision that 
explicitly consider legally protect characteristics. Indeed, such model could be more pernicious 
precisely because the bias stem from problem with the training data that be easy to overlook. While 
data scientist often learn and care deeply about the challenge pose by sample bias, the difficulty or 
impossibility of establish ground truth, and the many way to measure model performance, there be an 
urgent need to support research to develop more rigorous method for establish whether a model 
exhibit objectionable bias. 

Law and policy often look to disparate impact analysis a a way to establish whether a decision procedure 
might be discriminatory. Such analysis ask whether the decision-making in question result in a disparity 
in outcome along line of race, gender, age or other protect characteristics. If, for example, white job 
applicant receive offer of employment at a rate 20% high than black applicants, this might suggest 
that the process of assess job applicant suffers from some kind of bias. It might, however, also 
indicate that the legitimate quality sought by the employer happen to be held at uneven rate by 
member of different racial groups. And it can be exceedingly difficult to establish whether a disparate 
impact stem from the former or the latter—especially when a researcher do not have direct access to 
the model or training data, or do not fully comprehend the data-generating process. 

Much of the research to date that aim to measure bias in system that rely on machine learn have be 
perform under adversarial conditions. Outsiders have observe how system respond to different 
inputs, and have attempt to uncover ethically salient difference in outputs. Such technique be 
commonly know a “algorithmic auditing” and they be what many commentator have in mind when 
they call for “algorithmic accountability.” Researchers operating under these condition face considerable 
challenge in make well-justified claim about the source of bias. By necessity, most of these audit 
have so far focus on system that be consumer-facing, to which researcher can input some data and 
observe the output; the result be a series of important case dominate by instance of machine learn 
apply to web services. Research be need not only to explore way to overcome these challenges, but 
also to understand whether different method would be necessary or more effective when organization 
attempt to audit their own model or grant outsider access. 

Supporting the emerge field of fairness-aware machine learn 
Computer scientist have begin to investigate how concern with fairness and reduce or eliminate 
unwanted discrimination might become part of the model-building process. In particular, researcher have 



4 

developed a number of different formal definition of fairness that model can be force to satisfy. One 
such notion be group parity, which require that model generate equal outcome for member of, for 
example, different racial groups. This requirement, however, might well be in tension with a notion of 
individual fairness, where assessment be expect to be maximally accurate for each individual. Others 
have propose that fairness would be best serve by require that a machine learn algorithm classify 
different population with the same true positive rate; for instance, when such an algorithm be use to 
decide who get a loan, the algorithm should have an equal probability of classify a loan-worthy 
individual a loan-worthy, irrespective of which subpopulation that individual be from. Still others have 
worried about case where accuracy rate be comparable, but where the type of error differs between 
group and where these error have different costs, a in ProPublica’s story on recidivism prediction. One 
group might be subject to a high rate of false positive (potentially very costly for the affected 
individual) while the other experience a high rate of false negative (potentially desirable from the 
individual’s perspective). 

These approach have tend to trace the source of unfairness back to different weakness in machine 
learning. Some assume that the main problem resides with training data, which may suffer from all sort 
of biases. The task, in such cases, be to compensate for flaw in the data from which the machine will 
learn. Others have identify case where machine learn fails to perform a well for minority group 
even when the training data be pristine, often because minority group do not conform to the same pattern 
a the majority group. This work assumes that the task be to develop method that can generate model 
with more even performance across a diverse population, but to do so without have to drag down the 
model’s performance for the majority group. 

The field have begin to grapple with the tension between different notion of fairness, but there be 
significant disagreement about the appropriate direction for future research. Some see hard trade-off 
between compete idea of fairness; others wonder if data scientist just need incentive to collect more 
data (both training example and a large set of features) to close the gap in performance; still others 
wonder if test of validity be even a legitimate measure of fairness, give how deeply bias may suffuse 
both the training data and the data held out for testing. Shielding machine learn from this taint may 
require a much more aggressive strategy that assumes that the actual distribution of quality and 
capacity across the population be far more equal than compromise data might suggest. Recent 
scholarship have further complicate this debate by establish the impossibility of achieve parity 
across a set of intuitive measure of fairness when group differ in their underlie rate of important 
behaviors.8 Thus hard choice will be necessary, and deep understand required. 

These discussion need to involve a more diverse range of stakeholders, both to improve the quality of the 
research and to gain legitimacy and buy-in. The value underlie the different notion of fairness need to 
be debate more openly and explicitly. Computer scientist can contribute in many way here—e.g., by 
offering technical solutions, such a taxonomy for fairness and algorithm for achieve fairness. 
Investment should provide the resources, both material and intellectual, to support and foster these 
discussions, and to push the field to develop tool that make clear the full range of possibility for 
define and achieve fairness. Future research will also need to consider how organization would 
deploy these method in practice. Paradoxically, most of the method propose so far can only achieve 
fairness by take class membership (e.g., gender) into account explicitly. In other words, organization 
would have to collect information that could easily serve a the basis for intentional discrimination in 

8 Kleinberg, Jon, Sendhil Mullainathan, and Manish Raghavan, "Inherent trade-off in the fair determination of risk scores," 
arXiv:1609.05807, 2016. 
Chouldechova, Alexandra, "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments," arXiv: 
1703.00056, 2017. 
Corbett-Davies, Sam, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq, "Algorithmic decision make and the cost of 
fairness," arXiv:1701.08230, 2017. 



5 

order to prevent unintentional discrimination. Organizations might also balk at demand to collect more 
information, even in the interest of improve how well machine learn performs for minority groups, if 
do so would appear to intrude on people’s privacy, involve significant expense, or create a perceive 
acknowledgement of wrongdoing or even actual liability. 

Providing transparency into and control over the data-driven inference make about citizen 
Discussions and analysis of fairness and bias in algorithmic decision-making be stymie by the difficulty 
for citizens, users, regulators, and even researcher to understand the reason that data-driven inference 
be made. Research into building so-called “interpretable” or “comprehensible” machine-learned model 
have receive some attention for decades, but many researcher and practitioner still believe that such 
models—beyond the simplest—are essentially black boxes. Recent work suggests that we can build 
model that be possible for human to meaningfully inspect without sacrifice accuracy,9 but this area 
need substantially more attention, both in clarify what count a “interpretable” or “comprehensible” 
and when such property be desirable or necessary.10 

However, building such model be only one strand of research into transparency of data-driven inferences. 
Often the crucial interest be not in understand the model, but in understand the precise reason for a 
particular inference. As discuss above: I be deny credit. Why? Research into explain the 
decision make by data-driven system be even sparser than research into comprehensible models—but 
possibly more practically useful. Encouragingly, explain an individual decision may actually be easy 
than explain a complex model behind the decision.11 For example, one may take a counterfactual 
approach:12 consider the algorithm input a a collection of evidence, what be the minimal set of 
evidence the removal of which would inhibit the inference? If we be interested in provide an 
explanation that might inform future behavior, we might instead ask: which feature (characteristics, 
aspect of behavior, etc.) would be the least costly for an individual to change, so a to produce the 
desire change in the inference?13 We need much more research on explain inferences, and do so 
efficiently and effectively, before we can be confident that we be indeed give sufficient transparency. 

As a society, we also may want to give citizen control over the data that be use to make inference 
about them. Someone may not want their visit history to gay right website to be use in decisions— 
automatic or otherwise—that be make about him or her. Decision-specific transparency seem to be a 
prerequisite for give such control.14 

More deeply, in order truly to understand the civil right implication of data-driven systems, we don’t 
only need to understand the model and the decision that they make, but we also need to understand why 
the model be a they are! This tie model comprehensibility to all the other research stream described 
in this document, a model be a they be because of the selection of machine learn algorithm, the 
selection of training data (and evaluation data), and more insidiously, many other decision make in the 
process of formulate the problem and the evolution of the system. 

Looking beyond the algorithm for the source of unfairness, discrimination, etc. 

9 Zeng, Jiaming, Berk Ustun, and Cynthia Rudin, "Interpretable classification model for recidivism prediction," 
arXiv:1503.07810, 2015. 
10 Lipton, Zachary C, "The mythos of model interpretability," arXiv:1606.03490, 2016. 
11 Martens, David, and Foster Provost, "Explaining Data-Driven Document Classifications," MIS Quarterly 38.1 , pp 73-99, 
2014. 
12 Ibid. and Chen, et al. “Enhancing Transparency and Control when Drawing Data-Driven Inferences about Individuals,” 2016 
ICML Workshop on Human Interpretability in Machine Learning, https://arxiv.org/abs/1606.08063. 
13 NB: This be a much more difficult problem than the former, a it require model causal relations, costs/benefits, and 
statistical dependency involve the individual, rather than just causal relation between the input and output of the model. 
14 Ibid. and Chen, et al. “Enhancing Transparency and Control when Drawing Data-Driven Inferences about Individuals,” 2016 
ICML Workshop on Human Interpretability in Machine Learning, https://arxiv.org/abs/1606.08063. 



6 

One very crucial aspect of data-driven decision-making be only just begin to be take into account in 
discussion of and research into ethics, data science, and civil rights, although it do not surprise savvy 
practitioners: the technical formulation of the problem make all the difference. It have long be accepted 
within data science circle that building data-driven system be a process15 that involves carefully 
understand the problem to be addressed, understand the data available (sometimes at a cost), and 
formulate the problem to which machine learning/statistical inference algorithm will be applied. Take 
a an example supervise predictive modeling. Formulating the problem involves: decide on the 
instance to be modeled, craft a definition of the target variable, obtain “labels” (ground truth or 
proxy for it) for the training data, select an appropriate sample of the data from which to train, and 
then engineering a set of feature that will be predictive (or use algorithm that build the feature 
autonomously). In practice, each of these choice often incorporates approximations, proxies, surrogates, 
and biases. In the ideal case, these be chosen with full consideration of the likely (side) effects, but in 
practice, not only be the consequence of the choice not apparent; sometimes the bias and 
approximation be not even well understood by the researcher and practitioners. For example, an unseen 
racial bias in prior decision-making may be recapitulate in model learn from those data. An 
unrecognized selection bias in data sample may miss a critical subpopulation—for example, consider 
sample data from user of smartphones. 

A robust understand of the ethical use of data-driven system need substantial focus on the possible 
threat to civil right that may result from the formulation of the problem. Such threat be insidious, 
because problem formulation be iterative. Many decision be make early and quickly, before there be any 
notion that the effort will lead to a successful system, and only rarely be prior problem-formulation 
decision revisit with a critical eye. In addition, system whose underlie knowledge evolves over 
time, be it via continual machine learn or manual intervention, may themselves be make implicit 
decision on (for example) the selection of the population. What be the implication for fair treatment, if 
fair treatment be not consider in the design of the selection mechanisms? A system that start with a 
small bias may unwittingly magnify it. 

Creating cross-disciplinary scholar 
The recognition that application of big data and data science can implicate civil right have spur call 
for great involvement of social scientists, lawyers, and policy expert in the development, deployment, 
and review of data-driven systems. While laudable, call for cross-disciplinary collaboration, and 
especially collaborative research, on these issue rarely consider the challenge that member from these 
different community will face when attempt to engage with one another. Those who work with data 
science, include computer scientists, be rarely train in law and policy; expert in civil right rarely 
have a background in machine learn or computational statistics. Neither be well prepared to identify the 
many way that a particular application of data science may implicate civil rights—or what to do in such 
cases. 

The difficulty of cross-disciplinary collaboration have already lead to a rather trouble pattern in work on 
civil right and data science. Critical write often struggle to recognize how machine-learned system 
differ from other type of formalize decision-making and how these difference introduce novel danger 
for civil rights, only some of which can be effectively address with standard policy instruments. 
Likewise, data science researcher concerned with civil right have tend to tackle issue of fairness a if 
such weighty topic have not already receive considerable attention in law, social science, and 
philosophy. 

Work integrate civil right and data science cannot be easily divide between collaborators, where the 
more expert team member would handle each task. Meaningful legal analysis will require technical 

15 F. Provost & T. Fawcett, Data Science for Business, O’Reilly Media, 2013. 



7 

expertise; rigorous technical review will depend on a nuanced understand of legal concepts. Valuable 
breakthrough be most likely to come from researcher who combine expertise in both domains. Future 
investment in research should foster collaboration that do more than put different community in 
contact; investment should support the training necessary to cultivate a future generation of researcher 
who be simultaneously expert in both fields. 

While facilitation of collaboration between researcher be important, a workforce that understands the 
relationship between ethic and data be also key. The recent groundswell of interest in data science, and 
the emergence of new interdisciplinary graduate and undergraduate program in this area, provide a 
natural opportunity here. The challenge be that these curriculum be already overly crowd and often 
patch together from exist course in different departments. Adding a single course about data, ethics, 
algorithmic bias, fairness, accountability and the law to such a program would be a good start; a good 
idea would be to thread the associate idea through all of the course in a coherent manner. As data 
science method and tool become integral and essential element of research across a broad range of 
disciplines, it would be worthwhile to broaden the exist research ethic training requirement to 
include these aspects. A national-level conversation about those important ideas—what they be and how 
to teach them—could help individual institution with those initiatives. 

Needless to say, it will be important to engage the educational research community in that conversation. 
Data science be also make inroad into the high-school curriculum, which provide even early 
opportunity to weave together knowledge about ethics, law, and data. There the issue be somewhat 
different; not only crowd curricula, but also the force of standardize testing, a well a teacher 
training. Development of easy-to-deploy material that convey the important concept and issues, while 
also align with exist learn objectives, will be essential to success here. This, too, should involve 
the educational research community. Initiatives like the National Science Foundation’s “STEM + C”16 
and the National Academy of Engineering’s series of workshop in this area17 can also usefully inform 
these conversations. 




















16 “STEM + Computing Partnerships (STEM+C),” National Science Foundation, 2017, 
https://www.nsf.gov/funding/pgm_summ.jsp?pims_id=505006. 
17 “Roundtable on Data Science Post-Secondary Education Meeting #5 Integrating Societal and Ethical Issues into Data Science,” 
The National Academies of Sciences Engineering Medicine, 2017, 
http://sites.nationalacademies.org/DEPS/BMSA/DEPS_178021. 



8 

Appendix 
Several recent meeting and report have brought to light issue and concern regard big data, data 
science, algorithmic decision-making, and civil rights: 


● The White House issue Big Data report in 2014, notably raise concern about discrimination 
and the inscrutability of algorithm and call for more research to establish how data science 
might implicate civil right and how to mitigate against these dangers. 

● The Federal Trade Commission issue it own report at the start of 2016, lay out how exist 
law apply to commercial use of data science that raise concern with discrimination and fairness, 
but also note gap in policy where commercial actor should exercise careful judgment, despite 
the lack of clear technical or ethical guidance. 

● The White House follow with a more detailed and narrowly focus report in 2016 on “Big 
Data: A Report on Algorithmic Systems, Opportunity, and Civil Rights”, which consider how 
data science could be a boon—but also a threat—to civil right in the area of consumer credit, 
employment, education, and criminal justice. 

● Alongside this report, the White House also release a document outline a National Privacy 
Research Strategy, which, notably, call for new work on the danger pose by “analytical 
algorithms” a issue distinct from traditional privacy concerns. 

● The White House recently issue another report on artificial intelligence that laid out in great 
technical detail concern with "Fairness, Safety, and Governance”—and the need for further 
investment in and research on these topics. 




For citation use: Barocas, Bradley, Honavar, & Provost. (2017). Big Data, Data Science, and Civil Rights: 
A white paper prepared for the Computing Community Consortium committee of the Computing 
Research Association. http://cra.org/ccc/resources/ccc-led-whitepapers/ 


This material be base upon work support by the National Science Foundation under Grant No. 
1136993. Any opinions, findings, and conclusion or recommendation express in this material be 
those of the author and do not necessarily reflect the view of the National Science Foundation. 


