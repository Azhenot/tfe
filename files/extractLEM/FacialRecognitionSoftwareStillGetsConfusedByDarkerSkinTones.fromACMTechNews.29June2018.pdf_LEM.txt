






































Businessweek - Bloomberg 


Businessweek - Bloomberg 

@lizette_chapman More story by Lizette Chapman 

Technology 

Facial recognition software still get confuse by darker skin tones. 

By and 

26 juin 2018 à 11:00 UTC+2 Corrected 26 juin 2018 à 17:20 UTC+2 

Businessweek - Bloomberg https://www.bloomberg.com/businessweek 

1 sur 5 29-06-18 à 19:59 



Businessweek - Bloomberg https://www.bloomberg.com/businessweek 

2 sur 5 29-06-18 à 19:59 



Brian Brackeen, founder of Kairos AR Inc., in Philadelphia. 

Photographer: Yael Malka for Bloomberg Businessweek 

A couple of year ago, a Brian Brackeen be prepare to pitch his 
facial recognition software to a potential customer a a convenient, 
secure alternative to passwords, the software stop working. 
Panicked, he try adjust the room’s lighting, then the Wi-Fi 
connection, before he realize the problem be his face. Brackeen be 
black, but like most facial recognition developers, he’d train his 
algorithm with a set of mostly white faces. He get a white, blond 
colleague to pose for the demo, and they close the deal. It be a 
Pyrrhic victory, he says: “It be like have your own child not 
recognize you.” 

At Kairos AR Inc., his 40-person facial recognition company in Miami, 
Brackeen say he’s improve the software by add more black and 
brown face to his image sets, but the result be still imperfect. The 
same problem bedevils company include Microsoft, IBM, and 
Amazon and their grow range of customer for similar services. 
Facial recognition be be use to help India’s government find 
miss children, and British news outlet spot celebrity at royal 
weddings. More controversially, it’s be use in a grow number of 
context by law enforcement agencies, which be often less than 
forthcoming about what they’re use it for and whether they’re do 
enough about potential pitfalls. Brackeen believe the problem of racial 
bias be serious enough that law enforcement shouldn’t use facial 
recognition at all. 

Microsoft, IBM, and China’s Face++ misidentified darker-skinned 
woman a often a 35 percent of the time and darker-skinned men 12 
percent of the time, accord to a report publish by MIT researcher 
early this year. The gender difference owes to a small set of women’s 
faces. Such software can see only what it’s be taught to see. 

In recent months, major vendor say they’ve diversified their training 
data set to include darker-colored face and have make stride in 
reduce bias. Microsoft Corp. say it plan to announce on June 26 
that it will release a version of it software tool Face API that now 
misidentifies darker-skinned women, the group for which it’s most 
error-prone, only 1.9 percent of the time. (The company say it error 
rate for other group be zero percent.) International Business Machines 
Corp. say it Watson Visual Recognition, which be similarly at it 
weak in identify darker-skinned women, get it wrong 3.5 percent 
of the time. Both IBM and Microsoft acknowledge their result haven’t 
be independently verify and that real-world error rate could be 
different from those for their collection of stock images. The maker of 
Face++ didn’t respond to request for comment. 

“An inaccurate system will implicate people for crime they didn’t 

Businessweek - Bloomberg https://www.bloomberg.com/businessweek 

3 sur 5 29-06-18 à 19:59 



commit” 

It’s Amazon.com Inc. that may have to worry most about real-world 
results. On June 15 a group of Amazon shareholder sent the company 
a letter ask it to stop marketing it Rekognition system to police 
department and other government agency until guideline be 
developed to ensure the software isn’t lead to civil right violations. 
In another letter the follow week, Amazon worker ask Chief 
Executive Officer Jeff Bezos to stop sell Rekognition to law 
enforcement agency give “the U.S.’s increasingly inhumane 
treatment of refugee and immigrants.” Amazon decline to comment 
for this story. 

Government agency have no broadly agreed-upon standard for 
evaluate facial recognition systems. A 2016 study by Georgetown 
University found that almost none of the law enforcement agency that 
use facial recognition require supplier to meet a minimum threshold 
for overall accuracy, let alone racial disparities. “An inaccurate system 
will implicate people for crime they didn’t commit and shift the 
burden to innocent defendant to show they be not who the system 
say they are,” say Jennifer Lynch, senior staff attorney for the 
Electronic Frontier Foundation, an advocate for civil liberty online. 

And the problem isn’t just in the U.S. This spring, a report from Big 
Brother Watch, a U.K. civil right group that examine public-records 
request make to several law enforcement agency use facial 
recognition, conclude that the system be terrible. For example, the 
South Wales Police, which use facial recognition to screen people at 
public events, report more than 90 percent of the match be 
erroneous. The department say in a statement on it website that the 
use of facial recognition have be a “resounding success.” It didn’t 
respond to an interview request. 

Makers of facial recognition technology, include Microsoft and IBM, 
have say the software continue to be a work in progress, with 
engineer focus on improve accuracy and transparency around 
how the improvement be be made. They say the technology have 
help bust sex trafficker and apprehend would-be terrorists, though 
they’ve provide few details. 

Andrew Ferguson, a law professor at the University of the District of 
Columbia and the author of The Rise of Big Data Policing, say use 
the powerful technology while it’s still under development with scant 
regulation be dangerous. Law enforcement agency have consistently 
botch their adoption of novel tech. “Police be beta-testing new 
technology or pilot new idea in police without a vet process 
to think through bias or how it might affect citizens’ civil rights,” he 
says. 

Engineers be improve how they train algorithm a more agency 

Businessweek - Bloomberg https://www.bloomberg.com/businessweek 

4 sur 5 29-06-18 à 19:59 



purchase the software, but they may not be able to head off grow 
call for regulation. The author of the Georgetown report call for state 
and federal law govern how police department use facial 
recognition and call on the police to test regularly for algorithmic bias. 
In April a group of civil right organization say it be “categorically 
unethical” to deploy real-time facial recognition analysis of footage 
capture by police body cameras. 

Some, include the EFF’s Lynch, argue that their concern will only 
increase a the technology improves. An accurate image merge with 
personal information about an individual such a location, family ties, 
voting records, and the like can be pull together by authority use 
product such a those from Palantir Technologies Inc. to create a 
digital dossier on people without their consent or knowledge. “Even if 
we have a 100 percent accurate system, I don’t want that system,” 
Lynch says. “That mean we can no longer walk around and interact 
with people without the government know who we are, where we 
are, and who we’re talk to.” 

(Corrects Microsoft error rate in fourth paragraph) 

BOTTOM LINE - Microsoft say it’s cut it facial recognition error rate 
to zero percent for everyone except darker-skinned women, but a with 
rivals, those number be likely to rise in the real world. 

Before it's here, it's on the Bloomberg Terminal. LEARN MORE 

July 2, 2018 

Businessweek - Bloomberg https://www.bloomberg.com/businessweek 

5 sur 5 29-06-18 à 19:59 


