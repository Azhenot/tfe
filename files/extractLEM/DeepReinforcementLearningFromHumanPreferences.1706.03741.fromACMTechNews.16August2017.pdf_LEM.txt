


















































Deep Reinforcement Learning 
from Human Preferences 

Paul F Christiano 
OpenAI 

paul@openai.com 

Jan Leike 
DeepMind 

leike@google.com 

Tom B Brown 
nottombrown@gmail.com 

Miljan Martic 
DeepMind 

miljanm@google.com 

Shane Legg 
DeepMind 

legg@google.com 

Dario Amodei 
OpenAI 

damodei@openai.com 

Abstract 

For sophisticated reinforcement learn (RL) system to interact usefully with 
real-world environments, we need to communicate complex goal to these systems. 
In this work, we explore goal define in term of (non-expert) human preference 
between pair of trajectory segments. We show that this approach can effectively 
solve complex RL task without access to the reward function, include Atari 
game and simulated robot locomotion, while provide feedback on less than 
1% of our agent’s interaction with the environment. This reduces the cost of 
human oversight far enough that it can be practically apply to state-of-the-art 
RL systems. To demonstrate the flexibility of our approach, we show that we can 
successfully train complex novel behavior with about an hour of human time. 
These behavior and environment be considerably more complex than any which 
have be previously learn from human feedback. 

1 Introduction 

Recent success in scale reinforcement learn (RL) to large problem have be driven in domain 
that have a well-specified reward function (Mnih et al., 2015, 2016; Silver et al., 2016). Unfortunately, 
many task involve goal that be complex, poorly-defined, or hard to specify. Overcoming this 
limitation would greatly expand the possible impact of deep RL and could increase the reach of 
machine learn more broadly. 

For example, suppose that we want to use reinforcement learn to train a robot to clean a table or 
scramble an egg. It’s not clear how to construct a suitable reward function, which will need to be a 
function of the robot’s sensors. We could try to design a simple reward function that approximately 
capture the intend behavior, but this will often result in behavior that optimizes our reward 
function without actually satisfy our preferences. This difficulty underlies recent concern about 
misalignment between our value and the objective of our RL system (Bostrom, 2014; Russell, 
2016; Amodei et al., 2016). If we could successfully communicate our actual objective to our agents, 
it would be a significant step towards address these concerns. 

If we have demonstration of the desire task, we can extract a reward function use inverse 
reinforcement learn (Ng and Russell, 2000). This reward function can then be use to train 
an agent with reinforcement learning. More directly, we can use imitation learn to clone the 
demonstrate behavior. However, these approach be not directly applicable to behavior that be 
difficult for human to demonstrate (such a control a robot with many degree of freedom but 
very non-human morphology). 

ar 
X 

iv 
:1 

70 
6. 

03 
74 

1v 
3 

[ 
st 

at 
.M 

L 
] 

1 
3 

Ju 
l 2 

01 
7 



An alternative approach be to allow a human to provide feedback on our system’s current behavior 
and to use this feedback to define the task. In principle this fit within the paradigm of reinforcement 
learning, but use human feedback directly a a reward function be prohibitively expensive for RL 
system that require hundred or thousand of hour of experience. In order to practically train deep 
RL system with human feedback, we need to decrease the amount of feedback require by several 
order of magnitude. 

Our approach be to learn a reward function from human feedback and then to optimize that reward 
function. This basic approach have be consider previously, but we confront the challenge involve 
in scale it up to modern deep RL and demonstrate by far the most complex behavior yet learn 
from human feedback. 

In summary, we desire a solution to sequential decision problem without a well-specified reward 
function that 

1. enables u to solve task for which we can only recognize the desire behavior, but not 
necessarily demonstrate it, 

2. allows agent to be taught by non-expert users, 

3. scale to large problems, and 

4. be economical with user feedback. 

RL algorithm environment 

observation 

action 

human 
feedback 

reward predictorpredicted 
reward 

Figure 1: Schematic illustration of our approach: 
the reward predictor be train asynchronously 
from comparison of trajectory segments, and the 
agent maximizes predict reward. 

Our algorithm fit a reward function to the hu- 
man’s preference while simultaneously training 
a policy to optimize the current predict reward 
function (see Figure 1). We ask the human to 
compare short video clip of the agent’s behav- 
ior, rather than to supply an absolute numerical 
score. We found comparison to be easy for hu- 
man to provide in some domains, while be 
equally useful for learn human preferences. 
Comparing short video clip be nearly a fast a 
compare individual states, but we show that 
the result comparison be significantly more 
helpful. Moreover, we show that collect feed- 
back online improves the system’s performance 
and prevents it from exploit weakness of 
the learn reward function. 

Our experiment take place in two domains: Atari game in the Arcade Learning Environment (Belle- 
mare et al., 2013), and robotics task in the physic simulator MuJoCo (Todorov et al., 2012). We 
show that a small amount of feedback from a non-expert human, range from fifteen minute to five 
hours, suffices to learn most of the original RL task even when the reward function be not observable. 
We then consider some novel behavior in each domain, such a perform a backflip or drive 
with the flow of traffic. We show that our algorithm can learn these behavior from about an hour of 
feedback—even though it be unclear how to hand-engineer a reward function that would incentivize 
them. 

1.1 Related Work 

A long line of work study reinforcement learn from human rating or rankings, include Akrour 
et al. (2011), Pilarski et al. (2011), Akrour et al. (2012), Wilson et al. (2012), Sugiyama et al. (2012), 
Wirth and Fürnkranz (2013), Daniel et al. (2015), El Asri et al. (2016), Wang et al. (2016), and 
Wirth et al. (2016). Other line of research considers the general problem of reinforcement learn 
from preference rather than absolute reward value (Fürnkranz et al., 2012; Akrour et al., 2014), 
and optimize use human preference in setting other than reinforcement learn (Machwe and 
Parmee, 2006; Secretan et al., 2008; Brochu et al., 2010; Sørensen et al., 2016). 

Our algorithm follow the same basic approach a Akrour et al. (2012) and Akrour et al. (2014). They 
consider continuous domain with four degree of freedom and small discrete domains, where they 
can assume that the reward be linear in the expectation of hand-coded features. We instead consider 

2 



physic task with dozen of degree of freedom and Atari task with no hand-engineered features; 
the complexity of our environment force u to use different RL algorithm and reward models, and 
to cope with different algorithmic tradeoffs. One notable difference be that Akrour et al. (2012) and 
Akrour et al. (2014) elicit preference over whole trajectory rather than short clips. So although we 
gather about two order of magnitude more comparisons, our experiment require less than one order 
of magnitude more human time. Other difference focus on change our training procedure to cope 
with the nonlinear reward model and modern deep RL, for example use asynchronous training and 
ensembling. 

Our approach to feedback elicitation closely follow Wilson et al. (2012). However, Wilson et al. 
(2012) assumes that the reward function be the distance to some unknown “target” policy (which be 
itself a linear function of hand-coded features). They fit this reward function use Bayesian inference, 
and rather than perform RL they produce trajectory use the MAP estimate of the target policy. 
Their experiment involve “synthetic” human feedback which be drawn from their Bayesian model, 
while we perform experiment with feedback gather from non-expert users. It be not clear if the 
method in Wilson et al. (2012) can be extend to complex task or if they can work with real human 
feedback. 

MacGlashan et al. (2017), Pilarski et al. (2011), Knox and Stone (2009), and Knox (2012) perform 
experiment involve reinforcement learn from actual human feedback, although their algorithmic 
approach be less similar. In MacGlashan et al. (2017) and Pilarski et al. (2011), learn only occurs 
during episode where the human trainer provide feedback. This appear to be infeasible in domain 
like Atari game where thousand of hour of experience be require to learn a high-quality policy, 
and would be prohibitively expensive even for the simplest task we consider. TAMER (Knox, 2012; 
Knox and Stone, 2013) also learn a reward function, however they consider much simpler setting 
where the desire policy can be learn relatively quickly. 

Our work could also be see of a specific instance of the cooperative inverse reinforcement learn 
framework (Hadfield-Menell et al., 2016). This framework considers a two-player game between 
a human and a robot interact with an environment with the purpose of maximize the human’s 
reward function. In our set the human be only allow to interact with this game by state their 
preferences. 

Compared to all prior work, our key contribution be to scale human feedback up to deep reinforcement 
learn and to learn much more complex behaviors. This fit into a recent trend of scale reward 
learn method to large deep learn systems, for example inverse RL (Finn et al., 2016), imitation 
learn (Ho and Ermon, 2016; Stadie et al., 2017), semi-supervised skill generalization (Finn et al., 
2017), and bootstrapping RL from demonstration (Silver et al., 2016; Hester et al., 2017). 

2 Preliminaries and Method 

2.1 Setting and Goal 

We consider an agent interact with an environment over a sequence of steps; at each time t the 
agent receives an observation ot ∈ O from the environment and then sends an action at ∈ A to the 
environment. 

In traditional reinforcement learning, the environment would also supply a reward rt ∈ R and the 
agent’s goal would be to maximize the discount sum of rewards. Instead of assume that the 
environment produce a reward signal, we assume that there be a human overseer who can express 
preference between trajectory segments. A trajectory segment be a sequence of observation and 
actions, σ = ((o0, a0), (o1, a1), . . . , (ok−1, ak−1)) ∈ (O ×A)k. Write σ1 � σ2 to indicate that the 
human prefer trajectory segment σ1 to trajectory segment σ2. Informally, the goal of the agent be 
to produce trajectory which be prefer by the human, while make a few query a possible to 
the human. 

More precisely, we will evaluate our algorithms’ behavior in two ways: 

3 



Quantitative: We say that preference � be generate by a reward function1 r : O ×A → R if(( 
o10, a 

1 
0 

) 
, . . . , 

( 
o1k−1, a 

1 
k−1 
)) 
� 
(( 
o20, a 

2 
0 

) 
, . . . , 

( 
o2k−1, a 

2 
k−1 
)) 

whenever 

r 
( 
o10, a 

1 
0 

) 
+ · · ·+ r 

( 
o1k−1, a 

1 
k−1 
) 
> r 
( 
o20, a 

2 
0 

) 
+ · · ·+ r 

( 
o2k−1, a 

2 
k−1 
) 
. 

If the human’s preference be generate by a reward function r, then our agent ought to 
receive a high total reward accord to r. So if we know the reward function r, we can 
evaluate the agent quantitatively. Ideally the agent will achieve reward nearly a high a if it 
have be use RL to optimize r. 

Qualitative: Sometimes we have no reward function by which we can quantitatively evaluate 
behavior (this be the situation where our approach would be practically useful). In these 
cases, all we can do be qualitatively evaluate how well the agent satisfies to the human’s 
preferences. In this paper, we will start from a goal express in natural language, ask a 
human to evaluate the agent’s behavior base on how well it fulfills that goal, and then 
present video of agent attempt to fulfill that goal. 

Our model base on trajectory segment comparison be very similar to the trajectory preference 
query use in Wilson et al. (2012), except that we don’t assume that we can reset the system to 
an arbitrary state2 and so our segment generally begin from different states. This complicates the 
interpretation of human comparisons, but we show that our algorithm overcomes this difficulty even 
when the human raters have no understand of our algorithm. 

2.2 Our Method 

At each point in time our method maintains a policy π : O → A and a reward function estimate 
r̂ : O ×A → R, each parametrized by deep neural networks. 
These network be update by three processes: 

1. The policy π interacts with the environment to produce a set of trajectory {τ1, . . . , τ i}. 
The parameter of π be update by a traditional reinforcement learn algorithm, in order 
to maximize the sum of the predict reward rt = r̂(ot, at). 

2. We select pair of segment 
( 
σ1, σ2 

) 
from the trajectory {τ1, . . . , τ i} produce in step 1, 

and send them to a human for comparison. 
3. The parameter of the mapping r̂ be optimize via supervise learn to fit the comparison 

collect from the human so far. 

These process run asynchronously, with trajectory flow from process (1) to process (2), human 
comparison flow from process (2) to process (3), and parameter for r̂ flow from process (3) 
to process (1). The follow subsection provide detail on each of these processes. 

2.2.1 Optimizing the Policy 

After use r̂ to compute rewards, we be left with a traditional reinforcement learn problem. We 
can solve this problem use any RL algorithm that be appropriate for the domain. One subtlety be 
that the reward function r̂ may be non-stationary, which lead u to prefer method which be robust 
to change in the reward function. This lead u to focus on policy gradient methods, which have be 
apply successfully for such problem (Ho and Ermon, 2016). 

In this paper, we use advantage actor-critic (A2C; Mnih et al., 2016) to play Atari games, and trust 
region policy optimization (TRPO; Schulman et al., 2015) to perform simulated robotics tasks. In 

1Here we assume here that the reward be a function of the observation and action. In our experiment in 
Atari environments, we instead assume the reward be a function of the precede 4 observations. In a general 
partially observable environment, we could instead consider reward function that depend on the whole sequence 
of observations, and model this reward function with a recurrent neural network. 

2Wilson et al. (2012) also assumes the ability to sample reasonable initial states. But we work with high 
dimensional state space for which random state will not be reachable and the intend policy inhabits a 
low-dimensional manifold. 

4 



each case, we use parameter setting which have be found to work well for traditional RL tasks. 
The only hyperparameter which we adjust be the entropy bonus for TRPO. This be because TRPO 
relies on the trust region to ensure adequate exploration, which can lead to inadequate exploration if 
the reward function be changing. 

We normalize the reward produce by r̂ to have zero mean and constant standard deviation. This be 
a typical preprocessing step which be particularly appropriate here since the position of the reward be 
underdetermined by our learn problem. 

2.2.2 Preference Elicitation 

The human overseer be give a visualization of two trajectory segments, in the form of short movie 
clips. In all of our experiments, these clip be between 1 and 2 second long. 

The human then indicates which segment they prefer, that the two segment be equally good, or that 
they be unable to compare the two segments. 

The human judgment be record in a database D of triple 
( 
σ1, σ2, µ 

) 
, where σ1 and σ2 be the 

two segment and µ be a distribution over {1, 2} indicate which segment the user preferred. If the 
human selects one segment a preferable, then µ put all of it mass on that choice. If the human 
mark the segment a equally preferable, then µ be uniform. Finally, if the human mark the segment 
a incomparable, then the comparison be not include in the database. 

2.2.3 Fitting the Reward Function 

We can interpret a reward function estimate r̂ a a preference-predictor if we view r̂ a a latent factor 
explain the human’s judgment and assume that the human’s probability of prefer a segment 
σi depends exponentially on the value of the latent reward sum over the length of the clip:3 

P̂ 
[ 
σ1 � σ2 

] 
= 

exp 
∑ 
r̂ 
( 
o1t , a 

1 
t 

) 
exp 

∑ 
r̂(o1t , a 

1 
t ) + exp 

∑ 
r̂(o2t , a 

2 
t ) 
. (1) 

We choose r̂ to minimize the cross-entropy loss between these prediction and the actual human 
labels: 

loss(r̂) = − 
∑ 

(σ1,σ2,µ)∈D 

µ(1) log P̂ 
[ 
σ1 � σ2 

] 
+ µ(2) log P̂ 

[ 
σ2 � σ1 

] 
. 

This follow the Bradley-Terry model (Bradley and Terry, 1952) for estimate score function 
from pairwise preferences, and be the specialization of the Luce-Shephard choice rule (Luce, 2005; 
Shepard, 1957) to preference over trajectory segments. It can be understood a equate reward 
with a preference rank scale analogous to the famous Elo rank system developed for chess (Elo, 
1978). Just a the difference in Elo point of two chess player estimate the probability of one player 
defeat the other in a game of chess, the difference in predict reward of two trajectory segment 
estimate the probability that one be chosen over the other by the human. 

Our actual algorithm incorporates a number of modification to this basic approach, which early 
experiment discover to be helpful and which be analyze in Section 3.3: 

• We fit an ensemble of predictors, each train on |D| triple sample from D with replace- 
ment. The estimate r̂ be define by independently normalize each of these predictor and 
then average the results. 

• A fraction of 1/e of the data be held out to be use a a validation set for each predictor. 
We use `2 regularization and adjust the regularization coefficient to keep the validation loss 
between 1.1 and 1.5 time the training loss. In some domain we also apply dropout for 
regularization. 

• Rather than apply a softmax directly a described in Equation 1, we assume there be a 
10% chance that the human responds uniformly at random. Conceptually this adjustment be 
need because human raters have a constant probability of make an error, which doesn’t 
decay to 0 a the difference in reward difference becomes extreme. 

3Equation 1 do not use discounting, which could be interpret a model the human to be indifferent 
about when thing happen in the trajectory segment. Using explicit discounting or infer the human’s discount 
function would also be reasonable choices. 

5 



2.2.4 Selecting Queries 

We decide how to query preference base on an approximation to the uncertainty in the reward 
function estimator, similar to Daniel et al. (2014): we sample a large number of pair of trajectory 
segment of length k, use each reward predictor in our ensemble to predict which segment will be 
prefer from each pair, and then select those trajectory for which the prediction have the high 
variance across ensemble members. This be a crude approximation and the ablation experiment in 
Section 3 show that in some task it actually impairs performance. Ideally, we would want to query 
base on the expect value of information of the query (Akrour et al., 2012; Krueger et al., 2016), 
but we leave it to future work to explore this direction further. 

3 Experimental Results 

We implement our algorithm in TensorFlow (Abadi et al., 2016). We interface with Mu- 
JoCo (Todorov et al., 2012) and the Arcade Learning Environment (Bellemare et al., 2013) through 
the OpenAI Gym (Brockman et al., 2016). 

3.1 Reinforcement Learning Tasks with Unobserved Rewards 

In our first set of experiments, we attempt to solve a range of benchmark task for deep RL without 
observe the true reward. Instead, the agent learns about the goal of the task only by ask a human 
which of two trajectory segment be better. Our goal be to solve the task in a reasonable amount of 
time use a few query a possible. 

In our experiments, feedback be provide by contractor who be give a 1-2 sentence description 
of each task before be ask to compare several hundred to several thousand pair of trajectory 
segment for that task (see Appendix B for the exact instruction give to contractors). Each trajectory 
segment be between 1 and 2 second long. Contractors respond to the average query in 3-5 seconds, 
and so the experiment involve real human feedback require between 30 minute and 5 hour of 
human time. 

For comparison, we also run experiment use a synthetic oracle whose preference over trajectory 
exactly reflect reward in the underlie task. That is, when the agent query for a comparison, instead 
of send the query to a human, we immediately reply by indicate a preference for whichever 
trajectory segment actually receives a high reward in the underlie task4. We also compare to 
the baseline of RL training use the real reward. Our aim here be not to outperform but rather to 
do nearly a well a RL without access to reward information and instead rely on much scarcer 
feedback. Nevertheless, note that feedback from real human do have the potential to outperform 
RL (and a show below it actually do so on some tasks), because the human feedback might 
provide a better-shaped reward. 

We describe the detail of our experiment in Appendix A, include model architectures, modifica- 
tions to the environment, and the RL algorithm use to optimize the policy. 

3.1.1 Simulated Robotics 

The first task we consider be eight simulated robotics tasks, implement in MuJoCo (Todorov 
et al., 2012), and include in OpenAI Gym (Brockman et al., 2016). We make small modification 
to these task in order to avoid encode information about the task in the environment itself (the 
modification be described in detail in Appendix A). The reward function in these task be linear 
function of distances, position and velocities, and all be a quadratic function of the features. We 
include a simple cartpole task (“pendulum”) for comparison, since this be representative of the 
complexity of task study in prior work. 

Figure 2 show the result of training our agent with 700 query to a human rater, compare to 
learn from 350, 700, or 1400 synthetic queries, a well a to RL learn from the real reward. 

4In the case of Atari game with sparse rewards, it be relatively common for two clip to both have zero 
reward in which case the oracle output indifference. Because we consider clip rather than individual states, 
such tie never make up a large majority of our data. Moreover, tie still provide significant information to the 
reward predictor a long a they be not too common. 

6 



Figure 2: Results on MuJoCo simulated robotics a measure on the tasks’ true reward. We compare 
our method use real human feedback (purple), our method use synthetic feedback provide by 
an oracle (shades of blue), and reinforcement learn use the true reward function (orange). All 
curve be the average of 5 runs, except for the real human feedback, which be a single run, and 
each point be the average reward over five consecutive batches. For Reacher and Cheetah feedback 
be provide by an author due to time constraints. For all other tasks, feedback be provide by 
contractor unfamiliar with the environment and with our algorithm. The irregular progress on 
Hopper be due to one contractor deviate from the typical label schedule. 

With 700 label we be able to nearly match reinforcement learn on all of these tasks. Training 
with learn reward function tends to be less stable and high variance, while have a comparable 
mean performance. 

Surprisingly, by 1400 label our algorithm performs slightly good than if it have simply be give 
the true reward, perhaps because the learn reward function be slightly good shaped—the reward 
learn procedure assigns positive reward to all behavior that be typically follow by high 
reward. 

Real human feedback be typically only slightly less effective than the synthetic feedback; depend 
on the task human feedback range from be half a efficient a ground truth feedback to be 
equally efficient. On the Ant task the human feedback significantly outperform the synthetic 
feedback, apparently because we ask human to prefer trajectory where the robot be “standing 
upright,” which prove to be useful reward shaping. (There be a similar bonus in the RL reward 
function to encourage the robot to remain upright, but the simple hand-crafted bonus be not a 
useful.) 

3.1.2 Atari 

The second set of task we consider be a set of seven Atari game in the Arcade Learning Environ- 
ment (Bellemare et al., 2013), the same game present in Mnih et al., 2013. 

Figure 3 show the result of training our agent with 5,500 query to a human rater, compare to 
learn from 350, 700, or 1400 synthetic queries, a well a to RL learn from the real reward. 
Our method have more difficulty match RL in these challenge environments, but nevertheless it 
display substantial learn on most of them and match or even exceeds RL on some. Specifically, 
on BeamRider and Pong, synthetic label match or come close to RL even with only 3,300 such 
labels. On Seaquest and Qbert synthetic feedback eventually performs near the level of RL but learns 
more slowly. On SpaceInvaders and Breakout synthetic feedback never match RL, but nevertheless 
the agent improves substantially, often passing the first level in SpaceInvaders and reach a score of 
20 on Breakout, or 50 with enough labels. 

7 



Figure 3: Results on Atari game a measure on the tasks’ true reward. We compare our method use 
real human feedback (purple), our method use synthetic feedback provide by an oracle (shades of 
blue), and reinforcement learn use the true reward function (orange). All curve be the average 
of 3 runs, except for the real human feedback which be a single run, and each point be the average 
reward over about 150,000 consecutive frames. 

Figure 4: Four frame from a single backflip. The agent be train to perform a sequence of backflips, 
land upright each time. The video be available at https://goo.gl/MhgvIU. 

On most of the game real human feedback performs similar to or slightly bad than synthetic 
feedback with the same number of labels, and often comparably to synthetic feedback that have 40% 
few labels. This may be due to human error in labeling, inconsistency between different contractor 
label the same run, or the uneven rate of label by contractors, which can cause label to be 
overly concentrate in narrow part of state space. The latter problem could potentially be address 
by future improvement to the pipeline for outsource labels. On Qbert, our method fails to learn 
to beat the first level with real human feedback; this may be because short clip in Qbert can be 
confuse and difficult to evaluate. Finally, Enduro be difficult for A3C to learn due to the difficulty 
of successfully passing other car through random exploration, and be correspondingly difficult to 
learn with synthetic labels, but human labelers tend to reward any progress towards passing cars, 
essentially shape the reward and thus outperform A3C in this game (the result be comparable 
to those achieve with DQN). 

3.2 Novel behavior 

Experiments with traditional RL task help u understand whether our method be effective, but the 
ultimate purpose of human interaction be to solve task for which no reward function be available. 

Using the same parameter a in the previous experiments, we show that our algorithm can learn 
novel complex behaviors. We demonstrate: 

1. The Hopper robot perform a sequence of backflips (see Figure 4). This behavior be 
train use 900 query in less than an hour. The agent learns to consistently perform a 
backflip, land upright, and repeat. 

8 

https://goo.gl/MhgvIU 


Figure 5: Performance of our algorithm on MuJoCo task after remove various components, a 
described in Section Section 3.3. All graph be average over 5 runs, use 700 synthetic label 
each. 

2. The Half-Cheetah robot move forward while stand on one leg. This behavior be 
train use 800 query in under an hour. 

3. Keeping alongside other car in Enduro. This be train with roughly 1,300 query 
and 4 million frame of interaction with the environment; the agent learns to stay almost 
exactly even with other move car for a substantial fraction of the episode, although it get 
confuse by change in background. 

Videos of these behavior can be found at https://goo.gl/MhgvIU. These behavior be train 
use feedback from the authors. 

3.3 Ablation Studies 

In order to good understand the performance of our algorithm, we consider a range of modifications: 

1. We pick query uniformly at random rather than prioritize query for which there be 
disagreement (random queries). 

2. We train only one predictor rather than an ensemble (no ensemble). In this setting, we also 
choose query at random, since there be no longer an ensemble that we could use to estimate 
disagreement. 

3. We train on query only gather at the begin of training, rather than gather through- 
out training (no online queries). 

4. We remove the `2 regularization and use only dropout (no regularization). 
5. On the robotics task only, we use trajectory segment of length 1 (no segments). 
6. Rather than fitting r̂ use comparisons, we consider an oracle which provide the true 

total reward over a trajectory segment, and fit r̂ to these total reward use mean square 
error (target). 

The result be present in Figure 5 for MuJoCo and Figure 6 for Atari. 

Of particular interest be the poor performance of offline reward predictor training; here we find 
that due to the nonstationarity of the occupancy distribution, the predictor capture only part of the 
true reward, and maximize this partial reward can lead to bizarre behavior that be undesirable a 
measure by the true reward (Amodei et al., 2016). For instance, on Pong offline training sometimes 
lead our agent to avoid lose point but not to score points; this can result in extremely long volley 

9 

https://goo.gl/MhgvIU 


Figure 6: Performance of our algorithm on Atari task after remove various components, a 
described in Section 3.3. All curve be an average of 3 run use 5,500 synthetic label (see minor 
exception in Section A.2). 

that repeat the same sequence of event ad infinitum (videos at https://goo.gl/L5eAbk). This 
type of behavior demonstrates that in general human feedback need to be intertwine with RL 
learn rather than provide statically. 

Our main motivation for elicit comparison rather than absolute score be that we found it much 
easy for human to provide consistent comparison than consistent absolute scores, especially on the 
continuous control task and on the qualitative task in Section 3.2; nevertheless it seem important 
to understand how use comparison affect performance. For continuous control task we found 
that predict comparison work much good than predict scores. This be likely because the 
scale of reward varies substantially and this complicates the regression problem, which be smooth 
significantly when we only need to predict comparisons. In the Atari task we clipped reward and 
effectively only predict the sign, avoid these difficulty (this be not a suitable solution for the 
continuous control task because the relative magnitude of the reward be important to learning). In 
these task comparison and target have significantly different performance, but neither consistently 
outperform the other. 

We also observe large performance difference when use single frame rather than clips5. In order 
to obtain the same result use single frame we would need to have collect significantly more 
comparisons. In general we discover that ask human to compare longer clip be significantly 
more helpful per clip, and significantly less helpful per frame. We found that for short clip it take 
human raters a while just to understand the situation, while for longer clip the evaluation time be 
a roughly linear function of the clip length. We try to choose the shortest clip length for which 
the evaluation time be linear. In the Atari environment we also found that it be often easy to 
compare longer clip because they provide more context than single frames. 

4 Discussion and Conclusions 

Agent-environment interaction be often radically cheaper than human interaction. We show that by 
learn a separate reward model use supervise learning, it be possible to reduce the interaction 
complexity by roughly 3 order of magnitude. Not only do this show that we can meaningfully 
train deep RL agent from human preferences, but also that we be already hit diminish return 

5We only ran these test on continuous control task because our Atari reward model depends on a sequence 
of consecutive frame rather than a single frame, a described in Section A.2 

10 

https://goo.gl/L5eAbk 


on further sample-complexity improvement because the cost of compute be already comparable to 
the cost of non-expert feedback.6 

Although there be a large literature on preference elicitation and reinforcement learn from unknown 
reward functions, we provide the first evidence that these technique can be economically scale up to 
state-of-the-art reinforcement learn systems. This represent a step towards practical application 
of deep RL to complex real-world tasks. 

Future work may be able to improve the efficiency of learn from human preferences, and expand 
the range of task to which it can be applied. 

In the long run it would be desirable to make learn a task from human preference no more difficult 
than learn it from a programmatic reward signal, ensure that powerful RL system can be apply 
in the service of complex human value rather than low-complexity goals. 

Acknowledgments 

We thank Olivier Pietquin, Bilal Piot, Laurent Orseau, Pedro Ortega, Victoria Krakovna, Owain 
Evans, Andrej Karpathy, Igor Mordatch, and Jack Clark for reading draft of the paper. We thank 
Tyler Adkisson, Mandy Beri, Jessica Richards, Heather Tran, and other contractor for provide the 
data use to train our agents. Finally, we thank OpenAI and DeepMind for provide a supportive 
research environment and for support and encourage this collaboration. 

References 
Martin Abadi et al. Tensorflow: Large-scale machine learn on heterogeneous distribute systems. 

arXiv preprint arXiv:1603.04467, 2016. 

Riad Akrour, Marc Schoenauer, and Michele Sebag. Preference-based policy learning. Machine 
learn and knowledge discovery in databases, page 12–27, 2011. 

Riad Akrour, Marc Schoenauer, and Michèle Sebag. April: Active preference learning-based 
reinforcement learning. In Joint European Conference on Machine Learning and Knowledge 
Discovery in Databases, page 116–131, 2012. 

Riad Akrour, Marc Schoenauer, Michèle Sebag, and Jean-Christophe Souplet. Programming by 
feedback. In International Conference on Machine Learning, page 1503–1511, 2014. 

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 
Concrete problem in AI safety. arXiv preprint arXiv:1606.06565, 2016. 

Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning 
Environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 
47:253–279, 2013. 

Nick Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, 2014. 

Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. The method 
of pair comparisons. Biometrika, 39(3/4):324–345, 1952. 

Eric Brochu, Tyson Brochu, and Nando de Freitas. A bayesian interactive optimization approach 
to procedural animation design. In Proceedings of the 2010 ACM SIGGRAPH/Eurographics 
Symposium on Computer Animation, page 103–112. Eurographics Association, 2010. 

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and 
Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016. 

Christian Daniel, Malte Viering, Jan Metz, Oliver Kroemer, and Jan Peters. Active reward learning. 
In Robotics: Science and Systems, 2014. 

6For the Atari experiment we be use a virtual machine with 16 CPUs and one Nvidia K80 GPU which 
cost ~$700/month on GCE. Training take about a day, so the compute cost be ~$25. Training with 5k label 
corresponds roughly to 5 hour of human labour, at US minimum wage this total ~$36. 

11 



Christian Daniel, Oliver Kroemer, Malte Viering, Jan Metz, and Jan Peters. Active reward learn 
with a novel acquisition function. Autonomous Robots, 39(3):389–405, 2015. 

Layla El Asri, Bilal Piot, Matthieu Geist, Romain Laroche, and Olivier Pietquin. Score-based 
inverse reinforcement learning. In International Conference on Autonomous Agents and Multiagent 
Systems, page 457–465, 2016. 

Arpad Elo. The Rating of Chessplayers, Past and Present. Arco Pub., 1978. 

Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control 
via policy optimization. In International Conference on Machine Learning, volume 48, 2016. 

Chelsea Finn, Tianhe Yu, Justin Fu, Pieter Abbeel, and Sergey Levine. Generalizing skill with 
semi-supervised reinforcement learning. In International Conference on Learning Representations, 
2017. 

Johannes Fürnkranz, Eyke Hüllermeier, Weiwei Cheng, and Sang-Hyeun Park. Preference-based 
reinforcement learning: A formal framework and a policy iteration algorithm. Machine learning, 
89(1-2):123–156, 2012. 

Dylan Hadfield-Menell, Stuart Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse 
reinforcement learning. In Advances in Neural Information Processing Systems, page 3909–3917, 
2016. 

Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Andrew 
Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou, Joel Z Leibo, and Audrunas 
Gruslys. Learning from demonstration for real world reinforcement learning. arXiv preprint 
arXiv:1704.03732, 2017. 

Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural 
Information Processing Systems, page 4565–4573, 2016. 

W Bradley Knox and Peter Stone. Interactively shape agent via human reinforcement: The 
TAMER framework. In International Conference on Knowledge Capture, page 9–16, 2009. 

W. Bradley Knox and Peter Stone. Learning non-myopically from human-generated reward. In Jihie 
Kim, Jeffrey Nichols, and Pedro A. Szekely, editors, IUI, page 191–202. ACM, 2013. ISBN 
978-1-4503-1965-2. URL http://doi.acm.org/10.1145/2449396. 

William Bradley Knox. Learning from human-generated reward. PhD thesis, University of Texas at 
Austin, 2012. 

David Krueger, Jan Leike, Owain Evans, and John Salvatier. Active reinforcement learning: Observ- 
ing reward at a cost. In Future of Interactive Learning Machines, NIPS Workshop, 2016. 

R Duncan Luce. Individual choice behavior: A theoretical analysis. Courier Corporation, 2005. 

James MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, David Roberts, Matthew E Taylor, and 
Michael L Littman. Interactive learn from policy-dependent human feedback. arXiv preprint 
arXiv:1701.06049, 2017. 

AT Machwe and IC Parmee. Introducing machine learn within an interactive evolutionary design 
environment. In DS 36: Proceedings DESIGN 2006, the 9th International Design Conference, 
Dubrovnik, Croatia, 2006. 

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan 
Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint 
arXiv:1312.5602, 2013. 

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, 
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles 
Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane 
Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 
518(7540):529–533, 2015. 

12 

http://doi.acm.org/10.1145/2449396 


Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim 
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous method for deep reinforcement 
learning. In International Conference on Machine Learning, page 1928–1937, 2016. 

Andrew Y Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In International 
Conference on Machine learning, page 663–670, 2000. 

Patrick M Pilarski, Michael R Dawson, Thomas Degris, Farbod Fahimi, Jason P Carey, and Richard 
Sutton. Online human training of a myoelectric prosthesis controller via actor-critic reinforcement 
learning. In International Conference on Rehabilitation Robotics, page 1–7, 2011. 

Stuart Russell. Should we fear supersmart robots? Scientific American, 314(6):58, 2016. 

John Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region 
policy optimization. In International Conference on Machine Learning, page 1889–1897, 2015. 

Jimmy Secretan, Nicholas Beato, David B D Ambrosio, Adelein Rodriguez, Adam Campbell, and 
Kenneth O Stanley. Picbreeder: Evolving picture collaboratively online. In Conference on Human 
Factors in Computing Systems, page 1759–1768, 2008. 

Roger N Shepard. Stimulus and response generalization: A stochastic model relate generalization 
to distance in psychological space. Psychometrika, 22(4):325–345, 1957. 

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, 
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, 
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine 
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with 
deep neural network and tree search. Nature, 529(7587):484–489, 2016. 

Patrikk D Sørensen, Jeppeh M Olsen, and Sebastian Risi. Breeding a diversity of super mario 
behavior through interactive evolution. In Computational Intelligence and Games (CIG), 2016 
IEEE Conference on, page 1–7. IEEE, 2016. 

Bradly C Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning. In International 
Conference on Learning Representations, 2017. 

Hiroaki Sugiyama, Toyomi Meguro, and Yasuhiro Minami. Preference-learning base inverse 
reinforcement learn for dialog control. In INTERSPEECH, page 222–225, 2012. 

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physic engine for model-based control. 
In International Conference on Intelligent Robots and Systems, page 5026–5033, 2012. 

Sida I Wang, Percy Liang, and Christopher D Manning. Learning language game through interaction. 
arXiv preprint arXiv:1606.02447, 2016. 

Aaron Wilson, Alan Fern, and Prasad Tadepalli. A Bayesian approach for policy learn from 
trajectory preference queries. In Advances in Neural Information Processing Systems, page 
1133–1141, 2012. 

Christian Wirth and Johannes Fürnkranz. Preference-based reinforcement learning: A preliminary 
survey. In ECML/PKDD Workshop on Reinforcement Learning from Generalized Feedback: 
Beyond Numeric Rewards, 2013. 

Christian Wirth, J Fürnkranz, Gerhard Neumann, et al. Model-free preference-based reinforcement 
learning. In AAAI, page 2222–2228, 2016. 

13 



A Experimental Details 

Many RL environment have termination condition that depend on the behavior of the agent, such 
a end an episode when the agent dy or fall over. We found that such termination condition 
encode information about the task even when the reward function be not observable. To avoid this 
subtle source of supervision, which could potentially confound our attempt to learn from human 
preference only, we remove all variable-length episodes: 

• In the Gym version of our robotics tasks, the episode end when certain parameter go 
outside of a prescribed range (for example when the robot fall over). We replace these 
termination condition by a penalty which encourages the parameter to remain in the range 
(and which the agent must learn). 

• In Atari games, we do not send life loss or episode end signal to the agent (we do continue 
to actually reset the environment), effectively convert the environment into a single 
continuous episode. When provide synthetic oracle feedback we replace episode end 
with a penalty in all game except Pong; the agent must learn this penalty. 

Removing variable length episode leaf the agent with only the information encode in the 
environment itself; human feedback provide it only guidance about what it ought to do. 

At the begin of training we compare a number of trajectory segment drawn from rollouts of an 
untrained (randomly initialized) policy. In the Atari domain we also pretrain the reward predictor 
for 200 epoch before begin RL training, to reduce the likelihood of irreversibly learn a bad 
policy base on an untrained predictor. For the rest of training, label be fed in at a rate decay 
inversely with the number of timesteps; after twice a many timesteps have elapsed, we answer about 
half a many query per unit time. The detail of this schedule be described in each section. This 
“label annealing” allows u to balance the importance of have a good predictor from the start with 
the need to adapt the predictor a the RL agent learns and encounter new states. When training 
with real human feedback, we attempt to similarly anneal the label rate, although in practice this be 
approximate because contractor give feedback at uneven rates. 

Except where otherwise state we use an ensemble of 3 predictors, and draw a factor 10 more clip 
pair candidate than we ultimately present to the human, with the present clip be select via 
maximum variance between the different predictor a described in Section 2.2.4. 

A.1 Simulated Robotics Tasks 

The OpenAI Gym continuous control task penalize large torques. Because torque be not di- 
rectly visible to a human supervisor, these reward function be not good representative of human 
preference over trajectory and so we remove them. 

For the simulated robotics tasks, we optimize policy use trust region policy optimization (TRPO, 
Schulman et al., 2015) with discount rate γ = 0.995 and λ = 0.97. The reward predictor be a two- 
layer neural network with 64 hidden unit each, use leaky ReLUs (α = 0.01) a nonlinearities.7 We 
compare trajectory segment that last 1.5 seconds, which varies from 15 to 60 timesteps depend 
on the task. 

We normalize the reward prediction to have standard deviation 1. When learn from the reward 
predictor, we add an entropy bonus of 0.01 on all task except swimmer, where we use an entropy 
bonus of 0.001. As note in Section 2.2.1, this entropy bonus help to incentivize the increase 
exploration need to deal with a change reward function. 

We collect 25% of our comparison from a randomly initialize policy network at the begin of 
training, and our rate of label after T frame 2 ∗ 106/(T + 2 ∗ 106). 

7All of these reward function be second degree polynomial of the input features, and so if we be 
concerned only with these task we could take a simpler approach to learn the reward function. However, 
use this more flexible architecture allows u to immediately generalize to task for which the reward function 
be not so simple, a described in Section 3.2. 

14 



A.2 Atari 

Our Atari agent be train use the standard set of environment wrapper use by Mnih et al. 
(2015): 0 to 30 no-ops in the begin of an episode, max-pooling over adjacent frames, stack 
of 4 frames, a frameskip of 4, life loss end an episode (but not reset the environment), and 
reward clipped to [−1, 1]. 
Atari game include a visual display of the score, which in theory could be use to trivially infer 
the reward. Since we want to focus instead on infer the reward from the complex dynamic 
happen in the game, we replace the score area with a constant black background on all seven 
games. On BeamRider we additionally blank out the enemy ship count, and on Enduro we blank out 
the speedometer. 

For the Atari task we optimize policy use the A3C algorithm (Mnih et al., 2016) in synchronous 
form (A2C), with policy architecture a described in Mnih et al. (2015). We use standard setting for 
the hyperparameters: an entropy bonus of β = 0.01, learn rate of 0.0007 decayed linearly to reach 
zero after 80 million timesteps (although run be actually train for only 50 million timesteps), 
n = 5 step per update, N = 16 parallel workers, discount rate γ = 0.99, and policy gradient use 
Adam with α = 0.99 and � = 10−5. 

For the reward predictor, we use 84x84 image a input (the same a the input to the policy), and 
stack 4 frame for a total 84x84x4 input tensor. This input be fed through 4 convolutional layer 
of size 7x7, 5x5, 3x3, and 3x3 with stride 3, 2, 1, 1, each have 16 filters, with leaky ReLU 
nonlinearities (α = 0.01). This be follow by a fully connect layer of size 64 and then a scalar 
output. All convolutional layer use batch norm and dropout with α = 0.5 to prevent predictor 
overfitting. In addition we use `2 regularization with the adapative scheme described in Section 2.2.3. 
Since the reward predictor be ultimately use to compare two sum over timesteps, it scale be arbitrary, 
and we normalize it to have a standard deviation of 0.05 (we could equivalently have adjust our 
learn rate and entropy bonus, but this choice allow u to use the same parameter a for the real 
reward function). 

We compare trajectory segment of 25 timesteps (1.7 second at 15 fps with frame skipping). 

We collect 500 comparison from a randomly initialize policy network at the begin of training, 
and our rate of label after T frame of training be decrease every 5 ∗ 106 frames, to be roughly 
proportional to 5 ∗ 106/(T + 5 ∗ 106). 
The predictor be train asynchronously from the RL agent, and on our hardware typically process 
1 label per 10 RL timesteps. We maintain a buffer of only the last 3,000 label and loop over this 
buffer continuously; this be to ensure that the predictor give enough weight to new label (which can 
represent a shift in distribution) when the total number of label becomes large. 

In the ablation study of Figure 5b, pretraining have 5,000 label rather than 5,500, and the “target” 
beamrider curve be average over 2 run rather than 3. 

B Instructions Provided to Contractors 

B.1 MuJoCo 

Giving feedback 

Sign up for a slot in the spreadsheet. Then go to the appropriate URL’s that we give you, and you’ll 
be repeatedly present with two video clip of an AI control a virtual robot. 

Look at the clip and select the one in which good thing happen. Only decide on event you 
actually witness in the clip. 

Here’s a guide on what constitutes good and bad behavior in each specific domain: 

• Hopper: the “center” of the robot be the joint closest to the pointy end. The first priority be 
for the center of the robot to move to the right (moving to the left be bad than not move 
at all). If the two robot be roughly tie on this metric, then the tiebreaker be how high the 
center is. 

15 



• Walker: the “center” of the robot be the joint where the three limb meet. The first priority 
be for the center of the robot to move to the right. If the two robot be roughly tie on this 
metric, then the tiebreaker be how high the center is. 

• Swimmer: the “center” of the robot be the mark in the middle of it body. The center should 
move to the right a fast a possible. 

• Cheetah: the robot should move to the right a fast a possible. 
• Ant: the first priority be for the robot to be stand upright, and fail that for the center of 

the robot to be a high up a possible. If both robot be upright or neither is, the tie breaker 
be whichever one be move faster to the right. 

• Reacher: the green dot on the robot arm should be a close a possible to the red dot. Being 
near for a while and far for a while be bad than be at an intermediate distance for the 
entire clip. 

• Pendulum: the pendulum should be point approximately up. There will be a lot of tie 
where the pendulum have fall and a lot of “can’t tells” where it be off the side of the screen. 
If you can see one pendulum and it hasn’t fall down, that’s good than be unable to see 
the other pendulum. 

• Double-pendulum: both pendulum should be point approximately up (if they fall down, 
the cart should try to swing them back up) and the cart should be near the center of the track. 
Being high for a while and low for a while be bad than be at an intermediate distance 
the entire time. 

If both clip look about the same to you, then click “tie”. If you don’t understand what’s go on in 
the clip or find it hard to evaluate, then click “can’t tell”. 

You can speed up your feedback by use the arrow key 
left and right select clips, up be a tie, down be “can’t tell”. 

FAQ 

I get an error say that we’re out of clips. What’s up? Occasionally the server may run out of 
clip to give you, and you’ll see an error message. This be normal, just wait a minute and refresh the 
page. If you don’t get clip for more than a couple minutes, please ping @tom on slack. 

Do I need to start right at the time list in the spreadsheet? Starting 10 minute before or after 
the list time be fine. 

B.2 Atari 

In this task you’ll be try to teach an AI to play Atari game by give it feedback 
on how well it be playing. 

IMPORTANT. First play the game yourself for 5 minute 

Before provide feedback to the AI, play the game yourself for a five minute to get a sense of how 
it works. It’s often hard to tell what the game be about just by look at short clips, especially if 
you’ve never played it before. 

Play the game online for 5 minutes.8 You’ll need to press F12 or click the GAME RESET button to 
start the game. Then set a timer for 5 minute and explore the game to see how it works. 

Giving feedback 

Sign up for a slot in the spreadsheet. Then go to the appropriate URL’s that we give you, and you’ll 
be repeatedly present with two video clip of an AI play the game. 

Look at the clip and select the one in which good thing happen. For example, if the left clip 
show the AI shoot an enemy ship while the right clip show it be shot by an enemy ship, then 
good thing happen in the left clip and thus the left clip be better. Only decide on action you actually 
witness in the clip. 

8e.g. http://www.free80sarcade.com/2600_Beamrider.php 

16 

http://www.free80sarcade.com/2600_Beamrider.php 


Here’s a guide on what constitutes good and bad play in each specific game: 

• BeamRider: shoot enemy ship (good), and don’t get shot (very bad) 
• Breakout: hit the ball with the paddle, break the color blocks, and don’t let the ball fall 

off the bottom of the screen 
• Enduro: pas a many car a you can, and don’t get pass by car 
• Pong: knock the ball past the opponent’s orange paddle on the left (good), and don’t let it 

go past your green paddle on the right (bad) 
• Qbert: change the color of a many block a you can (good), but don’t jump off the side or 

run into enemy (very bad) 
• SpaceInvaders: shoot enemy ship (good), and don’t let your ship (the one at the bottom of 

the screen) get shot (very bad) 
• SeaQuest: Shoot the fish and enemy submarine (good) and pick up the scuba divers. Don’t 

let your submarine run out of air or get hit by a fish or torpedo (very bad) 
• Enduro (even mode): Avoid passing car OR get pass by them, you want to stay 

even with other car (not have any around be OK too) 

Don’t worry about how the agent get into the situation it be in (for instance, it doesn’t matter if 
one agent have more lives, or be now on a more advanced level); just focus on what happens in the clip 
itself. 

If both clip look about the same to you, then click “tie”. If you don’t understand what’s go on 
in the clip or find it hard to evaluate, then click “can’t tell”. Try to minimize respond “can’t tell” 
unless you truly be confused. 

You can speed up your feedback by use the arrow key 
left and right select clips, up be a tie, down be “can’t tell”. 

FAQ 

I get an error say that we’re out of clips. What’s up? Occasionally the server may run out of 
clip to give you, and you’ll see an error message. This be normal, just wait a minute and refresh the 
page. If you don’t get clip for more than a couple minutes, please ping @tom on slack. 

If the agent be already dead when the clip starts, how should I compare it? If the clip be after 
get kill (but not show the dying), then it performance during the clip be neither good nor 
bad. You can treat it a purely average play. If you see it die, or it’s possible that it contains a frame 
of it dying, then it’s definitely bad. 

Do I need to start right at the time list in the spreadsheet? Starting 30 minute before or after 
the list time be fine. 

17 


1 Introduction 
1.1 Related Work 

2 Preliminaries and Method 
2.1 Setting and Goal 
2.2 Our Method 
2.2.1 Optimizing the Policy 
2.2.2 Preference Elicitation 
2.2.3 Fitting the Reward Function 
2.2.4 Selecting Queries 


3 Experimental Results 
3.1 Reinforcement Learning Tasks with Unobserved Rewards 
3.1.1 Simulated Robotics 
3.1.2 Atari 

3.2 Novel behavior 
3.3 Ablation Studies 

4 Discussion and Conclusions 
A Experimental Details 
A.1 Simulated Robotics Tasks 
A.2 Atari 

B Instructions Provided to Contractors 
B.1 MuJoCo 
B.2 Atari 


