




































amdahl.dvi 


AFIPS spring joint computer conference����� 

Validity of the single processor approach to achieve large scale 

compute capabilities� 

Gene M� Amdahl 

IBM Sunnyvale� California 

� INTRODUCTION 

For over a decade prophet have voiced the contention that the organization of a single computer 

have reach it limit and that truly signi�cant advences can be make only by interconnection of a 

multiplicity of computer in such a manner a to permit cooperative solution� Variously the proper 

direction have be point out a general purpose computer with a generalize interconnection 

of memories� or a specialized computer with geometrically related memory interconnection and 

control by one or more instruction streams� 

Demonstration be make of the continued validity of the single processor approach and of the 

weakness of the multiple processor approach in term of application to real problem and their 

attendant irregularities� 

The argument present be base on statistical characteristic of computation on computer 

over the last decade and upon the operational requirement within problem of physical interest� 

An additional reference will be one of the most thorough analysis of relative computer capability 

currently publish �Changes in Computer Performance�� Datamation� September ����� Professor 

Kenneth F� Knight� Stanford School of Business Asministration� 

The �rst characteristic of interest be the fraction of the computational load which be associate 

with data management housekeeping� This fraction have be very nearly constant for about ten 

years� and account for � 
of the execute instruction in production runs� In an entirely dedicate 

special purpose environment this might be reduce by a factor of two� but it be highly improbably 

that it could be reduce by a factor of three� The nature of this overhead appear to be sequential 

so that it be unlikely to be amenable to parallel processing techniques� Overhead alone would then 

place an upper limit on throughput of �ve to seven time the sequential processing rate� even if the 

�This paper be retyped a the present form by Guihai Chen� He wish you would enjoy reading this historical 

paper� 

� 



housekeep be do in a separate processor� The non housekeep part of the problem could 

exploit at most a processor of performance three to four time the performance of the housekeep 

processor� A fairly obvious conclusion which can be drawn at this point be that the e�ort expend 

on achieve high parallel processing rate be waste unless it be accompany by achievement in 

sequential processing rate of very nearly the same magnitude� 

Data management housekeep be not the only problem to plague oversimpli�ed approach to 

high speed computation� The physical problem which be of practical interest tend to have rather 

signi�cant complications� Examples of these complication be a follows� boundary be likely to 

be irregular 
interior be inhomogeneous 
computation require may be dependent on the state 

of the variable at each point 
propagation rate of di�erent physical e�ects may be quite di�erent 

the rate of convergence� or convergence at all may be strongly dependent on sweep through the 

array along di�erent ax on succeed passes� etc� The e�ect of each of these complication be 

very severe on any computer organization base on geometrically related processor in a parallel 

processing system� Even the existence of regular rectangular boundary have the interest property 

that for spatial dimension of N there be �N di�erent point geometry to be dealt with in a near 

neighbor computation� If the second near neighbor be also involved� there would be �N 

di�erent point geometry to contend with� An irregular boundary compound this problem a do 

an inhomogeneous interiors� Computations which be dependent on the state of variable would 

require the processing at each point to consume approximately the same computational time a 

the sum of computation of all physical e�ects within a large region� Di�erences of change in 

propagation rate may a�ect the mesh point relationships� 

Ideally the computation of the action of the neighbor point upon the point under consid� 

eration involves their value at a previous time proportional to the mesh space and inversely 

proportional to the propagation rate� Since the time step be normally kept constant� a faster prop� 

agation rate for some e�ects would imply interaction with more distant points� Finally the fairly 

common practice of sweep through the mesh along di�erent ax on succeed pass pose 

problem of data management which a�ects all processors� however it a�ects geometrically related 

processor more severely by require transpose all point in storage in addition to the revise 

input�output scheduling� A realistic assessment of the e�ect of these irregularity on a simpli�ed 

and regularize abstraction of the problem yield a degradation in the vicinity of one�half to one 

order of magnitude� 

To sum up the e�ects of data management housekeep and of problem irregularities� the 

author have compare three di�erent machine organization involve approximately equal amount 

� 



of hardware� Machine A have thirty two arithmetic execution unit control by a single instruction 

stream� Machine B have pipelined arithmetic execution unit with up to three overlap operation 

on vector of eight elements� Machine C have the same pipelined execution units� but initiation of 

individual operation at the same rate a Machine B permit vector element operations� The 

performance of these three machine be plot in Figure � a a function of the fraction of the 

number of instruction which permit parallelism� The probable region of operation be center 

around a point correspond to �� 
data management overhead and � 
of the problem operation 

force to be sequential� 

Figure 1 

The historic performance versus cost of computer have be explore very thoroughly by Profes� 

sor Knight� The carefully analyze data he present re�ects not just execution time for arithmetic 

operation and cost of minimum of recommend con�gurations� He include memory capacity ef� 

fects� input�output overlap experienced� and special functional capabilities� The best statistical �t 

obtain corresponds to a performance proportional to the square of cost at any technological level� 

This result very e�ectively support the often invoked �Grosch�s Law�� Utilizing this analysis� one 

can argue that if twice the amount of hardware be exploit in a single system� one could expect 

to obtain four time the performance� The only di�culty be involve in know how to exploit this 

additional hardware� At any point in time it be di�cult to foresee how the precious bottleneck in 

a sequential computer will be e�ectively overcome� If it be easy they would not have be left a 

bottlenecks� It be true by historical example that the successive obstacle have be hurdled� so it be 

appropriate to quote the Rev� Adam Clayton Powell��Keep the faith� baby�� If alternatively one 

decide to improve the performance by put two processor side by side with share memory� 

one would �nd approximately ��� time a much hardware� The additional two tenth in hardware 

accomplish the crossbar switch for the sharing� The result performance achieve would 

be about ���� The latter �gure be derive from the assumption of each processor utilize half of 

� 



the memory about half of the time� The result memory con�icts in the share system would 

extend the execution of one of two operation by one quarter of the execution time� The net result 

be a price performance degradation to �� rather than an improvement to �� for the single large 

processor� 

Comparative analysis with associative processor be far less easy and obvious� Under certain 

condition of regular format there be a fairly direct approach� Consider an associative processor de� 

sign for pattern recognition in which decision within individual element be forward to some 

set of other elements� In the associative processor design the receive element would have a set of 

source address which recognize by associative technique whether or not it be to receive the de� 

cision of the currently declare element� To make a correspond special purpose non�associative 

processor one would consider a receive element and it source address a an instruction� with 

binary decision maintain in registers� Considering the use of the �lm memory� an associative 

cycle would be longer than a non�destructive read cycle� In such a real analogy the special purpose 

non�associative processor can be expect to take about one�fourth a many memory cycle a the 

associative version and only about one sixth of the time� These �gures be compute on the full 

recognition task with somewhat di�ering ratio in each phase� No blanket claim be intend here� 

but rather that each requirement should be investigate from both approaches� 

� Notes by Guihai Chen 

� The very famous Amdahl�s Law� present a in the follow formula� be deprive from this 

paper� However� Amdahl give only a literal description which be paraphrase by latecomer 

a follows� 

Speedup � 
� 

r � 
rp 
n 

where r � rp � � and r represent the ratio of the sequential portion in one program� 

� Only a small part of this paper� exactly the fourth paragraph� contributes to the Amdahl�s 

Law� This paper also discuss some other important problems� For example� Amdahl have 

forseen many negative factor plague the parallel computation of irregular problems� such 

a �� boundary be likely to be irregular 
�� interior be inhomogeneous 
�� computation 

require may be dependent on the state of the variable at each point 
�� propagation rate 

of di�erent physical e�ects may be quite di�erent 
�� the rate of convergence� or convergence 

at all may be strongly dependent on sweep through the array along di�erent ax on 

succeed passes� etc� 

� 


