




















































Look, Listen and Learn 


Look, Listen and Learn 

Relja Arandjelović† 

relja@google.com 

Andrew Zisserman†,∗ 

zisserman@google.com 

†DeepMind ∗VGG, Department of Engineering Science, University of Oxford 

Abstract 

We consider the question: what can be learnt by look- 
ing at and listen to a large number of unlabelled videos? 
There be a valuable, but so far untapped, source of infor- 
mation contain in the video itself – the correspondence 
between the visual and the audio streams, and we intro- 
duce a novel “Audio-Visual Correspondence” learn task 
that make use of this. Training visual and audio network 
from scratch, without any additional supervision other than 
the raw unconstrained video themselves, be show to suc- 
cessfully solve this task, and, more interestingly, result in 
good visual and audio representations. These feature set 
the new state-of-the-art on two sound classification bench- 
marks, and perform on par with the state-of-the-art self- 
supervise approach on ImageNet classification. We also 
demonstrate that the network be able to localize object in 
both modalities, a well a perform fine-grained recognition 
tasks. 

1. Introduction 

Visual and audio event tend to occur together; not al- 
way but often: the movement of finger and sound of the 
instrument when a piano, guitar or drum be played; lip 
move and speech when talking; car move and engine 
noise when observe a street. The visual and audio event 
be concurrent in these case because there be a common 
cause. In this paper we investigate whether we can use this 
simple observation to learn about the world both visually 
and aurally by simply watch and listen to videos. 

We ask the question: what can be learnt by training vi- 
sual and audio network simultaneously to predict whether 
visual information (a video frame) corresponds or not to au- 
dio information (a sound snippet)? This be a looser require- 
ment than that the visual and audio event occur in sync. It 
only require that there be something in the image that cor- 
relates with something in the audio clip – a car present in 
the video frame, for instance, correlate with engine noise; 
or an exterior shot with the sound of wind. 

Our motivation for this work be three fold: first, a in 

many recent self-supervision task [1, 6, 8, 20, 21, 24, 35, 
36], it be interest to learn from a virtually infinite source 
of free supervision (video with visual and audio mode in 
this case) rather than require strong supervision; second, 
this be a possible source of supervision that an infant could 
use a their visual and audio capability develop; third, we 
want to know what can be learnt, and how well the network 
be trained, for example in the performance of the visual and 
audio network for other tasks. 

Of course, we be not the first to make the observa- 
tion that visual and audio event co-occur, and to use 
their concurrence or correlation a supervision for train- 
ing a network. In a series of recent and inspire pa- 
pers [2, 11, 22, 23], the group at MIT have investigate pre- 
cisely this. However, their goal be always to train a single 
network for one of the modes, for example, train a visual 
network to generate sound in [22, 23]; or train an audio 
network to correlate with visual output in [2, 11], where 
the visual network be pre-trained and fix and act a a 
teacher. In earlier, pre deep-learning, approach the obser- 
vation be use to beautiful effect in [14] show “pixels 
that sound” (e.g. for a guitar) learnt use CCA. In contrast, 
we train both visual and audio network and, somewhat sur- 
prisingly, show that this be beneficial – in that our perfor- 
mance improves substantially over that of [2] when train 
on the same data. 

In summary: our goal be to design a system that be able to 
learn both visual and audio semantic information in a com- 
pletely unsupervised manner by simply look at and lis- 
tening to a large number of unlabelled videos. To achieve 
this we introduce a novel Audio-Visual Correspondence 
(AVC) learn task that be use to train the two (visual and 
audio) network from scratch. This task be described in sec- 
tion 2, together with the network architecture and training 
procedure. In section 3 we describe what semantic informa- 
tion have be learnt, and ass the performance of the audio 
and visual networks. We find, which we have not anticipated, 
that this task lead to quite fine grain visual and audio 
discrimination, e.g. into different instruments. In term of 
quantitative performance, the audio network exceed those 
recently train for audio recognition use visual super- 

1 

ar 
X 

iv 
:1 

70 
5. 

08 
16 

8v 
2 

[ 
c 

.C 
V 

] 
1 

A 
ug 

2 
01 

7 

lphilippe 
Zone de texte 
Knowledge from multi input channels, purely orthogonal 

=> Richer because orthogonal per definition 

Is it just that??? 

To be clarified! 



Vision subnetwork 

Audio subnetwork 

Fusion layer Correspond? 

Audio-visual correspondence detector network 

Yes / No 

Figure 1. Audio-visual correspondence task (AVC). A network 
should learn to determine whether a pair of (video frame, short 
audio clip) correspond to each other or not. Positives be (frame, 
audio) extract from the same time of one video, while negative 
be a frame and audio extract from different videos. 

vision, and the visual network have similar performance to 
those train for other, purely visual, self-supervision tasks. 
Furthermore, we show, a an add benefit, that we be able 
to localize the source of the audio event in the video frame 
(and also localize the correspond region of the sound 
source) use activation visualization. 

In term of prior work, the most closely related deep 
learn approach that we know of be ‘SyncNet’ in [5]. 
However, [5] be aim at learn to synchronize lip-regions 
and speech for lip-reading, rather than the more general 
video and audio material consider here for learn se- 
mantic representations. More generally, the AVC task be a 
form of co-training [4], where there be two ‘views’ of the 
data, and each view provide complementary information. 
In our case the two view be visual and audio (and each can 
determine semantic information independently). A similar 
scenario arises when the two view be visual and language 
(text) a in [9, 18, 31] where a common embed be learnt. 
However, usually one (or both) of the network (for image 
and text) be pre-trained, in contrast to the approach take 
here where no supervision be require and both network be 
train from scratch. 

2. Audio-visual correspondence learn 
The core idea be to use a valuable but so far untapped 

source of information contain in the video itself – the cor- 
respondence between visual and audio stream available by 
virtue of them appear together at the same time in the 
same video. By see and hearing many example of a 
person play a violin and example of a dog barking, and 
never, or at least very infrequently, see a violin be 
played while hearing a dog bark and vice versa, it should be 
possible to conclude what a violin and a dog look and sound 
like, without ever be explicitly taught what be a violin or 
a dog. 

We leverage this for learn by an audio-visual corre- 
spondence (AVC) task, illustrate in Figure 1. The AVC 
task be a simple binary classification task: give an example 
video frame and a short audio clip – decide whether they 

correspond to each other or not. The correspond (posi- 
tive) pair be the one that be take at the same time from 
the same video, while mismatch (negative) pair be ex- 
tracted from different videos. The only way for a system to 
solve this task be if it learns to detect various semantic con- 
cepts in both the visual and the audio domain. Indeed, we 
demonstrate in Section 3.5 that our network automatically 
learns relevant semantic concept in both modalities. 

It should be note that the task be very difficult. The 
network be make to learn visual and audio feature and 
concept from scratch without ever see a single label. 
Furthermore, the AVC task itself be quite hard when do 
on completely unconstrained video – video can be very 
noisy, the audio source be not necessarily visible in the video 
(e.g. camera operator speaking, person narrate the video, 
sound source out of view or occluded, etc.), and the audio 
and visual content can be completely unrelated (e.g. edit 
video with add music, very low volume sound, ambient 
sound such a wind dominate the audio track despite other 
audio event be present, etc.). Nevertheless, the result 
in Section 3 show that our network be able to fairly success- 
fully solve the AVC task, and in the process learn very good 
visual and audio representations. 

2.1. Network architecture 

To tackle the AVC task, we propose the network struc- 
ture show in Figure 2. It have three distinct parts: the vision 
and the audio subnetworks which extract visual and audio 
features, respectively, and the fusion network which take 
these feature into account to produce the final decision on 
whether the visual and audio signal correspond. Here we 
describe the three part in more detail. 
Vision subnetwork. The input to the vision subnetwork be 
a 224 × 224 colour image. We follow the VGG-network 
[30] design style, with 3× 3 convolutional filters, and 2× 2 
max-pooling layer with stride 2 and no padding. The net- 
work can be segment into four block of conv+conv+pool 
layer such that inside each block the two conv layer have 
the same number of filters, while consecutive block have 
double filter numbers: 64, 128, 256 and 512. At the very 
end, max-pooling be perform across all spatial location 
to produce a single 512-D feature vector. Each conv layer be 
follow by batch normalization [12] and a ReLU nonlin- 
earity. 
Audio subnetwork. The input to the audio subnetwork be a 
1 second sound clip convert into a log-spectrogram (more 
detail be provide late in this section), which be thereafter 
treat a a greyscale 257× 199 image. The architecture of 
the audio subnetwork be identical to the vision one with the 
exception that input pixel be 1-D intensity instead of 3- 
D colour and therefore the conv1 1 filter size be 3× 
small compare to the vision subnetwork. The final audio 
feature be also 512-D. 

2 



concat 

1024 

fc1 1024x128 

128 

fc2 128x2 

2 

softmax 

2 

V 
be 

io 
n 
s 

u 
b 
n 

e 
tw 

o 
rk 

224x224x3 

A 
u 
d 
io 

s 
u 
b 
n 
e 
tw 

o 
rk 

conv1_1 3x3x64 

224x224x64 

conv1_2 3x3x64 

224x224x64 

pool1 2x2 

112x112x64 

conv2_1 3x3x128 

112x112x128 

conv2_2 3x3x128 

112x112x128 

pool2 2x2 

56x56x128 

conv3_1 3x3x256 

56x56x256 

conv3_2 3x3x256 

56x56x256 

pool3 2x2 

28x28x256 

conv4_1 3x3x512 

28x28x512 

conv4_2 3x3x512 

28x28x512 

pool4 28x28 

1x1x512 

conv1_1 3x3x64 

257x199x64 

conv1_2 3x3x64 

257x199x64 

pool1 2x2 

128x99x64 

conv2_1 3x3x128 

128x99x128 

conv2_2 3x3x128 

128x99x128 

pool2 2x2 

64x49x128 

conv3_1 3x3x256 

64x49x256 

conv3_2 3x3x256 

64x49x256 

pool3 2x2 

32x24x256 

conv4_1 3x3x512 

32x24x512 

conv4_2 3x3x512 

32x24x512 

pool4 32x24 

1x1x512 

257x199x1 

log-spectrogram 

1 second 48kHz audio 

Figure 2. L3-Net architecture. Each block represent a single 
layer with text provide more information – first row: layer name 
and parameters, second row: output feature map size. Layers with 
a name prefix conv, pool, fc, concat, softmax be convo- 
lutional, max-pooling, fully connected, concatenation and softmax 
layers, respectively. The list parameter are: conv – kernel size 
and number of channels, pool – kernel size, fc – size of the 
weight matrix. The stride of pool layer be equal to the kernel size 
and there be no padding. Each convolutional layer be follow by 
batch normalization [12] and a ReLU nonlinearity, and the first 
fully connect layer (fc1) be follow by ReLU. 

Fusion network. The two 512-D visual and audio fea- 
tures be concatenate into a 1024-D vector which be pass 
through the fusion network to produce a 2-way classifica- 
tion output, namely, whether the vision and audio corre- 
spond or not. It consists of two fully connect layers, with 
ReLU in between them, and the intermediate feature size of 
128-D. 

2.2. Implementation detail 

Training data sampling. A non-corresponding frame- 
audio pair be compile by randomly sample two different 
video and pick a random frame from one and a random 
1 second audio clip from the other. A correspond frame- 
audio pair be create by sample a random video, pick 
a random frame in that video, and then pick a random 

1 second audio clip that overlap in time with the sample 
frame. This provide additional training sample compare 
to simply sample the 1 second audio with the frame at it 
mid-point. We use standard data augmentation technique 
for images: each training image be uniformly scale such 
that the small dimension be equal to 256, follow by ran- 
dom crop into 224 × 224, random horizontal flipping, 
and brightness and saturation jittering. Audio be only aug- 
mented by change the volume up to 10% randomly but 
consistently across the sample. 
Log-spectrogram computation. The 1 second audio be re- 
sample to 48 kHz, and a spectrogram be compute with 
window length of 0.01 second and a half-window overlap; 
this produce 199 window with 257 frequency bands. The 
response map be pass through a logarithm before feed 
it into the audio subnetwork. 
Training procedure. We use the Adam optimizer [15], 
weight decay 10−5, and perform a grid search on the learn- 
ing rate, although 10−4 usually work well. The network 
be train on 16 GPUs in parallel with synchronous train- 
ing implement in TensorFlow, where each worker pro- 
cessed a 16-element batch, thus make the effective batch 
size of 256. For a training set of 400k 10 second videos, the 
network be train for two days, during which it have see 
60M frame-audio pairs. 

3. Results and discussion 
Our “look, listen and learn” network (L3-Net) approach 

be evaluate and examine in multiple ways. First, the per- 
formance of the network on the audio-visual correspon- 
dence task itself be investigated, and compare to supervise 
baselines. Second, the quality of the learnt visual and audio 
feature be test in a transfer learn setting, on visual and 
audio classification tasks. Finally, we perform a qualitative 
analysis of what the network have learnt. We start by intro- 
ducing the datasets use for training. 

3.1. Datasets 

Two video datasets be use for training the networks: 
Flickr-SoundNet and Kinetics-Sounds. 
Flickr-SoundNet [2]. This be a large unlabelled dataset of 
completely unconstrained video from Flickr, compile by 
search for popular tags, but no tag or any sort of ad- 
ditional information apart from the video themselves be 
used. It contains over 2 million video but for practical rea- 
son we use a random subset of 500k video (400k train- 
ing, 50k validation and 50k test) and only use the first 10 
second of each video. This be the dataset that be use for 
training the L3-Net for the transfer learn experiment in 
Sections 3.3 and 3.4. 
Kinetics-Sounds. While our goal be to learn from com- 
pletely unconstrained videos, have a label dataset be 

3 



useful for quantitative evaluation. For this purpose we take 
a subset (much small than Flickr-SoundNet) of the Ki- 
netics dataset [13], which contains YouTube video man- 
ually annotate for human action use Mechanical Turk, 
and cropped to 10 second around the action. The subset 
contains 19k 10 second video clip (15k training, 1.9k val- 
idation, 1.9k test) form by filter the Kinetics dataset 
for 34 human action classes, which have be chosen to be 
potentially manifest visually and aurally, such a play- 
ing various instrument (guitar, violin, xylophone, etc.), us- 
ing tool (lawn mowing, shovel snow, etc.), a well a 
perform miscellaneous action (tap dancing, bowling, 
laughing, singing, blowing nose, etc.); the full list be give 
in appendix A. Although this dataset be fairly clean by con- 
struction, it still contains considerable noise, e.g. the bowl- 
ing action be often accompany by loud music at the bowl- 
ing alley, human voice (camera operator or video narra- 
tions) often mask the sound of interest, and many video 
contain sound track that be completely unrelated to the 
visual content (e.g. music montage for a snow shovel 
video). 

3.2. Audio-visual correspondence 

First we evaluate the performance of our method on the 
task it be train to solve – decide whether a frame 
and a 1 second audio clip correspond (Section 2). For the 
Kinetics-Sounds dataset which contains label videos, we 
also evaluate two supervise baseline in order to gauge 
how well the AVC training compare to supervise training. 

Supervised baselines. For both baseline we first train vi- 
sion and audio network independently on the action clas- 
sification task, and then combine them in two different 
ways. The vision network have an identical feature extrac- 
tion trunk a our vision subnetwork (Section 2.1), on top 
of which two fully connect layer be attach (sizes: 
512×128 and 128×34) to perform classification into the 34 
Kinetics-Sounds classes. The audio classification network 
be construct analogously. The direct combination base- 
line computes the audio-video correspondence score a the 
similarity of class score distribution of the two networks, 
compute a the scalar product between the 34-D network 
softmax outputs, and decides that audio and video be in 
correspondence if the score be large than a threshold. The 
motivation behind this baseline be that if the vision network 
believe the frame contains a dog while the audio network 
be confident it hears a violin, then the (frame, audio) pair 
be unlikely to be in correspondence. The supervise pre- 
training baseline take the feature extraction trunk from 
the two train networks, assembles them into our network 
architecture by concatenate the feature and add two 
fully connect layer (Section 2.1). The weight of the 
feature extractor be frozen and the fully connect lay- 
er be train on the AVC task in the same manner a our 

Method Flickr-SoundNet Kinetics-Sounds 
Supervised direct – 65% 
Supervised pretraining – 74% 
L3-Net 78% 74% 

Table 1. Audio-visual correspondence (AVC) results. Test set 
accuracy on the AVC task for the L3-Net, and the two supervise 
baseline on the label Kinetics-Sounds dataset. The number 
of positive and negative be the same, so chance get 50%. All 
method be train on the training set of the respective datasets. 

network. This be the strong baseline a it directly cor- 
responds to our method, but with feature learnt in a fully 
supervise manner. 

Results and discussion. Table 1 show the result on the 
AVC task. The L3-Net achieves 74% and 78% on the two 
datasets, where chance be 50%. It should be note that the 
task itself be quite hard due to the unconstrained nature of 
the video (Section 2), a well a due to the very local input 
data which lack context – even human find it hard to judge 
whether an isolated frame and an isolated single second of 
audio correspond; informal human test indicate that hu- 
man be only a few percent good than the L3-Net. Fur- 
thermore, the supervise baseline do not beat the L3-Net 
a “supervised pretraining” performs on par with it, while 
“supervised direct combination” work significantly bad 
as, unlike “supervised pretraining”, it have not be train 
for the AVC task. 

3.3. Audio feature 

In this section we evaluate the power of the audio repre- 
sentation that emerges from the L3-Net approach. Namely, 
the L3-Net audio subnetwork train on Flickr-SoundNet be 
use to extract feature from 1 second audio clips, and the 
effectiveness of these feature be evaluate on two standard 
sound classification benchmarks: ESC-50 and DCASE. 

Environmental sound classification (ESC-50) [26]. This 
dataset contains 2000 audio clips, 5 second each, equally 
balance between 50 classes. These include animal 
sounds, natural soundscapes, human non-speech sounds, in- 
terior/domestic sounds, and exterior/urban noises. The data 
be split into 5 predefined fold and performance be measure 
in term of mean accuracy over 5 leave-one-fold-out evalu- 
ations. 

Detection and classification of acoustic scene and event 
(DCASE) [32]. We consider the scene classification task of 
the challenge which contains 10 class (bus, busy street, 
office, open air market, park, quiet street, restaurant, super- 
market, tube, tube station), with 10 training and 100 test 
clip per class, where each clip be 30 second long. 

Experimental procedure. To enable a fair direct compar- 
ison with the current state-of-the-art, Aytar et al. [2], we 
follow the same experimental setup. Multiple overlap 

4 



(a) ESC-50 (b) DCASE 
Method Accuracy 
SVM-MFCC [26] 39.6% 
Autoencoder [2] 39.9% 
Random Forest [26] 44.3% 
Piczak ConvNet [25] 64.5% 
SoundNet [2] 74.2% 
Ours random 62.5% 
Ours 79.3% 
Human perf. [26] 81.3% 

Method Accuracy 
RG [27] 69% 
LTT [19] 72% 
RNH [28] 77% 
Ensemble [32] 78% 
SoundNet [2] 88% 
Ours random 85% 
Ours 93% 

Table 2. Sound classification. “Ours random” be an additional 
baseline which show the performance of our network without L3- 
training. Our L3-training set the new state-of-the-art by a large 
margin on both benchmarks. 

Method Top 1 accuracy 
Random 18.3% 
Pathak et al. [24] 22.3% 
Krähenbühl et al. [16] 24.5% 
Donahue et al. [7] 31.0% 
Doersch et al. [6] 31.7% 
Zhang et al. [36] (init: [16]) 32.6% 
Noroozi and Favaro [21] 34.7% 
Ours random 12.9% 
Ours 32.3% 

Table 3. Visual classification on ImageNet. Following [36], our 
feature be evaluate by training a linear classifier on the Ima- 
geNet training set and measure the classification accuracy on 
the validation set. For more detail and discussion see Section 
3.4. All performance number apart from ours be provide by au- 
thor of [36], show only the best performance for each method 
over all parameter choice (e.g. Donahue et al. [7] achieve 27.1% 
instead of 31.0% when take feature from pool5 instead of 
conv3). 

subclips be extract from each record and described 
use our features. For 5 second recording from ESC-50 
we extract 10 equally space 1 second subclips, while for 
the 6 time longer DCASE recordings, 60 subclips be ex- 
tracted per clip. The audio feature be obtain by max- 
pool the last convolutional layer of the audio subnetwork 
(conv4 2), before the ReLU, into a 4 × 3 × 512 = 6144 
dimensional representation (the conv4 2 output be orig- 
inally 16 × 12 × 512). The feature be preprocessed us- 
ing z-score normalization, i.e. shift and scale to have a 
zero mean and unit variance. A multi-class one-vs-all lin- 
ear SVM be trained, and at test time the class score for a 
record be compute a the mean over the class score 
for it subclips. 

Results and discussion. Table 2 show the result on ESC- 
50 and DCASE. On both benchmark we convincingly beat 
the previous state-of-the-art, SoundNet [2], by 5.1% and 
5% absolute. For ESC-50 we reduce the gap between the 
previous best result and the human performance by 72% 
while for DCASE we reduce the error by 42%. The result 

be especially impressive a SoundNet us two vision net- 
work train in a fully supervise manner on ImageNet and 
Places2 a teacher for the audio network, while we learn 
both the vision and the audio network without any super- 
vision whatsoever. Note that we train our network with a 
random subset of the SoundNet video for efficiency pur- 
poses, so it be possible that further gain can be achieve by 
use all the available training data. 

3.4. Visual feature 

In this section we evaluate the power of the visual repre- 
sentation that emerges from the L3-Net approach. Namely, 
the L3-Net vision subnetwork train on Flickr-SoundNet 
be use to extract feature from images, and the effective- 
ness of these feature be evaluate on the ImageNet large 
scale visual recognition challenge 2012 [29]. 
Experimental procedure. We follow the experimental 
setup of Zhang et al. [36] where feature be extract from 
256 × 256 image and use to perform linear classification 
on ImageNet. As in [36], we take conv4 2 feature af- 
ter ReLU and perform max-pooling with equal kernel and 
stride size until feature dimensionality be below 10k; in our 
case this result in 4×4×512 = 8192-D features. A single 
fully connect layer be add to perform linear classifica- 
tion into the 1000 ImageNet classes. All the weight be 
frozen to their L3-Net-trained values, apart from the final 
classification layer which be train with cross-entropy loss 
on the ImageNet training set. The training procedure (data 
augmentation, learn rate schedule, label smoothing) be 
identical to [33], the only difference be that we use the 
Adam optimizer instead of RMSprop, and a 256×256 input 
image instead of 299 × 299 a it fit our architecture good 
and to be consistent with [36]. 
Results and discussion. Classification accuracy on the Im- 
ageNet validation set be show in Table 3 and contrast 
with other unsupervised and self-supervised methods. We 
also test the performance of random features, i.e. our L3- 
Net architecture without AVC training but with a train 
classification layer. 

Our L3-Net-trained feature achieve 32.3% accuracy 
which be on par with other state-of-the-art self-supervised 
method of [6, 7, 21, 36], while convincingly beating ran- 
dom initialization, data-dependent initialization [16], and 
Context Encoders [24]. It should be note that these meth- 
od use the AlexNet [17] architecture which be different to 
ours, so the result be not fully comparable. On the one 
hand, our architecture when train from scratch in it en- 
tirety achieves a high performance (59.2% v AlexNet’s 
51.0%). On the other hand, it be deeper which make it 
harder to train a can be see from the fact that our random 
feature perform bad than theirs (12.9% v AlexNet’s 
18.3%), and that all compete method hit peak perfor- 
mance when they use early layer (e.g. [7] drop from 

5 



Fingerpicking Lawn mow P. accordion P. bass guitar P. saxophone Typing Bowling P. clarinet P. organ 

Figure 3. Learnt visual concept (Kinetics-Sounds). Each column show five image that most activate a particular unit of the 512 in 
pool4 for the vision subnetwork. Note that these feature do not take sound a input. Videos come from the Kinetics-Sounds test set 
and the network be train on the Kinetics-Sounds train set. The top row show the dominant action label for the unit (“P.” stand for 
“playing”). 

Fingerpicking Lawn mow P. accordion P. bass guitar P. saxophone Typing Bowling P. clarinet P. organ 

Figure 4. Visual semantic heatmap (Kinetics-Sounds). Examples correspond to the one in Figure 3. A semantic heatmap be obtain a 
a slice of activation from conv4 2 of the vision subnetwork that corresponds to the same unit from pool4 a in Figure 3, i.e. the unit 
that responds highly to the class in question. 

6 



31.0% to 27.1% when go from conv3 to pool5). In 
fact, when measure the improvement achieve due to 
AVC or self-supervised training versus the performance of 
the network with random initialization, our AVC training 
beat all competitors. 

Another important fact to consider be that all compete 
method actually use ImageNet image when training. Al- 
though they do not make use of the labels, the underlie 
image statistic be the same: object be fairly central in the 
image, and the network have seen, for example, abundant 
image of 120 bread of dog and thus potentially learnt 
their distinguish features. In contrast, we use a com- 
pletely separate source of training data in the form of frame 
from Flickr video – here the object be in general not cen- 
tred, it be likely that the network have never see a “Tibetan 
terrier” nor the majority of other fine-grained categories. 
Furthermore, video frame have vastly different low-level 
statistic to still images, with strong artefact such a mo- 
tion blur. With these factor hamper our network, it be 
impressive that our visual feature L3-Net-trained on Flickr 
video perform on par with self-supervised state-of-the-art 
train on ImageNet. 

3.5. Qualitative analysis 

In this section we analyse what be it that the network 
have learnt. We visualize the result on the test set of the 
Kinetics-Sounds and Flickr-SoundNet datasets, so the net- 
work have not see the video during training. 

3.5.1 Vision feature 

To probe what the vision subnetwork have learnt, we pick 
a particular ‘unit’ in pool4 (i.e. a component of the 512 
dimensional pool4 vector) and rank the test image by 
it magnitude. Figure 3 show the image from Kinetics- 
Sounds that activate particular unit in pool4 the most (i.e. 
be ranked high by it magnitude). As can be seen, the 
vision subnetwork have automatically learnt, without any ex- 
plicit supervision, to recognize semantic entity such a 
guitars, accordions, keyboards, clarinets, bowling alleys, 
lawn or lawnmowers, etc. Furthermore, it have learnt finer- 
grain category a well a it be able to distinguish be- 
tween acoustic and bass guitar (“fingerpicking” be mostly 
associate with acoustic guitars). 

Figure 4 show heatmaps for the Kinetics-Sounds im- 
age in Figure 3, obtain by simply display the spa- 
tial activation of the correspond vision unit (i.e. if the 
k component of pool4 be chosen, then the k channel of 
conv4 2 be displayed – since the k component be just the 
spatial max over this channel (after ReLU)). Objects be 
successfully detect despite significant clutter and occlu- 
sions. It be interest to observe the type of cue that the 
network decides to use, e.g. the “playing clarinet” unit, in- 

stead of try to detect the entire clarinet, seem to mostly 
activate on the interface between the player’s face and the 
clarinet. 

Figures 5 and 6 show visual concept learnt by the L3- 
Net on the Flickr-SoundNet dataset. It can be see that the 
network learns to recognize many scene category (Figure 
5), such a outdoors, concert, water, sky, crowd, text, rail- 
way, etc. These be useful for the AVC task as, for example, 
crowd indicate a large event that be associate with a dis- 
tinctive sound a well (e.g. a football game), text indicates 
narration, and outdoors scene be likely to be accompa- 
nied with wind sounds. It should be note that though at 
first sight some category seem trivially detectable, it be not 
the case; for example, “sky” detector be not equivalent to 
the “blueness” detector a it only fire on “sky” and not on 
“water”, and furthermore there be separate unit sensitive 
to “water surface” and to “underwater” scenes. The network 
also learns to detect people a user uploaded content be sub- 
stantially people-oriented – Figure 6 show the network have 
learnt to distinguish between babies, adult and crowds. 

3.5.2 Audio feature 

Figure 7 show what particular audio unit be sensitive 
to in the Kinetics-Sounds dataset. For visualization pur- 
poses, instead of show the sound form, we display the 
video frame that corresponds to the sound. It can be see 
that the audio subnetwork, again without any supervision, 
manages to learn various semantic entities, a well a per- 
form fine-grained classification (“fingerpicking” v “play- 
ing bass guitar”). Note that some unit be naturally con- 
fuse – the “tap dancing” unit also responds to “pen tap- 
ping”, while the “saxophone” unit be sometimes confuse 
with a “trombone”. These be reasonable mistakes, espe- 
cially when take into account that the sound input be only 
one second in length. The audio concept learnt on the 
Flickr-SoundNet dataset (Figure 8) follow the same pattern 
a the visual one – the network learns to distinguish var- 
ious scene category such a water, underwater, outdoors 
and windy scenes, a well a human-related concept like 
baby and human voices, crowds, etc. 

Figure 9 show spectrogram and their semantic 
heatmaps, illustrate that our L3-Net learns to detect au- 
dio events. For example, it show clear preference for low 
frequency when detect bass guitars, attention to wide 
frequency range when detect lawnmowers, and temporal 
‘steps’ when detect fingerpicking and tap dancing. 

3.5.3 Versus random feature 

Could the result in Figures 3, 4, 5, 6, 7 8, and 9 simply 
be obtain by chance due to examine a large number 
of units, a colourfully illustrate by the dead salmon ex- 
periment [3]? It be unlikely a there be only 512 unit in 

7 



Outdoor Concert Outdoor sport 

Cloudy sky Sky Water surface Underwater 

Horizon Railway Crowd Text 

Figure 5. Learnt visual concept and semantic heatmaps (Flickr-SoundNet). Each mini-column show five image that most activate 
a particular unit of the 512 in pool4 of the vision subnetwork, and the correspond heatmap (for more detail see Figures 3 and 4). 
Column title be a subjective name of concept the unit respond to. 

8 



Baby Face Head Crowd 

Figure 6. Learnt human-related visual concept and semantic heatmaps (Flickr-SoundNet). Each mini-column show five image that 
most activate a particular unit of the 512 in pool4 of the vision subnetwork, and the correspond heatmap (for more detail see Figures 
3 and 4). Column title be a subjective name of concept the unit respond to. 

Fingerpicking Lawn mow P. accordion P. bass guitar P. saxophone Typing P. xylophone Tap dance Tickling 

Figure 7. Learnt audio concept (Kinetics-Sounds). Each column show five sound that most activate a particular unit in pool4 of the 
audio subnetwork. Purely for visualization purposes, a it be hard to display sound, the frame of the video that be align with the sound be 
show instead of the actual sound form, but we stress that no vision be use in this experiment. Videos come from the Kinetics-Sounds test 
set and the network be train on the Kinetics-Sounds train set. The top row show the dominant action label for the unit (“P.” stand for 
“playing”). 

9 



Baby voice Human voice Male voice People Crowd 

Music Concert Sport Clapping Water Underwater Windy Outdoor 

Figure 8. Learnt audio concept (Flickr-SoundNet). Each mini-column show sound that most activate a particular unit of the 512 in 
pool4 of the audio subnetwork. Purely for visualization purposes, a it be hard to display sound, the frame of the video that be align with 
the sound be show instead of the actual sound form, but we stress that no vision be use in this experiment. Column title be a subjective 
name of concept the unit respond to. Note that for the “Human voice”, “Male voice”, “Crowd”, “Music” and “Concert” examples, the 
respective clip do contain the relevant audio despite the frame look a if it be unrelated, e.g. the third example in the “Concert” column 
do contain loud music sounds. Audio clip contain the five concatenate 1 sample correspond to each mini-column be host 
on YouTube and can be reach by click on the respective mini-columns; this YouTube playlist (https://goo.gl/ohDGtJ) contains all 16 
examples. 

10 

http://youtu.be/MrADE51ahHQ 
http://youtu.be/L-SbBq2xJVo 
http://youtu.be/i8khQ-MY9Qg 
http://youtu.be/ET3i_yZm4Ww 
http://youtu.be/vs5-5u2C-KI 
http://youtu.be/GBL0475ctoQ 
http://youtu.be/Ax6Xcqn1Gxc 
http://youtu.be/Tt41s6xezkM 
http://youtu.be/A5aEXSUbGjg 
http://youtu.be/cqQY7TrqQdw 
http://youtu.be/8BPQd_DytyI 
http://youtu.be/GlbhKNqNCGw 
http://youtu.be/GWW5scO468o 
http://youtu.be/_AR2dCWd3aY 
http://youtu.be/gyIdY6ncxpc 
http://youtu.be/7DxHhEewhwA 
https://goo.gl/ohDGtJ 


pool4 to choose from, and many of those be found to 
be highly correlate with a semantic concept. Nevertheless, 
we repeat the same experiment with a random network 
(i.e. a network that have not be trained), and have fail 
to find such correlation. In more detail, we examine how 
many out of the action class in Kinetics-Sounds have a 
unit in pool4 which show high preference for the class. 
For the vision subnetwork the preference be determine by 
rank all image by their unit activation, and retain the 
top 5; if 4 out of these 5 image correspond to one class, 
then that class be deem to have a high-preference for the 
unit (a similar procedure be carry out for the audio sub- 
network use spectrograms). Our train vision and audio 
network have high-preference unit for 10 and 11 out of a 
possible 34 action classes, respectively, compare to 1 and 1 
for the random vision and audio networks. Furthermore, if 
the threshold for deem a unit to be high-preference be re- 
duced to 3, our train vision and audio subnetworks cover 
23 and 20 classes, respectively, compare to the 4 and 3 of 
a random network, respectively. These result confirm that 
our network have indeed learnt semantic features. 

Furthermore, Figure 10 show the comparison between 
the train and the non-trained (i.e. network with random 
weights) L3-Net representation for the visual and the au- 
dio modalities, on the Kinetics-Sounds dataset, use the 
t-SNE visualization [34]. It be clear that training for the 
audio-visual correspondence task produce representation 
that have a semantic meaning, a video contain the same 
action class often cluster together, while the random net- 
work’s representation do not exhibit any clustering. There 
be still a fair amount of confusion in the representations, but 
this be expect a no class-level supervision be provide and 
class can be very alike. For example, an organ and a piano 
be quite visually similar a they contain keyboards, and the 
visual difference between a bass guitar and an acoustic gui- 
tar be also quite fine; these similarity be reflect in the 
closeness or overlap of respective cluster in Figure 10(c) 
(e.g. a note earlier, “fingerpicking” be mostly associate 
with acoustic guitars). 

We also evaluate the quality of the L3-Net embeddings 
by cluster them with k-means into 64 cluster and re- 
port the Normalized Mutual Information (NMI) score 
between the cluster and the ground truth action classes. 
Results in Table 4 confirm the emergence of semantics a 

Method Vision Audio 
Random assignment 0.165 0.165 
Ours random (L3-Net without training) 0.204 0.219 
Ours (L3-Net self-supervised training) 0.409 0.330 

Table 4. Clustering quality. Normalized Mutual Information 
(NMI) score between the unsupervised clustering of feature em- 
bedding and the Kinetics-Sounds labels. 

Fingerpicking Lawn mow P. bass guitar Tap dance 

Figure 9. Audio semantic heatmaps (Kinetics-Sounds). Each 
pair of column show a single action class (top, “P.” stand for 
“playing”), five log-spectrograms (left) and spectrogram semantic 
heatmaps (right) for the class. Horizontal and vertical ax cor- 
respond to the time and frequency dimensions, respectively. A 
semantic heatmap be obtain a a slice of activation of the unit 
from conv4 2 of the audio subnetwork which show preference 
for the consider class. 

the L3-Net embeddings outperform the best random base- 
line by 50-100%. 

The t-SNE visualization also show some interest fea- 
tures, such a the “typing” class be divide into two clus- 
ters in the visual domain. Further investigation reveals that 
all frame in one cluster show both a keyboard and hands, 
while the second cluster contains much few hands. Sepa- 
rating these two case can be a good indication of whether 
the type action be happen at the moment capture by 
the (frame, 1 second sound clip) pair, and thus whether 
the type sound be expect to be heard. Furthermore, we 
found that the “typing” audio sample appear in three clus- 
ters – the two fairly pure cluster (outlined in Figure 10(a)) 
correspond to strong type sound and talk while typ- 
ing, respectively, and the remain cluster, which be very 
impure and intermingle with other action classes, mostly 
corresponds to silence and background noise. 

4. Discussion 
We have show that the network train for the AVC 

task achieves superior result on sound classification to re- 
cent method that pre-train and fix the visual network (one 
each for ImageNet and Scenes), and we conjecture that the 
reason for this be that the additional freedom of the visual 
network allows the learn to good take advantage of the 
opportunity offer by the variety of visual information in 
the video (rather than be restrict to see only through 

11 



the eye of the pre-trained network). Also, the visual fea- 
tures that emerge from the L3-Net be on par with the state- 
of-the-art among self-supervised approaches. Furthermore, 
it have be demonstrate that the network automatically 
learns, in both modalities, fine-grained distinction such a 
bass versus acoustic guitar or saxophone versus clarinet. 

The localization visualization result be reminiscent of 
the classic highlight pixel in [14], except in our case we 
do not just learn the few pixel that move (concurrent with 
the sound) but instead be able to learn extend region 
correspond to the instrument. 

We motivate this work by consider correlation of 
video and audio events. However, we believe there be ad- 
ditional information in concurrency of the two streams, a 
concurrency be strong than correlation because the event 
need to be synchronise (of course, if event be concurrent 
then they will correlate, but not vice versa). Training for 
concurrency will require video (multiple frames) a input, 
rather than a single video frame, but it would be interest 
to explore what more be gain from this strong condition. 

In the future, it would be interest to learn from the 
recently release large dataset of video curated accord 
to audio, rather than visual, event [10] and see what subtle 
visual semantic category be discovered. 

References 
[1] P. Agrawal, J. Carreira, and J. Malik. Learning to see by 

moving. In Proc. ICCV, 2015. 
[2] Y. Aytar, C. Vondrick, and A. Torralba. SoundNet: Learning 

sound representation from unlabeled video. In NIPS, 2016. 
[3] C. M. Bennett, M. B. Miller, and G. L. Wolford. Neural cor- 

relates of interspecies perspective take in the post-mortem 
Atlantic salmon: An argument for multiple comparison cor- 
rection. NeuroImage, 2009. 

[4] A. Blum and T. Mitchell. Combining label and unlabeled 
data with co-training. In Computational learn theory, 
1998. 

[5] J. S. Chung and A. Zisserman. Out of time: Automated lip 
sync in the wild. In Workshop on Multi-view Lip-reading, 
ACCV, 2016. 

[6] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised vi- 
sual representation learn by context prediction. In Proc. 
CVPR, 2015. 

[7] J. Donahue, P. Krähenbühl, and T. Darrell. Adversarial fea- 
ture learning. In Proc. ICLR, 2017. 

[8] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and 
T. Brox. Discriminative unsupervised feature learn with 
convolutional neural networks. In NIPS, 2014. 

[9] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. A. 
Ranzato, and T. Mikolov. Devise: A deep visual-semantic 
embed model. In NIPS, 2013. 

[10] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, 
W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter. Au- 
dio Set: An ontology and human-labeled dataset for audio 
events. In ICASSP, 2017. 

[11] D. Harwath, A. Torralba, and J. R. Glass. Unsupervised 

learn of spoken language with visual context. In NIPS, 
2016. 

[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating 
deep network training by reduce internal covariate shift. In 
Proc. ICML, 2015. 

[13] W. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, 
S. Vijayanarasimhan, F. Viola, T. Green, T. Back, P. Natsev, 
M. Suleyman, and A. Zisserman. The Kinetics human action 
video dataset. CoRR, abs/1705.06950, 2017. 

[14] E. Kidron, Y. Y. Schechner, and M. Elad. Pixels that sound. 
In Proc. CVPR, 2005. 

[15] D. P. Kingma and J. Ba. Adam: A method for stochastic 
optimization. In Proc. ICLR, 2015. 

[16] P. Krähenbühl, C. Doersch, J. Donahue, and T. Darrell. Data- 
dependent initialization of convolutional neural networks. 
In Proc. ICLR, 2015. 

[17] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet 
classification with deep convolutional neural networks. In 
NIPS, page 1106–1114, 2012. 

[18] J. Lei Ba, K. Swersky, S. Fidler, and R. Salakhutdinov. Pre- 
dicting deep zero-shot convolutional neural network use 
textual descriptions. In Proc. ICCV, 2015. 

[19] D. Li, J. Tam, and D. Toub. Auditory scene classification us- 
ing machine learn techniques. IEEE AASP Challenge on 
Detection and Classification of Acoustic Scenes and Events, 
2013. 

[20] I. Misra, C. L. Zitnick, and M. Herbert. Shuffle and learn: 
Unsupervised learn use temporal order verification. In 
Proc. ECCV, 2016. 

[21] M. Noroozi and P. Favaro. Unsupervised learn of visual 
representation by solve jigsaw puzzles. In Proc. ECCV, 
2016. 

[22] A. Owens, P. Isola, J. McDermott, A. Torralba, E. Adelson, 
and W. Freeman. Visually indicate sounds. In Proc. CVPR, 
2016. 

[23] A. Owens, W. Jiajun, J. McDermott, W. Freeman, and 
A. Torralba. Ambient sound provide supervision for visual 
learning. In Proc. ECCV, 2016. 

[24] D. Pathak, P. Krähenbühl, J. Donahue, T. Darrell, and A. A. 
Efros. Context encoders: Feature learn by inpainting. In 
Proc. CVPR, 2016. 

[25] K. J. Piczak. Environmental sound classification with con- 
volutional neural networks. In IEEE Workshop on Machine 
Learning for Signal processing, 2015. 

[26] K. J. Piczak. ESC: Dataset for environmental sound classifi- 
cation. In Proc. ACMM, 2015. 

[27] A. Rakotomamonjy and G. Gasso. Histogram of gradient 
of time-frequency representation for audio scene classifica- 
tion. IEEE/ACM Transactions on Audio, Speech, and Lan- 
guage Processing, 2015. 

[28] G. Roma, W. Nogueira, and P. Herrera. Recurrence quantifi- 
cation analysis feature for environmental sound recognition. 
In IEEE Workshop on Applications of Signal Processing to 
Audio and Acoustics (WASPAA), 2013. 

[29] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, 
S. Ma, S. Huang, A. Karpathy, A. Khosla, M. Bernstein, 
A. Berg, and F. Li. Imagenet large scale visual recognition 
challenge. IJCV, 2015. 

[30] K. Simonyan and A. Zisserman. Very deep convolutional 
network for large-scale image recognition. In Proc. ICLR, 

12 



2015. 
[31] R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot 

learn through cross-modal transfer. In NIPS, 2013. 
[32] D. Stowell, D. Giannoulis, E. Benetos, M. Lagrange, and 

M. D. Plumbley. Detection and classification of acoustic 
scene and events. In IEEE Transactions on Multimedia, 
2015. 

[33] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. 
Rethinking the Inception architecture for computer vision. In 
Proc. CVPR, 2016. 

[34] L. Van der Maaten and G. Hinton. Visualizing data use 
t-SNE. Journal of Machine Learning Research, 2008. 

[35] X. Wang and A. Gupta. Unsupervised learn of visual rep- 
resentations use videos. In Proc. ICCV, 2015. 

[36] R. Zhang, P. Isola, and A. A. Efros. Colorful image coloriza- 
tion. In Proc. ECCV, 2016. 

A. Kinetics-Sounds 
The 34 action class take from the Kinetics dataset 

[13] to form the Kinetics-Sounds dataset 3.1 are: blow- 
ing nose, bowling, chop wood, rip paper, shuffle 
cards, singing, tap pen, typing, blowing out, dribble 
ball, laughing, mow the lawn by push lawnmower, 
shovel snow, stomping, tap dancing, tap guitar, tick- 
ling, fingerpicking, patting, play accordion, play bag- 
pipes, play bass guitar, play clarinet, play drums, 
play guitar, play harmonica, play keyboard, play- 
ing organ, play piano, play saxophone, play trom- 
bone, play trumpet, play violin, play xylophone. 

13 



(a) Audio Learnt 

sing 

type 

laugh 

fingerpicking 

play accordion 

play bass guitar 

play clarinet 

play drum 

play organ 

play piano 

play saxophone 

play trombone 

play xylophone 

(b) Audio Random 

r 

type 

type 
laughingp. xylophone 

p. drum 

p. bass guitar 

ngerpicking 

p. piano 
p. organ 

p. accordion 

sing 

type 

laugh 

fingerpicking 

play accordion 

play bass guitar 

play clarinet 

play drum 

play organ 

play piano 

play saxophone 

play trombone 

play xylophone 

(c) Visual Learnt (d) Visual Random 

fingerpicking 

p. bass 

guitar 

p. organ 

p. piano 

p. drumsp. clarinet 

p. accordion 

type 

type 
laugh 

Figure 10. t-SNE visualization [34] of learnt representation (Kinetics-Sounds). The (a,c) and (b,d) show the two-dimensional t-SNE 
embeddings for the train versus non-trained (i.e. network with random weights) L3-Net, respectively. For visualization purpose only, 
we colour the t-SNE embeddings use the Kinetics-Sounds labels, but no label be use for training the L3-Net. For clarity and reduce 
clutter, only a subset of action (13 class out of 34) be shown. Some clearly noticeable cluster be manually highlight by enclose 
them with ellipses. Best view in colour. 

14 




