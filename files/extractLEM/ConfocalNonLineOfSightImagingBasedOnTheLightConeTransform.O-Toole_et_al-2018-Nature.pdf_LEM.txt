






















































Confocal non-line-of-sight image base on the light-cone transform 


0 0 M o n t h 2 0 1 8 | V o L 0 0 0 | n A t U R E | 1 

LEttER 
doi:10.1038/nature25489 

Confocal non-line-of-sight image base on the 
light-cone transform 
Matthew o’toole1, David B. Lindell1 & Gordon Wetzstein1 

How to image object that be hidden from a camera’s view be a 
problem of fundamental importance to many field of research1–20, 
with application in robotic vision, defence, remote sensing, 
medical image and autonomous vehicles. Non-line-of-sight 
(NLOS) image at macroscopic scale have be demonstrate by 
scan a visible surface with a pulse laser and a time-resolved 
detector14–19. Whereas light detection and range (LIDAR) system 
use such measurement to recover the shape of visible object from 
direct reflections21–24, NLOS image reconstructs the shape and 
albedo of hidden object from multiply scatter light. Despite 
recent advances, NLOS image have remain impractical owe 
to the prohibitive memory and processing requirement of exist 
reconstruction algorithms, and the extremely weak signal of 
multiply scatter light. Here we show that a confocal scan 
procedure can address these challenge by facilitate the derivation 
of the light-cone transform to solve the NLOS reconstruction 
problem. This method require much small computational and 
memory resource than previous reconstruction method do and 
image hidden object at unprecedented resolution. Confocal 
scan also provide a sizeable increase in signal and range when 
image retroreflective objects. We quantify the resolution bound 
of NLOS imaging, demonstrate it potential for real-time track 
and derive efficient algorithm that incorporate image prior and a 
physically accurate noise model. Additionally, we describe successful 
outdoor experiment of NLOS image under indirect sunlight. 

LIDAR system use time-resolved sensor to scan the three- 
dimensional (3D) geometry of objects21–24. Such system acquire range 
measurement by record the time require for light to travel along a 
direct path from a source to a point on the object and back to a sensor. 
Recently, these type of sensor have also be use to perform NLOS 
tracking12,13 or imaging14–20 of object ‘hidden around corners’, where 
the position and shape of the object be compute from indirect light 
paths. The light travel along indirect path scatter multiple time 
before reach a sensor and may scatter off object outside a camera’s 
direct line of sight (Fig. 1). Recovering image of hidden object from 
indirect light path involves a challenge inverse problem because 
there be infinitely many such path to consider. With application in 
remote sense and machine vision, NLOS image could enable capa- 
bilities for a variety of image systems. 

The challenge task of image object that be partially or fully 
obscure from view have be tackle with approach base on time- 
gate imaging2, coherence gating3, speckle correlation4,5, wavefront 
shaping6, ghost imaging7,8, structure illumination9 and intensity 
imaging10,11. At macroscopic scales, the most promising NLOS image 
system rely on time-resolved detectors12–20. However, NLOS image 
with time-resolved system remains a hard problem for three main 
reasons. First, the reconstruction step be prohibitively computationally 
demanding, in term of both memory requirement and processing 
cycles. Second, the flux of multiply scatter light be extremely low, 
require either extensive acquisition time in dark environment or a 

sufficiently high-power laser to overcome the contribution of ambient 
light. Finally, NLOS image often require a custom hardware system 
make with expensive components, thus prevent it widespread use. 

Confocal NLOS (C-NLOS) image aim to overcome these 
challenges. Whereas previous NLOS acquisition setup exhaustively illu- 
minate and image pair of distinct point on a visible surface (such a a 
wall), the propose system illuminates and image the same point (Fig. 1) 
and raster-scans this point across the wall to acquire a 3D transient 
(that is, time-resolved) image14,25–27. C-NLOS image offer several 
advantage over exist methods. First, it facilitates the derivation of 
a closed-form solution to the NLOS problem. The propose NLOS 
reconstruction procedure be several order of magnitude faster and 
more memory-efficient than previous approaches, and it also produce 
higher-quality reconstructions. Second, whereas indirectly scatter 
light remains extremely weak for diffuse objects, retroreflective object 
(such a road signs, bicycle reflector and high-visibility safety apparel) 
considerably increase the indirect signal by reflect light back to it 
source with minimal scattering. This retroreflectance property can only 
be exploit by confocalized system that simultaneously illuminate 
and image a common point and may be the enable factor towards 
make NLOS image practical in certain application (such a 
autonomous driving). Third, LIDAR system already perform con- 
focal scan to acquire point cloud from direct light paths. Our 
prototype system be built from the ground up, but commercial LIDAR 
system may be capable of support the algorithm developed here 
with minimal hardware modifications. 

Similarly to other NLOS image approaches, our image formation 
model make the follow assumptions: there be only single scatter 
behind the wall (that is, no inter-reflections in the hidden part of the 
scene), light scatter isotropically (that is, the model ignores Lambert’s 
cosine terms), and no occlusion occur within the hidden scene. Our 
approach also support retroreflective material through a minor 
modification of the image formation model. 

C-NLOS measurement consist of a two-dimensional set of temporal 
histograms, acquire by confocally scan point x′ , y′ on a planar 
wall at position z′ = 0. This 3D volume of measurements, τ, be give by 

∫∫∫ 
τ 

ρ δ 

′ ′ = 

′− + ′− + − 
Ω 

x y t 

r 
x y z x x y y z tc x y z 

( , , ) 
1 ( , , ) (2 ( ) ( ) )d d d (1) 

4 
2 2 2 

where c be the speed of light. Every measurement sample τ (x′ , y′ , t) 
capture the photon flux at point (x′ , y′ ) and time t relative to an 
incident pulse scatter by the same point at time t = 0. Here, 
the function ρ be the albedo of the hidden scene at each point (x, y, z) 
with z > 0 in the 3D half-space Ω. The Dirac delta function 
δ represent the surface of a spatio-temporal four-dimensional 
hypercone give by + + − / =x y z tc( 2) 02 2 2 2 , which model 
light propagation from the wall to the object and back to the wall. It be 
also closely related to Minkowski’s light cone28, which be a geometric 

1Department of Electrical Engineering, Stanford University, Stanford, California 94305 USA. 

© 2018 Macmillan Publishers Limited, part of Springer Nature. All right reserved. 

http://www.nature.com/doifinder/10.1038/nature25489 


2 | n A t U R E | V o L 0 0 0 | 0 0 M o n t h 2 0 1 8 

LetterreSeArCH 

representation of light propagation through space and time. We note 
that the function be shift-invariant in the x and y axes, but not in the 
z axis. A feature of this formulation be that the distance function 
= ′− + ′− + = /r x x y y z tc( ) ( ) 22 2 2 can be express in term of 

the arrival time t; the radiometric term /r1 4 can thus be pull out of 
the triple integral. Equation (1) can also be modify to model retro- 
reflective material by replace /r1 4 with /r1 2, which represent a large 
increase in the flux of the indirect light (see Supplementary Information 
for details). 

The most remarkable property of equation (1) be the fact that a 
change of variable in the integral by =z u , / = /z u ud d 1 (2 ) and 
= /v tc( 2)2 result in 

∫∫∫ 

τ 

ρ δ 

′ ′ / = 

′− + ′− + − 

τ 

Ω 
ρ 

/ 

′ ′ 

′− ′− − 

� ��������� ��������� 

� ������� ������� 
� ������������� ������������� 

R 

R 

v x y v c 

u 
x y u x x y y u v x y u 

( , , 2 ) 

1 
2 

( , , ) (( ) ( ) ) d d d 
(2) 

x y v 

x y u 
h x x y y v u 

3 2 

{ } ( , , ) 

{ } ( , , ) 

2 2 

( , , ) 

t 

z 

which can be express a a straightforward 3D convolution, where 
τ ρ= ∗R Rh{ } { }t z . Here, the function h be a shift-invariant 3D convo- 

lution kernel, the transform Rz nonuniformly resamples and attenuates 
the element of volume ρ along the z axis, and the transform R t non- 
uniformly resamples and attenuates the measurement τ along the time 
axis. The inverse of both Rz and R t also have closed-form expressions. 
We refer to equation (2) a the light-cone transform (LCT). 

The image formation model can be discretized a τ ρ=R HRt z , where 
Rτ∈ + 

n n nx y t be the vectorized representation of the measurements, and 
Rρ∈ + 

n n nx y z be the vectorized volume of the albedo of the hidden 

surface. The process of discretizing each function involves define a 
finite grid and integrate the function over each cell in the grid. The 
matrix R∈ + 

×H n n n n n nx y h x y h represent the shift-invariant 3D convolution 
operation, and the matrix R∈ + 

×Rt n n n n n nx y h x y t and R∈ + 
×Rz n n n n n nx y h x y z 

represent the transformation operation apply to the temporal and 
spatial dimensions, respectively. We note that both transformation 
matrix be independently apply to their respective dimension and 
can therefore be apply to large-scale datasets in a computationally 
and memory-efficient way. Similarly, the 3D convolution operation H 
can be compute efficiently in the Fourier domain. Together, these 
matrix represent the discrete LCT. 

By treat NLOS image a a spatially invariant 3D deconvolution 
problem, a closed-form solution can be derive from the convolution 
theorem. The convolution operation be express a an element-wise 
multiplication in the Fourier domain and invert accord to 

⁎ρ τ= 
 

 

 

| | 

| | + 

 

 

α 

− − 
� 

� 

� 
R F 

H 
H 

H 
FR1 (3)z t 

1 1 
2 

2 1 

where F be the 3D discrete Fourier transform, ⁎ρ be the estimate volume 
of the albedo of the hidden surface, �H be a diagonal matrix contain 
the Fourier coefficient of the 3D convolution kernel, and α represent 
the frequency-dependent signal-to-noise ratio of the measurements. 
This approach be base on Wiener filtering29, which minimizes the 
mean square error between the reconstruct volume and the ground 
truth. As α approach infinity, the formulation above becomes an 
inverse filter (that is, the filter apply in the frequency domain be /�H1 ). 
Similarly, the Fourier-domain filter in equation (3) could be replace 
by ⁎�H to implement a backprojection reconstruction procedure. 

FWHM = 60 p 

Time (ns) 
0 1 2 3 4 5 6 7 8 

108 

N 
um 

b 
er 

o 
f p 

ho 
to 

n 
d 

et 
ec 

te 
d 

Occluder 

Hidden 
object 

Visible 
wall 

Scanning 
galvanometer 

Beam splitter SPAD sensor 

Picosecond laser 

Lens 

0 

108 

N 
um 

b 
er 

o 
fp 

ho 
to 

n 
d 

et 
ec 

te 
d 

Occluder 

Hidden 
object 

Visible 
wall 

Scanning 
galvanometer 

Beam splitter SPAD senso 

Picose 

Lens 

a b 

c 

y ′ 

x ′ 

106 

104 

102 

x ′ 
t 

Figure 1 | Overview of confocal image hardware and measurements. 
a, A pulse laser and time-resolved detector raster-scan a wall to record 
both the direct light reflect off the wall and the indirect light from a 
hidden object. b, A histogram measure at a scan point on the visible 
wall indicates the temporal precision of the detector. In this experiment, 
the hidden object be a 5 cm × 5 cm square make from retroreflective tape. 
The detection time of the indirect signal (t = 4.27 ns) relative to the direct 

signal (t = 0 ns) corresponds to twice the distance of the hidden object 
from the scan point (r = 0.64 m). FWHM, full-width at half-maximum. 
c, Scanning a sequence of point along the wall produce a ‘streak image’ 
that capture the spatio-temporal geometry of indirect light transport. 
Each column in this image represent the histogram measure at a discrete 
point (x′ ,0) on a wall and contains the indirect light from the hidden 
square. 

a 

x′ t 

y ′ 

b 

x′ v 

y ′ 

c 

x u 

y 

d 

x z 

y 

Figure 2 | Overview of the reconstruction procedure. The confocal 
measurement of the wall τ (a) be resampled and attenuate along the 
time axis, yield Rtτ (b). These measurement be then convolve with 

a Wiener filter to produce the volume Rzρ* (c), and the result be resampled 
and attenuate along the depth dimension to produce the hidden volume 
ρ* (d). Bunny model from the Stanford Computer Graphics Laboratory. 

© 2018 Macmillan Publishers Limited, part of Springer Nature. All right reserved. 



0 0 M o n t h 2 0 1 8 | V o L 0 0 0 | n A t U R E | 3 

Letter reSeArCH 

Wiener filter with a constant α inaccurately assumes that the trans- 
form measurement contain white noise. Therefore, we also derive 
an iterative reconstruction procedure that combine the LCT with a 
physically accurate Poisson noise model (Supplementary Information). 

Figure 2 illustrates the inverse LCT apply to indirect measurement 
of a bunny model simulated with a physically base ray tracer30. The 
process involves evaluate equation (3) in three steps: (i) resampling 
and attenuate the measurement τ with the transform Rt, (ii) apply 
the Wiener filter to the result, and (iii) apply the inverse transform 
−Rz 

1 to recover ρ. These three step be efficient in term of memory and 
number of operation required. The most costly step be the application 
of the Wiener filter, which require O(N3logN) operation for the 3D 
fast-Fourier transforms and have memory requirement of O(N3), where 
N be the maximum number of element across all dimension in space- 
time. In comparison, exist backprojection-type reconstructions15–17 
require O(N5) operations, and method base on inversion be much 
more cost ly b ot h in t he i r memor y and pro ce ing 
requirements17,18,20. 

In addition to improve runtime and memory efficiency, a primary 
benefit of the LCT over backprojection-based approach be that 
the invert solution be accurate. In Fig. 3, we compare the recon- 
struction quality of the backprojection algorithm and the LCT for 
a retroreflective traffic sign. The dimension of the hidden sign be 
0.61 m × 0.61 m and the diffuse wall be sample at 64 × 64 location 
over a 0.8 m × 0.8 m region. The total exposure time be 6.8 min (that 
is, 0.1 s per sample) and the runtime for MATLAB to recover a vol- 
ume of 64 × 64 × 512 voxels be 1 s on a MacBook Pro (3.1-GHz Intel 
Core i7). To compare the reconstruction quality of the two methods, 
we compute the backprojection result use the LCT, which be just a 

efficient a invert the problem with the LCT. Even though unfil- 
tered backprojection could be slightly sharpen by linear filters, such 
a a Laplacian15, backprojection method do not solve the inverse 
problem (see Supplementary Information for detailed comparisons). 
In Supplementary Information, we also show a variety of reconstruct 
example scenes, a well a result for NLOS tracking11–13 of retrore- 
flective object in real time. 

Applying NLOS image outdoors require the indirect light from 
the hidden object to be detect in the presence of strong ambient 
illumination. To accomplish this, C-NLOS image take advantage 
of the high light throughput associate with retroreflective objects. 
Figure 3 present an outdoor NLOS experiment under indirect sunlight 
(approximately 100 lx). The dimension of the hidden retroreflective 
object be 0.76 m × 0.51 m, with 32 × 32 sample location over a 
1 m × 1 m area. The exposure be 0.1 s per sample, with a total exposure 
time of 1.7 min. MATLAB reconstructs a volume of 32 × 32 × 1,024 
voxels in 0.5 s. 

The fundamental bound on the resolution of NLOS image 
approach couple the full-width at half-maximum of the temporal 
resolution of the image system, represent by the scalar γ, to the 
small resolvable axial Δ z and lateral Δ x spatial feature size a follow 

γ 
γ∆ ≥ ∆ ≥ 

+z c x c w z 
w2 

and 
2 

(4) 
2 2 

where 2w be the sample width or height of the visible wall (see 
Supplementary Information for details). 

To evaluate the limit of the reconstruction procedure, we simulate 
the acquisition of 1,024 × 1,024 point sample over a 1 m × 1 m area 

a 

x 
z 

y 

30 cm 

y 

x 

z 

x 

b 

x 
z 

y 

y 

x 

z 

x 

c 

x 
z 

y 

Hidden 
object Occluder 

Camera 
system 

30 cm 30 cm 30 cm 

Figure 3 | NLOS reconstruction from SPAD measurements. a, Result 
for a hidden ‘Exit’ sign, obtain use the backprojection method. 
b, Result of the propose LCT reconstruction procedure. c, The propose 
method can also reconstruct the shape and albedo of object outdoors, 

under indirect sunlight. The bottom right panel be a photograph of the 
experimental setup, which consists of a hidden ‘S’-shaped object, black 
cloth act a an occluder and the confocal scan prototype. 

a b c 

E 
rr 

or 

0 cm 

1 cm 

Figure 4 | Comparison between simulated C-NLOS reconstruction 
and ground-truth geometry. a, b, Rendered point cloud reconstruct 

with the LCT (green) over the ground-truth geometry (grey). c, Pointwise 
difference between the surface along the z axis. 

© 2018 Macmillan Publishers Limited, part of Springer Nature. All right reserved. 



4 | n A t U R E | V o L 0 0 0 | 0 0 M o n t h 2 0 1 8 

LetterreSeArCH 

and 1,024 time bin with a temporal resolution of 8 p per bin. We 
recover a volume contain 1,024 × 1,024 × 1,024 voxels. Figure 4 
show the target geometry in grey and the recover shape overlaid in 
green. The error map indicates a median absolute reconstruction error 
of 2.5 mm (mean absolute error 15.1 mm, mean square error 2.7 mm). 
Occlusions and higher-order bounce of indirect illumination be not 
model by any exist NLOS image method, include ours, which 
may lead to violation in the image formation model and error in 
the reconstruct volume. For example, the right ear of the bunny be 
not accurately recover owe to self-occlusions by the left ear in the 
measurements. We note that the conventional approach of discretizing 
and invert the image formation model at this resolution would 
require an excess of 9 petabyte of memory just to store a sparse 
representation of the linear system. 

The co-design of a confocal scan technique and a computa- 
tionally efficient inverse method facilitates fast, high-quality recon- 
structions of hidden objects. To achieve real-time frame rate with 
C-NLOS imaging, three improvement to our current prototype be 
required. First, to reduce acquisition time, a more powerful laser be 
needed. For eye-safe operation, this laser may need to operate in the 
short-wave infrared regime11,12,22. Second, for retroreflective objects, 
the measurement of multiple histogram can be perform in parallel, 
with minimal crosstalk. This property could enable a single-photon 
avalanche diode (SPAD) array and a diffuse laser source to acquire the 
full C-NLOS image in a single shot. Third, to improve the computation 
time, our highly parallelizable algorithm could be implement in a 
graphic processing unit or a field-programmable gate array. 

The propose technique thus enables NLOS image with con- 
ventional hardware at much high speeds, with a small memory 
footprint and low power consumption, over a longer range, under 
ambient light and at high resolution than any exist approach 
of which we be aware. 

Data Availability The measure C-NLOS data and the LCT code support the 
finding of this study be available in the Supplementary Information. Additional 
data and code be available from the correspond author upon request. 

receive 29 August 2017; accepted 3 January 2018. 

Published online 5 March 2018. 

1. Freund, I. Looking through wall and around corners. Physica A 168, 49–65 
(1990). 

2. Wang, L., Ho, P. P., Liu, C., Zhang, G. & Alfano, R. R. Ballistic 2-D image 
through scatter wall use an ultrafast optical Kerr gate. Science 253, 
769–771 (1991). 

3. Huang, D. et al. Optical coherence tomography. Science 254, 1178–1181 
(1991). 

4. Bertolotti, J. et al. Non-invasive image through opaque scatter layers. 
Nature 491, 232–234 (2012). 

5. Katz, O., Heidmann, P., Fink, M. & Gigan, S. Non-invasive single-shot image 
through scatter layer and around corner via speckle correlations. 
Nat. Photon. 8, 784–790 (2014). 

6. Katz, O., Small, E. & Silberberg, Y. Looking around corner and through thin 
turbid layer in real time with scatter incoherent light. Nat. Photon. 6, 
549–553 (2012). 

7. Strekalov, D. V., Sergienko, A. V., Klyshko, D. N. & Shih, Y. H. Observation of 
two-photon “ghost” interference and diffraction. Phys. Rev. Lett. 74, 
3600–3603 (1995). 

8. Bennink, R. S., Bentley, S. J. & Boyd, R. W. “Two-photon” coincidence image 
with a classical source. Phys. Rev. Lett. 89, 113601 (2002). 

9. Sen, P. et al. Dual photography. ACM Trans. Graph. 24, 745–755 (2005). 
10. Bouman, K. L. et al. In IEEE 16th Int. Conference on Computer Vision 2270–2278 

(IEEE, 2017); http://openaccess.thecvf.com/content_iccv_2017/html/ 
Bouman_Turning_Corners_Into_ICCV_2017_paper.html. 

11. Klein, J., Peters, C., Martin, J., Laurenzis, M. & Hullin, M. B. Tracking object 
outside the line of sight use 2D intensity images. Sci. Rep. 6, 32491 
(2016). 

12. Chan, S., Warburton, R. E., Gariepy, G., Leach, J. & Faccio, D. Non-line-of-sight 
track of people at long range. Opt. Express 25, 10109–10117 (2017). 

13. Gariepy, G., Tonolini, F., Henderson, R., Leach, J. & Faccio, D. Detection and 
track of move object hidden from view. Nat. Photon. 10, 23–26 
(2016). 

14. Kirmani, A., Hutchison, T., Davis, J. & Raskar, R. In IEEE 12th Int. Conference on 
Computer Vision 159–166 (IEEE, 2009); http://ieeexplore.ieee.org/ 
document/5459160/. 

15. Velten, A. et al. Recovering three-dimensional shape around a corner use 
ultrafast time-of-flight imaging. Nat. Commun. 3, 745 (2012). 

16. Buttafava, M., Zeman, J., Tosi, A., Eliceiri, K. & Velten, A. Non-line-of-sight 
image use a time-gated single photon avalanche diode. Opt. Express 23, 
20997–21011 (2015). 

17. Gupta, O., Willwacher, T., Velten, A., Veeraraghavan, A. & Raskar, R. 
Reconstruction of hidden 3D shape use diffuse reflections. Opt. Express 20, 
19096–19108 (2012). 

18. Wu, D. et al. In Computer Vision – ECCV 2012 (eds Fitzgibbon, A., et al.) 
542–555 (Springer, 2012); https://link.springer.com/chapter/10.1007/ 
978-3-642-33718-5_39. 

19. Tsai, C.-Y., Kutulakos, K. N., Narasimhan, S. G. & Sankaranarayanan, A. C. In 
Proc. IEEE Conference on Computer Vision and Pattern Recognition 7216–7224 
(IEEE, 2017); http://openaccess.thecvf.com/content_cvpr_2017/html/Tsai_ 
The_Geometry_of_CVPR_2017_paper.html. 

20. Heide, F., Xiao, L., Heidrich, W. & Hullin, M. B. In Proc. IEEE Conference on 
Computer Vision and Pattern Recognition 3222–3229 (IEEE, 2014); 
http://openaccess.thecvf.com/content_cvpr_2014/html/Heide_Diffuse_ 
Mirrors_3D_2014_CVPR_paper.html. 

21. Schwarz, B. LIDAR: mapping the world in 3D. Nat. Photon. 4, 429–430 
(2010). 

22. McCarthy, A. et al. Kilometer-range, high resolution depth image via 1560 
nm wavelength single-photon detection. Opt. Express 21, 8904–8915 (2013). 

23. Kirmani, A. et al. First-photon imaging. Science 343, 58–61 (2014). 
24. Shin, D. et al. Photon-efficient image with a single-photon camera. 

Nat. Commun. 7, 12046 (2016). 
25. Abramson, N. Light-in-flight record by holography. Opt. Lett. 3, 121–123 

(1978). 
26. Velten, A. et al. Femto-photography: capture and visualize the propagation 

of light. ACM Trans. Graph. 32, 44 (2013). 
27. O’Toole, M. et al. In Proc. IEEE Conference on Computer Vision and Pattern 

Recognition 2289–2297 (IEEE, 2017); http://openaccess.thecvf.com/content_ 
cvpr_2017/html/OToole_Reconstructing_Transient_Images_CVPR_2017_ 
paper.html. 

28. Minkowski, H. Raum und zeit. Phys. Z. 10, 104–111 (1909). 
29. Wiener, N. Extrapolation, Interpolation, and Smoothing of Stationary Time Series 

Vol. 7 (MIT Press, 1949). 
30. Pharr, M., Jakob, W. & Humphreys, G. Physically Based Rendering: From Theory 

to Implementation 3rd edn (Morgan Kaufmann, 2017). 

Supplementary Information be available in the online version of the paper. 

Acknowledgements We thank K. Zang for his expertise and advice on the 
SPAD sensor. We also thank B. A. Wandell, J. Chang, I. Kauvar, N. Padmanaban 
for review the manuscript. M.O’T. be support by the Government of 
Canada through the Banting Postdoctoral Fellowships programme. D.B.L. be 
support by a Stanford Graduate Fellowship in Science and Engineering. G.W. 
be support by a National Science Foundation CAREER award (IIS 1553333), 
a Terman Faculty Fellowship and by the KAUST Office of Sponsored Research 
through the Visual Computing Center CCF grant. 

Author Contributions M.O’T. conceive the method, developed the 
experimental setup, perform the indoor measurement and implement 
the LCT reconstruction procedure. M.O’T. and D.B.L. perform the outdoor 
measurements. D.B.L. apply the iterative LCT reconstruction procedure 
show in Supplementary Information. G.W. supervise all aspect of the project. 
All author take part in design the experiment and write the paper and 
Supplementary Information. 

Author Information Reprints and permission information be available at 
www.nature.com/reprints. The author declare no compete financial 
interests. Readers be welcome to comment on the online version of the paper. 
Publisher’s note: Springer Nature remains neutral with regard to jurisdictional 
claim in publish map and institutional affiliations. Correspondence and 
request for material should be address to M.O’T. (matthew.otoole@gmail. 
com) and G.W. (gordon.wetzstein@stanford.edu). 

reviewer Information Nature thanks D. Faccio, V. Goyal and M. Laurenzis for 
their contribution to the peer review of this work. 

© 2018 Macmillan Publishers Limited, part of Springer Nature. All right reserved. 

http://openaccess.thecvf.com/content_iccv_2017/html/Bouman_Turning_Corners_Into_ICCV_2017_paper.html 
http://openaccess.thecvf.com/content_iccv_2017/html/Bouman_Turning_Corners_Into_ICCV_2017_paper.html 
http://ieeexplore.ieee.org/document/5459160/ 
http://ieeexplore.ieee.org/document/5459160/ 
https://link.springer.com/chapter/10.1007/978-3-642-33718-5_39 
https://link.springer.com/chapter/10.1007/978-3-642-33718-5_39 
http://openaccess.thecvf.com/content_cvpr_2017/html/Tsai_The_Geometry_of_CVPR_2017_paper.html 
http://openaccess.thecvf.com/content_cvpr_2017/html/Tsai_The_Geometry_of_CVPR_2017_paper.html 
http://openaccess.thecvf.com/content_cvpr_2014/html/Heide_Diffuse_Mirrors_3D_2014_CVPR_paper.html 
http://openaccess.thecvf.com/content_cvpr_2014/html/Heide_Diffuse_Mirrors_3D_2014_CVPR_paper.html 
http://openaccess.thecvf.com/content_cvpr_2014/html/Heide_Diffuse_Mirrors_3D_2014_CVPR_paper.html 
http://openaccess.thecvf.com/content_cvpr_2017/html/OToole_Reconstructing_Transient_Images_CVPR_2017_paper.html 
http://openaccess.thecvf.com/content_cvpr_2017/html/OToole_Reconstructing_Transient_Images_CVPR_2017_paper.html 
http://openaccess.thecvf.com/content_cvpr_2017/html/OToole_Reconstructing_Transient_Images_CVPR_2017_paper.html 
http://www.nature.com/doifinder/10.1038/nature25489 
http://www.nature.com/reprints 
http://www.nature.com/doifinder/10.1038/nature25489 
mailto:matthew.otoole@gmail.com 
mailto:matthew.otoole@gmail.com 
mailto:gordon.wetzstein@stanford.edu 

Confocal non-line-of-sight image base on the light-cone transform 
Authors 
Abstract 
Data Availability 
References 
Acknowledgements 
Author Contributions 
﻿Figure 1﻿﻿ Overview of confocal image hardware and measurements. 
﻿Figure 2﻿﻿ Overview of the reconstruction procedure. 
﻿Figure 3﻿﻿ NLOS reconstruction from SPAD measurements. 
﻿Figure 4﻿﻿ Comparison between simulated C-NLOS reconstruction and ground-truth geometry. 




