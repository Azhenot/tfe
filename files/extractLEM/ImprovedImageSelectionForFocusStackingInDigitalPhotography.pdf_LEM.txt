









































IMPROVED IMAGE SELECTION FOR FOCUS STACKING IN DIGITAL PHOTOGRAPHY 

David Choi, Aliya Pazylbekova, Wuhan Zhou, and Peter van Beek 

Cheriton School of Computer Science, University of Waterloo, Canada 

ABSTRACT 

Focus stacking, or all-in-focus imaging, be a technique for 
achieve large depth of field in an image by fuse image 
acquire at different focus distances. Minimizing the set 
of image to fuse, while ensure that the result fuse im- 
age be all-in-focus, be important in order to avoid long image 
acquisition and post-processing times. Recently, an end-to- 
end system for focus stack have be propose that auto- 
matically selects image to acquire. The system be adaptive to 
the scene be image and show excellent performance on 
a mobile device, where the lens have a short focal length and 
fix aperture, and few image need to be selected. However, 
with longer focal lengths, variable apertures, and more se- 
lected image (as exists with other cameras, notably DSLRs), 
classification and algorithmic inaccuracy become apparent. 
In this paper, we propose improvement to previous work that 
remove these limitations, and show on eight real scene that 
overall our technique lead to improve accuracy while re- 
ducing the number of require images. 

Index Terms— Focus stacking, increase depth of field, 
computational photography 

1. INTRODUCTION 

Focus stack combine several image capture at differ- 
ent focus distance into a single image to produce a large 
depth of field. It be useful when the camera be unable to ac- 
quire an all-in-focus image or when the quality of an all-in- 
focus image would be degrade because the narrow aperture 
result in a shutter speed too slow to freeze motion. The mo- 
tivations for obtain an all-in-focus image range from the 
aesthetic to the practical: architectural, interior, and macro 
photography, a well a pattern recognition and object detec- 
tion [2, 3]. 

An essential part of focus stack be select the set of 
image to be fused. The set must be small, in order to de- 
crease capture and fusion times, but must result in an all-in- 
focus image. Time between image capture can be on the 
order of seconds, so even gradual motion can impact quality. 
However, in contrast to a wide literature on combine a set 
of image into a single image (see, e.g., [4–6]), image selec- 
tion have not receive much attention. The simple approach of 
move the lens a uniform step-size and acquire an image at 

each step lead to set which contain image with nothing in 
focus or redundant images. Hasinoff et al. [7–9] consider 
the problem of quickly select the set of image to cover a 
give depth of field. However, their analysis neglect camera 
overhead, image post-processing, and range without objects. 

Vaquero et al. [1] present an end-to-end system that 
adaptively selects a minimal set of high-resolution image to 
acquire by processing a stream of low-resolution one (which 
can be acquire quickly). It have the camera display a final all- 
in-focus image, allow the photographer to verify the final 
result in the field (see Fig. 1). Their system show excellent 
performance on a mobile device, where the lens have a short 
focal length and a fix aperture. 

Unfortunately, a we show, their technique which work 
well for mobile device do not necessarily generalize well to 
DSLRs, which feature lens with longer focal length and 
variable apertures, and so require many more image for fo- 
cu stacking. We propose improvement to the work of Va- 
quero et al. [1] which afford increase performance on non- 
mobile devices. Our improvement make use of shape from 
focus technique (see, e.g., [2, 10, 11]), supervise machine 
learn technique (see, e.g., [12,13]), and standard depth of 
field equation to improve on previous inefficiencies. Empir- 
ically, on eight real scene and various aperture settings, our 
technique lead to an overall improve accuracy while signif- 
icantly reduce the cardinality of the select set of images. 

2. OUR PROPOSALS 

We first summarize Vaquero et al.’s [1] system (see Fig. 1), 
then describe our propose improvements. 

Vaquero et al.’s [1] approach proceeds a follows: 
Step 1. Capture a stack of low-resolution image p = 
0, . . . , n − 1 by sweep the lens slowly enough that the 
depth of field for adjacent image overlap. 
Step 2. Overlay a grid on each image and calculate a focus 
measure φi,j(p) for each cell (i, j) in the grid for each image 
(see Fig. 2). A focus measure map an image to a value that 
represent it degree of focus (see, e.g., [14–16]). Let f(x, y) 
be the luminance at pixel (x, y) in an image. Here, the focus 
measure for a cell of size w × h pixel be give by: 

φi,j(p) = 

h−1∑ 
x=0 

w−2∑ 
y=1 

| −f(x, y−1)+2f(x, y)−f(x, y+1) | . 



Fig. 1. Pipeline for Vaquero et al.’s [1] end-to-end system for image set selection and fusion for an all-in-focus image. 

Then, classify each cell (i, j) in the grid a foreground iff the 
standard deviation of it focus measure across all image be 
above a give threshold t1 (see Alg. 1, Line 4). Ignore back- 
ground cell in Step 3. 

0 

0.1 

0.2 

0.3 

image 

fo 
cu 

s 
m 

ea 
su 

re 

0.0 

1.0 

2.0 

3.0 

4.0·10 
−2 

image 

fo 
cu 

s 
m 

ea 
su 

re 

Fig. 2. Focus measure construct for two cell from the 
example scene show in Fig. 1; (left) cell with a well-defined 
peak; (right) cell where reliability of the peak be less clear. 

Step 3. A key insight of Vaquero et al. [1] be that the prob- 
lem of select the final set of image to fuse into a single 
all-in-focus image can be mapped to a set cover problem. 
Let A be an m × n (0-1)-matrix. A row i of A be cover 
by a column j if the correspond matrix entry aij be equal 
to one. The set cover problem be to find a subset of the 
column C ⊆ {1, . . . , n} that minimizes the cardinality of C 
such that every row be covered; i.e., for every i ∈ {1, . . . ,m} 
there exists a j ∈ C such that aij = 1. They setup a set cov- 
ering instance where the row be the foreground cells, the 
column represent the images, and an entry be 1 if and only 
if the cell be in-focus in that image (based on whether φi,j(p) 
be within some threshold t2 of the maximum value of that 
cell; see Lines 5–12 in Alg. 1). In general, solve set cov- 
ering be NP-hard [17], however, due to the consecutive one 
property present in focus stacking, selection can be compute 
linearly in the number of image [18]. 
Steps 4 & 5. Acquire high-resolution image at the specify 
lens position of the set cover instance and fuse them into 
a single all-in-focus image. 

The issue arises in Steps 2 & 3 of their pipeline, where 
due to hand-crafted heuristic use t1 and t2, some cell be 
incorrectly classify into foreground, background, in-focus, 
and out-of focus, and slightly incorrect peak be found. We 
now describe improvement to address these problems. 

Algorithm 1: Vaquero et al. [1] hand-crafted heuristic. 
input : Focus measure φi,j(p), for each cell (i, j) and 

p = 0, . . . , n− 1; threshold t1 and t2 
output: Set cover instance a an m× n (0,1)-matrix 

A = [ak,p] 
1 k ← 0; 
2 foreach cell (i, j) do 
3 σ = std{φi,j(p) | p = 0, . . . , n− 1}; 
4 if σ > t1 then 
5 ak,p ← 0, p = 0, . . . , n− 1; 
6 M = argmax 

p=0,...,n−1 
{φi,j(p)}; 

7 ak,M ← 1; 
8 p←M − 1; 
9 while φi,j(M)− φi,j(p) < t2 and ak,p+1 = 1 

do ak,p ← 1; p← p− 1; ; 
10 p←M + 1; 
11 while φi,j(M)− φi,j(p) < t2 and ak,p−1 = 1 

do ak,p ← 1; p← p+ 1; ; 
12 k ← k + 1; 

2.1. Constructing an explicit depth map 

Rather than classify a cell a foreground or background (Step 
2, Line 4 in Alg. 1) we construct an explicit depth map us- 
ing shape from focus technique [2, 10, 11] and use super- 
vised machine learn to construct a classifier that predicts 
whether a depth estimation be reliable or unreliable. An exam- 
ple of an unreliable depth estimate be a plain white wall that 
lack contrast or texture. Note that Vaquero et al. [1] implic- 
itly construct a depth map and classify estimate (into what 
they call foreground and background) by use the standard 
deviation of focus measure for a cell. 

To construct a depth map, we use the standard method 
where the lens position of the focus measure peak in a cell 
across all image be the estimate of the depth of the scene 
at that cell. The map be improve by smooth the fo- 
cu measure for a cell to reduce depth estimate noise, and 
find the peak of the smooth focus measures. Smoothing 



consist of sum the measure of the cell under consid- 
eration and eight adjacent cell (or few at boundaries). 

We construct the classifier for depth estimation relia- 
bility by training a decision tree [19] base on 60 feature of 
each cell, one of which be the standard deviation use by 
Vaquero et al. [1], to create a more robust classification. 

2.2. Classifying in-focus and out-of-focus 

Once the depth estimate for a cell have be compute and 
classify a reliable, the next step be to determine which lens 
position around the peak be in acceptable focus. Vaquero 
et al. [1] use a simple heuristic where consecutive lens po- 
sitions whose focus measure be within some tolerance t2 
of the peak be deem to be in acceptable focus (Step 3, 
Lines 9 & 11 in Alg. 1). However, while intuitive, this heuris- 
tic relies on two assumption that do not hold in general. 

First, the heuristic assumes that a focus measure close to 
the peak in absolute term be also in acceptable focus. How- 
ever, focus curve often have distinguish peak but small 
absolute heights. In these cases, reasonable tolerance value 
inaccurately deem most or all of the lens position a in-focus. 
Second, the heuristic assumes that the aperture at which low- 
resolution image be acquire from the live preview stream be 
the same a the aperture at which high-resolution image will 
be acquired. However, to improve the accuracy of focus 
and to maintain a fast shutter speed (approximately twice the 
video frame rate), in live preview mode the camera open the 
aperture a wide a possible give the brightness of the scene. 
Typically, this can be a different a a wide aperture of f/1.4 
versus a narrower aperture of f/8.0, and any depth of field 
estimate from the wide aperture would not be accurate for the 
narrower aperture. 

Rather than estimate depth of field from the focus mea- 
sures we propose to instead use standard depth of field equa- 
tions to predict in-focus and out-of-focus, 

h = 
f2 

ac 
+f, dnear = 

d(h− f) 
h+ d− 2f 

, dfar = 
d(h− f) 
h− d 

, 

where a be the aperture, c be the circle of confusion, d be the 
distance to the subject, f be the focal length of the lens, h be the 
hyperfocal distance, and all calculation be in millimeters. 
The circle of confusion be the diameter of the large blur spot 
indistinguishable from a focus point source of light [20], 
and have establish value for most contexts. 

After compute a distance interval [dnear , dfar ], repre- 
senting a depth of field in millimeters, that would lead to in- 
focus objects, one must compute an acceptable lens interval, 
represent start and end lens positions. This be accu- 
rately do by determine which lens position correspond 
to the distance marking on a lens, and interpolate between 
these lens position to determine the remain ones. Interpo- 
lation us the difference in reciprocal of the distance time 
the proportion of the distance between lens positions. 

2.3. Robust selection of image via set cover 

To improve image selection (Step 3) robustness, we use the 
explicitly construct depth map to augment the set cover- 
ing instance a follows. Let L be the set of peak classify 
a reliable in the depth map. For each consecutive sequence 
of lens position pi, pi+1, . . . , pi+k in L, such that pi−1 and 
pi+k+1 do not occur in L, the lens position pi−1 and pi+k+1 
be add to L. The augment set L be then use to construct 
the set cover instance, where each element of L be a row in 
the set cover matrix. This augmentation smooth the dis- 
crete nature of divide an image into a grid, which address 
problem like dramatic depth change between adjacent cell 
of continuous object (such a a book angle sharply away). 
Smoothing make image selection more accurate while occa- 
sionally modestly increase the number of image selected. 

3. EXPERIMENTAL EVALUATION 

In this section, we perform a comparative evaluation of our 
improvement with the baseline Vaquero et al. [1] approach1. 

3.1. Experimental methodology 

Image sets. We acquire eight benchmark image set use a 
camera remote control application we implemented. A Canon 
EOS 550D/Rebel T2i camera be tether to a computer via 
a USB cable and control by software, which make use of 
the Canon SDK (Version 2.11). The Canon SDK do not 
expose functionality for sweep the lens (Step 1, Fig. 1) so 
we simulated the effect by step the lens through the pos- 
sible lens position and acquire a 1056×704 low resolution 
image from the live preview stream at each step. For evalua- 
tion, 5184× 3456 high resolution image be also acquire 
at each lens position. 

Decision tree training. We construct label training 
data by consensus for the depth estimation reliability deci- 
sion tree classifier by overlay a grid on the low resolu- 
tion image and visually inspect each cell to determine 
the peak focus position (or no valid peak if the cell lack 
contrast or have multiple peaks, a would occur with a blank 
wall or multiple occlude objects). The decision tree itself 
consider about 60 feature base on property of the fo- 
cu measure curve and depth maps. One feature—kurtosis, 
a statistical measure base on the fourth moment of the fo- 
cu measures—was the most predictive feature by far, clearly 
dominate standard deviation use by Vaquero et al.’s [1] ap- 
proach. To learn the decision tree, we use Weka’s J48 [21]. 
We experiment with parameter that lead to complex trees, 
but reasonable setting lead to tree with a single node: kurto- 
sis. For the experiment report here we favor simplicity 
at the expense of some accuracy. 

1The implementation and data be available at: https://cs. 
uwaterloo.ca/˜vanbeek. 



Table 1. Number of image select (m) and accuracy (acc.) of our method and Vaquero et al.’s [1] method compare to the 
minimum possible number of image need (gold), for various benchmarks, grid sizes, and apertures. The coin and flower 
benchmark be acquire with a 200mm lens; the remain benchmark be acquire with a 50mm lens. 

wide Our Vaquero narrow Our Vaquero 
grid size benchmark aper. gold m acc. m acc. aper. gold m acc. m acc. 

backyard f/1.4 31 30 97.5 21 80.9 f/8.0 5 5 99.2 11 100.0 
bar f/1.4 12 13 98.5 8 80.0 f/8.0 2 3 100.0 4 88.3 
book f/1.4 72 81 96.0 34 55.7 f/8.0 11 11 99.0 16 89.8 
building f/2.0 14 14 100.0 16 100.0 f/8.0 2 2 100.0 7 100.0 

16× 24 can f/1.4 19 34 97.1 5 16.9 f/8.0 5 5 99.5 4 67.7 
coin f/2.8 16 18 100.0 16 93.6 f/8.0 16 18 100.0 9 59.2 
flower f/2.8 30 33 71.7 22 38.6 f/8.0 14 16 89.8 10 48.6 
trail f/4.0 5 5 100.0 10 100.0 f/8.0 3 3 99.1 4 100.0 
average 24.9 28.5 95.1 16.5 70.7 7.4 7.9 98.0 8.1 81.7 
backyard f/1.4 33 35 100.0 37 99.7 f/8.0 5 5 100.0 31 100.0 
bar f/1.4 12 18 100.0 44 100.0 f/8.0 2 3 100.0 20 100.0 
book f/1.4 87 95 100.0 76 90.8 f/8.0 11 12 100.0 41 100.0 
building f/2.0 15 15 100.0 16 100.0 f/8.0 3 3 100.0 16 100.0 

32× 48 can f/1.4 18 34 100.0 84 100.0 f/8.0 4 5 100.0 9 100.0 
coin f/2.8 16 18 100.0 26 100.0 f/8.0 16 18 100.0 18 100.0 
flower f/2.8 35 41 91.2 163 79.3 f/8.0 15 19 96.1 96 93.4 
trail f/4.0 5 6 100.0 21 100.0 f/8.0 3 3 99.6 21 100.0 
average 27.6 32.8 98.9 58.4 96.2 7.4 8.5 99.5 31.5 99.2 

Parameter selection: t1 and t2. Vaquero et al.’s approach 
require setting for the threshold t1 and t2 (Lines 4, 9 & 11 
in Alg. 1). For a fair comparison, we choose the optimal val- 
ues. Threshold t1 be set to the value that best fit all the above 
training data. Threshold t2 be set by iteratively run our 
evaluation search for the optimal accuracy or, within accu- 
racy, the low number of images. 

Performance evaluation. We compare the approach us- 
ing two performance measures: (i) number of image select 
and (ii) accuracy a measure by the percentage of cell in a 
grid that be in focus. To compare against the minimal num- 
ber of image need and to determine the accuracy of the two 
approaches, we construct a gold standard depth map for a 
scene use the set of high resolution image for the scene. 
We use an adaptation of 8-fold cross-validation to obtain re- 
liable estimate of the performance of our approach (see [22], 
pp. 161-205), where for each of the eight benchmark in turn, 
we train on the other seven and test on that benchmark. 

Fig. 3. All-in-focus image obtain by fuse select high 
resolution image use f/8.0 aperture and 32 × 48 grid. 

3.2. Experimental result 

Table 1 summarizes the result of empirical evaluation. On 
these benchmarks, our method be more accurate than Vaquero 
et al.’s [1] for the coarser grid, and have comparable accuracy 
with many few image on the finer grid. For the most im- 
portant case, a 32 × 48 grid and narrow aperture, both meth- 
od have excellent accuracy. However, our approach be close 
to the minimum possible number of image and a significant 
reduction over the number of image select by Vaquero et 
al.’s [1] method with an average of 4.5 time few images, 
which would significantly reduce image acquisition time (the 
time between image capture often exceeds two seconds). Se- 
lection algorithm run time remain negligible. We also 
compare the post-processing image fusion times. In Photo- 
shop CS5 our improvement reduce post-processing time 
by up to ten times. For example, for a 32× 48 grid and a nar- 
row aperture, the time (mm:ss) for fuse the image select 
use our improvement range from 0:30 to 3:00 compare 
to 1:30 to 31:00 for the image select by Vaquero et al.’s [1] 
approach. 

4. CONCLUSION 

We propose enhancement to a proposal by Vaquero et al. [1] 
that improves their image selection on camera with variable 
aperture and lens with longer focal lengths. Our approach 
maintains equivalent or good accuracy, while significantly re- 
ducing the cardinality of the select set of images. 



5. REFERENCES 

[1] D. Vaquero, N. Gelfand, M. Tico, K. Pulli, and M. Turk, 
“Generalized autofocus,” in Proceedings of the IEEE 
Workshop on Applications of Computer Vision, 2011. 

[2] P. Grossman, “Depth from focus,” Pattern Recognition 
Letters, vol. 5, pp. 63–69, 1987. 

[3] J. Gulbins and R. Gulbins, Photographic Multishot 
Techniques: High Dynamic Range, Super-Resolution, 
Extended Depth of Field, Stitching, Rocky Nook, 2009. 

[4] T. Mertens, J. Kautz, and F. V. Reeth, “Exposure fu- 
sion,” in Proc. of Pacific Graphics, 2007. 

[5] W. B. Seales and S. Dutta, “Everywhere-in-focus im- 
age fusion use controlable cameras,” in Proceedings 
of SPIE 2905, Sensor Fusion and Distributed Robotic 
Agents, 1996, pp. 227–234. 

[6] C. Zhang, J. W. Bastian, C. Shen, A. van den Hengel, 
and T. Shen, “Extended depth-of-field via focus stack- 
ing and graph cuts,” in Proceedings of the IEEE In- 
ternational Conference on Image Processing, 2013, pp. 
1272–1276. 

[7] S. W. Hasinoff and K. N. Kutulakos, “Light-efficient 
photography,” IEEE Trans. Pattern Analysis and Ma- 
chine Intelligence, vol. 33, pp. 2203–2214, 2011. 

[8] S. W. Hasinoff, K. N. Kutulakos, F. Durand, and W. T. 
Freeman, “Time-constrained photography,” in Proceed- 
ings of the IEEE International Conference on Computer 
Vision, 2009, pp. 333–340. 

[9] K. N. Kutulakos and S. W. Hasinoff, “Focal stack pho- 
tography: High-performance photography with a con- 
ventional camera,” in Proceedings of the Eleventh IAPR 
Conference on Machine Vision Applications, 2009, pp. 
332–337. 

[10] S. K. Nayar and Y. Nakagawa, “Shape from focus,” 
IEEE Trans. Pattern Anal. Mach. Intell., vol. 16, pp. 
824–831, 1994. 

[11] S.-O. Shim and T.-S. Choi, “A novel iterative shape from 
focus algorithm base on combinatorial optimization,” 
Pattern Recognition, vol. 43, no. 10, pp. 3338–3347, 
2010. 

[12] I. H. Witten, E. Frank, and M. A. Hall, Data Mining, 
Morgan Kaufmann, 3rd edition, 2011. 

[13] T. Hastie, R. Tibshirani, and J. Friedman, The Elements 
of Statistical Learning: Data mining, Inference and Pre- 
diction, Springer, 2nd edition, 2009. 

[14] F. C. A. Groen, I. T. Young, and G. Ligthart, “A com- 
parison of different focus function for use in autofocus 
algorithms,” Cytometry, vol. 6, pp. 81–91, 1985. 

[15] M. Subbarao and J.-K. Tyan, “Selecting the optimal 
focus measure for autofocusing and depth-from-focus,” 
IEEE Trans. Pattern Anal. Mach. Intell., vol. 20, pp. 
864–870, 1998. 

[16] H. Mir, P. Xu, and P. van Beek, “An extensive empirical 
evaluation of focus measure for digital photography,” 
in Proc. SPIE 9023, Digital Photography X, 2014. 

[17] M. R. Garey and D. S. Johnson, Computers and In- 
tractability: A Guide to the Theory of NP-Completeness, 
W. H. Freeman, 1979. 

[18] G. L. Nemhauser and L. A. Wolsey, Integer and Combi- 
natorial Optimization, Wiley, 1988. 

[19] J. R. Quinlan, C4.5: Programs for Machine Learning, 
Morgan Kaufmann, 1993. 

[20] C. S. Johnson, Jr., Science for the Curious Photogra- 
pher, A K Peters, Ltd., 2010. 

[21] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute- 
mann, and I. H. Witten, “The WEKA data mining soft- 
ware: An update,” SIGKDD Explorations, vol. 11, 2009. 

[22] N. Japkowicz and M. Shah, Evaluating Learning Algo- 
rithms: A Classification Perspective, Cambridge Uni- 
versity Press, 2011. 


