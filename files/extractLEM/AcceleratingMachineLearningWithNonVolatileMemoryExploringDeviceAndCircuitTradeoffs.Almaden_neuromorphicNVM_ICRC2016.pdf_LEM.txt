









































Accelerating Machine Learning with Non-Volatile 
Memory: explore device and circuit tradeoff 

Alessandro Fumarola∗‡, Pritish Narayanan∗, Lucas L. Sanches∗, Severin Sidler∗‡, Junwoo Jang∗†, 
Kibong Moon†, Robert M. Shelby∗, Hyunsang Hwang†, and Geoffrey W. Burr∗ 

∗IBM Research–Almaden, 650 Harry Road, San Jose, CA 95120, Tel: (408) 927–1512, Email: gwburr@us.ibm.com 
†Department of Material Science and Engineering, Pohang University of Science and Technology, Pohang 790-784, Korea 

‡EPFL, Lausanne, CH–1015 Switzerland 

Abstract—Large array of the same nonvolatile memory 
(NVM) be developed for Storage-Class Memory (SCM) – 
such a Phase Change Memory (PCM) and Resistance RAM 
(ReRAM) – can also be use in non-Von Neumann neuromor- 
phic computational schemes, with device conductance serve 
a synaptic “weight.” This allows the all-important multiply- 
accumulate operation within these algorithm to be perform 
efficiently at the weight data. 

In contrast to other group work on Spike-Timing De- 
pendent Plasticity (STDP), we have be explore the use of 
NVM and other inherently-analog device for Artificial Neural 
Networks (ANN) train with the backpropagation algorithm. 
We recently show a large-scale (165,000 two-PCM synapses) 
hardware-software demo (IEDM 2014, [1], [2]) and analyze the 
potential speed and power advantage over GPU-based training 
(IEDM 2015, [3]). 

In this paper, we extend this work in several useful directions. 
We ass the impact of undesired, time-varying conductance 
change, include drift in PCM and leakage of analog CMOS ca- 
pacitors. We investigate the use of non-filamentary, bidirectional 
ReRAM device base on PrCaMnO, with an eye to develop- 
ing material variant that provide suitably linear conductance 
change. And finally, we explore tradeoff in design peripheral 
circuitry, balance simplicity and area-efficiency against the 
impact on ANN performance. 

I. INTRODUCTION 

By perform computation at the location of data, non-Von 
Neumann (non–VN) compute ought to provide significant 
power and speed benefit (Fig. 1) on specific and assumably 
important tasks. For one such non–VN approach — on-chip 

CPU Memory 

BUS 

Von Neumann 
“Bottleneck”(a) (b) 

Fig. 1. In the Von Neumann architecture (a), data (both operation and 
operands) must move to and from the dedicate Central Processing Unit (CPU) 
along a bus. In contrast, in a Non–Von Neumann architecture, distribute 
computation take place at the location of the data, reduce the time and 
energy spent move data around [1]. 

training of large-scale ANN use NVM-based synapsis [1]– 
[4] — viability will require several things. First, despite the 
inherent imperfection of NVM device such a Phase Change 
Memory (PCM) [1], [2] or Resistive RAM (RRAM) [4], such 
NVM-based network must achieve competitive performance 
level (e.g., classification accuracies) when compare to ANN 
train use CPUs or GPUs. Second, the benefit of perform- 
ing computation at the data (Fig. 2) must confer a decide 
advantage in either training power or speed (or preferably, 
both). And finally, any on-chip accelerator should be appli- 
cable towards network of different type (fully–connected 
“Deep” NN or Convolutional NN) and/or be reconfigurable 
for network of different shape (wide, with many neurons, or 
deep, with many layers). 

We briefly review our work [1]–[4] in assess the accu- 
racy, speed and power potential of on-chip NVM–based ML. 

A. Comparative analysis of speed and power 

We have previously assess the potential advantages, in 
term of speed and power, of on-chip machine learn (ML) 
of large-scale artificial neural network (ANN) use Non- 
Volatile Memory (NVM)-based synapses, in comparison to 
conventional GPU–based hardware [3]. 

Under moderately-aggressive assumption for parallel–read 
and –write speed, PCM-based on-chip machine learn can 

Selector deviceSynaptic weight 

NVMN1 

N2 

N1 

M1 

M2 
P 

P1 

O1 
pair 

Conductance 

1 

N2 

P2 

O2 

G+ G– 

Nn 

Oo 
+ ‐ + ‐ 

Nn Mm 
MmMm Pp 

M1 
+ 

Fig. 2. Neuro-inspired non-Von Neumann compute [1]–[4], in which 
neuron activate each other through dense network of programmable synaptic 
weights, can be implement use dense crossbar array of nonvolatile 
memory (NVM) and selector device-pairs [1]. 

978-1-5090-1370-8/16/$31.00 c© 2016 IEEE 



26.5x 

1.9x 
5.9x 8.2x 

20.3x 

0 15x 0 49x 0 89x 1.79x 
3.58xTrainingtime 0.15x 0.49x 0.89x 

1msec 
300usec 

time 

PCM (conservative) 
(per example) 

10usec 
30usec 
100usec GPU PCM (aggressive) 

#1 #2 #3 #4 #5Network: #1 #2 #3 #4 #5 
7 layer 7 layer 4 layer 7 layer 4 layer 

7.7e6 synapsis 36e6 synapsis 52e6 synapsis 450e6 synapsis 485e6 synapsis 
51 GB/sec (24%) 84 GB/sec (34%) 99 GB/sec (40%) 250 GB/sec (100%) 250 GB/sec (100%) 

Network: 

100W 
10W 
1W PCM ( i ) 

GPU 
768 GFLOPS (14%) 1136 GFLOPS (25%) 1447 GFLOPS (32%) 4,591 GFLOPS (100%) 4,591 GFLOPS (100%) 

34, 200x 13, 600x 5, 700x 890x2,850x 1,130x 620x 

1W 
100mW 
10mW 

Training 

PCM (aggressive) 
PCM (conservative) 

2 470x34, 200x 13, 600x 5, 700x 890x, 620x 220x 120x 
g 

power 2, 470x 

Fig. 3. Predicted training time (per ANN example) and power for 5 
ANNs, range from 0.2GB to nearly 6GB [3]. Under moderately-aggressive 
assumption for parallel–read and –write speed, PCM-based on-chip machine 
learn can offer low power and faster training for both large and small 
network [3]. 

250 125 10528 250 
hidden 
neuron 

125 
hidden 
neuron 

10 
output 
neuron 

“0” 

x1 
Cropped 
(22x24 

528 
input 

neuron 

A 

A 

x1 
B 

wij 

(22x24 
pixel) 
MNIST 
image 

“1” 

xi 
A 

xj 
B 

ij 

x wA 

xj 
“8” 

B 

xi wij 

xj =f(xi wij) 

xj 
“9” 

x528 

B A 
A 

x250 
B 

Fig. 4. In forward evaluation of a multilayer perceptron, each layer’s neuron 
drive the next layer through weight wij and a nonlinearity f(). Input neuron 
be driven by input (for instance, pixel from successive MNIST image 
(cropped to 22×24)); the 10 output neuron classify which digit be present 
[1]. 

potentially offer low power and faster training (per ANN 
example) than GPU-based training for both large and small 
network (Fig. 3), even with the time and energy require for 
occasional RESET (forced by the large asymmetry between 
gentle partial-SET and abrupt RESET in PCM). Critical here 
be the design of area-efficient read/write circuitry, so that many 
copy of this circuitry operate in parallel (each handle a 
small number of column (rows), cs). 

B. Potential for competitive classification accuracy 

Using 2 phase-change memory (PCM) device per synapse, 
we demonstrate a 3–layer perceptron (fully-connected ANN) 
with 164,885 synapsis [1], train with backpropagation [5] 
on a subset (5000 examples) of the MNIST database of 
handwritten digit [6] (Fig. 4), use a modify weight-update 
rule compatible with NVM+selector crossbar array [1]. We 
prove that this weight-update modification do not degrade 
the high “test” (generalization) accuracy such a 3–layer 
network inherently delivers on this problem when train in 
software [1]. However, nonlinearity and asymmetry in PCM 

conductance response limited both “training” and “test” accu- 
racy in our original, mixed hardware-software experiment to 
82–83% [1] (Fig. 5). 

Asymmetry (between the gentle conductance increase of 
PCM partial–SET and the abruptness of PCM RESET) be 
mitigate by an occasional RESET strategy, which could be 
both infrequent and inaccurate [1]. While in these initial 
experiments, network parameter such a learn rate η have 
to be tune very carefully, a modify ‘LG’ algorithm offer 
wider tolerance to η, high classification accuracies, and 
low training energy [3] (Fig. 6). 

Tolerancing result show that all NVM-based ANN can 
be expect to be highly resilient to random effect (NVM 
variability, yield, and stochasticity), but highly sensitive to 
“gradient” effect that act to steer all synaptic weight 
[1]. We show that a bidirectional NVM with a symmetric, 
linear conductance response of finite but large dynamic range 
(e.g., each conductance step be relatively small) can deliver the 
same high classification accuracy on the MNIST digit a 

90 

100 

% 
] 500 x 661 PCM = (2 PCM/synapse * 164,885 synapses) + 730 unused PCM 

80 

90 

ac 
y 

[% 

Experiment 

60 

70 
ac 

cu 
r 

80 
90 

100 Matched simulation 

40 

50 

nt 
al 

a 

50 
60 
70 

20 

30 

ri 
m 

en 

Map of 
final 10 

20 
30 
40 

10 

20 

Training epochEx 
pe 

final 
PCM 0 5 10 15 200 

10 

conductance (5000 imageseach) 
0 1 2 

0 
g p each) 

Fig. 5. Training accuracy for a 3–layer perceptron of 164,885 hardware- 
synapsis [1], with all weight operation take place on a 500 × 661 array of 
mushroom-cell PCM devices. Also show be a match computer simulation 
of this NN, use parameter extract from the experiment [1]. 

100 

% 
] 

90 

New 
technique 

80 

ac 
y 

[ 

70 
on test set 

on training set 

60 

cc 
ur 

a 70 

IEDM 2014 

t t t 
40ed 

a 
c 

50 

on training set 

IEDM 2014 
condition 

on test set 

20 

m 
ul 

at 
e 

30 

0 

Si 
m 10 IEDM 

2014 experiment 
0.1 1 

0 

Learning rate 
10 

p 

Fig. 6. A large number of synapsis tend to “dither,” with frequent update 
whose aggregate effect ought to be zero (but which be non-zero due to the 
nonlinearity and asymmetry of NVM–based synapses). By suppress update 
of such synapses, NN performance can be improve and training energy 
reduced, while reduce the need to tune the learn rate precisely. 



a conventional, software-based implementation (Fig. 7). One 
key observation be the importance of avoid constraint on 
weight magnitude that arise when the two conductance be 
either both small or both large — e.g., synapsis should remain 
in the center stripe of the “G-diamond” [2]. 

In this paper, we extend upon these observation and address 
several different yet useful topics. We ass the impact of 
undesired, time-varying conductance change, include drift in 
Phase Change Memory (PCM) and leakage of analog CMOS 
capacitors. We investigate the use of non-filamentary, bidi- 
rectional ReRAM device base on PrCaMnO (PCMO), with 
an eye to develop material variant that provide suitably 
linear conductance change. And finally, we explore tradeoff in 
design peripheral circuitry, balance simplicity and area- 
efficiency against the impact on ANN performance. 

C. Jump-table concept 

A highly useful concept in model the behavior of real 
NVM device for neuromorphic application be the concept 
of a “jump-table.” For backpropagation training, where one 
or more copy of the same program pulse be apply 
to the NVM for adjust the weight [1], we simply need 
one jump-table for potentiation (SET) and one for depression 
(RESET). 

With a pair of such jump-tables, we can capture the nonlin- 
earity of conductance response a a function of conductance 

100 
Training set Targets: 

y 
[% 

] Training set 97% 
94% 

(60,000)Trained 
with 

5 000 

ur 
ac 

y 

90 Linear, unbounded, 
symmetricTest set 

Trained with 
60,000 image 

(5,000)5,000 
image 

d 
ac 

cu symmetric 
Fully bidirectional 

Linear, bounded, symmetric 

Test set 

80 

at 
ed 

Linear, bounded, symmetric 

im 
ul 

Dynamic range 
(# f l d d t f G t G )S 70 

5 10 100 200 50020 50 

(# of pulse need to move from Gmin to Gmax) 

Fig. 7. When the dynamic range of the linear response be large, the 
classification accuracy can now reach the peak accuracy support by the 
original neural network (a test accuracy of 94% when train with 5,000 
images; of 97% when train with all 60,000 images) [2]. 

C 
o 

n 
d 

u 
ct 

an 
ce 

G 
[a 

.u 
.] 

# of pulse 

0 

100 

200 

300 

400 

C 
h 

an 
ge 

in 
co 

n 
d 

u 
ct 

an 
ce 

D 
G 

[a 
.u 

.] 

Conductance G [a.u.] 

a) b) 

300 

400 

500 

600 

700 

800 

900 

1000 

1100 

1200 

0 2 8 12 16 204 6 10 14 18 

400 600 800 1000 1200 

Fig. 8. (a) Example median (blue) and ±1σ (red) conductance response 
for potentiation. (b) associate jump-table that fully capture this (artificially 
construct in this case) conductance response, with cumulative probability 
plot in color (from 0 to 100%) of any conductance change ∆G at any 
give initial conductance G. 

(e.g., the same pulse might create a large “jump” at low 
conductance, but a much small jump at high conductance), 
the asymmetry between positive (SET) and negative (RESET) 
conductance changes, and the inherent stochastic nature of 
each jump. Fig. 8(a) plot median conductance change for 
potentiation (blue) together with the ±1σ stochastic variation 
about this median change (red). Fig. 8(b) show the jump- 
table that fully capture this conductance response, plot 
the cumulative probability (in color, from 0 to 100%) of any 
conductance change ∆G at any give initial conductance G. 
This table be ideal for computer simulation because a random 
number r (uniform deviate, between 0.0 and 1.0) can be 
convert to a result ∆G produce by a single pulse by 
scan along the row associate with the conductance G 
(of the device before the pulse be applied) to find the point at 
which the table entry just exceeds r. 

We have previously use a measure jump-table to simulate 
the SET response of PCM device [1]. We have recently 
publish a study of various artificially-constructed jump- 
tables, in order to help develop an intuitive understand of 
the impact that various feature of such jump-tables have on 
the classification performance in the ANN application [7]. 

II. TIME-DEPENDENT CONDUCTANCE RESPONSE 

One aspect of Phase Change Memory that we do not 
address in our original tolerancing paper [1] be the role 
of resistance drift [8], also know a amorphous relaxation. 
As show in Fig. 9, after a RESET operation, amorphous 
relaxation cause conductance to decrease, rapidly at first 
but then more and more slowly. Here we model this in our 
Neural Network simulator for the network of Fig. 4, for 
an otherwise near-perfect PCM device, in which partial-SET 
conductance increase be gentle and linear (each ∼0.5% of 
the conductance extent) and Occasional-RESET be perform 
fairly frequently (every 100 examples) with high precision. 
The time response for drift start upon RESET operations, with 
partial-SET operation assume only to shift the conductance 
state without affect the underlie time-response of the 
amorphous relaxation. 

As expected, a drift coefficient increase dramatically (to 
the value of ν ∼ 0.1 observe for fully amorphous (strong 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 20 40 60 80 1001 
10 

100 
103 

104 
105 

106 
107 

108 

Time TimeR 
e 

la 
ti 

ve 
co 

n 
d 

u 
ct 

an 
ce 

G / t-º 

º ~ 0.1 
º ~ 0.1 

º ~ 0.01 
º ~ 0.01 

Fig. 9. After a RESET operation, amorphous relaxation cause conductance 
to decrease, rapidly at first but then more and more slowly. Plots show the 
same evolution of linear conductance a a function of time on log- (left) and 
linear- (right) scales, for two different value of drift coeffcient ν. 



RESET) states), then accuracy be eventually affected (Fig. 10). 
However, for the much low ν value (ν ∼ 0.005 – 0.01) 
associate with SET and the near-SET state relevant to PCM- 
base implementation of neural networks, accuracy be only 
minimally affected. 

We perform a similar study for the case of fully volatile 
analog memory elements, such a CMOS capacitors, in which 
any conductance state immediately begin to decay towards 
zero after a program operation. This study be perform 
with perfectly linear bidirectional conductance with ∼0.5% 
conductance change per pulse, and thus without drift, be 
identical to the right-hand side of Fig 7, where accuracy 
becomes extremely high for high synaptic dynamic range. 

In this study, we quantify the effective decay constant (the 
“RC time constant”) not in absolute units, but relative to 
the time require for training of a single data example (e.g., 
forward evaluation, reverse propagation, and weight update). 
As show in Fig. 11, accuracy be strongly affected a soon 
a the ratio between the RC time-constant and the time-per- 
example fall below 10,000. However, these initial result 
reveal an extremely interest dependence on the choice 

0.0001 0.001 0.01 0.1 

Drift coefficient º 
60 

65 

70 

75 

80 

85 

90 

95 

100 

NO 
drift 

Test 

Training 

C 
la 

s 
s 
if 
ic 

a 
ti 
o 

n 
a 

c 
c 
u 

ra 
c 
y 

Fig. 10. At the large drift coefficient associate with fully-amorphous 
RESET phase change memory device (ν ∼ 0.1), neural network accuracy 
be significantly degraded. At the value of ν ∼ 0.005–0.01 relevant to the 
SET and the near-SET state that dominate the PCM-based implementation 
of neural networks, accuracy be only slightly degraded. (Results show for 10 
epoch of simulated training on 5000 example from the MNIST dataset.) 

100 1000 10k 100k 1M 
10 

20 

30 

40 

50 

60 

70 

80 

90 

100 

RC time-constant / time-per-training-example 

/ 

C 
la 

s 
s 
if 
ic 

a 
tio 

n 
a 

c 
c 
u 

ra 
c 
y 

Fig. 11. Neural network accuracy be strongly affected a soon a the RC 
time-constant becomes less than 10,000× large than the time need for 
each training example. (Results show for 10 epoch of simulated training on 
5000 example from the MNIST dataset, all at the same global learn rate, 
η ∼ 1). 

of learn rate, imply that some further optimization may 
be possible. Fig. 12 show that the same global learn rate 
which be optimal for a truly non-volatile conductance (infinite 
RC time-constant) be decidedly sub-optimal when the RC time- 
constant becomes lower. This implies that it be good to either 
update so many weight that one can counteract the loss of 
conductance by retrain those weights, or so few that the 
number of weight be touch (and thus place into a mode 
where they will decay rapidly) be much lower. 

III. IMPACT OF MEASURED PCMO CONDUCTANCE 
RESPONSE 

We have previously study the impact of the conductance 
response of PCMO material by fitting a set of function to 
the average conductance response [4]. However, this approach 
be limited by the discrepancy between the real conductance 
response and the function chosen, and it do not include any 
stochastic aspect of the conductance response, for scenario 
where the conductance response can vary significantly from 
the average conductance response. 

Here, we study the use of measure jump-tables for the non- 
filamentary RRAM material PrxCa1−xMnO3, also know a 
PCMO. 

A. Analog Bidirectional Switching 

Resistive switch in PCMO-based device be cause by 
slow and gradual drift of oxygen ion and vacancy in the 
polycrystalline PCMO layer. Injection (removal) of oxygen 
ion take place at the PCMO-oxide (-metal) interface through 
oxidation (reduction) reactions. Asymmetry in the device 
structure and the oxidation-reduction reaction contribute to 
the asymmetry in the switch characteristics, but PCMO- 
base NVMs show gradual SET and RESET characteristics. 
Thus, unlike Phase Change Memory (PCM) materials, there be 
no need to stop training and perform an Occasional–RESET 
operation [1]. Both the average conductance response and it 
statistical behavior can be described by a measure jump table 
(Fig. 13). (Note that unlike non-filamentary RRAM such a 

0.1 1 10 100 

Learning Rate 
0 

10 

20 

30 

40 

50 

60 

70 

80 

90 

100 

C 
la 

s 
s 
if 
ic 

a 
tio 

n 
a 

c 
c 
u 

ra 
c 
y 

RC time-constant / time-per-training-example 

=/ 

= 104 

= 3000 

= 1000 

solid: training 
open: test 

Fig. 12. For truly non-volatile weight (infinite RC time constant), neural 
network accuracy be optimize by use a global learn rate that be large 
enough to affect a moderate number of weights, but not so many that chaos 
ensues. However, a the RC time constant decreases, the volatility of the 
conductance state favor either a large learn rate (e.g., we adjust for 
the decay weight by retrain many more of them) or, curiously, low 
learn rate (assumably reduce the number of recently-touched weight 
that cannot be trust not to move without be actively programmed). 



PCMO, a filament-based RRAM such a HfOx, TaOx, or 
TiOx exhibit only gradual RESET characteristics, meaning 
that such filamentary RRAM device will likely still require 
an “Occasional–SET” step just like PCM.) 

B. Fabrication process 

A 10nm PCMO polycrystalline layer be deposit on 
a 50-nm-thick Pt layer, which serve a bottom electrode. 
Next, an 100-nm-thick SiNx layer be deposit by plasma- 
enhance chemical vapor deposition, and via-holes (from 0.15 
to 1.0 µm) be form by conventional lithography and 
reactive ion etching. The Al and Mo layer (20nm and 3nm, 
respectively) and an 50-nm-thick Pt layer (top electrode) be 

G (% of maxG) 

G (% of maxG) 

Measured conductance 
changeDG (a.u.) 

Measured conductance 
changeDG (a.u.) 

SET 

RESET 

100% 

50% 

0% 

Fig. 13. Jump-table of Al/Mo/PCMO-based RRAM device for positive (SET) 
and negative (RESET) conductance changes. Unlike Phase Change Memory 
(PCM) devices, these material provide both gradual RESET and gradual 
SET, enable truly bidirectional programming. 50000 total SET pulse (- 
4.0V, 10ms) and RESET pulse (3.5V, 10ms) follow by -1V read pulse 
be use on three identically-sized (200nm) devices. 

Dwij+=hSET *DG 
+ 

SET,ij 

xi 

δj 

Gij+ Gij– 





wij = Gij+ - Gij- 

Dwij+=hRESET*DG 
- 
RESET,ij 


(or) 

Fig. 14. Schematic show crossbar-compatible [1] weight-update rule for 
Analog bidirectional NVMs. Weight increase (decreases) can be implement 
either a a SET operation on G+ (G−) or a RESET operation on G− (G+) 
devices. Asymmetry in the partial SET and RESET operation be compensate 
by apply a different learn rate parameter (ηSET , ηRESET ) that 
modulates the number of pulse fire from the neuron into the array. 

deposit and pattern by conventional lithography. Electrical 
characteristic of the Al/Mo/PCMO-based resistive memory 
device be measure use an Agilent B1500A. 

C. Simulated performance 

A three-layer perceptron with two PCMO-based device 
per synapse be simulated perform a classification task 
on the MNIST database (same network show in Fig. 4). 
Fig. 13 plot the model conductance response of the resistive 
switch elements. For average value of conductance G (e.g., 
the central region of the plot), the response be mostly linear, 
although somewhat asymmetric, with different average jump 
value for SET and RESET. In constrast, for extreme value 
of the conductance (left and right edge of each jump-table), 
a high degree of non-linearity be observed. However, we have 
previously observe that when the extent of the non-linear 
region be sufficiently small, high classification accuracy can 
still be achieve [7]. 

The network parameter be tune to achieve a good 
performance, with particular focus give to the ratio of 
ηSET/ηRESET, use to compensate the asymmetry of the jump- 
table. Fig. 14 show a schematic version of the crossbar- 
compaibile weight update rule for backpropagation, in which 
upstream neuron fire a set of pulse (shown in red) along the 
horizontal word-lines, base solely on their knowledge of xi 
and the global learn rate (η = ηSET) [1]. Simultaneously, the 
downstream neuron first pulse (shown in magenta) along the 
vertical bit-lines connect to a large number of G+ and G− 

conductances. These pulse be base only on the downstream 
neuron’s knowledge of δj and the global learn rate. 

Because these pulse affect all the device along the share 
word-lines and bit-lines, their amplitude and duration cannot 
be tune to optimize the program of any one particular 
conductance value. This lead to significant problem when 
conductance response be nonlinear, since the same pulse can 
cause small conductance to increase much more significantly 
than conductance that be already large. 

Fig. 15. Simulated training and test accuracy for a three-layer perceptron 
use PCMO-based device a synaptic weights. The asymmetry between 
positive and negative jump can be compensate by tune individually 
the learn rate for SET and RESET (see Fig. 14). The classification 
accuracy of the network improves a the ratio of SET to RESET learn 
rate (ηSET/ηRESET) increases. 



However, the downstream neuron can easily fire different 
pulse-trains on the separate G+ and G− bit-lines, and knowl- 
edge of δj can be sufficient to identify whether SET or RESET 
will occur (xi need only be constrain to be non-negative). 
Thus it be straightforward to apply a different global learn 
rate for RESET and for SET, thus lead to more or few 
pulses, and provide a way to compensate for jump-table 
asymmetry. Fig. 15 show that classification accuracy can be 
improve for the Al/Mo/PCMO jump-tables show in Fig. 13, 
with an optimal ratio of ηSET / ηRESET of approximately 3–4. 

D. Switching Energy 

The switch energy of the device be measure by 
integrate the product between the voltage and the current for 
the duration of a program pulse (10ms). The conductance 
be measure with read pulse of −1V . PCMO-based memory 
device (like other non-filamentary switch elements) show 
a dependence of the program energy on the active area. 
Switching energy range from sub-nJ to ten of µJ be 
measure on device with hole size from 0.15nmto 1µm 
(Fig. 16(a). The switch energy be then normalize with 
respect to the active device area (Fig. 16(b)) to show a good 
linear dependence between switch current and device hole- 
size. Following the trend from 150nm down to 25nm, one 
can anticipate an improvement in switch energy by roughly 

Fig. 16. Switching (a) energy a a function of conductance and (b) energy 
density a a function of conductance density, measure for Al/Mo/PCMO- 
base device with -1V reading voltage. 

35×. If the switch time could potentially be reduce from 
10ms down to 10ns, then one would be able to achieve femto- 
Joule switch energy. Such aggressive scale of both device 
area and switch time would be necessary in order to enable 
highly-parallelized weight update operations. 

IV. CIRCUIT NEEDS 

A crossbar-array-based neural network implement the 
multiply-accumulate operation at the heart of most neural 
network algorithm extremely efficiently, through Ohm’s law 
follow by current summation (Kirchoff’s Current law). 
However, an important consideration be the design of highly 
area-efficient neuron circuit that reside at the edge of these 
array enable read and write of many synaptic row or 
column in parallel. Such high parallelism be essential if we 
wish to achieve order of magnitude performance and power 
benefit over conventional CPU/GPU approach [3]. Given 
this need for a large number of distinct copy of neural 
circuit that can be execute in parallel, it be critical to embrace 
approximate functionality (for e.g. non-linear squash func- 
tions, calculate and multiply derivative etc.) rather than 
rigorously-precise yet highly area-inefficient functionality. 

In this section, we present example of design choice that 
simplify the underlie hardware by leverage the inherent 
tolerance of ANN algorithm to error. We discus circuit 
need for the forward- and reverse-evaluate operations, in- 
cluding precision/range of the compute neuron activation 
and backpropagated errors, use piecewise linear (PWL) 
approximation of non-linear squash functions, and sim- 
plifying the derivative include during reverse propagation to 
avoid complex floating-point arithmetic operations. We then 
demonstrate that these approximation do not significantly 
degrade classification accuracy a compare to neuron im- 
plementations with rigorously-precise functionality. 

A. Circuit-Needs for Forward and Reverse Propagate 

Forward propagation (Fig. 17) in a fully connect neural 
network involves the calculation of the neuron activation of 
a hidden/output layer, base on the neuron activation of the 
previous layer and the intervene synaptic weights. This be 
a two-stage process, with the multiply- accumulate operation 
occur in the crossbar array, and the non-linear squash 
function apply at the periphery. One commonly use function 
in software implementation be tanh() (the hyperbolic-tangent 
function), which be difficult to implement exactly unless a 
large number of transistor be included. However, a piece- 
wise linear implementation of this squash function would 
be fairly straightforward to implement (Fig. 17). 

A second design choice be the range of distinct neuron 
activation value that need to be support by the hardware. In 
a digital implementation this translates into the number of bits, 
which would have area implication depend on the amount 
of local storage required, a well a the resolution of any ana- 
log to digital conversion circuit use to convert signal from 
the crossbar array into those bits. In an analog implementation, 



Forward Propagation 

xA1 

xA2 

xAn 

xB1 

xB2 

xBn 

xBj = f(Swij xAi) 

f( . ): non-linear 
activation 

Input/ Hidden 
Neurons 

Hidden/Output 
Neurons 

tanh 
piece- 
linear 

w11 

Accumulated Sum 

R 
a 

n 
g 

e 
o 

f 
N 

eu 
ro 

n 
S 

ta 
te 

s 

) 

linear 

-wise 
linear 

Fig. 17. Forward Propagation operation in a Deep Neural Network. The 
multiply-accumulate operation occurs on the crossbar array. Neuron circuitry 
must handle the non-linear squash function. 

this would directly translate into the resolution between analog 
voltage level and/or time-steps. 

Reverse propagation (Fig. 18) be similar to forward propaga- 
tion, but from output/hidden neuron to precede hidden neu- 
rons. The quantity δ, know a the correction or error, together 
with the forward-propagated neuron activations, control the 
weight update for neural network training (see Fig. 14). An 
important distinction from forward propagation be that the non- 
linear squash function be not applied. Instead, the multiply- 
accumulate sum (integrated on the crossbar array, but in a 
direction orthogonal to the integration perform during the 
forward-propagate step) need to be scale by the derivative 
of the activation function, a evaluate at the neuron activation 
value. Again, an exact tanh() derivative be not efficient to 
compute and multiply. 

Instead, a step-function derivative with two distinct state 
can be used. Multiplication by derivative value of zero and 
one be fairly straightforward to implement in hardware. This 
corresponds to simply enable or disable the transmission 
of an accumulate sum-of-deltas from any neuron stage to 
the precede stage. However, multiplication by arbitrary 
scale factor may be difficult to achieve since floating-point 
multiplier be not readily available. The impact of such 
approximation on neural network training be study in the 
next subsection. 

B. Results: Circuit Approximations 

We explore the impact of the aforementioned circuit 
approximation on the training and test performance of the 
MNIST dataset of handwritten digit through simulations. A 
subset of only 5000 training image from the original dataset 
of 60000 image be used. Images be cropped to 24× 22 
pixels. The same 3-layer neural network (528-250-125-10) be 
use (Fig. 4. A crossbar-compatible weight update rule [1] 
be use to emulate how weight update would be do on a 
real crossbar array. The baseline training and test accuracy 
assume 20 epoch of training, 256 neuron activation states, 
a tanh() activation function and exact derivative be found 
to be 99.7% and 93.6% respectively (blue curve and star, 
Fig. 19). Note that, a per Fig. 7, both training and test 

3 

Reverse Propagation 

xA1 

xA2 

xAn 

xB1 

xB2 

xBn 

δAi = f’(x 
A)●(Swij δBj) 

f’( ): derivative of 
activation function 

Input/ Hidden 
Neurons 

Hidden/Output 
Neurons 

tanh 

piece 
-wise 
linear 

w11 

Fig. 18. Reverse Propagation operation in a Deep Neural Network. Multiply- 
accumulate operation on δ occurs on the crossbar array. Neuron circuitry 
must handle generation and multiplication of the derivative of the squash 
function. 

Training Epoch 
2 106 14 184 128 16 20 

% 
C 
or 
re 
ct 

tanh 
piece‐wise linear 

Fig. 19. Training and test accuracy obtain on MNIST with tanh() and 
piece-wise linear activation functions. PWL achieves test accuracy comparable 
to tanh(). 

accuracy increase (to ∼100% and ∼97-98%) when all 60,000 
example be use for training. 

Fig. 19 also show the training and test accuracy use 
a piece-wise linear (PWL) activation function. On MNIST, 
one observes that the test accuracy obtain (92.7%) be al- 
ready comparable to the full tanh() implementation. Further 
improvement in test accuracy can be obtain by optimize 
the low value of the derivative. This be akin to the intentional 
implemention of ‘leaky’ derivative in some conventional 
machine learn techniques, especially in the case of Rectified 
Linear Units (ReLU). A leaky derivative ensures that some 
contribution from the downstream neuron get pass on to 
early stages, thereby participate in the program of 
those weights. 

Fig. 20 show that the test accuracy can be further improve 
to 93.2% when the derivative of the piecewise-linear squash 
function at extreme value be make non-zero. However, the 
multiplication operation be non-trivial. In a digital implemen- 
tation, one might be able to do bit-shift operation (restricting 
derivative value to power of 2). An analog implementation 
can offer more freedom, since we need only enable one of two 
non-zero scale factor when transmit accumulate analog 
voltage to precede stages. 

In addition to the squash function and it derivative, 
the impact of the number of distinct neuron activation and 
error state on the test accuracy be analyzed. Values from 8 



% 
T 
e 
t A 
cc 
ur 
ac 
y 

91.5 

92 

92.5 

93 

Derivative Low Value 
0 0.10.05 0.15 

Non‐zero 
Low Derivative 

Fig. 20. Optimizing the low derivative value enables further improvement 
in test accuracy, yet require some circuit complexity to implement an 
approximate multiplication function. 

% 
T 
e 
t A 
cc 
ur 
ac 
y 

85 

89 

91 

93 

87 

Number of Neuron States 
8 3216 64 128 256 

tanh 

piece‐wise linear 
with non‐zero deriv. 

Fig. 21. If the number of distinct neuron activation and error state be low 
than 32, then test accuracy degrades. However, reduce the total number 
of neuron state can help enable significantly more area-efficient peripheral 
circuitry. 

to 256 be consider (Fig. 21). High test accuracy be 
maintain down to 32 distinct neuron state for both the 
tanh() and piece-wise linear implementations. Reducing the 
total number of neuron state can be extremely beneficial in 
area-efficient circuit design. In a digital implementation, this 
allows a reduction in the total number of latch or flip-flops. 
In an analog implementation, it permit a wider separation of 
analog voltage levels, relax noise constraint and enable 
simpler circuits. 

V. CONCLUSION 

We have study several aspect of system design when Non- 
Volatile Memory (NVM) device be employ a the synaptic 
weight element for on-chip acceleration of the backpropaga- 
tion training of large-scale artificial neural network (ANN). 

We have assess the impact of undesired, time-varying 
conductance change, include drift in Phase Change Memory 
(PCM) device and leakage of analog CMOS capacitors. We 
have investigate the use of non-filamentary, bidirectional 
ReRAM device base on PrCaMnO, which can be consider 
a promising material variant that could potentially provide both 
gradual conductance increase and conductance decrease. And 
finally, we have explore some of the tradeoff in design 
peripheral circuitry, balance simplicity and area-efficiency 
against the impact on ANN performance for the nonlinear 
squash function, the evaluation of it derivation, and the 

number of resolvable level when integrate both x (forward- 
propagate) and δ (reverse-propagate) values. 

We briefly review our previous work towards achieve 
competitive performance (classification accuracies) for such 
ANN with both Phase-Change Memory [1], [2] and non- 
filamentary ReRAM base on PrCaMnO (PCMO) [4], and 
towards assess the potential advantage for ML training 
over GPU–based hardware in term of speed (up to 25× faster) 
and power (from 120–2850× low power) [3]. We discuss 
the “jump-table” concept, previously introduce to model 
real-world NVM such a PCM [1] or PCMO, to describe 
the full cumulative distribution function (CDF) of result 
conductance-change at each possible conductance value, for 
both potentiation (SET) and depression (RESET). 

While the ‘LG’ algorithm, together with other approaches, 
should help a nonlinear, asymmetric NVM (such a PCM) act 
more like an ideal linear, bidirectional NVM, the identification 
of NVM device and/or pulse-schemes that can offer a con- 
ductance response that be at least partly linear, use circuitry 
that can be highly area-efficient (and thus massively-parallel), 
will help significantly in achieve equally-high classification 
accuracy while offering faster and lower-power training than 
conventional GPUs and CPUs. 

REFERENCES 
[1] G. W. Burr, R. M. Shelby, C. di Nolfo, J. W. Jang, R. S. Shenoy, 

P. Narayanan, K. Virwani, E. U. Giacometti, B. Kurdi, and H. Hwang, 
“Experimental demonstration and tolerancing of a large-scale neural 
network (165,000 synapses), use phase-change memory a the synaptic 
weight element,” in IEDM, 2014, p. 29.5. 

[2] G. W. Burr, R. M. Shelby, S. Sidler, C. di Nolfo, J. Jang, I. Boybat, 
R. S. Shenoy, P. Narayanan, K. Virwani, E. U. Giacometti, B. Kurdi, and 
H. Hwang, “Experimental demonstration and tolerancing of a large–scale 
neural network (165,000 synapses), use phase–change memory a the 
synaptic weight element,” IEEE Trans. Electr. Dev., vol. 62, no. 11, pp. 
3498–3507, 2015. 

[3] G. W. Burr, P.Narayanan, R. M. Shelby, S. Sidler, I. Boybat, C. di 
Nolfo, and Y. Leblebici, “Large–scale neural network implement 
with nonvolatile memory a the synaptic weight element: comparative 
performance analysis (accuracy, speed, and power),” in IEDM Technical 
Digest, 2015, p. 4.4. 

[4] J.-W. Jang, S. Park, G. W. Burr, H. Hwang, and Y.-H. Jeong, “Optimiza- 
tion of conductance change in Pr1−xCaxMnO3–based synaptic device 
for neuromorphic systems,” IEEE Electron Device Letters, vol. 36, no. 5, 
pp. 457–459, 2015. 

[5] D. Rumelhart, G. E. Hinton, and J. L. McClelland, “A general framework 
for parallel distribute processing,” in Parallel Distributed Processing. 
MIT Press, 1986. 

[6] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learn 
apply to document recognition,” Proceedings of the IEEE, vol. 86, 
no. 11, p. 2278, 1998. 

[7] S. Sidler, I. Boybat, R. M. Shelby, P. Narayanan, J. Jang, A. Fumarola, 
K. Moon, Y. Leblebici, H. Hwang, and G. W. Burr, “Large-scale neural 
network implement with non-volatile memory a the synaptic weight 
element: impact of conductance response,” in ESSDERC 2016, 2016, p. 
to appear. 

[8] A. Pirovano, A. L. Lacaita, F. Pellizzer, S. A. Kostylev, A. Benvenuti, 
and R. Bez, “Low–field amorphous state resistance and threshold voltage 
drift in chalcogenide materials,” IEEE Trans. Electr. Dev., vol. 51, no. 5, 
pp. 714–719, 2004. 


