










































Classifiers and their Metrics Quantified 


1 
2 
3 
4 
5 
6 
7 
8 
9 

10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 

DOI: 10.1002/minf.201700127 

Classifiers and their Metrics Quantified 
J. B. Brown*[a] 

Methods Corner www.molinf.com 

© 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim Mol. Inf. 2018, 37, 1700127 (1 of 11) 1700127 

http://orcid.org/0000-0002-4637-1531 
http://orcid.org/0000-0002-4637-1531 
http://orcid.org/0000-0002-4637-1531 
http://orcid.org/0000-0002-4637-1531 
www.molinf.com 


1 
2 
3 
4 
5 
6 
7 
8 
9 

10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 

Abstract: Molecular model frequently construct classi- 
fication model for the prediction of two-class entities, such 
a compound bio(in)activity, chemical property (non)exis- 
tence, protein (non)interaction, and so forth. The model 
be evaluate use well know metric such a accuracy or 
true positive rates. However, these frequently use metric 
apply to retrospective and/or artificially generate pre- 
diction datasets can potentially overestimate true perform- 
ance in actual prospective experiments. Here, we systemati- 

cally consider metric value surface generation a a 
consequence of data balance, and propose the computa- 
tion of an inverse cumulative distribution function take 
over a metric surface. The propose distribution analysis 
can aid in the selection of metric when formulate study 
design. In addition to theoretical analyses, a practical 
example in chemogenomic virtual screen highlight the 
care require in metric selection and interpretation. 

Keywords: Classifiers · metric · prediction · model · performance assessment 

1 Introduction 

Computational model for molecular phenomenon have 
become a mainstream tool in the academic and industrial 
research communities.[1,2] Aside from purely experimental 
medicinal chemist who often prioritize their experience 
and intuition, many team consider the result of computa- 
tional prediction when proceed to experimental valida- 
tion.[3] Perhaps the most common computational model be 
the discrete classification model, and within discrete 
classification, the two-class discriminant be frequent. In these 
models, the objective be to fit a mathematical function to 
discriminate between example with “yes”/”positive”/”ac- 
tive”/”true” label and example with “no”/”negative”/”in- 
active”/”false” labels. In molecular informatics, example of 
the two-class discriminant include predict if compound 
have a specific property or not (e.g, chemical stability under 
a give set of conditions), if protein interact with each 
other or not, or if ligand and receptor have a strong 
interaction or not a measure by IC50 , EC50 , Ki , Kd , etc. 

In many cases, model be compute from some 
descriptor or fingerprint representation of the molecules, 
and the model’s ability to discriminate between the two 
class be evaluate by consider the result of prediction 
on an additional dataset. For two-class problems, this yield 
four type of results, a the predict example be pre- 
label with their know class and additionally have a label 
result from prediction. The four result type be true 
positive (TP), false positive (FP) know a false discovery 
or type-I errors, true negative (TN), and false negative (FN) 
know a miss discovery or type-II error (see Figure 1). 
The collection of all four result type be often refer to a 
the confusion matrix. 

While the raw count of these four primary outcome 
be informative, researcher often summarize a confusion 
matrix by a single, real-valued metric, to decide on the 
quality of a model, to facilitate rank of method or 
datasets, and so forth. Two common, and potentially most 
intuitive, metric be the true positive rate (TPR) and 
accuracy (ACC) metrics: 

TPR ¼ TP 
TPþ FN ACC ¼ 

TPþ TN 
TPþ FPþ TNþ FN : 

The former gauge how well positive instance be in 
fact classify by the model, and the latter gauge how well 
both positive and negative instance be jointly classified. 
Hit find team may refer to the TPR when they wish to 
forecast the virtual screen of a large chemical library, and 
hit-to-lead and lead optimization team might refer to ACC 
when consider the selectivity of a molecule, that is, the 
molecule’s character to only interact with the target 
intend by the molecule development team. 

Studies that have execute prospective experimental 
validation of computationally predict molecular proper- 
tie often report low to medium success rate despite 
moderate to high model metric performance.[4–6] While it be 
true that our understand of the exact molecular under- 

[a] J. B. Brown 
Kyoto University Graduate School of Medicine, Laboratory of Mo- 
lecular Biosciences, 606-8501, E-109 Konoemachi, Sakyo, Kyoto, 
Japan 
E-mail: jbbrown@kuhp.kyoto-u.ac.jp 

Supporting information for this article be available on the WWW 
under https://doi.org/10.1002/minf.201700127 
© 2018 The Authors. Published by Wiley-VCH Verlag GmbH & Co. 
KGaA. 
This be an open access article under the term of the Creative 
Commons Attribution Non-Commercial License, which permit 
use, distribution and reproduction in any medium, provide the 
original work be properly cite and be not use for commercial 
purposes. 

Figure 1. Confusion matrix. 

Methods Corner www.molinf.com 

© 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim Mol. Inf. 2018, 37, 1700127 (2 of 11) 1700127 

https://doi.org/10.1002/minf.201700127 
www.molinf.com 


1 
2 
3 
4 
5 
6 
7 
8 
9 

10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 

pinning of process be a continually evolve process and 
therefore we cannot build a perfect discriminant (model) of 
a phenomenon, a more fundamental problem exists in the 
metric we often use to evaluate our results. In fact, metric 
such a TPR and ACC often overestimate the “true” model 
performance. Action base on the mislead metric incurs 
a risk to a project which can lead to misfortune, typically a 
a result of overconfidence in the discriminator computed. 

In the remainder of this article, we illustrate the concern 
around metric such a TPR and ACC by shift analysis 
away from single-point numeric estimate to metric value 
surface (heatmaps) and consider some statistical prop- 
erties of the metrics. While some previous attempt to 
objectify metric exist,[7–16] this article is, to the best of our 
knowledge, the first to generically ass metric surface 
and their interplay with data balance. Further, we also 
propose a method to quantitatively cross-compare metrics, 
and examine a practical case study in chemogenomic virtual 
screening. 

2 Visual Cues Signaling Caution in Metric 
Interpretation 

From the confusion matrix, many metric be possible. In 
addition to the TPR and ACC introduce earlier, here we 
consider the Balanced Accuracy (BA), Positive Predictive 
Value (PPV), F-measure and it derivative F1-score (F1), True 
Negative Rate (TNR), and the Matthews Correlation Coef- 
ficient (MCC). These be define mathematically a follows: 

PPV ¼ TP 
TPþ FP TNR ¼ 

TN 
TNþ FP BA ¼ 

TPRþ TNR 
2 

F1 ¼ 2 
* PPV*TPRð Þ 
PPV þ TPR 

MCC ¼ TP 
*TN� FP*FN 

ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 

ðTPþ FPÞðTPþ FNÞðTNþ FPÞðTNþ FNÞ 
p : 

One fact we can immediately note from these defini- 
tions be how many type of result from the confusion 
matrix be include in each metric. TPR, TNR, and PPV 
include two type of results, and BA and F1 include three 
type of results. MCC and ACC include all four result type 
in their formulations, though only MCC include both type-I 
error and type-II error in it numerator, and in a multi- 

plicative manner. The range of value for the metric be 
[�1,1] for the MCC, and [0,1] for all others. 

Let u consider a simple example of two prediction 
experiments, and the result metric value for MCC and 
ACC. In the first experiment, we have TP = 1000, TN = 2100, 
FP = 150, FN = 650. We might consider this to be a rather 
successful prediction. The result ACC be 0.80 and 
result MCC be 0.58. Immediately, we notice that the MCC 
penalizes the type-I and type-II error more than the ACC 
metric. 

In the second experiment, let u assume that there be 
few positive in the dataset, and that TP = 500, with TN, 
FP, and FN the same a above. We might also consider this 
to be successful base on the low false discovery rate (FPR), 
and when we evaluate the experiment by ACC the value be 
0.76. 

This be where we have deceive ourselves. Despite the 
high value of ACC, a re-examination of the data would 
reveal that we make more type-II error than we correctly 
detect the positives. While the metric difference in the 
experiment for ACC be only 0.04, the latter experiment’s 
MCC be 0.44, a metric difference of 0.14. We come to 
understand that the MCC be a more challenge metric to 
score high on, and that over-expectations can be easily 
borne when use the ACC without consider the back- 
ground data. 

The additional issue we must be aware of be the ratio of 
data between the two classes. In many machine learn 
scenarios, equal number of positive and negative instance 
be use for model calculation and evaluation. However, in 
molecular informatics, this be a skew of reality. For 
example, in experimentally-based drug discovery and 
chemical biology, hit molecule be typically found at rate 
of 1 %.[17,18] Even in less extreme example where 10 %–25 % 
of the data belongs to the ‘positive’ (hit) class, metric such 
a ACC can yield high value even if the model predicts 
everything a a negative (see below). 

We can enhance our understand of each metric’s 
implication by consider the entire space of value it can 
take a a consequence of all possible type-I and type-II error 
rates, or alternatively and perhaps more intuitively, a a 
consequence of all possible TPRs and TNRs. As note above, 
these rate be impact by the ratio of data classes, and so 
should also be a factor in our interpretations. 

We begin by visualize the MCC and consider the 
impact of data ratio (Figure 2, left). We immediately note 

J. B. Brown obtain dual B.S. degree in Computer Science and Mathematics from the University of Evansville 
(Indiana, USA). Following undergraduate studies, he work at the NIH clinical center, contribute to a computational 
platform for automate diagnostic radiology image analysis and therapy decision support. In 2004, he be award a 
full scholarship from the Japanese government, and begin his garduate study at Kyoto University, complete a 
Ph.D. in bio- and chemo-informatics in 2010. After a post-doctoral appointment in pharmaceutical sciences, he be 
promote to an assistant professor at the Kyoto University Graduate School of Medicine in 2014. In 2015, he be 
award an independent (PI) Junior Associate Professor position within the same graduate school, and found his 
Life Science Informatics Research Unit, a unit hybridize clinical, chemical, and biological informatics. 

Methods Corner www.molinf.com 

© 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim Mol. Inf. 2018, 37, 1700127 (3 of 11) 1700127 

www.molinf.com 


1 
2 
3 
4 
5 
6 
7 
8 
9 

10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 

that have either high TPR or TNR be not a sufficient 
condition for high MCC. Extreme optimization on either 
positive or negative can yield MCC value close to 0. 
Rather, both error rate must be low in order to achieve a 
high MCC. The situation in practical, imbalanced data be 
even more extreme (Figure 2, right); the region of the MCC 
surface which be at or above 0.6 be considerably reduce 
compare to the surface for balance data. 

When we consider the same pair of data ratio condition 
but evaluate use ACC a show in Figure 3, it becomes 
clear to u why ACC can trigger high expectations. Though 
extreme optimization on one class yield MCC value of 0, it 
yield ACC value of 0.5 in balance datasets. Even worse, in 
hit and lead discovery application with discovery rate of 
10 % or less, a model can achieve a suggestively strong ACC 

of 0.8 or high without even predict a positive/active 
entity. 

As an additional way of analyze the metrics, we might 
consider the distribution of value within the metric space 
show in Figure 2 and Figure 3. This would yield a 
probability distribution function (assuming a small tolerance 
or bandwidth parameter), and we could compute the 
correspond cumulative distribution function. Instead, let 
u consider the “inverse cumulative distribution function 
(iCDF)”, define a the fraction of value in the metric space 
that be great than or equal to a give metric value 
threshold. Our motivation for this be that we might ask, 
what be the probability of get at least the value X for a 
particular metric? We can sample the performance matrix 
for the fraction of value that be at least a give value, and 

Figure 2. Metric landscape for MCC, balance dataset versus imbalanced dataset. 

Figure 3. Metric landscape for ACC, balance dataset versus imbalanced dataset. 

Methods Corner www.molinf.com 

© 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim Mol. Inf. 2018, 37, 1700127 (4 of 11) 1700127 

www.molinf.com 


1 
2 
3 
4 
5 
6 
7 
8 
9 

10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 

take over the value domain of the metric, this yield a 
continuous plot and subsequent visual interpretation of the 
odds of obtain a particular value or good of a metric. 
The result mirror image of a classic cumulative distribu- 
tion function suggests the prefix “inverse”. Just a with the 
metric space visualization, the iCDF inherently depends on 
the class ratio. 

The iCDF analog of Figures 2 and 3 be show in 
Figure 4. It confirms that the probability of obtain 
metric value of 0.6 or 0.8 be many fold high for ACC 
than for MCC, regardless of data ratio. The iCDF curve 
reinforce why misfortune may occur when base decision 
on model ACC values; the probability of achieve an ACC 
of 0.8 actually increase with a trend toward an imbalanced 
dataset. In contrast, one can feel more confident in a model 
that achieves MCC of 0.8, for the probability of such be 
rather low a an effect of it formulation. 

Herein we have assume that the prior probability of 
obtain any particular value in the metric surface space be 
uniform. Yet in practical applications, this assumption will 

not hold, and prior will be influence by datasets. There- 
fore, we can argue that the iCDFs give in Figure 4, notably 
for MCC, be optimistic estimates, and in reality, the gap 
between MCC and AUC iCDFs may in fact be even large in 
practice. 

The surface of the ACC, TNR, BA, F1, PPV, and MCC 
metric be place together in Figure 5 for a balance ratio 
of data. For balance data, we see that the F1 score also 
penalizes a high type-II error rate and yield a metric value 
close to 0. This be a consequence of use the PPV and TPR 
in tandem. However, it be possible to over-optimize on 
positive and score high with F1, so some caution be 
recommended. For balance datasets, ACC and BA be 
synonymous. 

Turning to imbalanced datasets dominate by nega- 
tives/inactives (Figure 6), we see the rough correlation 
between the ACC and TNR surfaces, which reiterates the 
caution involve in use ACC. A predictor yield an ACC 
of 0.9 on a strongly imbalanced dataset might potentially 
be a predictor of negative and otherwise little more than 

Figure 4. Metric iCDFs. Matthews Correlation Coefficient (left) versus Accuracy (right). Balanced datasets (above) versus imbalanced datasets 
(below). 

Methods Corner www.molinf.com 

© 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim Mol. Inf. 2018, 37, 1700127 (5 of 11) 1700127 

www.molinf.com 


1 
2 
3 
4 
5 
6 
7 
8 
9 

10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 

chance. The BA metric be unchanged with respect to the 
data ratios. The clearest effect a a function of data ratio be 
on the F1 and PPV metrics. Much like MCC, the probability 
of obtain high F1 and PPV score be strongly 
diminish when data be imbalanced, so model with high 
value under these condition could be construe to be 
legitimately predictive in prospective applications. 

Here a well, it be helpful to consider the iCDFs of the 
metrics. In Figure 7, an overlay of the iCDFs of the six 
metric be shown, with separate view for balance and 
imbalanced data. For balance data, it be clearly more 
challenge to achieve a MCC value of 0.6 compare to 
other metrics. When data be imbalanced, ACC, BA, and TNR 
have high odds of achieve 0.6 or 0.8 than F1, MCC, or 

Figure 5. Multi-metric surface comparison, Balanced data. 

Figure 6. Multi-metric surface comparison, Imbalanced data. 

Methods Corner www.molinf.com 

© 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim Mol. Inf. 2018, 37, 1700127 (6 of 11) 1700127 

www.molinf.com 


1 
2 
3 
4 
5 
6 
7 
8 
9 

10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 

PPV, and thus the former three statistic must be use with 
caution, particularly if the statistic will be a part of select 
model that will guide prospective applications. 

Figures 5 and 6 demonstrate the clear shift in metric 
surface a a result of data imbalance, and Figure 7 
objectifies the metric surface by iCDF analysis. An 
expand analysis be still further possible by continuously 
vary the positive-negative data ratio and connect the 
per-ratio snapshots. In the supplementary data, the inter- 
ested reader can find animation of the shift in metric 
surface and iCDF. What do we learn from such animations? 
They provide u a good understand of the dependence 
of a metric on the data ratio, and we gain skill in 
interpret the significance of a give two-class model 
experiment. Also, these animation expand our perspective 
for prepare prospective study design, by select metric 
appropriate to the study context (vide supra). 

We additionally provide pseudocode for the reader to 
develop visualization of new metric approach base on a 
confusion matrix. Code use the python program 
language style, the NumPy matrix library,[19] and the 
Matplotlib visualization library[20] be given. First, the under- 
lie surface matrix must be compute a follows. 

Listing 1 – Code for Surface Matrix 

“metric” be a calculable function use the argument TP, TN, 
FP, and FN. 
“nPos” and “nNeg” refer to the number of positive and 
negative represent the data. 

In the scheme in Listing 1, it be recommend to enforce 
that the “gridSize” parameter will be a value that evenly 
divide 100. The parameter will control the granularity of 
visualization and amount of memory require to compute 
the metric matrices. 

Second, the matrix generate be to be visualized. Using 
the matplotlib library, this be relatively trivial. We add code 
to handle the case where a metric cannot be compute in 
extreme case such a all example predict into the same 
class. This be handle in Listing 1 by the try-except clause 
for error cause by division by zero. 

Listing 2 – Code for Metric Matrix Visualization 

“fig” be a matplotlib Figure object, and “axes” be a matplotlib 
Axes object whose parent be “fig”. The “mat” be generate by 
Listing 1. A matplotlib Colormap object “cmap” be provide 
to standardize the same color scheme across the metrics. 
Additional argument store in the “args” key-value map- 
ping control the final visualization rendered. 

Figure 7. iCDFs of metrics, for balance (left) and imbalanced (right) datasets. 

Methods Corner www.molinf.com 

© 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim Mol. Inf. 2018, 37, 1700127 (7 of 11) 1700127 

www.molinf.com 


1 
2 
3 
4 
5 
6 
7 
8 
9 

10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 

3 Metrics and the Receiver Operating 
Characteristic Curve 

Another highly common method for assess the perform- 
ance of a model method be the generation of a receiver 
operating characteristic (ROC) curve, and the area under 
such a generate curve. ROC curve be most commonly 
built by use cross-validation technique that perform 
multiple round of dataset splitting, such that each round 
of prediction value can be include in a more comprehen- 
sive list of threshold which will yield a per-threshold pair of 
value for TPR and Type-I error rate (the TPR and Type-I 
error will be compute use the entire collection of 
prediction span all round of dataset splitting). Thresh- 
old value could be, to name only a few, distance from 
hyperplanes when use the SVM algorithm[21] or the 
percentage of tree vote when use a random forest 
algorithm.[22] A plot of all TPR and Type-I error rate use all 
threshold then result in a curve indicate the tradeoff 
between optimize on the two metrics. If a threshold can 
be found such that it can discriminate all or most of the 
positive from the negatives, then it will result in an area 
under the ROC curve close to 1. For further explanation, see 
the literature on ROC and extension to ROC.[15] 

To analyze the AUC in the same metric surface frame- 
work, one modification be needed. In the case of ACC, MCC, 
and similar metrics, fix count of positive and negative 
(and hence their ratio) completely determine the result 
surface (because we can iterate the grid of TPR and TNR 
independent of an actual model). However, in the case of 
AUC, a range of threshold obtain from cross-validated 
model and prediction be require for metric calculation. 
Therefore, instead of generate a grid of TPR/TNR, we must 
generate a grid of positive and negative dataset sizes, and 
execute cross-validation with accompany AUC calcula- 
tion at each point. Repeated execution and computation 
of average AUC minimize outlier value that may occur. 

We execute two AUC calculation and surface analysis 
experiments. In the first, we use artificial stochastic classifier 
function (random selector base on underlie distribu- 
tions) a surrogate of model compute from real data. In 
the second, we apply the SVM algorithm to large-scale 
public GPCR GLASS[23] and Kinase SARfari[24] ligand-target 
bioactivity datasets, where the active/inactive threshold 
be set to 100nM/1-10uM, and intermediate strength 
interaction be discard (for further data processing 
details, see Reker et al.[25]); the human-based, filter data- 

set contains 49815 active (71 %) versus 20145 inactives for 
GLASS, and 19231 active (48 %) versus 20475 inactives for 
SARfari, with compound and protein respectively repre- 
sented by their MACCS key and dipeptide frequency 
representations. Experiments for the artificial classifier use 
200 iteration for AUC averaging, and experiment for 
GPCR/kinase bioactivity classification compute average 
AUC over 20 iteration of 3-fold cross-validation. In addition 
to AUC calculation, a subset of data be held out for external 
prediction when use the real datasets. A diagram of 
execution flow be provide a supplementary data. 

For the artificial classifiers, we find that average AUCs 
be not influence by the size of positive and negative 
(see supplementary data). Rather, the surface be domi- 
nated by the parameter of the stochastic selection process 
(e. g., mean and standard deviation of a random Gaussian 
variable). 

In contrast, however, we find that AUC value in the real 
dataset be influence by positive/negative size, and they 
be simultaneously influence by model parameter (e. g., 
tolerance factor “C” and radius parameter g for SVMs use 
radial basis function kernels). Importantly, we find that the 
AUC grows in proportion to the size of data available for 
cross-validation, and that it be possible to obtain similar AUC 
value for datasets with opposite positive/negative ratios. In 
extreme case with many data in one class, we return to an 
argument similar to ACC on a dominantly negative dataset 
(Figures 3 and 6); it may be relatively trivial to build even a 
linear classifier that separate the dominant class from the 
infrequent class. Such result again suggest that careful 
interpretation of model performance be required, and 
that comparison of AUC value on datasets of differ 
source or size require caution. 

In addition to cross-validated AUC, it be possible to 
consider cross-validated MCC or F1 score. In the case of 
the latter metrics, they require the pre-specification of a 
threshold value which be compare against the raw output 
of the classifier, at which raw prediction result be 
discretized, and the metric be calculate from the result 
confusion matrix. We perform this additional analysis by 
set the threshold for MCC and F1 to the obvious value 
of 0, which mean to simply use the sign of the raw output 
correspond to which side of the SVM hyperplane an 
example be classify on. 

For both the GLASS and SARfari datasets, we compute 
a single model use a fix ratio of subsampled data at a 
give grid point, and evaluate external prediction on the 
remain portion. Experiments be do use ratio of 
0.33, 0.5, and 0.7, respectively reflect scenario where a 
majority of data be use for CV, where data be evenly 
split for CV and external prediction, and where data be 
reduce such that predictive performance on a much large 
external set could be evaluated. As an example, at an 
external data ratio of 50 %, where the imbalanced grid point 
contain 234 active and 1234 inactive ligand-target pair 
subsampled from the large database, 117 active and 617 

Methods Corner www.molinf.com 

© 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim Mol. Inf. 2018, 37, 1700127 (8 of 11) 1700127 

www.molinf.com 


1 
2 
3 
4 
5 
6 
7 
8 
9 

10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 

inactives form the external prediction set, and the 
remain data (of equal size) be use for 3-fold cross- 
validation. Hence each round of 3-fold CV could build 
model use 117*(2/3) = 78 active and 411 inactives, and 
predict on the remain 206 a well a obtain their raw 
value to use a threshold in ROC curve generation. Models 
use to evaluate the external set would be built on the full 
117 active and 617 inactives. Where the external ratio be 
0.7, for example, the model building be on 70 active and 
370 actives, with external prediction on 164 active and 864 
inactives. 

In the upper half of Figure 8, the cross-validated AUC 
and cross-validated MCC/F1 be evaluate over the grid of 
data size for the SVM with an RBF kernel, where the 
external split ratio be 0.5. The trend of AUC to grow with 
imbalance be clear, a well a a further increase in average 
AUC a the number of positive and negative exponen- 
tially grow. The MCC also grows a the data available for 
cross-validation grows, but unlike AUC, MCC performance 
at extreme be 0, and it be not until a moderate number of 
sample from both class be available that the MCC climb 

to a positive number. Interestingly, we see a dichotomy in 
F1-CV metric surface behavior, where F1 be potentially over- 
estimate for model with disproportionate number of 
actives, and like MCC, at or close to 0 for dominantly 
inactive datasets. Like MCC and AUC, F1 performance be 
appropriately high when both large number of active 
and inactives be included. 

Next, we consider how well each metric perform in 
cross-validation setting versus external prediction settings. 
The results, show in the bottom-left panel of Figure 8, 
suggest that cross-validated training performance accu- 
rately estimate external prediction performance, though 
admittedly, experience suggests that external performance 
be often low than training performances, which be 
observe for other parameter set tested. It may be the 
case that repeat trial nullified the effect of any one poor 
external prediction, and that the random sample select 
ligand-target pair from similar distributions. 

In practical situation of prospective prediction, a single 
threshold must be apply to generate the list of prospec- 
tive validation to execute, and so while AUC be computable 

Figure 8. Cross-validation, external prediction, metrics, and their cross-correlations. 

Methods Corner www.molinf.com 

© 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim Mol. Inf. 2018, 37, 1700127 (9 of 11) 1700127 

www.molinf.com 


1 
2 
3 
4 
5 
6 
7 
8 
9 

10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 

in a retrospective setting, it be not transferrable to prospec- 
tive settings. To address this, we checked the correlation 
between cross-validated AUC and MCC at the baseline 
threshold of 0. No correlation be observed, a show in 
the bottom-middle panel of Figure 8. This be concordant 
with another recent study examine correlation on 
artificially generate data.[16] 

We further considered, however, that it be possible to 
select a single point on the ROC curve which contains a 
permissible false discovery rate, and use that threshold a 
the criterion for MCC or F1 evaluation. In other words, if we 
determine by cross-validation the threshold that maximize 
the true positive rate up to the extent we tolerate error, 
then that could serve a the threshold in our prospective 
application. Solving the threshold value for a tolerable false 
discovery rate of 10 %, we then apply the F1 and MCC 
metric with such threshold, and again examine the 
relationship between cross-validated AUC and external 
prediction. As show in Figure 8 (bottom-middle), this 
correlation be also weak, though the thresholded MCC 
slightly good correlate to AUC than default-thresholded 
MCC. Hence, generate expectation of success base on 
multi-threshold AUC may fail to correlate with single- 
threshold ACC/MCC/etc in post-experiment, prospective 
performance analyses. 

Finally, at a large scale, we ask what the correlation 
of cross-validated AUC would be with external AUC, of CV– 
MCC with external MCC, of CV-AUC with external MCC, and 
of CV-AUC with thresholded MCC, by iterate test over a 
small parameter grid and across both datasets (4 SVM 
parameter set * 3 external ratio * 2 datasets), and ask 
what the correlation of these pair be at each grid point. 
That is, correlation metric be use to ass the fit 
between a pair of metric use all point of the positive/ 
negative subsampling grid and result metrics. Correla- 
tions be compute not only by Pearson correlation but 
also by alternatively force the regression line to pas 
through the origin, a propose previously by Golbraikh 
and Tropsha.[26] 

As can be expected, the distribution of Pearson 
correlation be high than the origin-fitting correlations, 
a show in the bottom-right panel of Figure 8. Importantly, 
we see that when base decision on the correlation 
obtain in the more stringent correlation fit, the AUC-AUC 
and MCC–MCC metric pair emerge with the strong 
correlation. Yet, a argue above, use the entire range of 
raw prediction value for the external set to generate an 
AUC be only applicable in retrospective scenarios. Therefore 
by elimination, it would appear that cross-validated MCC be 
the most reliable method to estimate prospective perform- 
ance on a similar-sized external prediction. 

4 Conclusion and Future Outlook 

Using the tandem of metric surface and iCDFs, we can 
improve our understand of the consequence of select- 
ing a particular metric in order to evaluate predictive 
performance of two-class discriminants. We find that TPR, 
TNR, or ACC alone run the risk of deception; the physicist 
Richard Feynman once remark that “… you must not fool 
yourself, and you be the easy person to fool.” 

Metrics such a MCC or F1 score provide more realistic 
estimate of real-world model performance, and in imbal- 
anced datasets skewed toward negative data, PPV might be 
indicative of expect discovery rates. The practical chemo- 
genomic virtual screen experiment show a lack of 
correlation between MCC and F1, and between AUC and 
MCC. Taken together, the best advice we can suggest be to 
take a multi-metric view of molecular modeling, and place 
the model task context at the center of metric interpreta- 
tion. 

Though the MCC might appear to be the “best” metric 
available by include all four type of result from the 
confusion matrix, several prediction problem do not lend 
themselves to use of the metric. For example, evaluate 
prediction of DNA sequence variant from next-generation 
sequence frequently lead to focus on TP, FP, and FN 
results; though it be technically possible to count the total 
number of nucleotide that be correctly not call 
variants, this number would be cosmological compare to 
the other three raw count in the confusion matrix, and it 
therefore stand to reason to use TPR, PPV, or their 
combination in the form of the F1 score or more general F- 
measure. 

What can the scientific community do in order to close 
the gap between compute prediction expectation and 
experimental validation success rates? The thorough consid- 
eration of metric prior to the execution of a study be 
certainly a crucial element. If a new classification evaluation 
metric be suggested, then it surface should be visualize 
and it iCDF should be compare to others such a the 
one give in this report. A high probability of obtain a 
particular metric value a assess by iCDF should be cross- 
reference with it surface visualization to determine if 
there be a risk or element of deception result from many 
possible way to arrive at the metric value in question (i. e., 
there be many way to achieve ACC of at least 0.6 in 
inbalanced data). Computational experiment may consider 
the philosophy discuss herein to perform repeat 
execution of a model-predict experiment such that less 
than half of the data per class be subsampled for model 
selection with the majority remainder use for prediction. 
Recent model method have show that often only a 
fraction of a dataset be sufficient to build a predictive 
model.[25,27,28] As in the prior studies, if distribution of 
prediction performance can be show to be normally 
distribute by the Kolmogorov-Smirnov test, we can 

Methods Corner www.molinf.com 

© 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim Mol. Inf. 2018, 37, 1700127 (10 of 11) 1700127 

www.molinf.com 


1 
2 
3 
4 
5 
6 
7 
8 
9 

10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
53 
54 
55 
56 

consider use such a fact to forecast the chance of success 
in a true prospective experiment. 

Finally, we remark that the discussion here be contain 
to two-class prediction, while some molecular informatics 
model task may need to classify an instance amongst 
three or more class (i. e., protein sequence secondary 
structure prediction, or prediction of ligand a strong/ 
weak/intermediate). In these cases, the visualization might 
be expand to a 3D voxel representation, but for four or 
more classes, view of subspace of the metric space be 
required. A well-known metric for the multi-class discrim- 
inant be Cohen’s Kappa Coefficient.[29] The iCDF concept 
introduce herein could be apply by shift from a 
matrix (2-tensor) to a generalize tensor yield by the 
computation of all per-class prediction rates. The iCDF 
would be continue to be influence by the data ratio, 
where in the generalize case a partition of data class 
ratio must be provided. 

Supplementary Data 

Animations of the relationship between data ratio and 
metric surface, and between data ratio and iCDF be 
available online. A fully executable standalone tool for 
surface generation be also provided. Stochastic selection and 
practical chemogenomic model AUC result can also be 
retrieved. Results analogous to Figure 8 for the Kinase 
SARfari dataset be provided, and observation of correla- 
tions between CV-AUC and MCC/MCC(t)/F1 for many 
parameter set be make available. 

Conflict of Interest 

None declared. 

Acknowledgements 

The author wish to thank G. Schneider, M. Vogt, C. Rakers, 
and D. Reker for helpful discussion during the develop- 
ment of this work. Computational resource be support 
by grant 16H06306 and 17 K20043 from the Japanese 
Society for the Promotion of Science, and by the Kyoto 
University Ishizue research support program. 

References 

[1] G. Schneider, Nat. Rev. Drug Discovery 2010, 9, 273–276. 
[2] J. Bajorath, M. L. Barreca, A. Bender, R. Bryce, M. Hutter, C. 

Laggner, C. Laughton, Y. Martin, J. Mitchell, A. Padova, et al., 
Future Med. Chem. 2011, 3, 909–21. 

[3] D. E. Clark, Expert Opin. Drug Discovery 2008, 3, 841–851. 
[4] P. Ripphausen, B. Nisius, L. Peltason, J. Bajorath, J. Med. Chem. 

2010, 53, 8461–8467. 
[5] P. Ripphausen, B. Nisius, J. Bajorath, Drug Discovery Dev. 2011, 

16, 372–376. 
[6] P. Ripphausen, D. Stumpfe, J. Bajorath, Future Med. Chem. 

2012, 4, 603–613. 
[7] C. W. Chu, J. D. Holliday, P. Willett, Bioorganic Med. Chem. 2012, 

20, 5366–5371. 
[8] C.-W. Chu, J. D. Holliday, P. Willett, J. Chem. Inf. Model. 2009, 49, 

155–161. 
[9] A. Abdo, B. Chen, C. Mueller, N. Salim, P. Willett, J. Chem. Inf. 

Model. 2010, 50, 1012–1020. 
[10] B. Chen, R. F. Harrison, G. Papadatos, P. Willett, D. J. Wood, X. Q. 

Lewell, P. Greenidge, N. Stiefl, J. Comput.-Aided Mol. Des. 2007, 
21, 53–62. 

[11] G. Jurman, S. Riccadonna, C. Furlanello, PLoS One 2012, 7, 
e41882. 

[12] J. C. D. Lopes, F. M. do Santos, A. Martins-Jos�, K. Augustyns, 
H. De Winter, J. Cheminform. 2017, 9, 7. 

[13] J. Luo, M. Schumacher, A. Scherer, D. Sanoudou, D. Megherbi, 
T. Davison, T. Shi, W. Tong, L. Shi, H. Hong, et al., Pharmacoge- 
nomics J. 2010, 10, 278–291. 

[14] P. Baldi, R. Nasr, J. Chem. Inf. Model. 2010, 50, 1205–1222. 
[15] S. J. Swamidass, C.-A. Azencott, K. Daily, P. Baldi, Bioinformatics 

2010, 26, 1348–1356. 
[16] S. Boughorbel, F. Jarray, M. El-Anbari, PLoS One 2017, 12, 

e0177678. 
[17] C. Lopez-Sambrooks, S. Shrimal, C. Khodier, D. P. Flaherty, N. 

Rinis, J. C. Charest, N. Gao, P. Zhao, L. Wells, T. A. Lewis, et al., 
Nat. Chem. Biol. 2016, 12, 1023–1030. 

[18] B. Severyn, T. Nguyen, M. D. Altman, L. Li, K. Nagashima, G. N. 
Naumov, S. Sathyanarayanan, E. Cook, E. Morris, M. Ferrer, 
et al., J. Biomol. Screening 2016, 21, 989–997. 

[19] S. Van Der Walt, S. C. Colbert, G. Varoquaux, 2011, DOI 10.1109/ 
MCSE.2011.37. 

[20] J. D. Hunter, Comput. Sci. Eng. 2007, 9, 90–95. 
[21] C. Cortes, V. Vapnik, Mach. Learn. 1995, 20, 273–297. 
[22] L. Breiman, Mach. Learn. 2001, 45, 5–32. 
[23] W. K. B. Chan, H. Zhang, J. Yang, J. R. Brender, J. Hur, A. �zg�r, 

Y. Zhang, Bioinformatics 2015, 31, btv302-. 
[24] A. P. Bento, A. Gaulton, A. Hersey, L. J. Bellis, J. Chambers, M. 

Davies, F. A. Kr�ger, Y. Light, L. Mak, S. McGlinchey, et al., 
Nucleic Acids Res. 2014, 42, D1083–90. 

[25] D. Reker, P. Schneider, G. Schneider, J. Brown, Future Med. 
Chem. 2017, 9, 381–402. 

[26] A. Golbraikh, A. Tropsha, J. Mol. Graph. Model. 2002, 20, 269– 
76. 

[27] T. Lang, F. Flachsenberg, U. Von Luxburg, M. Rarey, J. Chem. Inf. 
Model. 2016, 56, 12–20. 

[28] C. Rakers, D. Reker, J. B. Brown, J. Comput. Aided Chem. 2017, 8, 
124–142. 

[29] J. Cohen, Educ. Psychol. Meas. 1960, 20, 37–46. 

Received: October 26, 2017 
Accepted: January 3, 2018 

Published online on January 23, 2018 

Methods Corner www.molinf.com 

© 2018 Wiley-VCH Verlag GmbH & Co. KGaA, Weinheim Mol. Inf. 2018, 37, 1700127 (11 of 11) 1700127 

www.molinf.com 

