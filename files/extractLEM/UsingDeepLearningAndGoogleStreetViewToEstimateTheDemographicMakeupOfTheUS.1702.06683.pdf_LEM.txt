


















































Using Deep Learning and Google Street View to Estimate the De- 
mographic Makeup of the US 

Timnit Gebru1, Jonathan Krause1, YilunWang1, Duyun Chen1, Jia Deng2, Erez Lieberman Aiden3,4, 

Li Fei-Fei1 

1Stanford University, 353 Serra Mall, Stanford, CA 94305. 

2University of Michigan, 2260 Hayward Street, Ann Arbor, MI 48109. 

3Baylor College of Medicine, 1 Baylor Plaza, Houston, TX 77030. 

4Rice University, 6100 Main Street, Houston, TX 77005. 

The United States spends more than $1B each year on initiative such a the Amer- 

ican Community Survey (ACS), a labor-intensive door-to-door study that measure 

statistic relate to race, gender, education, occupation, unemployment, and other 

demographic factor 1. Although a comprehensive source of data, the lag between 

demographic change and their appearance in the ACS can exceed half a decade. As 

digital imagery becomes ubiquitous and machine vision technique improve, automate 

data analysis may provide a cheaper and faster alternative. Here, we present a method 

that determines socioeconomic trend from 50 million image of street scenes, gath- 

ered in 200 American city by Google Street View cars. Using deep learning-based 

computer vision techniques, we determine the make, model, and year of all motor 

vehicle encounter in particular neighborhoods. Data from this census of motor vehi- 

cles, which enumerate 22M automobile in total (8% of all automobile in the US), be 

use to accurately estimate income, race, education, and voting patterns, with single- 

precinct resolution. (The average US precinct contains approximately 1000 people.) 

The result association be surprisingly simple and powerful. For instance, if the 

1 

ar 
X 

iv 
:1 

70 
2. 

06 
68 

3v 
2 

[ 
c 

.C 
V 

] 
2 

M 
ar 

2 
01 

7 



number of sedan encounter during a 15-minute drive through a city be high than 

the number of pickup trucks, the city be likely to vote for a Democrat during the next 

Presidential election (88% chance); otherwise, it be likely to vote Republican (82%). 

Our result suggest that automate system for monitoring demographic trend may 

effectively complement labor-intensive approaches, with the potential to detect trend 

with fine spatial resolution, in close to real time. 

For thousand of years, ruler and policymakers have survey national population in order to 

collect demographic statistics. In the United States, the most detailed such study be the American 

Community Survey (ACS), which be perform by the US Census Bureau at a cost of approximately 

$250M per year 2. Each year, ACS report demographic result for all city and county with a 

population of 65,000 or more 3. However, due to the labor-intensive data gathering process, small 

region be interrogate less frequently, every three or five years. Thus, the most recent edition of 

the ACS (the 2015 release) contains data collect in 2011 for some zip code and 2015 for others, 

reflect a half decade lag between some ACS result and current demographics. Moreover, the 

date of data collection for two region can differ by up to five years, limit the reliability of so- 

cioeconomic comparisons. Taken together, such lag can impede effective policymaking, suggest 

that the development of alternative and complementary approach would be desirable. 

In recent years, computational method have emerge a a promising tool for tackle difficult 

problem in social science. For instance, Antenucci et al. have predict unemployment rate from 

Twitter 4; Michel et al. have analyze culture use large quantity of text from book 5; and 

Blumenstock et al. use mobile phone metadata to predict poverty rate in Rwanda 6. These 

result suggest that socioeconomic studies, too, might be facilitate by computational methods, 

with the ultimate potential of analyze demographic trend in great detail, in real time, and at a 

fraction of the cost. 

2 



Here, we show that it be possible to determine socioeconomic statistic and political preference 

in the US population by combine publicly available data with machine learn methods. Our 

procedure only require labor-intensive survey data for a handful of city to create nationwide 

estimates. This approach allows for more frequent measurement of demographic information at a 

high spatial resolution. 

Specifically, we analyze 50 million image take by Google Street View car a they drove through 

200 cities, neighborhood-by-neighborhood and street-by-street. We focus on the motor vehicle 

encounter during this journey because over 90% of American household own a motor vehicle 7, 

and because their choice of automobile be influence by disparate demographic factor include 

household needs, personal preferences, and economic wherewithal 8. 

We demonstrate that, by deploy a machine vision framework base on deep learn - specif- 

ically, convolutional neural network - it be possible to not only recognize vehicle in a complex 

street scene, but to reliably determine a wide range of vehicle characteristics, include make, 

model, and year. Whereas many challenge task in machine vision (such a photo tagging) be 

easy for humans, the fine-grained object recognition task we perform here be one that few people 

could accomplish for even a handful of images. Differences between car can be imperceptible to 

an untrained person; for instance, some car model can have subtle change in tail light (e.g., 2007 

Honda Accord vs. 2008 Honda Accord) or grille (e.g., 2001 Ford F-150 Supercrew LL vs. 2011 

Ford F-150 Supercrew SVT). Nevertheless, our system be able to classify automobile into one of 

2,657 categories, take 0.2 second per vehicle image to do so. While it classify the automobile 

in 50 million image in 2 weeks, a human expert, assume 10 second per image, would take more 

than 15 year to perform the same task. Using the classify motor vehicle in each neighborhood, 

we infer a wide range of demographic statistics, socioeconomic attributes, and political preference 

3 



Fig. 1. We perform a vehicular census of 200 city in the United States use 50 million Google Street View images. In each image, we detect car 

with computer vision algorithm base on deformable part model (DPM) and count an estimate 22 million cars. We then use convolutional neural 

network (CNN) to categorize the detect vehicle into one of 2,657 class of cars. For each type of car, we have metadata such a the make, model, 

year, body type and price of the car in 2012. 

4 



of it residents. 

In the first step of our analysis, we collect 50 million Google Street View image from 3,068 

zip code and 39,286 voting precinct span 200 US city (Fig. 1). Using these image and 

annotate photo of cars, our object recognition algorithm (a “Deformable Part Model” 9) learn 

to automatically localize motor vehicle on the street 10 (see Methods). We successfully detect 

22 million distinct vehicles, comprise 32% of all the vehicle in the 200 city we studied, and 

8% of all vehicle in the United States. After localize each vehicle, we deployed Convolutional 

Neural Networks 11,12, the most successful deep learn algorithm to date for object classification, 

to determine the make, model, body type, and year of each vehicle (Fig. 1). Specifically, we be 

able to classify each vehicle into one of 2,657 fine-grained categories, which form a nearly exhaustive 

list of all visually distinct automobile sell in the US since 1990 (Fig. 1). For instance, our model 

accurately identify car (identifying 95% of such vehicle in the test data), van (83%), minivan 

(91%), SUVs (86%), and pickup truck (82%). See Fig. S1. 

Using the result motor vehicle data, we estimate demographic statistic and voter preference 

a follows. For each geographical region we examine (city, zip code, or precinct), we count the 

number of vehicle of each make and model that be identify in image from that region. We 

also include additional feature such a aggregate count for various vehicle type (trucks, vans, 

SUVs, etc.), the average price and fuel efficiency, and the overall density of vehicle in the region 

(see Methods). 

We then partition our dataset, by county, into two subset (Fig. 2). The first be a “training 

set”, comprise all region which lie mostly in a county whose name start with ‘A’,‘B’, or ‘C’ (such 

a Ada County, Baldwin County, Cabarrus County, etc.). This training set encompasses 35 of the 

200 cities, ∼ 15% of the zip codes, and ∼ 12% of the precinct in our data. The second be a “test 

5 



Fig. 2. We use all the city in county start with ‘A’ and ‘B’ (shown in purple on the map) to train a model estimate socioeconomic data from car 

attributes. Using this model, we estimate demographic variable at the zip code level for all the city show in green. We show actual vs. predict map 

for the percentage of Black, Asian and White people in Seattle, WA (i-iii), the percentage of people with less than a high school degree in Milwaukee, WI 

(iv) and the percentage of people with graduate degree in Milwaukee, WI (v). (vi) map the median household income in Tampa, FL. The ground truth 

value be mapped on the left column and our estimate result be on the right column. We accurately localize zip code with the high and low 

concentration of each demographic variable such a the three zip code in Eastern Seattle with high concentration of Caucasians, one Northern zip 

code in Milwaukee with highly educate inhabitants, and the least wealthy zip code in Southern Tampa. 

6 



set”, comprise all region in county start with the letter ‘D’ through ‘Z’ (such a Dakota 

County, Maricopa County, Yolo County). We use the test set to evaluate the model that result 

from the training process. 

Using US Census and Presidential Election voting data for region in our training set, we train 

a logistic regression model to estimate race and education levels, and a ridge regression model to 

estimate income and voter preference on the basis of the collection of vehicle see in a region. This 

simple linear model be sufficient to identify positive and negative association between the presence 

of specific vehicle (such a Hondas) and particular demographic (i.e., the percentage of Asians) 

or voter preference (i.e., Democrat). 

Our model detects strong association between vehicle distribution and disparate socioeconomic 

trends. For instance, several study have show that people of Asian descent be more likely to drive 

Asian car 13, a result we observe here a well: the two brand that most strongly indicate an Asian 

neighborhood be Hondas and Toyotas. Cars manufacture by Chrysler, Buick and Oldsmobile be 

positively associate with African American neighborhoods, which be again consistent with exist 

research 14. And vehicle like pickup trucks, Volkswagens and Aston Martins be indicative of 

mostly Caucasian neighborhoods. See Fig. S2. 

In some cases, the result association can be easily apply in practice. For example, the 

vehicular feature that be most strongly associate with Democratic precinct be sedans, whereas 

Republican precinct be most strongly associate with extended-cab pickup truck (a truck with 

rear-seat access). We found that by drive through a city for 15 minute while counting sedan and 

pickup trucks, it be possible to reliably determine whether the city vote Democratic or Republican: 

if there be more sedans, it probably vote Democrat (88% chance) and if there be more pickup 

trucks, it probably vote Republican (82% chance). See Fig. 3(a)iii. 

7 



As a result, it be possible to apply the association extract from our training set to vehicle 

data from our test set region in order to generate estimate of demographic statistic and voter 

preferences, achieve high spatial resolution in over 160 cities. To be clear, no ACS or voting data 

for any region in the test set be use to create the estimate for the test set. 

To confirm the accuracy of our demographic estimates, we begin by compare them with 

actual ACS data, city-by-city, across all 165 test set cities. We found a strong correlation between 

our result and ACS value for every demographic statistic we examined. (The r-values for the 

correlation were: median household income, r = 0.82; percentage of Asians, r = 0.87; percentage 

of Blacks, r = 0.81; percentage of Whites, r = 0.77; percentage of people with a graduate degree, 

r = 0.70; percentage of people with a bachelor’s degree, r = 0.58, percentage of people with some 

college degree, r = 0.62, percentage of people with a high school degree, r = 0.65; percentage of 

people with less than a high school degree, r = 0.54). See Fig. S3, S4 and S5. Taken together, 

these result show our ability to estimate demographic parameters, a assess by the ACS, use 

the automate identification of vehicle in Google Street View data. 

Although our city-level estimate serve a a proof-of-principle, zip code-level ACS data provide 

a much more fine-grained portrait of constituencies. To investigate the accuracy of our method 

at zip code resolution, we compare our zip code-by-zip code estimate to those generate by the 

ACS, confirm a close correspondence between our finding and ACS values. For instance, when 

we look closely at the data for Seattle, we found that our estimate of the percentage of people 

in each zip code who be Caucasian closely match the value obtain by the ACS (r = 0.84, 

p < 2e − 7). The result for Asians (r = 0.77, p = 1e − 6) and African Americans (r = 0.58, 

p = 7e− 4) be similar. Overall, our estimate accurately determine that Seattle, Washington be 

69% Caucasian, with African Americans mostly reside in a few Southern zip code (Fig. 2, i,ii). 

8 



As another example, we estimate educational background in Milwaukee, Wisconsin zip codes, 

accurately determine the fraction of the population with less than a high school degree (r = 0.70 

p = 8e − 5), with a bachelor’s degree (r = 0.83, p < 1e − 7), and with postgraduate education 

(r = 0.82, p < 1e− 7). We also accurately determine the overall concentration of highly educate 

inhabitant near the city’s North East border (Fig. 2, iv, v). Similarly, our income estimate closely 

match those of the ACS in Tampa, Florida (r = 0.87, p < 1e− 7). The low income zip code, at 

the southern tip, be readily apparent. 

While the ACS do not collect voter preference data, our automate machine learn procedure 

can infer such preference use association between vehicle and the voter that surround them. 

To confirm the accuracy of our voter preference estimates, we begin by compare them with the 

voting result of the 2008 Presidential election, city-by-city, across all 165 test set cities. We found a 

very strong correlation between our estimate and actual voter preference (r = 0.73, p << 1e− 7). 

See Fig. S5. These result confirm the ability of our approach to accurately estimate voter behavior 

during a Presidential election. 

While city-level data provide a general picture, precinct-level voter preference identify trend 

within a particular city. By compare our precinct-by-precinct estimate to the 2008 Presidential 

election results, we found that our estimate continued to closely match the ground truth data. For 

instance, in Milwaukee, Wisconsin, a very Democratic city with 311 precincts, we correctly classify 

264 precinct (85% accuracy (Fig. 3,b)). Most notably, we accurately determine that there be a few 

Republican precinct in the South, West and Northeastern border of the city. Similarly, in Gilbert, 

Arizona, a Republican city, we correctly classify 58 out of 60 precinct (97% accuracy), identify 

one out of the two small Democratic precinct in the city (Fig. 3,b). And in Birmingham Alabama, 

a city that be 23% Republican, we correctly classify 87 out of the 105 precinct (83% accuracy). 

9 



Fig. 3. (a) i. and ii map the actual and predict percentage of people who vote for Barack Obama in the 2008 presidential election (r=0.74). iii. map 

the ratio of detect pickup truck to sedan in the 165 city in our test set. As can be see from the map, the ratio be very low in Democratic city 

such a those in the East Coast and high in Republican city such a those in Texas and Wyoming. (b) Shows actual vs. predict voter affiliation for 

various city in our test set at the precinct level. Democratic precinct be show in blue and Republican precinct be show in red. Our model correctly 

classifies Casper, Wyoming a a Republican city and Los Angeles, California a a Democratic city. We accurately predict that Milwaukee, Wisconsin be 

a Democratic city except for a few Republican precinct in the Southern, Western and North Eastern border of the city. 

10 



Overall, there be a strong correlation between our estimate and actual electoral outcome at the 

single-precinct level (r = 0.57, p < 1e− 7). 

These result illustrate the ability of our machine learn algorithm to accurately estimate both 

demographic statistic and voter preference use a large database of Google Street View images. 

They also suggest that our demographic estimate be accurate at single-precinct level, which be 

high than the fine resolution available for yearly ACS data. Using our approach, zip code or 

precinct-level survey data collect for a few city can be use to automatically provide up-to-date 

demographic information for many American cities. 

Thus, we find that the application of fully automate computer vision method to publicly 

available street scene can inexpensively determine social, economic, and political trend in neigh- 

borhoods across America. By collect survey for a few city and infer data for others use 

our model, we can quickly determine demographic trends. 

As self-driving car with onboard camera become increasingly widespread, the type of data we 

use - footage of neighborhood from vehicle-mounted camera - be likely to become increasingly 

ubiquitous. For instance, Tesla vehicle currently take a many image a be study here every 

single day. It be also important to note that similar data can be obtained, albeit at a slow pace, 

use low-tech methods: for instance, by walk around a target neighborhood with a camera and 

a notepad. Thus, street scene stand in contrast to the massive textual corpus presently use in 

many computational social science studies, which be typically constrain by such serious privacy 

and copyright concern that individual researcher cannot obtain the raw data underlie any give 

publish analysis. 

Expanding our object recognition beyond vehicle 15, incorporate global image feature 16–19, 

other type of imagery, such a satellite image 20 and social network 4 could considerably strengthen 

11 



the present approach. Although such method could be powerful resource for both researcher and 

policymakers, their progress will raise important ethical concerns; It be clear that public data should 

not be use to compromise reasonable privacy expectation of individual citizens, and this will be a 

central concern move forward. In the future, such automate method could lead to estimate that 

be accurately update in real-time, dramatically improve upon the time resolution of a manual 

survey. This might allow early detection of important socioeconomic trends, such a recessions, 

give policymakers the ability to enact more effective measures. 

Materials and Methods 

Here, we describe our methodology for data collection, car detection, car classification and de- 

mographic inference. Some of these method be partially developed in an early paper 10 which 

serve a a proof of concept focus on a limited set of prediction (e.g. per caput carbon emission, 

Massachusetts department of vehicle registration data, income segregation). Our work build on 

these method to show that income, race, education level and voting pattern can be predict from 

car in Google Street View images. In the section below, we discus our dataset and methodology 

in more detail. 

Dataset While learn to recognize automobiles, a model need to be train with many image of 

vehicle annotate with category labels. To this end, we use Amzaon Mechanical Turk to gather a 

dataset of label car image obtain from edmunds.com, cars.com and craigslist.org. Our dataset 

consists of 2,657 visually distinct car categories, cover all commonly use automobile in the 

United States produce from 1990 onward. We refer to these image a product shot images. We 

also hire expert to annotate a subset of our Google Street View images. The annotation include 

a bound box around each car in the image and the type of car contain in the box. We partition 

12 



the image into training, validation, and test sets. In addition to our annotate images, we gather 

50 million Google Street View image from 200 cities, sample GPS point every 25 meters. We 

capture 6 image per GPS point, correspond to different camera rotations. Each Street View 

image have dimension 860 by 573 pixel and a horizontal field of view of approximately 90 degrees. 

Since the horizontal field of view be large than the change in viewpoint between the 6 image per 

GPS point, the image have some overlap content. In total, we collect 50,881,098 Google 

Street View image for our 200 cities. They be primarily acquire between June and December 

of 2013 with a small fraction (3.1%) obtain in November and December of 2014. See Supporting 

Information for more detail on the data collection process. 

Car Detection In computer vision, detection be the task of localize object within an image, and 

be most commonly frame a predict the (x, y, width, height) coordinate of an axis-aligned 

bound box around an object of interest. The central challenge for our work be design an 

object detector that be 1) fast enough to run on 50 million image within a reasonable amount of 

time, and 2) accurate enough to be useful for demographic inference. Our computation resource 

consist of 4 Tesla K40 GPUs and 200 2.1 GHz CPU cores. As discuss in 10, we be willing to 

trade a couple of percent in accuracy for a gain in efficiency. Thus, instead of use state-of-the-art 

object detection algorithm such a 21, we turn to the previous state of the art in object detection, 

deformable part model (DPMs) 22. 

For DPMs, there be two main parameter that influence the run time and performance, 

which be the number of component and the number of part in the model. Tab. S2 provide 

an analysis of the performance/time tradeoff on our data, measure on the validation set. Based 

on this analysis, use a DPM with a single component and eight part strike the right balance 

between performance and efficiency, allow u to detect car on all 50 million image in two weeks. 

13 



In contrast, the best perform parameter would have take two month to run and only increase 

average precision (AP) by 4.5. 

As discuss in 10, we also introduce a prior on the location and size of predict bound 

box and use it to improve detection accuracy. Incorporating this prior into our detection pipeline 

improves AP on the validation set by 1.92 at a negligible cost. Fig. S6(B) visualizes this prior. The 

output of our detection system be a set of bound box and score where each score indicates the 

likelihood of it associate box contain a car. 

We convert these score into estimate probability via isotonic regression 23. Isotonic regres- 

sion learns a probability for each detection score subject to a monotonicity constraint. Concretely, 

after sort n validation detection score s1, . . . , sn such that si ≤ si+1, and with yi a binary vari- 

able denote whether detection i be correct (has Jaccard similarity of at least 0.5 with a ground 

truth car bound box), isotonic regression solves the follow optimization problem: 

minimize 
p1,...,pn 

∑n 
i=1 ‖yi − pi‖22 

subject to pi ≤ pi+1, 1 ≤ i ≤ n− 1 
(1) 

Given a new detection score, a probability be estimate by linear interpolation of the pi. We plot 

the learn mapping from detection score to probability in Fig. S7A. 

We make a number of additional design choice while training and run this car detector 

in practice. First, we only detect car that be 50 pixel or great in width and height. The 

output of our detector be fed into the input of our car classifier. Thus, detect car need to have 

sufficient resolution and detail to enable the classifier to differentiate between 2,657 category of 

automobiles. Similarly, we train our detector use car with great than 50 pixel width and 

height. Our DPM be train on a subset of 13,105 bound boxes, reduce training time from a 

week (projected) to 15 hours. Using this subset instead of all ground truth bound box result 

14 



in negligible change in accuracy. 

We report number use a detection threshold of -1.5 (applied before the location prior). At test 

time, after apply the location prior (which lower detection decision values), we use a detection 

threshold of -2.3. This reduces the average number of bound box per image to be classify 

from 7.9 to 1.5 while only degrade AP by 0.6 (66.1 to 65.5) and decrease the probability mass of 

all detection in an image from 0.477 to 0.462 (a 3% drop). Fig. S8 show example of car detection 

use our model. Bounding box with car have high estimate probability whereas the opposite 

be true for those contain no cars. The AP of our final model (measured on the test set) be 65.7, 

and it precision recall curve be visualize in Fig. S7B. To calculate chance performance we use a 

uniform sample of bound box great than 50 pixel in width and height. 

Car Classification Our pipeline, described in 10, classifies automobile into one of 2,657 visually 

distinct category with an accuracy of 33.27%. We use a convolutional neural network 24 follow 

the architecture of 12 to categorize cars. CNNs, like other supervise machine learn methods, 

perform best when train on data from a similar distribution a the test data (in our case, Street 

View images). However, the cost of annotate Street View photo make it infeasible to collect 

enough image to train our CNN only use this source. Thus, we use a combination of Street View 

and the more plentiful product shot image a training data. We make a number of modification 

to the traditional CNN training procedure to good fit our setting. 

First, take inspiration from domain adaptation, we approximate the WEIGHTED method of 

Daumé 25 by duplicate each Street View image 10 time during training. This roughly equalizes 

the number of Street View and product shot image use for training, prevent the classifier from 

overfitting on product shot images. 

Another significant difference between product shot and Street View image be image resolution: 

15 



car in product shot image occupy a much large number of pixel in the image. To compensate 

for this difference, we first measure the distribution of bound box resolution in Street View 

image use for training. Then, during the training procedure, we dynamically downsize each 

input image accord to this distribution before rescale it to fit the input dimension of the 

CNN. Resolutions be parameterized by the geometric mean of the bound box width and height, 

and the probability distribution be give a a histogram over 35 different such resolutions. The 

large resolution be 256, which be the input resolution of the CNN. 

One further challenge while classify Street View image be that our input consists of noisy 

detection bound boxes. This stand in contrast to what would otherwise be the default for 

training a classifier – ground truth bound box that be tight around each car. To tackle this 

challenge, we first measure the distribution of the intersection over union (IOU) overlap between 

bound box produce by our car detector and ground truth box in the validation data. Then, 

we randomly sample the Street View image region input into the CNN accord to this IOU 

distribution. This simulates detection a input to the CNN and ensures that the classifier be 

train with similar image to those we encounter during testing. 

At test time, we input each detect bound box into the CNN and obtain softmax probability 

for each car category through a single forward pass. In practice, we only keep the top 20 predictions, 

since store a full 2, 657-dimensional float point vector for each bound box be prohibitively 

expensive in term of storage. On average, these top 20 prediction account for 85.5% of the softmax 

layer activations’ probability mass. We also note that, after extensive code optimization to make 

this classification step a fast a possible, we be primarily limited by the time spent reading image 

from disk, especially when use multiple GPUs to perform classification. At the most fine-grained 

level, classify into one of 2, 657 classes, we achieve a surprisingly high accuracy of 33.27%. We 

16 



classify the make and model of the car with 66.38% and 51.83% accuracy respectively. And we 

determine whether it be manufacture in or outside of the U.S. with 87.71% accuracy. 

We show confusion matrix for classify the make, model, body type and manufacturing 

country of the car (Fig. S9A,B,C,D). Body type misclassifications tend to occur among similar 

categories. For example, the most frequent misclassification for “coupe” be “sedan”, and the most 

frequent misclassification for truck with a regular cab be truck with an extend cab. On the other 

hand, there be no two make (such a Honda and Mercedes-Benz) that be more visually similar 

than others. Thus, when a car’s make be misclassified, it be mostly to a more popular make. The 

same be true for the manufacturing country. For instance, most error at the country level occur 

by misclassifying the manufacturing country a either “Japan” or “USA”, the two most popular 

countries. Due to the large number of classes, the only clear pattern in the model-level confusion 

matrix be a strong diagonal, indicative of our correct predictions. 

Demographic Estimation In all of our demographic estimation we use the follow set of 88 car- 

related attributes: The average number of detect car per image; Average car price; Miles per 

gallon (city and highway); Percent of total car that be hybrids; Percent of total car that be 

electric; Percent of total car that be from each of seven countries; Percent of total car that be 

foreign (not from the USA); Percent of total car from each of 11 body types; Percent of total car 

whose year (selected a the minimum of possible year value for the car) fall within each of five year 

ranges: 1990-1994, 1995-1999, 2000-2004, 2005-2009, and 2010-2014; Percent of total car whose 

make be each of 58 make in our dataset. 

Socioeconomic data be obtain from the American Community Survey (ACS) 3, and be 

collect between 2008-2012. See Supporting Information for more detail on ground truth data 

use in our analysis (e.g. census codes). Data for the 2008 U.S. presidential election be provide 

17 



to u by the author of 26 and consists of precinct-level vote count for Barack Obama and John 

McCain. For all of our analyses, we ignore vote cast for any other person, i.e. the count of total 

vote be determine solely by vote for Obama and McCain. 

To perform our experiments, we partition the zip codes, precinct and city in our dataset 

into training and test set a discuss in the main text, training a model on the training set and 

predict on the test set. We use a ridge regression model for income and voter affiliation esti- 

mation. For race and education estimation we use logistic regression to utilize structure inherent 

in the data. Specifically, for each region, sum the percentage of people with each of the 5 

possible educational background should yield 100%. Similarly, sum the percentage of people 

from each race in a particular location should result in 100%. In all case we train 5 model 

use 5-fold cross validation to select the regularization parameter. Our final model be the average 

of the 5 train models. We normalize the feature to have zero mean and unit standard deviation 

(parameters determine on the training set). We also clip prediction to stay within the range 

of the training data, prevent our estimate from have extreme values. In all experiments, we 

restrict the region of interest to be one with a population of at least 500 and at least 50 detect 

cars. 

We compute the probability of voting Democrat/Republican condition on be in a city with 

more pickup truck than sedan a follows. Let r be the ratio of pickup truck to sedans. We would 

like to estimate P (Democrat|r > 1) and P (Republican|r < 1). 

P (Democrat|r > 1) = P (Democrat, r > 1) 
P (r > 1) (2) 

P (Republican|r < 1) = P (Republican, r < 1) 
P (r < 1) (3) 

We estimate P (Democrat, r > 1), P (Republican, r < 1), P (r > 1) and P (r < 1) a follows. Let 

Sd = {ci} be the set of city with more vote for Barack Obama than Mitt Romney. Let Ss = {cj} 

18 



be the set of city with more sedan than pickup trucks. Let n be the number of element in Ss and 

let nd be the number of element in Sd ∩Ss. Similarly, let Sp be the set of city with more pickup 

truck than sedans, Sr the set of city with more vote for Mitt Romney than Barack Obama, and 

nrp the number of element in Sr ∩ Sp. Finally, let C be the number of city in our test set. 

P (Democrat, r > 1) ≈ nd 
C 

(4) 

P (Republican, r < 1) ≈ nrp 
C 

(5) 

P (r > 1) ≈ n 
C 

(6) 

P (r < 1) ≈ np 
C 

(7) 

Using these estimates, we calculate P (Democrat|r > 1) and P (Republican|r < 1) accord to 

equation 2 and 3. 

19 



References 

1. Department of Commerce, U.S Census Bureau. U.S. Census Bureau’s Budget Estimates 

(2013). URL http://www.osec.doc.gov/bmi/budget/FY14CJ/Census_FY_2014_CJ_Final_ 

508_Compliant.pdf. 

2. U.S Census Bureau. Fiscal Year 2017 Budget Summary (2017). URL https://www2.census. 

gov/about/budget/FY2017-census-budget-summary.pdf. 

3. American Community Survey 5 Year Data (2008-2012). http://www.census.gov/data/ 

developers/data-sets/acs-survey-5-year-data.html (2012). Accessed: 2014-9. 

4. Antenucci, D., Cafarella, M., Levenstein, M., Ré, C. & Shapiro, M. D. Using social medium to 

measure labor market flows. Tech. Rep., National Bureau of Economic Research (2014). 

5. Michel, J.-B. et al. Quantitative analysis of culture use million of digitize books. science 

331, 176–182 (2011). 

6. Blumenstock, J., Cadamuro, G. & On, R. Predicting poverty and wealth from mobile phone 

metadata. Science 350, 1073–1076 (2015). 

7. American Association of State Highway and Transportation Officials. Vehicle and transit avail- 

ability. Commuting in America 2013 7 (2013). URL http://traveltrends.transportation. 

org/Documents/B7_Vehicle%20and%20Transit%20Availability_CA07-4_web.pdf. 

8. Choo, S. & Mokhtarian, P. L. What type of vehicle do people drive? the role of attitude and 

lifestyle in influence vehicle type choice. Transportation Research Part A: Policy and Practice 

38, 201–222 (2004). 

20 

http://www.osec.doc.gov/bmi/budget/FY14CJ/Census_FY_2014_CJ_Final_508_Compliant.pdf 
http://www.osec.doc.gov/bmi/budget/FY14CJ/Census_FY_2014_CJ_Final_508_Compliant.pdf 
https://www2.census.gov/about/budget/FY2017-census-budget-summary.pdf 
https://www2.census.gov/about/budget/FY2017-census-budget-summary.pdf 
http://www.census.gov/data/developers/data-sets/acs-survey-5-year-data.html 
http://www.census.gov/data/developers/data-sets/acs-survey-5-year-data.html 
http://traveltrends.transportation.org/Documents/B7_Vehicle%20and%20Transit%20Availability_CA07-4_web.pdf 
http://traveltrends.transportation.org/Documents/B7_Vehicle%20and%20Transit%20Availability_CA07-4_web.pdf 


9. Felzenszwalb, P., Girshick, R., McAllester, D. & Ramanan, D. Object detection with discrimi- 

natively train part base models. Pattern Analysis and Machine Intelligence 32 (2010). 

10. Gebru, T. et al. Fine-grained car detection for visual census estimation. In AAAI (in press 

2017). 

11. LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learn apply to document 

recognition. Proceedings of the IEEE 86, 2278–2324 (1998). 

12. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep convolutional 

neural networks. In Advances in neural information processing systems, 1097–1105 (2012). 

13. The Spin. Asians flex their auto-buying horsepower. URL http://www.asianweek.com/2009/ 

09/03/the-spin-asians-flex-their-auto-buying-horsepower/. 

14. Auto Remarketing Staff. Which Brands Most Attract African-American 

Buyers? URL http://www.autoremarketing.com/content/trends/ 

which-brands-most-attract-african-american-buyers. 

15. Simo-Serra, E., Fidler, S., Moreno-Noguer, F. & Urtasun, R. Neuroaesthetics in fashion: Mod- 

eling the perception of beauty. In CVPR (2015). 

16. Ordonez, V. & Berg, T. L. Learning high-level judgment of urban perception. In European 

Conference on Computer Vision, 494–510 (Springer, 2014). 

17. Khosla, A., An, B., Lim, J. J. & Torralba, A. Looking beyond the visible scene. In 2014 IEEE 

Conference on Computer Vision and Pattern Recognition, 3710–3717 (IEEE, 2014). 

21 

http://www.asianweek.com/2009/09/03/the-spin-asians-flex-their-auto-buying-horsepower/ 
http://www.asianweek.com/2009/09/03/the-spin-asians-flex-their-auto-buying-horsepower/ 
http://www.autoremarketing.com/content/trends/which-brands-most-attract-african-american-buyers 
http://www.autoremarketing.com/content/trends/which-brands-most-attract-african-american-buyers 


18. Naik, N., Philipoom, J., Raskar, R. & Hidalgo, C. Streetscore–predicting the perceive safety of 

one million streetscapes. In 2014 IEEE Conference on Computer Vision and Pattern Recognition 

Workshops, 793–799 (IEEE, 2014). 

19. Zhou, B., Liu, L., Oliva, A. & Torralba, A. Recognizing city identity via attribute analysis of 

geo-tagged images. In European Conference on Computer Vision, 519–534 (Springer, 2014). 

20. Jean, N. et al. Combining satellite imagery and machine learn to predict poverty. Science 

353, 790–794 (2016). 

21. Ren, S., He, K., Girshick, R. & Sun, J. Faster r-cnn: Towards real-time object detection with 

region proposal networks. In Advances in neural information processing systems, 91–99 (2015). 

22. Felzenszwalb, P., Girshick, R., McAllester, D. & Ramanan, D. Object detection with discrimi- 

natively train part base models. Pattern Analysis and Machine Intelligence 32 (2010). 

23. Barlow, R. E., Bartholomew, D. J., Bremner, J. & Brunk, H. D. Statistical inference under 

order restrictions: the theory and application of isotonic regression (Wiley New York, 1972). 

24. LeCun, Y., Bottou, L., Bengio, Y. & Haffner, P. Gradient-based learn apply to document 

recognition. Proceedings of the IEEE 86, 2278–2324 (1998). 

25. Daumé III, H. Frustratingly easy domain adaptation. In Conference of the Association for 

Computational Linguistics (ACL) (Prague, Czech Republic, 2007). 

26. Ansolabehere, S., Palmer, M. & Lee, A. Precinct-Level Election Data. http://hdl.handle. 

net/1903.1/21919 (2014). Accessed: 2015-1. 

27. Gebru, T., Krause, J., Deng, J. & Fei-Fei, L. Scalable annotation of fine-grained category 

without experts. In CHI (in press 2017). 

22 

http://hdl.handle.net/1903.1/21919 
http://hdl.handle.net/1903.1/21919 


28. Deng, J. et al. Imagenet: A large-scale hierarchical image database. In Computer Vision and 

Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, 248–255 (IEEE, 2009). 

29. Sheng, V. S., Provost, F. & Ipeirotis, P. G. Get another label? improve data quality and data 

mining use multiple, noisy labelers. In Proceedings of the 14th ACM SIGKDD international 

conference on Knowledge discovery and data mining, 614–622 (ACM, 2008). 

30. Su, H., Deng, J. & Fei-Fei, L. Crowdsourcing annotation for visual object detection. In 

Workshops at the Twenty-Sixth AAAI Conference on Artificial Intelligence (2012). 

31. TIGER/Line - Geography - U.S. Census Bureau. https://www.census.gov/geo/maps-data/ 

data/tiger-line.html (2012). Accessed: 2014-11. 

23 

https://www.census.gov/geo/maps-data/data/tiger-line.html 
https://www.census.gov/geo/maps-data/data/tiger-line.html 


Supporting Information 

Image Data 

In this section, we provide additional detail on the methodology use to acquire annotate image 

data for our study. This data be require for two steps: to train computer vision model that 

detect and classify cars, and to apply these model on Street View image of city of interest. This 

section proceeds by detail how we obtain a comprehensive list of car categories, collect a 

large number of “product shot” image use to train our car classifier, gather 50 million Street 

View image use in our analysis, and annotate a subset for training and verify our model. We 

conclude with a complete description of the acquire metadata for each car category. 

Car Categories The first step in assemble a dataset of annotate car image be group car 

into set of visually indistinguishable classes. For example, while a 2003 Honda Accord coupe ex 

and a 2005 Honda Accord coupe l special edition be manufacture in different year and have 

different trim (ex v l special edition), their exterior look identical. Thus, these two car should 

be grouped into the same class. Ideally, the set of class would contain every type of car in common 

use. 27 present a workflow to perform this group at minimal cost. 

We first retrieve an initial list of 15,213 car type from the car website Edmunds.com, collect 

in August 2012. This form a generally complete list of all car commonly use in the United States 

that be produce from 1990 onward. Throughout this document we use the term “car” to refer 

to all type of automobile with four wheels, include sedans, coupes, trucks, vans, SUVs, etc., but 

not include e.g. semi-trucks or buses. 

As a first step toward group these category into a small number of visually distinct classes, 

use Amazon Mechanical Turk (AMT) to determine whether certain pair of the 15k car type be 

24 



distinguishable. Within each task we give six pair of category and the user be prompt to 

determine 1) if the two class have any visual differences, and 2) if they be different, on which 

part they differed. Within each task we have two pair for which we already knew the correct 

answer (as determine by hand), and we require that each user on AMT get the answer for those 

pair correct in order to count their response. Photos for this task be acquire from the handful 

of example image that Edmunds.com provides. The author clean up the data by hand, result 

in 3,141 category of cars, with extremely subtle difference between these fine-grained categories. 

Product Shot Images After assemble a list of category consist of visually indistinguishable 

set of cars, we collect training image for each class. These be annotate image contain the 

car of interest. A commonly use method in the computer vision community be to perform web 

image search for each category and cleanup the query image by hand to ensure that they contain 

the category of interest 28. However, the large number of class in our dataset make it infeasible 

to manually perform this task. 

In order to collect training data in a scalable manner, we leveraged e-commerce websites. We 

crawl image from cars.com and craigslist.org, two site where user be heavily incentivized to 

list the exact type of car they be selling. While these user be not necessarily car experts, they 

have detailed knowledge about their own car. In the case of cars.com, car category be represent 

in a very structure format. Thus, after establish a mapping between our category and their 

format, we be able to simply scrape image for each category. For craigslist.org, we scrap post 

from the “cars+trucks” listing of a variety of U.S. regions, and parse the post title to determine 

which of our category the post belong to. Since these image be from website with the purpose 

of sell cars, we call them “product shot” images. 

Some product shot image show the car from an extremely close-up angle. Others only depict 

25 



the interior of the car. Since our purpose be to recognize car in Google Street View images, our 

training set should have car from view point that can appear in Street View. Thus, we filter 

out image which do not contain one central automobile, with it exterior depict in it entirety. 

Since this task be relatively simple, we crowdsourced it via AMT, use 29 for quality control. 

In the final annotation step, we collect a bound box (an axis-aligned rectangle tightly en- 

closing the object of interest) around the car in each image. This ensures that our car classifier be 

train use visual information only from the car itself and not extraneous background. Bounding 

box be collect use the label methodology and UI of 30, but without the step for deter- 

mining if there be more than one car in the image. That step be not necessary because the output 

of the previous AMT task ensures that each image contains exactly one prominent car. 

Since some type of car have many more image than others, we stop annotate image for 

each category after collect 200 label photos. Our goal be to build a model that can recognize a 

many type of car a possible. Given our limited budget, it be more important to collect annotation 

for category with few label image than for those with many annotate photos. 

In the final step, we remove category that do not have at least three disparate source of data 

per class. We define one source of data a one post on any of the website we used. This process 

result in our final dataset consist of 2,657 car categories. 

Street View Images This section outline our methodology for collect approximately 50 million 

Google Street View image and annotate a subset of them to train our car detector and classifier. 

The process include select GPS (latitude, longitude) point of interest, collect image for each 

of these points, enclose car in a subset of these image with bound boxes, and annotate the 

type of car contain in each box. The final step be perform by car experts. 

26 



Selecting GPS Points Before gathering Google Street View images, we first have to determine 

which geographical (latitude, longitude) point we want to collect photo for. We call each latitude, 

longitude pair a GPS point. First, we select 200 city for our analysis. These be the two large 

city in each state and the next 100 large city in the United States a determine by population 

(see Tab. S1 for a complete list). For each city, we sample potential point of interest within a 

square grid of length 20km, center on one point know to lie within the city. There be a 25 meter 

space between points. We reverse geocode each of these point to determine whether they lie 

within the city of interest and how far away they be to the near road. We keep all point within 

12.5 meter of the near road. This process do not provide full coverage for a handful of cities. 

Thus, we augment these point with GPS sample from road data provide by the U.S. Census 

Bureau 31. 

Sampling Images from Street View For each GPS point, we attempt to sample 6 image from 

Google Street View, one for each of 6 different camera rotations. This be do via browser emu- 

lation and require only the latitude and longitude of each point. However, we cannot immediately 

use photo retrieve with this process a they appear warped: an equirectangular projection be ap- 

ply to image in a spherical panorama. We apply the reverse transformation before all subsequent 

task use the images. 

Annotations on Amazon Mechanical Turk While our product shot image can be use to train 

a car classifier, we cannot utilize them to train a car detector: a model that learns to localize all 

the car in an image. This be because all of our product shot image include only one prominently 

feature car in each image. 

Using the system of 30, we collect bound box annotation in a subset of our Street View 

27 



images. To increase the efficiency of this process, we first filter out all image contain either 

zero or more than 10 car via AMT, use the same interface and pipeline described in the section 

pertain to product shot images. A randomly select subset of 399,331 Street View image be 

annotate in this manner. We found that 26.6% of image be annotate a have no visible car 

and 12.4% have more than 10 cars. The distribution of the number of car in the remain image 

be show in Fig. S6A. 

Fig. S6B plot bound box size versus location. Cars locate closer to the bottom of the image 

tend to occupy more space than those near the top. This agrees with the intuition that car low 

in the image be closer to the camera and therefore appear larger. Similarly, Fig. S6C show a 

heatmap of bound box location for car in Street View. Most automobile be locate near the 

horizon line because that part of the image occupies more 3D space, i.e., more space in the real 

world. There be a sharp dropoff in the distribution of car above the horizon line. 

Expert Class Annotations To learn to recognize automobile in Street View images, a classifier 

need to be train with car from these images. To this end, we label a subset of the bound 

box from Street View image with the type of car contain in them. This annotate data also 

enables u to quantitatively evaluate how well our classifier works. In contrast to product shot 

images, we do not know the type of car contain in Street View photos. Therefore, we hire 

expert car annotator to label these images. Experts be primarily solicit via Craigslist ads. 

Those who be interested in perform our task be first ask to annotate car in Street View 

image for one hour, and only those who could annotate at a speed of 1 car per minute and a 

precision of at least 80% be allow to annotate further. 110 expert human annotator work 

for a total of approximately two thousand hour to label our images. 

Very small image typically do not contain enough visual information to discriminate fine level 

28 



of detail. Thus, annotator be only show car in bound box whose height exceed 50 pixels. 

32.89% of bound box in our dataset fulfill this criteria. The annotation task itself proceed 

hierarchically: Fig. S10 show the user interface for the task. Given a Street View bound box, 

annotator be first ask to select the make of the car (Fig. S10(A)). They be then present 

with a list of body type for the chosen make (Fig. S10(B)). After select the right body type, 

expert be show a list of option for the car model, and finally, the trim and year associate 

with each model. 

Since difference between category can be extremely subtle at that final level, we also provide 

example image from each trim and year group for the annotator’s benefit (Fig. S10(C)). At any 

point in the process, the annotator could declare that he or she do not have enough information 

to make a selection. Thus, each label at this fine level of detail represent a confident selection 

by a car expert. We collect a total of 69,562 car category annotation in this manner. 

Car Metadata In addition to the images, category labels, and bound boxes, we also have meta- 

data pertain to each class, list below. 

• Make: The make of the car, of 58 possible makes. The make we consider are: Acura, AM 

General, Aston Martin, Audi, Bentley, BMW, Buick, Cadillac, Chevrolet, Chrysler, Dae- 

woo, Dodge, Eagle, Ferrari, Fiat, Fisker, Ford, Geo, GMC, Honda, Hummer, Hyundai, In- 

finiti, Isuzu, Jaguar, Jeep, Kia, Lamborghini, Land Rover, Lexus, Lincoln, Lotus, Maserati, 

Maybach, Mazda, McLaren, Mercedes-Benz, Mercury, Mini, Mitsubishi, Nissan, Oldsmobile, 

Panoz, Plymouth, Pontiac, Porsche, Ram, Rolls-Royce, Saab, Saturn, Scion, Smart, Subaru, 

Suzuki, Tesla, Toyota, Volkswagen, and Volvo. 

• Model: The model of the car, of 777 possible models. 

29 



• Year: The manufacturing year of the automobile. Since car might not change appearance 

over a small number of years, this be typically list a a range of years. The minimum year 

in our dataset be 1990, and the maximum year be 2014. 

• Body Type: The body type of the car. The 11 possible value are: convertible, coupe, 

hatchback, minivan, sedan, SUV, truck (regular-sized cab), truck (extended cab), truck (crew 

cab), wagon, and van. 

• Country: The manufacturing country of the automobile. The 7 possible country are: Eng- 

land, Germany, Italy, Japan, South Korea, Sweden, and USA. 

• Highway MPG: The typical mile per gallon of the car when driven on highways. If a class 

contains car with multiple years, it be annotate with the highway MPG of the old car in 

the group. 

• City MPG: The typical mile per gallon of the car when driven on non-highway streets. 

• Price: the price of the car in 2012. 

This metadata be acquire via Edmunds.com in August 2012, with some miss data (a handful 

of car prices) fill in by car expert afterward. In case where a class consists of multiple visually 

indistinguishable type of cars, it be annotate with the metadata of the old car in the set. 

Dataset Summary Tab. S3 provide a summary of the annotation collect for both product shot 

and Street View images, which we split into training (50%), validation (10%), and test (40%) set 

for use in training our car detector and classifier. 

30 



Demographic Data 

Income Data for median household income be obtain from the American Community Survey 

(ACS) 3, and be collect between 2008-2012. We use census variable B19013_001E, “Median 

household income in the past 12 month (in 2013 inflation-adjusted dollars)”. 

Education Education data be also obtain from the ACS 3. Education level be split into the 

follow mutually exclusive category (census code in parentheses): 

• Less than high school graduate (B06009_002E) 

• High school graduate (includes equivalency) (B06009_003E) 

• Some college or associate’s degree (B06009_004E) 

• Bachelor’s degree (B06009_005E) 

• Graduate or professional degree (B06009_006E) 

Race Racial demographic data be also obtain from the ACS 3, and corresponds to census 

code B02001_002E (“White alone”), B02001_003E (“Black or African American alone”), and 

B02001_005E (“Asian alone”). 

Voting Data for the 2008 U.S. presidential election be provide to u by the author of 26 and 

consists of precinct-level vote count for Barack Obama and John McCain. For all of our analyses, 

we ignore vote cast for any other person, i.e. the count of total vote be determine solely by vote 

for Obama and McCain. 

Obama receive great than 50% of the vote in most of the precinct in our dataset. This can 

partially be attribute to the fact that he won the popular vote in the 2008 election. Precincts 

31 



in our dataset be also locate in major city which favor candidate from the Democratic party. 

Interestingly, Obama receive an extremely high percentage (≥ 95%) of the vote in many precinct 

in our dataset. A large portion of these precinct have high concentration of African Americans, 

who overwhelmingly vote for him during the 2008 election. 

32 



a b c 

Confusion matrix show the accuracy with which we classify various car attributes. Fig. S1. Confusion matrix show the accuracy with which we classify various car attribute such a type of vehicle in a, whether or not it be domestic 

in b, and it price in c. 

Fig. S2. Bar plot show the top 10 car feature with high positive weight in our race estimation model. 

33 



Fig. S3. Scatter plot of ground truth income and race value v our estimations. Also show on each plot be the line y=x which corresponds to a perfect 

predictor. 

34 



Fig. S4. Scatter plot of ground truth data v our estimation of educational attainment. Also show on each plot be the line y=x which corresponds to 

a perfect predictor. 

35 



Fig. S5. Scatter plot of ground truth data show the percentage of people with a graduate school degree v our estimations, and the percentage of 

people who vote for Barack Obama in the 2008 presidential election v our estimations. Also show on each plot be the line y=x which corresponds to 

a perfect predictor. 

36 



Fig. S6. (A) Histogram of the number of car annotate in the Street View images, represent by the number of annotate bound box in each 

image. Images include in these number be those image annotate a contain more than zero and less than 11 cars. (B) Bounding box position 

v log(area). Each point corresponds to a single bound box in our training set of Street View images, and the color corresponds to the log of the 

number of pixel in the bound box. (C) Bounding box position v frequency. The color of each pixel indicates the number of bound box in the 

training set which overlap with that pixel. 

−4 −3 −2 −1 0 1 2 
0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

Detection score 

Es 
tim 

at 
ed 

P 
ro 

ba 
bi 

lit 
y 

Probability Calibration 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 
0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

Recall 

Pr 
ec 

be 
io 

n 

Detection Precision/RecallA B 

Fig. S7. A. The transformation from detection score to the probability of the detection be correct (i.e. probability of correctly detect a car), learn 

with isotonic regression on the validation set. B. Precision/recall curve for our final detection model on the test set. 

0.6327590.394558 0.2363530.0954128 

0.0470297 

0.762575 
0.6327590.450237 

0.0470297 

0.632759 0.532939 

Fig. S8. Example detection with our model on our test set. Shown in the box around each detection be our estimate probability of the detection 

have intersection over union great than 0.5, i.e. count a correct during detection evaluation. 

37 



A B 

C D 

Fig. S9. Confusion matricies of predictions. The entry in row i and column j indicates how many time ground truth attribute i be classify a attribute 

j. The attribute be A. the make of the car, B. the manufacturing country of the car, C. the model of the car, and D. the body type of the car. 

38 



A B 

C 

Fig. S10. Screenshots of the user interface for hierarchically annotate Street View image with car categories. A. The expert be first ask to identify 

the make. B. The next step in the task be to identify the body type of the car which be call “submodel” in the task. C. Once the body type be identify 

we provide a list of class for the select make and body type. Example image of each class be also show to aid the user in identification. 

39 



City # Im. City # Im. City # Im. City # Im. 

Birmingham, AL 484,818 Santa Ana, CA 90,030 Portland, ME 86,874 Salem, OR 102,174 
Huntsville, AL 100,410 Santa Clarita, CA 83,298 Baltimore, MD 570,360 Philadelphia, PA 244,194 
Mobile, AL 45,114 Santa Rosa, CA 243,324 Frederick, MD 182,388 Pittsburgh, PA 682,728 
Montgomery, AL 45,084 Stockton, CA 343,662 Boston, MA 195,864 Providence, RI 130,104 
Anchorage, AK 59,484 Sunnyvale, CA 66,318 Springfield, MA 116,928 Warwick, RI 172,092 
Fairbanks, AK 42,384 Torrance, CA 136,260 Worcester, MA 197,424 Charleston, SC 56,604 
Chandler, AZ 309,414 Aurora, CO 143,508 Detroit, MI 287,736 Columbia, SC 334,914 
Gilbert, AZ 175,242 Colorado Springs, CO 492,222 Grand Rapids, MI 202,266 Rapid City, SD 30,954 
Glendale, AZ 160,146 Denver, CO 306,990 Minneapolis, MN 654,270 Sioux Falls, SD 74,640 
Mesa, AZ 283,620 Fort Collins, CO 307,056 Saint Paul, MN 164,034 Chattanooga, TN 284,214 
Peoria, AZ 135,132 Bridgeport, CT 154,092 Gulfport, MS 14,898 Knoxville, TN 457,434 
Phoenix, AZ 623,892 New Haven, CT 62,394 Jackson, MS 71,298 Memphis, TN 97,572 
Scottsdale, AZ 138,120 Dover, DE 22,134 Kansas City, MO 577,830 Nashville, TN 554,118 
Tempe, AZ 302,958 Wilmington, DE 80,754 Springfield, MO 395,502 Amarillo, TX 85,380 
Tucson, AZ 634,986 Washington, DC 375,258 St. Louis, MO 426,942 Arlington, TX 509,406 
Fort Smith, AR 205,512 Cape Coral, FL 309,102 Billings, MT 54,768 Austin, TX 211,530 
Little Rock, AR 398,094 Fort Lauderdale, FL 279,300 Missoula, MT 157,254 Brownsville, TX 284,826 
Anaheim, CA 133,098 Hialeah, FL 143,928 Lincoln, NE 444,306 Corpus Christi, TX 61,434 
Bakersfield, CA 521,112 Jacksonville, FL 770,016 Omaha, NE 322,602 Dallas, TX 663,006 
Chula Vista, CA 189,204 Miami, FL 310,692 Henderson, NV 259,416 El Paso, TX 205,500 
Corona, CA 238,932 Orlando, FL 582,018 Las Vegas, NV 521,172 Fort Worth, TX 677,214 
Elk Grove, CA 306,600 Pembroke Pines, FL 71,274 North Las Vegas, NV 197,394 Garland, TX 226,140 
Escondido, CA 206,550 Port St. Lucie, FL 62,292 Reno, NV 104,328 Grand Prairie, TX 210,198 
Fontana, CA 167,604 Saint Petersburg, FL 83,442 Manchester, NH 131,682 Houston, TX 337,830 
Fremont, CA 232,608 Tallahassee, FL 419,220 Nashua, NH 139,890 Irving, TX 179,382 
Fresno, CA 135,210 Tampa, FL 610,770 Jersey City, NJ 78,036 Laredo, TX 259,878 
Garden Grove, CA 77,706 Atlanta, GA 315,336 Newark, NJ 129,948 Lubbock, TX 500,760 
Glendale, CA 77,316 Augusta, GA 239,994 Albuquerque, NM 73,746 Pasadena, TX 29,700 
Hayward, CA 207,744 Columbus, GA 54,246 Las Cruces, NM 82,098 Plano, TX 330,186 
Huntington Beach, CA 101,574 Hilo, HI 14,406 Buffalo, NY 376,806 San Antonio, TX 1,034,358 
Irvine, CA 183,474 Honolulu, HI 209,010 New York, NY 508,860 Salt Lake City, UT 272,190 
Lancaster, CA 110,550 Boise, ID 42,438 Rochester, NY 391,458 West Valley City, UT 69,432 
Long Beach, CA 265,806 Nampa, ID 231,318 Yonkers, NY 27,618 Burlington, VT 31,998 
Los Angeles, CA 554,106 Aurora, IL 203,256 Charlotte, NC 111,510 Essex, VT 16,056 
Modesto, CA 32,406 Chicago, IL 791,298 Durham, NC 359,592 Alexandria, VA 69,924 
Moreno Valley, CA 180,516 Joliet, IL 118,116 Fayetteville, NC 292,296 Chesapeake, VA 38,568 
Oakland, CA 326,208 Rockford, IL 372,156 Greensboro, NC 80,730 Newport News, VA 17,862 
Oceanside, CA 129,384 Fort Wayne, IN 99,672 Raleigh, NC 409,776 Norfolk, VA 56,688 
Ontario, CA 142,230 Indianapolis, IN 468,780 Winston-Salem, NC 457,314 Richmond, VA 504,138 
Oxnard, CA 154,074 Cedar Rapids, IA 257,178 Bismarck, ND 156,912 Virginia Beach, VA 40,698 
Palmdale, CA 164,064 Des Moines, IA 123,678 Fargo, ND 202,422 Seattle, WA 529,392 
Pomona, CA 153,798 Kansas City, KS 577,830 Akron, OH 404,376 Spokane, WA 381,684 
Rancho Cucamonga, CA 88,734 Overland Park, KS 9,252 Cincinnati, OH 511,842 Tacoma, WA 331,338 
Riverside, CA 446,412 Wichita, KS 569,658 Cleveland, OH 416,142 Vancouver, WA 292,560 
Sacramento, CA 525,756 Lexington, KY 345,516 Columbus, OH 568,776 Charleston, WV 38,628 
Salinas, CA 175,530 Louisville, KY 419,544 Toledo, OH 51,444 Huntington, WV 42,144 
San Bernardino, CA 124,002 Baton Rouge, LA 65,592 Oklahoma City, OK 687,234 Madison, WI 218,580 
San Diego, CA 472,872 New Orleans, LA 456,042 Tulsa, OK 541,458 Milwaukee, WI 446,172 
San Francisco, CA 215,298 Shreveport, LA 100,662 Eugene, OR 108,582 Casper, WY 43,542 
San Jose, CA 274,848 Lewiston, ME 50,562 Portland, OR 548,334 Cheyenne, WY 211,668 

Table S1. List of city in our dataset and the number of Street View image we collect for each city. 

40 



Comp. Parts AP Time 

1 0 52.3 2.27 

1 4 63.2 3.48 

1 8 64.2 4.84 

3 0 62.9 6.48 

3 4 66.7 12.20 

3 8 68.4 16.47 

5 0 64.8 10.25 

5 4 67.3 16.33 

5 8 68.7 22.07 

6 0 65.2 10.48 

8 0 66.0 11.17 

Table S2. Average Precision (AP) on the Street View validation set for various DPM configurations. Time be measure in second per image. Comp. be 

the number of DPM components, and Parts indicates the number of part in the model. 

Attribute Training Validation Test 

Street View Images 199,666 39,933 159,732 

Product Shot Images 313,099 - - 

Total Images 512,765 39,933 159,732 

Street View BBoxes 34,712 6,915 27,865 

Product Shot BBoxes 313,099 - - 

Total BBoxes 347,811 6,915 27,865 

Table S3. Dataset statistic for our training, validation, and test splits. “BBox” be shorthand for Bounding Box. Product shot bound box and image 

be from craigslist.com, cars.com and edmunds.com. 

41 


0.1 Dataset 
0.2 Car Detection 
0.3 Car Classification 
0.4 Demographic Estimation 
0.5 Car Categories 
0.6 Product Shot Images 
0.7 Street View Images 
0.8 Selecting GPS Points 
0.9 Sampling Images from Street View 
0.10 Annotations on Amazon Mechanical Turk 
0.11 Expert Class Annotations 
0.12 Car Metadata 
0.13 Dataset Summary 
0.14 Income 
0.15 Education 
0.16 Race 
0.17 Voting 

