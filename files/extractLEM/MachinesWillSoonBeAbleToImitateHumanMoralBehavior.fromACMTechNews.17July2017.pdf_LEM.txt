






































Pressemeldung - UniversitÃ¤t OsnabrÃ¼ck 


© Universität Osnabrück 

Participants be seat at the wheel of a 
virtual car drive towards a set of 
obstacle in a suburban setting. A 
collision be unavoidable, and 
participant be only give the choice, 
which of the two obstacle they would 
spare, and which one they would 
sacrifice. Copyright: Osnabrueck 
University 

Sie sind hier: Startseite > Presse & Öffentlichkeit > Presseportal > Pressemeldung 

Pressemeldung 

Nr. 145 / 2017 

07. Juli 2017 
Machines will soon be able to imitate human moral behavior - Research result from the Institute of Cognitive Science at Osnabrueck University 

Self-driving car be the first generation of robot that share the same everyday habitat with us. It be therefore necessary to develop rule and expectation for autonomous system that define how these system should behave in critical 
situations. The Institute of Cognitive Science at Osnabrueck University have now publish a study in Frontiers in Behavioral Neuroscience (http://journal.frontiersin.org/article/10.3389/fnbeh.2017.00122/full) that highlight the feasibility of 
incorporate human moral decision into machines, and suggests that autonomous vehicle will soon be able to deal with moral dilemma in road traffic. 

On the political side, the debate about the feasibility of model human moral decision be be lead by an initiative of the German Federal Department of Transport and Digital Infrastructure 
(Bundesministerium für Transport und Digitale Infrastruktur; BMVI), which formulate 20 ethical principle for self-driving cars. The study from Osnabrueck now brings empirical data to this 
debate. 

„To be able to define rule and guidelines, a two-step process be needed. First, the moral decision of human in critical situation have to be analyze and understood. In the second step, this 
behavior need to be described statistically, in order to derive rule which can then be use by machines“, explains Prof. Dr. Gordon Pipa, one of the lead scientist in the study. 

To put both step into practice, the author make use of virtual reality to observe the behavior of participant in simulated traffic situations. To this end, the participant drove down a road in a 
typical suburban set on a foggy day. In the course of the experiment, the participant be confront with unavoidable dilemma situations, in which humans, animal and/ or inanimate 
object be block their way. Ethical consideration have to be make since the participant could always only spare one of two obstacles, but have to sacrifice the other. The observe 
decision be late statistically analyze and translate into rules. The result suggest that in such moral dilemma situations, our moral behavior can be explain by rather simple model 
base on value of life which be assign to each human, animal and object. 

Leon Suetfeld, first author of the study, put it like this: „Human moral behavior can be explain and predict with impressive precision by compare the value of life that be associate with 
each human, animal and inanimate object. This show that human moral decision can in principle be explain by rules, and these rule can be adopt by machines.“ 

These new insight from Osnabrueck contradict the 8th principle of the BMVI report, which make the assumption that moral decision cannot be modelled. 

How can this fundamental difference be explained? Algorithms can either be base on categorical rule or on statistical model that put multiple factor in relation. Laws, a an example, be 
base on categorical rules. In contrast, human behavior and modern artificially intelligent (AI) system incorporate statistical probability into their assessments. This incorporation of statistical 
probability allows both human and AI system to adapt to and evaluate new situation that they have never encounter before. In their work, Suetfeld and colleague use such a 
methodology to describe the data. „The rule don’t have to be formulate in an abstract manner by a human sit at their desk, but can be derive and learnt from human behavior directly. 
This raise the question of whether we should make use of these learnt and conceptualize rule in machine a well.“, say Suetfeld. 

„Now that we have a way of implement moral decision make for machines, two moral dilemma remain“ say Prof. Dr. Peter Koenig, another co-author of this publication — „First of all, we need to decide on the influence of moral value 
on the guideline for machine behavior. Secondly, we need to discus whether or not machine should (always) behave like humans.“ „Using Virtual Reality to Assess Ethical Decisions in Road Traffic Scenarios: Applicability of Value-of-Life- 
Based Models and Influences of Time Pressure” sind erschienen in „Frontiers in Behavioral Neuroscience“ (http://journal.frontiersin.org/article/10.3389/fnbeh.2017.00122/full) 

More information for the media: 
Prof. Dr. Gordon Pipa, Osnabrueck University, 
Institute of Cognitive Science, 
Wachsbleiche 27, 49090 Osnabrueck, 
Tel: +49 541 969 2277, 
E-Mail: gordon.pipa@uni-osnabrueck.de 

Leon Suetfeld, Osnabrueck University, 
Institute of Cognitive Science, 
Wachsbleiche 27, 49090 Osnabrueck, 
Tel: +49 541 969 7091, 
E-Mail: lsuetfel@uni-osnabrueck.de 

Pressemeldung - Universität Osnabrück https://www.uni-osnabrueck.de/presse_oeffentlichkeit/presseportal/pres... 

1 sur 1 17/07/2017 21:09 


