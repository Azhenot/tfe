









































Supporting Information:
Computer Vision Uncovers Predictors of Physical Urban Change

Nikhil Naik∗, Scott Duke Kominers, Ramesh Raskar, Edward L. Glaeser, and César A. Hidalgo

Contents

1. Data and Methods 1
1.1. The Street View Image Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2. Image Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.3. Removal of Unsuitable Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.4. Streetchange Calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.5. Removing Erroneous Pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.6. Image Quality and Streetchange . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.7. Validating Streetchange . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.8. Streetscore: Generalization Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

2. Regressions 7
2.1. Do Social Characteristics Predict Changes in Streetscore? . . . . . . . . . . . . . . . . . . . . . . 8
2.2. The Filtering Hypothesis of Urban Change . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

3. Additional Examples and Map Visualizations 8

1. Data and Methods

Using a dataset consisting of human-coded image comparisons, we train a computer vision algorithm to predict
perceived safety of individual street scenes (“Streetscore”); we then obtain “Streetchange” by comparing that
measurement across Street View images of the same locations from 2007 (the “2007 image”) and 2014 (the “2014
image”).

1.1. The Street View Image Dataset

We first describe the Street View image dataset used in this paper. We obtained 360◦ panorama images of
streetscapes from five US cities using the Google Street View Application Programming Interface (API). Each
panorama was associated with a unique identifier (“panoid”), latitude, longitude, and time stamp (which specified

∗E-mail: naik@fas.harvard.edu

1



the month and year of image capture). We extracted an image cutout from each panorama by specifying the head-
ing and pitch of the camera relative to the Street View vehicle. We obtained a total of 1,645,760 image cutouts
for street blocks in Baltimore, Boston, Detroit, New York, and Washington DC, captured in years 2007 (the “2007
panel”) and 2014 (the “2014 panel”). For the street blocks that lack images for either 2007 or 2014, we completed
the “2007” and “2014” panels using images from the closest years for which data was available. As a result, 5%
of the images in the “2007” panel are from either 2008 or 2009. Similarly, 12% of the images in the “2014” panel
are from 2013. We matched image cutouts from the 2007 and 2014 panels by using their geographical locations
(i.e. latitude and longitude) and by choosing the same heading and pitch. This process gave us images that show
the same place, from the same point of view, but in different years. A large majority of images in our dataset were
captured between the months of April and August, to avoid a change of season between the two images of the
same location.

Next, we describe the computer vision algorithm used for obtaining Streetchange—a measure for physical urban
change—from the 2007 and 2014 image panels.

1.2. Image Feature Extraction

Our computer vision algorithms work with a number of structured and unstructured features of the image data:
First, we used the Geometric Layout algorithm [1] to assign pixel-wise semantic labels in four geometric classes:
“ground,” “buildings,” “trees,” and “sky.” Next, we extracted two different image features separately for the pixels
of the four geometric classes:

• We generated a texton dictionary [2] by convolving the images with a Gaussian filter bank and clustering
their responses together; every pixel was then assigned to the nearest cluster center, creating a texton map.
We computed 512-dimensional histograms with texton maps of the four geometric classes. We call this
feature Geometric Texton Histograms (GTH).

• We calculated the GIST feature descriptor [3]—a global image feature that provides a low-dimensional
representation of the spatial layout properties of a scene—for each of the geometric classes.

1.3. Removal of Unsuitable Images

A small number of Street View image pairs in the sample were unsuitable for comparison. In particular: some
images were over-exposed, out of focus, or blurred; others had significant changes in greenery coverage likely
driven by seasonal changes rather than urban foliage improvements. To eliminate these unsuitable pairs, we used
a series of automated data cleaning methods:

• First, we removed over-exposed images, which typically result from the sun shining directly into the camera.
To identify over-exposed pixels, we converted each image to the CIELAB color space, in which the L
channel represents lightness and a, b channels represent the color. The color channels were combined as
C = (a, b)T . We computed an over-exposure valueM (between 0 and 1) at each pixel following methods
introduced by Guo et al. [4]: At pixel i,

Mi =
1

2
· tanh

(
δ ·

(
(Li − LT ) + (CT − ||Ci||2)

))
,

with the constants set to δ = 1/60, LT = 80, and CT = 40. We obtained the meanMskyi ofMi over all the
pixels that belonged to the “sky” geometric class, as predicted by the Geometric Layout algorithm [1]. We
discarded the image pairs in which at least one of the images i hadMskyi > 0.85, indicating over-exposure.
We discarded all image pairs containing at least one over-exposed image.

2



2007 2014 2007 2014

2007 2014 2007 2014
Streetchange = +0.5934 Streetchange = +0.5960

Streetchange = +0.2787 Streetchange = +0.7401

Figure S1. The Streetchange algorithm is robust to large weather and seasonal changes. In this example, our algorithm
assigns a small Streetchange value to the image pairs, even though there is a drastic change in weather between the two
images. Images courtesy of Google, Inc.

• Second, we removed images that were out-of-focus or contained motion blur. To detect such images, we
computed the Absolute Central Moment (ACMO) of each image, a statistical measure that allows a simul-
taneous optimization of both focus and exposure [5]. If the normalized value of ACMO was less than 0.2,
we labeled the image as blurred. We discarded all image pairs containing at least one blurred image.

• Finally, we discarded all image pairs in which the number of pixels in the image occupied by the “tree” object
class (again, as predicted by the Geometric Layout algorithm [1]) changed by more than 10% between the
2007 image and the 2014 image. This process eliminated image pairs in which only one of the images had
significant occlusion of buildings by trees.

1.4. Streetchange Calculation

Having removed the images unsuitable for urban change detection, we predicted the “Streetscores” of the remain-
ing images using a support vector regression model trained with computer vision features and aggregate scores
obtained from the crowdsourced study by Salesses et al. [6], as described next.

Salesses et al. [6] created an online crowdsourced game in which participants were shown images of streetscapes
randomly chosen from New York, Boston, Linz and Salzburg. Participants were asked to choose one of the two
images in response to three questions: “Which place looks safer?”, “Which place looks more upper class?”, and
“Which place looks unique?”. In the Salesses et al. [6] study, 7,872 unique participants from 91 countries provided
186,188 comparisons (“clicks”) of image pairs drawn from a pool of 4,109 images for the question “Which place
looks safer?”

Following Naik et al. [7], we converted the 186,188 pairwise comparisons for the question “Which place looks
safer?” to ranked scores using a Bayesian ranking algorithm called Trueskill [8]. We call the Trueskill perceived
safety score for each image that image’s Streetscore; these images’ Streetscores are “true scores” derived from
aggregations of human assessments. We used the Streetscores obtained from human assessments to train a machine
learning model that uses the GTH and GIST features of the corresponding images (described in Section 1.2) to
predict how humans would score the perceived safety of Street View images. As we are only seeking to predict

3



(a) Image Pairs with Location Errors - Examples

2007 2014

2007 2014 2007 2014

2007 2014
(b) Image Pairs with Significant Occlusion Errors - Examples

Figure S2. A human operator eliminated the small fraction of invalid image pairs containing location-coding errors or signif-
icant occlusion of buildings by large vehicles. Images courtesy of Google, Inc.

the human perception of American cities, we restricted the training sample to the 2,920 human-coded images from
New York and Boston.

We used ν-Support Vector Regression (ν-SVR) [9] to predict image Streetscores. Given a set of training images
with feature vectors x and Streetscores q ∈ R, ν-SVR with a linear kernel generates a weight vector w and a bias
term b under a set of constraints. The two variables (w, b) are used to predict the Streetscore for a new image with
feature vector x′ by evaluating q′ = w · x′ + b. We measured the accuracy of our predictor using the Coefficient
of Determination (R2). We obtained R2 = 0.57 over fivefold cross-validation on the training set.

In this paper we dropped the Geometric Color Histogram (GCH) features used by Naik et al. [7], since GCH
features were more sensitive to weather changes than GIST and texton histograms. Dropping the GCH features,
however, did not substantially reduce the predictor accuracy—the R2 dropped from 0.5884 to 0.5709.

Next, we used the Streetscore predictor to calculate urban change from image pairs. As our predictor is a weight
vector trained using image features on top of the four geometric classes (ground, building, trees and sky), we were
able to compute the contribution of each geometric class to the Streetscore of each image. We chose to discard the
contribution of the “trees” and “sky” classes since their scores depend on the season and weather at the time of
image capture. Note that the “trees” class contains only large trees (and not landscaping), allowing us to account
for changes in the built environment due to landscaping as part of the “ground” class. Figure S1 shows examples
of image pairs with large weather and seasonal changes which have been accurately scored by our algorithm.

After computing the Streetscore for each image in a 2007–2014 image pair, we calculated “Streetchange” as the
difference between the 2014 image’s Streetscore and the 2007 image’s Streetscore.

1.5. Removing Erroneous Pairs

While we were able to computationally eliminate pairs containing over-exposed, blurred, or occluded images,
we discovered two additional sources of error that made a small number of image pairs invalid for Streetchange
calculation. The first source of error was incorrect location information for one or both images in an image pair.
For these images, the geographic coordinates (latitude and longitude) obtained from Google Street View did not

4



2007 2014 2007 2014

2007 2014 2007 20142007 2014
Streetchange = +0.6624 Streetchange = +0.8564

Streetchange = +0.7432 Streetchange = +0.6184

Figure S3. The Streetchange algorithm is robust to the change in Street View image quality between years 2007 and 2014.
Images courtesy of Google, Inc.

match with the actual geographic coordinates of the locations at which the images were captured (Figure S2-(a)).
The second source of error was partial or complete occlusion of buildings by large vehicles (Figure S2-(b)), which
were not removed by the procedure described in Section 1.3.

Our algorithm calculated a large positive or negative Streetchange value for image pairs containing location-
coding or vehicle-occlusion errors, since the two images in such pairs look very different (Figure S2). Due to
large variation in visual appearance within and across image pairs that contained these errors, we were unable to
automatically eliminate such image pairs with a computer vision algorithm. Therefore, a human operator observed
image pairs whose Streetchange value was larger than four standard deviations of change in Streetscore (which
amounted to only 1,849 image pairs out of a total of 822,880—less than 0.23% of the sample). The operator
manually eliminated image pairs which contained location-coding errors or vehicle-occlusion errors.

1.6. Image Quality and Streetchange

We also note that there is a difference in image quality between Street View images captured in year 2007 and
Street View images from year 2008 or after (due to improvements in Google’s imaging hardware). But the
Streetchange algorithm is robust to change in image quality from 2007 and 2014 (Figure S3) for a few reasons.
First, a significant fraction of images in the training set are also from 2007, which helps to mitigate the effect of
change in image quality. Second, we process the images at a resolution of 400 × 300 pixels; the difference in
quality is not significant at this resolution. Finally, a higher fraction of 2007 Street View images (as compared to
year 2014) tend to be over-exposed; but we discard such images from our calculations.

1.7. Validating Streetchange

We validated our final Streetchange measures using three sources: a survey on Amazon Mechanical Turk (AMT),
a survey of graduate students in MIT’s School of Architecture and Planning, and data from Boston’s Planning and
Development Authority.

5



Figure S4. Screenshot of the Amazon Mechanical Turk experimental interface used for validating Streetchange. Insets
courtesy of Google, Inc.

Validation with human observers: For the validation experiment on AMT, we selected 1,565 image pairs
(roughly 1% of the final sample) using inverse transform sampling on Streetchanges. We presented AMT workers
with two image pairs side-by-side (Figure S4) drawn randomly from the 1,565 image pairs and asked the following
question:

Select the image pair that has the larger change in buildings, streets and sidewalks. Please ignore
changes in traffic, trees and weather. If the amount of change in the pairs is similar, select “Almost
the same”.

We obtained 28,170 pairwise comparisons for the 1,565 image pairs from 116 users—36 pairwise comparisons on
average. We converted these pairwise comparisons to ranked scores using the Microsoft Trueskill algorithm [8].
Trueskill converges to a stable estimate of ranked scores after 12–36 comparisons, so we had enough comparisons
to obtain accurate scores. We call this score AMT–Streetchange. A higher value of AMT–Streetchange is indicative
of a larger absolute physical change in the image pair, as observed by the AMT users.

Next, we obtained binned ranks between 1 and 30 for both AMT–Streetchange and the absolute value of Streetchange
output of our algorithm. Comparing the two, we found a Spearman’s rank correlation of 0.72 (p < 1 × 10−5)
between them. These results indicate that the algorithm outputs on Streetchanges are consistent with human judg-
ments on changes in urban environment.

For the validation experiment with graduate students in MIT’s School of Architecture and Planning, we presented
the students (number of participants = 3) with 100 image pairs, where 50 image pairs contained large negative
Streetchange and 50 image pairs contained large positive Streetchange. We asked them to choose if the image pair
shows signs of positive change, negative change, or no change, and aggregated their responses with a majority
rule. The students agreed with positive Streetchange (as scored by our algorithm) for 88% of the image pairs and
they agreed with negative Streetchange for 59% of the image pairs. The lower agreement in the negative change
was a result of students classifying demolition of blighted properties as a positive change (while our algorithm
tends to classify them as negative).

6



Table S1. Summary Statistics for the Boston Validation Experiment (Section 1.7) ( N = 222 )

Variables Description Mean SD Min Max

Streetchange 2007–2014 Mean Streetchange 2007–2014 of
all sampled street blocks within a
census tract

1.290 0.509 −1.599 2.613

Log Total Square Footage
2012–2014

Total square footage built per
square mile within a census tract

13.527 1.548 10.109 17.202

Table S2. Streetscore: Generalization Performance (R2)
XXXXXXXXXXXTrain

Test
New York Boston

New York 0.5399 0.5033
Boston 0.5028 0.5384

Validation with infrastructure development data: In the third validation experiment, we tested the relationship
between improvements in Streetscore and improvements in infrastructure, using data on new developments in
Boston (for summary statistics, see Table S1). We collected data on all public and private building projects
from the Boston Planning and Development Agency (BPDA). We computed the total new square footage built
per square mile for each census tract during the period 2012–2014 and tested its relationship with Streetchange
2007–2014. We expect census tracts where more square footage was built during 2012–2014 to be associated
with a higher Streetchange, due to the physical improvements in these neighborhoods between the 2007 and 2014
image panels. And indeed, we find that infrastructure improvements are positively and significantly associated to
Streetchange—one standard deviation increase in log total square footage corresponds to roughly half a standard
deviation increase in Streetchange. We estimate:

Streetchange 2007 − 2014 = 1.620
(0.069)

+
0.159∗∗∗

(0.035)
· Log total square footage (1)

These results provide empirical evidence for the connection between improvements in Streetscore and improve-
ments in infrastructure.

1.8. Streetscore: Generalization Performance

Computer vision algorithms might have difficulty generalizing to out-of-sample data. Since we compute Streetscores
for images from Baltimore, Detroit, and Washington DC using an algorithm trained with images from Boston and
New York, we would like to estimate whether the Streetscore predictor can generalize without a significant drop
in accuracy. So we performed an experiment where we trained a Streetscore predictor using images just from New
York and measured the accuracy of its predictions on Boston images in our dataset, and vice versa. We found that
the R2 drops by only 0.036 on average during cross-city prediction (Table S2).

2. Regressions

We calculated Streetchange for 2007–2014 image pairs sampled uniformly from Baltimore, Boston, Detroit, New
York, and Washington DC. Using tract boundaries from the 2010 US Census, we aggregated 2007 Streetscore and
2007–2014 Streetchange across each census tract. We obtained census tract characteristic data from the 2000 US
Census, adjusted to the 2010 census tract boundaries [10]. For summary statistics, see Table S3.

7



2.1. Do Social Characteristics Predict Changes in Streetscore?

We now present the cross-sectional demographic and economic correlates of the 2007 Streetscore and 2007–2014
Streetchange (Table S4). For each census tract, we considered the following socioeconomic indicators from the
2000 US Census: population density, level of education, median income, housing price and rental costs, housing
vacancy, race, and poverty. We find that the socioeconomic characteristics that best predict higher Streetscore in
2007—density and education—are also the best predictors of increases in Streetscore between 2007 and 2014.
These relationships hold regardless of whether we control for the 2007 Streetscore or other variables. We find that
other variables, such as income, housing prices, rent, race, and poverty have little or no predictive power in our
context.

2.2. The Filtering Hypothesis of Urban Change

In addition to the invasion and tipping hypotheses of urban change discussed in main text, we evaluated the filtering
hypothesis [11] of urban change. The filtering hypothesis suggests cycles in which neighborhoods gradually decay
until they get upgraded. To test the hypothesis that building age shapes streetscape change, we regressed the 2007
Streetscore and 2007–2014 Streetchange on the shares of the building stock (as of the year 2000) built during
different decades. The data grouped together all buildings erected before 1939. There is weak support for the
filtering hypothesis in our dataset (Table S5): we found that neighborhoods with newer housing stocks score
higher than neighborhoods built in the 1950s. However, we cannot rule out the possibility that our finding is also
reflective of differences in the perception of various architectural styles, as neighborhoods built before 1939 (prior
to the widespread adoption of modernist architecture in the US) also score highly.

3. Additional Examples and Map Visualizations

Figures S5–S7 present additional examples of positive Streetchange from the five cities in our dataset. The exam-
ples show that Streetchange is able to detect both upgrading and new construction. Figure S8 shows additional
examples of negative Streetchange, which are associated with urban blight and decline in upkeep.

Figures S9–S28 present map visualizations for Log Population Density 2000, Share College Education 2000,
Streetscore 2007, and Streetchange 2007–2014 for the five cities in our dataset.

References

[1] Hoiem D, Efros AA, Hebert M (2008) Putting objects in perspective. International Journal of Computer Vision 80(1):3–
15.

[2] Malik J, Belongie S, Leung T, Shi J (2001) Contour and texture analysis for image segmentation. International Journal
of Computer Vision 43(1):7–27.

[3] Oliva A, Torralba A (2001) Modeling the shape of the scene: A holistic representation of the spatial envelope. Interna-
tional Journal of Computer Vision 42(3):145–175.

[4] Guo D, Cheng Y, Zhuo S, Sim T (2010) Correcting over-exposure in photographs. IEEE Conference on Computer
Vision and Pattern Recognition pp. 515–521.

[5] Shirvaikar MV (2004) An optimal measure for camera focus and exposure. Southeastern Symposium on System Theory
pp. 472–475.

[6] Salesses P, Schechtner K, Hidalgo CA (2013) The collaborative image of the city: Mapping the inequality of urban
perception. PLoS One 8(7):e68400.

8



[7] Naik N, Philipoom J, Raskar R, Hidalgo CA (2014) Streetscore – Predicting the perceived safety of one million
streetscapes. IEEE Conference on Computer Vision and Pattern Recognition Workshops pp. 793–799.

[8] Herbrich R, Minka T, Graepel T (2006) Trueskill: A Bayesian skill rating system. Advances in Neural Information
Processing Systems pp. 569–576.

[9] Schölkopf B, Smola AJ, Williamson RC, Bartlett PL (2000) New support vector algorithms. Neural Computation
12(5):1207–1245.

[10] Logan JR, Xu Z, Stults BJ (2014) Interpolating US decennial census tract data from as early as 1970 to 2010: A
longitudinal tract database. The Professional Geographer 66(3):412–420.

[11] Margolis SE (1982) Depreciation of housing: An empirical consideration of the filtering hypothesis. Review of Eco-
nomics and Statistics 64(1):90–96.

9



Table S3. Summary Statistics (N = 2513)

Variables Description Mean SD Min Max

Streetscore 2007 Mean Streetscore 2007 of all
sampled street blocks within a
census tract

7.757 2.587 1.681 18.93

Streetchange 2007–2014 Mean Streetchange 2007–2014 of
all sampled street blocks within a
census tract

1.39 0.779 −4.076 6.121

Log Population Density 2000 Log of population density within a
census tract

−4.655 1.22 −15.29 −2.48

Share College Education 2000 Share of adults with a four-year
college degrees within a census
tract

0.254 0.216 0 1

Log Median Income 2000 Log of median income of adults
within a census tract

4.54 0.206 3.884 5.276

Log Monthly Rent 2000 Log of median monthly rent within
a census tract

6.40 0.390 4.595 7.601

Log Housing Price 2000 Log of median housing price
within a census tract

5.223 0.311 3.938 6

Poverty Rate 2000 Share of households under poverty
line within a census tract

0.218 0.137 0 1

Share African-American 2000 Share of African-Americans within
a census tract

0.366 0.371 0 1

Share Hispanic 2000 Share of Hispanics within a census
tract

0.192 0.221 0 0.927

Share Vacant Units 2000 Share of vacant units within a
census tract

0.036 0.055 0 0.348

Share Built 1990-2000 Share of housing stock built during
1990-2000 within a census tract

0.587 0.874 0 1

Share Built 1980-1989 Share of housing stock built during
1980-1989 within a census tract

0.404 0.573 0 0.659

Share Built 1970-1979 Share of housing stock built during
1970-1979 within a census tract

0.694 0.744 0 0.719

Share Built 1960-1969 Share of housing stock built during
1960-1969 within a census tract

0.123 0.983 0 1

Share Built 1950-1959 Share of housing stock built during
1950-1959 within a census tract

0.164 0.105 0 0.748

Share Built 1940-1949 Share of housing stock built during
1940-1949 within a census tract

0.171 0.956 0 0.815

Share Built before 1940 Share of housing stock built before
1940 within a census tract

0.394 0.201 0 1

All socioeconomic variables are from the 2000 US Census. Streetscore 2007 and Streetchange 2007–2014 are computed using the method
described in Section 1 of this document. All data are aggregated at the census tract level.

10



Table S4. Relationship Between Social Characteristics and Changes in
Streetscore

Coefficient
Independent Variables Streetscore

2007
Streetchange
2007–2014

Share College Education 2000 2.024*** 1.099***
(0.483) (0.162)

Log Population Density 2000 0.765*** 0.080***
(0.097) (0.026)

Log Median Income 2000 0.186 −0.118
(0.920) (0.242)

Log Monthly Rent 2000 1.191*** −0.149
(0.260) (0.096)

Log Housing Price 2000 0.986*** 0.175*
(0.274) (0.090)

Poverty Rate 2000 3.815*** 0.300
(1.114) (0.312)

Share African-American 2000 0.151 −0.002**
(0.225) (0.001)

Share Hispanic 2000 0.878** 0.312***
(0.365) (0.118)

Share Vacant Units 2000 0.917*** 0.090***
(0.158) (0.033)

Streetscore 2007 −0.012
(0.013)

All models control for city fixed effects. ∗ ∗ ∗p < 0.01, ∗ ∗ p < 0.05, ∗p < 0.1
Standard errors corrected for spatial correlation are in parentheses. Regressions are esti-

mated with a constant that is not reported.

11



Table S5. Evidence of Filtering

Coefficient
Independent Variables Streetscore

2007
Streetchange
2007–2014

Share Built 1990-2000 4.390*** 0.410
(1.422) (0.341)

Share Built 1980-1989 4.638*** 0.677*
(1.406) (0.346)

Share Built 1970-1979 2.077** 0.079
(0.948) (0.259)

Share Built 1960-1969 2.367** 0.343
(1.119) (0.288)

Share Built 1940-1949 −0.195 -0.720**
(1.227) (0.324)

Share Built Before 1940 4.614*** 0.323*
(0.618) (0.194)

Streetscore 2007 0.000
(0.014)

Log Population Density 2000 0.106***
(0.033)

Share College Education 2000 0.648***
(0.106)

All models control for city fixed effects. ∗ ∗ ∗p < 0.01, ∗ ∗ p < 0.05, ∗p < 0.1
Standard errors corrected for spatial correlation are in parentheses. Regressions are

estimated with a constant that is not reported.

12



2007 2014 2007 2014

Figure S5. Additional examples of positive Streetchange. The first three rows show examples from New York City. The next
three rows show examples from Boston. Images courtesy of Google, Inc.

13



2007 2014 2007 2014

Figure S6. Additional examples of positive Streetchange. The first three rows show examples from Washington DC. The next
three rows show examples from Baltimore. Images courtesy of Google, Inc.

14



2007 2014 2007 2014

Figure S7. Additional examples of positive Streetchange. All examples from Detroit. Images courtesy of Google, Inc.

2007 2014 2007 2014

Figure S8. Additional examples of negative Streetchange from the five cities in our dataset. Images courtesy of Google, Inc.

15



Legend

<12000
12000 - 26400 
26400 - 45800 
45800 - 76000 
 >76000 

Per Square Mile

Figure S9. New York City: Log Population Density 2000.
16



0 - 8 
8 - 15 
15 - 23 
23 - 50 
50 - 100 

Legend

Percent

Figure S10. New York City: Share College Education 2000.
17



Legend

0.0 - 5.4 
5.4 - 7.0 
7.0 - 8.3 
8.3 - 12.0 
12.0 - 18.9 

Streetscore

Figure S11. New York City: Streetscore 2007.
18



Legend

-4.7 - 0.8 
0.9 - 1.2 
1.2 - 1.5 
1.5 - 1.9 
1.9 - 6.2 

Streetchange

Figure S12. New York City: Streetchange 2007–2014.
19



Legend

<12000
12000 - 26400 
26400 - 45800 
45800 - 76000 
 >76000 

Per Square Mile

Figure S13. Boston: Log Population Density 2000.

20



0 - 8 
8 - 15 
15 - 23 
23 - 50 
50 - 100 

Legend

Percent

Figure S14. Boston: Share College Education 2000.

21



Legend

0.0 - 5.4 
5.4 - 7.0 
7.0 - 8.3 
8.3 - 12.0 
12.0 - 18.9 

Streetscore

Figure S15. Boston: Streetscore 2007.

22



Legend

-4.7 - 0.8 
0.9 - 1.2 
1.2 - 1.5 
1.5 - 1.9 
1.9 - 6.2 

Streetchange

Figure S16. Boston: Streetchange 2007–2014.

23



Legend

<12000
12000 - 26400 
26400 - 45800 
45800 - 76000 
 >76000 

Per Square Mile

Figure S17. Washington DC: Log Population Density 2000.

24



0 - 8 
8 - 15 
15 - 23 
23 - 50 
50 - 100 

Legend

Percent

Figure S18. Washington DC: Share College Education 2000.

25



Legend

0.0 - 5.4 
5.4 - 7.0 
7.0 - 8.3 
8.3 - 12.0 
12.0 - 18.9 

Streetscore

Figure S19. Washington DC: Streetscore 2007.

26



Legend

-4.7 - 0.8 
0.9 - 1.2 
1.2 - 1.5 
1.5 - 1.9 
1.9 - 6.2 

Streetchange

Figure S20. Washington DC: Streetchange 2007–2014.

27



Legend

<12000
12000 - 26400 
26400 - 45800 
45800 - 76000 
 >76000 

Per Square Mile

Figure S21. Baltimore: Log Population Density 2000.

28



0 - 8 
8 - 15 
15 - 23 
23 - 50 
50 - 100 

Legend

Percent

Figure S22. Baltimore: Share College Education 2000.

29



Legend

0.0 - 5.4 
5.4 - 7.0 
7.0 - 8.3 
8.3 - 12.0 
12.0 - 18.9 

Streetscore

Figure S23. Baltimore: Streetscore 2007.

30



Legend

-4.7 - 0.8 
0.9 - 1.2 
1.2 - 1.5 
1.5 - 1.9 
1.9 - 6.2 

Streetchange

Figure S24. Baltimore: Streetchange 2007–2014.

31



Legend

<12000
12000 - 26400 
26400 - 45800 
45800 - 76000 
 >76000 

Per Square Mile

Figure S25. Detroit: Log Population Density 2000.

32



0 - 8 
8 - 15 
15 - 23 
23 - 50 
50 - 100 

Legend

Percent

Figure S26. Detroit: Share College Education 2000.

33



Legend

0.0 - 5.4 
5.4 - 7.0 
7.0 - 8.3 
8.3 - 12.0 
12.0 - 18.9 

Streetscore

Figure S27. Detroit: Streetscore 2007.

34



Legend

-4.7 - 0.8 
0.9 - 1.2 
1.2 - 1.5 
1.5 - 1.9 
1.9 - 6.2 

Streetchange

Figure S28. Detroit: Streetchange 2007–2014.

35


