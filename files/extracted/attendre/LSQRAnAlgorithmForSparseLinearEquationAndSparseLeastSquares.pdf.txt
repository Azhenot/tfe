

































LSQR: An Algorithm for Sparse Linear 
Equations and Sparse Least Squares 

CHRISTOPHER C. PAIGE 
McGill University, Canada 
and 
MICHAEL A. SAUNDERS 
Stanford University 

An iterative method is given for solving Ax ~ffi b and minU Ax - b 112, where the matrix A is large and 
sparse. The method is based on the bidiagonalization procedure of Golub and Kahan. It is analytically 
equivalent to the standard method of conjugate gradients, but possesses more favorable numerical 
properties. 

Reliable stopping criteria are derived, along with estimates of standard errors for x and the 
condition number of A. These are used in the FORTRAN implementation of the method, subroutine 
LSQR. Numerical tests are described comparing I~QR with several other conjugate-gradient algori- 
thms, indicating that I~QR is the most reliable algorithm when A is ill-conditioned. 

Categories and Subject Descriptors: G.1.2 [Numerical Analysis]: ApprorJmation--least squares 
approximation; G.1.3 [Numerical Analysis]: Numerical Linear Algebra--linear systems (direct and 
tterative methods); sparse and very large systems 

General Terms: Algorithms 

Additional Key Words and Phrases: analysis of variance 

The Algorithm: LSQR: Sparse Linear Equations and Least Square Problems. ACM Trans. Math. 
Softw. 8, 2 (June 1982). 

1. INTRODUCTION 

A n u m e r i c a l  m e t h o d  is p r e s e n t e d  he re  for c o m p u t i n g  a so lu t i on  x to  e i the r  of the  

fol lowing prob lems:  

U n s y m m e t r i c  equa t ions :  solve A x  ffi b 

L i n e a r  leas t  squares :  m i n i m i z e  ][ A x  - b 112 

This work was supported in part by the Natural Sciences and Engineering Research Council of 
Canada Grant A8652, the New Zealand Department of Scientific and Industrial Research, the U.S. 
Office of Naval Research under Contract N00014-75-C-0267, National Science Foundation Grants 
MCS 76-20019 and ENG 77-06761, and the Department of Energy under Contract DE.AC03- 
76SF00326, PA No. DE-AT03-76ER72018. 
Authors' addresses: C.C. Paige, School of Computer Science, McGill University, Montreal, P.Q., 
Canada H3C 3G1; M.A. Saunders, Department of Operations Research, Stanford University, Stanford, 
CA 94305. 
Permission to copy without fee all or part of this material is granted provided that the copies are not 
made or distributed for direct commercial advantage, the ACM copyright notice and the title of the 
publication and its date appear, and notice is given that copying is by permission of the Association 
for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific 
permission. 
© 1982 ACM 0098-3500/82/0300-0043 $00.75 

ACM Transactions on Mathematwal Software, Vol 8, No. 1, March 1982, Pages 43-71. 



44 • C .C.  Paige and M. A. Saunders 

where A is a real matrix with m rows and n columns and b is a real vector. It will 
usually be true that m _> n and rank(A) = n, but these conditions are not 
essential. The method, to be called algorithm LSQR, is similar in style to the 
well-known method of conjugate gradients (CG) as applied to the least-squares 
problem [10]. The matrix A is used only to compute products of the form A v  and 
ATu for various vectors v and u. Hence A will normally be large and sparse or will 
be expressible as a product of matrices that are sparse or have special structure. 
A typical application is to the large least-squares problems arising from analysis 
of variance (6.g., [12]). 

CG-like methods are iterative in nature. They are characterized by t h e i r  need 
for only a few vectors of working storage and by their theoretical convergence 
within at most n iterations (if exact arithmetic could be performed). In practice 
such methods may require far fewer or far more than n iterations to reach an 
acceptable approximation to x. The methods are most useful when A is well 
conditioned and has many nearly equal singular values. These properties occur 
naturally in many applications. In other cases it is often possible to divide the 
solution procedure into a direct and an iterative part, such that  the iterative part 
has a better conditioned matrix for which CG-like methods will converge more 
quickly. Some such transformation methods are considered in [21]. 

Algorithm LSQR is based on the bidiagonalization procedure of Golub and 
Kahan [9]. It generates a sequence of approximations {xk } such that  the residual 
norm II rk [[2 decreases monotonically, where rk  = b - A x k .  Analytically, the 
sequence (xh} is identical to the sequence generated by the standard CG algo- 
rithm and by several other published algorithms. However, LSQR is shown by 
example to be numerically more reliable in various circumstances than the other 
methods considered. 

The FORTRAN implementation of LSQR [22] is designed for practical appli- 
cation. It incorporates reliable stopping criteria and provides the user with 
computed estimates of the following quantities: x ,  r = b - A x ,  A Tr, II r 112, It A II F, 
standard errors for x ,  and the condition number of A. 

1.1 Notation 

Matrices are denoted by A, B . . . . .  vectors by v, w , . . . ,  and scalars by ~, f l , . . . .  
Two exceptions are c and s, which denote the significant components of an 
elementary orthogonal matrix, such that c 2 + s 2 = 1. For a vector v, H v [[ always 
denotes the Euclidean norm H v [[2 = (vTv) 1/2. For a matrix A, [[A [[ will usually 
mean the Frobenius norm, HA HF = ( ~ a 2 )  1/2, and the condition number for an 
unsymmetric matrix A is defined by cond(A) = ]] A ]] II A + ]], where A + denotes the 
pseudoinverse of A. The relative precision of floating-point arithmetic is e, the 
smallest machine-representable number such that  1 + e > 1. 

2. MOTIVATION VIA THE LANCZOS PROCESS 

In this section we review the symmetric Lanczos process [13] and its use in 
solving symmetric linear equations B x  = b. Algorithm LSQR is then derived by 
applying the Lanczos process to a particular symmetric system. Although a more 
direct development is given in Section 4, the present derivation may remain 
useful for a future error analysis of LSQR, since many of the rounding error 
properties of the Lanczos process are already known [18]. 

ACM Transactions on Mathematmal Software, Vol 8, No 1, March 1982. 



LSQR: An Algorithm for Sparse Linear Equattons and Sparse Least Squares • 45 

Given a symmetric matrix B and a starting vector b, the Lanczos process is a 
method for generating a sequence of vectors { v,) and scalars { a, ), (fli} such that 
B is reduced to tridiagonal form. A reliable computational form of the method is 
the following. 

T h e  L a n c z o s  p roces s  (reduction to tridiagonal form): 

fll vl -- b, 

w, = Bvt - fl, v,-~ ] 

a, vTw'  I '  i =  1, 2 , . . . ,  (2.1) 

~ t+ l  Vt+l W t  ~ OltVt 

where vo - 0 and each fl, _> 0 is chosen so that II v, II = 1 (i > 0). The situation after 
k steps is summarized by 

B V k  = Vk Tk + flk+lvk+le T (2.2) 

where Tk - tridiag(fl, a,, fl,+l) and Vk ~ [Vl, v2 . . . . .  vk]. If there were no rounding 
error we would have V T Vk = I, and the process would therefore terminate with 
fl,+~ = 0 for some k ___ n. Some other stopping criterion is needed in practice, since 
ilk+, is unlikely to be negligible for any k. In any event, eq. (2.2) holds to within 
machine precision. 

Now suppose we wish to solve the symmetric system B x  = b. Multiplying (2.2) 
by an arbitrary k-vector yk, whose last element is ~/h, gives B V k y k  ~- Vk Tkyk + 
flk+~Vk+~k. Since Vk (fl~ el) = b by definition, it follows that ifyk and xk are defined 
by the equations 

Thyk = file1, 
(2.3) 

xk = Vkyk, 

then we shall have Bxk  = b + ~lkflk+lVk+~ to working accuracy. Hence xk may be 
taken as the exact solution to a perturbed system and will solve the original 
system whenever 7/kflk+l is negligibly small. 

The above arguments are not complete, but they provide at least some 
motivation for defining the sequence of vectors {Xk} according to (2.3). It is now 
possible to derive several iterative algorithms for solving B x  = b, each character- 
ized by the manner in which yk is eliminated from (2.3) (since it is usually not 
practical to compute each Yk explicitly). In particular, the method of conjugate 
gradients is known to be equivalent to using the Cholesky factorization Tk 
LkDk L~ and is reliable when B (and hence Tk) is positive definite, while algorithm 
SYMMLQ employs the orthogonal factorization Tk --/:k Q~ to retain stability for 
arbitrary symmetric B. (See [20] for further details of these methods.) 

2.1 The Least-Squares System 

We now turn to a particular system that arises from the "damped least-squares" 
problem 

mioll[:l]X-[:]ll: ,2,, 
ACM Transactmns on Mathematmal Software, Vol. 8, No. 1, March 1982 



46 C.C. Paige and M. A. Saunders 

where A and b are given data and ~ is an arbitrary real scalar. The solution of 
(2.4) satisfies the symmetric (but indefinite) system 

A r 

where r is the residual vector b - Ax. When the Lanczos process is applied to this 
system, a certain structure appears in relations (2.1)-(2.3). In particular, (2.1) 
reduces to the procedure defined as Bidiag 1 in Section 3, while (2.3) can be 
permuted after 2k + 1 iterations to the form 

[ '  1P +,l _- 
(2.6) 

Irk] ffi [ U~+I 0 lrt.+,l, 
x ,  V,j L Y' J 

where Bk is (k + 1) x k and lower bidiagonal. We see that  y~ is the solution of 
another damped least-squares problem, 

min l ] [B;]yk - [ f l~ l ] [ [2  , (2.7) 

which can be solved reliably using orthogonal transformations. This observation 
forms the basis of algorithm LSQR. 

2.2 A Different Starting Vector 
For completeness we note that  a second least-squares algorithm can be derived 
in an analogous way. Defining s = -Ax, we can write (2.5) as 

A s 

and apply the Lanczos process again, using the same matrix as before but with 
the new starting vector shown. This time, (2.1) reduces to Bidiag 2 as defined in 
Section 3, while (2.3) can be permuted to the form 

-x IJLy  J -O,e, ' 
(2.9) 

x, V, JLyk j ' 
after 2k iterations, where Rk is k × k and upper bidiagonal. (The odd-numbered 
systems are not useful because T2k-~ is singular when ~ = 0.) We see that  yk 
satisfies the system 

(RTRk + ~2I)yk ffi 01el, (2.10) 

which could easily be solved. However, (2.10) proves to be one form of the normal 
equations associated with (2.7), and the algorithm it suggests for computing x 
ACM Transactlov.s on Mathematical  Software, VoL 8, No. 1, March 1982. 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 47 

therefore has unsatisfactory numerical properties. We clarify this mat te r  in 
Section 7.4. 

2.3 The Role of 

The quantities generated by the Lanczos process from (2.5) and (2.8) are Bk, Uk+~, 
Vk and Rk,  Pk ,  Vk, respectively. These are all i n d e p e n d e n t  o f  k ,  which means 
tha t  they are the same as those generated when k ffi 0. We shall therefore assume 
from now on tha t  k ffi 0. A given k can be accommodated when solving (2.7), as 
shown in [22]. Methods for actually choosing ?, are beyond the scope of this 
paper. 

3. THE BIDIAGONALIZATION PROCEDURES 

The preceding use of the Lanczos process results in two forms of a bidiagonali- 
zation procedure due to Golub and Kahan  [9]. We state these forms as procedures 
Bidiag 1 and 2, and then give some unexpected relationships between them. 

B i d i a g  I (starting vector b; reduction to lower bidiagonal form): 

f l l u l  = b, a l v l  = A Tul.  

fl,+lu,+~ = A v ,  - a ,u ,  ~ ,  i ffi 1, 2 . . . . .  (3.1) 

0~+1V~+l = ATu~+I  ~ flt+lVz ) 
The scalars a, _> 0 and fl, _> 0 are chosen so tha t  [[ u,[I -- [[ viii ffi 1. With  the 
definitions 

Uk  ~- [ U l ,  U2 ,  • • . ,  Uk], B2 012 

Vk ~ [Vl,  V2, V, ], Bk -- [33 " ' .  ' 
• ' ' ,  " .  Otk 

( w h e r e  Bk is the rectangular matrix introduced in Section 2), the recurrence 
relations (3.1) may  be rewritten as 

Uk+~(fl~e~) ffi b, (3.2) 

A Vk -~ Uk+lBk, (3 .3 )  

T ATUk+I -~ V k B  T + Olk+lVk+lek+l. (3 .4 )  

T V If  exact arithmetic were used, then we would also have UT+~ Uk+~ ffi I and Vk k 
= /, but, in any event, the previous equations hold to within machine precision. 

B i d i a g  2 (starting vector A T b ;  reduction to upper bidiagonal form): 

Oavl ffi A T b ,  p~pl  ffi A v l .  

O,+lV,+~ ffi A T p ,  -- p ,v ,  ~ ,  i ffi 1, 2 . . . . .  (3.5) 
p,+lp,+l = Av ,+l  -- 0,+1p, J 

ACM Transactions on Mathematmal Software, VoL 8, No 1, March 1982. 



48 • C.C. Paige and M. A. Saunders 

Again, p, > 0 and O, > 0 are chosen so that  II P, II = ]1 vi II = 1. In this case, if 

Pk  -- [p l ,  p2, . . . ,pk], 

Yk --- Iv , ,  v2, . . . ,  vk],  

Rk ~ 

pl 

p2 03 

QQm "'o 

pk-1 

01, 

we may rewrite (3.5) as 

Vk(Ole l )  ffi A Tb, (3.6) 

A Vk ffi P k R k ,  (3.7) 

V, T 0 T A T p k  ffi k R k  + k+lVk+lek, (3.8) 

T V and with exact arithmetic we would also have P T P k  = V k  k = L 
Bidiag 2 is the procedure originally given by Golub and Kahan (with the 

particular starting vector A Tb ). Either procedure may be derived from the other 
by choosing.the appropriate starting vector and interchanging A and A T. 

3.1 Relationship Between the Bidiagonalizations 

The principal connection between the two bidiagonalization procedures is that 
the matrices Vk are the same for each, and that  the identity 

B T B k  f f i  R T R k  (3.9) 

holds. This follows from the fact that v~ is the same in both eases, and Vk is 
mathematically the result of applying k steps of the Lanezos process (2.2) with 
B = A TA. The rather surprising conclusion is that  Rk must be identical to the 
matrix that  would be obtained from the conventional QR faetorization of Bk. 
Thus 

where Qk is orthogonal. In the presence of rounding errors, these identities will, 
of course, cease to hold. However, they throw light on the advantages of algorithm 
LSQR over two earlier methods, LSCG and L S L Q ,  as discussed in Section 7.4. 

The relationship between the orthonormal matrices Uh and Pk can be shown 
to be 

for some vector rk. We also have the identities 

0/12 '4" ~ 2  ____. p2,  0~1 ~1 = 0l ,  

ACM TransacUons on Mathematlcal Software, Vol 8, No. 1, March 1982 

(3.11) 

for i > 1. 
(3.12) 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 49 

4. ALGORITHM LSQR 

The quantities generated from A and b by Bidiag 1 will now be used to solve the 
least-squares problem, min II b - A x  II. 

Let the quantities 

xk = Vkyk ,  (4.1) 

rk = b - A x k ,  (4.2) 

tk+l ffi file1 - B k y k ,  (4.3) 

be defined in terms of some vector yk. I t  readily follows from (3.2) and (3.3) tha t  
the equation 

rk = Uk+ltk+l (4.4) 

holds to working accuracy. Since we want  H rk [[ to be small, and since Uk+l is 
bounded and theoretically orthonormal,  this immediately suggests choosing yk to 
minimize U tk+~ H. Hence we are led naturally to the least-squares problem 

min II/ , el -- B k y k  II (4.5) 

which forms the basis for LSQR. 
Computationally, it is advantageous to solve (4.5) using the s tandard QR 

factorization of Bk [8], tha t  is, the same factorization (3.10) tha t  links the two 
bidiagonalizations. This takes the form 

pl 82 
p2 03 

t , Qo ° 

pk-1 
Pk 

~2 

84 ~k-~ 
(4.6) 

where Qk = Qk, k ÷ l . . .  Q2,3QI,2 is a product of plane rotations (e.g., [25]) designed 
to eliminate the subdiagonals f12, fla . . . .  of Bk. The vectors yk and tk+l could then 
be found from 

R k y k  = h ,  (4.7) [o] 
tk+~ = Q [  ~h+~ " (4.8) 

However, yk in (4.7) will normally have r/o elements in common with yk-1. Instead 
we note tha t  [Rk f~] is the same as [Rk-1 fk-1] with a new row and column 
added. Hence, one way of combining (4.1) and (4.7) efficiently is according to 

xk = V k R ~ l f k  =-- D k f k ,  (4.9) 

where the columns of Dk -- [all d2 . - .  dk] can be found successively from the 

ACM Transac t ions  on Ma thema tma l  Software, Vol. 8, No  1, March  1982 



50  • C . C .  Paige and M. A. Saunders 

R k D k  = V T by forward substitution. With  do = x0 = 0, this gives system T T 

dk = ! (v ,  - Okdk-1), (4.10) 
pk 

xk = Xk-I + ~kdk,  (4.11) 

and only the most  recent i terates need be saved. The  broad outline of algorithm 
LSQR is now complete. 

4.1 Recurrence Relations 

The QR factorization (4.6) is determined by constructing the k t h  plane rotat ion 
Qk,k+l to operate on rows k and k + 1 of the transformed [Bk /~le~] to annihilate 
fl~+l. This  gives the following simple recurrence relation: 

o t'] o,,+, <,,k] 
s ,  - c ,  J L P , + l  ~,+1 = ~ ,+ ,  ~-k+, ' (4.12) 

where #51 --- al ,  ~1 -- ill, and the scalars ca and sk are the nontrivial elements of 
Qk,k+J. The  quantities #54, ~k are intermediate scalars tha t  are subsequently 
replaced by pk, d~. 

The rotations Qk.k+I are discarded as soon as they  have been used in (4.12), 
since Q, itself is not  required. We see tha t  negligible work is involved in computing 
the QR factorization to obtain Rk, fk, and ~k+l. 

Some of the work in (4.10) can be eliminated by using vectors Wk -- pkdk in 
place of dk. The main steps of LSQR can now be summarized as follows. (As 
usual the scalars a, _ 0 and fl, ___ 0 are chosen to normalize the corresponding 
vectors; for example, al vl ffi A T e 1  implies the computat ions 61 = ATul, a] = II 1~1 II, 
vl = (1 / a l )~1 . )  

Algort thm L S Q R  

(1) (Initialize.) 

fl=u= = b, a~v~ - - - -  A Tul, Wl = Vl, Xo = O, 

(2) For i -- 1, 2, 3 . . . .  repeat steps 3-6. 
(3) (Continue the bidiagonalization.) 

(a) /~ ,+1u,+1 ffi A t , ,  - a , u ,  

(b) a,+lv,+l = ATu,+I  --/~,+=v,. 
(4) (Construct and apply next orthogonal transformation.) 

(a) p, = (g~ + P , % , ) ' / ~  

(b )  c, = F,/p, 
(c )  s, = # , + , t p ,  

(d) O,+l = s,a,+l 

(e) E+~ = -c,.,+~ 

(f)~, = c,~, 

(g) ~,+, = s,~,. 

ACM Transactions on Mathematical  Software, Vol 8, No 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equat,ons and Sparse Least Squares • 51 

(5) (Update x, w.) 

(a) x, = x , - I  + (¢, lp , )w,  

(b) w,+, = v,+~ - (8,+l/p,)w,. 

(6) (Test for convergence.) 
Exit if some stopping criteria (yet to be discussed) have been met. 

4.2 Historical Note 

The previous algorithm was derived by the authors in 1973. An independent 
derivation is included in the work of Van Heijst et al. [24]. 

5. ESTIMATION OF NORMS 

Here we show that estimates of the quantities II rk II, IIATrkll, Ilxkll, IIA II, and 
cond(A) can be obtained at minimal cost from items already required by LSQR. 
All five quantities will be used later to formulate stopping rules. 

Knowledge of [[ A H and perhaps cond(A) can also provide useful debugging 
information. For example, a user must define his matrix A by providing two 
subroutines to compute products of the form A v  and A T u .  These subroutines will 
typically use data derived from earlier computations, and may employ rather 
complex data structures in order to take advantage of the sparsity of A. If the 
estimates of II A I] and/or cond(A) prove to be unexpectedly high or low, then at 
least one of the subroutines is likely to be incorrect. As a rule of thumb, we 
recommend that all columns of A be scaled to have unit length (11Aej ]1 ffi 1, j = 
1 , . . . ,  n), since this usually removes some unnecessary ill-conditioning from the 
problem. Under these circumstances, a programming error should be suspected 
if the estimate of [[ A H differs by a significant factor from n 1/2 (since the particular 
norm estimated will be IIA NF). 

For the purposes of estimating norms, we shall often assume that the orthog- 
onality relations UkTUk = I and v k T V k  = I hold, and that I[ Uk 112 -- H Vk 112 --- 1. In 
actual computations these are rarely true, but the resulting estimates have proved 
to be remarkably reliable. 

5.1 Estimates of H rk I] and H A Trk II 
From (4.4) and (4.8) we have 

-- T 
rk = qJk+l U k + l Q k  ek+l (5.1) 

(which explains the use of rk in (3.11)); hence, by assuming UT+IUk+I  ffi L we 
obtain the estimate 

IIr,  II = S k + ,  ffi ~,SkSk-1 " ' '  Sl, (5.2) 

where the form of ~k+~ fOllOWS from (4.12). LSQR is unusual in not having the 
residual vector rk explicitly present, but we see that II rk II is available essentially 
free. Clearly the product of sines in (5.2) decreases monotonically. It should 
converge to zero if the system A x  ffi b is  compatible. Otherwise it will converge to 
a positive finite limit. 

For least-squares problems a more important quantity is ATrk, which would be 
zero at the final iteration if exact arithmetic were performed. From (5.1), (3.4), 

ACM Transac t tons  on M a t h e m a t i c a l  Sof tware ,  VoL 8, No.  1, M a r c h  1982 



52 • C.C. Paige and M. A. Saunders 

and (4.6) we have 

ATr4 ~4+l(V4B T + e T T = ak+lV4+l 4+l )Qke4+l  

= [?k+,V4[R T 0]e4+, + ~4+lak+l(eW+lQWe4+l)V4+l. 

The first term vanishes, and it is easily seen that  the (k + 1)st diagonal of Q4 is 
-c4. Hence we have 

A Wr4 = -- (~4+ l O14+l Ck )O4+ l (5.3) 

and 

IIATrkll = ~k+l~k+,l  ckl (5.4) 

to working accuracy. No orthogonality assumptions are needed here. 

5.2 An Estimate of Ilxk II 

The upper bidiagonal matr ix Rk may be reduced to lower bidiagonal form by the 
orthogonal factorization 

R w 4(~k = £4, (5.5) 
where Q4 is a suitable product of plane rotations. Defining 5k by the system 

/~4~4 -- fk, (5.6) 

it follows that  xk --- ( V4 R[1 ) f4 = ( Vk 0 T) 5k ------- 1Y¢4 54. Hence, under the assumption 
that  v T v k  ---- I, we can obtain the estimate 

II x,  II = II e, II. (5.7) 
Note that  the leading parts of L4, ~4, l~r4, and Ek do not change after iteration k. 
Hence we find that  estimating II xk II v ia  (5.5)-(5.7) cos t s  only 13 multiplications 
per iteration, which is negligible for large n. 

5.3 Estimation of II A lit and cond(A) 

It is clear from (3.1) that all the v, lie in the range of A T and are therefore 
orthogonal to the null space of A and A TA. With appropriate orthogonality 
assumptions we have from (3.3) that  

BTBk = V T A T A V k ,  

and so from the Courant-Fischer minimax theorem the eigenvalues of BTBk are 
interlaced by those of ATA and are bounded above and below by the largest and 
smallest nonzero eigenvalues at A TA. The same can therefore be said of the 
singular values of Bk compared with those of A. It follows that  for the 2- and F- 
norms, 

II Bk II - II A II, (5.8) 

where equality will be obtained in the 2-norm for some k _< rank(A) if b is not 
orthogonal to the left-hand singular vector of A corresponding to its largest 
singular value. Equality will only be obtained for the F-norm if b contains 
components of all left-hand singular vectors of A corresponding to nonzero 

ACM Transactions on Mathematmal Software, Vol 8, No 1, March 1982. 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 53 

singular values. Nevertheless,  we will use H Bk [IF as a monotonical ly increasing 
est imate of the size of A. 

The  foregoing also implies tha t  B T B k  TR = R k k is nonsingular and for the  2- and 
F-norms 

[I R ;  ~ [[ = I[ S~  [[ _< [[A + [[. (5.9) 

The  remarks on equali ty are the same, except  now "largest singular value" is 
replaced by "smallest nonzero singular value." 

Combining these results with the definition Dk = V k R ~  1 in (4.9) now gives 

1 <_ II B~ II IID~ I I -  IIAII IIA+ll--  cond(A) (5.10) 

for the 2- and F-norms. Hence we take I[ Bk IIFII Dk Hr as a monotonical ly increasing 
est imate of cond(A), which starts at  the optimistic est imate H B1 NFI[ D~ HF -- 1. 

Use of Frobenius norms allows the est imates to be accumulated cheaply, since 
[[ B ,  [[~ = [[ B,-1 [[~ + a~ + fl~+~ and [[ Dk I[~ = II D,-1 ][~ + [[ dk lie. T h e  individual 
terms in the sum [[ dk [[2 _ y,,,.1 6~k can be used fur ther  for estimating s tandard  
errors, as we show next. 

5.4 Standard Errors 

In regression problems with m > n = rank(A),  the s tandard error  in the  i th  
component  of the true solution x is taken to be & where 

s '  z _ 11 b - A x  [I 2 o,, (5.11) 
m - - n  

and o,  - eT(ATA)-~e ,  is the ith diagonal e lement  of (ATA) -~. Now from (3.3) and 
(3.10) we have V T A T A V k  = R T R k ,  which with (4.9) gives 

D T A  TADk = L 

Assuming tha t  p remature  terminat ion does not  occur, it follows tha t  with exact  
~ T r ~  r~T  e ar i thmetic  D n D  T = (ATA) -l, and we can approximate the a ,  by  o}~ ~ - e, ~ k ~ k  ,. 

Since D k D  [ = Dk_~DT_I + d k d  T, we have 

o~, ~ = o~, ~-'~ + ~,~, o~, °~ -= O, 

and the e~  ~ are monotonical ly increasing est imates of the o,,. 
In the implementat ion of LSQR we accumulate  o!, k~ for each i, and up- 

on terminat ion at  i teration k we set 1 = max(m - n, 1) and output  the square 
roots of 

s!k~ _ 1[ b - Axkl[ 2 o~k ) 
1 

as est imates of the s, in (5.11). The  accuracy  of these est imates cannot  be 
guaranteed,  especially if terminat ion occurs early for some reason. However,  we 
have obtained one reassuring comparison with the statistical package GLIM [16]. 

On a modera te ly  ill-conditioned problem of dimensions 171 by  38 (cond(A) - 
103, relative machine precision = 10-11), an accurate solution xk was obtained 
after  69 iterations, and at  this stage all s! k~ agreed to at  least one digit with the s, 
ou tput  by GLIM, and many  components  agreed more  closely. 

ACM Transac tmns  on Mathematmal  Software,  Vol. 8, No. 1, March  1982 



54 • C . C .  Paige and M. A. Saunders 

A further comparison was obtained from the 1033 by 434 gravity-meter problem 
discussed in [21]. For this problem a sparse QR factorization was constructed, 
Q A  = [0s], and the quantities a,  were computed accurately using R T v i  = ei, ai, ffi 
[[ v, [[2. Again, the estimates of s} k) from LSQR proved to be accurate to at least 
one significant figure, and the larger values were accurate to three or more digits. 

Note that  s, 2 estimates the variance of the i th component of x, and that s~ k)~ 
approximates this variance estimate. In an analogous manner, we could approxi- 
mate certain covariance estimates by accumulating 

= + = o ,  

for any specific pairs (i,j),  and then computing 

][ b - A x k  [I 2 _(,~ 
~ q  

1 

on termination. This facility has not been implemented in LSQR and we have 
not investigated the accuracy of such approximations. Clearly only a limited 
number of pairs (i, j )  could be dealt with efficiently on large problems. 

6. STOPPING CRITERIA 

An iterative algorithm must include rules for deciding whether the current iterate 
x~ is an acceptable approximation to the true solution x. Here we formulate 
stopping rules in terms of three dimensionless quantities ATOL, BTOL, and 
CONLIM, which the user will be required to specify. The first two rules apply to 
compatible and incompatible systems, respectively. The third rule applies to 
both. They are 

$1: Stop if I[ r~ [[ <__ BTOL[[ b [[ + ATOI~[ A [[ [I Xk U" 

]~4 Trk [[ < ATOL. $2: Stop ifHA[[ []rk[[- 

$3: Stop if cond(A) ___ CONLIM. 

We can implement these rules efficiently using the estimates of ][ rk [[, [[ A [] F, and 
so forth, already described. 

The criteria $1 and $2 are based on allowable perturbations in the data. The 
user may therefore set ATOL and BTOL according to the accuracy of the data. 
For example, if (A, b) are the given data and (Z~, ~) represents the (unknown) 
true values, then 

ATOL = [[ A - ~ [[ 
II A II 

should be used if an estimate of this is available; similarly for BTOL. 
Criterion $3 represents an attempt to regularize ill-conditioned systems. 

ACM Transactions on Mathematmal Software, Vol 8, No, 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 55 

6.1 Compatible Systems 

To justify S1, let r~ = b - Axh as usual, and define the quantities 

64 - BTOL[I b II + ATOLII AII [I xk U, 

rk 
g k  - -  BTOLII b II 

rk 
=- A T O M  d II II II 

Then rk = gk + hk, and 

( A  + h~Xx~k~ ) Xk = b - gk 

SO that  xk is the exact solution for a system with both A and b perturbed. It can 
be seen that  these perturbations are within their allowable bounds when the 
inequality of S 1 holds. Hence, criterion S 1 is consistent with the ideas of backward 
rounding error analysis and with knowledge of data accuracy. Since this argument 
does not depend on orthogonality, $1 can be used in any method for solving 
compatible linear systems. 

6.2 Incompatible Systems 
Stewart [23] has observed that i f  

a n d  

where 

rk ffi b -- Axk 

fk = b - (A + E k ) X k  

rkrWA 

iir, ' 

then (A + E k )  T rk = 0, SO that Xk and fk are the exact solution and residual for a 
system with A perturbed. Since H Ek 112 = ]1 A Trk I]/]] rk I], the perturbation to A will 
be negligible if the test in $2 is satisfied. 

In our particular method it happens that Ekxk ffi 0, since (5.3) shows that  x~ ffi 
Vkyk is theoretically orthogonal to ATrk. Hence fk = rk, SO both xk and rk are 
exact for the perturbed system. This strengthens the case for using rule $2. 

In practice we find that [I A Trk [I/I] rk I] can vary rather dramatically with k, but 
it does tend to stabilize for large k, and the stability is more apparent for LSQR 
than for the standard method of conjugate gradients (see [[ ATrk I] in Figures 3 and 
4, Section 8). Criterion $2 is sufficient to ensure that  xk is an acceptable solution 
to the least-squares problem, but the existence of an easily computable test that 
is both sufficient and necessary remains an open question [23]. 

6.3 Ill-Conditioned Systems 

Stopping rule S3 is a heuristic based on the following arguments. Suppose that  A 
has singular values a~ _> o2  - -  . - .  --> a .  > 0. It has been observed in some problems 

ACM Transact ions  on Mathemat ica l  Software, Vol. 8, No. 1, March  1982. 



56 C.C. Pmge and M A. Saunders 

tha t  as k increases the estimate ]] Bk lit H Dk [Iv -~ cond(A) in (5.10) temporari ly 
levels off near some of the values of the ordered sequence ol /ol ,  ol/o2, . . .  , 
a l / o , ,  with varying numbers of iterations near each level. This tends to happen 
when the smaller o, are very close together, and therefore suggests criterion $3 
as a means of regularizing such problems when they  are very ill-conditioned, as 
in the discretization of ill-posed problems (e.g., [15]). 

For example, if the singular values of A were known to be of order 1, 0.9, 10 -3, 
10 -6, 10 -7, the effect of the two smallest singular values could probably be 
suppressed by setting CONLIM = 10 4. 

A more direct interpretation of rule $3 can be obtained from the fact tha t  xk 
= D k f h .  First, suppose tha t  the singular value decomposition of A is A = U Z V  T 
where u T u  = V T v  = W T = I ,  ~ -~ diag(al, o2, . . .  , On), and let 

A(r)= u~(r )v  T 

be defined by setting or+~ . . . . .  On ---- 0. A common method  for regularizing the 
least-squares problem is to compute x (r) --  V ~ ( r ) + U T b  for some r _< n, since it can 
readily be shown tha t  the size of x (r) is bounded according to 

II A [12 [[ x (r) 1[ ~ __ ___ cond(A(r)). 
II b II or 

In the case of LSQR, we have II xk II -- II Dk I1~ II b II, and so if rule $3 has not  yet  
caused termination, we know tha t  II Bk II F II x~ II / lib II --- II Bk II F II D~ II ~ < CONLIM. 
Since U Bk IIF usua l ly  increases to order U A [Iv quite early, we effectively have 

IlAIIr Ilxkll < CONLIM, 
II b II 

which is exactly analogous to the bound above. 

6.4 Singular Systems 

It  is sometimes the case tha t  rank(A) < n. Known dependencies can often be 
eliminated in advance, but  others may  remain if only through errors in the data  
or in formulation of the problem. 

With  conventional (direct) methods  i t  is usually possible to detect  rank defi- 
ciency and to advise the user tha t  dependencies exist. In the present context it is 
more difficult to provide such useful information, but  we recognize the need for 
a method tha t  at  least does not  fail when applied (perhaps unknowingly) to a 
singular system. In such cases we again suggest the parameter  CONLIM as a 
device for controlling the computation. Our experience with LSQR on singular 
problems is tha t  convergence to an acceptable solution occurs normally, but  if 
iterations are allowed to continue, the computed xk will begin to change again 
and then grow quite rapidly until 

[[A I[ ]] Xk [I = 1 (6.1) 
I[ b [I E 

(while ]1 rk II remains of reasonable size). The est imate of cond(A) typically grows 
large b e f o r e  the growth in xk. A moderate  value of CONLIM (say 1/e 1/2) may  
therefore cause termination at  a useful solution. 

ACM Transactions on Mathematmal Software, Vol 8, No. 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 57 

In some cases it can be useful to set CONLIM as large as 1/E and allow xk to 
diverge. In this context we note tha t  the algorithm SYMMLQ [20] can be applied 
to singular symmetric systems, and tha t  extreme growth in the resulting II xk II 
forms an essential part  of a practical method for computing eigenvectors of large 
symmetric matrices [14]. By analogy, in the presence of rounding errors LSQR 
will usually produce an approximate singular vector of the matrix A. In fact, using 
(6.1) and II rk II - II b II, we see tha t  the normalized vector xk = xk / l l  xk II will usually 
satisfy 

1 
AEk  = ~ (b - rk) 

= E ~ ( b - r k )  

= 0(~)IIAII 

for large enough k, and hence will lie very nearly in the null space of A. The  
vector 2k may reveal to the user where certain unexpected dependencies exist. 
Suitable new rows could then be added to A. 

7. OTHER METHODS 

Several other conjugate-gradient methods are discussed here. All except the first 
(CGLS) are stated using notation consistent with Sections 3-6 ih order to 
illustrate certain analytic identities. 

7.1 CGLS 

If  the conjugate-gradient method for symmetric positive definite systems is 
applied naively to the normal equations A T A x  = A T b ,  the method  does not  
perform well on ill-conditioned systems. To a large extent this is due to the 
explicit use of vectors of the form A TAp, .  An algorithm with better  numerical 
properties is easily derived by a slight algebraic rearrangement,  making use of 
the intermediate vector A p ,  [10]. I t  is usually stated in notation similar to the 
following. 

Algor i thm C G L S  

(1) Se t ro=b ,  s o = A T b ,  p~=so ,  yo=ilSoll 2, xo=0. 

(2) For t = 1, 2, 3, . . .  repeat the following: 

(a) q, = Ap,  

(b) a, = ~,,_,/ l lq,  II ~ 

(c) x, = x,-~ + a,p, 

(d) r, = r,-i - a,q, 

(e) s, = A Tr, 

( f )  r ,  = IIs, lff 
(g) ,e, = r,l~,,-, 

(h) p,+l = s, + fl,p,. 
ACM Transactions on Mathematical  Software, Vol. 8, No. 1, March 1982. 



58 • C.C. Paige and M. A. Saunders 

A practical implementation of the method would also need to compute ]] r, ]], 
[I xi [I, and an estimate of II A II in order to use the stopping criteria developed in 
Section 6. Otherwise the method is clearly simple and economical. Analytically 
it generates the same points x, as LSQR. The vectors v,+~ and d, in LSQR are 
proportional to si and p,, respectively. 

Note that  qi and s, just given can share the same workspace. A FORTRAN 
implementation of CGLS has been given by Bj6rck and Elfving [3]. This incor- 
porates an acceleration (preconditioning) technique in a way that requires mini- 
mal additional storage. 

7.2 Craig's Method for A x  ffi b 

A very simple method is known for solving compatible systems A x  ffi b. This is 
Craig's method, as described in [6]. It is derivable from Bidiag 1, as shown by 
Paige [17], and differs from all other methods discussed here by minimizing the 
error norm [I xk - x H at each step, rather than the residual norm U b - A x k  [[ ffi 
II A (xk - x)I[. We review the derivation briefly. 

If Lk is the first k rows of Bk, 

L k  ~- 
~2 0/2 

Q,o ",° 

~k ~k 

then eqs. (3.3)-(3.4) describing Bidiag 1 may be restated as 

A V k  ffi U k L k  + flk+aUk+le T, 

A T u k  ffi V k L  T" (7.1) 

Craig's method is defined by the equations 

LkZk  -~ f l l e l ,  xk  ffi V k z k ,  (7.2) 

and we can show from (7.1) that the residual vector satisfies rk - b - A V k Z k  f f i  
--~kflk+lUk+l and hence U T r k  ffi 0. We can therefore expect rk to vanish (analyti- 
cally) for some k _ n. 

The vectors Zk a n d  xk  are  readily computed from 

h ffi - / ~ - '  ~k- l ,  x ,  ffi x , - 1  + ~ , v , ,  
~k 

where ~o - -1 .  Since the increments v~ form an orthogonal set, there is no danger 
of cancellation, and the step lengths ~k are bounded by [ ~k I - ]1 zk H - H xk I] -< II x H. 
We can therefore expect the method to possess good numerical properties. This 
is confirmed by the comparison in Section 8. 

7.3 Extension of Craig's Method 

A scheme for extending Craig's method to least-squares problems was suggested 
by Paige in [17]. The vectors in (7.2) were retained and an additional vector of 

ACM Transactions on Mathematmal Software, Vol 8, No 1, March 1982. 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 59 

the form Vk Wk was computed in parallel. On termination, a suitable scalar yk was 
computed and the final solution taken to be 

Xk ffi ( V k Z k )  - -  y k ( V k W k )  ~ V k y k .  

In the present context this method may be interpreted as a means of solving the 
least-squares system (2.7), namely, 

. ..,,]rt,,.,l__ 
BT L Y' J 

using the fact that the underdetermined system BTtk+I ---- 0 has a unique solution 
apart from a scalar multiple. 

In practice we have found that the method is stable only when b lies in the 
range of A. Further details are given in [21]. 

7.4 LSOG and LSLQ 

A second algorithm for least-squares problems was given by Paige [17]. This is 
algorithm LSCG, based on Bidiag 2. In the notation of Section 3 this algorithm 
is defined by the equations 

R T R k y k  ffi 01el, Xk = Vkyk  (7.3) 

and implemented in the form RTfk  ---- 01e~, Xk ffi ( V k R ~ ) f k .  Given the relations 
between the two bidiagonalizations, we now recognize that this is analytically 
equivalent to LSQR, but numerically inferior, since it is effectively solving the 
least-squares problem mini] Bkyk -- file1 [[ by using the corresponding normal 
equations. (The latter are B T B k y k  ffi BTfl~e~ ffi a~fl~e~ and by (3.9) and (3.12) this 
is equivalent to the first equation in (7.3).) 

Algorithm LSLQ [19] is a refinement of LSCG, but again it is based on Bidiag 
2 and the normal equations just given, and is therefore inferior to LSQR on ill- 
conditioned problems. The refinement has been described in Section 5.2, giving 
xk -- IYVk 5k, where lYtrk is theoretically orthonormal, the intention being to avoid 
any possible cancellation that could occur in accumulating xk = Dk f~ •- ( V k R ~  1) fk. 
The same refinement can easily be made to LSQR, and it was implemented in an 
earlier version of the algorithm for the same reason. However, we have not been 
able to detect any numerical difference between xk ffi I~¢kSk and Xk = Dkfk in the 
two versions of LSQR, so the fear of cancellation appears to have been unfounded. 
We have therefore retained the slightly more economical Xk = Dkfk, which also 
allows cond(A ) to be estimated from ]] Dk IIF, as already described. 

Algorithms LSCG and LSLQ need not be considered further. 

7.5 Chen's Algorithm RRLS 

Another algorithm based on Bidiag 2 has been described by Chen [4]. This is 
algorithm RRLS, and it combines Bidiag 2 with the so-called residual-reducing 
method of Householder [11]. In the notation of Section 3 it may be described as 
follows. The residual-reducing property is implicit in steps 2(b) and (c). 

Algorithm R R L S  

(1) Set ro ffi b, 8~v~ = A T b ,  w l  = v l ,  Xo = O. 

ACM Transac tmns  on MathemaUcai  Software, Vol 8, No. 1, March  1982 



60 C.C. Palge and M. A. Saunders 

(2) For i -- 1, 2, 3 . . . .  repeat the  following: 

(a) p , p ,  ffi A w,  

(b) k, •pTr ,  

(c) r, ffi r,-1 - ~,p, 

(d) 8,+iv,+l = A T p ,  - -  p , v ~  

(e) x, ffi x,-1 + (k,/p,)w, 

(f) w,÷l = v,+~ - (O,+Jp,)w,, 

where the scalars p, and 0, are chosen so that UP, H = ]] v, ]] = 1. 

As with CGLS, a practical implementat ion would also require ]] r, ]] and I[ x, [[. The  
square root of the sum ~ - 1  (p~ + 0~+1) = l] Rk ]]~ ffi ]] Bk lid could be used to 
estimate ][ A ]IF, and II ATr ,  H could also be est imated cheaply. 

Note tha t  the vectors v, are generated as in Bidiag 2, but  the vectors p, come 
instead from step 2(a). Substi tuting the latter into step 2(d) shows tha t  RRLS 
requires explicit computat ion of the vectors A T A w ,  (ignoring normalization by 
p,). Unfortunately this must  cast doubt  on the numerical properties of the 
method,  particularly when applied to compatible systems. Indeed we find tha t  
for some systems A x  = b, the final norms I[ r, [I and II x, - x [[ are larger, by a factor 
approaching cond(A), than  those obtained by CGLS and LSQR. This is illustrated 
in Section 8.3. 

A second algorithm called R R L S L  has been described by Chen [4], in which 
the residual-reducing method is combined with Bidiag 1. However, the starting 
vector used is AATb (rather than  b), and products of the form A T A w ,  are again 
required, so tha t  improved performance seems unlikely. Chen reports tha t  RRLS 
and R R L S L  behaved similarly in all test  cases tried. 

In spite of the above comments,  we have also observed ill-conditioned least- 
squares problems for which RRLS obtains far g r e a t e r  accuracy than  would 
normally be expected of any method (see Section 8.4 for a possible explanation). 
Because of this unusual behavior, we have investigated a residual-reducing 
version of LSQR as now described. 

7.6 RRLSQR 

If  the residual vector r, is explicitly introduced, algorithm LSQR as summarized 
in Section 4.1 can be modified slightly. First, the residual-reducing approach 
requires step 5(a) to be replaced by the two steps 

r ,  ffi r,_~ - X , p , ,  x, = x,-1 + ,k,w,, 

where p, = A w ,  and X, ffi p Tr , -1 / l ip ,  ns. (In this case p, is unnormalized.) Second, 
the product A w ,  can be used to eliminate A v ,  from Bidiag 1, leading to an 
alternative method, 

fl,+lu,+l ffi A w ,  - II r,-1 [-------[ r,-l, (7.4) 

for generating each fit and u,. (This result is difficult to derive, but  the key 
relation is p, / ] IP,  ]1 = c,r,-1 /][ r,-1U + s,u,+l, which may  be deduced from (3.11).) 

The remainder of LSQR is retained, including the QR factorization of Bk. The 

ACM Transact ions  on Mathematmal  Software, Vol 8, No 1, March  1982 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 61 

coefficient of r,-1 in (7.4) can be expressed in several ways; for example, 

E ~ 1 1) ' - '  ala2 . . .  a, 
II r , _ ,  II = - - ( -  . . .  P , '  

where ~', comes from the system L k z k  = fl~ el of Craig's method.  Different  formulas 
lead to different i teration paths, but  no variation appears  to be consistently be t te r  
than  the rest. 

A summary  of the resulting algorithm follows. 

Algor i thm R R L S Q R  

(1) ro=b ,  f l l u l = b ,  alVl=ATub WL=Vb Xo=O, 

(2) For i = 1, 2, 3, . .. repeat steps 3-6. 

(3) (a) p, = Aw,  

(b) X, = pTr,_l/ l lp,  I]2 

(c) r, = r . - h , p ,  

(d) fl,+lu,+1 = p, - (E/~,)r,-, 

(e)  at+l  V~+l = A T u t + I  -- ~t+lVt 

(4) Compute p,, c,, s,, E+,, P~+~, ~,,+~ as in Section 4.1, step 4. 

(5) (a) x, = x ,_~+X,w,  

(b) W z + I  = V , + I  - -  (Ot+l/Pt)Wt 

(6) Exit if appropriate. 

This  adapt ion of Bidiag 1 to obtain R R L S Q R  is analogous to (and was mot ivated  
by) Chen's  adaption of Bidiag 2 to obtain RRLS.  Note,  however,  tha t  there  are 
no products  of the form A TAw, .  In practice we find tha t  R R L S Q R  typically 
performs at  least as well as LSQR, as measured by the limiting H x, - x ]] attainable.  
Fur thermore ,  it at tains the same unusually high accuracy achieved by  RRLS  on 
certain ill-conditioned least-squares problems. On these grounds R R L S Q R  could 
sometimes be the preferred method.  However,  its work and storage requirements  
are significantly-higher than  for the other  methods  considered. 

7.7 Storage and Work 

The  storage and work requirements  for the most  promising algorithms are 
summarized in Table  I. Recall tha t  A is m by n and tha t  for least-squares 
problems m may  be considerably larger than  n. Craig's method  is applicable only 
to compatible systems A x  = b, which usually means rn -- n. 

All methods  require the starting vector  b. If  necessary this may  be overwri t ten 
by the first m-vector  shown (r  or u). The  m-vector  A v  shown for Craig and LSQR 
represents  working storage to hold products  of the form A v  and A T u .  (An n- 
vector  would be needed if m < n.) In some applications this could be dispensed 
with if the bidiagonalization operations A v  - a u  and A TU -- fl V were implemented  
to overwrite u and v, respectively. Similarly, the n-vector  AWp for RRLS  could in 
some cases be computed  wi thout  extra storage. 

ACM Transactions on Mathematical Software, Vol. 8, No. 1, March 1982. 



62 • C.C. Paige and M. A. Saunders 

Table I 

Storage Work 

m n m /'$ 

Craig 
(Ax  = b only) u, A v  x, v 3 4 

C G I ~  r, q x, p 2 3 
I ~ Q R  u, A v  x, v, w 3 5 
RRLS r , p  x , v , w ,  A T p  4 5 
RRLSQR r, u, p x, v, w 6 5 

The work shown for each method is the number of multiplications per iteration. 
For example, LSQR requires 3m + 5n multiplications. (A further 2n multiplica- 
tions are needed to accumulate estimates of cond(A ) and standard errors for x.) 
Practical implementations of CGLS and RRLS would require a further m + n 
multiplications to compute H r, I[ and [[ x, [[ for use in stopping rules, although this 
could be limited to every tenth iteration, say, without serious consequence. 

All methods require one product A v  and one product A T o  each iteration. This 
could dominate the work requirements in some applications. 

8. NUMERICAL COMPARISONS 

Here we compare LSQR numerically with four of the methods discussed in 
Section 7, denoted by CRAIG, CGLS, RRLS, and RRLSQR. The machine used 
was a Burroughs B6700 with relative precision ~ = 0.5 × 8 -12 = 0.7 × 10 -11. 

The results given here are complementary to those given by Elfving [5], who 
compares CGLS with several other conjugate-gradient algorithms and also inves- 
tigates their performance on problems where A is singular. 

8.1 Generation of Test Problems 

The following steps may be used to generate a test problem min [[ b - A x  I[ with 
known solution x. 

(1) Choose vectors x, y, z, c and diagonal matrix D arbitrarily, with U Y [I ffi [[ z II 
ffi 1. (For any chosen m >_ n, the vectors should be of dimensions n, m, n, and 
m - n, respectively.) 

(2) Define 

Y = I - 2 y y  T, 

(3) Compute 

The minimal residual norm is then II r I[ = II c II. Since A and D have the same 
singular values, the condition of the problem is easily specified. 

The particular problems used here will be called 

P ( m ,  n, d , p )  

ACM Transactions on Mathematical Software, Vol 8, No 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equat,ons and Sparse Least Squares • 63 

T a b l e  I I  

log,011 ~, - ~ II 
Case 

(m = n = 10, c o n d ( A  ) = 10 s) k = 60 80 100 120 

1 A = Y D Z ( a s  ~ o v e )  - 0 . 3  - 3 . 3  - 3 . 3  - 3 3  
2 A = Y D  - 0 . 5  - 3 . 9  - 3 . 9  - 4 . 1  

3 A = D Z  - 2 . 1  - 5 . 9  - 5 . 9  - 9 . 2  
4 A = D - 9 . 4  - 9 . 4  - 9 . 4  - 9 . 4  

to indicate dependence on four integer parameters ,  where d represents  duplication 
of singular values and p is a power factor. The  matr ix  D is of the  form diag(o~'), 
with each o, duplicated d times. Specific values for x, y, z, c, and D were generated 
as follows: 

(1) x = ( n - l , n - 2 ,  n - 3  . . . . .  2 ,1 ,0 )  w. 
(2) y, = sin(4cri/m),  z, = cos(4~ri/n),  followed by normalization so tha t  [[ y [[ = [[ z [[ 

--~1. 

(3) c = (1 /m,  - 2 / m ,  3 /m ,  . . . , +.(m - n ) / m )  T. 
(4) o, = [(i - 1 + d ) / d ] d / n ,  where integer division is used for the te rm in square 

brackets. Choosing n = q d  to be a multiple of d leads to d copies of each 
singular value: 

_ ( o , ) =  , . . . .  q , q  . . . . .  q . . . .  , q  . . . .  , . 

[For reference, this gives [[ x [[ = n ( n / 3 )  1/2, [[ r[[ = [[ c [['-~ [(m - n ) / m ]  ((rn - n ) /  
3) ~/2, ][ A ]] ~ = [[ D [[ F = (n /3 )  ~/2, cond(A) = cond(D)  = (o, /ol)  p = qP.] 

The  orthogonal  matrices Y and Z are intended to reduce the possibility of 
anomalous numerical  behavior. For  example, when LSQR was applied to four 
cases of the problem P (10, 10, 1, 8), the following error  norms resulted (Table II). 
Since each case was a compatible system A x  = b, we normally would expect  an 
error  norm approaching [] x [[. cond(A ). ~ = 10 -2, so tha t  case 1 is the most  realistic. 
In case 2 the error  was concentra ted in the first and second components  of xk 
(with the remaining components  accurate almost to working precision), whereas 
in cases 3 and 4 the final xk was virtually exact in spite of the high condition 
number  of A. 

Although cases 2-4 represent  less expensive test  problems, it is clear tha t  
results obtained from them could be very misleading. In the  following subsections 
we use only case 1. Even these test  problems may be less than  completely general. 
This  possibility is discussed in Section 8.4. 

8.2 Ax = b: Deterioration of CGLS ,n Adverse Circumstances 

Figure 1 illustrates the performance of five methods  on the ill-conditioned system 
P(10, 10, 1, 8), tha t  is, m = n = 10, one copy of each singular value, cond(A)  -- 
l0 s. The  quantit ies log~011rk [[ and log~oll xk - x I[ are plot ted against i terat ion 
number  k. 

This  example distinguishes the s tandard conjugate-gradient  me thod  CGLS 
from the remaining methods.  All except  CGLS reduced II rk II and ]1 xk - x II to a 
satisfactory level before k -- 120. 

ACM Transactions on Mathematical Software, Vol. 8, No. 1, March 1982. 



64 • C.C. Paige and M. A. Saunders 

IO91ollxv-xll 

L A ~ L A ~ ~ A 

o - CRAIG  

A - C G L S  
x = R R L S  

<> = L S O R  
+ - R R L S O R  

Iogloll%ll 

! | ! ! 
| ! I t I I t ,, . I 

10 2 0  3 0  4 0  5 0  6 0  7 0  8 0  9 0  100  110 120 

Fig. 1. All i l l -condit ioned s y s t e m s  Ax  -- b, n -- 10, cond(A)  = l0 s. C G L S  is unab le  to reduce  I[ rk [[ or  
I[ xk - x [[ sat isfactor i ly  C R A I G  exhibi t s  severe  f luc tua t ions  in [[ rk [[ 

ACM Transactions on Mathematical Software, Vol 8, No 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 65 

Also apparent is the erratic behavior of [I rk [[ for method CRAIG, a potential  
penalty for minimizing [[ x~ - x [] at  each step without  regard to [[ rk [[. In theory all 
other methods minimize ][ rk ]] at  each step and also reduce [[ xk - x [] monotonically. 

If  any method is to be preferred in this example it would be LSQR, since it 
reached limiting accuracy at  iteration 76 and stayed at  essentially the same point 
thereafter. With values ATOL = BTOL = e, the stopping rule S1 as implemented 
in LSQR would have caused termination at  k = 76 as desired. 

8.3  Ax = b: Failure of RRLS 

Figure 2 illustrates the same five methods applied to a larger problem P (40, 40, 
4, 7), in which each singular value is repeated four times and cond(A) ~- 107. In 
this case all methods except RRLS reduced [[ rk [] satisfactorily to about  10 -9 for 
k >_ 105. For method RRLS,  [[ rk [[ remained of order 10 -~ for k >_ 30 up to k = 250, 
and zero digits of accuracy were obtained in xk. 

A similar disparity between RRLS and the remaining methods was observed 
on the problems P (40, 40, 4, p) ,  p -= 5, 6, cond(A ) = l0 p. In fairness, Chen [4] did 
not  intend RRLS to be applied to compatible systems. However, the success of 
the other least-squares methods suggests tha t  this is not an unreasonable demand. 

8.4 min [[ A x  - b [[: High Accuracy by RRLS and RRLSQR 

Figure 3 shows the performance of four least-squares methods on the ill-condi- 
tioned problem P(20, 10, 1, 6). Since cond(A) 2 -- 10 ~2 = l /e ,  we would normally 
expect at  most  one digit of accuracy in the final xk. This is achieved by LSQR 
and CGLS, with LSQR showing a smoother decrease of ][ AWrk [[. 

In contrast, the residual-reducing methods achieved at  least six digits of 
accuracy in xk. Similarly, three or four digits of accuracy were obtained on the 
problem P(20, 10, 1, 8), for which cond(A) = l0 s is so high tha t  no digits could be 
expected. At first sight it may appear tha t  the residual-reducing methods  possess 
some advantage on least-squares problems. However, this anomalous behavior 
cannot be guaranteed; for example, it did not  occur on P(80, 40, 4, 6), as shown 
in Figure 4. Also, the final value of [[ A Trh [[ is no smaller than  for LSQR and this 
is really the more important  quantity. 

Par t  of the explanation for these occasional anomalies may  lie in the following. 
Suppose the original data  (A, b) have solution and residual (~, f), while perturbed 
data (A + ~A, b + t~b) have (~ + ~x, ~ + ~r). I f A  + ~A has full column rank, then  
it is straightforward to show tha t  

8x = (A + 8A)+(Sb - ~A£) + ((A + ~A)T(A + ~A))-I~ATfi. 

In the present example ~ = 0.7 x 10 -1~, ][ A ][ 2 -- 1, cond(A) =- 106, [[ b ][ - 2.4, [[ ~ [[ 
17, ][ ~ ][ = 1. If the perturbations were caused by rounding errors in the initial 

data, then [[ 8A [[ = e, [[ ~ b [[ = E, and the first term in the expression for ~x could 
be about as large as 10 -4 in norm, and the second could be of order 7. Figure 3 
suggests the second term is unusually small for the RRLS and RRLSQR com- 
putations. Looking at  the particular form of the test problem, if we write 

Y =  [Y1, Y2], A = Y, DZ, ~= Y2c, 

ACM Transactmns on Mathematical Software, Vol. 8, No. 1, March 1982. 



6 6  C . C .  P a i g e  a n d  IVl. A .  S a u n d e r s  

2 

0 

-2  

-4  

tOgloIIxk-xll 

toglollrl, II 

-6  

-8  

- 1 0  

a : CRAIG 

: C G L S  

× : R R L S  

o : LSQR 

+ : R R L S Q R  

I I I I I I i I I I I 

10 2 0  3 0  4 0  5 0  6 0  7 0  8 0  9 0  100 110 

Fig. 2. A n  i l l -condi t ioned  s y s t e m  A x  = b, n = 40, c o n d ( A  ) = 107. R R L S  is u n a b l e  to  r e d u c e  [[ r ,  II o r  
il x~ - x II sa t i s fac tor i ly .  

| 

120 

ACM Transactions on Mathematical Software, Vol 8, No 1, March 1982 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares . 67 

~-,, ~ 5 -5 -'5 ~,- 5 ~ 

Iog:011xk-xll 

tOglo IIATrk II 

A = C G L S  
x = R R L S  
o = L S O R  
+ = R R L S O R  

10 2 0  3 0  4 0  5 0  6 0  7 0  8 0  9 0  100  

Fig. 3. An  ill-conditioned problem, minll A x  - b I[, m = 20, n = 10, cond(A)  := 10 ~. R R L S  and R R L S Q R  
achieve anomalous ly  high a c c u r a c y  in  xh .  

ACM Transacgmns on Mathematical Software, Col. 8, No. 1, March 1982 



68 C.C. Paige and M. A. Saunders 

we have 

(A  + 3A)  + = A + = Z T D - 1 Y  T, 

and the second term in 6x is effectively 

ZTD-2Z~A Ty2c. 

Now suppose 6A is simply an equivalent perturbation in A caused when A 
multiplies a vector v in our test  case. Using the results of rounding error analyses 
given by Wilkinson [25], 

(A + ~ A ) v  - f l (Y~f l (Dfl (Zv)))  = (Y~ + ~Y1)(D + 6D)(Z + 6Z)v,  

where I[ ~ YI [I = ¢ I[ Y1 [[, and so forth; hence 

6A - 3Y~(D + 6 D ) ( Z  + 6Z) + Y , ( D 6 Z  + 6 D ( Z  + 6Z)). 

Using this 6A in the second term for 6x effectively gives 

Z T D - I ( I  + D - I Z 6 Z T D ) ( I  + D-16D)S yTy2c ,  

which is bounded above by about 7 × 10 -6 in norm, rather than 7 as expected. 
This gives a hint of what might be happening above, since a more realistic 
problem would not admit such a relation between rounding errors and residual. 
This does not invalidate the other numerical comparisons, but it does emphasize 
the care needed when constructing artificial test problems. 

8.5 min I l A x  - bll: Normal Behavior 
Figure 4 illustrates more typical performance of the four methods, using the least- 
squares problem P(80, 40, 4, 6) for which cond(A) = 106. All methods reduced 
I]AWrk[] to a satisfactory level, and the final error norm is consistent with a 
conventional sensitivity analysis of the least-squares problem; in this case, no 
more than one significant digit can be expected. Note that  CGLS converged more 
slowly than the other methods. It also displayed rather undesirable fluctuations 
in ][ A Wrk [I considerably beyond the point at which the other methods reached 
limiting accuracy. 

8.6 Some Results Using Greater Precision 

Algorithm LSQR was also applied to the previous four test problems on an IBM 
370 computer using double-precision arithmetic, for which E = 2.2 × 10 -16. With 
increased precision, LSQR gave higher accuracy and also required fewer steps to 
attain this accuracy. This is best seen by referring to the figures. In Figure 1 the 
log of the residual reached -14.4 at the forty-eighth step and stayed there; the 
log of the error was then -8.6, but decreased 20 steps later to -9.3 and stayed 
there. In Figure 2 the logs of the residual and error were -13.8 and -8.0 at step 
44 and differed negligibly from these values thereafter. In Figure 3, logm II A Trk II 
-- -14.6 and lOglo [[ xh - x [I = -6.0 at k -- 32 and thereafter, while in Figure 4, 
loglo H ATrk [[ = --13.9 and loglo I[xh - x ][ = -4.6 at k = 36, with little change for 
larger k. 

8.7 Other Results 

Algorithms CGLS and LSQR have been compared independently by BjSrck [1], 
confirming that on both compatible and incompatible systems LSQR is likely to 
ACM Transact ions  on Mathemat ica l  Software, Vol 8, No 1, March  1982 



LSQR' An Algorithm for Sparse Linear Equations and Sparse Least Squares 69 

togl011Xk-xll 

/ I°gIoIIAT rkll 

"1 

-1 

A = CGLS 
x = R R L S  

o : LSOR 
+ = R R L S Q R  

I I I I I t i I i I 

10 20 30  4 0  50 60  70  8 0  9 0  100 
Fig. 4. An  fl l-condltmned problem, minll A x  - b II, m = 80, n = 40, cond(A) = 106. All m e t h o d s  obta in  
a sat isfactory solution, a l though  CGLS exhibits  slower convergence and  undes i rable  f luc tuat ions  in 

It ATrk II. 

ACM Transactions on Mathematical Software, Vol. 8, No. 1, March 1982 



70 C.C. Palge and M. A. Saunders 

obtain more accurate solutions in fewer iterations. The difference is not significant 
when A is reasonably well conditioned, but in extreme cases CGLS may need up 
to twice as many iterations to obtain comparable precision (Figures 1-4). 

9. SUMMARY 

A direct method may often be preferable to the iterative methods discussed here; 
for instance, the methods given by Bjorck and Duff [2] and George and Heath 
[7] show great promise for sparse least squares. Nevertheless, iterative methods 
will always retain advantages for certain applications. For example, conjugate- 
gradient methods converge extremely quickly if A is of the form M - N where 
M T M  ffi I and N has low rank. They also have low storage requirements (for both 
code and workspace). 

Our aim has been to present the derivation of a new conjugate-gradient 
algorithm, along with details of its implementation and sufficient experimental 
evidence to suggest that it compares favorably with other similar methods and 
that it can be relied upon to give satisfactory numerical solutions to problems of 
practical importance (see also [21]). 

Reliable stopping criteria were regarded here as being essential to any iterative 
method for solving the problems Ax ffi b and min II A x  - b II. The criteria developed 
for LSQR may be useful for other solution methods. Estimates of II A ]1, cond(A), 
and standard errors for x have also been developed to provide useful information 
to the user at minimal cost. 

In closing, we make the following recommendations: 

(1) The symmetric conjugate-gradient algorithm should be applied to the normal 
equations A T A x  •- A Wb  only if there is reason to believe that very few 
iterations will produce a satisfactory estimate of x. 

(2) The least-squares adaptation of symmetric CG will always be more reliable, 
at the expense of slightly more storage and work per iteration. (This is the 
algorithm of Hestenes and Stiefel, described in Section 7.1 of this paper as 
algorithm CGLS.) The additional expense is negligible unless m >> n. 

(3) If A is at all ill-conditioned, algorithm LSQR should be more reliable than 
CGLS, again at the expense of more storage and work per iteration. 

ACKNOWLEDGMENTS 

We wish to thank Dr. Robert  Davies of the New Zealand DSIR for his assistance 
and advice on many aspects of this work. We are also grateful to Dr. Derek 
Woodward of the DSIR for the feedback obtained during his application of LSQR 
to the analysis of gravity-meter observations, to Professor/kke Bjorck for his 
helpful comments on the manuscript, and to Professor Gene Golub and the 
referee for some additional useful suggestions. 

REFERENCES 

1. BJORCK, ~. Use of conjugate gradients for solving linear least squares problems. In Duff, I.S. 
(Ed.), Conjugate-Gradwnt Methods and Stmilar Techntques, Rep. AERE R-9636, Computer 
Science and Systems Division, AERE Harwell, England, 1979, 48-71. 

2. BJORCK, ~ ,  AND DUFF, I.S. A direct method for the solution of sparse linear least squares 
problems. Lmear Algebra Appl. 34 (1980), 43-67. 

ACM TransacUons on Mathematmal Software, Vol 8, No. 1, March 1982. 



LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares • 71 

3. BJORCK, A, AND ELFVING, T. Accelerated projection methods for computing pseudoinverse 
solutions of systems of linear equations. Res Rep. LITH-MAT-R-1978-5, Dep. Mathematics, 
Linkoping Univ., Linkoping, Sweden, 1978. 

4. CHEN, Y.T. Iterative methods for linear least-squares problems. Res. Rep. CS-75-04, Dep. of 
Computer Science, Univ. Waterloo, Waterloo, Ont., Canada, 1975. 

5. ELFVING, T. On the conjugate gradient method for solving linear least-squares problems. Res. 
Rep. L1TH-MAT-R-1978-3, Dep. Mathematics, Linkoping Univ., Link~ping, Sweden, 1978. 

6. FADDEEV, D.K., AND FADDEEVA, V.N. Computattonal Methods of Linear Algebra, Freeman, 
London, 1963. 

7. GEORGE, A, AND HEATH, M T. Solution of sparse linear least squares problems using Givens 
rotations. Linear Algebra Appl. 34 (1980), 69-83. 

8. GOLUB, G.H. Numerical methods for solving linear least-squares problems. Numer. Math. 7 
(1965), 206-216. 

9. GOLUB, G.H., AND KAHAN, W. Calculating the singular values and pseudoinverse of a matrix. 
SIAM J. Numer. Anal. 2 (1965), 205-224. 

10. HESTENES, M.R., AND STIEFEL, E. Methods of conjugate gradients for solving linear systems J. 
Res. N.B.S. 49 (1952), 409-436. 

11. HOUSEHOLDER, A.S. Terminating and non-terminating iterations for solving linear systems. 
SIAM J. Appl. Math. 3 (1955), 67-72. 

12. KENNEDY, W.J., AND GENTLE, J.E. Stattstwal Computing. Marcel Dekker, Inc., New York and 
Basel, 1980. 

13. LANCZOS, C. An iteration method for the solution of the eigenvalue problem of linear differential 
and integral operators. J Res. N.B.S. 45 (1950), 255-282. 

14. LEwis, J.G. Algorithms for sparse matrix eigenvalue problems. Res. Rep. STAN-CS-77-595, 
Stanford Univ., Stanford, CA, 1977. 

15. NASHED, M.Z. Aspects of generalized inverses in analysis and regularization. In Nashed, M.A. 
(Ed.), Generahzed Inverses and Applwat~ons, Academic Press, New York, 1976, 193-244. 

16. NELDER, J.A. GLIMManual. Numerical Algorithms Group, 13 Banbury Road, Oxford, England, 
1975 

17. PAIGE, C.C. Bidiagonallzatlon of matrices and solution of linear equations. SIAM J. Numer. 
Anal. 11 (1974), 197-209. 

18. PAIGE, C.C. Error analysis of the Lanczos algorithm for tridiagonalizing a symmetric matrix. J. 
Inst. Maths. Appl 18 (1976), 341-349. 

19 PAIGE, C.C., AND SAUNDERS, M.A. Solution of sparse indefinite systems of equations and least- 
squares problems Res. Rep. STAN-CS-73-399, Stanford Univ., Stanford, CA, 1973. 

20 PAIGE, C.C., AND SAUNDERS, M.A. Solution of sparse mdefinite systems of linear equations. 
SIAM J. Numer. Anal. 12 (1975), 617-629. 

21 PAIGE, C.C., AND SAUNDERS, M.A. A bidiagonalization algorithm for sparse linear equations 
and least-squares problems. Rep. SOL 78-19, Dep. Operations Research, Stanford Univ., Stanford, 
CA, 1978. 

22. PAIGE, C.C., AND SAUNDERS, M.A. LSQR' Sparse linear equations and least-squares problems. 
ACM Trans. Math. Softw., to appear. 

23. STEWART, G.W. Research, development and LINPACK. In Rice, J.R. (Ed.), Mathematwal 
Software III, Academic Press, New York, 1977, pp. 1-14. 

24. VAN HEIJST, J., JACOBS, J., AND SCHERDERS, J. Kleinste-kwadraten problemen. Dep. Mathe- 
matics Rep., Eindhoven University of Technology, Eindhoven, The Netherlands, August 1976 

25. WILKINSON, J.H. The Algebraw Etgenvalue Problem. Oxford University Press (Clarendon), 
New York, 1965. 

Received June 1980; revised September 1981, accepted November 1981 

ACM Transactmns on Mathematical Software, Vol. 8, No. 1, March 1982 


