

































Bansal, Gutierrez, and Keiser: Quantile Judgments to Deduce Probability Distributions
Operations Research 00(0), pp. 000–000, c© 0000 INFORMS 1

Online Appendix A

1. Proof of Lemma 1

Clearly μ0 = E[X0] = 1. For a location-scale random variable X, we have X = θ1 + θ2Z where Z is the

standardized random variable corresponding to X, when θ1 = 0 and θ2 = 1. Denoting E[Zj ] = κj , we obtain

claim (a) from

E[Xj ] = E[(θ1 + θ2Z)
j ] =

j∑

i=0

(
j

i

)

θi1θ
j−i
2 κj−i (16)

where the last equality follows from the binomial expansion of the term (θ1 + θ2Z)j .

To obtain (b) we substitute X = θ1 + θ2Z and μ1 = θ1 + θ2κ1 from (16) into E[(X −μ1)j ] to get

E[(X −μ1)
j ] = E[(θ1 + Zθ2 − θ1 −κ1θ2)

j ] = θj2E[(Z −κ1)
j ] (17)

From the binomial expansion of the last term, and using E[Zj−i] = κj−i, we obtain θ
j
2E[(Z − κ1)

j ] =

θj2
∑j

i=0

(
j

i

)
(−κ1)iκj−i. �

2. Proof of Theorem 1

The Lagrangian of the problem is

L = wk
tΩwk − [Z

twk − ak][λ] (18)

where λ = [λ1, λ2]
t is the vector of Lagrange multipliers for vector θt = (θ1, θ2) ∈ R×R++. Then, the first

order optimality conditions are given by the following m +2 equations:

∇wkL =2Ωwk −Zλ = 0, (19)

∇λL =−Z
twk + ak = 0. (20)

This is a set of m + 2 linear equations with the same number of unknowns. It can also be seen from the

formulation that this system of equations has a full rank so a unique solution exists. We first reduce (19) by

solving for λ; to this end we pre-multiply (19) by ZtΩ−1, and obtain

2Ztwk = Z
tΩ−1Zλ (21)

The inverse Ω−1 exists because Ω is positive definite. Now, substituting Ztwk = ak from (20), we obtain

from (21)

λ = 2(ZtΩ−1Z)−1ak (22)

The inverse (ZtΩ−1Z)−1 exists because Z has a full rank and Ω−1 is positive definite. Substituting λ from

(22) into (19) reduces (19) to a system of m equations with m unknowns, and we solve this system as follows

2Ωwk −Zλ = 0⇒ 2Ωwk = 2Z(Z
tΩ−1Z)−1ak ⇒wk = Ω

−1Z(ZtΩ−1Z)−1ak (23)

�



Bansal, Gutierrez, and Keiser: Quantile Judgments to Deduce Probability Distributions
2 Operations Research 00(0), pp. 000–000, c© 0000 INFORMS

3. Proof of Proposition 2

We showed above that

wktZ = akt. (24)

For the estimation of mean μ1, we have a1 = [1, κ1], which then implies that
∑

w∗1i = 1. Similarly, for the

estimation of standard deviation μ2, a2 =
[
0,
√

κ2 −κ21
]
, which implies that

∑
w∗2i = 0. �

4. Proof of the Variance of μ̂k for Section 5.1

We are interested in the variance of wktq̂. Since wktq̂ = wkt(Zθ + �) = wktZθ + wkt� and the term

wktZθ is a constant, it follows that the variance of wktq̂ is equal to the variance of wkt� which is given by

wktΩwk for k = 1,2. Replacing the expressions for wk obtained in Theorem 1 we obtain the variance as

wk
tΩwk = ak

t

(ZtΩ−1Z)−1ZtΩ−1ΩΩ−1Z(ZtΩ−1Z)−1ak = ak
t

(ZtΩ−1Z)−1ak. (25)

Note that a1 = [1, κ1]
t for estimating μ1, a2=

[
0,
√

κ2 −κ21
]t

for estimating μ2.

�

5. Proof of Proposition 3

(a) For the mean, we note that variance of the distribution of a sample mean of size N1 is equal to μ22/N1

where μ22 is the population variation. Equating this variance with the variance [1, κ1](Z
tΩ−1Z)−1[1, κ1]

t

obtained above in (25), we obtain N1 =
μ2

2

[1,κ1](ZtΩ−1Z)−1[1,κ1]
t .

(b) Stuart and Ord (1994) show on p 352 that the variance of sample standard deviation can be approximated

as

V ar(S)≈
E[(x−μ1)4]− (E[(x−μ1)j ])2

4N2μ22
(26)

We know from Lemma 1 that

E[(X −μ1)
i] = θi2

i∑

j=0

(−κ1)
jκi−j (27)

From Lemma 1, we know that θ2 = μ2/
√

κ2 −κ21. Substituting this in (27), we obtain E[(X − μ1)
i] =

μi2
∑i

j=0(−κ1)
jκi−j/

(√
κ2 −κ21

)i
Substituting this in (26), we obtain

V ar(S)≈
μ2

2

4N2






∑4
j=0(−κ1)

jκ4−j

(κ2 −κ21)2
−

(∑2
j=0(−κ1)

jκ2−j

)2

(κ2 −κ21)2




 (28)

Equating this with the variance in (25) [0,
√

κ2 −κ21](Z
tΩ−1Z)−1[0,

√
κ2 −κ21]

t
, we obtain

N2 ≈
μ2

2




∑4

j=1(−κ1)
jκ4−j

(κ2−κ
2
1)

2 −
(∑2j=1(−κ1)jκ2−j)

2

(κ2−κ
2
1)

2





4[0,
√

κ2−κ21](Z
tΩ−1Z)−1[0,

√
κ2−κ21]

t . �

6. Proof of Proposition 4

(a) We start by noting the general result in Theorem 1: w∗k
t = akt(Z

t
Ω−1Z)−1Z

t
Ω−1. Next, we define each of



Bansal, Gutierrez, and Keiser: Quantile Judgments to Deduce Probability Distributions
Operations Research 00(0), pp. 000–000, c© 0000 INFORMS 3

these components when j=1,2,...,n experts provide quantile judgments. The matrix Zt = [Z0
t,Z0

t, ...,Z0
t]

where Z0
t appears n times, once for each expert. The subscript 0 simply suggests that this is a constant

matrix since all experts provide judgments for the same set of quantiles.. The matrix Ω is a mn×mn block

diagonal matrix with diagonal blocks rjΩ0 where Ω0 is of size m×m. Then,

1. (Z
t
Ω−1Z) = Z0

t

Ω−10 Z0[
∑N

j=1(1/rj)] = Z0
t

Ω−10 Z0R where R = [
∑N

j=1(1/rj)]

2. Z
t
Ω−1 = [Z0

t

Ω−10 (1/r1),Z0
t

Ω−10 (1/r2), ...,Z0
t

Ω−10 (1/rN)]

3. It follows from points 1 and 2 above that w∗k
t = akt(Z0

t

Ω−10 Z0)
−1Z

t

0Ω
−1
0 [(1/r1)/R, (1/r2)/R, ..., (1/rN)/R].

Now we can write this expression as w∗k
t = wck

t[(1/r1)/R, (1/r2)/R, ..., (1/rN)/R] where wck
t =

akt(Z0
t

Ω−10 Z0)
−1Z

t

0Ω
−1
0 . Further, the vector w

∗
k
t is composed of the m weights for each expert j : w∗k

t =

[w1k
t
,w2k

t
, ...,wnk

t]. It follows from these two relations that wjk = αjw
c
k where the expert j ’s marginal weight

is equal to αj = (1/rj)/R.

(b) Consider the case when expert j is the only expert available with matrix Ω0 for eliciting quantiles Z0.

Then R = (1/rj). Substituting this expression in point 3 above, it follows that the weight for this expert is

equal to (1/rj)/R = 1, and the weights for his quantile judgments are equal to his independent weights.

�

7. Proof for Proposition 5: 1) The proof follows from the text above the proposition.

2) We will first obtain explicit expressions for the weights w∗k = Ω
−1Z(ZtΩ−1Z)−1ak with Ω = KΩ′

with Ω′ = I, and then show that these weights are identical to the weights obtained using the formulation

minμ1,μ2

{
∑m

i=1

(
Φ−1(pi;μ1, μ2)− q̂i

)2
}

. For rigor purposes, we provide the result for a more general case

when the diagonals elements of Ω′ are equal to 1, and the off-diagonal elements are equal to the correlation

value ρ, and for brevity we will provide the proof for μ1. The analysis for μ2 is analogous.

To obtain the explicit expressions of the weights we need three intermediate results.

(a) Ω−1 = 1
(1−ρ)K2

(
I − ρM1

1+(m−1)ρ

)
. To establish this claim define M1 as an (m×m) matrix of ones and verify

that

ΩΩ−1 =
1

1− ρ

(

(1− ρ)I + ρM1

)(

I −
ρ

1+ (m− 1)ρ
M1

)

,

=
1

1− ρ

[

(1− ρ)I + ρM1 −
ρ(1− ρ)

1+ (m− 1)ρ
M1 −

ρ2m

1+ (m− 1)ρ
M1

]

= I.

(b)

ZtΩ−1 =
1

K2(1+ (m− 1)ρ)

(
1 ∙ ∙ ∙ 1 ∙ ∙ ∙ 1

(1+(m−1)ρ)z1−S1ρ
1−ρ

∙ ∙ ∙ (1+(m−1)ρ)zi−S1ρ
1−ρ

∙ ∙ ∙ (1+(m−1)ρ)zm−S1ρ
1−ρ

)

.

To establish this claim we use (a) to obtain

Z
t

Ω−1 =
1

(1− ρ)K2

(
1 ∙ ∙ ∙ 1 ∙ ∙ ∙ 1
z1 ∙ ∙ ∙ zi ∙ ∙ ∙ zm

)(
I −

ρ

1+ (m− 1)ρ
M1

)

=
1

K2(1+ (m− 1)ρ)

(
1 ∙ ∙ ∙ 1

(1+(m−1)ρ)z1−S1ρ
1−ρ

∙ ∙ ∙ (1+(m−1)ρ)zm−S1ρ
1−ρ

)

.

(c) Denote S1 ≡
∑m

i=1 zi and S2 ≡
∑m

i=1 z
2
i , then the inverse of (2× 2) matrix Z

tΩ−1Z is obtained as

(ZtΩ−1Z)−1 =
K2

mS2 −S21

(
S2(1+ (m− 1)ρ)−S21ρ −S1(1− ρ)

−S1(1− ρ) m(1− ρ)

)

.



Bansal, Gutierrez, and Keiser: Quantile Judgments to Deduce Probability Distributions
4 Operations Research 00(0), pp. 000–000, c© 0000 INFORMS

Therefore, the weights for the mean are obtained as w∗μ1 = [1, κ1](Z
tΩ−1Z)−1ZtΩ−1, which on simplifica-

tion reduce to

wi =
(
S2 − ziS1 + nziκ1 −S1κ1 − 2κ1S1 +2mκ

2
1

)
mS2 −S

2
1(29)

2) We now show that these weights coincide with the weights obtained for the minimization of the least

squares minμ1,μ2 F =

{
∑m

i=1

(
μ2 −

κ1√
κ2−κ21

μ2 + zi
μ2√

κ2−κ21
− q̂i

)2
}

. We will drop the indices over the summa-

tion in the rest of the proof. Taking the first order derivatives, we obtain:

∂F

∂μ1
= 2

∑(
μ1 + μ2

zi −κ1√
κ2 −κ21

− q̂i
)
= 0 (30)

∂F

∂μ2
= 2

∑(
μ1 + μ2

zi −κ1√
κ2 −κ21

− q̂i
) zi −κ1√

κ2 −κ21
= 0 (31)

Now, we can write (30) as mμ1 + μ2
∑

zi√
κ2−κ21

−μ2
mκ1√
κ2−κ21

−
∑

q̂i = 0, or, using
∑

zi = S1 equivalently,

μ1 =
μ2(mκ1 −S1)

m
√

κ2 −κ21
+

∑
q̂i

m
(32)

Next, we can simplify (31) using
∑

z2i = S2 and obtain,

μ2 =

√
κ2 −κ21(

∑
qizi −

∑
qiκ1 + μ1(S1 −mκ1))

S2 − 2κ1S1 + nκ21
(33)

Now, substituting (33) into (32), and simplification, we obtain

μ1 =

∑
qi
(
S2 − ziS1 + nziκ1 −S1κ1 − 2κ1S1 +2mκ21

)

mS2 −S21
(34)

which implies the weights of

wμi =

(
S2 − ziS1 + nziκ1 −S1κ1 − 2κ1S1 +2mκ21

)

mS2 −S21
(35)

These weights are identical to the weights obtained in the least squares formulation in (29). The weights

for μ2 can be shown to be equal similarly. �

References:

Stuart, Alan, J Keith Ord. 1994. Kendalls advanced theory of statistics. vol. i. distribution theory. Arnold,

London .


