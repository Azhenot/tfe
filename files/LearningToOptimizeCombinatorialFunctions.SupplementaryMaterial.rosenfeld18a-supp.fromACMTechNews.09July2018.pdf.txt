






















































Learning to Optimize Combinatorial Functions: Supplementary Material


Learning to Optimize Combinatorial Functions:
Supplementary Material

Nir Rosenfeld 1 Eric Balkanski 1 Amir Globerson 2 Yaron Singer 1

Theorem 1. For all α > 1 and �, δ > 0, for m sufficiently
large, there exists a family of functions F and a function
MPMAC(·) such that

• for all �′, δ′ > 0: F is α-PMAC-learnable with sample
complexity MPMAC(n, δ′, �′, α), and

• given strictly less than MPMAC(n, δ, 1− (1− �)1/m, α)
samples, F is not α-DOPS, i.e.,

MDOPS(n,m, δ, �, α) ≥MPMAC(n, δ, 1−(1−�)1/m, α).

Proof. Fixα > 1 and � > 0. Define p := 1−(1−�)1/m+�s,
for some small constant �s > 0, and let S1, . . . , S1/p be
1/p arbitrary distinct sets. The hard class of functions is
F = {fi}i∈[1/p] where

fi(S) =

{
α if S = Si
1
2 otherwise

Consider the distributionD which is the uniform distribution
over sets S1, . . . , S1/p, so Sj is drawn with probability p
for all j ∈ [1/p]. We first argue that the sample complexity
for PMAC-learning f over D is at most

MPMAC(n, δ
′, �′, α) =

{
0 if �′ ≥ p

log(1/δ′)
log(1/(1−p)) if �

′ < p

Note that if �′ ≥ p, f̃(S) = 1/2 for all S is correct with
probability 1− p ≥ 1− �′ over S ∼ D and with probability
1 over the samples. If �′ < p, if there exists sample Si such
that f(Si) = α, then f̃(Si) = α, and f̃(S) = 1/2 for all
other S. Note that that this is correct with probability 1 over
S ∼ D if Si is in the samples. The probability that Si is in

1Harvard University 2Tel Aviv University. Correspondence to:
Nir Rosenfeld <nir.rosenfeld@g.harvard.edu>.

Proceedings of the 35 th International Conference on Machine
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018
by the author(s).

the samples

1− (1− p)m = 1− (1− p)
log(1/δ′)

log(1/(1−p))

= 1− e
log(δ′)

log(1−p) log(1−p)

= 1− δ′.

Thus, f̃ is correct with probability 1− δ′ over the samples.
Next, we argue that for all δ > 0 and m sufficiently large,
the sample complexity for DOPS is at least

MPMAC(n, δ, 1− (1− �)1/m, α) =

MPMAC(n, δ, p− �s, α) =
log(1/δ)

log(1/(1− p))
.

Consider the random function fi where i ∈ [1/p] is uni-
formly random. Let F ′ be the randomized collection of
functions fi such that Si is in the testing set but not in the
training set. Since Si is not in the testing set, we have that
for all fi ∈ F ′ and for all sets S in the testing set,

fi(S) = 1.

Thus, the functions in F ′ are indistinguishable from the
samples in the training set. This implies that the decisions
of the algorithm are independent of the random variable i,
conditioned on fi ∈ F ′. Let S be the set in the testing set
that is returned by the algorithm, we obtain that

E
i:fi∈F ′

[fi(S)] = Pr
i:fi∈F ′

[S = Si] · α+ Pr
i:fi∈F ′

[S 6= Si] ·
1

2

≤ α
|F ′|

+
1

2

since S is independent of i conditioned on fi ∈ F ′. Con-
sider the case where Si is not in the training set with prob-
ability strictly greater than δ. The probability that Si is in
the testing set is 1− (1− p)m = �+ �s. Thus a function is
in F ′ with probability at least δ(� + �s). Note that 1/p is
arbitrarily large if m is arbitrarily large. Thus, |F ′| > 2α
with arbitrarily large probability if m is arbitrarily large for
fixed �, δ, and α. Combining with the previous inequality,



Distributional Optimization from Samples: Supplementary Material

this implies that

E
i:fi∈F ′

[fi(S)] < 1 =
1

α
· fi(Si)

=
1

α
· E
i:fi∈F ′

[ max
S∈Ste

fi(S)]

where the last equality is since Si ∈ Ste for all i ∈ F ′.
Thus, there exists at least one function fi ∈ F such that the
algorithm does not obtain an α-approximation when Si is
in the testing set and not in the training set.

The probability that Si is in the testing set is 1 − (1 −
p)m = �+ �s. Thus, Si needs to be in the training set with
probability at least 1− δ, otherwise we don’t get an α-apx
with probability 1− �. The probability that Si is not in the
training set is (1− p)m. Thus, we need δ > (1− p)m, or

m >
log(1/δ)

log(1/(1− p))
= mPMAC(n, δ, p− �s, α)
= mPMAC(n, δ, 1− (1− �)1/m, α).


