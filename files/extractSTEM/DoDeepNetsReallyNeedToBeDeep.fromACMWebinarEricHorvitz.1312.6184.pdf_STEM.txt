


















































Do deep net realli need to be deep? 
*** 

draft for nip 2014 (not camera readi copy) 
*** 

lei jimmi Ba 
univers of toronto 

jimmy@psi.utoronto.ca 

rich caruana 
microsoft research 

rcaruana@microsoft.com 

abstract 

currently, deep neural network be the state of the art on problem such a speech 
recognit and comput vision. In thi paper we empir demonstr that 
shallow feed-forward net can learn the complex function previous learn by 
deep net and achiev accuraci previous onli achiev with deep models. 
moreover, in some case the shallow neural net can learn these deep function 
use the same number of paramet a the origin deep models. On the timit 
phonem recognit and cifar-10 imag recognit tasks, shallow net can be 
train that perform similarli to complex, well-engineered, deeper convolut 
architectures. 

1 introduct 

you be give a train set with 1M label points. when you train a shallow neural net with one 
fully-connect feed-forward hidden layer on thi data you obtain 86% accuraci on test data. when 
you train a deeper neural net a in [1] consist of a convolut layer, pool layer, and three 
fully-connect feed-forward layer on the same data you obtain 91% accuraci on the same test set. 

what be the sourc of thi improvement? Is the 5% increas in accuraci of the deep net over the 
shallow net because: a) the deep net have more parameters; b) the deep net can learn more complex 
function give the same number of parameters; c) the deep net have good bia and learn more 
interesting/us function (e.g., becaus the deep net be deeper it learn hierarch represent 
[5]); d) net without convolut can’t easili learn what net with convolut can learn; e) current 
learn algorithm and regular method work good with deep architectur than shallow 
architectures[8]; f) all or some of the above; g) none of the above? 

there have be attempt to answer the question above. It have be show that deep net coupl 
with unsupervis layer-by-lay pre-train technique[10] [19] work well. In [8], the author show 
that depth combin with pre-train provid a good prior for model weights, thu improv gen- 
eralization. there be well-known earli theoret work on the represent capac of neural 
nets. for example, it be prove that a network with a larg enough singl hidden layer of sigmoid 
unit can approxim ani decis boundary[4]. empir work, however, show that it be difficult 
to train shallow net to be a accur a deep nets. for vision tasks, a recent studi on deep con- 
volut net suggest that deeper model be prefer under a paramet budget [7]. In [5], the 
author train shallow net on sift featur to classifi a large-scal imagenet dataset and show 
that it be challeng to train larg shallow net to learn complex functions. and in [17], the author 
show that deeper model be more competit than shallow model in speech acoust modeling. 

1 

ar 
X 

iv 
:1 

31 
2. 

61 
84 

v7 
[ 

c 
.L 

G 
] 

1 
1 

O 
ct 

2 
01 

4 



In thi paper we provid empir evid that shallow net be capabl of learn the same 
function a deep nets, and in some case with the same number of paramet a the deep nets. We 
do thi by first train a state-of-the-art deep model, and then train a shallow model to mimic the 
deep model. the mimic model be train use the model compress scheme describ in the next 
section. remarkably, with model compress we be abl to train shallow net to be a accur 
a some deep models, even though we be not abl to train these shallow net to be a accur a 
the deep net when the shallow net be train directli on the origin label train data. If a 
shallow net with the same number of paramet a a deep net can learn to mimic a deep net with 
high fidelity, then it be clear that the function learn by that deep net do not realli have to be deep. 

2 train shallow net to mimic deep net 

2.1 model compress 

the main idea behind model compress be to train a compact model to approxim the function 
learn by a larger, more complex model. for example, in [3], a singl neural net of modest size 
could be train to mimic a much larg ensembl of model — although the small neural net 
contain 1000 time few parameters, often they be just a accur a the ensembl they be 
train to mimic. model compress work by pass unlabel data through the large, accur 
model to collect the score produc by that model. thi synthet label data be then use to 
train the small mimic model. the mimic model be not train on the origin labels—it be train 
to learn the function that be learn by the larg model. If the compress model learn to mimic 
the larg model perfectli it make exactli the same predict and mistak a the complex model. 

surprisingly, often it be not (yet) possibl to train a small neural net on the origin train data to be 
a accur a the complex model, nor a accur a the mimic model. compress demonstr 
that a small neural net could, in principle, learn the more accur function, but current learn 
algorithm be unabl to train a model with that accuraci from the origin train data; instead, we 
must train the complex intermedi model first and then train the neural net to mimic it. clearly, 
when it be possibl to mimic the function learn by a complex model with a small net, the function 
learn by the complex model wasn’t truli too complex to be learn by a small net. thi suggest 
to u that the complex of a learn model, and the size of the represent best use to learn that 
model, be differ things. In thi paper we appli model compress to train shallow neural net 
to mimic deeper neural nets, therebi demonstr that deep neural net may not need to be deep. 

2.2 mimic learn via regress logit with L2 loss 

On both timit and cifar-10 we train shallow mimic net use data label by either a deep 
net, or an ensembl of deep nets, train on the origin timit or cifar-10 train data. the 
deep model be train in the usual way use softmax output and cross-entropi cost function. the 
shallow mimic models, however, instead of be train with cross-entropi on the 183 p valu 
where pk = ezk/ 

∑ 
j e 

zj output by the softmax layer from the deep model, be train directli on 
the 183 log probabl valu z, also call logit, befor the softmax activation. 

train on these logit valu make learn easi for the shallow net by place emphasi on 
all predict targets. becaus the logit captur the logarithm relationship between the proba- 
biliti predictions, a student model train on logit have to learn all of the addit fine detail 
relationship between label that be not obviou in the probabl space yet be learn by the 
teacher model. for example, assum there be three target that the teacher predict with probabl 
[2e − 9, 4e − 5, 0.9999]. If we use these probabl a predict target directli to minim a 
cross entropi loss function, the student will focu on the third target and easili ignor the first and 
second target. alternatively, one can extract the logit predict from the teacher model and obtain 
our new target [10, 20, 30]. the student will learn to regress the third target, yet it still learn the 
first and second target along with their rel difference. the logit valu provid richer informa- 
tion to student to mimic the exact behaviour of a teach model. moreover, consid a second train 
case where the teacher predict logit [−10, 0, 10]. after softmax, these logit yield the same pre- 
dict probabl a [10, 20, 30], yet clearli the teacher have learn intern to model these two 
case veri differently. By train the student model on the logit directly, the student be good abl 

2 



to learn the intern model learn by the teacher, without suffer from the inform loss that 
occur after pass through the logit to probabl space. 

We formul the snn-mimic learn object function a a regress problem give train 
data {(x(1), z(1)),...,(x(t ), z(t )) }: 

l(w,β) = 1 
2T 

∑ 
t 

||g(x(t);w,β)− z(t)||22, (1) 

where, W be the weight matrix between input featur x and hidden layer, β be the weight from 
hidden to output units, g(x(t);w,β) = βf(wx(t)) be the model predict on the tth train data 
point and f(·) be the non-linear activ of the hidden units. the paramet W and β be updat 
use standard error back-propag algorithm and stochast gradient descent with momentum. 

We have also experi with other differ mimic loss function, such a minim the KL 
diverg kl(pteacher‖pstudent) cost function and L2 loss on the probability. logit regress out- 
perform all the other loss function and be one of the key techniqu for obtain the result in the 
rest of thi paper. We found that normal the logit from the teacher model, by subtract the 
mean and divid the standard deviat of each target across the train set, can improv the L2 
loss slightli dure training. normal be not crucial for obtain a good student model. 

2.3 speeding-up mimic learn by introduc a linear layer 

To match the number of paramet in a deep net, a shallow net have to have more non-linear hidden 
unit in a singl layer to produc a larg weight matrix W . when train a larg shallow neural 
network with mani hidden units, we find it be veri slow to learn the larg number of paramet in the 
weight matrix between input and hidden layer of size o(hd), where D be input featur dimens 
and H be the number of hidden units. becaus there be mani highli correl paramet in thi 
larg weight matrix gradient descent converg slowly. We also notic that dure learning, shallow 
net spend most of the comput in the costli matrix multipl of the input data vector and 
larg weight matrix. the shallow net eventu learn accur mimic functions, but train to 
converg be veri slow (multipl weeks) even with a gpu. 

We found that introduc a bottleneck linear layer with k linear hidden unit between the input 
and the non-linear hidden layer speed up learn dramatically: we can factor the weight matrix 
W ∈ rh×d into the product of two low rank matrices, U ∈ rh×k and V ∈ rk×d, where 
k << d,h . the new cost function can be write as: 

l(u, V, β) = 1 
2T 

∑ 
t 

||βf(uv x(t))− z(t)||22 (2) 

the weight U and V can be learnt by back-propag through the linear layer. thi re- 
parameter of weight matrix W not onli increas the converg rate of the shallow mimic 
nets, but also reduc memori space from o(hd) to o(k(h +d)). 

factor weight matrix have be previous explor in [16] and [20]. while these prior work 
focu on use matrix factor in the last output layer, our method be appli between input and 
hidden layer to improv the converg speed dure training. 

the reduc memori usag enabl u to train larg shallow model that be previous infeas 
due to excess memori usage. the linear bottl neck can onli reduc the represent power 
of the network, and it can alway be absorb into a signl weight matrix W . 

3 timit phonem recognit 

the timit speech corpu have 462 speaker in the train set. there be a separ develop set 
for cross-valid includ 50 speakers, and a final test set with 24 speakers. the raw waveform 
audio data be pre-process use 25m ham window shift by 10m to extract fourier- 
transform-bas filter-bank with 40 coeffici (plu energy) distribut on a mel-scale, togeth 

3 



with their first and second tempor derivatives. We includ +/- 7 nearbi frame to formul the 
final 1845 dimens input vector. the data input featur be normal by subtract the mean 
and divid by the standard deviat on each dimension. all 61 phonem label be repres in 
tri-state, i.e., 3 state for each of the 61 phonemes, yield target label vector with 183 dimens 
for training. At decod time these be map to 39 class a in [13] for scoring. 

3.1 deep learn on timit 

deep learn be first success appli to speech recognit in [14]. We follow the same 
framework and train two deep model on timit, dnn and cnn. dnn be a deep neural net con- 
sist of three fully-connect feedforward hidden layer consist of 2000 rectifi linear unit 
(relu) [15] per layer. cnn be a deep neural net consist of a convolut layer and max-pool 
layer follow by three hidden layer contain 2000 relu unit [2]. the cnn be train use 
the same convolut architectur a in [6]. We also form an ensembl of nine cnn models, 
ecnn. 

the accuraci of dnn, cnn, and ecnn on the final test set be show in tabl 1. the error rate 
of the convolut deep net (cnn) be about 2.1% good than the deep net (dnn). the tabl also 
show the accuraci of shallow neural net with 8000, 50,000, and 400,000 hidden unit (snn-8k, 
snn-50k, and snn-400k) train on the origin train data. despit have up to 10x a mani 
paramet a dnn, cnn and ecnn, the shallow model be 1.4% to 2% less accur than the 
dnn, 3.5% to 4.1% less accur than the cnn, and 4.5% to 5.1% less accur than the ecnn. 

3.2 learn to mimic an ensembl of deep convolut timit model 

the most accur singl model we train on timit be the deep convolut architectur in [6]. 
becaus we have no unlabel data from the timit distribution, we be forc to use the same 1.1m 
point in the train set a unlabel data for compress by throw away their labels.1 re-us 
the train set reduc the accuraci of the mimic models, increas the gap between the teacher and 
mimic model on test data: model compress work best when the unlabel set be much larg 
than the train set, and when the unlabel sampl do not fall on train point where the teacher model 
be more like to have overfit. To reduc the impact of the gap caus by perform compress 
with the origin train set, we train the student model to mimic a more accur ensembl of deep 
convolut models. 

We be abl to train a more accur model on timit by form an ensembl of 9 deep, convo- 
lution neural nets, each train with somewhat differ train sets, and with architectur with 
differ kernel size in the convolut layers. We use thi veri accur model, ecnn, a the 
teacher model to label the data use to train the shallow mimic nets. As describ in section 2.2, 
the logit (log probabl of the predict values) from each cnn in the ecnn model be averag 
and the averag logit be use a final regress target to train the mimic snns. 

We train shallow mimic net with 8k (snn-mimic-8k) and 400k (snn-mimic-400k) hidden 
unit on the re-label 1.1m train points. As describ in section 2.3, both mimic model have 
250 linear unit between the input and non-linear hidden layer to speed up learn — preliminari 
experi suggest that for timit there be littl benefit from use more than 250 linear units. 

3.3 compress result for timit 

the bottom of tabl 1 show the accuraci of shallow mimic net with 8000 relu and 400,000 
relu (snn-mimic-8k and -400k) train with model compress to mimic the ecnn. surpris- 
ingly, shallow net be abl to perform a well a their deep counter-part when train with model 
compress to mimic a more accur model. A neural net with one hidden layer (snn-mimic-8k) 
can be train to perform a well a a dnn with a similar number of parameters. furthermore, if we 
increas the number of hidden unit in the shallow net from 8k to 400k (the larg we could train), 
we see that a neural net with one hidden layer (snn-mimic-400k) can be train to perform com- 
parabl to a cnn even though the snn-mimic-400k net have no convolut or pool layers. 

1that snn can be train to be a accur a dnn use onli the origin train data data highlight 
that it should be possibl to train accur snn on the origin train data give good learn algorithms. 

4 



architectur # param. # hidden unit per 

snn-8k 8k + dropout ∼12m ∼8k 23.1%train on origin data 
snn-50k 50k + dropout ∼100m ∼50k 23.0%train on origin data 
snn-400k 250l-400k + dropout ∼180m ∼400k 23.6%train on origin data 
dnn 2k-2k-2k + dropout ∼12m ∼6k 21.9%train on origin data 
cnn c-p-2k-2k-2k + dropout ∼13m ∼10k 19.5%train on origin data 
ecnn ensembl of 9 cnn ∼125m ∼90k 18.5% 

snn-mimic-8k 250l-8k ∼12m ∼8k 21.6%no convolut or pool layer 
snn-mimic-400k 250l-400k ∼180m ∼400k 20.0%no convolut or pool layer 

tabl 1: comparison of shallow and deep models: phone error rate (per) on timit core test set. 

76 

77 

78 

79 

80 

81 

82 

83 

1 10 100 

A 
c 
c 
u 
ra 

c 
y 
o 

n 
T 

IM 
IT 

D 
e 
v 
S 

e 
t 

number of paramet (millions) 

shallownet 
deepnet 

shallowmimicnet 
convolut net 
ensembl of cnn 

75 

76 

77 

78 

79 

80 

81 

82 

1 10 100 

A 
c 
c 
u 
ra 

c 
y 
o 

n 
T 

IM 
IT 

T 
e 
s 
t 

S 
e 
t 

number of paramet (millions) 

shallownet 
deepnet 

shallowmimicnet 
convolut net 
ensembl of cnn 

figur 1: accuraci of snns, dnns, and mimic snn vs. # of paramet on timit dev (left) and 
test (right) sets. accuraci of the cnn and target ecnn be show a horizont line for reference. 

thi be interest becaus it suggest that a larg singl hidden¡ layer without a topolog custom 
design for the problem be abl to reach the perform of a deep convolut neural net that 
be care engin with prior structur and weight share without ani increas in the number 
of train examples, even though the same architectur train on the origin data could not. 

figur 1 show the accuraci of shallow net and deep net train on the origin timit 1.1m data, 
and shallow mimic net train on the ecnn targets, a a function of the number of paramet in 
the models. the accuraci of the cnn and the teacher ecnn be show a horizont line at the top 
of the figures. when the number of paramet be small (about 1 million), the snn, dnn, and snn- 
mimic model all have similar accuracy. As the size of the hidden layer increas and the number 
of paramet increases, the accuraci of a shallow model train on the origin data begin to lag 
behind. the accuraci of the shallow mimic model, however, match the accuraci of the dnn until 
about 4 million parameters, when the dnn begin to fall behind the mimic. the dnn asymptot 
at around 10m parameters, while the shallow mimic continu to increas in accuracy. eventu 
the mimic asymptot at around 100m paramet to an accuraci compar to that of the cnn. 
the shallow mimic never achiev the accuraci of the ecnn it be tri to mimic (becaus there 
be not enough unlabel data), but it be abl to match or exceed the accuraci of deep net (dnns) 
have the same number of paramet train on the origin data. 

5 



4 object recognition: cifar-10 

To verifi that the result on timit gener to other learn problem and task domains, we ran 
similar experi on the cifar-10 object recognit task[12]. cifar-10 consist of a set 
of natur imag from 10 differ object classes: airplane, automobile, bird, cat, deer, dog, frog, 
horse, ship, truck. the dataset be a label subset of the 80 million tini imag dataset[18] and be 
divid into 50,000 train and 10,000 test images. each imag be 32x32 pixel in 3 color channels, 
yield input vector with 3072 dimensions. We prepar the data by subtract the mean and 
divid the standard deviat of each imag vector to perform global contrast normalization. We 
then appli zca whiten to the normal images. thi pre-process be the same use in [9]. 

4.1 learn to mimic a deep convolut neural network 

deep learn current achiev state-of-the-art accuraci on mani comput vision problems. the 
key to thi success be deep convolut net with mani altern layer of convolutional, pool 
and non-linear units. recent advanc such a dropout be also import to prevent over-fit in 
these deep nets. 

We follow the same approach a with timit: An ensembl of deep cnn model be use to label 
cifar-10 imag for model compression. the logit predict from thi teacher model be use 
a regress target to train a mimic shallow neural net (snn). cifar-10 imag have a high 
dimens than timit (3072 vs. 1845), but the size of the cifar-10 train set be onli 50,000 
compar to 1.1 million exampl for timit. fortunately, unlik timit, in cifar-10 we have 
access to unlabel data from a similar distribut by use the super set of cifar-10: the 80 
million tini imag dataset. We add the first 1 million imag from the 80 million set to the origin 
50,000 cifar-10 train imag to creat a 1.05m mimic train (transfer) set. 

cifar-10 imag be raw pixel for object view from mani differ angl and positions, 
wherea timit featur be human-design filter-bank features. In preliminari experi we 
observ that non-convolut net do not perform well on cifar-10 no matter what their depth. 
instead of raw pixels, the author in [5] train their shallow model on the sift features. similarly, 
[7] use a base convolut and pool layer to studi differ deep architectures. We follow the 
approach in [7] to allow our shallow model to benefit from convolut while keep the model 
a shallow a possible, and introduc a singl layer of convolut and pool in our shallow mimic 
model to act a a featur extractor to creat invari to small translat in the pixel domain. the 
snn-mimic model for cifar-10 thu consist of a convolut and max pool layer follow 
by fulli connect 1200 linear unit and 30k non-linear units. As before, the linear unit be there 
onli to speed learning; they do not increas the model’ represent power and can be absorb 
into the weight in the non-linear layer after learning. 

result on cifar-10 be consist with those from timit. tabl 2 show result for the shallow 
mimic models, and for much-deep convolut nets. the shallow mimic net train to mimic 
the teacher cnn (snn-cnn-mimic-30k) achiev accuraci compar to cnn with multipl 
convolut and pool layers. and by train the shallow model to mimic the ensembl of 
cnn (snn-ecnn-mimic-30k), accuraci be improv an addit 0.9%. the mimic model 
be abl to achiev accuraci previous unseen on cifar-10 with model with so few layers. 
although the deep convolut net have more hidden unit than the shallow mimic models, becaus 
of weight sharing, the deeper net with multipl convolut layer have few paramet than the 
shallow fully-connect mimic models. still, it be surpris to see how accur the shallow mimic 
model are, and that their perform continu to improv a the perform of the teacher model 
improv (see further discuss of thi in section 5.2). 

5 discuss 

5.1 whi mimic model can Be more accur than train on origin label 

It may be surpris that model train on the predict target take from other model can be 
more accur than model train on the origin labels. there be a varieti of reason whi thi can 
happen: 

6 



architectur # param. # hidden unit err. 

dnn 2000-2000 + dropout ∼10m 4k 57.8% 

snn-30k 128c-p-1200l-30k ∼70m ∼190k 21.8%+ dropout input&hidden 
single-lay 4000c-p ∼125m ∼3.7b 18.4%featur extract follow by svm 
cnn[11] 64c-p-64c-p-64c-p-16lc ∼10k ∼110k 15.6%(no augmentation) + dropout on lc 
cnn[21] 64c-p-64c-p-128c-p-fc 

∼56k ∼120k 15.13%(no augmentation) + dropout on fc 
and stochast pool 

teacher cnn 128c-p-128c-p-128c-p-1000fc 
∼35k ∼210k 12.0%(no augmentation) + dropout on fc 

and stochast pool 
ecnn ensembl of 4 cnn ∼140k ∼840k 11.0%(no augmentation) 
snn-cnn-mimic-30k 64c-p-1200l-30k ∼54m ∼110k 15.4%train on a singl cnn with no regular 
snn-cnn-mimic-30k 128c-p-1200l-30k ∼70m ∼190k 15.1%train on a singl cnn with no regular 
snn-ecnn-mimic-30k 128c-p-1200l-30k ∼70m ∼190k 14.2%train on ensembl with no regular 

tabl 2: comparison of shallow and deep models: classif error rate on cifar-10. key: c, 
convolut layer; p, pool layer; lc, local connect layer; fc, fulli connect layer 

• if some label have errors, the teacher model may elimin some of these error (i.e., cen- 
sor the data), thu make learn easi for the student: on timit, there be mislabel 
frame introduc by the hmm forced-align procedure. 

• if there be region in the p(y|x) that be difficult to learn give the features, sampl den- 
sity, and function complexity, the teacher may provid simpler, soft label to the student. 
the complex in the data set have be wash away by filter the target through the 
teacher model. 

• learn from the origin hard 0/1 label can be more difficult than learn from the 
teacher’ condit probabilities: on timit onli one of 183 output be non-zero on each 
train case, but the mimic model see non-zero target for most output on most train 
cases. moreover, the teacher model can spread the uncertainti over multipl output when 
it be not confid of it prediction. yet, the teacher model can concentr the probabl 
mass on one (or few) output on easi cases. the uncertainti from the teacher model be far 
more inform to guid the student model than the origin 0/1 labels. thi benefit 
appear to be further enhanc by train on logits. 

0 2 4 6 8 10 12 14 
number of epoch 

74.0 

74.5 

75.0 

75.5 

76.0 

76.5 

77.0 

77.5 

P 
h 
o 
n 
e 
R 

e 
c 
o 
g 
n 
it 

io 
n 
A 

c 
c 
u 
ra 

c 
y 

snn-8k 

snn-8k + dropout 

snn-mimic-8k 

figur 2: train shallow mimic model prevent 
overfitting. 

the mechan abov can be see a form 
of regular that help prevent overfit in 
the student model. shallow model train on 
the origin target be more prone to overfit- 
ting than deep models—they begin to overfit 
befor learn the accur function learn 
by deeper model even with dropout (see fig- 
ure 2). If we have more effect regular 
method for shallow models, some of the per- 
formanc gap between shallow and deep mod- 
el might disappear. model compress ap- 
pear to be a form of regular that be ef- 
fectiv at reduc thi gap. 

7 



5.2 the capac and represent 
power of shallow model 

78 

79 

80 

81 

82 

83 

78 79 80 81 82 83 

A 
cc 

u 
ra 

cy 
o 

f 
M 

im 
ic 

M 
od 

el 
o 

n 
D 

ev 
S 

et 

accuraci of teacher model on dev set 

mimic with 8k non-linear unit 
mimic with 160k non-linear unit 

y=x (no student-teach gap) 

figur 3: accuraci of student model continu to 
improv a accuraci of teacher model improves. 

figur 3 show result of an experi with 
timit where we train shallow mimic mod- 
el of two size (snn-mimic-8k and snn- 
mimic-160k) on teacher model of differ 
accuracies. the two shallow mimic model be 
train on the same number of data points. the 
onli differ between them be the size of the 
hidden layer. the x-axi show the accuraci of 
the teacher model, and the y-axi be the accu- 
raci of the mimic models. line parallel to the 
diagon suggest that increas in the accuraci 
of the teacher model yield similar increas in 
the accuraci of the mimic models. although 
the data do not fall perfectli on a diagonal, 
there be strong evid that the accuraci of the 
mimic model continu to increas a the ac- 
curaci of the teacher model improves, suggest- 
ing that the mimic model be not (yet) run 
out of capacity. when train on the same targets, snn-mimic-8k alway perform bad than 
snn-mimic-160k that have 10 time more parameters. although there be a consist perform 
gap between the two model due to the differ in size, the small shallow model be eventu- 
alli abl to achiev a perform compar to the larg shallow net by learn from a good 
teacher, and the accuraci of both model continu to increas a teacher accuraci increases. thi 
suggest that shallow model with a number of paramet compar to deep model be like 
capabl of learn even more accur function if a more accur teacher and/or more unlabel 
data becom available. similarly, on cifar-10 we saw that increas the accuraci of the teacher 
model by form an ensembl of deep cnn yield commensur increas in the accuraci of the 
student model. We see littl evid that shallow model have limit capac or represent 
power. instead, the main limit appear to be the learn and regular procedur use to 
train the shallow models. 

5.3 parallel distribut process vs. deep sequenti process 

our result show that shallow net can be competit with deep model on speech and vision tasks. 
one potenti benefit of shallow net be that train them scale well with the modern parallel 
hardware. In our experi the deep model usual requir 8–12 hour to train on nvidia gtx 
580 gpu to reach the state-of-the-art perform on timit and cifar-10 datasets. although 
some of the shallow mimic model have more paramet than the deep models, the shallow model 
train much faster and reach similar accuraci in onli 1–2 hours. 

also, give parallel comput resources, at run-tim shallow model can finish comput in 
2 or 3 cycl for a give input, wherea a deep architectur have to make sequenti infer through 
each of it layers, expend a number of cycl proport to the depth of the model. thi benefit 
can be import in on-lin infer set where data parallel be not a easi to achiev 
a it be in the batch infer setting. for real-tim applic such a surveil or real-tim 
speech translation, a model that respond in few cycl can be beneficial. 

6 futur work 

the tini imag dataset contain 80 million images. We be current investig if by label 
these 80m imag with a teacher, it be possibl to train shallow model with no convolut or 
pool layer to mimic deep convolut models. 

thi paper focu on train the shallowest-poss model to mimic deep model in order to 
good understand the import of model depth in learning. As suggest in section 5.3, there be 
practic applic of thi work a well: student model of small-to-medium size and depth can be 
train to mimic veri large, high accuraci deep models, and ensembl of deep models, thu yield 

8 



good accuraci with reduc runtim cost than be current achiev without model compression. 
thi approach allow one to adjust flexibl the trade-off between accuraci and comput cost. 

In thi paper we be abl to demonstr empir that shallow model can, at least in principle, 
learn more accur function without a larg increas in the number of parameters. the algorithm 
we use to do this—train the shallow model to mimic a more accur deep model, however, 
be awkward. It depend on the avail of either a larg unlabel data set (to reduc the gap 
between teacher and mimic model) or a teacher model of veri high accuracy, or both. develop 
algorithm to train shallow model of high accuraci directli from the origin data without go 
through the intermedi teacher model would, if possible, be a signific contribution. 

7 conclus 

We demonstr empir that shallow neural net can be train to achiev perform pre- 
viousli achiev onli by deep model on the timit phonem recognit and cifar-10 imag 
recognit tasks. single-lay fully-connect feedforward net train to mimic deep model can 
perform similarli to well-engin complex deep convolut architectures. the result suggest 
that the strength of deep learn may aris in part from a good match between deep architectur 
and current train procedures, and that it may be possibl to devis good learn algorithm to 
train more accur shallow feed-forward nets. for a give number of parameters, depth may make 
learn easier, but may not alway be essential. 

acknowledg We thank Li deng for gener help with timit, Li deng and ossama abdel- 
hamid for code for the timit convolut model, chri burges, Li deng, ran gilad-bachrach, 
tapa kanungo and john platt for discuss that significantli improv thi work, and mike ault- 
man for help with the gpu cluster. 

refer 
[1] ossama abdel-hamid, abdel-rahman mohamed, hui jiang, and gerald penn. appli con- 

volut neural network concept to hybrid nn-hmm model for speech recognition. In 
acoustics, speech and signal process (icassp), 2012 ieee intern confer on, 
page 4277–4280. ieee, 2012. 

[2] ossama abdel-hamid, Li deng, and dong yu. explor convolut neural network struc- 
ture and optim techniqu for speech recognition. interspeech 2013, 2013. 

[3] cristian bucilu, rich caruana, and alexandru niculescu-mizil. model compression. In pro- 
ceed of the 12th acm sigkdd intern confer on knowledg discoveri and data 
mining, page 535–541. acm, 2006. 

[4] georg cybenko. approxim by superposit of a sigmoid function. mathemat of 
control, signal and systems, 2(4):303–314, 1989. 

[5] yann N dauphin and yoshua bengio. big neural network wast capacity. arxiv preprint 
arxiv:1301.3583, 2013. 

[6] Li deng, jinyu li, jui-t huang, kaisheng yao, dong yu, frank seide, michael seltzer, 
geoff zweig, xiaodong he, jason williams, et al. recent advanc in deep learn for speech 
research at microsoft. icassp 2013, 2013. 

[7] david eigen, jason rolfe, rob fergus, and yann lecun. understand deep architectur 
use a recurs convolut network. arxiv preprint arxiv:1312.1847, 2013. 

[8] dumitru erhan, yoshua bengio, aaron courville, pierre-antoin manzagol, pascal vincent, 
and sami bengio. whi do unsupervis pre-train help deep learning? the journal of 
machin learn research, 11:625–660, 2010. 

[9] ian goodfellow, david warde-farley, mehdi mirza, aaron courville, and yoshua bengio. 
maxout networks. In proceed of the 30th intern confer on machin learning, 
page 1319–1327, 2013. 

[10] g.e. hinton and r.r. salakhutdinov. reduc the dimension of data with neural net- 
works. science, 313(5786):504–507, 2006. 

9 



[11] g.e. hinton, N. srivastava, A. krizhevsky, I. sutskever, and r.r. salakhutdinov. im- 
prove neural network by prevent co-adapt of featur detectors. arxiv preprint 
arxiv:1207.0580, 2012. 

[12] alex krizhevski and geoffrey hinton. learn multipl layer of featur from tini images. 
comput scienc department, univers of toronto, tech. rep, 2009. 

[13] k-f lee and h-w hon. speaker-independ phone recognit use hidden markov models. 
acoustics, speech and signal processing, ieee transact on, 37(11):1641–1648, 1989. 

[14] abdel-rahman mohamed, georg E dahl, and geoffrey hinton. acoust model use 
deep belief networks. audio, speech, and languag processing, ieee transact on, 20(1): 
14–22, 2012. 

[15] V. nair and g.e. hinton. rectifi linear unit improv restrict boltzmann machines. In proc. 
27th intern confer on machin learning, page 807–814. omnipress madison, 
wi, 2010. 

[16] tara N sainath, brian kingsbury, vika sindhwani, ebru arisoy, and bhuvana ramabhadran. 
low-rank matrix factor for deep neural network train with high-dimension out- 
put targets. In acoustics, speech and signal process (icassp), 2013 ieee intern 
confer on, page 6655–6659. ieee, 2013. 

[17] frank seide, gang li, and dong yu. convers speech transcript use context- 
depend deep neural networks. In interspeech, page 437–440, 2011. 

[18] antonio torralba, robert fergus, and william T freeman. 80 million tini images: A larg data 
set for nonparametr object and scene recognition. pattern analysi and machin intelligence, 
ieee transact on, 30(11):1958–1970, 2008. 

[19] P. vincent, H. larochelle, I. lajoie, Y. bengio, and p.a. manzagol. stack denois autoen- 
coders: learn use represent in a deep network with a local denois criterion. the 
journal of machin learn research, 11:3371–3408, 2010. 

[20] jian xue, jinyu li, and yifan gong. restructur of deep neural network acoust model 
with singular valu decomposition. proc. interspeech, lyon, france, 2013. 

[21] matthew D zeiler and rob fergus. stochast pool for regular of deep convolut 
neural networks. arxiv preprint arxiv:1301.3557, 2013. 

10 


1 introduct 
2 train shallow net to mimic deep net 
2.1 model compress 
2.2 mimic learn via regress logit with L2 loss 
2.3 speeding-up mimic learn by introduc a linear layer 

3 timit phonem recognit 
3.1 deep learn on timit 
3.2 learn to mimic an ensembl of deep convolut timit model 
3.3 compress result for timit 

4 object recognition: cifar-10 
4.1 learn to mimic a deep convolut neural network 

5 discuss 
5.1 whi mimic model can Be more accur than train on origin label 
5.2 the capac and represent power of shallow model 
5.3 parallel distribut process vs. deep sequenti process 

6 futur work 
7 conclus 

