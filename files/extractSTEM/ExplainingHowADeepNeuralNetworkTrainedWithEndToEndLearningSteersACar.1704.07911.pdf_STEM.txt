


















































explain how a deep neural network train with 
end-to-end learn steer a car 

mariusz bojarski 
nvidia corpor 
holmdel, NJ 07733 

philip yere 
nvidia corpor 
holmdel, NJ 07733 

anna choromanaska 
new york univers 
new york, NY 10012 

krzysztof choromanski 
googl research 

new york, NY 10011 

bernhard firner 
nvidia corpor 
holmdel, NJ 07733 

lawrenc jackel 
nvidia corpor 
holmdel, NJ 07733 

ur muller 
nvidia corpor 
holmdel, NJ 07733 

abstract 

As part of a complet softwar stack for autonom driving, nvidia have creat 
a neural-network-bas system, know a pilotnet, which output steer angl 
give imag of the road ahead. pilotnet be train use road imag pair with 
the steer angl gener by a human drive a data-collect car. It deriv 
the necessari domain knowledg by observ human drivers. thi elimin the 
need for human engin to anticip what be import in an imag and forese 
all the necessari rule for safe driving. road test demonstr that pilotnet 
can success perform lane keep in a wide varieti of drive conditions, 
regardless of whether lane mark be present or not. 
the goal of the work describ here be to explain what pilotnet learn and how 
it make it decisions. To thi end we develop a method for determin which 
element in the road imag most influenc pilotnet’ steer decision. result 
show that pilotnet inde learn to recogn relev object on the road. 
In addit to learn the obviou featur such a lane markings, edg of roads, 
and other cars, pilotnet learn more subtl featur that would be hard to antic- 
ipat and program by engineers, for example, bush line the edg of the road 
and atyp vehicl classes. 

1 introduct 

A previou report [1] describ an end-to-end learn system for self-driv car in which a con- 
volut neural network (cnn) [2] be train to output steer angl give input imag of the 
road ahead. thi system be now call pilotnet. the train data be imag from a front-fac 
camera in a data collect car coupl with the time-synchron steer angl record from 
a human driver. the motiv for pilotnet be to elimin the need for hand-cod rule and 
instead creat a system that learn by observing. initi result be encouraging, although major 
improv be requir befor such a system can drive without the need for human intervention. 
To gain insight into how the learn system decid what to do, and thu both enabl further system 
improv and creat trust that the system be pay attent to the essenti cue for safe steer- 
ing, we develop a simpl method for highlight those part of an imag that be most salient 

1 

ar 
X 

iv 
:1 

70 
4. 

07 
91 

1v 
1 

[ 
c 

.C 
V 

] 
2 

5 
A 

pr 
2 

01 
7 



output: vehicl control 

fully-connect layer10 neuron 
fully-connect layer50 neuron 
fully-connect layer100 neuron 

1164 neuron 

convolut 
featur map 
64@1x18 

convolut 
featur map 
64@3x20 

convolut 
featur map 
48@5x22 

convolut 
featur map 
36@14x47 

convolut 
featur map 
24@31x98 

normal 
input plane 
3@66x200 

normal 

input plane 
3@66x200 

3x3 kernel 

3x3 kernel 

5x5 kernel 

5x5 kernel 

5x5 kernel 

flatten 

figur 1: pilotnet architecture. 

in determin steer angles. We call these salient imag section the salient objects. A detail 
report describ our salienc detect method can be found in [3] 

sever method for find salienc have be describ by other authors. among them be sen- 
sitiv base approach [4, 5, 6], deconvolut base one [7, 8], or more complex one like 
layer-wis relev propag (lrp) [9]. We believ the simplic of our method, it fast exe- 
cution on our test car’ nvidia drivetm PX 2 AI car computer, along with it nearli pixel level 
resolution, make it especi advantag for our task. 

1.1 train the pilotnet self-driv system 

pilotnet train data contain singl imag sampl from video from a front-fac camera in 
the car, pair with the correspond steer command (1/r), where r be the turn radiu of the 
vehicle. the train data be augment with addit image/steering-command pair that simul 
the vehicl in differ off-cent and off-orientationpoistions. for the augment images, the target 
steer command be appropri adjust to one that will steer the vehicl back to the center of 
the lane. 

onc the network be trained, it can be use to provid the steer command give a new image. 

2 pilotnet network architectur 

the pilotnet architectur be show in figur 1. the network consist of 9 layers, includ a normal- 
izat layer, 5 convolut layer and 3 fulli connect layers. the input imag be split into yuv 

2 



averag 

sca 
le u 

p 

final visual mask 
1@66x200 

pointwis 
multipl 

figur 2: block diagram of the visual method that identifi the salient objects. 

plane and pass to the network. the first layer of the network perform imag normalization. the 
normal be hard-cod and be not adjust in the learn process. 

the convolut layer be design to perform featur extract and be chosen empir 
through a seri of experi that vari layer configurations. stride convolut be use in 
the first three convolut layer with a 2×2 stride and a 5×5 kernel and a non-strid convolut 
with a 3×3 kernel size in the last two convolut layers. 
the five convolut layer be follow with three fulli connect layer lead to an output 
control valu that be the invers turn radius. the fulli connect layer be design to function 
a a control for steering, but note that by train the system end-to-end, there be no hard boundari 
between which part of the network function primarili a featur extractor and which serv a the 
controller. 

3 find the salient object 

the central idea in discern the salient object be find part of the imag that correspond to 
locat where the featur maps, describ above, have the great activations. 

the activ of the higher-level map becom mask for the activ of low level use the 
follow algorithm: 

1. In each layer, the activ of the featur map be averaged. 
2. the top most averag map be scale up to the size of the map of the layer below. the 

up-scal be do use deconvolution. the paramet (filter size and stride) use for the 

3 



deconvolut be the same a in the convolut layer use to gener the map. the 
weight for deconvolut be set to 1.0 and bia be set to 0.0. 

3. the up-scal averag map from an upper level be then multipli with the averag map 
from the layer below (both be now the same size). the result be an intermedi mask. 

4. the intermedi mask be scale up to the size of the map of layer below in the same way 
a describ step 2. 

5. the up-scal intermedi map be again multipli with the averag map from the layer 
below (both be now the same size). thu a new intermedi mask be obtained. 

6. step 4 and 5 abov be repeat until the input be reached. the last mask which be of the 
size of the input imag be normal to the rang from 0.0 to 1.0 and becom the final 
visual mask. 

thi visual mask show which region of the input imag contribut most to the output of 
the network. these region identifi the salient objects. the algorithm block diagram be show in 
figur 2. 

the process of creat the visual mask be illustr in figur 3. the visual mask 
be overlaid on the input imag to highlight the pixel in the origin camera imag to illustr the 
salient objects. 

result for variou input imag be show in figur 4. notic in the top imag the base of car a 
well a line (dash and solid) indic lane be highlighted, while a nearli horizont line from 
a crosswalk be ignored. In the middl imag there be no lane paint on the road, but the park 
cars, which indic the edg of the road, be highlighted. In the low imag the grass at the edg 
of the road be highlighted. without ani coding, these detect show how pilotnet mirror the way 
human driver would use these visual cues. 

figur 5 show a view insid our test car. At the top of the imag we see the actual view through the 
windshield. A pilotnet monitor be at the bottom center display diagnostics. 

figur 6 be a blowup of the pilotnet monitor. the top imag be captur by the front-fac camera. 
the green rectangl outlin the section of the camera imag that be fed to the neural network. 
the bottom imag display the salient regions. note that pilotnet identifi the partial occlud 
construct vehicl on the right side of the road a a salient object. To the best of our knowledge, 
such a vehicle, particularli in the pose we see here, be never part of the pilotnet train data. 

4 analysi 

while the salient object found by our method clearli appear to be one that should influenc steer- 
ing, we conduct a seri of experi to valid that these object actual do control the 
steering. To perform these tests, we segment the input imag that be present to pilotnet into two 
classes. 

class 1 be meant to includ all the region that have a signific effect on the steer angl output 
by pilotnet. these region includ all the pixel that correspond to locat where the visual 
mask be abov a threshold. these region be then dilat by 30 pixel to counteract the increas 
span of the higher-level featur map layer with respect to the input image. the exact amount of 
dilat be determin empirically. the second class includ all pixel in the origin imag 
minu the pixel in class 1. If the object found by our method inde domin control of the 
output steer angle, we would expect the following: if we creat an imag in which we uniformli 
translat onli the pixel in class 1 while maintain the posit of the pixel in class 2 and use thi 
new imag a input to pilotnet, we would expect a signific chang in the steer angl output. 
however, if we instead translat the pixel in class 2 while keep those in class 1 fix and feed 
thi imag into pilotnet, then we would expect minim chang in pilotnet’ output. 

figur 7 illustr the process describ above. the top imag show a scene captur by our 
data collect car. the next imag show highlight salient region that be identifi use the 
method of section 3. the next imag show the salient region dilated. the bottom imag show a 
test imag in which the dilat salient object be shifted. 

4 



figur 3: left: averag featur map for each level of the network. right: intermedi visual- 
izat mask for each level of the network. 

figur 4: exampl of salient object for variou imag inputs. 

5 



figur 5: view insid our test car 

the abov predict be inde born out by our experiments. figur 8 show plot of pilotnet 
steer output a a function of pixel shift in the input image. the blue line show the result when 
we shift the pixel that includ the salient object (class 1). the red line show the result when we 
shift the pixel not includ in the salient objects. the yellow line show the result when we shift 
all the pixel in the input image. 

shift the salient object result in a linear chang in steer angl that be nearli a larg a 
that which occur when we shift the entir image. shift just the background pixel have a much 
small effect on the steer angle. We be thu confid that our method do inde find the most 
import region in the imag for determin steering. 

5 conclus 

We describ a method for find the region in input imag by which pilotnet make it steer 
decisions, i. e., the salient objects. We further provid evid that the salient object identifi by 
thi method be correct. the result substanti contribut to our understand of what pilotnet 
learns. 

examin of the salient object show that pilotnet learn featur that “make sense” to a human, 
while ignor structur in the camera imag that be not relev to driving. thi capabl be 
deriv from data without the need of hand-craft rules. In fact, pilotnet learn to recogn subtl 

6 



figur 6: the pilotnet monitor from figur 5 abov 

figur 7: imag use in experi to show the effect of image-shift on steer angle. 

7 



figur 8: plot of pilotnet steer output a a function of pixel shift in the input image. 

featur which would be hard to anticip and program by human engineers, such a bush line 
the edg of the road and atyp vehicl classes. 

refer 
[1] mariusz bojarski, david del testa, daniel dworakowski, bernhard firner, beat flepp, prasoon goyal, 

lawrenc D. jackel, mathew monfort, ur muller, jiakai zhang, xin zhang, jake zhao, and karol zieba. 
end to end learn for self-driv cars, april 25 2016. url: http://arxiv.org/abs/1604. 
07316, arxiv:arxiv:1604.07316. 

[2] Y. lecun, B. boser, J. S. denker, D. henderson, R. E. howard, W. hubbard, and L. D. jackel. backprop- 
agat appli to handwritten zip code recognition. neural computation, 1(4):541–551, winter 1989. 
url: http://yann.lecun.org/exdb/publis/pdf/lecun-89e.pdf. 

[3] mariusz bojarski, anna choromanska, krzysztof choromanski, bernhard firner, larri jackel, ur muller, 
and karol zieba. visualbackprop: visual cnn for autonom driving, novemb 16 2016. url: 
https://arxiv.org/abs/1611.05418, arxiv:arxiv:1611.05418. 

[4] D. baehrens, T. schroeter, S. harmeling, m.i kawanabe, K. hansen, and k.-r. müller. how to explain 
individu classif decisions. J. mach. learn. res., 11:1803–1831, 2010. 

[5] K. simonyan, A. vedaldi, and A. zisserman. deep insid convolut networks: visualis imag 
classif model and salienc maps. In workshop proc. iclr, 2014. 

[6] P. M. rasmussen, T. schmah, K. H. madsen, T. E. lund, S. C. strother, and L. K. hansen. visual of 
nonlinear classif model in neuroimag - sign sensit maps. biosignals, page 254–263, 
2012. 

[7] M. D. zeiler, G. W. taylor, and R. fergus. adapt deconvolut network for mid and high level 
featur learning. In iccv, 2011. 

[8] M. D. zeiler and R. fergus. visual and understand convolut networks. In eccv, 2014. 

[9] S. bach, A. binder, G. montavon, F. klauschen, k.-r. müller, and W samek. On pixel-wis explan 
for non-linear classifi decis by layer-wis relev propagation. plo one, 10(7):e0130140, 2015. 
url: http://dx.doi.org/10.1371/journal.pone.0130140. 

8 

http://arxiv.org/abs/1604.07316 
http://arxiv.org/abs/1604.07316 
http://arxiv.org/abs/arxiv:1604.07316 
http://yann.lecun.org/exdb/publis/pdf/lecun-89e.pdf 
https://arxiv.org/abs/1611.05418 
http://arxiv.org/abs/arxiv:1611.05418 
http://dx.doi.org/10.1371/journal.pone.0130140 

