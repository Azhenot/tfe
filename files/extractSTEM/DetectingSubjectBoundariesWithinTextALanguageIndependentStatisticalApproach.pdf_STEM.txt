





































detect subject boundari within text: A languag independ statist approach 


detect subject boundari within text: A languag 
independ statist approach 

K o r i n R i c h m o n d 
( k o r : i . n © c o g s c ± . ed. ac . uk) 

A n d r e w S m i t h 
(aj s©cogsci, ed. ac . uk) 

(joint authorship) 
centr for cognit scienc 

2 buccleuch place 
edinburgh eh8 9lw 

scotland 

E i n a t A m i t a y 
( e i n a t @ c o g s c i . ed. ac . uk) 

A b s t r a c t 

We describ here an algorithm for detect- 
ing subject boundari within text base 
on a statist lexic similar measure. 
hearst have alreadi tackl thi problem 
with good result (hearst, 1994). one of 
her main assumpt be that a chang in 
subject be accompani by a chang in vo- 
cabulary. use thi assumption, but by 
introduc a new measur of word signif- 
icance, we have be abl to build a ro- 
bust and reliabl algorithm which exhibit 
improv accuraci without sacrific lan- 
guag independency. 

1 I n t r o d u c t i o n 

automat detect of subject divis within a 
text be consid to be a veri difficult task even for 
humans, let alon machines. but such subject di- 
vision be use in more complex task in text pro- 
cess such a text summarisation. An automat 
method for mark subject boundari be highli de- 
sirable. hearst (hearst, 1994) address thi prob- 
lem by appli a statist method for detect 
subject within text. 

hearst describ an algorithm for what she call 
text tiling, which be a method for detect subject 
boundari within a text. the underli assump- 
tion of thi algorithm be that there be a high proba- 

biliti that word which be relat to a certain sub- 
ject will be repeat whenev that subject be men- 
tioned. anoth basic assumpt be that when a 
new subject emerg the choic of vocabulari will 
change, and will stay consist within the subject 
boundari until the next chang in subject. these 
basic notion of vocabulari consist within sub- 
ject boundari lead to a method for divid text 
base on calcul vocabulari similar between 
two adjac window of text. 

each potenti subject boundari be identifi and 
assign a correspond valu base on the lexic 
similar between two window of text, one on ei- 
ther side of the subject boundary. the valu for all 
potenti boundari be plot on a graph, creat 
peak and troughs. the trough repres chang 
in vocabulari use and therefore, accord to the 
underli assumption, a chang in subject. A divi- 
sion mark be insert where a signific local min- 
imum be detect on the graph. hearst measur 
approxim 80% success in detect of subject 
boundari on some texts. 

We decid to adopt hearst' underli assump- 
tion that a chang in subject will entail a chang in 
vocabulary. our aim be to make the algorithm a 
languag independ and comput expedi- 
ent a possibl , while also improv accuraci and 
reliability. 

4 7 



I preprocess } stage I 

¥ 
icalculatc a signific 1 stage 2 

valu for each word J 

V 
icalcul bia lexical] stage 3 

correspondenf J 

- v - 
I smooth result 1 stage 4 

¥ 
I insert break ] stage 5 

figur 1: algorithm structure. 

2 D e s i g n 

the algorithm be divid into five distinct stages. 
figur 1 show the sequential, modular structur of 
the algorithm. each stage of the algorithm be de- 
scribe in more detail below. 

2.1 P r e p r o c e s s i n g ( s t age 1) 

In her implement of the texttil algorithm 
hearst ignor preprocessing, claim it do not af- 
fect the result (hearst, 1994). By preprocess we 
mean lemmatizing, stemming, convert upper to 
low case etc. test thi assumpt on her algo- 
r i thm inde seem not to chang the results. how- 
ever, use preprocess in conjunct with stage 
2 of our algorithm, do improv results. It be impor- 
tant for our algorithm that morpholog differ 
between semant relat word be resolved, so 
that word like "bankrupt" and "bankruptcy", for 
example, be identifi a the same word. 

2.2 C a l c u l a t i n g a s ign i f icanc va lue fo r 
each w o r d ( s t age 2) 

hearst treat a text more or less a a bag of word 
in it statist analysis. but natur languag 
be no doubt more structur than this. differ 
word have differ semant function and rela- 
tionship with respect to the topic of discourse. We 
can broadli distinguish two extrem categori of 
words; content word versu function words. con- 
tent word introduc concepts, and be the mean 
for the express of idea and facts, for exampl 
nouns, proper nouns, adject and so on. function 

word (for exampl determiners, auxiliari verb etc.) 
support and coordin the combin of content 
word into meaning sentences. obviously, both 
be need to form meaning sentences, but, intu- 
itively, it be the content word that carri most weight 
in defin the actual topic of discourse. base on 
thi intuition, we believ it would be advantag 
to identifi these content word in a text. It would 
then be possibl to bia the calcul of lexic 
correspond (stage 3) take into account the 
high signific of these word rel to func- 
tion words. 

We would ideal like firstli to reduc the effect 
of noisi non-cont word on the algorithm' per- 
formance, and secondli to pay more attent to 
word with a high semant content. In her imple- 
mentation, hearst at tempt to do thi by have a 
finit list of problemat word that be filter out 
from the text befor the statist analysi take 
place (hearst, 1994). these problemat word be 
primarili function word and low semant content 
words, such a determiners, conjunctions, preposi- 
tion and veri common nouns. 

church and gale (church and gale, 1995) men- 
tion the correl between a word' semant 
content and variou measur of it distribut 
throughout corpora. they show that: "word rate 
vari from genr to genre, topic to topic, author 
to author, document to document, section to sec: 
tion, paragraph to paragraph. these factor tend 
to decreas the entropi and increas the other test 
variables". one of these other test variabl men- 
tion by church and gale be burstiness. they at- 
tribut the innov of the notion of bursti 
to slava katz, who, pertain to thi topic, write 
(katz, 1996): "the notion of burstiness.., will be 
use for the characteris of two close relat 
but distinct phenomena: (a) document-level bursti- 
ness, i.e. multipl occurr of a content word or 
phrase in a singl text document, which be contrast 
with the fact that most other document contain no 
other instanc of thi word or phrase at all; and (b) 
within-docu bursti (or bursti proper), 
i.e. close proxim of all or some individu in- 
stanc of a content word or phrase within a doc- 
ument exhibit multipl occurrence." katz have 
highlight mani interest featur of the distri- 
bution of content words, which do not conform to 
the predict of statist model such a the pois- 
son. katz (katz, 1996) state that, when a concept 
name by a content word be topic for the document, 
then that content word tend to be characteris 
by multipl and bursti occurrence. He claim that, 
while a singl occurr of a topic use content 

4 8 



word or phrase be possible, it be more like that a 
newli introduc topic entity, will be repeated, "if 
not for break the monoton effect of pronoun 
use, then for emphasi or clarity". He also claim 

o 

that, unlik function words, the number of instanc 
E 

of a specif content word be not directli associ 
with the document length, but be rather a function ~ 
of how much the document be about the concept ex- i 

~5 press by that word. 

z 

therefore, the characterist distribut pattern 
of topic content words, which contrast markedli 
with that of non-top and non-cont words, 
could provid a use aid in identifi the seman- 
tical relev word within a text. brief mention 
should be make of the work do by justeson and 
katz (justeson and katz, 1995), which, to a certain 
degree, relat to the requir of our task. In 
their paper, justeson and katz describ some lin- 
guistic properti of technic terminology, and use 
them to formul an algorithm to identifi the tech- 
nical term in a give document. however, their al- 
gorithm deal with complex noun phrase only, and, 
although the technic term identifi by their al- 
gorithm be gener highli topical, the algorithm 
do not provid the context sensit inform 
of how topic each incid of a give meaning- 
ful phrase is, rel to it direct environment. It 
be precis thi inform that be need to judg 
the content of a particular segment of text. 

although katz (katz, 1996) acknowledg what 
he call two distinct, but close related, form 
of burstiness, he concentr on model the 
inter-docu distribut of content word and 
phrases. He then us the inter-docu distri- 
bution to make infer about probabl of 
the repeat occurr of content word and phrase 
within a singl document. anoth diverg be- 
tween what katz have do so far and what the task 
of subject boundari insert requires, be that he 
decid to ignor the issu of coincident repeti- 
tion of non-top use content word and sim- 
pli equat "singl occurr with non-top oc- 
currence, and multipl occurr with topic occur- 
rence." 

10 

0 

6 

4 

2 

i i t i t i t t i 

0 o.oi 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1 
propol ' f i~i freiltlerlci of word 

figur 2: calcul of number of near neigh- 
bours. 

n 

significance(x) = -1 × Z arctan ( dx,i 
n i = l twicd ] (1) 

where x be an individu word in the document and 
dx,i be the distanc between word x and it ith near- 
est neighbour. the 1st near neighbour of word x 
be the near occurr of the same word. the 2nd 
near neighbour of x be the near occurr of 
the same word ignor the 1st near neighbour. In 
general, the ith near neighbour of x be the near- 
est occurr of the same word ignor the 1st, 
2nd, 3 rd , . . . , ( i - 1)th near neighbours. W be the 
total number of word in the text. w be the number 
of occurr of the word like x. n be the number 
of near neighbour to includ in the calcul 
and depend on the overal frequenc of the word in 
the text. thi formula will yield a signific score 
that lie within the rang 0 to ~ (high signific 
to low significance). thi number be then normalis 
to between 0 and 1, with 0 indic a veri low sig- 
nificance, and 1 indic a veri high significance. 
the exact valu of n be calcul separ for each 
distinct word, use the follow formula: 

( 8 ) 
n- - - - 1 + e-2°~-(~ -°°2) + 2 (2) 

We have implement a method which assign an 
estim signific score base on a measur of 
two context depend properties; local bursti 
and global frequency. the heart of our solut 
to the problem of assign context-bas valu of 
topic signific to all word in a text, can be 
sum up in the follow formula: 

thi be essenti a sigmoid function with the 
rang vari between two and ten, a show in fig- 
ure 2. the constant scale and translat the function 
to yield the desir behaviour, which be deriv 
empirically. the number of near neighbour to 
consid in equat 1 increas with the word' fre- 
quency. for example, when calcul the signif- 

4 9 



0.9 
i 

0.8 

0.7 

0.6 

0.5 

0.4 

0.3 

0.2 

0.i 

Q O 

2 h e ° • 

CI 
I0 O 

o & o 
o 

2¢ •o t , 

$ • • 

i i i i i i 

~00 400 600 800 1000 1200 1400 
word posi~on in text 

figur 3: signific values. 

icanc of the least frequent words, onli two near- 
est neighbour be considered. but for the most 
frequent occur words, the number of near 
neighbour be ten. figur 3 show the main featur 
of the perform of thi signific assign al- 
gorithm when test on a sampl text. the result 
for three veri differ word be shown. 

two gener trend be the most import fea- 
ture of thi graph. firstly, elev signific 
score be associ with local cluster of a word. 
for exampl the cluster of three occurr of "soft- 
ware" (a content word) at the end of the document 
have high signific scores. thi contrast with 
the rel isol occurr of the word "soft- 
ware" in the middl of the document, which be 
deem to be littl more signific than sever oc- 
currenc of the word "the" (a function word). sec- 
ondly, frequent word tend to receiv low signifi- 
canc scores. for example, even local cluster of the 
word "the" onli receiv rel low signific 
scores, simpli becaus the word have a high frequenc 
throughout the document. conversely, "mcnealy" 
(a high semant content word), which onli occur in 
a cluster of three, receiv a high signific value. 
the import result show by the graph be that 
content word (real name such a "mcnealy") re- 
ceiv high signific valu than function word 
("the"). 

We found that an optim solut to the problem 
of balanc local densiti against global frequenc 
be rather elusive. for example, the word at the 
centr of a cluster automat receiv a high 
score, wherea it may be more desir to have all 
the member of a cluster assign a score lie in a 
narrow range. there be mani other contenti 
issu which need to be investigated, such a the use 

of the ratio of all the occurr of a word in a give 
text to the total length of that text in order to calcu- 
late the rel signific measure. base on in- 
tuition, partli deriv from katz' discuss (katz, 
1996) of the relationship between document length 
and word frequency, the exact natur of thi rela- 
tionship across variou document length may not 
be reliabl enough. It may be more consist to 
consid thi ratio within a constant window size, 
e.g. 1000 words. 

the advantag of thi simpl statist method 
of distinguish signific content word from non- 
content word be that no word need to be remov 
befor allow the algorithm to proceed. the out- 
put of thi stage be a normalis signific score 
(0-1) for each word in the text. thi signific 
score can then be take into account when analys 
the text for subject boundaries. 

2.3 C a l c u l a t e B i a s e d lex ica l 
C o r r e s p o n d e n c e s ( s t ag e 3) 

let u consid two set of words, set A and set 
B. the main aim of thi stage of the process be 
concern with calcul a correspond mea- 
sure between two such set depend on how similar 
they are, where similar be defin a a measur off 
lexic correspondence. If mani word be share by 
both set A and B, then the lexic correspond 
between the two set be high. If the two set do not 
share mani words, then the correspond be low. 
now let A t be the subset of A that contain onli 
those word that occur somewher in B. and let B' 
be the subset of B that contain onli those word 
that occur somewher in A. the lexic correspon- 
denc between set A and B can then be calcul 
use the simpl formula: 

correspondence- I~ + ~L 
2 

thi yield a valu within the rang 0 to 1. iai can 
be re-written a 1+1+1+1+1 .... by add a 1 for 
everi word in A. each word have alreadi be give 
a signific valu a describ in stage 2 of the 
algorithm and thi inform be take into account 
by re-defin iai a s l+s2+s3+. . , where sl be the 
signific valu assign to the first word in A, s2 
the second and so on. the same can be do for A ', 
B and B ~. the formula now take the averag of 
the bia ratios. all thi mean be that instead of 
each word count for '1' in a set, it count for it 
signific valu (a valu between 0 (insignificant) 
and 1 (highli significant)). the result be that each 
word affect the correspond measur accord 
to it signific in the text. 

50 



set A set B 

figur 4: word sets. 

So far, a word that occur onli in A and not in B, 
contribut zero to jan[. thi mean that a highli 
signific word occur onli in A have exactli the 
same effect a an insignific word occur onli 
in A. In other word the signific bias be onli 
take place for word that appear in both A and B. 
therefore, the formula actual use is: 

correspondence= l~i ~ -~ "k i-~p-~i 
2 

where A" be the subset of A which contain onli 
those word that occur in A and not in B. sim- 
ilarly, B ~ be the subset of B which contain onli 
those word that occur in B and not in A. thi be 
show in figur 4. 

recal that [a[, [m'[, [a"[, [b], [b'] and is"[ be 
not calcul by add one for each word in each 
set, but by sum the signific valu of the 
word in each set. 

thi stage of the process look at the output 
from the signific calcul stage and consid 
everi sentenc break in turn - start at the top 
of the document and work down. the algorithm 
assign a correspond measur to each sentenc 
break a follows: firstly, set A be gener by tak- 
ing all the word in the previou fifteen sentences. 
next, set B be gener by take all the word 
in the follow fifteen sentences. 1 now set A p, 
A ~, B ~ and B ~ be gener a describ and then 
the formula abov be appli which assign a cor- 
respond valu to the sentenc break current 
under consideration. the algorithm then move to 
the next sentenc break and repeat the process. 

the output from thi stage of the algorithm be a 
list of sentenc break number (1..n, with n = num- 
ber of sentenc in the document) and a lexic cor- 
respond measure. these number provid the 
input for stage four - smoothing. 

1fi f teen s e n t e n c e s t u r n s ou t to be t h e o p t i m u m win- 
dow size for t h e va t m a j o r i t y of t ex t s . T h i s be b e c a u s e 
i t be a b o u t t h e s a m e a t h e ave rage s e g m e n t size. 

2.4 S m o o t h i n g ( s t age 4) 

A graph can be plot with lexic correspond 
along the y-axi and sentenc number along the x- 
axis. In order to distinguish the signific peak 
and trough from the mani minor fluctuations, a 
simpl smooth algorithm be used. take three 
neighbour point on the graph, p1, p2, p3: 

P3 

o A. 
..~......~:~ 

..................................................................... .......................... i iui::: 

~ X 

figur 5: smoothing. 

the line p1p3 be bisect and thi point be label 
A. P2 be perturb by a constant amount (not dee- 
pendent on the distanc between A and p2) toward 
A. thi new point be label B and becom the 
new p2. thi be perform simultan on ev- 
eri point on the graph. the process be k then iter 
a fix number of times. the result be that nois 
be flatten out while the larg peak and trough 
remain (although slightli smaller). 

the output from thi stage be simpli the sentenc 
break number and their new, smooth correspon- 
denc values. 

2.5 I n s e r t i n g s u b j e c t b o u n d a r i e s ( s t age 5) 

consid the graph describ in the previou sec- 
tion, gener subject boundari be simpli a mat- 
ter of identifi local minimum on the graph. the 
confid of a boundari be calcul from the 
'depth' of the local minimum. thi depth be calcu- 
late simpli by take the averag of the height of 
the 'peak' (rel to the height of the minimum) on 
either side of the minimum. thi now yield a list 
of candid subject boundari and an associ 
confid measur for each one. break be then in- 
sert into the origin text at the place correspond- 
ing to the local minimum if their confid valu sat- 
isfi a 'minimum confidence' criterion. thi cut-off 
criterion be arbitrary, and in our implement can 
be specifi at run time. 

3 R e s u l t s 

figur 6 show the result of process the first 
800 sentenc from an edit of the time newspa- 
per. the sentenc number (x-axis) be plot against 
the correspond (y-axis) between the two win- 
dow of text on either side of that sentence. 

51 



. i 

10 

0 

- 10 

- 20 

- 30 

- 40 

- 50 

- 50 

- 70 

- aO 

-go 
0 

10 

0 • 

- 10 

- 20 

- 30 

- 40 

- 60 

- 50 

- 70 

- 50 

"g~o0 

-10 

- 20 

- 30 

- 40 

- 50 

- 50 

- 70 

- 50 

- 90 
400 

- 4O 

-50 

- 90 
500 

| 

5O 
| 

1 O0 
sen tenc 

I | I 

2 5 O 3 0 q 3 5 O 
~ e n t e n c e 

| 

4 6 0 
i l 

600 550 
sen tenc 

| i | 

550 700 750 
sen tenc 

figur 6:800 sentenc from the time newspaper. 

52 



actua l sub jec t 
boundar y 

36 
61 
779 
109 

146 
165 
175 
203 

244 
278 
304 
333 
356 
376 

boundar y found Er ro r 
by a lgor i thm 

36 0 
60 1 
79 0 
109 0 
134 + 
145 1 
165 0 
174 1 
203 0 
214 + 
244 0 
278 0 
304 0 
332 1 
355 1 
375 1 

tabl 1: the time 

A larg neg valu indic a low degre of 
correspond and a small neg valu or a pos- 
itiv valu indic a high degre of correspondence. 
the vertic line mark actual articl boundaries. 

the advantag of use a text such a thi be that 
there can be no doubt from ani human judg a to 
where the boundari occur, i.e. between articles. 
the local min ima on the graph signifi the bound- 
ari a determin by the algorithm. the vertic 
bar signifi the actual articl boundaries. the re- 
sult of the first 400 sentenc be summaris in 
tabl 1. 

the algorithm locat 53% of the articl bound- 
ari precis and 95% of the boundari to within 
an accuraci of a singl sentence. everi articl 
boundari be identifi to within an accuraci of 
two sentences. the algorithm make no use of end- 
of-paragraph markers. It also found some addit 
subject boundari in the middl of articles. these 
be denot by a ' + ' in the error column. mani ex- 
t ra subject boundari be found in the long articl 
(start at sentenc 430). It be worth note that 
the min ima occur within thi articl be not a 
pronounc a the actual articl boundari them- 
selves. thi section of the graph reflect a long arti- 
cle which contain a number of differ subtopics. 

A newspap be an easi test for such an algorithm 
though. figur 7 show a graph for an expositori 
text - a 200 sentenc psycholog paper write by 
a fellow student. again the local min ima indic 
where the algorithm consid a subject boundari 
to occur and the vertic line be the obviou break 
in the text (mainli befor new headings) a judg 
by the author. the result be summaris in tabl 
2. 

thi t ime the algorithm precis locat 50% of 
the boundaries. It found 63% of the boundari to 
within an accuraci of a singl sentenc and 88% to 

actua l subjec t 
boundar y 

7 
22 

59 
72 

96 
121 

162 

184 

boundar y found E r ro r 
by a lgor i thm 

77 0 
22 0 
42 + 
58 1 
772 0 
77 + 
92 4 

118 3 
137 + 
156 + 
161 1 
177 + 
184 0 
191 + 

tabl 2: expositori text 

within an accuraci of two sentences. thi level of 
accuraci be obtain consist for a varieti of 
differ texts. again, it should be mention that 
the algorithm found more break than be immedi- 
ate obviou to a human judge. however, it should 
be note that these extra break be usual de- 
note by small minima, and on inspect the vast 
majori ty of them be in sensibl places. 

the algorithm have a certain resolv power. As 
the subject mat ter becom more and more homoge- 
neous, the number of subject break the algorithm 
find decreases. for some texts, thi result in veri 
few divis be made. By take a small win- 
dow size (the number of sentenc to look at either 
side of each possibl sentenc break), the resolv 
power 'of the algorithm can be increas make it 
more sensit to chang in the vocabulary. how- 
ever, the reliabl of the algorithm decreas with 
the increas resolv power. the default window 
size be fifteen sentenc and thi work well for all 
but the most homogen of texts. In thi case a 
window size of around six be more effective. A low 
window size increas the resolv power, but de- 
creas the accuraci of the algorithm. the window 
size be a paramet of our implementat ion. 

o 

- 1 o 

- 3 o 

- 70 
~ o 1 ¢ o 1 s o 

figur 7: expositori text. 

53 



4 S u m m a r y 

base on our investigation, we believ that hearst' 
origin intuit that lexic correspond can be 
exploit to identifi subject boundari be a sound 
one. the addit of the signific measur repre- 
sent an improv on hearst' algorithm imple- 
ment by the berkeley digit librari project. 

furthermore, thi algorithm be languag indepen- 
dent except for the preprocess stage (which can 
be omit with onli a modest degrad in per- 
formance). In order to improv accuracy, languag 
depend method could be considered. such meth- 
od might includ the insert of convent dis- 
cours marker in order to detect prefer break 
point (e.g. repetit of the same syntact struc- 
ture, and convent paragraph open such as: 
"on the other hand...", "the above...", etc.). an- 
other method would be to make use of a thesaurus, 
sinc we have found that human judgement be often 
base on synonym inform such a real syn- 
onym or anaphora. the abov issu be discuss 
in variou articl (morri and hirst, 1991); (mor- 
ris, 1988) and (givon, 1983) which studi discours 
marker and synonym information. 

anoth interest line of research would be to 
use the inform from stage two of the algorithm 
to discov the signific word of a section, and 
therebi attach a label to it. thi would be particu- 
larli use for inform retriev applications. 

5 A c k n o w l e d g e m e n t s 

thi problem be set a an assign on the data 
intens linguist cours organis by chri brew 
at the hcrc, edinburgh university. thank to him, 
Jo calder and marc moen for guidanc and advic 
throughout the project. thank to esrc and ep- 
src for funding. 

katz, S. M. 1996. distribut of context word and 
phrase in text and languag modelling. natur 
languag engineering, 2(1):15-59. 

morris, J. 1988. lexic cohesion, the thesaurus, 
and the structur of text. technic report csri- 
219, comput system research institute, uni- 
versiti of toronto. 

morris, J. and G. hirst. 1991. lexic cohes 
comput by thesaur relat a an indic 
of the structur of text. comput linguis- 
tics, 17(1):21-48. 

R e f e r e n c e s 

church, K. W. and W. A. gale. 1995. poisson mix- 
tures. natur languag engineering, 1(2):163- 
190. 

givon, T. 1983. topic continu in discourse: 
A quantit cross-languag study. philadel- 
phia: john benjamin publish company. 

hearst, M. A. 1994. multi-paragraph segment 
of expositori text. In acl '94, la cruces, nm. 

justeson, J. S. and S. M. katz. 1995. technic ter- 
minology: some linguist properti and an algo- 
rithm for identif in text. natur languag 
engineering, 1(1) :9-27. 

54 


