


















































“whi should I trust you?” 
explain the predict of ani classifi 

marco tulio ribeiro 
univers of washington 
seattle, WA 98105, usa 
marcotcr@cs.uw.edu 

sameer singh 
univers of washington 
seattle, WA 98105, usa 
sameer@cs.uw.edu 

carlo guestrin 
univers of washington 
seattle, WA 98105, usa 
guestrin@cs.uw.edu 

abstract 
despit widespread adoption, machin learn model re- 
main mostli black boxes. understand the reason behind 
predict is, however, quit import in assess trust, 
which be fundament if one plan to take action base on a 
prediction, or when choos whether to deploy a new model. 
such understand also provid insight into the model, 
which can be use to transform an untrustworthi model or 
predict into a trustworthi one. 

In thi work, we propos lime, a novel explan tech- 
niqu that explain the predict of ani classifi in an in- 
terpret and faith manner, by learn an interpret 
model local around the prediction. We also propos a 
method to explain model by present repres indi- 
vidual predict and their explan in a non-redund 
way, frame the task a a submodular optim prob- 
lem. We demonstr the flexibl of these method by 
explain differ model for text (e.g. random forests) 
and imag classif (e.g. neural networks). We show the 
util of explan via novel experiments, both simul 
and with human subjects, on variou scenario that requir 
trust: decid if one should trust a prediction, choos 
between models, improv an untrustworthi classifier, and 
identifi whi a classifi should not be trusted. 

1. introduct 
machin learn be at the core of mani recent advanc in 
scienc and technology. unfortunately, the import role 
of human be an oft-overlook aspect in the field. whether 
human be directli use machin learn classifi a tools, 
or be deploy model within other products, a vital concern 
remains: if the user do not trust a model or a prediction, 
they will not use it. It be import to differenti between 
two differ (but related) definit of trust: (1) trust a 
prediction, i.e. whether a user trust an individu predict 
suffici to take some action base on it, and (2) trust 
a model, i.e. whether the user trust a model to behav in 
reason way if deployed. both be directli impact by 

permiss to make digit or hard copi of all or part of thi work for person or 
classroom use be grant without fee provid that copi be not make or distribut 
for profit or commerci advantag and that copi bear thi notic and the full citat 
on the first page. copyright for compon of thi work own by other than the 
author(s) must be honored. abstract with credit be permitted. To copi otherwise, or 
republish, to post on server or to redistribut to lists, requir prior specif permiss 
and/or a fee. request permiss from permissions@acm.org. 

kdd 2016 san francisco, ca, usa 
c© 2016 copyright held by the owner/author(s). public right licens to acm. 

isbn 978-1-4503-4232-2/16/08. . . $15.00 

doi: http://dx.doi.org/10.1145/2939672.2939778 

how much the human understand a model’ behaviour, a 
oppos to see it a a black box. 

determin trust in individu predict be an import 
problem when the model be use for decis making. when 
use machin learn for medic diagnosi [6] or terror 
detection, for example, predict cannot be act upon on 
blind faith, a the consequ may be catastrophic. 

apart from trust individu predictions, there be also a 
need to evalu the model a a whole befor deploy it “in 
the wild”. To make thi decision, user need to be confid 
that the model will perform well on real-world data, accord 
to the metric of interest. currently, model be evalu 
use accuraci metric on an avail valid dataset. 
however, real-world data be often significantli different, and 
further, the evalu metric may not be indic of the 
product’ goal. inspect individu predict and their 
explan be a worthwhil solution, in addit to such 
metrics. In thi case, it be import to aid user by suggest 
which instanc to inspect, especi for larg datasets. 

In thi paper, we propos provid explan for indi- 
vidual predict a a solut to the “trust a prediction” 
problem, and select multipl such predict (and expla- 
nations) a a solut to the “trust the model” problem. 
our main contribut be summar a follows. 

• lime, an algorithm that can explain the predict of ani 
classifi or regressor in a faith way, by approxim 
it local with an interpret model. 

• sp-lime, a method that select a set of repres 
instanc with explan to address the “trust the 
model” problem, via submodular optimization. 

• comprehens evalu with simul and human sub- 
jects, where we measur the impact of explan on 
trust and associ tasks. In our experiments, non-expert 
use lime be abl to pick which classifi from a pair 
gener good in the real world. further, they be abl 
to greatli improv an untrustworthi classifi train on 
20 newsgroups, by do featur engin use lime. 
We also show how understand the predict of a neu- 
ral network on imag help practition know when and 
whi they should not trust a model. 

2. the case for explan 
by“explain a prediction”, we mean present textual or 

visual artifact that provid qualit understand of the 
relationship between the instance’ compon (e.g. word 
in text, patch in an image) and the model’ prediction. We 
argu that explain predict be an import aspect in 

ar 
X 

iv 
:1 

60 
2. 

04 
93 

8v 
3 

[ 
c 

.L 
G 

] 
9 

A 
ug 

2 
01 

6 

http://dx.doi.org/10.1145/2939672.2939778 


sneez 
weight 
headach 
no fatigu 
age 

flu sneez 

headach 

model data and predict 

explain 
(lime) 

explan 

explain 
(lime) 

human make 
decis explan 

no fatigu 

sneez 

headach 

activ 

human make decis 

figur 1: explain individu predictions. A model predict that a patient have the flu, and lime highlight 
the symptom in the patient’ histori that lead to the prediction. sneez and headach be portray a 
contribut to the “flu” prediction, while “no fatigue” be evid against it. with these, a doctor can make 
an inform decis about whether to trust the model’ prediction. 

get human to trust and use machin learn effectively, 
if the explan be faith and intelligible. 

the process of explain individu predict be illus- 
trate in figur 1. It be clear that a doctor be much good 
posit to make a decis with the help of a model if 
intellig explan be provided. In thi case, an ex- 
planat be a small list of symptom with rel weight – 
symptom that either contribut to the predict (in green) 
or be evid against it (in red). human usual have prior 
knowledg about the applic domain, which they can use 
to accept (trust) or reject a predict if they understand the 
reason behind it. It have be observed, for example, that 
provid explan can increas the accept of movi 
recommend [12] and other autom system [8]. 

everi machin learn applic also requir a certain 
measur of overal trust in the model. develop and 
evalu of a classif model often consist of collect- 
ing annot data, of which a held-out subset be use for 
autom evaluation. although thi be a use pipelin for 
mani applications, evalu on valid data may not 
correspond to perform “in the wild”, a practition 
often overestim the accuraci of their model [20], and 
thu trust cannot reli sole on it. look at exampl 
offer an altern method to ass truth in the model, 
especi if the exampl be explained. We thu propos 
explain sever repres individu predict of a 
model a a way to provid a global understanding. 

there be sever way a model or it evalu can go 
wrong. data leakage, for example, defin a the uninten- 
tional leakag of signal into the train (and validation) 
data that would not appear when deploy [14], potenti 
increas accuracy. A challeng exampl cite by kauf- 
man et al. [14] be one where the patient ID be found to be 
heavili correl with the target class in the train and 
valid data. thi issu would be incred challeng 
to identifi just by observ the predict and the raw 
data, but much easi if explan such a the one in 
figur 1 be provided, a patient ID would be list a an 
explan for predictions. anoth particularli hard to 
detect problem be dataset shift [5], where train data be 
differ than test data (we give an exampl in the famou 
20 newsgroup dataset late on). the insight give by expla- 
nation be particularli help in identifi what must be 
do to convert an untrustworthi model into a trustworthi 
one – for example, remov leak data or chang the 
train data to avoid dataset shift. 

machin learn practition often have to select a model 
from a number of alternatives, requir them to ass 
the rel trust between two or more models. In figur 

figur 2: explain individu predict of com- 
pet classifi tri to determin if a document 
be about “christianity” or “atheism”. the bar chart 
repres the import give to the most rele- 
vant words, also highlight in the text. color indi- 
cate which class the word contribut to (green for 
“christianity”, magenta for “atheism”). 

2, we show how individu predict explan can be 
use to select between models, in conjunct with accuracy. 
In thi case, the algorithm with high accuraci on the 
valid set be actual much worse, a fact that be easi to see 
when explan be provid (again, due to human prior 
knowledge), but hard otherwise. further, there be frequent 
a mismatch between the metric that we can comput and 
optim (e.g. accuracy) and the actual metric of interest 
such a user engag and retention. while we may not 
be abl to measur such metrics, we have knowledg about 
how certain model behavior can influenc them. therefore, 
a practition may wish to choos a less accur model for 
content recommend that do not place high import 
in featur relat to “clickbait” articl (which may hurt 
user retention), even if exploit such featur increas 
the accuraci of the model in cross validation. We note 
that explan be particularli use in these (and other) 
scenario if a method can produc them for ani model, so 
that a varieti of model can be compared. 

desir characterist for explain 
We now outlin a number of desir characterist from 
explan methods. 

An essenti criterion for explan be that they must 
be interpretable, i.e., provid qualit understand 
between the input variabl and the response. We note that 
interpret must take into account the user’ limitations. 
thus, a linear model [24], a gradient vector [2] or an addit 
model [6] may or may not be interpretable. for example, if 



hundr or thousand of featur significantli contribut 
to a prediction, it be not reason to expect ani user to 
comprehend whi the predict be made, even if individu 
weight can be inspected. thi requir further impli 
that explan should be easi to understand, which be 
not necessarili true of the featur use by the model, and 
thu the “input variables” in the explan may need 
to be differ than the features. finally, we note that the 
notion of interpret also depend on the target audience. 
machin learn practition may be abl to interpret small 
bayesian networks, but layman may be more comfort 
with a small number of weight featur a an explanation. 

anoth essenti criterion be local fidelity. although it be 
often imposs for an explan to be complet faith 
unless it be the complet descript of the model itself, for 
an explan to be meaning it must at least be local 
faithful, i.e. it must correspond to how the model behav in 
the vicin of the instanc be predicted. We note that 
local fidel do not impli global fidelity: featur that 
be global import may not be import in the local 
context, and vice versa. while global fidel would impli 
local fidelity, identifi global faith explan that 
be interpret remain a challeng for complex models. 

while there be model that be inher interpret [6, 
17, 26, 27], an explain should be abl to explain ani model, 
and thu be model-agnost (i.e. treat the origin model 
a a black box). apart from the fact that mani state-of- 
the-art classifi be not current interpretable, thi also 
provid flexibl to explain futur classifiers. 

In addit to explain predictions, provid a global 
perspect be import to ascertain trust in the model. 
As mention before, accuraci may often not be a suitabl 
metric to evalu the model, and thu we want to explain 
the model. build upon the explan for individu 
predictions, we select a few explan to present to the 
user, such that they be repres of the model. 

3. local interpret 
model-agnost explan 

We now present local interpret model-agnost expla- 
nation (lime). the overal goal of lime be to identifi an 
interpret model over the interpret represent 
that be local faith to the classifier. 

3.1 interpret data represent 
befor we present the explan system, it be impor- 

tant to distinguish between featur and interpret data 
representations. As mention before, interpret expla- 
nation need to use a represent that be understand 
to humans, regardless of the actual featur use by the 
model. for example, a possibl interpret represent 
for text classif be a binari vector indic the pres- 
enc or absenc of a word, even though the classifi may 
use more complex (and incomprehensible) featur such a 
word embeddings. likewis for imag classification, an in- 
terpret represent may be a binari vector indic 
the “presence” or “absence” of a contigu patch of similar 
pixel (a super-pixel), while the classifi may repres the 
imag a a tensor with three color channel per pixel. We 
denot x ∈ Rd be the origin represent of an instanc 
be explained, and we use x′ ∈ {0, 1}d 

′ 
to denot a binari 

vector for it interpret representation. 

3.2 fidelity-interpret trade-off 
formally, we defin an explan a a model g ∈ G, 

where G be a class of potenti interpret models, such 
a linear models, decis trees, or fall rule list [27], i.e. a 
model g ∈ G can be readili present to the user with visual 
or textual artifacts. the domain of g be {0, 1}d 

′ 
, i.e. g act 

over absence/pres of the interpret components. As 
not everi g ∈ G may be simpl enough to be interpret - 
thu we let ω(g) be a measur of complex (a oppos to 
interpretability) of the explan g ∈ G. for example, for 
decis tree ω(g) may be the depth of the tree, while for 
linear models, ω(g) may be the number of non-zero weights. 

let the model be explain be denot f : Rd → R. In 
classification, f(x) be the probabl (or a binari indicator) 
that x belong to a certain class1. We further use πx(z) a a 
proxim measur between an instanc z to x, so a to defin 
local around x. finally, let l(f, g, πx) be a measur of 
how unfaith g be in approxim f in the local defin 
by πx. In order to ensur both interpret and local 
fidelity, we must minim l(f, g, πx) while have ω(g) be 
low enough to be interpret by humans. the explan 
produc by lime be obtain by the following: 

ξ(x) = argmin 
g∈g 

l(f, g, πx) + ω(g) (1) 

thi formul can be use with differ explan 
famili G, fidel function L, and complex measur Ω. 
here we focu on spars linear model a explanations, and 
on perform the search use perturbations. 

3.3 sampl for local explor 
We want to minim the locality-awar loss l(f, g, πx) 

without make ani assumpt about f , sinc we want the 
explain to be model-agnostic. thus, in order to learn 
the local behavior of f a the interpret input vary, we 
approxim l(f, g, πx) by draw samples, weight by 
πx. We sampl instanc around x 

′ by draw nonzero 
element of x′ uniformli at random (where the number of 
such draw be also uniformli sampled). given a perturb 

sampl z′ ∈ {0, 1}d 
′ 

(which contain a fraction of the nonzero 
element of x′), we recov the sampl in the origin repre- 
sentat z ∈ Rd and obtain f(z), which be use a a label for 
the explan model. given thi dataset Z of perturb 
sampl with the associ labels, we optim eq. (1) to 
get an explan ξ(x). the primari intuit behind lime 
be present in figur 3, where we sampl instanc both 
in the vicin of x (which have a high weight due to πx) 
and far away from x (low weight from πx). even though 
the origin model may be too complex to explain globally, 
lime present an explan that be local faith (linear 
in thi case), where the local be captur by πx. It be worth 
note that our method be fairli robust to sampl nois 
sinc the sampl be weight by πx in eq. (1). We now 
present a concret instanc of thi gener framework. 

3.4 spars linear explan 
for the rest of thi paper, we let G be the class of linear 

models, such that g(z′) = wg ·z′. We use the local weight 
squar loss a L, a defin in eq. (2), where we let πx(z) = 
exp(−d(x, z)2/σ2) be an exponenti kernel defin on some 
1for multipl classes, we explain each class separately, thu 
f(x) be the predict of the relev class. 



figur 3: toy exampl to present intuit for lime. 
the black-box model’ complex decis function f 
(unknown to lime) be repres by the blue/pink 
background, which cannot be approxim well by 
a linear model. the bold red cross be the instanc 
be explained. lime sampl instances, get pre- 
diction use f , and weigh them by the proxim 
to the instanc be explain (repres here 
by size). the dash line be the learn explan 
that be local (but not globally) faithful. 

distanc function D (e.g. cosin distanc for text, L2 distanc 
for images) with width σ. 

l(f, g, πx) = 
∑ 

z,z′∈z 

πx(z) 
( 
f(z)− g(z′) 

)2 
(2) 

for text classification, we ensur that the explan be 
interpret by let the interpret represent be 
a bag of words, and by set a limit K on the number of 
words, i.e. ω(g) =∞1[‖wg‖0 > k]. potentially, K can be 
adapt to be a big a the user can handle, or we could 
have differ valu of K for differ instances. In thi 
paper we use a constant valu for K, leav the explor 
of differ valu to futur work. We use the same Ω for 
imag classification, use “super-pixels” (comput use 
ani standard algorithm) instead of words, such that the 
interpret represent of an imag be a binari vector 
where 1 indic the origin super-pixel and 0 indic a 
gray out super-pixel. thi particular choic of Ω make 
directli solv eq. (1) intractable, but we approxim it by 
first select K featur with lasso (use the regular 
path [9]) and then learn the weight via least squar (a 
procedur we call k-lasso in algorithm 1). sinc algo- 
rithm 1 produc an explan for an individu prediction, 
it complex do not depend on the size of the dataset, 
but instead on time to comput f(x) and on the number 
of sampl N . In practice, explain random forest with 
1000 tree use scikit-learn (http://scikit-learn.org) on a 
laptop with N = 5000 take under 3 second without ani 
optim such a use gpu or parallelization. explain- 
ing each predict of the incept network [25] for imag 
classif take around 10 minutes. 

ani choic of interpret represent and G will 
have some inher drawbacks. first, while the underli 
model can be treat a a black-box, certain interpret 
represent will not be power enough to explain certain 
behaviors. for example, a model that predict sepia-ton 
imag to be retro cannot be explain by presenc of absenc 
of super pixels. second, our choic of G (spars linear models) 
mean that if the underli model be highli non-linear even 
in the local of the prediction, there may not be a faith 
explanation. however, we can estim the faith of 

algorithm 1 spars linear explan use lime 

require: classifi f , number of sampl N 
require: instanc x, and it interpret version x′ 

require: similar kernel πx, length of explan K 
Z ← {} 
for i ∈ {1, 2, 3, ..., N} do 

z′i ← sampl around(x′) 
Z ← Z ∪ 〈z′i, f(zi), πx(zi)〉 

end for 
w ← k-lasso(z,k) . with z′i a features, f(z) a target 
return w 

the explan on Z, and present thi inform to the 
user. thi estim of faith can also be use for 
select an appropri famili of explan from a set of 
multipl interpret model classes, thu adapt to the 
give dataset and the classifier. We leav such explor 
for futur work, a linear explan work quit well for 
multipl black-box model in our experiments. 

3.5 exampl 1: text classif with svm 
In figur 2 (right side), we explain the predict of a 
support vector machin with rbf kernel train on uni- 
gram to differenti “christianity” from “atheism” (on a 
subset of the 20 newsgroup dataset). although thi classifi 
achiev 94% held-out accuracy, and one would be tempt 
to trust it base on this, the explan for an instanc 
show that predict be make for quit arbitrari reason 
(word “posting”, “host”, and “re” have no connect to 
either christian or atheism). the word “posting” appear 
in 22% of exampl in the train set, 99% of them in the 
class “atheism”. even if header be removed, proper name 
of prolif poster in the origin newsgroup be select by 
the classifier, which would also not generalize. 

after get such insight from explanations, it be clear 
that thi dataset have seriou issu (which be not evid 
just by studi the raw data or predictions), and that thi 
classifier, or held-out evaluation, cannot be trusted. It be also 
clear what the problem are, and the step that can be take 
to fix these issu and train a more trustworthi classifier. 

3.6 exampl 2: deep network for imag 
when use spars linear explan for imag classifiers, 
one may wish to just highlight the super-pixel with posi- 
tive weight toward a specif class, a they give intuit 
a to whi the model would think that class may be present. 
We explain the predict of google’ pre-train incept 
neural network [25] in thi fashion on an arbitrari imag 
(figur 4a). figur 4b, 4c, 4d show the superpixel expla- 
nation for the top 3 predict class (with the rest of the 
imag gray out), have set K = 10. what the neural 
network pick up on for each of the class be quit natur 
to human - figur 4b in particular provid insight a to 
whi acoust guitar be predict to be electric: due to the 
fretboard. thi kind of explan enhanc trust in the 
classifi (even if the top predict class be wrong), a it show 
that it be not act in an unreason manner. 

http://scikit-learn.org 


(a) origin imag (b) explain electr guitar (c) explain acoust guitar (d) explain labrador 

figur 4: explain an imag classif predict make by google’ incept neural network. the top 
3 class predict be “electr guitar” (p = 0.32), “acoust guitar” (p = 0.24) and “labrador” (p = 0.21) 

4. submodular pick for 
explain model 

although an explan of a singl predict provid 
some understand into the reliabl of the classifi to the 
user, it be not suffici to evalu and ass trust in the 
model a a whole. We propos to give a global understand 
of the model by explain a set of individu instances. thi 
approach be still model agnostic, and be complementari to 
comput summari statist such a held-out accuracy. 

even though explan of multipl instanc can be 
insightful, these instanc need to be select judiciously, 
sinc user may not have the time to examin a larg number 
of explanations. We repres the time/pati that human 
have by a budget B that denot the number of explan 
they be will to look at in order to understand a model. 
given a set of instanc X, we defin the pick step a the 
task of select B instanc for the user to inspect. 

the pick step be not depend on the exist of explana- 
tion - one of the main purpos of tool like modeltrack [1] 
and other [11] be to assist user in select instanc them- 
selves, and examin the raw data and predictions. however, 
sinc look at raw data be not enough to understand predic- 
tion and get insights, the pick step should take into account 
the explan that accompani each prediction. moreover, 
thi method should pick a diverse, repres set of expla- 
nation to show the user – i.e. non-redund explan 
that repres how the model behav globally. 

given the explan for a set of instanc X (|x| = n), 
we construct an n× d′ explan matrix W that repres 
the local import of the interpret compon for 
each instance. when use linear model a explanations, 
for an instanc xi and explan gi = ξ(xi), we set wij = 
|wgij |. further, for each compon (column) j in W, we 
let Ij denot the global import of that compon in 
the explan space. intuitively, we want I such that 
featur that explain mani differ instanc have high 
import scores. In figur 5, we show a toy exampl W, 
with n = d′ = 5, where W be binari (for simplicity). the 
import function I should score featur f2 high than 
featur f1, i.e. I2 > i1, sinc featur f2 be use to explain 
more instances. concret for the text applications, we set 
Ij = 

√∑n 
i=1wij . for images, I must measur someth 

that be compar across the super-pixel in differ images, 

f1 f2 f3 f4 f5 

cover featur 
figur 5: toy exampl W. row repres in- 
stanc (documents) and column repres featur 
(words). featur f2 (dot blue) have the high im- 
portance. row 2 and 5 (in red) would be select 
by the pick procedure, cover all but featur f1. 

algorithm 2 submodular pick (sp) algorithm 

require: instanc X, budget B 
for all xi ∈ X do 
Wi ← explain(xi, x′i) . use algorithm 1 

end for 
for j ∈ {1 . . . d′} do 

Ij ← 
√∑n 

i=1 |wij | . comput featur import 
end for 
V ← {} 
while |V | < B do . greedi optim of Eq (4) 

V ← V ∪ argmaxi c(v ∪ {i},w, I) 
end while 
return V 

such a color histogram or other featur of super-pixels; we 
leav further explor of these idea for futur work. 

while we want to pick instanc that cover the import 
components, the set of explan must not be redund 
in the compon they show the users, i.e. avoid select 
instanc with similar explanations. In figur 5, after the 
second row be picked, the third row add no value, a the 
user have alreadi see featur f2 and f3 - while the last row 
expos the user to complet new features. select the 
second and last row result in the coverag of almost all the 
features. We formal thi non-redund coverag intuit 
in eq. (3), where we defin coverag a the set function c 
that, give W and I, comput the total import of the 
featur that appear in at least one instanc in a set V . 



c(v,w, I) = 
d′∑ 

j=1 

1[∃i∈v :wij>0]ij (3) 

the pick problem, defin in eq. (4), consist of find the 
set V, |V | ≤ B that achiev high coverage. 

pick(w, I) = argmax 
v,|v |≤b 

c(v,w, I) (4) 

the problem in eq. (4) be maxim a weight coverag 
function, and be np-hard [10]. let c(v ∪{i},w, i)−c(v,w, I) 
be the margin coverag gain of add an instanc i to a set 
V . due to submodularity, a greedi algorithm that iter 
add the instanc with the high margin coverag gain to 
the solut offer a constant-factor approxim guarante 
of 1−1/e to the optimum [15]. We outlin thi approxim 
in algorithm 2, and call it submodular pick. 

5. simul user experi 
In thi section, we present simul user experi to 

evalu the util of explan in trust-rel tasks. In 
particular, we address the follow questions: (1) are the 
explan faith to the model, (2) can the explan 
aid user in ascertain trust in predictions, and (3) are 
the explan use for evalu the model a a whole. 
code and data for replic our experi be avail 
at https://github.com/marcotcr/lime-experiments. 

5.1 experi setup 
We use two sentiment analysi dataset (book and dvds, 

2000 instanc each) where the task be to classifi prod- 
uct review a posit or neg [4]. We train decis 
tree (dt), logist regress with L2 regular (lr), 
near neighbor (nn), and support vector machin with 
rbf kernel (svm), all use bag of word a features. We 
also includ random forest (with 1000 trees) train with 
the averag word2vec emb [19] (rf), a model that be 
imposs to interpret without a techniqu like lime. We 
use the implement and default paramet of scikit- 
learn, unless note otherwise. We divid each dataset into 
train (1600 instances) and test (400 instances). 

To explain individu predictions, we compar our pro- 
pose approach (lime), with parzen [2], a method that 
approxim the black box classifi global with parzen 
windows, and explain individu predict by take the 
gradient of the predict probabl function. for parzen, 
we take the K featur with the high absolut gradient 
a explanations. We set the hyper-paramet for parzen and 
lime use cross validation, and set N = 15, 000. We also 
compar against a greedi procedur (similar to marten 
and provost [18]) in which we greedili remov featur that 
contribut the most to the predict class until the predict 
chang (or we reach the maximum of K features), and a 
random procedur that randomli pick K featur a an 
explanation. We set K to 10 for our experiments. 

for experi where the pick procedur applies, we either 
do random select (random pick, rp) or the procedur 
describ in §4 (submodular pick, sp). We refer to pick- 
explain combin by add RP or SP a a prefix. 

5.2 are explan faith to the model? 
We measur faith of explan on classifi that 

be by themselv interpret (spars logist regress 

random parzen greedi lime 
0 

25 

50 

75 

100 

R 
ec 

al 
l ( 

% 
) 

17.4 

72.8 
64.3 

92.1 

(a) spars LR 

random parzen greedi lime 
0 

25 

50 

75 

100 

R 
ec 

al 
l ( 

% 
) 

20.6 

78.9 

37.0 

97.0 

(b) decis tree 

figur 6: recal on truli import featur for two 
interpret classifi on the book dataset. 

random parzen greedi lime 
0 

25 

50 

75 

100 

R 
ec 

al 
l ( 

% 
) 

19.2 

60.8 63.4 

90.2 

(a) spars LR 

random parzen greedi lime 
0 

25 

50 

75 

100 

R 
ec 

al 
l ( 

% 
) 

17.4 

80.6 

47.6 

97.8 

(b) decis tree 

figur 7: recal on truli import featur for two 
interpret classifi on the dvd dataset. 

and decis trees). In particular, we train both classifi 
such that the maximum number of featur they use for ani 
instanc be 10, and thu we know the gold set of featur 
that the be consid import by these models. for 
each predict on the test set, we gener explan and 
comput the fraction of these gold featur that be recov 
by the explanations. We report thi recal averag over all 
the test instanc in figur 6 and 7. We observ that 
the greedi approach be compar to parzen on logist 
regression, but be substanti bad on decis tree sinc 
chang a singl featur at a time often do not have an 
effect on the prediction. the overal recal by parzen be low, 
like due to the difficulti in approxim the origin high- 
dimension classifier. lime consist provid > 90% 
recal for both classifi on both datasets, demonstr 
that lime explan be faith to the models. 

5.3 should I trust thi prediction? 
In order to simul trust in individu predictions, we first 

randomli select 25% of the featur to be “untrustworthy”, 
and assum that the user can identifi and would not want 
to trust these featur (such a the header in 20 newsgroups, 
leak data, etc). We thu develop oracl “trustworthiness” 
by label test set predict from a black box classifi a 
“untrustworthy” if the predict chang when untrustworthi 
featur be remov from the instance, and “trustworthy” 
otherwise. In order to simul users, we assum that user 
deem predict untrustworthi from lime and parzen ex- 
planat if the predict from the linear approxim 
chang when all untrustworthi featur that appear in the 
explan be remov (the simul human “discounts” 
the effect of untrustworthi features). for greedi and random, 
the predict be mistrust if ani untrustworthi featur 
be present in the explanation, sinc these method do not 
provid a notion of the contribut of each featur to the 
prediction. thu for each test set prediction, we can evalu 
whether the simul user trust it use each explan 
method, and compar it to the trustworthi oracle. 

use thi setup, we report the F1 on the trustworthi 

https://github.com/marcotcr/lime-experi 


tabl 1: averag F1 of trustworthi for differ 
explain on a collect of classifi and datasets. 

book dvd 

LR NN RF svm LR NN RF svm 

random 14.6 14.8 14.7 14.7 14.2 14.3 14.5 14.4 
parzen 84.0 87.6 94.3 92.3 87.0 81.7 94.2 87.3 
greedi 53.7 47.4 45.0 53.3 52.4 58.1 46.6 55.1 
lime 96.6 94.5 96.2 96.7 96.6 91.8 96.1 95.6 

0 10 20 30 
# of instanc see by the user 

45 

65 

85 

% 
c 

or 
re 

ct 
c 

ho 
ic 

e 

sp-lime 
rp-lime 
sp-greedi 
rp-greedi 

(a) book dataset 

0 10 20 30 
# of instanc see by the user 

45 

65 

85 
% 

c 
or 

re 
ct 

c 
ho 

ic 
e 

sp-lime 
rp-lime 
sp-greedi 
rp-greedi 

(b) dvd dataset 

figur 8: choos between two classifiers, a the 
number of instanc show to a simul user be 
varied. averag and standard error from 800 runs. 

predict for each explan method, averag over 100 
runs, in tabl 1. the result indic that lime domin 
other (all result be signific at p = 0.01) on both datasets, 
and for all of the black box models. the other method either 
achiev a low recal (i.e. they mistrust predict more 
than they should) or low precis (i.e. they trust too mani 
predictions), while lime maintain both high precis and 
high recall. even though we artifici select which featur 
be untrustworthy, these result indic that lime be help 
in assess trust in individu predictions. 

5.4 can I trust thi model? 
In the final simul user experiment, we evalu whether 

the explan can be use for model selection, simul 
the case where a human have to decid between two compet 
model with similar accuraci on valid data. for thi 
purpose, we add 10 artifici “noisy” features. specifically, 
on train and valid set (80/20 split of the origin 
train data), each artifici featur appear in 10% of the 
exampl in one class, and 20% of the other, while on the 
test instances, each artifici featur appear in 10% of the 
exampl in each class. thi recreat the situat where the 
model use not onli featur that be inform in the real 
world, but also one that introduc spuriou correlations. We 
creat pair of compet classifi by repeatedli train 
pair of random forest with 30 tree until their valid 
accuraci be within 0.1% of each other, but their test accuraci 
differ by at least 5%. thus, it be not possibl to identifi the 
good classifi (the one with high test accuracy) from the 
accuraci on the valid data. 

the goal of thi experi be to evalu whether a user 
can identifi the good classifi base on the explan of 
B instanc from the valid set. the simul human 
mark the set of artifici featur that appear in the B 
explan a untrustworthy, follow which we evalu 
how mani total predict in the valid set should be 
trust (a in the previou section, treat onli mark 
featur a untrustworthy). then, we select the classifi with 

few untrustworthi predictions, and compar thi choic to 
the classifi with high held-out test set accuracy. 

We present the accuraci of pick the correct classifi 
a B varies, averag over 800 runs, in figur 8. We omit 
sp-parzen and rp-parzen from the figur sinc they do not 
produc use explanations, perform onli slightli good 
than random. lime be consist good than greedy, irre- 
spectiv of the pick method. further, combin submodular 
pick with lime outperform all other methods, in particular 
it be much good than rp-lime when onli a few exampl 
be show to the users. these result demonstr that the 
trust assess provid by sp-select lime explana- 
tion be good indic of generalization, which we valid 
with human experi in the next section. 

6. evalu with human subject 
In thi section, we recreat three scenario in machin 

learn that requir trust and understand of predict 
and models. In particular, we evalu lime and sp-lime 
in the follow settings: (1) can user choos which of two 
classifi gener good (§ 6.2), (2) base on the explana- 
tions, can user perform featur engin to improv the 
model (§ 6.3), and (3) be user abl to identifi and describ 
classifi irregular by look at explan (§ 6.4). 

6.1 experi setup 
for experi in §6.2 and §6.3, we use the “christianity” 

and “atheism” document from the 20 newsgroup dataset 
mention beforehand. thi dataset be problemat sinc it 
contain featur that do not gener (e.g. veri inform 
header inform and author names), and thu valid 
accuraci consider overestim real-world performance. 

In order to estim the real world performance, we creat 
a new religion dataset for evaluation. We download atheism 
and christian websit from the dmoz directori and 
human curat lists, yield 819 webpag in each class. 
high accuraci on thi dataset by a classifi train on 20 
newsgroup indic that the classifi be gener use 
semant content, instead of place import on the data 
specif issu outlin above. unless note otherwise, we 
use svm with rbf kernel, train on the 20 newsgroup 
data with hyper-paramet tune via the cross-validation. 

6.2 can user select the best classifier? 
In thi section, we want to evalu whether explan 

can help user decid which classifi gener better, i.e., 
which classifi would the user deploy “in the wild”. specif- 
ically, user have to decid between two classifiers: svm 
train on the origin 20 newsgroup dataset, and a version 
of the same classifi train on a “cleaned” dataset where 
mani of the featur that do not gener have be man- 
ualli removed. the origin classifi achiev an accuraci 
score of 57.3% on the religion dataset, while the “cleaned” 
classifi achiev a score of 69.0%. In contrast, the test accu- 
raci on the origin 20 newsgroup split be 94.0% and 88.6%, 
respect – suggest that the bad classifi would be 
select if accuraci alon be use a a measur of trust. 

We recruit human subject on amazon mechan turk – 
by no mean machin learn experts, but instead peopl 
with basic knowledg about religion. We measur their 
abil to choos the good algorithm by see side-by- 
side explan with the associ raw data (a show 
in figur 2). We restrict both the number of word in each 
explan (k) and the number of document that each 



greedi lime 
40 

60 

80 

100 

% 
c 

or 
re 

ct 
c 

ho 
ic 

e 
68.0 

75.0 
80.0 

89.0 
random pick (rp) 
submodular pick (rp) 

figur 9: averag accuraci of human subject (with 
standard errors) in choos between two classifiers. 

0 1 2 3 
round of interact 

0.5 

0.6 

0.7 

0.8 

R 
ea 

l w 
or 

ld 
a 

cc 
ur 

ac 
y 

sp-lime 
rp-lime 
No clean 

figur 10: featur engin experiment. each 
shade line repres the averag accuraci of sub- 
ject in a path start from one of the initi 10 sub- 
jects. each solid line repres the averag across 
all path per round of interaction. 

person inspect (b) to 6. the posit of each algorithm 
and the order of the instanc see be random between 
subjects. after examin the explanations, user be ask 
to select which algorithm will perform best in the real world. 
the explan be produc by either greedi (chosen 
a a baselin due to it perform in the simul user 
experiment) or lime, and the instanc be select either 
by random (rp) or submodular pick (sp). We modifi the 
greedi step in algorithm 2 slightli so it altern between 
explan of the two classifiers. for each setting, we repeat 
the experi with 100 users. 

the result be present in figur 9. note that all of 
the method be good at identifi the good classifier, 
demonstr that the explan be use in determin 
which classifi to trust, while use test set accuraci would 
result in the select of the wrong classifier. further, we see 
that the submodular pick (sp) greatli improv the user’ 
abil to select the best classifi when compar to random 
pick (rp), with lime outperform greedi in both cases. 

6.3 can non-expert improv a classifier? 
If one note that a classifi be untrustworthy, a common 

task in machin learn be featur engineering, i.e. modifi 
the set of featur and retrain in order to improv gener- 
alization. explan can aid in thi process by present 
the import features, particularli for remov featur 
that the user feel do not generalize. 

We use the 20 newsgroup data here a well, and ask ama- 
zon mechan turk user to identifi which word from the 
explan should be remov from subsequ training, for 
the bad classifi from the previou section (§6.2). In each 
round, the subject mark word for delet after observ 

B = 10 instanc with K = 10 word in each explan (an 
interfac similar to figur 2, but with a singl algorithm). 
As a reminder, the user here be not expert in machin 
learn and be unfamiliar with featur engineering, thu 
be onli identifi word base on their semant content. 
further, user do not have ani access to the religion dataset 
– they do not even know of it existence. We start the experi- 
ment with 10 subjects. after they mark word for deletion, 
we train 10 differ classifiers, one for each subject (with the 
correspond word removed). the explan for each 
classifi be then present to a set of 5 user in a new round 
of interaction, which result in 50 new classifiers. We do a 
final round, after which we have 250 classifiers, each with a 
path of interact trace back to the first 10 subjects. 

the explan and instanc show to each user be 
produc by sp-lime or rp-lime. We show the averag 
accuraci on the religion dataset at each interact round 
for the path origin from each of the origin 10 subject 
(shade lines), and the averag across all path (solid lines) 
in figur 10. It be clear from the figur that the crowd 
worker be abl to improv the model by remov featur 
they deem unimport for the task. further, sp-lime 
outperform rp-lime, indic select of the instanc 
to show the user be crucial for effici featur engineering. 

each subject take an averag of 3.6 minut per round 
of cleaning, result in just under 11 minut to produc 
a classifi that gener much good to real world data. 
each path have on averag 200 word remov with sp, 
and 157 with rp, indic that incorpor coverag of 
import featur be use for featur engineering. further, 
out of an averag of 200 word select with sp, 174 be 
select by at least half of the users, while 68 by all the 
users. along with the fact that the varianc in the accuraci 
decreas across rounds, thi high agreement demonstr 
that the user be converg to similar correct models. thi 
evalu be an exampl of how explan make it easi 
to improv an untrustworthi classifi – in thi case easi 
enough that machin learn knowledg be not required. 

6.4 Do explan lead to insights? 
often artifact of data collect can induc undesir 

correl that the classifi pick up dure training. these 
issu can be veri difficult to identifi just by look at 
the raw data and predictions. In an effort to reproduc 
such a setting, we take the task of distinguish between 
photo of wolv and eskimo dog (huskies). We train a 
logist regress classifi on a train set of 20 images, 
hand select such that all pictur of wolf have snow in 
the background, while pictur of huski do not. As the 
featur for the images, we use the first max-pool layer 
of google’ pre-train incept neural network [25]. On 
a collect of addit 60 images, the classifi predict 
“wolf” if there be snow (or light background at the bottom), 
and “husky” otherwise, regardless of anim color, position, 
pose, etc. We train thi bad classifi intentionally, to 
evalu whether subject be abl to detect it. 

the experi proce a follows: we first present a 
balanc set of 10 test predict (without explanations), 
where one wolf be not in a snowi background (and thu the 
predict be “husky”) and one huski be (and be thu predict 
a “wolf”). We show the “husky” mistak in figur 11a. the 
other 8 exampl be classifi correctly. We then ask the 
subject three questions: (1) Do they trust thi algorithm 



(a) huski classifi a wolf (b) explan 

figur 11: raw data and explan of a bad 
model’ predict in the “huski v wolf” task. 

befor after 

trust the bad model 10 out of 27 3 out of 27 
snow a a potenti featur 12 out of 27 25 out of 27 

tabl 2: “huski v wolf” experi results. 

to work well in the real world, (2) why, and (3) how do 
they think the algorithm be abl to distinguish between these 
photo of wolf and huskies. after get these responses, 
we show the same imag with the associ explanations, 
such a in figur 11b, and ask the same questions. 

sinc thi task requir some familiar with the notion of 
spuriou correl and generalization, the set of subject 
for thi experi be graduat student who have take at 
least one graduat machin learn course. after gather 
the responses, we have 3 independ evalu read their 
reason and determin if each subject mention snow, 
background, or equival a a featur the model may be 
using. We pick the major to decid whether the subject 
be correct about the insight, and report these number 
befor and after show the explan in tabl 2. 

befor observ the explanations, more than a third 
trust the classifier, and a littl less than half mention 
the snow pattern a someth the neural network be use 
– although all specul on other patterns. after examin 
the explanations, however, almost all of the subject identi- 
fie the correct insight, with much more certainti that it be 
a determin factor. further, the trust in the classifi also 
drop substantially. although our sampl size be small, 
thi experi demonstr the util of explain indi- 
vidual predict for get insight into classifi know 
when not to trust them and why. 

7. relat work 
the problem with reli on valid set accuraci a 

the primari measur of trust have be well studied. practi- 
tioner consist overestim their model’ accuraci [20], 
propag feedback loop [23], or fail to notic data leak [14]. 
In order to address these issues, research have propos 
tool like gestalt [21] and modeltrack [1], which help user 
navig individu instances. these tool be complemen- 
tari to lime in term of explain models, sinc they do 
not address the problem of explain individu predictions. 
further, our submodular pick procedur can be incorpor 
in such tool to aid user in navig larg datasets. 

some recent work aim to anticip failur in machin 

learning, specif for vision task [3, 29]. let user 
know when the system be like to fail can lead to an 
increas in trust, by avoid “silli mistakes” [8]. these 
solut either requir addit annot and featur 
engin that be specif to vision task or do not provid 
insight into whi a decis should not be trusted. further- 
more, they assum that the current evalu metric be 
reliable, which may not be the case if problem such a data 
leakag be present. other recent work [11] focu on ex- 
pose user to differ kind of mistak (our pick step). 
interestingly, the subject in their studi do not notic the 
seriou problem in the 20 newsgroup data even after look- 
ing at mani mistakes, suggest that examin raw data 
be not sufficient. note that groce et al. [11] be not alon in 
thi regard, mani research in the field have unwittingli 
publish classifi that would not gener for thi task. 
use lime, we show that even non-expert be abl to 
identifi these irregular when explan be present. 
further, lime can complement these exist systems, and 
allow user to ass trust even when a predict seem 
“correct” but be make for the wrong reasons. 

recogn the util of explan in assess trust, 
mani have propos use interpret model [27], espe- 
cialli for the medic domain [6, 17, 26]. while such model 
may be appropri for some domains, they may not appli 
equal well to other (e.g. a superspars linear model [26] 
with 5− 10 featur be unsuit for text applications). in- 
terpretability, in these cases, come at the cost of flexibility, 
accuracy, or efficiency. for text, elucidebug [16] be a full 
human-in-the-loop system that share mani of our goal 
(interpretability, faithfulness, etc). however, they focu on 
an alreadi interpret model (naiv bayes). In comput 
vision, system that reli on object detect to produc 
candid align [13] or attent [28] be abl to pro- 
duce explan for their predictions. these are, however, 
constrain to specif neural network architectur or inca- 
pabl of detect “non object” part of the images. here we 
focu on general, model-agnost explan that can be 
appli to ani classifi or regressor that be appropri for 
the domain - even one that be yet to be proposed. 

A common approach to model-agnost explan be learn- 
ing a potenti interpret model on the predict of 
the origin model [2, 7, 22]. have the explan be a 
gradient vector [2] captur a similar local intuit to 
that of lime. however, interpret the coeffici on the 
gradient be difficult, particularli for confid predict 
(where gradient be near zero). further, these explan ap- 
proxim the origin model globally, thu maintain local 
fidel becom a signific challenge, a our experi 
demonstrate. In contrast, lime solv the much more feasi- 
ble task of find a model that approxim the origin 
model locally. the idea of perturb input for explan 
have be explor befor [24], where the author focu on 
learn a specif contribut model, a oppos to our 
gener framework. none of these approach explicitli take 
cognit limit into account, and thu may produc 
non-interpret explanations, such a a gradient or linear 
model with thousand of non-zero weights. the problem 
becom bad if the origin featur be nonsens to 
human (e.g. word embeddings). In contrast, lime incor- 
porat interpret both in the optim and in our 
notion of interpret representation, such that domain and 
task specif interpret criterion can be accommodated. 



8. conclus and futur work 
In thi paper, we argu that trust be crucial for effect 
human interact with machin learn systems, and that 
explain individu predict be import in assess 
trust. We propos lime, a modular and extens ap- 
proach to faith explain the predict of ani model in 
an interpret manner. We also introduc sp-lime, a 
method to select repres and non-redund predic- 
tions, provid a global view of the model to users. our 
experi demonstr that explan be use for a 
varieti of model in trust-rel task in the text and imag 
domains, with both expert and non-expert users: decid 
between models, assess trust, improv untrustworthi 
models, and get insight into predictions. 

there be a number of avenu of futur work that we 
would like to explore. although we describ onli spars 
linear model a explanations, our framework support the 
explor of a varieti of explan families, such a de- 
cision trees; it would be interest to see a compar 
studi on these with real users. one issu that we do not 
mention in thi work be how to perform the pick step for 
images, and we would like to address thi limit in the 
future. the domain and model agnostic enabl u to 
explor a varieti of applications, and we would like to inves- 
tigat potenti us in speech, video, and medic domains, 
a well a recommend systems. finally, we would like 
to explor theoret properti (such a the appropri 
number of samples) and comput optim (such 
a use parallel and gpu processing), in order to 
provid the accurate, real-tim explan that be critic 
for ani human-in-the-loop machin learn system. 

acknowledg 
We would like to thank scott lundberg, tianqi chen, and 
tyler johnson for help discuss and feedback. thi 
work be support in part by onr award #w911nf-13- 
1-0246 and #n00014-13-1-0023, and in part by terraswarm, 
one of six center of starnet, a semiconductor research 
corpor program sponsor by marco and darpa. 

9. refer 
[1] S. amershi, M. chickering, S. M. drucker, B. lee, 

P. simard, and J. suh. modeltracker: redesign 
perform analysi tool for machin learning. In human 
factor in comput system (chi), 2015. 

[2] D. baehrens, T. schroeter, S. harmeling, M. kawanabe, 
K. hansen, and k.-r. müller. how to explain individu 
classif decisions. journal of machin learn 
research, 11, 2010. 

[3] A. bansal, A. farhadi, and D. parikh. toward transpar 
systems: semant character of failur modes. In 
european confer on comput vision (eccv), 2014. 

[4] J. blitzer, M. dredze, and F. pereira. biographies, 
bollywood, boom-box and blenders: domain adapt 
for sentiment classification. In associ for 
comput linguist (acl), 2007. 

[5] J. Q. candela, M. sugiyama, A. schwaighofer, and N. D. 
lawrence. dataset shift in machin learning. mit, 2009. 

[6] R. caruana, Y. lou, J. gehrke, P. koch, M. sturm, and 
N. elhadad. intellig model for healthcare: predict 
pneumonia risk and hospit 30-day readmission. In 
knowledg discoveri and data mine (kdd), 2015. 

[7] M. W. craven and J. W. shavlik. extract tree-structur 
represent of train networks. neural inform 
process system (nips), page 24–30, 1996. 

[8] M. T. dzindolet, S. A. peterson, R. A. pomranky, L. G. 
pierce, and H. P. beck. the role of trust in autom 
reliance. int. J. hum.-comput. stud., 58(6), 2003. 

[9] B. efron, T. hastie, I. johnstone, and R. tibshirani. least 
angl regression. annal of statistics, 32:407–499, 2004. 

[10] U. feige. A threshold of ln n for approxim set cover. J. 
acm, 45(4), juli 1998. 

[11] A. groce, T. kulesza, C. zhang, S. shamasunder, 
M. burnett, w.-k. wong, S. stumpf, S. das, A. shinsel, 
F. bice, and K. mcintosh. you be the onli possibl oracle: 
effect test select for end user of interact machin 
learn systems. ieee trans. softw. eng., 40(3), 2014. 

[12] J. L. herlocker, J. A. konstan, and J. riedl. explain 
collabor filter recommendations. In confer on 
comput support cooper work (cscw), 2000. 

[13] A. karpathi and F. li. deep visual-semant align for 
gener imag descriptions. In comput vision and 
pattern recognit (cvpr), 2015. 

[14] S. kaufman, S. rosset, and C. perlich. leakag in data 
mining: formulation, detection, and avoidance. In 
knowledg discoveri and data mine (kdd), 2011. 

[15] A. kraus and D. golovin. submodular function 
maximization. In tractability: practic approach to hard 
problems. cambridg univers press, februari 2014. 

[16] T. kulesza, M. burnett, w.-k. wong, and S. stumpf. 
principl of explanatori debug to person 
interact machin learning. In intellig user interfac 
(iui), 2015. 

[17] B. letham, C. rudin, T. H. mccormick, and D. madigan. 
interpret classifi use rule and bayesian analysis: 
build a good stroke predict model. annal of appli 
statistics, 2015. 

[18] D. marten and F. provost. explain data-driven 
document classifications. mi q., 38(1), 2014. 

[19] T. mikolov, I. sutskever, K. chen, G. S. corrado, and 
J. dean. distribut represent of word and phrase 
and their compositionality. In neural inform 
process system (nips). 2013. 

[20] K. patel, J. fogarty, J. A. landay, and B. harrison. 
investig statist machin learn a a tool for 
softwar development. In human factor in comput 
system (chi), 2008. 

[21] K. patel, N. bancroft, S. M. drucker, J. fogarty, A. J. ko, 
and J. landay. gestalt: integr support for 
implement and analysi in machin learning. In user 
interfac softwar and technolog (uist), 2010. 

[22] I. sanchez, T. rocktaschel, S. riedel, and S. singh. toward 
extract faith and descript represent of latent 
variabl models. In aaai spring syposium on knowledg 
represent and reason (krr): integr symbol 
and neural approaches, 2015. 

[23] D. sculley, G. holt, D. golovin, E. davydov, T. phillips, 
D. ebner, V. chaudhary, M. young, and j.-f. crespo. 
hidden technic debt in machin learn systems. In 
neural inform process system (nips). 2015. 

[24] E. strumbelj and I. kononenko. An effici explan of 
individu classif use game theory. journal of 
machin learn research, 11, 2010. 

[25] C. szegedy, W. liu, Y. jia, P. sermanet, S. reed, 
D. anguelov, D. erhan, V. vanhoucke, and A. rabinovich. 
go deeper with convolutions. In comput vision and 
pattern recognit (cvpr), 2015. 

[26] B. ustun and C. rudin. superspars linear integ model 
for optim medic score systems. machin learning, 
2015. 

[27] F. wang and C. rudin. fall rule lists. In artifici 
intellig and statist (aistats), 2015. 

[28] K. xu, J. ba, R. kiros, K. cho, A. courville, 
R. salakhutdinov, R. zemel, and Y. bengio. show, attend 
and tell: neural imag caption gener with visual 
attention. In intern confer on machin learn 
(icml), 2015. 

[29] P. zhang, J. wang, A. farhadi, M. hebert, and D. parikh. 
predict failur of vision systems. In comput vision 
and pattern recognit (cvpr), 2014. 


1 introduct 
2 the case for explan 
3 local interpret model-agnost explan 
3.1 interpret data represent 
3.2 fidelity-interpret trade-off 
3.3 sampl for local explor 
3.4 spars linear explan 
3.5 exampl 1: text classif with svm 
3.6 exampl 2: deep network for imag 

4 submodular pick forexplain model 
5 simul user experi 
5.1 experi setup 
5.2 are explan faith to the model? 
5.3 should I trust thi prediction? 
5.4 can I trust thi model? 

6 evalu with human subject 
6.1 experi setup 
6.2 can user select the best classifier? 
6.3 can non-expert improv a classifier? 
6.4 Do explan lead to insights? 

7 relat work 
8 conclus and futur work 
9 refer 

