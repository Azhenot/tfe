











































ieee AI & ethic summit 2016 - Ar t i f ic ia l intel l igenc & ethic – who doe the thinking? 1 

ieee AI & ethic summit 2016 
artifici intellig & ethic 
– who doe the thinking? 

tuesday 15 novemb 2016 
the sofitel brussel europ 
place jourdan 1, brussels, 1040, belgium 

join the convers at: 
#ieeeaisummit 

www.ieee.org 
© ieee 2016 

summit report 



2 

www.ieee.org 

artifici intellig can be program to perform 
a human. but be it the smartest thing to do? 

artifici intellig be quickli find it way into the life of peopl across the world thank to the imagin 

and hard work of technologist and innovators, mani of whom be ieee members. As AI becom a great part 

of our everyday life so do the discuss about manag it risk and rewards. 



ieee encourag innov to follow their idea — from the first insight thought to assess and discuss how 

that implement idea affect the futur of humankind. As more applic of AI be developed, ieee member 

be lead the discuss on how to integr ethic consider into the design of AI innov and on 

open-end question that we need to keep in mind a we work to achiev ai’ potenti benefit to humanity. 

ieee: foster technolog innov for the benefit of humanity. 



ieee AI & ethic summit 2016 - Ar t i f ic ia l intel l igenc & ethic – who doe the thinking? 3 

www.ieee.org 

artifici intellig can be program to perform 
a human. but be it the smartest thing to do? 

artifici intellig be quickli find it way into the life of peopl across the world thank to the imagin 

and hard work of technologist and innovators, mani of whom be ieee members. As AI becom a great part 

of our everyday life so do the discuss about manag it risk and rewards. 



ieee encourag innov to follow their idea — from the first insight thought to assess and discuss how 

that implement idea affect the futur of humankind. As more applic of AI be developed, ieee member 

be lead the discuss on how to integr ethic consider into the design of AI innov and on 

open-end question that we need to keep in mind a we work to achiev ai’ potenti benefit to humanity. 

ieee: foster technolog innov for the benefit of humanity. 

tabl of content 

welcome: marko delimar – chair, ieee european public polici initi 

keynot speech: wojciech wiewiórowski – assist supervisor, european 
dataprotect supervisor (edps) 

q&a 

session 1: autonom system – manag risk and reward 

session 2: program human ethics: cui bono? 

session 3: social implic – peril & promis of AI 

final thought — john C. haven 

5 

7 


9 

10 

14 

18 

22 



4 welcome: marko del imar – chair, ieee european publ ic pol ici ini t iat ive 



ieee AI & ethic summit 2016 - Ar t i f ic ia l intel l igenc & ethic – who doe the thinking? 5 

welcome: marko delimar – chair, ieee european public polici initi 

the chair of the ieee european public polici initi 
welcom the audienc to the “highli relevant” summit 
on artifici intellig and ethics. “we reli more and 
more on artifici intellig in mani applications,” say 
delimar, further explain that ieee be bring togeth 
thought leader in ethic and artifici intellig in 
order to contribut to “an ongo dialogu regard the 
issu that must be consid and address prior to 
the widespread adopt of artifici intellig centric 
technologies.” 

delimar say that recent innov and develop 
in artifici intellig herald it fully-fledg arriv in 
everyth from automobil to cognit computing. but 
he added, the question remains: “will all of thi benefit 
humanity?” 

“A simpl answer be ‘ye of course.’ the full answer be 
much more complex than that and that be one of our 
convers here today,” he continued. 

delimar then outlin some of the numer ethic 
concern that have to be addressed: what safeguard 
should be put in place to protect the massiv amount 
of person data need to power artifici intelligence? 
when can we expect shift in respons in the 
workplace? what will those shift look like? and will there 
be impact on the workers? how do we ensur complianc 
with safeti standards? who take respons if artifici 
intellig malfunctions? 

“it’ up to u to build a world where we want artifici 
intellig to oper within. We have all see the 
termin movi and i’m tri veri hard not to end thi 
speech with ‘i’ll be back’,” he joked. 

“we be all too familiar with cinemat and literari 
storylin in which artifici intellig run amok and 
human suffers. but thi be fiction. In the real world, 
decis and deliber must come befor widespread 
adoption. from comput to commun technology, 
that model have be true for more than a century,” say 
delimar, cite electricity’ roll out more than 125 year 
ago, and the internet that have be around for more than 
25 years. 

“the best approach to build an ethic futur for artifici 
intellig be base within the global commun with a 
mani concern entiti a possibl particip in the 
discussion. onli then will foundat ethic artifici 
intellig deliv it promise,” he concluded.  



6 keynot speech: wojciech wiewiórowski – assist supervisor, european dataprotect ion supervisor (edps) 

keynot speech: wojciech wiewiórowski – assist supervisor, european 
data protect supervisor (edps) 

wiewiórowski open hi address with a parabl from 
nick bostrom’ book superintelligence: It be the stori of 
some sparrow that decid they need an owl to help them 
build nest faster — how easi life would be! — despit 
the fact that they know noth about owls. there be onli 
one sparrow who be a bit skeptic about it a owl be big 
and difficult to manag and he fear the sparrow don’t 
know what they’r get into. He be told he have a fret 
temperament. 

“the role of the european data protect supervisor be not 
to be the prophet of the apocalypse,” say wiewiórowski, 
“but we want to keep the posit of ask questions. 
sometim difficult questions...even sometim the 
question [people] do not want to hear.” 

the assist edp also drew on the world of fiction to 
illustr hi points: “there be of cours a lot of hope 
and expect around artifici intellig that be 
creat in the last half of the century. and of cours there 
be fear a well. when we watch the movi that we 
alreadi heard about, we think about hal 9000, we think 
about skynet, we think about the replic from blade 
runner,” say wiewiórowski, note that, accord to the 
story, some of the fiction android be assembl in 
2016. 

“but now the artifici intellig be veri practic and 
artifici intellig be the realiti that have alreadi woven 
it way into our everyday life with navig system with 
spam filter and weather forecast just to name a few,” he 
continued. 

wiewiórowski mention ibm’ watson supercomput 
and ross, an AI lawyer describ by richard suskind in hi 
book the end of the lawyers. “thi amount of attent 
show that it’ not too earli to talk about artifici 
intellig and it implic to everyday life,” say 
wiewiórowski. It be for thi reason that the global data 
protect authorities’ confer in marrakesh chose the 
challeng of artifici intellig a the main topic of 
discussion, he explained. 

the univers declar of human right take human 
digniti a it start point say wiewiórowski. “in earli 
21st centuri individu be increasingli disclos 
inform about themselv and digit profil can be 

creat thank to artifici intellig techniqu and 
share in microseconds. the use of artifici intellig to 
predict people’ behavior risk stigmatization, reinforc 
exist stereotypes, social and cultur segregation, 
and exclusion, subvert individu choic and equal 
opportunities.” 

and wiewiórowski do see some risks. sinc artifici 
intellig system learn from the inform provid 
and have no way of see “the big picture,” whatev 
bia be introduc in train will be reinforc and will 
influenc the predict made, explain wiewiórowski. 
“if those predict be use to take decisions, a viciou 
circl start to work and self-fulfil propheci can be 
created,” he warned. 

“machin learn be one of the most research subject 
on artifici intellig and it involv the construct 
of algorithm that can learn and make predict use 
data. machin learn algorithm repres knowledg 
and structur which sometim cannot be translat into 
a form which be intellig for u or without sacrific it 
meaning. and thi be serious. It have seriou implic 
for data protection, a it mean that we may not have 
appropri inform about how our person data be 
used, and importantli how decis concern u be 
take therefor make it imposs to meaning 
consent to use our data,” he said. 

“data protect framework in europ requir 
organ and control of data to be transparent. As 
far a the data be concerned, but also a far a the algorithm 
that be in use be concerned, thi be especi demand 
in the world of machin learn where algorithm which 
be in use may be unknown and unpredict even for 
those who have develop them.” 

“in the near futur data protect author will deal 
with the case where machin learn have be use 
for challeng or support a decision,” wiewiórowski 
predicted. “we need to adopt a realist approach to artifici 
intelligence. We now have a window of opportun to 
build the right valu in today’ technology.” 

but how do the stori of the sparrow and the owl end? 
We don’t know, especi in the absenc of actual owl to 
practic on. 



ieee AI & ethic summit 2016 - Ar t i f ic ia l intel l igenc & ethic – who doe the thinking? 7 

“the role of the european data 

protect supervisor be not to be 

the prophet of the apocalypse,” 

“I come to thi confer look for an owl to practic 
on a well,” conclud wiewiórowski, “and I have also 
come here to meet those who know more about owl 
than I do.”  



8 q&a 



q&a 

when ask whether new regulatori scheme and 
new law for AI should be develop or whether 
it fit into the exist bodi of law and regulation, 
wiewiórowski replied: “one of the bad thing that 
we can do be to regul fast. it’ not a good idea. 
We have our data protect regulation, which in 
my field tri to be the gener solut for all 
the problems. Of cours it will not answer all the 
questions, but at the moment that’ what we have. 
I would not be the support of strong regul 
befor we realli find out what we be regulating.” 

“we have exampl in the histori of european law, 
a well a other situations, that go the other 
way around. and the best exampl for me be the 
direct on electron signature. the idea in thi 
direct be actual great. the onli problem be 
that the market didn’t want to follow thi idea. So 
even if it be great in the beginning, it be not 
use in practice. and after fifteen year we found 
ourselv more or less in the same posit we 
be at the moment when the regul be 
created,” he said. 

“after a car accid we 

use to cross-examin the 

driver; now it will be robots” 

ieee AI & ethic summit 2016 - Ar t i f ic ia l intel l igenc & ethic – who doe the thinking? 9 

 



10 session 1: autonom system – manag ing risk and reward(edps) 

session 1: autonom system – manag risk and reward 

the moderator, john C. havens, author and execut 
director of the ieee global initi for ethic 
consider in artifici intellig and autonom 
systems, open the panel by point out that “the advent 
and increas sophist of autonom system 
offer signific potenti benefit in divers applic 
domain includ manufactur and transportation, 
healthcar and financi services, exploration, mainten 
and repair. As well a cost and risk reduction, potenti 
benefit includ enhanc productivity, precis and 
accuracy, good health outcomes, low mortal and 
injuri rate due to human error, a well a opportun 
for great human creativity.” 

however, he continued, “these be counterbal 
by a broad rang of ethical, social, philosoph and 
legal concerns, includ further dehuman warfare, 
creat existenti threat and damag the fabric of 
human society.” 

the main question for the panel, from the perspect of 
reduc the likelihood of neg a well a unintend 
consequences, be what be the best way to manag risk and 
reward? 

jérôme perrin, VP scientif director of group renault, 
who come from an academ background in physic now 
focu on ethic question of machin learn and the 
autom car. “autom vehicl be a special kind of 
robot becaus they be oper by non-expert people,” 
he said. they also interact with mani social element in a 
complex environ and must be manag in real-tim 
for obviou safeti reasons. 

the question of safeti be the overrid theme of the 
panel, and unsurprisingli mani particip refer to 
semin scienc fiction writer, isaac asimov’ law of 
robot — three rule to which hi fiction robot must 
adher to in order to protect human life. 

“if you rememb asimov’ rules, one of them be 
basic that eventu the robot should self-sacrific 
or commit suicid if would harm a human being,” say 
perrin. “in the case of autom cars, where you have 
a human be outsid and insid the vehicle, there be a 
debate. It be about manag risk.” 

engin may design the algorithms, but if a user can 
set these paramet then be it the driver’ respons 
or liabil if someth go wrong, he added. We must 
defin the level of car automation. “suppos I could tune 
my car to say I don’t want a scratch at ani cost,” say 
perrin. 

kay firth-butterfield, a distinguish scholar, barrist 
and judge, co-found of AI austin and co-found of 
the consortium on law and ethic of AI and robotics, at 
the robert strauss center for intern secur and 
law, univers of texas, austin said: “in common law it 
could be argu that if you set your car up so that it onli 
kill other peopl not you, it could be determin to be 
premedit murder.” 

“we, at AI austin, be look at AI in healthcar and 
education,” explain firth-butterfield, but she believ 
that ethic principl can appli to ani company. “we 
need to design in ethic procedur and we need to do 
that throughout the product cycl and even into the sale 
cycles. one of the problem we have in the unit states, 
be that an american compani be design onli to serv 
it shareholders, maxim profit be the onli motivation, 
in europ you have differ motivations. I think there 
be room for someth call a chief valu offic in 
companies,” she said. 

raja chatila, director of the institut of intellig 
system and robotics, univers pierr and mari 
curie, pari discuss whether or not it make sens to 
have preemptiv ban and where to draw the lines. “A 
preemptiv ban on what?” he asked. “mi point be you 
have to be clear what you be banning. are you banning, 
for example, mari scott of scott curi from discov 
radium that be use everi day in hospit to cure people? 
Or be you ban oppenheim from lead the 
manhattan project? even with the manhattan project 
and the decis to use the atom bomb, debat be still 
ongo be it right or wrong mani year after the event. 
So ban be realli someth that you have to reflect 
upon for a long time befor make a decision.” 

“in fact,” chatila continued, “there have be discuss 
on autonom weapon for the past three year in the 
unit nation on the convent on unconvent 
weapon to decid if a ban should be pronounc or � 



10 session 1: autonom system – manag ing risk and reward(edps) ieee AI & ethic summit 2016 - Ar t i f ic ia l intel l igenc & ethic – who doe the thinking? 11 



12 session 1: autonom system – manag ing risk and reward(edps) 

not, and the ban, if it be decid would be on veri specif 
definit of autonom weapons. So I don’t believ 
that ban preemptiv be realli useful. If it’ applied, 
it will stop u — I mean human — from do research 
or from understand phenomenon or develop use 
applications.” 

the next question be then how to enforc a ban if one 
be agreed: “there be bans, for exampl there be an 
intern convent about the prolifer of nuclear 
weapon and if you know the story, it’ quit difficult to 
realli apply.” say chatila. “ban be an interest idea 
if we be sure we know what we be banning. take arms. 
they be not ban in the unit state and probabl 
won’t be for a long time, but they be ban unless 
specif condit appli in most european countries. We 
know what that ban means: that ani person cannot 
just buy a weapon and walk around with it. We have 

be live with weapon for millennia, so we know what 
we be talk about and we know the consequences. 
therefore, ban have practic sense,” he explained. 

“but when we be tri to ban someth that we don’t 
understand yet – that’ quit challenging. I would say it’ 
probabl imposs and certainli not desir at the 
stage where we be tri to discov what we be doing,” 
he continued. 

“it’ also kind of a sad ironi in term of ethics. If you 
ban someth too fast, obvious you’r make a 
judgment about it and your point of view may not be 
the same around the world from differ countries,” 
comment havens. 

“the question be alway how much green wash be 
push what’ actual go on in the company,” say 

� 



firth-butterfield, prompt haven to remark that “ethic 
be the new green,” a comment that reson with mani 
at the summit. 

“that’ whi I feel we could see ethic panel within 
companies, we could see chief valu officers, we could 
see chief AI offic where all of these would help valid 
and to help brand say ‘we be behav ethically, and we 
have bought into that,’ and it be good for profit margins,” 
explain firth-butterfield. 

“I think that it’ probabl the way to go and i’m encourag 
by, for example, the AI partnership where you have five 
big AI compani say we recogn that AI be someth 
that we need to think about self-regulating. that’ an 
import need. 

“but I want to come back to someth we mention 
earli and that’ vulner people. In the law, we think 
about vulner peopl a peopl with disabilities, peopl 
with child and I think it’ actual realli import that 
we do you continu on with thi convers about ethic 
and regul and think particularli about our children. 
when I be lead the ethic advisori panel at lucid 
ai. My first pick be actual someon from unicef and 
everybodi ask why. but actual if thi be thi be about 
our child and the future, we should be think about 
protect our vulner kids. ai in barbie’ listen all 
the time, for example. We need to think more structur 
about regul on that,” she said. 

juha heikkilä, head of unit, robot and artifici 
intellig in the european commission’ DG connect 
depart which have be fund robot for 
approxim 12 year say that he see huge potenti in 
robot and autom to do away with those “dull, dirti 
and danger jobs.” 

“we have conduct a eurobaromet survey on the public 
attitud on robot and autonom system and these 
would of cours obvious involv ai, ai-bas system 
and AI components. We do that twice in fact and make 
some interest discoveri on people’ attitud on 
how they respond to robots. so, for example, eighty-nin 
percent of peopl think AI be a technolog that requir 
care management. seventi percent of peopl think that 
these kind of system could steal their jobs. On the other 

hand, a veri high percentag — more than seventi percent 
— thought robot can be veri useful. becaus they help 
peopl and they can be do dull and danger jobs,” 
he said. 

accord to heikkilä a veri mix pictur be emerg of 
the public mood and percept in thi area. 

“we see on the one hand, trust, and on the other hand 
also all a lot of skeptic about the technology. So it’ not 
all negative, not all positive. perhap the most interest 
find be that, at least in some respects, familiar tend 
to allevi fears, so those who be familiar with these 
system tend to have few fear about it — they don’t 
have a kneejerk reaction if you like.” 

ieee AI & ethic summit 2016 - Ar t i f ic ia l intel l igenc & ethic – who doe the thinking? 13 

“the question be alway how 

much green wash be push 

what’ actual go on in the 

company” 

 



14 session 2: program human ethics: cui bono? 

session 2: program human 
ethics: cui bono? 

there have be propos to program ethic 
algorithm into machines, such a car or robots. 
but be machin even capabl of make what 
human consid ethic or moral decisions? that 
be the question tackl by the second panel. 

accord to joanna bryson from the depart 
of comput scienc at univers of bath and 
the center for inform technolog polici at 
princeton univers there be at least three way 
we can program ethic algorithms. 

“one possibl be that human explicitli program 
in the instructions. We just say ‘in thi condition, 
do that,’ ‘in thi other condition, do this.’ So we can 
set the priorities. there be still ethic concern 
about that. for example, if all the peopl involv 
be male, will they overlook consider of 
diversity?” she said. 

“anoth possibl be for system to learn rule 
automatically. even then, you can train system 
to learn rule or you could do sort of statist 
learning, or non-symbol learn like the deep 
neural network where we have less explicit access 
afterward to see what’ be learned. I don’t think 
that’ the big concern. we’r not that good at 
explain whi human do thing either. I think the 
big problem there be — and thi be someth 
actual the white hous have be say a great 
deal — be if we learn about society, and how we do 
thing now, we may entrench bad thing a well a 
good things,” state bryson. 

“so machin learn off of our flaw societi will 
replic it flaws,” she warned. 

but she point out that be what human learn 
do too, and it’ onli okay “if it doesn’t get 

“ethic be the new green” 



ieee AI & ethic summit 2016 - Ar t i f ic ia l intel l igenc & ethic – who doe the thinking? 15 

complet codified, and set down and use to perpetu 
the error we’v alreadi made. it’ incred import to 
distinguish between what I would call norm versu 
descriptive.” 

scienc be descriptive, polici isn’t. “scienc be about 
explain and predicting. polici govern be about 
say what should happen,” she explained. 

“when we consid our ethic we should never think of 
human strictli a means. some peopl think that say we 
shouldn’t think of human a mean [at all], but of cours 
we’r the means. we’r the one who do everything. So 
human be the means. but we’r not onli the means, 
we must alway be the ends. and I think that veri much 
inform my ethic that human be of ultim import 
and that we have to be supported,” say bryson. 

event moderator, havens, add to bryson’ comment 
by mention the morpholog of robots: “the major 
of embodi robots, 90 percent, mean one that be 
design to look like people, look like attractive, young, 
white women. what a shocker!” he joke befor make 
a seriou point. “I have noth against young, attractive, 
white women, but whi would you build robot that onli 
look like one type of person versu like the commun 
where they’r go to be placed?” 

sarah spiekermann from the institut for manag 
inform systems, vienna univers of econom 
and business, say that languag be part of the problem. 
“the IT world have use terminolog from the human 
for decad and misus those term by promis 
thing and valu that the system have never live up 
to. let’ say one of the valu in a system should be the 
system’ transpar or the degre to which the system 
be maintain a person’ privacy. the first thing be that 
you actual need to understand by the valu itself, be 
what be privaci or what be transparency,” she said. “half 
of my book be about make these distinct clear to 
bring valu definit and understand of valu to 
comput science.” 

“we need to break down the valu that we want to 
have in the system. We need to model and document 
the system we be build and we have to run system 
through risk assessments. but, risk assess where we 

can plug in ani valu into a system and not just secur a 
we alreadi have secur risk assess type of privaci 
risk assessments.” 

“ethic theori be not just about prevent harm or be 
moral. it’ also about make good decis — simpl 
a that. and for a compani to make a good decis it 
have to maxim the valu proposit of the system. the 
valu proposit be at the core of the busi model 
and an ethic develop lifecycl have that goal a 
well. it’ not just prevent harm. it’ also foster the 
valu of assist for the servic of humans,” continu 
spiekermann. 

ask whether machin be capabl of make ethic 
or moral decis or if human alway need to be in the 
loop, the third speaker, corrin cath, a phd student at the 
univers of oxford and an alan ture institut doctor 
student replied: “the question be not whether human 
should alway be in the loop, becaus they alway are 
to some degree.” 

“even if you work with algorithm that come from neural 
network where human be for a larg part take out, 
whatev that algorithm be go to be appli to will most 
like affect people,” she said. “but I think thu far, we 
have see that when we let algorithm just run amok, we 
get veri bad outcom especi for vulner people: 
look at algorithm for sentenc in the courtroom, look 
at algorithm for predict policing, even look at tri 
to hire new peopl and you have an algorithm look at 
their cvs.” 

An algorithm be not go to flag up all the bia that 
be in societi in it results, say cath argu that you 
need a human to recogn subtl racism or sexism or 
ani of the other -ism at play. “I would definit say you 
need a human be in the loop to be abl to catch those 
thing and call them out. and we haven’t even start 
speak about autonom weapon yet! there, if you 
take the human out, you also get into a veri grey zone of 
intern humanitarian law.” 

“I see a lot be peopl say ‘machin can do ethics’ or 
‘we can program this.’ i’m not sure they can. I think they 
can approxim ethics, but human have choice. We have 
agency. there be a lot of ‘squishy’ concept that go into � 



16 session 2: program human ethics: cui bono? 

make ethic decis includ our understand 
of human digniti and our understand of compassion. 
these be veri difficult thing to teach an algorithm to do 
and a such I would be veri hesit to outsourc ethic 
decision-mak to a machine,” conclud cath. 

madi delvaux, member of the european parliament, who 
have take the lead in AI and robot issu say that she 
repres the normal citizen who be interest in the 
develop of robot and tri to understand what 
be go on. 

“how can you explain an algorithm to a normal person? 
I be not so optimist when I follow the discuss 
in my group. our concern a polici maker be to make 
it access to the majority. We want transpar on 
the algorithm — how to get it be anoth matter. I don’t 
think that technolog make peopl more equal, becaus 
access to technolog be an enorm consideration,” she 
say befor focu on the issu of privacy, transpar 
and trust. 

“in the parliament we have veri passion debat on 
privaci and data protect and I think, for example, care 
robot who have access to a veri privat sphere should 
have mechan that you can switch off from time to 
time. We want more privacy. My question to the scientif 
commun would be, be it possibl to deliv that?” 
say delvaux. 

“the other concern be about transpar and sometim 
i’m afraid when I talk to the specialists, becaus how 
can you explain it to a normal consumer? It should be 
explain in a way so that someon who’ not a scientist 
can understand what be happen with these objects.” 

with androids, delvaux want it to be clear that while 
some devic appear to show empathi they can never 
feel empathy. “we will not prevent [ai and robotics] from 
be on the market, but what can we do, be to make sure 
that at least human have the option of say ye we 
want thi or we don’t want this,” she said. 

bryson late take up the question of moral decisions, 
refer to the first panel’ discuss about self-driv 
cars. “we need to discriminate. there be two differ 
thing about appli ethic rule or have the robot 

make decis that we consid to be moral. you may 
have to embodi moral decis in the car, but that doesn’t 
mean that the car be respons for the decisions. thi 
be the key distinction. it’ incred ill-advis to say that 
the machin be respons for that decision. someon 
program that machine, someon own that machin 
and someon have make the decis to use that machine,” 
she say highlight the difficulti in ascrib liability. 

In her report for the european parliament, delvaux 
suggest ascrib a legal person for robots, at 
least for those robot with a high degre of artifici 
intellig and with self -learn capabl a a 
possibl way forward. the mep explain that it be an 
idea worth exploring, but debunk the idea that she be 
tri to “humanize” robots. 

� 



ieee AI & ethic summit 2016 - Ar t i f ic ia l intel l igenc & ethic – who doe the thinking? 17 

“thi be far from my intention,” she said. “but we ask the 
commiss to studi and to evalu the pro and con of 
such a solution. I have to confess that I don’t know. and 
there be so mani thing we don’t know yet, but I believ 
that in the develop of differ scenario it would be 
good if we studi the differ possibl befor it be 
too late.” 

“i’m realli enjoy thi discuss becaus I think it’ 
the first real discuss that i’v be to where I feel 
that peopl be realli get to some of the issues,” say 
cath. “but I want peopl to take away AI and robot for a 
minute, and just replac them with the word properti and 
look at them a property. I want peopl to ask question 
like whi be whi be we a a commun ask peopl if 
they can develop empathi for a robot or AI property. whi 

be we concern whether peopl can have attachments? 
whether they can feel supported? through my research, 
I begin to realiz that these be exactli the same kind 
of question market execut ask peopl in focu 
group — they want to know if you buy a product how it be 
go to make you feel.” 

spiekermann argu that the “post-digit age be about 
the demystif of technologies.” “A time where we 
know about the limit of technology, but where we work 
in tandem with power technologies. but a veri import 
part of that demystif process be thi idea of ‘better.’ 

that everyth that be digit be automat good be an 
import fallacy. I do believ in mani respect technolog 
help u to make good decisions. i’m not oppos 
to technology, but what I deepli resist be the idea that 
anyth that be new and digit be automat better.” 

bryson be more nuanced: “there’ a slipperi slope 
tradeoff argument. I know psycholog we alreadi be 
there. peopl alreadi think that i’m racist becaus I don’t 
think that AI be human. I mean, I get it — we’r alreadi 
awar that peopl be mistak the contemporari AI for 
someth that be human-like. everyth be chang so 
fast now that we’r have troubl keep up with it,” 
she concluded. 

“post-digit age be about the 

demystif of technologies” 

 



18 session 3: social implicat ion – peri l & promis of AI 

session 3: social implic – peril & promis of AI 

“there be clearli social implic and ethic 
challeng aris from the use of AI in rapidli evolv 
sector despit the technic capabl that may exist,” 
say haven introduc the third and final panel of the 
day. “there be often practic challeng associ 
with balanc compet demand on finit capac 
and resourc in a context where decis relat to 
the suitabl or priorit of individu to access 
services. for instance, health insur and technolog 
innov may potenti be base on increasingli 
autom assess of perceiv risk factors. how 
do we ensur that critic decis make continu to 
incorpor a strong ethic dimens align with human 
valu in an increasingli complex black box decis 
make environment?” 

In other words, qui custodiet ipso custod — who 
watch the watchmen? 

nikolao mavridis, graduat of mit and founder director 
of the interact robot and media lab (irml) ask 
attende to “imagin a world where robot and AI be 
part of our everyday life and not onli a helpers, but also 
a companion and friends. and then to imagin a world in 
which we might all be abl to take part in giant intellig 
entiti that be make up of potenti thousand of 
human and machines.” 

even if such a thing be possibl onli for five or ten 
second a day, mavridi say such entiti might end up 
by far surpass the current limit both of human a 
well a artifici intelligence. but he also look at the 
labor implic of automation: “so for everi american 
worker with an hourli wage of less than $20 there be an 83 
percent probabl of autom of hi actual activ in 
the future. Of cours all of thi go veri well to the huge 
discuss that be take place at the moment regard 
what the real effect of autom will be on the labor 
force, how govern will deal with thi and whether 
we can chang it.” 

“one point I want to touch upon be the fact that all of these 
question regard ethic have variou stakehold that 
be play an import role and we don’t yet have either 
standard nor, I think, effect process for be abl 
to bring these stakehold together. take for exampl the 
question of robot in warfare: you have academia, you 

have governments, you have intern organizations, 
you have the militari industri and you have mass medium 
which all play a veri import role. and in the middl 
of all thi you have the citizen of the world. I would 
like to ask how much be the citizen of the world realli 
involv in the kind of possibl futur that we might 
have regard these technologies,” say mavridis. 

“the big question be how we present ourselves, how can 
we decid what kind of architect we want to be, and 
what kind of world we want to creat for the future. 
rememb that thi futur will not onli be our futur 
but also the futur of our children,” he concluded. 

“the challeng we be face when we discu artifici 
intellig be the classic issu we’r face with 
technolog impact assessments,” say paul nemitz, 
director for fundament right and union citizenship in 
DG justic at the european commission, befor go on 
to elabor on the ”precautionari principle.” 

the problem come when “artifici intellig be abl 
to do thing which we a human can’t foresee,” he said. 
use the exampl of environment law, he explained: 
“the precautionari principl say if two condit be 
fulfil namely, one, a chain of causal be put in motion 
and we don’t know where it’ go to lead. second, if 
the condition/situ can have a huge impact on human 
be — mayb even extinction, but mayb also the huge 
chang of the way we live — then we have the duti to 
invest in technolog impact assessment.” 

“also [we must] put ourselv into the polit and 
emot state to make decis today, to make sure 
that these long-term neg impact on human do not 
occur,” say nemitz. 

“thi be normal someth where ethic be veri important. 
but even more import is, of course, the law, which 
through a democrat process creat oblig — 
includ oblig that limit the abil to roll out and 
use such technology. So I think that’ the tip point.” 

“for me a a lawyer and fundament right policy- 
maker the key question is: have we alreadi reach the 
point where we have to admit and live with the fact that 
artifici intellig will creat it own new causalities? � 



ieee AI & ethic summit 2016 - Ar t i f ic ia l intel l igenc & ethic – who doe the thinking? 19 

“there be clearli social 

implic and ethic 

challeng aris from the 

use of AI in rapidli evolv 

sector despit the technic 

capabl that may exist” 



20 session 3: social implicat ion – peri l & promis of AI 

that rais a lot of issu which i’m convinced, in a 
democracy, legisl will eventu have to address,” 
nemitz said. “it’ on the horizon.” 

greg adamson, chair of ieee’ technic activities, ethics, 
societi and technolog initi say we shouldn’t trust a 
compani on the simpl basi that ‘they know what they’r 
doing,’ but rather on the precaut they’r take around 
creat those technologies. 

elabor on the idea that the output of machin 
learn be not alway discern to their creators, 
adamson use the exampl of the steam engine. “the 
steam engin be commerci around the last decad 
of the 18th centuri and at the time it be built, it be 
unclear how it actual worked. It be onli with the 
develop of thermodynam around the 1850 or 
1860 that scientist start to understand how the steam 
engin actual worked,” he said. 

“that may sound strang — how could you build someth 
when you don’t know how it would work? It mean initi 
you build it use rule of thumb. So you tri it out and 
you see if it blow up. If it blow up, you need to make 
the metal a bit thicker. you just keep on tri it out, but 
until you’v actual get a theori of thermodynamics, you 
can’t figur out how to maxim the design of a particular 
device, in thi case the steam engine.” 

“so we now come to the into the area of artifici 
intellig and if we look at the promis of the artifici 
intellig in the 1960s, I think thing haven’t move a 
quickli a expected. i’m not talk here about terminator, 
or sentient artifici intelligence,” say adamson, be 
he add it be reason to expect some sort of 
breakthrough in the next 12 month to 40 years. 

“when that breakthrough occurs, by definition, the 
compani or that organ that achiev it won’t 
understand whi they make the breakthrough. they won’t 
understand becaus if they understood they’d go and 
make the breakthrough now. basic lot of differ 
commun be tri out differ things,” he continued. 

“so they cannot say ‘trust u with this, we know how it 
works,’ becaus they don’t know how it work and may 
not know how it work for decades,” say adamson. “the 

suggest here be that instead of say we trust thi larg 
organization, we ask what precaut be be take 
today by thi organ a it’ develop thi work? 
obvious commerci organ will strongli resist 
the releas of their algorithms. that’ a truism. however, 
to ask them to releas that evid of the precaut be 
a differ sort of discussion.” 

auréli pols, data govern and privaci advocate, who 
be a member of the european data protect supervisor’ 
(edps) ethic advisori group, highlight human- 
machin hybrid system and examin where the human 
be sit in such a framework. 

“mi main question, take into account thi possibl futur 
and all of the other configur of human machin 
systems, be what be the role that human be play in 
relat to AI and to robotics?” 

“first of all we have the creator of artifici system and 
they might be the visionaries, or design or engineers. 
then you have the approv of such system — peopl 
who work on funding, peopl who be do plan 
with the ministries, etc. after that, the next stage be the 
interact partner of systems, i.e. peopl that be 
interact with a robot or with a machin and thi could 
be either in a collabor role or in a master-slav role 
or all the other possibl combin within a team,” 
she said. 

“beyond that, you slowli start to have the trainer of artifici 
systems. but from the moment you have a machin that 
reli on machin learn to form it behavior, the past 
experi that the machin have have from the person 
who train it, or past datasets, will explicitli chang how 
the machin will behav in the future,” say pols. 

nemitz, too, tackl the master-slav question in relat 
to human and AI systems. “I would say the simpl rule 
here be that a human be can never becom an object. 
the human be be a subject, which have it own right that 
we have to respect. I think in europ it would probabl 
be imposs to have a worker who have a sensor on hi 
belt that see how much the worker be moving. and if the 
worker be not move enough, and therefor not work 
enough, the machin algorithm automat decid 
you’r fired. thi type of model in europ would be 

� 



ieee AI & ethic summit 2016 - Ar t i f ic ia l intel l igenc & ethic – who doe the thinking? 21 

illeg and, I would think, everywher that valu 
human rights.” 

“but I think the principl must be that it’ human 
autonomi which be the precondit for human 
respons and onli if we maintain thi autonomi 
that it be that the human be decid and not the 
machine,” he said. although technolog move 
much faster than legislation, nemitz say thi isn’t 
necessarili a bad thing — we don’t want human 
right or democraci principl to chang a fast 
a technolog becaus we need some stabl 
constants. 

“we have now a window of 

opportun to build the right 

valu in today’ technology” 

 



22 final thought — john C. haven 

final thought — john C. haven 

how will machin know what we valu if we don’t know 
ourselves? 

As moder for the ieee AI & ethic summit, thi be the 
question I ask audienc member to consid a we 
close the event. thi be becaus while it’ easi to assum 
other peopl (academics, scientists, manufacturers) 
be creat artifici intellig technolog we be in 
fact all contribut to it creation via our data and daili 
interactions. 

and in thi sense, the ethic question that can sometim 
seem esoter becom veri pragmat – how will you 
want your autonom car to respond when it confront 
a small child in the middl of the road? should the vehicl 
kill the child to spare you a the driver if the accid 
won’t be your fault? 

whatev your specif answer, what be clear from the 
ieee AI & ethic summit be we must take the discuss 
around these issu into a realm where we be think 
of solut to potenti issu befor they happen. for 
instance, we cannot program algorithm without ensur 
they have both transpar and a level of account 
to demonstr their lack of neg bias. and becaus 
of legisl like the gdpr we cannot allow the arbitrari 
track of person data to continu without organ 
demonstr they’r util custom inform in a 
way that acceler trust, safety, and more accur data. 

while navig ethic consider be challenging, it’ 
certainli no more difficult than evolv machin that 
may someday attain human level sentience. and while 
it may be inappropri to mandat a particular ethic 
viewpoint for individuals, measur and build to their 
self-identifi valu be a must in our algorithm era. In 
thi regard, “ethic be the new green” and the compani 
that identifi and provabl align their product and servic 
with end use valu will demonstr their dedic to 
honor people’ ethic choic for themselves, their 
families, and their communities. 

So how will machin know what we valu if we 
don’t know ourselves? they won’t. unless we tell 
them, together.  



ieee AI & ethic summit - 2016 Ar t i f ic ia l intel l igenc & ethic – who doe the thinking? 23 

there’ no such thing a scienc fiction. 
just stuff we haven’t make yet. 

from 20,000 foot under the sea to the dark side of the moon, and everywher in between, 

you’ll find the work of ieee members. it’ in a thousand thing our founder could onli 

dream of in 1884. 

In fact, ieee member have take the stuff of scienc fiction and make it part of everi day 

life. that’ why, whether you need to draw on the knowledg of technology’ past pioneer or 

today’ innovators, one fact be clear—you’l see that ieee member aren’t just wait for the 

future, they’r engin it—and the best chapter be yet to come. 

ieee: foster technolog innov for the benefit of humanity. 

www.ieee.org 



24 

oper center: 
445 hoe lane 
piscataway, NJ 08854 usa 
phone: +1 732 981 0060 
fax: +1 732 981 966 
www.ieee.org 

global offices: 
bangalore, india 
beijing, china 
lo alamitos, ca, usa 
new york, ny, usa 
solaris, singapor 
tokyo, japan 
vienna, austria (open soon) 
washington, dc, usa 

disclaimers: 
i. the thought share be those of the individu speaker 

and do not necessarili repres the opinion of ieee. the 
present and session should not be consid legal 
or busi advic or recommend 

ii. ieee prohibit discrimination, harass and bullying. for 
more inform visit www.ieee.org/nondiscrimin 


