


















































accur intellig model with pairwis interact 

yin lou 
dept. of comput scienc 

cornel univers 
yinlou@cs.cornell.edu 

rich caruana 
microsoft research 

microsoft corpor 
rcaruana@microsoft.com 

johann gehrk 
dept. of comput scienc 

cornel univers 
johannes@cs.cornell.edu 

gile hooker 
dept. of statist scienc 

cornel univers 
giles.hooker@cornell.edu 

abstract 
standard gener addit model (gams) usual model 
the depend variabl a a sum of univari models. al- 
though previou studi have show that standard gam 
can be interpret by users, their accuraci be significantli 
less than more complex model that permit interactions. 

In thi paper, we suggest add select term of inter- 
act pair of featur to standard gams. the result 
models, which we call ga2m-models, for gener addi- 
tive model plu interactions, consist of univari term and 
a small number of pairwis interact terms. sinc these 
model onli includ one- and two-dimension components, 
the compon of ga2m-model can be visual and in- 
terpret by users. To explor the huge (quadratic) number 
of pair of features, we develop a novel, comput ef- 
ficient method call fast for rank all possibl pair of 
featur a candid for inclus into the model. 

In a large-scal empir study, we show the effect 
of fast in rank candid pair of features. In addition, 
we show the surpris result that ga2m-model have al- 
most the same perform a the best full-complex mod- 
el on a number of real datasets. thu thi paper postul 
that for mani problems, ga2m-model can yield model 
that be both intellig and accurate. 

categori and subject descriptor 
i.2.6 [comput methodologies]: learning—induct 

keyword 
classification, regression, interact detect 

1. introduct 
mani machin learn techniqu such a boost or 

bag trees, svm with rbf kernels, or deep neural net 

permiss to make digit or hard copi of all or part of thi work for person or 
classroom use be grant without fee provid that copi be not make or distribut 
for profit or commerci advantag and that copi bear thi notic and the full citat 
on the first page. copyright for compon of thi work own by other than the 
author(s) must be honored. abstract with credit be permitted. To copi otherwise, or 
republish, to post on server or to redistribut to lists, requir prior specif permiss 
and/or a fee. request permiss from permissions@acm.org. 
kdd’13, august 11–14, 2013, chicago, illinois, usa. 
copyright be held by the owner/author(s). public right licens to acm. 
acm 978-1-4503-2174-7/13/08 ...$15.00. 

be power classif and regress model for high- 
dimension predict problems. however, due to their 
complexity, the result model be hard to interpret for 
the user. but in mani applications, intellig be a im- 
portant a accuraci [19], and thu build model that user 
can understand be a crucial requirement. 

gener addit model (gams) be the gold stan- 
dard for intellig when onli univari term be con- 
sider [13, 19]. standard gam have the form 

g(e[y]) = 
∑ 

fi(xi), (1) 

where g be the link function. standard gam be easi to 
interpret sinc user can visual the relationship between 
the univari term of the gam and the depend vari- 
abl through a plot fi(xi) vs. xi. howev there be unfor- 
tunat a signific gap between the perform of the 
best standard gam and full complex model [19]. In 
particular, equat 1 do not model ani interact be- 
tween features, and it be thi limit that lie at the core 
of the lack of accuraci of standard gam a compar to 
full complex models. 

exampl 1. consid the function F (x) = log(x21x3) + 
x2x3. F have a pairwis interact (x2, x3), but no in- 
teract between (x1, x2) or (x1, x3), sinc log(x 

2 
1x3) = 

2 log(x1) + log(x3), which be additive. 

our first contribut in thi paper be to build model 
that be more power than gams, but be still intelligible. 
We observ that two-dimension interact can still be 
render a heatmap of fij(xi, xj) on the two-dimension 
xi, xj-plane, and thu a model that includ onli one- and 
two-dimension compon be still intelligible. therefor 
in thi paper, we propos build model of the form 

g(e[y]) = 
∑ 

fi(xi) + 
∑ 

fij(xi, xj); (2) 

we call the result model class gener addit model 
plu interactions, or short ga2ms. 

the main challeng in build ga2m be the larg num- 
ber of pair of featur to consider. We thu onli want to 
includ “true” interact that pa some statist test. 
To thi end, we focu on problem with up to thousand of 
featur sinc for truli high dimension problem (e.g., mil- 
lion of features), it be almost intract to test all possibl 
pairwis interact (e.g., trillion of featur pairs). 

exist approach for detect statist interact 
can be divid into two classes. one class of method di- 



rectli model and compar the interact effect and ad- 
ditiv effect [10, 11, 18, 25]. one drawback of these meth- 
od be that spuriou interact may be report over low- 
densiti region [15]. the second class of method measur 
the perform drop in the model if certain interact be 
not included; they compar the perform between re- 
strict and unrestrict models, where restrict model 
be not allow to model an interact in question [22]. 
although thi class of method do not suffer from the 
problem of low-dens regions, they be comput 
extrem expens even for pairwis interact detection. 

our second contribut in thi paper be to scale the con- 
struction of ga2m by propos a novel, extrem effici 
method call fast to measur and rank the strength of the 
interact of all pair of variables. our experi show 
that fast can effici rank all pairwis interact close 
to a ground truth ranking. 

our third contribut be an extens empir evalua- 
tion of ga2m-models. surprisingly, on mani of the dataset 
includ in our study, the perform of ga2m-model be 
close and sometim good than the perform of full- 
complex models. these result indic that ga2m-model 
not onli make a signific step in improv accuraci over 
standard gams, but in some case they actual come all the 
way to the perform of full-complex models. the per- 
formanc may be due to the difficulti of estim intrin- 
sical high dimension function from limit data, sug- 
gest that the bia associ with the ga2m structur be 
outweigh by a drop in variance. We also demonstr that 
the result model be intellig through a case study. 

In thi paper we make the follow contributions: 

• We introduc the model class ga2m. 

• We introduc our new method fast for effici in- 
teract detection. (section 4) 

• We show through an extens experiment evalua- 
tion that (1) ga2m have accuraci compar to full- 
complex models; (2) fast accur rank inter- 
action a compar to a gold standard; and (3) fast 
be comput efficient. (section 5) 

We start with a problem definit and a survey of relat 
work in section 2 and 3. 

2. problem definit 
let D = {(xi, yi)}n1 denot a dataset of size N , where 

xi = (xi1, ..., xin) be a featur vector with n featur and yi 
be the response. let x = (x1, ..., xn) denot the variabl or 
featur in the dataset. for u ⊆ {1, ..., n}, we denot by xu 
the subset of variabl whose index be in u. similarli x−u 
will indic the variabl with index not in u. To simplifi 
notation, we denot U1 = {{i}|1 ≤ i ≤ n}, U2 = {{i, j}|1 ≤ 
i < j ≤ n}, and U = U1 ∪ u2, i.e., U contain all index for 
all featur and pair of features. 

for ani u ∈ U , lethu denot the hilbert space of lebesgu 
measur function fu(xu), such that e[fu] = 0 and e[f 

2 
u] 

< ∞, equip with the inner product 〈fu, f ′u〉 = e[fuf ′u]. 
let H1 = 

∑ 
u∈u1 Hu denot the hilbert space of func- 

tion that have addit form F (x) = 
∑ 

u∈u1 fu(xu) on 
univari compnents; we call those compon shape func- 
tion [19]. similarli let H = 

∑ 
u∈u Hu denot the hilbert 

space of function of x = (x1, ..., xn) that have addit form 

F (x) = 
∑ 

u∈u fu(xu) on both one- and two-dimension 
shape functions. model describ by sum of low-ord 
compon be call gener addit model (gams), 
and in the remaind of the paper, we use gam to denot 
model that onli consist of univari terms. 

We want to find the best model F ∈ H that minim 
the follow object function: 

min 
f∈h 

e[l(y, F (x))], (3) 

where l(·, ·) be a non-neg convex loss function. when L 
be the squar loss, our problem becom a regress prob- 
lem, and if L be logist loss function, we be deal with a 
classif problem. 

3. exist approach 

3.1 fit gener addit model 
term in gam can be repres by a varieti of func- 

tions, includ spline [24], regress trees, or tree ensem- 
ble [9]. there be two popular method of fit gams: 
backfit [13] and gradient boost [10]. when the shape 
function be spline, fit gam reduc to fit general- 
ize linear model with differ bases, which can be solv 
by least squar or iter reweight least squar [25]. 

spline-bas method becom ineffici when model 
high order interact becaus the number of parame- 
ter to estim grow exponentially; tree-bas method 
be more suitabl in thi case. standard addit model- 
ing onli involv model individu featur (also call 
featur shaping). previou research show that gradient 
boost with ensembl of shallow regress tree be the 
most accur method among a number of altern [19]. 

3.2 interact detect 
In thi section, we briefli review exist approach to 

interact detection. 
anova. An addit model be fit with all pairwis inter- 

action term [13] and the signific of interact term be 
measur through an analysi of varianc (anova) test [25]. 
the correspond p-valu for each pair can then be com- 
puted; however, thi requir the comput of the full 
model, which be prohibit expensive. 

partial depend function. friedman and popescu 
propos the follow statist to measur the strength of 
pairwis interactions, 

h2ij = 

∑N 
k=1[f̂ij(xki, xkj)− f̂i(xki)− f̂j(xkj))] 

2∑n 
k=1 F̂ 

2 
ij(xki, xkj) 

(4) 

where f̂u(xu) = ex−u [F (xu, x−u)] be the partial depend 
function (pdf) [10, 11] and F be a complex multi-dimension 

function learn on the dataset. comput f̂u(xu) on the 
whole dataset be expensive, thu one often specifi a subset 
of size m on which to comput f̂u(xu). the complex be 
then o(m2). however, sinc partial depend function 
be comput base on uniform sampling, they may detect 
spuriou interact over low-dens region [15]. 

guide. guid test pairwis interact base on the 
χ2 test [18]. An addit model F be fit in H1 and residu 
be obtained. To detect interact for (xi, xj), guid 
divid the (xi, xj)-space into four quadrant by split the 
rang of each variabl into two half at the sampl median. 



then guid construct a 2×4 conting tabl use the 
residu sign a row and the quadrant a columns. the 
cell valu in the tabl be the number of “+” and “-” in 
each quadrant. these count permit the comput of a 
p-valu to measur the interact strength of a pair. while 
thi might be more robust to outliers, in practic it be less 
power than the method we propose. 

grove. sorokina et al. propos a grove-bas method to 
detect statist interact [22]. To measur the strength 
of a pair (xi, xj), they build both the restrict model rij(x) 
and unrestrict model F (x), where rij(x) be prevent 
from model an interact (xi, xj): 

rij(x) =f\i(x1, ..., xi−1, xi+1, ..., xn) 

+ f\j(x1, ..., xj−1, xj+1, ..., xn). (5) 

To correctli estim interact strength, such method re- 
quir a model to be highli predict when certain interac- 
tion be not allow to appear, and therefor mani learn 
algorithm be not applic (e.g., bag decis trees). 
To thi end, they choos to use addit grove [21]. 

they measur the perform a standard root mean 
squar error (rmse) and quantifi the interact strength 
iij by the differ between rij(x) and F (x), 

strmse(f (x)) = 
rmse(f (x)) 

std(f ∗(x)) 
(6) 

iij = strmse(rij(x))− strmse(f (x)) (7) 

where std(f ∗(x)) be calcul a standard deviat of the 
respons valu in the train set. the rank of all pair 
can be gener base on the strength iij . 

To handl correl among features, they use a vari- 
ant of backward elimin [12] to do featur selection. 
although grove be accur in practice, build restrict 
and unrestrict model be comput expens and 
therefor thi method be almost infeas for larg high di- 
mension datasets. 

4. our approach 
for simplic and without loss of generality, we focu 

in thi exposit on regress problems. sinc there be 
o(n2) pairwis interactions, it be veri hard to detect pair- 
wise interact when n be large. therefor we propos a 
framework use greedi forward stagewis select strategi 
to build the most accur model in H. 

algorithm 1 summar our approach call ga2m. We 
maintain two set S and Z, where S contain the select 
pair so far and Z be the set of the remain pair (line 1- 
2). We start with the best addit model F so far in hilbert 
space h1+ 

∑ 
u∈ Hu (line 4) and detect interact on the 

residu R (line 5). then for each pair in Z, we build an 
interact model on the residu R (line 6-7). We select 
the best interact pair and includ it in S (line 9-10). We 
then repeat thi process until there be no gain in accuracy. 

note that algorithm 1 will find an overcomplet set S by 
the greedi natur of the forward select strategy. when 
featur be correlated, it be also possibl that the algorithm 
includ fals pairs. for example, consid the function in 
exampl 1. If x1 be highli correl with x3, then (x1, x2) 
may look like an interact pair, and it may be includ in 
S befor we select (x2, x3). but sinc we will refit the model 
everi time we includ a new pair, it be expect that F will 

algorithm 1 ga2m framework 

1: S ← ∅ 
2: Z ← U2 
3: while not converg do 
4: F ← arg minf∈h1+∑u∈ Hu 12e[(i − F (x))2] 
5: R← y − F (x) 
6: for all u ∈ Z do 
7: Fu ← e[r|xu] 
8: u∗ ← arg minu∈z 12e[(r− fu(xu)) 

2] 
9: S ← S ∪ {u∗} 

10: Z ← Z − {u∗} 

xi 

xj 

cj 

ci ci 

cj cj 

figur 1: illustr for search cut on input 
space of xi and xj. On the left we show a heat map 
on the target for differ valu of xi and xj. ci and 
cj be cut for xi and xj, respectively. On the right 
we show an extrem simpl predictor of model 
pairwis interaction. 

perfectli model (x2, x3) and therefor (x1, x2) will becom 
a less import term in F . 

for larg high-dimension datasets, however, algorithm 
1 be veri expens for two reasons. first, fit interact 
model for o(n2) pair in Z can be veri expens if the 
model be non-trivial. second, everi time we add a pair, we 
need to refit the whole model, which be also veri expens 
for larg datasets. As we will see in section 4.1 and sec- 
tion 4.2, we will relax some of the constraint in algorithm 1 
to achiev good scalabl while still stay accurate. 

4.1 fast interact detect 
consid the conceptu addit model in equat 2, 

give a pair of variabl (xi, xj) we wish to measur how 
much benefit we can get if we model fij(xi, xj) instead of 
fi(xi) + fj(xj). sinc we start with shape individu fea- 
ture and alway detect interact on the residual, fi(xi)+ 
fj(xj) be presum model and therefor we onli need 
to look at the residu sum of squar (rss) for the inter- 
action model fij . the intuit be that when (xi, xj) be a 
strong interaction, model fij can significantli reduc the 
rss. however, we do not wish to fulli build fij sinc thi 
be a veri expens operation; instead we be look for a 
cheap substitute. 

4.1.1 overview 
our idea be to build an extrem simpl model for fij 

use cut on the input space of xi and xj , a illustr 
in figur 1. the simplest model we can build be to place 
one cut on each variable, i.e., we place one ci and one cut 



xi 

xj 

chtj(cj) CH 
t 
j(cj) 

C 
H 

ti (c 
i ) 

C 
H 

ti (c 
i ) 

a b 

c d 

a = pre-comput 

b = chti (ci)− a 

c = chtj(cj)− a 

d = chti (ci)− c 

figur 2: illustr for comput sum of target 
for each quadrant. given that the valu of red quad- 
rant be known, we can easili recov valu in other 
quadrant use margin cumul histograms. 

cj on xi and xj , respectively. those cut be parallel to 
the axes. the interact predictor tij be construct by 
take the mean of all point in each quadrant. We search 
for all possibl (ci, cj) and pick the best tij with the low 
rss, which be assign a weight for (xi, xj) to measur the 
strength of interaction. 

4.1.2 construct predictor 
näıv implement of fast be straightforward, but 

careless implement have veri high complex sinc we 
need to repeatedli build a lot of tij for differ cuts. the 
key insight for faster version of fast be that we do not 
need to scan through the dataset each time to comput tij 
and comput it rss. We show that by use veri sim- 
ple bookkeep data structures, we can greatli reduc the 
complexity. 

let dom(xi) = {v1i , ..., vdii } be a sort set of possibl 
valu for variabl xi, where di = |dom(xi)|. defin hti (v) 
a the sum of target when xi = v, and defin H 

w 
i (v) a 

the sum of weight (or counts) when xi = v. intuitively, 
these be the standard histogram when construct re- 
gression trees. similarly, we defin chti (v) and CH 

w 
i (v) 

a the cumul histogram for sum of target and sum 
of weights, respectively, i.e., chti (v) = 

∑ 
u≤vh 

t 
i (u) and 

chwi (v) = 
∑ 

u≤vh 
w 
i (u). accordingly, defin CH 

t 
i (v) =∑ 

u>vh 
t 
i (u) = CH 

t 
i (v 

di 
i ) − CH 

t 
i (v) and defin CH 

w 
i (v) =∑ 

u>vh 
w 
i (u) = CH 

w 
i (v 

di 
i ) − CH 

w 
i (v). furthermore, defin 

htij(u, v) and H 
w 
ij(u, v) a the sum of target and the sum 

of weights, respectively, when (xi, xj) = (u, v). 
consid again the input space for (xi, xj), we need a 

quick way to comput the sum of target and sum of weight 
for each quadrant. figur 2 show an exampl for comput 
sum of target on each quadrant. given the abov notations, 
we alreadi know the margin cumul histogram for xi 
and xj , but unfortun use these margin valu onli 
can not recov valu on four quadrants. thus, we have to 
comput valu for one quadrant. 

We show that it be veri easi and effici to comput all 
possibl valu for the red quadrant give ani cut (ci, cj) 
use dynam programming. onc that quadrant be known, 
we can easili recov valu in other quadrant use margin 
cumul histograms. We store those valu into lookup 
tables. let lt(ci, cj) = [a, b, c, d] be the lookup tabl for sum 

algorithm 2 constructlookupt 

1: sum← 0 
2: for q = 1 to dj do 
3: sum← sum+htij(v1i , vqj ) 
4: a[1][q]← sum 
5: l(v1i , v 

q 
j )← computev alues(ch 

t 
i , CH 

t 
j , a[1][q]) 

6: for p = 2 to di do 
7: sum← 0 
8: for q = 1 to dj do 
9: sum← sum+htij(vpi , v 

q 
j ) 

10: a[p][q]← sum+ a[p− 1][q] 
11: l(vpi , v 

q 
j )← computev alues(ch 

t 
i , CH 

t 
j , a[p][q]) 

of target on cut (ci, cj), and denot L 
w(ci, cj) = [a, b, c, d] 

a the lookup tabl for sum of weight on cut (ci, cj). 
algorithm 2 describ how to comput the lookup tabl 

lt. We focu on comput quadrant a and other quad- 
rant can be easili computed, which be handl by subrou- 
tine computev alues. given htij , we first comput a for 
the first row of Lt (line 3-5). let a[p][q] denot the valu 
for cut (p, q). note a[p][q] = a[p− 1][q] + 

∑ 
k≤qh 

t 
ij(v 

p 
i , v 

k 
j ). 

thu we can effici comput the rest of the lookup tabl 
row by row (line 6-11). 

onc we have Lt and lw, give ani cut (ci, cj), we can 
easili construct tij . for example, we can set the leftmost 
leaf valu in tij a L 

t(ci, cj).a/l 
w(ci, cj).a. It be easi to see 

that with those bookkeep data structures, we can reduc 
the complex of build predictor to o(1). 

4.1.3 calcul rss 
In thi section, we show that calcul rss for tij 

can be veri efficient. consid the definit of rss. let 
tij .r denot the predict valu on region r, where r ∈ 
{a, b, c, d}. 

rss = 

N∑ 
k=1 

(yk − tij(xk))2 (8) 

= 

( 
N∑ 

k=1 

y2k − 2 
∑ 
r 

tij .rl 
t.r + 

∑ 
r 

(tij .r) 
2lw.r 

) 
(9) 

In practic implementation, we onli need to care about∑ 
r(tij .r) 

2lw.r−2 
∑ 

r tij .rl 
t.r sinc we be onli interest 

in rel order of rss, and it be easi to see the com- 
plexiti of comput rss for tij be o(1). 

4.1.4 complex analysi 
for each pair (xi, xj), comput the histogram and cu- 

mul histogram need to scan through the data and 
therefor it complex be o(n). construct the lookup 
tabl take o(didj + N) time. thus, the time complex 
of fast be o(didj +n) for one pair (xi, xj). besides, sinc 
we need to store di-by-dj matrix for each pair, the space 
complex be o(didj). 

for continu features, didj can be quit large. however, 
we can discret the featur into b equi-frequ bins. 
such featur discret usual do not hurt the perfor- 
manc of regress tree [17]. As we will see in section 5, 
fast be not sensit to a wide rang of bs. therefore, the 
complex can be reduc to o(b2 + N) per pair when we 



discret featur into b bins. for small b (b ≤ 256), we 
can quickli process each pair. 

4.2 two-stag construct 
with fast, we can quickli rank of all pair in Z, the re- 

main pair set, and add the best interact to the model. 
however, refit the whole model after each pair be add 
can be veri expens for larg high-dimension datasets. 
therefore, we propos a two-stag construct approach. 

1. In stage 1, build the best addit model F inh1 use 
onli one-dimension components. 

2. In stage 2, fix the one-dimension functions, and build 
model for pairwis interact on residuals. 

4.2.1 implement detail 
To scale up to larg dataset and mani features, we dis- 

cretiz the featur into 256 equi-frequ bin for contin- 
uou features.1 We find such featur discret rare 
hurt the perform but substanti reduc the run- 
ning time and memori footprint sinc we can use one byte 
to store a featur value. besides, discret the featur re- 
move the sort requir for continu featur when 
search for the best cut in the space. 

previou research show that featur shape use gra- 
dient boost [10] with shallow regress tree ensembl 
can achiev the best accuraci [19]. We follow similar ap- 
proach (i.e., gradient boost with shallow tree-lik ensem- 
bles) in thi work. however, a regress tree be not the ideal 
learn method for each compon for two reasons. first, 
while regress tree be good a a gener shape function 
for ani xu, shape a singl featur be equival to cut 
on a line, but line cut can be make more effici than 
regress tree. second, use regress tree to shape pair- 
wise function can be problematic. recal that in stage 1, 
we obtain the best addit model after gradient boost 
converges. thi mean add more cut to ani one featur 
do not reduc the error, and equivalently, ani cut on a sin- 
gle featur be random. therefore, when we begin to shape 
pairwis interactions, the root test in a regress tree that 
be construct greedili top-down be random. 

similar to [19], to effect shape pairwis interactions, 
we build shallow tree-lik model on the residu a illus- 
trate in figur 3. We enumer all possibl cut ci on 
xi. given thi cut, we greedili search the best cut c 

1 
j in the 

region abov ci and similarli greedili search the best cut c 
2 
j 

in the region below ci. note we can reus the lookup tabl 
Lt and Lw we develop for fast for fast search of those 
three cuts. figur 3 show an exampl of comput the leaf 
valu give ci, c 

1 
j and c 

2 
j . similarly, we can quickli comput 

the rss give ani combin of 3 cut onc the leaf valu 
be available, just a we do in section 4.1.4, and therefor 
it be veri fast to search for the best combin of cut in 
thi space. similarly, we search for the best combin of 
3 cut with 1 cut on xj and 2 cut on xi and pick the good 
model with low rss. It be easi to see the complex be 
o(n + b2), where b be the number of bin for each featur 
and b = 256 in our case. 

1note that thi be not the number of bin use in fast, 
the interact detect process. here we use 256 bin for 
feature/pair shaping. 

xi 

xj 

c1j 

c2j 

ci 
a b 

c d 

a = lt(ci, c 
1 
j ).a/l 

w(ci, c 
1 
j ).a 

b = lt(ci, c 
1 
j ).b/l 

w(ci, c 
1 
j ).b 

c = lt(ci, c 
2 
j ).c/l 

w(ci, c 
2 
j ).c 

d = lt(ci, c 
2 
j ).d/l 

w(ci, c 
2 
j ).d 

figur 3: illustr for comput shape function 
for pairwis interaction. 

dataset size attribut %po 

delta 7192 6 - 
compact 8192 22 - 

pole 15000 49 - 
calhous 20640 9 - 
mslr10k 1200192 137 - 
spambas 4601 58 39.40 

gisett 6000 5001 50.00 
magic 19020 11 64.84 
letter 20000 17 49.70 

physic 50000 79 49.72 

tabl 1: datasets. 

4.2.2 further relax 
for larg datasets, even refit the model on select 

pair can be veri expensive. therefore, we propos to use 
the rank of fast right after stage 1, to select the top-k 
pair to S, and fit a model use the pair in S on the residu 
R, where K be chosen accord to comput power. 

4.2.3 diagnost 
model that combin both accuraci and intellig be 

important. usual S will still be an overcomplet set. for 
intelligibility, onc we have learn the best model in H, 
we would like to rank all term (one- and two-dimension 
components) so that we can focu on the most import 
features, or pairwis interactions. therefore, we need to 
assign weight for each term. We use 

√ 
e[f2u], the standard 

deviat of fu (sinc e[fu] = 0), a the weight for term 
u. note thi be a natur gener of the weight in 
the linear models; thi be easi to see sinc fi(xi) = wixi,√ 
e[f2i ] be equival to |wi| if featur be normal so 

that e[x2i ] = 1. 

5. experi 
In thi section we report experiment result on both syn- 

thetic and real datasets. the result in section 5.1 show 
ga2m learn model that be nearli a accur a full- 
complex random forest model while use term that de- 
pend onli on singl featur and pairwis interact and 
thu be intelligible. the result in section 5.2 demonstr 
that fast find the most import interact of o(n2) 
featur pair to includ in the model. section 5.3 compar 
the comput cost of fast and ga2m to compet 
methods. section 5.4 briefli discu sever import de- 



model delta compact pole calhous mslr10k mean 

linear regress 0.58±0.01 7.92±0.47 30.41±0.24 7.28±0.80 0.76±0.00 1.52±0.79 
gam 0.57±0.02 2.74±0.04 21.62±0.38 5.76±0.55 0.75±0.00 1.00±0.00 

ga2m rand - - 11.37±0.38 - 0.73±0.00 - 
ga2m coef - - 11.61±0.43 - 0.73±0.00 - 

ga2m order - - 10.81±0.29 - 0.74±0.00 - 
ga2m fast 0.55±0.02 2.53±0.02 10.59±0.35 5.00±0.91 0.73±0.00 0.84±0.20 

random forest 0.53±0.19 2.45±0.08 11.38±1.03 4.90±0.81 0.71±0.00 0.83±0.17 

tabl 2: rmse for regress datasets. each cell contain the mean rmse ± one standard deviation. 
averag normal score be show in the last column, calcul a rel improv over gam. 

model spambas gisett magic letter physic mean 

logist regress 6.22±0.93 15.78±3.28 17.11±0.08 27.54±0.27 30.02±0.37 1.79±1.25 
gam 5.09±0.64 3.95±0.65 14.85±0.28 17.84±0.20 28.83±0.24 1.00±0.00 

ga2m rand 5.04±0.52 3.53±0.61 - - 28.82±0.25 - 
ga2m coef 4.89±0.54 3.43±0.55 - - 28.74±0.37 - 

ga2m order 4.93±0.65 3.08±0.55 - - 28.76±0.34 - 
ga2m fast 4.78±0.70 2.91±0.38 13.88±0.32 8.62±0.31 28.20±0.18 0.81±0.21 

random forest 4.76±0.70 3.25±0.47 12.45±0.64 6.16±0.22 28.48±0.40 0.79±0.26 

tabl 3: error rate for classif datasets. each cell contain the error rate ± one standard deviation. 
averag normal score be show in the last column, calcul a rel improv over gam. 

sign choic make for fast and ga2m. finally, section 5.5 
conclud with a case study. 

5.1 model accuraci on real dataset 
We run experi on ten real dataset to show the accu- 

raci that ga2m can achiev with model that depend onli 
on 1-d featur and pairwis featur interactions. 

5.1.1 dataset 
tabl 1 summar the 10 datasets. five be regress 

problems: “delta” be the task of control the aileron of 
an f16 aircraft [1]. “compact” be from the delv repositori 
and describ the state of multius comput [2]. “pole” 
describ a telecommun problem [23]. “calhousing” 
describ how hous price depend on censu variabl [16]. 
“mslr10k” be a learning-to-rank dataset but we treat rele- 
vanc a regress target [3]. the other five dataset be bi- 
nari classif problems: the “spambase”, “magic” and 
“letter” dataset be from the uci repositori [4]. “gisette” 
be from the nip featur select challeng [5]. “physics” be 
from the kdd cup 2004 [6]. 

the featur in all dataset be discret into 256 equi- 
frequenc bins. for each model we includ at most 1000 
featur pairs; we includ all featur pair in the six problem 
with least dimension, and the top 1000 featur pair found 
by fast on the “pole”, “mslr10k”, “spambase”, “gisette”, 
and “physics” datasets. although it be possibl that high 
accuraci might be obtain by includ more or few fea- 
ture pairs, search for the optim number of pair be expen- 
sive and ga2m be reason robust to excess featur pairs. 
however, it be too expens to includ all featur pair on 
problem with mani features. We use 8 bin for fast in 
all experiments. 

5.1.2 result 
We compar ga2m to linear/logist regression, featur 

shape (gams) without interactions, and full-complex 

random forests. for regress problem we report root 
mean squar error (rmse) and for classif problem 
we report 0/1 loss. To compar result across differ datasets, 
we normal result by the error of gam on each dataset. 
for all experiments, we train on 80% of the data and hold 
asid 20% of the data a test sets. 

In addit to fast, we also consid three baselin meth- 
od on five high dimension datasets, i.e., ga2m rand, 
ga2m coef and ga2m order. ga2m rand mean we add 
same number of random pair to gam. ga2m order and 
ga2m coef use the weight of 1-d featur in gam to pro- 
pose pairs; ga2m order gener pair by the order of 1-d 
featur and ga2m coef gener pair by the product of 
weight of 1-d features. 

the regress and classif result be present in 
tabl 2 and tabl 3. As expected, the improv over 
linear model from shape individu featur (gams) be 
substantial: on averag featur shape reduc rmse 34% 
on the regress problems, and reduc 0/1 loss 44% on 
the classif problems. what be surprising, however, be 
that by add shape pairwis interact to the models, 
ga2m fast substanti close the accuraci gap between 
unintellig full-complex model such a random forest 
and gams. On some datasets, ga2m fast even outper- 
form the best random forest model. also, none of the base- 
line method perform compar ga2m fast. 

5.2 detect featur interact with fast 
In thi section we evalu how accur fast detect 

featur interact on synthet problems. 

5.2.1 sensit to the number of bin 
To evalu sensit of fast we use the synthet func- 

tion gener in [10] to gener random functions. becaus 
these be synthet function, we know the ground truth in- 
teract pair and use averag precis (area under the 
precision-recal curv evalu at true points) a the eval- 



0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

1.1 

2 4 8 16 32 64 128 256 

Av 
er 

ag 
e 

Pr 
ec 

be 
io 

n 

number of bin 

10^2 10^3 10^4 10^5 10^6 

(a) 10 features. 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

2 4 8 16 32 64 128 256 

Av 
er 

ag 
e 

Pr 
ec 

be 
io 

n 

number of bin 

10^2 10^3 10^4 10^5 10^6 

(b) 100 features. 

figur 4: sensit of fast to the number of bins. 

0.75 

0.8 

0.85 

0.9 

0.95 

1 

grove 
anova 

fast 
guid 

pdf 100 

pdf 200 

pdf 400 

pdf 800 

averag precis 

(a) 

10 

100 

1000 

10000 

100000 

1e+06 

grove 
anova 

pdf 800 

pdf 400 

pdf 200 

pdf 100 

guid 

fast 

time (s) 

(b) 

figur 5: precision/cost on synthet function. 

uation metric. We vari b = 2, 4, ..., 256 and the dataset 
size N = 102, 103, ..., 106. for each fix N , we gener 
dataset with n featur and k high order interact xu, 
where |u| = b1.5 + rc and r be drawn from an exponenti 
distribut with mean λ = 1. We experi with two 
cases: 10 featur with 25 high order interact and 100 
featur with 1000 high order interactions. 

figur 4 show the mean averag precis and varianc 
for 100 trial at each setting. As expected, averag pre- 
cision increas a dataset size increases, and decreas a 
the number of featur increas from 10 (left graph) to 100 
(right graph). when there be onli 10 featur and a mani 
a 106 samples, fast rank all true interact abov all 
non-interact pair (averag precis = 1) in most cases, 
but a the sampl size decreas or the problem difficulti 
increas averag precis drop below 1. In the graph on 
the right with 100 featur there be 4950 featur pairs, and 
fast need larg sampl size (106 or greater) to achiev av- 
erag precis abov 0.7, and a expect perform poorli 
when there be few sampl than pair of features. 

On these test problem the optim number of bin ap- 
pear to be about b = 8, with averag precis fall 
slightli for number of bin larg and small than 8. thi be 
a classic bias-vari tradeoff: small b reduc the chanc 
of overfit but at the risk of fail to model some kind 
of interactions, while larg b allow more complex interac- 
tion to be model but at the risk of allow some fals 
interact to be confus with weak true interactions. 

5.2.2 accuraci 
the previou section show that fast accur de- 

tect featur interact when the number of sampl be 
much larg than the number of featur pairs, but that ac- 
curaci drop a the number of featur pair grow compa- 
rabl to and then larg than the number of samples. In 
thi section we compar the accuraci of fast to the in- 
teract detect method discuss in section 3.2. for 
anova, we use R packag mgcv to comput p-valu un- 
der a wald test [25]. for pdf, we use rulefit packag and 
we choos m = 100, 200, 400, 800, where m be the sampl size 
that trade off effici and accuraci [7]. grove be avail 
in treeextra packag [8]. 

here we conduct experi on synthet data gener 
by the follow function [14, 22]. 

F (x) = πx1x2 
√ 

2x3 − sin−1(x4) + log(x3 + x5)− 
x9 
x10 

√ 
x7 
x8 
− x2x7 (10) 

variabl x4, x5, x8, x10 be uniformli distribut in [0.6, 1] 
and the other variabl be uniformli distribut in [0, 1]. 

We gener 10, 000 point for these experiments. figur 5(a) 
show the averag precis of the methods. On thi prob- 
lem, the grove and anova method be accur and rank 
all 11 true pair in the top of the list. fast be almost a 
good and correctli rank the top ten pairs. the other meth- 
od be significantli less accur than grove, anova, and 
fast. 

To understand whi fast do not pick up the 11th pair, 
we plot heat map of the residu of select pair in fig- 
ure 6. (x1, x2) and (x2, x7) be two of the correctli rank 
true pairs, (x1, x7) be a fals pair rank below the true pair 
fast detect correctli but abov the true pair it misses, and 
(x8, x10) be the true pair fast miss and rank below thi 
fals pair. the heat map show strong interact be easi 
to distinguish, but some fals interact such a (x1, x7) 
can have signal a strong a that of weak true interact 
such a (x8, x10). In fact, sorokina et al. found that x8 
be a weak feature, and do not consid pair that use x8 a 
interact on 5, 000 sampl [22], so we be near the thresh- 
old of detect of (x8, x10) go from 5, 000 to 10, 000 
samples. 

5.2.3 featur correl and spuriou pair 
If featur be correlated, spuriou interact may be 

detect becaus it be difficult to tell the differ between 
a true interact between x1 and x2 and the spuriou in- 
teract between x1 and x3 when x3 be strongli correl 
with x2; ani interact detect method such a fast 
that examin pair in isol will have thi problem. with 
ga2m, however, it be fine to includ some fals posit pair 
becaus ga2m be abl to post-filt fals posit pair by 
look at the term weight of shape interact in the 
final model. 

To demonstr this, we use the synthet function in 
equat 10, but make x6 correl to x1. We gener 
2 datasets, one with ρ(x1, x6) = 0.5 and the other with 
ρ(x1, x6) = 0.95, where ρ be the correl coefficient. We 
run fast on residu after featur shaping. We give the 
top 20 pair found by fast to ga2m, which then us gra- 
dient boost to shape those pairwis interactions. figur 7 
illustr how the weight of select pairwis interact 
evolv after each step of gradient boosting. although the 
pair (x2, x6) can be incorrectli introduc by fast becaus 
of the high correl between x1 and x6, the weight on thi 
fals pair decreas quickli a boost proceeds, indic 
that thi pair be spurious. thi not onli allow the model 
train on the pair to remain accur in the face of spu- 
riou pairs, but also reduc the weight (and ranking) give 
to thi shape term so that intellig be not be hurt by 
the spuriou term. 



0 

5 

10 

15 

20 

25 

30 

0 5 10 15 20 25 30 

x2 

x1 

"hm/0.1.txt" u 1:2:3 

-0.6 
-0.4 
-0.2 
0 
0.2 
0.4 
0.6 
0.8 

0 

5 

10 

15 

20 

25 

30 

0 5 10 15 20 25 30 

x7 

x2 

"hm/1.6.txt" u 1:2:3 

-0.6 
-0.4 
-0.2 
0 
0.2 
0.4 
0.6 
0.8 

(x1, x2) (x2, x7) 

0 

5 

10 

15 

20 

25 

30 

0 5 10 15 20 25 30 

x7 

x1 

"hm/0.6.txt" u 1:2:3 

-0.4 
-0.3 
-0.2 
-0.1 
0 
0.1 
0.2 
0.3 
0.4 

0 

5 

10 

15 

20 

25 

30 

0 5 10 15 20 25 30 

x1 
0 

x8 

"hm/7.9.txt" u 1:2:3 

-0.4 
-0.3 
-0.2 
-0.1 
0 
0.1 
0.2 
0.3 
0.4 

(x1, x7) (x8, x10) 

figur 6: true/spuri heat maps. featur be 
discret into 32 bin for visualization. 

0 
0.002 
0.004 
0.006 
0.008 

0.01 
0.012 
0.014 
0.016 
0.018 

0.02 

0 1000 2000 3500 

W 
ei 

gh 
t 

iter 

(x1, x2) 
(x2, x6) 
(x1, x3) 

(x7, x9) 
(x3, x6) 
(x2, x3) 

(x9, x10) 
(x3, x5) 
(x8, x9) 

(a) ρ(x1, x6) = 0.5 

0 
0.002 
0.004 
0.006 
0.008 

0.01 
0.012 
0.014 
0.016 

0 1000 2000 3500 

W 
ei 

gh 
t 

iter 

(x1, x2) 
(x2, x6) 
(x1, x3) 

(x7, x9) 
(x3, x6) 
(x2, x3) 

(x9, x10) 
(x3, x5) 
(x8, x9) 

(b) ρ(x1, x6) = 0.95 

figur 7: weight for pairwis interact term in 
the model. 

5.3 scalabl 
figur 5(b) illustr the run time of differ meth- 

od on 10, 000 sampl from equat 10. model build 
time be included. fast take about 10 second to rank 
all possibl pair while the two other accur methods, 
anova and grove, be 3-4 order of magnitud slower. 
grove, which be probabl the most accur interact de- 
tection method current available, take almost a week to 
run onc on thi data. thi show the advantag of fast; 
it be veri fast with high accuracy. On thi problem fast 
take less than 1 second to rank all pair and the major 
of time be devot to build the addit model. 

figur 8 show the run time of fast per pair on real 
datasets. It be clear that on real datasets, fast be both 
accur and efficient. 

5.4 design choic 
An altern to interact detect that we consid 

be to build ensembl of tree on residu after shape the 
individu featur and then look at tree statist to find 
combin of featur that co-occur in path more often 
than their independ rate warrants. By use 1-step look- 
ahead at the root we also hop to partial mitig the 
myopia of greedi featur instal to make interact 
more like to be detected. unfortunately, featur with 
high “co-occur counts” do not correl well with true 
interact on synthet test problems, and the best tree- 
base method we could devis do not detect interact 
a well a fast, and be consider more expensive. 

● 

● 

● 

● 

● 
●●● 

● 

● 

5e+03 2e+04 1e+05 5e+05 

2e 
− 

04 
5e 

− 
03 

1e 
− 

01 

size of dataset 

T 
im 

e 
(s 

) 
pe 

r 
pa 

ir 

spambas 
gissett 

delta 

compact 
pole 

magic letter 

calhous 
physic 

mslr10k 

figur 8: comput cost on real datasets. 

5.5 case study: learn to rank 
learning-to-rank be an import research topic in the 

data mining, machin learn and inform retriev com- 
munities. In thi section, we train intellig model with 
shape one-dimension featur and pairwis interact 
on the “mslr10k” dataset. A complet descript of fea- 
ture can be found in [3]. We show the top 10 most im- 
portant individu featur and their shape function in first 
two row of figur 9. the number abov each plot be the 
weight for the correspond term in the model. interest- 
ingly, we found bm25 [20], usual consid a a power 
featur for ranking, rank 70th (bm25 url) in the list af- 
ter shaping. other featur such a idf (invers document 
frequency) enjoy much high weight in the learn model. 

the last two row of figur 9 show the 10 most import 
pairwis interact and their term strengths. each of them 
show a clear interact that could not be model by addi- 
tive terms. the non-linear shape of the individu featur 
in the top plot and the pairwis interact in the bottom 
plot be intellig to expert and featur engineers, but 
would be well hidden in full-complex models. 

6. conclus 
We present a framework call ga2m for build intel- 

ligibl model with pairwis interactions. ad pairwis 
interact to tradit gam retain intelligibility, while 
substanti increas model accuracy. To scale up pair- 
wise interact detection, we propos a novel method call 
fast that effici measur the strength of all potenti 
pairwis interactions. 

acknowledgements. We thank the anonym review- 
er for their valuabl comments, and we thank nick craswel 
of microsoft bing for insight discussions. thi research 
have be support by the nsf under grant iis-0911036 
and iis-1012593. ani opinions, find and conclus or 
recommend express in thi materi be those of the 
author and do not necessarili reflect the view of the nsf. 

7. refer 
[1] http://www.liaad.up.pt/~ltorgo/regression/ 

datasets.html. 

[2] http: 
//www.cs.toronto.edu/~delve/data/datasets.html. 

[3] http: 
//research.microsoft.com/en-us/projects/mslr/. 

http://www.liaad.up.pt/~ltorgo/regression/datasets.html 
http://www.liaad.up.pt/~ltorgo/regression/datasets.html 
http://www.cs.toronto.edu/~delve/data/datasets.html 
http://www.cs.toronto.edu/~delve/data/datasets.html 
http://research.microsoft.com/en-us/projects/mslr/ 
http://research.microsoft.com/en-us/projects/mslr/ 


0.0093 0.0084 0.0061 0.0058 0.0057 

-0.2 

-0.1 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0 50 100 150 200 250 
-0.2 

0 

0.2 

0.4 

0.6 

0.8 

1 

1.2 

1.4 

0 50 100 150 200 250 
-0.2 

0 
0.2 
0.4 
0.6 
0.8 

1 
1.2 
1.4 
1.6 
1.8 

0 2 4 6 8 10 12 14 16 18 
-0.15 
-0.1 

-0.05 
0 

0.05 
0.1 

0.15 
0.2 

0.25 
0.3 

0.35 
0.4 

0 50 100 150 200 250 
-0.25 

-0.2 
-0.15 

-0.1 
-0.05 

0 
0.05 

0.1 
0.15 

0.2 
0.25 

0 50 100 150 200 250 

stream length bodi query-url click count cover queri term number sum of term frequenc bodi lair.ab bodi 
titl 

0.0055 0.0052 0.0050 0.0040 0.037 

-0.6 
-0.4 
-0.2 

0 
0.2 
0.4 
0.6 
0.8 

1 
1.2 

0 50 100 150 200 250 
-0.8 

-0.6 

-0.4 

-0.2 

0 

0.2 

0.4 

0 10 20 30 40 50 60 
-1 

-0.5 

0 

0.5 

1 

1.5 

2 

2.5 

3 

0 10 20 30 40 50 60 70 80 90 
-0.2 

-0.15 
-0.1 

-0.05 
0 

0.05 
0.1 

0.15 
0.2 

0.25 
0.3 

0 50 100 150 200 250 
-0.8 

-0.6 

-0.4 

-0.2 

0 

0.2 

0.4 

0.6 

0.8 

0 20 40 60 80 100 120 

min of term frequenc cover queri term ratio stream length url idf titl outlink number 
whole document titl 

6.6934e-4 6.6726e-4 5.5579e-4 3.4585e-4 3.0110e-4 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 
-0.3 

-0.2 

-0.1 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0 

50 

100 

150 

200 

250 

0 5 10 15 20 25 
-0.2 

-0.1 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0 

5 

10 

15 

20 

25 

0 10 20 30 40 50 60 70 80 90 
-0.2 
-0.1 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 250 
-0.6 

-0.4 

-0.2 

0 

0.2 

0.4 

0.6 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 250 
-0.15 

-0.1 

-0.05 

0 

0.05 

0.1 

0.15 

0.2 

0.25 

qualityscore2 vs. inlink number vs. number of slash in url vs. url click count vs. pagerank vs. min of stream 
siterank number of slash in url stream length url query-url click count length normal term 

frequenc whole document 

2.4755e-4 2.4104e-4 2.3218e-4 2.2952e-4 2.1643e-4 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 250 
-0.5 

-0.4 

-0.3 

-0.2 

-0.1 

0 

0.1 

0.2 

0.3 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 250 
-0.8 

-0.6 

-0.4 

-0.2 

0 

0.2 

0.4 

0 

50 

100 

150 

200 

0 20 40 60 80 100 120 
-0.15 
-0.1 
-0.05 
0 
0.05 
0.1 
0.15 
0.2 
0.25 
0.3 
0.35 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 250 
-0.2 

0 

0.2 

0.4 

0.6 

0.8 

1 

1.2 

0 

50 

100 

150 

200 

250 

0 50 100 150 200 250 
-0.6 
-0.5 
-0.4 
-0.3 
-0.2 
-0.1 
0 
0.1 
0.2 
0.3 
0.4 
0.5 

url dwell time vs. qualityscore2 vs. siterank vs. pagerank vs. query-url click count vs. 
query-url click count min of tf*idf url outlink number bm25 url length of url 

figur 9: shape of featur and pairwis interact for the “mslr10k” dataset with weights. top two 
row show top 10 strong features. next two row show top 10 strong interactions. 

[4] http://archive.ics.uci.edu/ml/. 

[5] http://www.nipsfsc.ecs.soton.ac.uk/. 

[6] http://osmot.cs.cornell.edu/kddcup/. 

[7] http: 
//www-stat.stanford.edu/~jhf/r-rulefit.html. 

[8] http://additivegroves.net. 

[9] E. bauer and R. kohavi. An empir comparison of 
vote classif algorithms: bagging, boosting, 
and variants. machin learning, 36(1):105–139, 1999. 

[10] J. friedman. greedi function approximation: a 
gradient boost machine. annal of statistics, 
29:1189–1232, 2001. 

[11] J. friedman and B. popescu. predict learn via 
rule ensembles. the annal of appli statistics, 
page 916–954, 2008. 

[12] I. guyon and A. elisseeff. An introduct to variabl 
and featur selection. the journal of machin 
learn research, 3:1157–1182, 2003. 

[13] T. hasti and R. tibshirani. gener addit 
models. chapman & hall/crc, 1990. 

[14] G. hooker. discov addit structur in black box 
functions. In kdd, 2004. 

[15] G. hooker. gener function anova diagnost 
for high-dimension function of depend variables. 
journal of comput and graphic statistics, 
16(3):709–732, 2007. 

[16] R. kelley pace and R. barry. spars spatial 

autoregressions. statist & probabl letters, 
33(3):291–297, 1997. 

[17] P. li, C. burges, and Q. wu. mcrank: learn to 
rank use multipl classif and gradient 
boosting. In nips, 2007. 

[18] W. loh. regress tree with unbias variabl 
select and interact detection. statistica sinica, 
12(2):361–386, 2002. 

[19] Y. lou, R. caruana, and J. gehrke. intellig 
model for classif and regression. In kdd, 2012. 

[20] C. D. manning, P. raghavan, and H. schütze. 
introduct to inform retrieval. cambridg 
univers press cambridge, 2008. 

[21] D. sorokina, R. caruana, and M. riedewald. addit 
grove of regress trees. In ecml, 2007. 

[22] D. sorokina, R. caruana, M. riedewald, and D. fink. 
detect statist interact with addit grove 
of trees. In icml, 2008. 

[23] S. M. weiss and N. indurkhya. rule-bas machin 
learn method for function prediction. journal of 
artifici intellig research, 3:383–403, 1995. 

[24] S. wood. thin plate regress splines. journal of the 
royal statist society: seri B (statist 
methodology), 65(1):95–114, 2003. 

[25] S. wood. gener addit models: an introduct 
with R. crc press, 2006. 

http://archive.ics.uci.edu/ml/ 
http://www.nipsfsc.ecs.soton.ac.uk/ 
http://osmot.cs.cornell.edu/kddcup/ 
http://www-stat.stanford.edu/~jhf/r-rulefit.html 
http://www-stat.stanford.edu/~jhf/r-rulefit.html 
http://additivegroves.net 

introduct 
problem definit 
exist approach 
fit gener addit model 
interact detect 

our approach 
fast interact detect 
overview 
construct predictor 
calcul rss 
complex analysi 

two-stag construct 
implement detail 
further relax 
diagnost 


experi 
model accuraci on real dataset 
dataset 
result 

detect featur interact with fast 
sensit to the number of bin 
accuraci 
featur correl and spuriou pair 

scalabl 
design choic 
case study: learn to rank 

conclus 
refer 

