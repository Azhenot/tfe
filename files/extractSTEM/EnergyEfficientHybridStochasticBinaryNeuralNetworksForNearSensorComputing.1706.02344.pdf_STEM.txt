















































energy-effici hybrid stochastic-binari neural network for near-sensor comput 




energy-effici hybrid stochastic-binari neural 

network for near-sensor comput 
vincent T. lee†, armin alaghi†, john P. hayes*, visvesh sathe‡, lui ceze† 

†depart of comput scienc and engineering, univers of washington, seattle, wa, 98198 
*depart of electr engin and comput science, univers of michigan, ann arbor, mi, 48109 

‡depart of electr engineering, univers of washington, seattle, wa, 98195 

{vlee2, armin}@cs.washington.edu, jhayes@eecs.umich.edu, sathe@uw.edu, luisceze@cs.washington.edu 

abstract— recent advanc in neural network (nns) exhibit 

unpreced success at transform large, unstructur data 

stream into compact higher-level semant inform for task 

such a handwrit recognition, imag classification, and speech 

recognition. ideally, system would employ near-sensor com- 

putat to execut these task at sensor endpoint to maxim 

data reduct and minim data movement. however, near- 

sensor comput present it own set of challeng such a 

oper power constraints, energi budgets, and commun 

bandwidth capacities. In thi paper, we propos a stochastic- 

binari hybrid design which split the comput between the 

stochast and binari domain for near-sensor NN applications. In 

addition, our design us a new stochast adder and multipli 

that be significantli more accur than exist adder and 

multipliers. We also show that retrain the binari portion of the 

NN comput can compens for precis loss introduc 

by shorter stochast bit-streams, allow faster run time at 

minim accuraci losses. our evalu show that our hybrid 

stochastic-binari design can achiev 9.8× energi effici 

savings, and application-level accuraci within 0.05% compar 

to convent all-binari designs. 

keywords—neur networks, stochast comput 

I. introduct 

sensor and actuat be critic for enabl electron circuit 
to interact with the physic world. inform acquir from 
sensor have becom essenti to applic from home 
autom to medic implant to environment surveillance. 
It be predict that the world soon will have an averag of 1,000 
sensor per person [8][11] which translat to a huge amount of 
raw data acquisition. the sheer volum of unstructur sensor 
data threaten to overwhelm storag and network communica- 
tion capacities, which be increasingli limit by aggress 
power and energi budgets. 

To reduc the storag and commun demand of raw 
sensor data, near-sensor comput have recent emerg a a 
design space for reduc these overhead [20]. near-sensor 
comput propos offload portion of the applic to 
comput unit or acceler co-loc with the sens 
device. the key insight be that by offload certain portion of 
comput such a imag featur extract (of an image- 
process pipeline) to sensor end points, high level semant 
inform can be transmit in place of larg unstructur 
data streams. Of particular interest be neural network (nns) 
which be a wide use class of algorithm for process raw 
unstructur data. nn excel at reason about raw data 
stream in applic such a object detection, handwrit 
recognition, and speech processing. recent work by Du et al. 

[12] show how a near-sensor NN acceler can dramat 
reduc the energi cost of the system. 

thi paper present a near-sensor stochastic-binari NN 
design which combin stochast comput (sc) with 
convent “binary” process and sensor data acquisit to 
improv energi effici and power consumption. SC be a re- 
emerg comput techniqu that perform comput on 
unari bit-stream repres probabl [14]. SC circuit be 
often cheaper than binari arithmet circuit [25]. for instance, 
multipl in SC can be implement by a singl and gate. 
the primari tradeoff for sc' simplic be increas computa- 
tion time, which lead to high energi consumpt for high 
precis calcul [2][22]. however, for applic that 
can toler reduc precision, SC can achiev compel 
power and energi effici gains. finally, stochast circuit 
be small in size and more error tolerant, make them suitabl 
for tini sensor oper in harsh environ [3][13]. 

stochast nn have be extens studi in the prior 
literatur [7][9][15]. however, past work propos fulli 
stochast design that have number length exceed 1,000 
clock cycl [7][15], which lead to high energi consumption. 
In addition, error introduc by multipl level of SC circuit 
compound a more level be execut [22]. In thi paper, we 
present a stochastic-binari hybrid NN system that exploit the 
benefit of sc, while mitig mani of it drawbacks. We onli 
employ SC in the first layer of an nn, so it oper directli on 
the sensor data therebi avoid the issu of compound 
error over multipl layers. We employ a new, significantli 
more accur SC adder and a determinist number gener 
scheme to further reduc energi consumption. finally, we 
compar our design’ accuraci to that of exist SC designs, 
and show our design have good energi effici than compet 
binari implementations. 

our contribut be a follows: 

1. A novel stochast adder for convolut nn which 
increas speed and/or accuracy, lead to a reduc 
energi cost compar to previou SC NN designs. 

2. A hybrid stochastic-binari NN design which combin 
signal acquisit and SC in the first NN layer, and us 
binari for the remain layer to avoid compound 
accuraci losses. 

3. show that retrain these remain NN layer can 
compens for precis loss introduc by sc. 

the rest of the paper be organ a follows. section II provid 
background on SC and nns. section iii introduc the new 





stochast adder design. section IV present our hybrid NN 
design, and result be discuss in section V and vi. 

ii. background 

thi section briefli review the relev concept of stochast 

comput and neural networks. 

A. stochast comput 

stochast comput be an altern method of comput first 
propos in the 1960 [14]. In sc, number be encod a bit- 
stream that be interpret a probabilities. for instance, the bit- 
stream X = 001011… denot a stochast number (sn) with 
valu pX = 0.5 becaus the probabl of see a 1 at a randomli 
chosen posit of X be 0.5. thi interpret allow arithmet 
function to be implement via simpl logic gates. for instance, 
the and gate in fig. 1a perform multipl on uncorrel 
inputs. the SC probabl pX or unipolar rang [0, 1], do not 
includ neg numbers, which be usual need for nns. 
As a result, nn often use bipolar numbers, where the valu of 

X be interpret a 2px  1, and therefor have rang [1, 1]. the 
precis of SC be mainli determin by the length N of the bit- 
stream. A bit-stream of length N encod a number at log2 N bit 
of precision. for example, a unipolar bit-stream of length 16 can 
encod the rang [0, 15] which be equival to the rang of a 
binari number with log216 = 4 bit of precision. 

In thi work, we use four SC primitives: adders, multipliers, 
stochast number gener (sngs), and stochastic-to-digit 
convert (fig. 1). these compon oper on unipolar 
numbers; they may implement a differ function when 
interpret in the bipolar domain. To perform convent 
stochast addition, two bit-stream X and Y be appli to the 
data input of a multiplex with the select bit driven by a bit- 
stream R of unipolar valu pR = 0.5 (fig. 1b). the output bit- 
stream encod pZ = 0.5(px + py). notic the scale factor of 0.5, 
a necessari featur of sc, keep the probabl in the unit 
interv [0, 1]. when compound over mani additions, the 
scale factor can lead to sever loss of precision. similar 
precis loss also occur with SC multiplication, which be 
realiz with an and gate (fig. 1a) sinc pZ = pX × py. one way 
to improv the qualiti of a function be to increas the length of 
the input bit-streams. however, sinc each bit of addit 
precis requir a doubl of bit-stream length thi quickli 
lead to excess run times. As a result, research have 
propos altern design that approxim the add operation. 
one exampl be to use an OR gate a an adder, which onli work 
accur if both input be close to zero [21]. hence, all 
exist SC adder design need addit uncorrel random 
number sourc and/or have limit accuracy. the need for extra 
random number sourc becom sever when mani number 
be to be added. ideally, we would like an adder that oper 

accur on mani input in short period of time, without 
requir addit uncorrel number sources. 

binary-to-stochast converters, which be commonli 
refer to a stochast number gener (sngs), and 
stochastic-to-binari convert be SC primit that allow 
convers between the binari and stochast domains. An sng 
compris a compar and a random number gener (fig. 
1c). for a give number px, the sng will produc a 1 with that 
probabl if the random number be less than px. convert 
analog signal to the stochast domain can be achiev by 
replac the sng compar with an analog one. In thi paper, 
we use an analog-to-stochast convert to convert the sensor 
data directli to stochast encodings, without the need for 
analog-to-digit convert (adcs). We also use a set of sng 
to gener the NN weights. 

the choic of sng configur affect the accuraci and 
consequ the energi consumpt of the SC circuit. tabl 1 
show the mean squar error (mse) of a 4-bit and 8-bit SC 
multipli for the follow sng schemes: (i) use the same 
linear feedback shift regist (lfsr) for both inputs, (ii) use a 
separ lfsr for each input, (iii) use low-discrep 
sequenc [4], and (iv) use a ramp-compar analog-to- 
stochast convert [13] for one input, and a low-discrep 
sequenc for the other. for thi work, we employ the last number 
gener scheme a it provid the best accuracy. the mse 
be calcul by exhaust test the multipli for everi 
possibl input value. 

To convert from stochast to binary, we simpli count the 1 
in the bit-stream by use a binari counter (fig. 1d). In our work, 
we use asynchron counter becaus they allow u to clock 
the SC part of the circuit faster. It be suffici to appli a new 
input to an asynchron counter, even if the previou input 
have not propag through the counter. the delay of a 
synchron counter, on the other hand, be rel large, so it 
cannot keep up with the speed of the SC circuit feed it. unlik 
the asynchron counter, a synchron counter fail if the next 
input arriv befor the previou input be propagated. 

B. neural network 

nn come in a wide rang of network topologies, and gener 

consist of an input layer, an output layer, and a number of 

hidden layer in between [24]. A layer be compos of neurons, 

each of which have a set of inputs, an output, and an activ 

function f(x), e.g., a rectifi linear unit. each neuron be 

connect to neuron in the previou layer; a connect be 

defin by a weight that be multipli by the previou neuron’ 

output. these valu be sum with other connections’ 

output and pass to an activ function. for instance, give 

a neuron y that be connect to k neuron in the previou layer 

(c) (d) 

B 
k 

x 

(b) 

x 

(a) 

and gate 

y 
z 

pZ = pX × pY 

binari counterbinari counter 

B = i x, iϵ[0, 2 
k] 

multiplex 

pZ = (px + py)/2 

x 

y 

z 

r 

0 

1 

0 

1 

multiplex 

pZ = (px + py)/2 

x 

y 

z 

r 

0 

1 

comparatork 
A 

B 

A < B 

binari number B 
pX = b/2 

k 

k 

random no. 

gener 
x 

comparatork 
A 

B 

A < B 

binari number B 
pX = b/2 

k 

k 

random no. 

gener 
x 


fig. 1. unipolar stochast arithmet primitives: (a) multiplier, (b) scale adder, (c) comparator-bas stochast number generator, and (d) 

stochastic-to-binari convert implement a a binari counter. 





with output valu �⃗� = {x0, x1, …, xk1} and connect weight 
�⃗⃗⃗� = {w0, w1, …, wk1} respectively, the output of neuron y be 

defin a 𝑦𝑜𝑢𝑡 = 𝑓(∑ 𝑥𝑖𝑤𝑖 
𝑘−1 
𝑖=0 ). 

neuron connect topolog can either be fulli connect 

or local connect to the previou layer. In fulli connect 

layers, each neuron be connect to everi neuron of the previou 

layer. In the local connect case, neuron be connect to a 

subset of neuron in the previou layer. local connect 

layer be often refer to a convolut layer becaus their 

connect from the previou layer take the form of a window. 

the result oper be mathemat equival to a 

convolut where the convolut kernel be simpli a matrix 

of the connect weights. finally, nn also may have max 

pool layers, which be local connect layer that 

subsampl a window in the previou layer and output the 

maximum value. 

To determin the weight for each layer, nn be train 

over an input train set use backpropag [24]. thi be a 

techniqu that iter over the train dataset and gradual 

adjust the weight base on the gradient of the error in the 

nn’ output function. the error metric vari across 

applic but a commonli use one for NN classif be 

the cross-entropi loss. one iter over the entir train set 

be know a an epoch. train be often supplement by 

dropout which be a train techniqu that randomli remov 

connect dure the train process at certain layer to 

prevent overfitting. onc the train process converg to a set 

of weights, a test set be use to evalu the qualiti of the NN 

model. the qualiti metric vari across applic but a 

commonli use metric be classif accuraci base on the 

output of the NN model. 

use SC for nn have a well-establish histori [7][17] 

date back to the 1990s. recent work propos fulli stochast 

NN design use fpga fabric and full custom asic [16]. 

similarly, ardakani et al. [6] propos an SC NN for digit 

recognit which outperform binari design by use shorter 

bit-stream (down to length 16). To the best of our knowledge, 

thi be the onli SC NN design that outperforms, albeit 

marginally, it binari counterpart in term of energi efficiency. 

however, unlik our approach, prior SC work us older, fulli 

connect NN topolog with onli two hidden layer which 

be small and less accur than current state-of-th art NN 

topolog like lenet-5 (use in our evaluation). finally, fulli 

stochast nn need longer bit-stream (N = 256 to 1024) to 

achiev reason accuracy. In contrast, our work do not 

execut the entir NN in the stochast domain. instead, we 

execut the first layer use sc, then allow high precis 

binari unit to finish the NN calculation. 

iii. stochast adder design 

unlik the basic stochast multiplier, the convent 

stochast add oper have undesir properti such a the 

enforc scale factor and an extra bit-stream. furthermore, 

the discard of some bit of each number (through 

multiplexing) lead to accuraci loss, which compound with 

multipl additions. 

We now propos a new stochast adder that be more accur 
and do not requir addit random inputs. but first we 
introduc a simpl circuit that implement the SC function pC = 
pa/2. A rudimentari implement be to use the multipli of 
fig. 1a where we assign A to one input, and a randomli 
gener bit-stream B of valu 1/2 to the other. note that for the 
multipl to work accurately, B have to be uncorrel to A. 
fig. 2a show anoth implement of the same function, in 
which a bit-stream B with valu 1/2 be gener from the bit- 
stream of A without requir an addit input. A toggl flip- 
flop (tff), which switch it output between 0 and 1 when it 
input be 1, be use for thi purpose. the area cost of a tff be no 
more than a random number gener that be requir for 
gener 1/2. more importantly, the bit-stream gener by 
the tff be alway uncorrel with it input bit-stream. thi 
mean that there be no constraint on the auto-correl of the 
input bit-stream, unlik common sequenti SC circuit that do 
not function a intend if the input be auto-correl [7]. 

fig. 2b show our propos tff-base adder. At each clock 
cycle, if the valu at X and Y be equal, they propag to the 
output. otherwise, the state of the tff be output and the tff be 
toggled. suppos the adder oper on two bit-stream of length 
20. recal for adds, there be a 0.5 normal constant, so the 
expect result be Z = 0.5(1/2 + 4/5) = 13/20 comput a 
follows: 

X = 0110 0011 0101 0111 1000 (1/2) 
Y = 1011 1111 0101 0111 1111 (4/5) 
Z = 0110 1011 0101 0111 1101 (13/20) 

the result of the adder be alway accur if the bit-stream 
length N be suffici to repres it. otherwise, the output will 
be round off to the near represent number. the 
direct of round depend on the initi state S0 of the tff. 

a 

(a) (b) 

b 

c 

T Q 

T Q 

0 

1 

x 
y 

z 

X = 0100 1010 (3/8) 

Y = 0010 0010 (1/4) 

Z0 = 0010 0010 (1/4) 

Z1 = 0100 1010 (3/8) 

(c) 


fig. 2. (a) stochast circuit with pC = pa/2, (b) propos tff-base 

stochast adder with pZ = (px + py)/2, and (c) exampl of it oper 

with two differ initi states. 

tabl 1. mse of stochast multipli for differ rng method 

(lower be better) 

number gener scheme 8-bit prec. 4-bit prec. 

one lfsr + shift version 2.78×103 2.99×103 

two lfsr 2.57×104 1.60×103 

low-discrep sequenc [4] 1.28×105 1.01×103 

ramp-compar [13] + [4] 8.66×106 7.21×104 

tabl 2. mse of stochast addit for differ sng method 

(lower be better) 

implement 8-bit prec. 4-bit prec. 

old adder 

(fig. 1b) 

random + lfsr 3.24×104 5.55×103 

random + tff 5.49×104 5.49×103 

lfsr + tff 1.06×104 2.66×103 

new adder (fig. 2b) 1.91×106 4.88×104 







If S0 = 0, a in the exampl above, the result will be round to 
the small of the two neighbor numbers. fig. 2c show how 
S0 affect the result. Z0 and Z1 be the output of the circuit with 
S0 = 0 and 1, respectively. the expect result be Z = 0.5(3/8 + 
1/4) = 5/16. sinc N = 8 be not suffici to repres 5/16 
exactly, the result be round to either 1/4 or 3/8. 

To quantifi the accuraci of our propos adder, we compar 
it to the adder of fig. 1b with three differ sng config- 
urations: (i) random bit-stream use for the data input and an 
lfsr use for the select input, (ii) random bit-stream for the 
data input and a tff that toggl everi cycl for the select 
input, and (iii) an lfsr use for the data input and a tff for 
the select inputs. while the first configur be more 
commonli used, we tri two more configur that provid 
a slight improvement. however, a see in tabl 2, our propos 
adder achiev significantli good accuracy. onc again, the 
mse be calcul by exhaust test the adder for everi 
possibl input value. 

iv. stochastic-binari neural network design 

We now present our stochastic-binari hybrid design for near- 

sensor NN computation. fig. 3 give an overview of the 

propos neural network layer and system design. To evalu 

it utility, we will use it to implement the first layer of the 

lenet-5 NN topolog [19]. 

A. signal acquisit 

imag sensor captur light intens and convert it to analog 

signals, which be convert to digit number for processing. 

In thi work, we use part of a ramp-compar analog-to-digit 

convert (adc) to convert the analog signal to the stochast 

domain. the convers circuit show in fig. 3 be function 

equival to an sng (fig. 1c), with some modifications: (i) the 

input be analog, and (ii) a ramp signal be appli to the second 

input of the compar rather than a random number generator. 

despit becom heavili auto-correlated, the bit-stream 

gener by thi convers circuit be still usabl for our SC 

design, becaus the propos adder circuit be insensit to 

input auto-correlation. previou work have show such analog- 

to-stochast convert be comparable, in term of cost and 

perform to regular adc [3][13]. furthermore, prior work 

[26] have show such convers oper on the order of 100 

pj, which be much low than the energi consum by 

comput (100 of nj/image). thus, we do not includ the 

cost of sensor data convers in our evaluations. 

B. stochast convolut neural network layer 

the stochast NN layer consist of 784 stochast dot-product 

unit show in fig. 3 which process the sensor input in parallel. 

becaus there be 32 differ first layer kernels, we perform 

parallel convolut 32 time per image. the convolut 

engin perform a basic dot-product oper follow by 

stochastic-to-binari convers and an activ function. 

more precisely, each convolut engin implements: 

𝑔(�⃗�, �⃗⃗⃗�) = 𝑠𝑖𝑔𝑛(�⃗� ∘ �⃗⃗⃗�) 

where �⃗� and �⃗⃗⃗� denot input window and kernel weights, 
respectively, and ∘ denot the dot-product oper (�⃗� ∘ �⃗⃗⃗� = 
∑ 𝑥𝑖𝑤𝑖 

𝑘−1 
𝑖=0 ). the activ function simpli output the sign of 

the dot product result and output either 1, 0, or 1. the weight 

input be share among all convolut engines, so the cost of 

gener them be amort across all units. 

sinc the comput involv neg numbers, the 

bipolar SC domain [1, 1] be a natur choic [7]. however, by 

employ bipolar sc, the decis point of activ 

function map to bit-stream with maximum fluctuat (i.e., 

unipolar valu 0.5). thi increas power usag and decreas 

accuracy. therefore, we adopt a differ approach which us 

onli unipolar oper by divid the weight into posit 

and neg bit-stream �⃗⃗⃗�𝑝𝑜𝑠 and �⃗⃗⃗�𝑛𝑒𝑔. We then perform two 

unipolar dot product operations, 𝑔𝑝𝑜𝑠 = �⃗� ∘ �⃗⃗⃗�𝑝𝑜𝑠 and 𝑔𝑛𝑒𝑔 = 

�⃗� ∘ �⃗⃗⃗�𝑛𝑒𝑔, follow by two asynchron counter to convert the 


fig. 3. system diagram of our propos near-sensor stochast nn. bottom – lenet-5 NN topology. middl – system pipeline. top – 

microarchitecture. purple, grey, and blue region denot analog, stochastic, and binari domains, respectively. 





result 𝑔𝑝𝑜𝑠 and 𝑔𝑛𝑒𝑔 to the binari domain. finally, the binari 

activ function be implement by a simpl comparator. As 

show in fig. 3, the rest of the NN oper in the binari 

domain. 

V. experiment result 

thi section present the result of experi with the 

propos SC NN design. We mainli compar our design with 

a similar all-binari implementation, but when possibl we also 

provid comparison with exist sc-base nns. 

A. experiment setup 

We use the mnist databas [18], a standard machin learn 

benchmark for handwritten digit recognition, to evalu 

accuracy. the benchmark consist of M = 70,000 imag of 

handwritten digit (0 to 9); each imag us a 28×28 8-bit 

greyscal encoding. A subset of 60,000 imag be use to train 

the nn, while the remain 10,000 imag be use to test it 

accuracy. classif accuraci be defin a the ratio of 

correctli classifi test imag to the total number of test 

images. then the misclassif rate be defin a one minu 

the classif accuracy. these metric be often multipli 

by 100 and report a a percentage. all NN train be 

perform use the tensorflow framework [1], and the kera 

librari [10] use a nvidia titan X gpu. for each stochast 

design, we built a custom c++ model to evalu it accuracy. 

previou work on SC nn [6][16] evalu NN topolog 

with onli fulli connect layer and achiev misclassif 

rate between 1.95% and 2.41%. On the other hand, our work 

us the lenet-5 topolog which have both convolut and 

fulli connect layers, and achiev misclassif rate 

around 1%. In practice, the number of convolut layer 

kernel and the size of the kernel use in lenet-5 vary; for our 

evaluation, we use a variant provid by the kera librari 

which have the topolog show in fig. 3. 

B. accuraci result and neural network retrain 

A key tradeoff in SC be reduc precis to enhanc 

performance. To quantifi the impact of reduc precis on 

classif accuracy, we build separ NN model which 

execut the first layer of lenet-5 at differ precis level (2 

to 8 bits). We also replac the standard rectifi linear 

activ function with a sign function, which do not impos 

a signific accuraci loss, but have a much simpler implement- 

tation in sc. We do not execut subsequ layer in the 

stochast domain sinc precis loss would compound and 

requir longer bit-stream to achiev accur results. 

for comparison, we evalu how precis reduct 

affect the fulli binari implementation. our experi show 

that simpli quantiz the first layer weight and replac the 

activ function with sign detect reduc classif 

accuraci by sever percentag point (up to 6.85% mis- 

classif rate for 4-bit precision). however, by retrain 

the rest of the NN weights, the NN model be abl to recov 

from the nois introduc by loss in precis and the new 

activ function (tabl 3). interestingly, we find that we can 

reduc precis down to 3 or 4 bit and still achiev excel 

misclassif rate (below 1%) after retraining. sinc the 

train process be also noisy, the classif accuraci do 

not alway exhibit monoton decreas behavior a 

precis be reduced. 

bit reduct of SC design exhibit similar accuraci losses, 

but lead to exponenti run time reduct and energi savings. 

however, stochast convolut present uniqu challenges. 

SC can be inexact at near-zero input values, and output valu 

be sensit to errors. prior work [5] show that a non-trivi 

percentag of NN valu be near zero, so we use weight scale 

and soft threshold a propos by kim et al. [16] to mitig 

these errors. weight scale normal the valu of each 

convolut kernel to use the full dynam rang [1, 1] while 

soft threshold forc a result to zero if it be within some 

threshold. finally, we also employ the retrain techniqu 

introduc earli in the binari domain of the design. 

We now compar the result classif accuraci use 

SC with our new adder and multiplier, and the convent 

adder and multipli introduc earli in fig. 1 that be use 

in prior work. tabl 3 show misclassif rate (lower be 

better) for each design. the result indic that our new adder 

and multipli gener achiev low misclassif rate 

than those in prior SC work (up to 2.92% better). We be also 

abl to achiev misclassif rate which be within 0.05% 

and 0.25% of the binari design for 8-bit and 4-bit precis 

respectively. further, the result show that retrain the NN 

model can compens for nois introduc by both precis 

reduct and sc. In particular, for our more accur adder and 

multipl scheme there be less nois that the retrain 

process must compens for than the old adder. note that the 

benefit of the retrain be onli possibl becaus we can 

oper in the high precis binari domain. finally, our 

result confirm that there be signific opportun for precis 

tabl 3. misclassif rate for full binari and hybrid stochastic-binari designs, and throughput-norm power, energi efficiency, and 

area result for binari and stochast convolut designs. 

design 8 bit 7 bit 6 bit 5 bit 4 bit 3 bit 2 bit 

misclassif 

rate (%) 

binari 0.89% 0.86% 0.89% 0.74% 0.79% 0.79% 1.30% 

old SC 2.22% 3.91% 1.30% 1.55% 1.63% 2.71% 4.89% 

thi work 0.94% 0.99% 1.04% 1.12% 1.04% 2.20% 43.82% 

normal 

power (mw) 

binari 40.95 mW 72.80 mW 121.52 mW 204.96 mW 325.36 mW 501.76 mW 683.20 mW 

thi work 33.17 mW 33.55 mW 33.26 mW 33.01 mW 33.20 mW 29.96 mW 28.35 mW 

energi effici 

(nj / frame) 

binari 670.92 nJ 596.38 nJ 497.74 nJ 419.76 nJ 333.17 nJ 256.90 nJ 174.90 nJ 

thi work 543.42 nJ 274.82 nJ 136.22 nJ 67.60 nJ 34.00 nJ 15.34 nJ 7.26 nJ 

area 

(mm2) 

binari 1.313 mm2 1.094 mm2 0.891 mm2 0.710 mm2 0.543 mm2 0.391 mm2 0.255 mm2 

thi work 1.321 mm2 1.282 mm2 1.240 mm2 1.200 mm2 1.166 mm2 1.110 mm2 1.057 mm2 





reduct in sc, which translat to exponenti reduct in 

bit-stream length and good run times, which we explor next. 

vi. power, area, and energi evalu 

We synthesize, place-and-route, and measur power use 

synopsi design compiler, IC compiler, and primetim for 

our design; we also use a 65nm tsmc library. for comparison, 

we evalu a slide window convolut engin a our binari 

baselin design [23]. activ factor for power measur 

be record use trace base on mnist test imag and 

weight from the tensorflow model. 

tabl 3 show the throughput-norm power, energi 

efficiency, and design area for both stochast and binari 

convolut designs. power measur be throughput- 

normal rel to the stochast design. for instance, a 

binari design oper at 0.25× the throughput and 2× the 

power rel to a stochast design would have a throughput- 

normal power of 8× rel to the stochast design. sinc 

run time of stochast design decreas exponenti with 

low precision, we find that the binari design must oper at 

exponenti high frequenc and power to match the increas 

in throughput. finally, we find the area and energi cost of the 

SC number gener be high than a singl SC dot product 

unit, but the cost be share and amort over mani units. 

sinc the actual oper frequenc will vari across 

applic demands, we contrast the throughput-norm 

power between the stochast and binari designs. throughput- 

normal power be more repres of energi effici 

sinc it be more agnost to the differ in frequenc and 

number of parallel unit in the design. In term of energi 

efficiency, our design break even with binari design at 8-bit 

precision, and be 9.8× more energi effici at 4-bit precision. 

furthermore, it achiev these gain with good classif 

accuraci than prior work. 

finally, we see that our stochast convolut design 

achiev reason area overhead rel to the binari one. 

the stochast convolut engin exhibit virtual no chang 

in resourc util sinc precis in SC onli affect the 

length of the bit-streams. however, binari design benefit from 

linear area reduct sinc reduc precis narrow the 

datapath. We find that our design achiev roughli the same 

area a the binari design at 8-bit precis but be 2× larg than 

the binari design at 4-bit precision. 

vii. conclus 

We present a convolut NN system which employ a 

hybrid stochastic-binari design for near-sensor computing. the 

design employ near-sensor SC use a novel stochast adder 

which be significantli more accur than previou adder 

designs. our simul show that with thi adder, the hybrid 

NN achiev up to 2.92% good accuraci than previou SC 

designs, and 9.8× good energi effici for convolut over 

all-binari designs. finally, we show that retrain the binari 

domain portion of the NN can compens for precis loss 

from sc. As nn becom increasingli commonplac in 

modern applications, the energi effici gain offer by SC 

will be invalu for meet the aggress power and energi 

budget of next gener sensor and emb devices. 

viii. acknowledg 

thi work be support in part by the nation scienc 

foundat under grant ccf-1318091 and grant ccf- 

1518703, and gener gift from oracl lab and microsoft. 

refer 

[1] M. adabi et al., tensorflow: large-scal machin learn on 
heterogen system [online], available: http://tensorflow.org/, 
[accessed: 17-sep-2016]. 

[2] J. M. de aguiar and S. P. khatri, “explor the viabil of stochast 
computing,” proc. iccd, pp. 391-394, 2015. 

[3] A. alaghi et al., “stochast circuit for real-tim image-process 
applications,” proc. dac, pp. 1-6, 2013. 

[4] A. alaghi and J. P. hayes, “fast and accur comput use 
stochast circuits,” proc. date, pp. 1-4, 2014. 

[5] J. albericio et al., “cnvlutin: ineffectual-neuron-fre deep neural 
network computing,” proc. isca, pp. 1-13, 2016. 

[6] A. ardakani et al., “vlsi implement of deep neural network use 
integr stochast computing,” proc. istc, pp. 216-220, 2016. 

[7] B. D. brown and H. C. card, “stochast neural computation. I. 
comput elements,” ieee trans. comp., pp. 891-905, 2001. 

[8] J. bryzek, “roadmap to a $ trillion mem market,” mem technolog 
symposium, vol. 23, 2012. 

[9] V. canal et al., “A new stochast comput methodolog for effici 
neural network implementation,” ieee trans. neural network and 
learn systems, pp. 551-564, 2016. 

[10] F. chollet., kera [online], available: https://github.com/fchollet/keras, 
[accessed: 17-sep-2016]. 

[11] H. G. chen et al., “asp vision: optic comput the first layer of 
convolut neural network use angl sensit pixels,” proc. 
cvpr, 2016. 

[12] Z. Du et al., “shidiannao: shift vision process closer to the 
sensor,” sigarch comput. archit. news, pp. 92-104, 2015. 

[13] D. fick et al., “mixed-sign stochast comput demonstr in an 
imag sensor with integr 2d edg detect and nois filtering,” 
proc. custom integr circuit confer (cicc), pp. 1-4, 2014. 

[14] b.r. gaines, “stochast comput systems,” advanc in inform 
system science, vol. 2, pp. 37-172, 1969. 

[15] Y. Ji et al., “A hardwar implement of a radial basi function neural 
network use stochast logic,” proc. date, pp. 880-883, 2015. 

[16] K. kim et al., “dynam energy-accuraci trade-off use stochast 
comput in deep neural networks,” proc. dac, pp. 124:1-6 , 2016. 

[17] y.-c. kim and M. A. shanblatt, “architectur and statist model of a 
pulse-mod digit multilay neural network,” ieee trans. neural 
networks, pp. 1109-1118, 1995. 

[18] Y. lecun et al., the mnist databas of handwritten digit [online], 
http://yann.lecun.com/exdb/mnist/, [accessed: 17-sep-2016]. 

[19] Y. lecun et al., “gradient-bas learn appli to document 
recognition,” proc. ieee, vol. 86, pp. 2278-2324, 1998. 

[20] R. likamwa et al., “redeye: analog convnet imag sensor architectur 
for continu mobil vision," proc. isca, 2016. 

[21] B. Li et al., “use stochast comput to reduc the hardwar 
requir for a restrict boltzmann machin classifier,” proc. 
fpga, pp. 36-41, 2016. 

[22] B. moon and M. verhelst, “energy-effici and accuraci of 
stochast comput circuit in emerg technologies,” ieee jour. 
emerg and select topic in circuit syst., pp. 475-486, 2014. 

[23] A. E. nelson, “implement of imag process algorithm on 
fpga hardware,” m.s. thesis, EE dept.,vanderbilt univ., 2000. 

[24] m.a. nielsen, neural network and deep learning, determin 
press, 2015. http://neuralnetworksanddeeplearning.com/. 

[25] W. qian et al., “an architectur for fault-toler comput with 
stochast logic,” ieee trans. comp., vol. 60, pp. 93-105, 2011. 

[26] N. verma and A. P. chandrakasan, “an ultra low energi 12-bit rate- 
resolut scalabl sar adc for wireless sensor nodes,” ieee journal 
of solid-st circuits, pp. 1196-1205, 2007. 




