




















































look, listen and learn 


look, listen and learn 

relja arandjelović† 

relja@google.com 

andrew zisserman†,∗ 

zisserman@google.com 

†deepmind ∗vgg, depart of engin science, univers of oxford 

abstract 

We consid the question: what can be learnt by look- 
ing at and listen to a larg number of unlabel videos? 
there be a valuable, but so far untapped, sourc of infor- 
mation contain in the video itself – the correspond 
between the visual and the audio streams, and we intro- 
duce a novel “audio-visu correspondence” learn task 
that make use of this. train visual and audio network 
from scratch, without ani addit supervis other than 
the raw unconstrain video themselves, be show to suc- 
cess solv thi task, and, more interestingly, result in 
good visual and audio representations. these featur set 
the new state-of-the-art on two sound classif bench- 
marks, and perform on par with the state-of-the-art self- 
supervis approach on imagenet classification. We also 
demonstr that the network be abl to local object in 
both modalities, a well a perform fine-grain recognit 
tasks. 

1. introduct 

visual and audio event tend to occur together; not al- 
way but often: the movement of finger and sound of the 
instrument when a piano, guitar or drum be played; lip 
move and speech when talking; car move and engin 
nois when observ a street. the visual and audio event 
be concurr in these case becaus there be a common 
cause. In thi paper we investig whether we can use thi 
simpl observ to learn about the world both visual 
and aural by simpli watch and listen to videos. 

We ask the question: what can be learnt by train vi- 
sual and audio network simultan to predict whether 
visual inform (a video frame) correspond or not to au- 
dio inform (a sound snippet)? thi be a looser require- 
ment than that the visual and audio event occur in sync. It 
onli requir that there be someth in the imag that cor- 
relat with someth in the audio clip – a car present in 
the video frame, for instance, correl with engin noise; 
or an exterior shot with the sound of wind. 

our motiv for thi work be three fold: first, a in 

mani recent self-supervis task [1, 6, 8, 20, 21, 24, 35, 
36], it be interest to learn from a virtual infinit sourc 
of free supervis (video with visual and audio mode in 
thi case) rather than requir strong supervision; second, 
thi be a possibl sourc of supervis that an infant could 
use a their visual and audio capabl develop; third, we 
want to know what can be learnt, and how well the network 
be trained, for exampl in the perform of the visual and 
audio network for other tasks. 

Of course, we be not the first to make the observa- 
tion that visual and audio event co-occur, and to use 
their concurr or correl a supervis for train- 
ing a network. In a seri of recent and inspir pa- 
per [2, 11, 22, 23], the group at mit have investig pre- 
cise this. however, their goal be alway to train a singl 
network for one of the modes, for example, train a visual 
network to gener sound in [22, 23]; or train an audio 
network to correl with visual output in [2, 11], where 
the visual network be pre-train and fix and act a a 
teacher. In earlier, pre deep-learning, approach the obser- 
vation be use to beauti effect in [14] show “pixel 
that sound” (e.g. for a guitar) learnt use cca. In contrast, 
we train both visual and audio network and, somewhat sur- 
prisingly, show that thi be benefici – in that our perfor- 
manc improv substanti over that of [2] when train 
on the same data. 

In summary: our goal be to design a system that be abl to 
learn both visual and audio semant inform in a com- 
plete unsupervis manner by simpli look at and lis- 
tene to a larg number of unlabel videos. To achiev 
thi we introduc a novel audio-visu correspond 
(avc) learn task that be use to train the two (visual and 
audio) network from scratch. thi task be describ in sec- 
tion 2, togeth with the network architectur and train 
procedure. In section 3 we describ what semant informa- 
tion have be learnt, and ass the perform of the audio 
and visual networks. We find, which we have not anticipated, 
that thi task lead to quit fine grain visual and audio 
discrimination, e.g. into differ instruments. In term of 
quantit performance, the audio network exceed those 
recent train for audio recognit use visual super- 

1 

ar 
X 

iv 
:1 

70 
5. 

08 
16 

8v 
2 

[ 
c 

.C 
V 

] 
1 

A 
ug 

2 
01 

7 

lphilipp 
zone de text 
knowledg from multi input channels, pure orthogon 

=> richer becaus orthogon per definit 

Is it just that??? 

To be clarified! 



vision subnetwork 

audio subnetwork 

fusion layer correspond? 

audio-visu correspond detector network 

ye / No 

figur 1. audio-visu correspond task (avc). A network 
should learn to determin whether a pair of (video frame, short 
audio clip) correspond to each other or not. posit be (frame, 
audio) extract from the same time of one video, while neg 
be a frame and audio extract from differ videos. 

vision, and the visual network have similar perform to 
those train for other, pure visual, self-supervis tasks. 
furthermore, we show, a an add benefit, that we be abl 
to local the sourc of the audio event in the video frame 
(and also local the correspond region of the sound 
source) use activ visualization. 

In term of prior work, the most close relat deep 
learn approach that we know of be ‘syncnet’ in [5]. 
however, [5] be aim at learn to synchron lip-region 
and speech for lip-reading, rather than the more gener 
video and audio materi consid here for learn se- 
mantic representations. more generally, the avc task be a 
form of co-train [4], where there be two ‘views’ of the 
data, and each view provid complementari information. 
In our case the two view be visual and audio (and each can 
determin semant inform independently). A similar 
scenario aris when the two view be visual and languag 
(text) a in [9, 18, 31] where a common emb be learnt. 
however, usual one (or both) of the network (for imag 
and text) be pre-trained, in contrast to the approach take 
here where no supervis be requir and both network be 
train from scratch. 

2. audio-visu correspond learn 
the core idea be to use a valuabl but so far untap 

sourc of inform contain in the video itself – the cor- 
respond between visual and audio stream avail by 
virtu of them appear togeth at the same time in the 
same video. By see and hear mani exampl of a 
person play a violin and exampl of a dog barking, and 
never, or at least veri infrequently, see a violin be 
play while hear a dog bark and vice versa, it should be 
possibl to conclud what a violin and a dog look and sound 
like, without ever be explicitli taught what be a violin or 
a dog. 

We leverag thi for learn by an audio-visu corre- 
spondenc (avc) task, illustr in figur 1. the avc 
task be a simpl binari classif task: give an exampl 
video frame and a short audio clip – decid whether they 

correspond to each other or not. the correspond (posi- 
tive) pair be the one that be take at the same time from 
the same video, while mismatch (negative) pair be ex- 
tract from differ videos. the onli way for a system to 
solv thi task be if it learn to detect variou semant con- 
cept in both the visual and the audio domain. indeed, we 
demonstr in section 3.5 that our network automat 
learn relev semant concept in both modalities. 

It should be note that the task be veri difficult. the 
network be make to learn visual and audio featur and 
concept from scratch without ever see a singl label. 
furthermore, the avc task itself be quit hard when do 
on complet unconstrain video – video can be veri 
noisy, the audio sourc be not necessarili visibl in the video 
(e.g. camera oper speaking, person narrat the video, 
sound sourc out of view or occluded, etc.), and the audio 
and visual content can be complet unrel (e.g. edit 
video with add music, veri low volum sound, ambient 
sound such a wind domin the audio track despit other 
audio event be present, etc.). nevertheless, the result 
in section 3 show that our network be abl to fairli success- 
fulli solv the avc task, and in the process learn veri good 
visual and audio representations. 

2.1. network architectur 

To tackl the avc task, we propos the network struc- 
ture show in figur 2. It have three distinct parts: the vision 
and the audio subnetwork which extract visual and audio 
features, respectively, and the fusion network which take 
these featur into account to produc the final decis on 
whether the visual and audio signal correspond. here we 
describ the three part in more detail. 
vision subnetwork. the input to the vision subnetwork be 
a 224 × 224 colour image. We follow the vgg-network 
[30] design style, with 3× 3 convolut filters, and 2× 2 
max-pool layer with stride 2 and no padding. the net- 
work can be segment into four block of conv+conv+pool 
layer such that insid each block the two conv layer have 
the same number of filters, while consecut block have 
doubl filter numbers: 64, 128, 256 and 512. At the veri 
end, max-pool be perform across all spatial locat 
to produc a singl 512-d featur vector. each conv layer be 
follow by batch normal [12] and a relu nonlin- 
earity. 
audio subnetwork. the input to the audio subnetwork be a 
1 second sound clip convert into a log-spectrogram (more 
detail be provid late in thi section), which be thereaft 
treat a a greyscal 257× 199 image. the architectur of 
the audio subnetwork be ident to the vision one with the 
except that input pixel be 1-d intens instead of 3- 
D colour and therefor the conv1 1 filter size be 3× 
small compar to the vision subnetwork. the final audio 
featur be also 512-d. 

2 



concat 

1024 

fc1 1024x128 

128 

fc2 128x2 

2 

softmax 

2 

V 
be 

io 
n 
s 

u 
b 
n 

e 
tw 

o 
rk 

224x224x3 

A 
u 
d 
io 

s 
u 
b 
n 
e 
tw 

o 
rk 

conv1_1 3x3x64 

224x224x64 

conv1_2 3x3x64 

224x224x64 

pool1 2x2 

112x112x64 

conv2_1 3x3x128 

112x112x128 

conv2_2 3x3x128 

112x112x128 

pool2 2x2 

56x56x128 

conv3_1 3x3x256 

56x56x256 

conv3_2 3x3x256 

56x56x256 

pool3 2x2 

28x28x256 

conv4_1 3x3x512 

28x28x512 

conv4_2 3x3x512 

28x28x512 

pool4 28x28 

1x1x512 

conv1_1 3x3x64 

257x199x64 

conv1_2 3x3x64 

257x199x64 

pool1 2x2 

128x99x64 

conv2_1 3x3x128 

128x99x128 

conv2_2 3x3x128 

128x99x128 

pool2 2x2 

64x49x128 

conv3_1 3x3x256 

64x49x256 

conv3_2 3x3x256 

64x49x256 

pool3 2x2 

32x24x256 

conv4_1 3x3x512 

32x24x512 

conv4_2 3x3x512 

32x24x512 

pool4 32x24 

1x1x512 

257x199x1 

log-spectrogram 

1 second 48khz audio 

figur 2. l3-net architecture. each block repres a singl 
layer with text provid more inform – first row: layer name 
and parameters, second row: output featur map size. layer with 
a name prefix conv, pool, fc, concat, softmax be convo- 
lutional, max-pooling, fulli connected, concaten and softmax 
layers, respectively. the list paramet are: conv – kernel size 
and number of channels, pool – kernel size, fc – size of the 
weight matrix. the stride of pool layer be equal to the kernel size 
and there be no padding. each convolut layer be follow by 
batch normal [12] and a relu nonlinearity, and the first 
fulli connect layer (fc1) be follow by relu. 

fusion network. the two 512-d visual and audio fea- 
ture be concaten into a 1024-d vector which be pass 
through the fusion network to produc a 2-way classifica- 
tion output, namely, whether the vision and audio corre- 
spond or not. It consist of two fulli connect layers, with 
relu in between them, and the intermedi featur size of 
128-d. 

2.2. implement detail 

train data sampling. A non-correspond frame- 
audio pair be compil by randomli sampl two differ 
video and pick a random frame from one and a random 
1 second audio clip from the other. A correspond frame- 
audio pair be creat by sampl a random video, pick 
a random frame in that video, and then pick a random 

1 second audio clip that overlap in time with the sampl 
frame. thi provid addit train sampl compar 
to simpli sampl the 1 second audio with the frame at it 
mid-point. We use standard data augment techniqu 
for images: each train imag be uniformli scale such 
that the small dimens be equal to 256, follow by ran- 
dom crop into 224 × 224, random horizont flipping, 
and bright and satur jittering. audio be onli aug- 
ment by chang the volum up to 10% randomli but 
consist across the sample. 
log-spectrogram computation. the 1 second audio be re- 
sampl to 48 khz, and a spectrogram be comput with 
window length of 0.01 second and a half-window overlap; 
thi produc 199 window with 257 frequenc bands. the 
respons map be pass through a logarithm befor feed 
it into the audio subnetwork. 
train procedure. We use the adam optim [15], 
weight decay 10−5, and perform a grid search on the learn- 
ing rate, although 10−4 usual work well. the network 
be train on 16 gpu in parallel with synchron train- 
ing implement in tensorflow, where each worker pro- 
cess a 16-element batch, thu make the effect batch 
size of 256. for a train set of 400k 10 second videos, the 
network be train for two days, dure which it have see 
60m frame-audio pairs. 

3. result and discuss 
our “look, listen and learn” network (l3-net) approach 

be evalu and examin in multipl ways. first, the per- 
formanc of the network on the audio-visu correspon- 
denc task itself be investigated, and compar to supervis 
baselines. second, the qualiti of the learnt visual and audio 
featur be test in a transfer learn setting, on visual and 
audio classif tasks. finally, we perform a qualit 
analysi of what the network have learnt. We start by intro- 
duce the dataset use for training. 

3.1. dataset 

two video dataset be use for train the networks: 
flickr-soundnet and kinetics-sounds. 
flickr-soundnet [2]. thi be a larg unlabel dataset of 
complet unconstrain video from flickr, compil by 
search for popular tags, but no tag or ani sort of ad- 
dition inform apart from the video themselv be 
used. It contain over 2 million video but for practic rea- 
son we use a random subset of 500k video (400k train- 
ing, 50k valid and 50k test) and onli use the first 10 
second of each video. thi be the dataset that be use for 
train the l3-net for the transfer learn experi in 
section 3.3 and 3.4. 
kinetics-sounds. while our goal be to learn from com- 
plete unconstrain videos, have a label dataset be 

3 



use for quantit evaluation. for thi purpos we take 
a subset (much small than flickr-soundnet) of the ki- 
netic dataset [13], which contain youtub video man- 
ualli annot for human action use mechan turk, 
and crop to 10 second around the action. the subset 
contain 19k 10 second video clip (15k training, 1.9k val- 
idation, 1.9k test) form by filter the kinet dataset 
for 34 human action classes, which have be chosen to be 
potenti manifest visual and aurally, such a play- 
ing variou instrument (guitar, violin, xylophone, etc.), us- 
ing tool (lawn mowing, shovel snow, etc.), a well a 
perform miscellan action (tap dancing, bowling, 
laughing, singing, blow nose, etc.); the full list be give 
in appendix A. although thi dataset be fairli clean by con- 
struction, it still contain consider noise, e.g. the bowl- 
ing action be often accompani by loud music at the bowl- 
ing alley, human voic (camera oper or video narra- 
tions) often mask the sound of interest, and mani video 
contain sound track that be complet unrel to the 
visual content (e.g. music montag for a snow shovel 
video). 

3.2. audio-visu correspond 

first we evalu the perform of our method on the 
task it be train to solv – decid whether a frame 
and a 1 second audio clip correspond (section 2). for the 
kinetics-sound dataset which contain label videos, we 
also evalu two supervis baselin in order to gaug 
how well the avc train compar to supervis training. 

supervis baselines. for both baselin we first train vi- 
sion and audio network independ on the action clas- 
sific task, and then combin them in two differ 
ways. the vision network have an ident featur extrac- 
tion trunk a our vision subnetwork (section 2.1), on top 
of which two fulli connect layer be attach (sizes: 
512×128 and 128×34) to perform classif into the 34 
kinetics-sound classes. the audio classif network 
be construct analogously. the direct combin base- 
line comput the audio-video correspond score a the 
similar of class score distribut of the two networks, 
comput a the scalar product between the 34-d network 
softmax outputs, and decid that audio and video be in 
correspond if the score be larg than a threshold. the 
motiv behind thi baselin be that if the vision network 
believ the frame contain a dog while the audio network 
be confid it hear a violin, then the (frame, audio) pair 
be unlik to be in correspondence. the supervis pre- 
train baselin take the featur extract trunk from 
the two train networks, assembl them into our network 
architectur by concaten the featur and add two 
fulli connect layer (section 2.1). the weight of the 
featur extractor be frozen and the fulli connect lay- 
er be train on the avc task in the same manner a our 

method flickr-soundnet kinetics-sound 
supervis direct – 65% 
supervis pretrain – 74% 
l3-net 78% 74% 

tabl 1. audio-visu correspond (avc) results. test set 
accuraci on the avc task for the l3-net, and the two supervis 
baselin on the label kinetics-sound dataset. the number 
of posit and neg be the same, so chanc get 50%. all 
method be train on the train set of the respect datasets. 

network. thi be the strong baselin a it directli cor- 
respond to our method, but with featur learnt in a fulli 
supervis manner. 

result and discussion. tabl 1 show the result on the 
avc task. the l3-net achiev 74% and 78% on the two 
datasets, where chanc be 50%. It should be note that the 
task itself be quit hard due to the unconstrain natur of 
the video (section 2), a well a due to the veri local input 
data which lack context – even human find it hard to judg 
whether an isol frame and an isol singl second of 
audio correspond; inform human test indic that hu- 
man be onli a few percent good than the l3-net. fur- 
thermore, the supervis baselin do not beat the l3-net 
a “supervis pretraining” perform on par with it, while 
“supervis direct combination” work significantli bad 
as, unlik “supervis pretraining”, it have not be train 
for the avc task. 

3.3. audio featur 

In thi section we evalu the power of the audio repre- 
sentat that emerg from the l3-net approach. namely, 
the l3-net audio subnetwork train on flickr-soundnet be 
use to extract featur from 1 second audio clips, and the 
effect of these featur be evalu on two standard 
sound classif benchmarks: esc-50 and dcase. 

environment sound classif (esc-50) [26]. thi 
dataset contain 2000 audio clips, 5 second each, equal 
balanc between 50 classes. these includ anim 
sounds, natur soundscapes, human non-speech sounds, in- 
terior/domest sounds, and exterior/urban noises. the data 
be split into 5 predefin fold and perform be measur 
in term of mean accuraci over 5 leave-one-fold-out evalu- 
ations. 

detect and classif of acoust scene and event 
(dcase) [32]. We consid the scene classif task of 
the challeng which contain 10 class (bus, busi street, 
office, open air market, park, quiet street, restaurant, super- 
market, tube, tube station), with 10 train and 100 test 
clip per class, where each clip be 30 second long. 

experiment procedure. To enabl a fair direct compar- 
ison with the current state-of-the-art, aytar et al. [2], we 
follow the same experiment setup. multipl overlap 

4 



(a) esc-50 (b) dcase 
method accuraci 
svm-mfcc [26] 39.6% 
autoencod [2] 39.9% 
random forest [26] 44.3% 
piczak convnet [25] 64.5% 
soundnet [2] 74.2% 
our random 62.5% 
our 79.3% 
human perf. [26] 81.3% 

method accuraci 
RG [27] 69% 
ltt [19] 72% 
rnh [28] 77% 
ensembl [32] 78% 
soundnet [2] 88% 
our random 85% 
our 93% 

tabl 2. sound classification. “our random” be an addit 
baselin which show the perform of our network without l3- 
training. our l3-train set the new state-of-the-art by a larg 
margin on both benchmarks. 

method top 1 accuraci 
random 18.3% 
pathak et al. [24] 22.3% 
krähenbühl et al. [16] 24.5% 
donahu et al. [7] 31.0% 
doersch et al. [6] 31.7% 
zhang et al. [36] (init: [16]) 32.6% 
noroozi and favaro [21] 34.7% 
our random 12.9% 
our 32.3% 

tabl 3. visual classif on imagenet. follow [36], our 
featur be evalu by train a linear classifi on the ima- 
genet train set and measur the classif accuraci on 
the valid set. for more detail and discuss see section 
3.4. all perform number apart from our be provid by au- 
thor of [36], show onli the best perform for each method 
over all paramet choic (e.g. donahu et al. [7] achiev 27.1% 
instead of 31.0% when take featur from pool5 instead of 
conv3). 

subclip be extract from each record and describ 
use our features. for 5 second record from esc-50 
we extract 10 equal space 1 second subclips, while for 
the 6 time longer dcase recordings, 60 subclip be ex- 
tract per clip. the audio featur be obtain by max- 
pool the last convolut layer of the audio subnetwork 
(conv4 2), befor the relu, into a 4 × 3 × 512 = 6144 
dimension represent (the conv4 2 output be orig- 
inal 16 × 12 × 512). the featur be preprocess us- 
ing z-score normalization, i.e. shift and scale to have a 
zero mean and unit variance. A multi-class one-vs-al lin- 
ear svm be trained, and at test time the class score for a 
record be comput a the mean over the class score 
for it subclips. 

result and discussion. tabl 2 show the result on esc- 
50 and dcase. On both benchmark we convincingli beat 
the previou state-of-the-art, soundnet [2], by 5.1% and 
5% absolute. for esc-50 we reduc the gap between the 
previou best result and the human perform by 72% 
while for dcase we reduc the error by 42%. the result 

be especi impress a soundnet us two vision net- 
work train in a fulli supervis manner on imagenet and 
places2 a teacher for the audio network, while we learn 
both the vision and the audio network without ani super- 
vision whatsoever. note that we train our network with a 
random subset of the soundnet video for effici pur- 
poses, so it be possibl that further gain can be achiev by 
use all the avail train data. 

3.4. visual featur 

In thi section we evalu the power of the visual repre- 
sentat that emerg from the l3-net approach. namely, 
the l3-net vision subnetwork train on flickr-soundnet 
be use to extract featur from images, and the effective- 
ness of these featur be evalu on the imagenet larg 
scale visual recognit challeng 2012 [29]. 
experiment procedure. We follow the experiment 
setup of zhang et al. [36] where featur be extract from 
256 × 256 imag and use to perform linear classif 
on imagenet. As in [36], we take conv4 2 featur af- 
ter relu and perform max-pool with equal kernel and 
stride size until featur dimension be below 10k; in our 
case thi result in 4×4×512 = 8192-d features. A singl 
fulli connect layer be add to perform linear classifica- 
tion into the 1000 imagenet classes. all the weight be 
frozen to their l3-net-train values, apart from the final 
classif layer which be train with cross-entropi loss 
on the imagenet train set. the train procedur (data 
augmentation, learn rate schedule, label smoothing) be 
ident to [33], the onli differ be that we use the 
adam optim instead of rmsprop, and a 256×256 input 
imag instead of 299 × 299 a it fit our architectur good 
and to be consist with [36]. 
result and discussion. classif accuraci on the im- 
agenet valid set be show in tabl 3 and contrast 
with other unsupervis and self-supervis methods. We 
also test the perform of random features, i.e. our l3- 
net architectur without avc train but with a train 
classif layer. 

our l3-net-train featur achiev 32.3% accuraci 
which be on par with other state-of-the-art self-supervis 
method of [6, 7, 21, 36], while convincingli beat ran- 
dom initialization, data-depend initi [16], and 
context encod [24]. It should be note that these meth- 
od use the alexnet [17] architectur which be differ to 
ours, so the result be not fulli comparable. On the one 
hand, our architectur when train from scratch in it en- 
tireti achiev a high perform (59.2% v alexnet’ 
51.0%). On the other hand, it be deeper which make it 
harder to train a can be see from the fact that our random 
featur perform bad than their (12.9% v alexnet’ 
18.3%), and that all compet method hit peak perfor- 
manc when they use earli layer (e.g. [7] drop from 

5 



fingerpick lawn mow P. accordion P. bass guitar P. saxophon type bowl P. clarinet P. organ 

figur 3. learnt visual concept (kinetics-sounds). each column show five imag that most activ a particular unit of the 512 in 
pool4 for the vision subnetwork. note that these featur do not take sound a input. video come from the kinetics-sound test set 
and the network be train on the kinetics-sound train set. the top row show the domin action label for the unit (“p.” stand for 
“playing”). 

fingerpick lawn mow P. accordion P. bass guitar P. saxophon type bowl P. clarinet P. organ 

figur 4. visual semant heatmap (kinetics-sounds). exampl correspond to the one in figur 3. A semant heatmap be obtain a 
a slice of activ from conv4 2 of the vision subnetwork that correspond to the same unit from pool4 a in figur 3, i.e. the unit 
that respond highli to the class in question. 

6 



31.0% to 27.1% when go from conv3 to pool5). In 
fact, when measur the improv achiev due to 
avc or self-supervis train versu the perform of 
the network with random initialization, our avc train 
beat all competitors. 

anoth import fact to consid be that all compet 
method actual use imagenet imag when training. al- 
though they do not make use of the labels, the underli 
imag statist be the same: object be fairli central in the 
image, and the network have seen, for example, abund 
imag of 120 bread of dog and thu potenti learnt 
their distinguish features. In contrast, we use a com- 
plete separ sourc of train data in the form of frame 
from flickr video – here the object be in gener not cen- 
tred, it be like that the network have never see a “tibetan 
terrier” nor the major of other fine-grain categories. 
furthermore, video frame have vastli differ low-level 
statist to still images, with strong artefact such a mo- 
tion blur. with these factor hamper our network, it be 
impress that our visual featur l3-net-train on flickr 
video perform on par with self-supervis state-of-the-art 
train on imagenet. 

3.5. qualit analysi 

In thi section we analys what be it that the network 
have learnt. We visual the result on the test set of the 
kinetics-sound and flickr-soundnet datasets, so the net- 
work have not see the video dure training. 

3.5.1 vision featur 

To probe what the vision subnetwork have learnt, we pick 
a particular ‘unit’ in pool4 (i.e. a compon of the 512 
dimension pool4 vector) and rank the test imag by 
it magnitude. figur 3 show the imag from kinetics- 
sound that activ particular unit in pool4 the most (i.e. 
be rank high by it magnitude). As can be seen, the 
vision subnetwork have automat learnt, without ani ex- 
plicit supervision, to recogn semant entiti such a 
guitars, accordions, keyboards, clarinets, bowl alleys, 
lawn or lawnmowers, etc. furthermore, it have learnt finer- 
grain categori a well a it be abl to distinguish be- 
tween acoust and bass guitar (“fingerpicking” be mostli 
associ with acoust guitars). 

figur 4 show heatmap for the kinetics-sound im- 
age in figur 3, obtain by simpli display the spa- 
tial activ of the correspond vision unit (i.e. if the 
k compon of pool4 be chosen, then the k channel of 
conv4 2 be display – sinc the k compon be just the 
spatial max over thi channel (after relu)). object be 
success detect despit signific clutter and occlu- 
sions. It be interest to observ the type of cue that the 
network decid to use, e.g. the “play clarinet” unit, in- 

stead of tri to detect the entir clarinet, seem to mostli 
activ on the interfac between the player’ face and the 
clarinet. 

figur 5 and 6 show visual concept learnt by the l3- 
net on the flickr-soundnet dataset. It can be see that the 
network learn to recogn mani scene categori (figur 
5), such a outdoors, concert, water, sky, crowd, text, rail- 
way, etc. these be use for the avc task as, for example, 
crowd indic a larg event that be associ with a dis- 
tinctiv sound a well (e.g. a footbal game), text indic 
narration, and outdoor scene be like to be accompa- 
nie with wind sounds. It should be note that though at 
first sight some categori seem trivial detectable, it be not 
the case; for example, “sky” detector be not equival to 
the “blueness” detector a it onli fire on “sky” and not on 
“water”, and furthermor there be separ unit sensit 
to “water surface” and to “underwater” scenes. the network 
also learn to detect peopl a user upload content be sub- 
stantial people-ori – figur 6 show the network have 
learnt to distinguish between babies, adult and crowds. 

3.5.2 audio featur 

figur 7 show what particular audio unit be sensit 
to in the kinetics-sound dataset. for visual pur- 
poses, instead of show the sound form, we display the 
video frame that correspond to the sound. It can be see 
that the audio subnetwork, again without ani supervision, 
manag to learn variou semant entities, a well a per- 
form fine-grain classif (“fingerpicking” v “play- 
ing bass guitar”). note that some unit be natur con- 
fuse – the “tap dancing” unit also respond to “pen tap- 
ping”, while the “saxophone” unit be sometim confus 
with a “trombone”. these be reason mistakes, espe- 
cialli when take into account that the sound input be onli 
one second in length. the audio concept learnt on the 
flickr-soundnet dataset (figur 8) follow the same pattern 
a the visual one – the network learn to distinguish var- 
iou scene categori such a water, underwater, outdoor 
and windi scenes, a well a human-rel concept like 
babi and human voices, crowds, etc. 

figur 9 show spectrogram and their semant 
heatmaps, illustr that our l3-net learn to detect au- 
dio events. for example, it show clear prefer for low 
frequenc when detect bass guitars, attent to wide 
frequenc rang when detect lawnmowers, and tempor 
‘steps’ when detect fingerpick and tap dancing. 

3.5.3 versu random featur 

could the result in figur 3, 4, 5, 6, 7 8, and 9 simpli 
be obtain by chanc due to examin a larg number 
of units, a colour illustr by the dead salmon ex- 
periment [3]? It be unlik a there be onli 512 unit in 

7 



outdoor concert outdoor sport 

cloudi sky ski water surfac underwat 

horizon railway crowd text 

figur 5. learnt visual concept and semant heatmap (flickr-soundnet). each mini-column show five imag that most activ 
a particular unit of the 512 in pool4 of the vision subnetwork, and the correspond heatmap (for more detail see figur 3 and 4). 
column titl be a subject name of concept the unit respond to. 

8 



babi face head crowd 

figur 6. learnt human-rel visual concept and semant heatmap (flickr-soundnet). each mini-column show five imag that 
most activ a particular unit of the 512 in pool4 of the vision subnetwork, and the correspond heatmap (for more detail see figur 
3 and 4). column titl be a subject name of concept the unit respond to. 

fingerpick lawn mow P. accordion P. bass guitar P. saxophon type P. xylophon tap danc tickl 

figur 7. learnt audio concept (kinetics-sounds). each column show five sound that most activ a particular unit in pool4 of the 
audio subnetwork. pure for visual purposes, a it be hard to display sound, the frame of the video that be align with the sound be 
show instead of the actual sound form, but we stress that no vision be use in thi experiment. video come from the kinetics-sound test 
set and the network be train on the kinetics-sound train set. the top row show the domin action label for the unit (“p.” stand for 
“playing”). 

9 



babi voic human voic male voic peopl crowd 

music concert sport clap water underwat windi outdoor 

figur 8. learnt audio concept (flickr-soundnet). each mini-column show sound that most activ a particular unit of the 512 in 
pool4 of the audio subnetwork. pure for visual purposes, a it be hard to display sound, the frame of the video that be align with 
the sound be show instead of the actual sound form, but we stress that no vision be use in thi experiment. column titl be a subject 
name of concept the unit respond to. note that for the “human voice”, “male voice”, “crowd”, “music” and “concert” examples, the 
respect clip do contain the relev audio despit the frame look a if it be unrelated, e.g. the third exampl in the “concert” column 
do contain loud music sounds. audio clip contain the five concaten 1 sampl correspond to each mini-column be host 
on youtub and can be reach by click on the respect mini-columns; thi youtub playlist (https://goo.gl/ohdgtj) contain all 16 
examples. 

10 

http://youtu.be/mrade51ahhq 
http://youtu.be/l-sbbq2xjvo 
http://youtu.be/i8khq-my9qg 
http://youtu.be/et3i_yzm4ww 
http://youtu.be/vs5-5u2c-ki 
http://youtu.be/gbl0475ctoq 
http://youtu.be/ax6xcqn1gxc 
http://youtu.be/tt41s6xezkm 
http://youtu.be/a5aexsubgjg 
http://youtu.be/cqqy7trqqdw 
http://youtu.be/8bpqd_dytyi 
http://youtu.be/glbhknqncgw 
http://youtu.be/gww5sco468o 
http://youtu.be/_ar2dcwd3ay 
http://youtu.be/gyidy6ncxpc 
http://youtu.be/7dxhheewhwa 
https://goo.gl/ohdgtj 


pool4 to choos from, and mani of those be found to 
be highli correl with a semant concept. nevertheless, 
we repeat the same experi with a random network 
(i.e. a network that have not be trained), and have fail 
to find such correlation. In more detail, we examin how 
mani out of the action class in kinetics-sound have a 
unit in pool4 which show high prefer for the class. 
for the vision subnetwork the prefer be determin by 
rank all imag by their unit activation, and retain the 
top 5; if 4 out of these 5 imag correspond to one class, 
then that class be deem to have a high-prefer for the 
unit (a similar procedur be carri out for the audio sub- 
network use spectrograms). our train vision and audio 
network have high-prefer unit for 10 and 11 out of a 
possibl 34 action classes, respectively, compar to 1 and 1 
for the random vision and audio networks. furthermore, if 
the threshold for deem a unit to be high-prefer be re- 
duce to 3, our train vision and audio subnetwork cover 
23 and 20 classes, respectively, compar to the 4 and 3 of 
a random network, respectively. these result confirm that 
our network have inde learnt semant features. 

furthermore, figur 10 show the comparison between 
the train and the non-train (i.e. network with random 
weights) l3-net represent for the visual and the au- 
dio modalities, on the kinetics-sound dataset, use the 
t-sne visual [34]. It be clear that train for the 
audio-visu correspond task produc represent 
that have a semant meaning, a video contain the same 
action class often cluster together, while the random net- 
work’ represent do not exhibit ani clustering. there 
be still a fair amount of confus in the representations, but 
thi be expect a no class-level supervis be provid and 
class can be veri alike. for example, an organ and a piano 
be quit visual similar a they contain keyboards, and the 
visual differ between a bass guitar and an acoust gui- 
tar be also quit fine; these similar be reflect in the 
close or overlap of respect cluster in figur 10(c) 
(e.g. a note earlier, “fingerpicking” be mostli associ 
with acoust guitars). 

We also evalu the qualiti of the l3-net embed 
by cluster them with k-mean into 64 cluster and re- 
port the normal mutual inform (nmi) score 
between the cluster and the ground truth action classes. 
result in tabl 4 confirm the emerg of semant a 

method vision audio 
random assign 0.165 0.165 
our random (l3-net without training) 0.204 0.219 
our (l3-net self-supervis training) 0.409 0.330 

tabl 4. cluster quality. normal mutual inform 
(nmi) score between the unsupervis cluster of featur em- 
bed and the kinetics-sound labels. 

fingerpick lawn mow P. bass guitar tap danc 

figur 9. audio semant heatmap (kinetics-sounds). each 
pair of column show a singl action class (top, “p.” stand for 
“playing”), five log-spectrogram (left) and spectrogram semant 
heatmap (right) for the class. horizont and vertic ax cor- 
respond to the time and frequenc dimensions, respectively. A 
semant heatmap be obtain a a slice of activ of the unit 
from conv4 2 of the audio subnetwork which show prefer 
for the consid class. 

the l3-net embed outperform the best random base- 
line by 50-100%. 

the t-sne visual also show some interest fea- 
tures, such a the “typing” class be divid into two clus- 
ter in the visual domain. further investig reveal that 
all frame in one cluster show both a keyboard and hands, 
while the second cluster contain much few hands. sepa- 
rate these two case can be a good indic of whether 
the type action be happen at the moment captur by 
the (frame, 1 second sound clip) pair, and thu whether 
the type sound be expect to be heard. furthermore, we 
found that the “typing” audio sampl appear in three clus- 
ter – the two fairli pure cluster (outlin in figur 10(a)) 
correspond to strong type sound and talk while typ- 
ing, respectively, and the remain cluster, which be veri 
impur and intermingl with other action classes, mostli 
correspond to silenc and background noise. 

4. discuss 
We have show that the network train for the avc 

task achiev superior result on sound classif to re- 
cent method that pre-train and fix the visual network (one 
each for imagenet and scenes), and we conjectur that the 
reason for thi be that the addit freedom of the visual 
network allow the learn to good take advantag of the 
opportun offer by the varieti of visual inform in 
the video (rather than be restrict to see onli through 

11 



the eye of the pre-train network). also, the visual fea- 
ture that emerg from the l3-net be on par with the state- 
of-the-art among self-supervis approaches. furthermore, 
it have be demonstr that the network automat 
learns, in both modalities, fine-grain distinct such a 
bass versu acoust guitar or saxophon versu clarinet. 

the local visual result be reminisc of 
the classic highlight pixel in [14], except in our case we 
do not just learn the few pixel that move (concurr with 
the sound) but instead be abl to learn extend region 
correspond to the instrument. 

We motiv thi work by consid correl of 
video and audio events. however, we believ there be ad- 
dition inform in concurr of the two streams, a 
concurr be strong than correl becaus the event 
need to be synchronis (of course, if event be concurr 
then they will correlate, but not vice versa). train for 
concurr will requir video (multipl frames) a input, 
rather than a singl video frame, but it would be interest 
to explor what more be gain from thi strong condition. 

In the future, it would be interest to learn from the 
recent releas larg dataset of video curat accord 
to audio, rather than visual, event [10] and see what subtl 
visual semant categori be discovered. 

refer 
[1] P. agrawal, J. carreira, and J. malik. learn to see by 

moving. In proc. iccv, 2015. 
[2] Y. aytar, C. vondrick, and A. torralba. soundnet: learn 

sound represent from unlabel video. In nips, 2016. 
[3] C. M. bennett, M. B. miller, and G. L. wolford. neural cor- 

relat of interspeci perspect take in the post-mortem 
atlant salmon: An argument for multipl comparison cor- 
rection. neuroimage, 2009. 

[4] A. blum and T. mitchell. combin label and unlabel 
data with co-training. In comput learn theory, 
1998. 

[5] J. S. chung and A. zisserman. out of time: autom lip 
sync in the wild. In workshop on multi-view lip-reading, 
accv, 2016. 

[6] C. doersch, A. gupta, and A. A. efros. unsupervis vi- 
sual represent learn by context prediction. In proc. 
cvpr, 2015. 

[7] J. donahue, P. krähenbühl, and T. darrell. adversari fea- 
ture learning. In proc. iclr, 2017. 

[8] A. dosovitskiy, J. T. springenberg, M. riedmiller, and 
T. brox. discrimin unsupervis featur learn with 
convolut neural networks. In nips, 2014. 

[9] A. frome, G. S. corrado, J. shlens, S. bengio, J. dean, M. A. 
ranzato, and T. mikolov. devise: A deep visual-semant 
emb model. In nips, 2013. 

[10] J. F. gemmeke, D. P. W. ellis, D. freedman, A. jansen, 
W. lawrence, R. C. moore, M. plakal, and M. ritter. au- 
dio set: An ontolog and human-label dataset for audio 
events. In icassp, 2017. 

[11] D. harwath, A. torralba, and J. R. glass. unsupervis 

learn of spoken languag with visual context. In nips, 
2016. 

[12] S. ioff and C. szegedy. batch normalization: acceler 
deep network train by reduc intern covari shift. In 
proc. icml, 2015. 

[13] W. kay, J. carreira, K. simonyan, B. zhang, C. hillier, 
S. vijayanarasimhan, F. viola, T. green, T. back, P. natsev, 
M. suleyman, and A. zisserman. the kinet human action 
video dataset. corr, abs/1705.06950, 2017. 

[14] E. kidron, Y. Y. schechner, and M. elad. pixel that sound. 
In proc. cvpr, 2005. 

[15] D. P. kingma and J. ba. adam: A method for stochast 
optimization. In proc. iclr, 2015. 

[16] P. krähenbühl, C. doersch, J. donahue, and T. darrell. data- 
depend initi of convolut neural networks. 
In proc. iclr, 2015. 

[17] A. krizhevsky, I. sutskever, and G. E. hinton. imagenet 
classif with deep convolut neural networks. In 
nips, page 1106–1114, 2012. 

[18] J. lei ba, K. swersky, S. fidler, and R. salakhutdinov. pre- 
dict deep zero-shot convolut neural network use 
textual descriptions. In proc. iccv, 2015. 

[19] D. li, J. tam, and D. toub. auditori scene classif us- 
ing machin learn techniques. ieee aasp challeng on 
detect and classif of acoust scene and events, 
2013. 

[20] I. misra, C. L. zitnick, and M. herbert. shuffl and learn: 
unsupervis learn use tempor order verification. In 
proc. eccv, 2016. 

[21] M. noroozi and P. favaro. unsupervis learn of visual 
represent by solv jigsaw puzzles. In proc. eccv, 
2016. 

[22] A. owens, P. isola, J. mcdermott, A. torralba, E. adelson, 
and W. freeman. visual indic sounds. In proc. cvpr, 
2016. 

[23] A. owens, W. jiajun, J. mcdermott, W. freeman, and 
A. torralba. ambient sound provid supervis for visual 
learning. In proc. eccv, 2016. 

[24] D. pathak, P. krähenbühl, J. donahue, T. darrell, and A. A. 
efros. context encoders: featur learn by inpainting. In 
proc. cvpr, 2016. 

[25] K. J. piczak. environment sound classif with con- 
volut neural networks. In ieee workshop on machin 
learn for signal processing, 2015. 

[26] K. J. piczak. esc: dataset for environment sound classifi- 
cation. In proc. acmm, 2015. 

[27] A. rakotomamonji and G. gasso. histogram of gradient 
of time-frequ represent for audio scene classifica- 
tion. ieee/acm transact on audio, speech, and lan- 
guag processing, 2015. 

[28] G. roma, W. nogueira, and P. herrera. recurr quantifi- 
cation analysi featur for environment sound recognition. 
In ieee workshop on applic of signal process to 
audio and acoust (waspaa), 2013. 

[29] O. russakovsky, J. deng, H. su, J. krause, S. satheesh, 
S. ma, S. huang, A. karpathy, A. khosla, M. bernstein, 
A. berg, and F. li. imagenet larg scale visual recognit 
challenge. ijcv, 2015. 

[30] K. simonyan and A. zisserman. veri deep convolut 
network for large-scal imag recognition. In proc. iclr, 

12 



2015. 
[31] R. socher, M. ganjoo, C. D. manning, and A. ng. zero-shot 

learn through cross-mod transfer. In nips, 2013. 
[32] D. stowell, D. giannoulis, E. benetos, M. lagrange, and 

M. D. plumbley. detect and classif of acoust 
scene and events. In ieee transact on multimedia, 
2015. 

[33] C. szegedy, V. vanhoucke, S. ioffe, J. shlens, and Z. wojna. 
rethink the incept architectur for comput vision. In 
proc. cvpr, 2016. 

[34] L. van der maaten and G. hinton. visual data use 
t-sne. journal of machin learn research, 2008. 

[35] X. wang and A. gupta. unsupervis learn of visual rep- 
resent use videos. In proc. iccv, 2015. 

[36] R. zhang, P. isola, and A. A. efros. color imag coloriza- 
tion. In proc. eccv, 2016. 

A. kinetics-sound 
the 34 action class take from the kinet dataset 

[13] to form the kinetics-sound dataset 3.1 are: blow- 
ing nose, bowling, chop wood, rip paper, shuffl 
cards, singing, tap pen, typing, blow out, dribbl 
ball, laughing, mow the lawn by push lawnmower, 
shovel snow, stomping, tap dancing, tap guitar, tick- 
ling, fingerpicking, patting, play accordion, play bag- 
pipes, play bass guitar, play clarinet, play drums, 
play guitar, play harmonica, play keyboard, play- 
ing organ, play piano, play saxophone, play trom- 
bone, play trumpet, play violin, play xylophone. 

13 



(a) audio learnt 

sing 

type 

laugh 

fingerpick 

play accordion 

play bass guitar 

play clarinet 

play drum 

play organ 

play piano 

play saxophon 

play trombon 

play xylophon 

(b) audio random 

r 

type 

type 
laughingp. xylophon 

p. drum 

p. bass guitar 

ngerpick 

p. piano 
p. organ 

p. accordion 

sing 

type 

laugh 

fingerpick 

play accordion 

play bass guitar 

play clarinet 

play drum 

play organ 

play piano 

play saxophon 

play trombon 

play xylophon 

(c) visual learnt (d) visual random 

fingerpick 

p. bass 

guitar 

p. organ 

p. piano 

p. drumsp. clarinet 

p. accordion 

type 

type 
laugh 

figur 10. t-sne visual [34] of learnt represent (kinetics-sounds). the (a,c) and (b,d) show the two-dimension t-sne 
embed for the train versu non-train (i.e. network with random weights) l3-net, respectively. for visual purpos only, 
we colour the t-sne embed use the kinetics-sound labels, but no label be use for train the l3-net. for clariti and reduc 
clutter, onli a subset of action (13 class out of 34) be shown. some clearli notic cluster be manual highlight by enclos 
them with ellipses. best view in colour. 

14 




