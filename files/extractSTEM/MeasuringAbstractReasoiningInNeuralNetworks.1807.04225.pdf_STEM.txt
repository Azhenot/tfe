









































measur abstract reason in neural network 

david g.t. barrett * 1 felix hill * 1 adam santoro * 1 ari S. morco 1 timothi lillicrap 1 

abstract 
whether neural network can learn abstract rea- 
sone or whether they mere reli on superfici 
statist be a topic of recent debate. here, we 
propos a dataset and challeng design to probe 
abstract reasoning, inspir by a well-known hu- 
man IQ test. To succeed at thi challenge, model 
must cope with variou generalis ‘regimes’ 
in which the train and test data differ in clearly- 
defin ways. We show that popular model such 
a resnet perform poorly, even when the train- 
ing and test set differ onli minimally, and we 
present a novel architecture, with a structur de- 
sign to encourag reasoning, that do signifi- 
cantli better. when we vari the way in which the 
test question and train data differ, we find that 
our model be notabl profici at certain form 
of generalisation, but notabl weak at others. We 
further show that the model’ abil to generalis 
improv markedli if it be train to predict sym- 
bolic explan for it answers. altogether, 
we introduc and explor way to both measur 
and induc strong abstract reason in neural 
networks. our freely-avail dataset should mo- 
tivat further progress in thi direction. 

1. introduct 
abstract reason be a hallmark of human intelligence. A 
famou exampl be einstein’ elev thought experiment, 
in which einstein reason that an equival relat 
exist between an observ fall in uniform acceler 
and an observ in a uniform gravit field. It be the 
abil to relat these two abstract concept that allow 
him to deriv the surpris predict of gener relativity, 
such a the curvatur of space-time. 

A human’ capac for abstract reason can be estim 

*equal contribution, order by surname. 1deepmind, london, 
unit kingdom. correspond to: <{barrettdavid; felixhill; 
adamsantoro}@google.com>. 

proceed of the 35 th intern confer on machin 
learning, stockholm, sweden, pmlr 80, 2018. copyright 2018 
by the author(s). 

A B C D 

E F G H 

A B C D 

E F G H 
(a) (b) 

C 
on 

te 
xt 

P 
an 

el 
s 

An 
sw 

er 
P 

an 
el 

s 

+1 

+1 

xor(panel 1, panel 2) 

figur 1. raven-styl progress matrices. In (a) the underly- 
ing abstract rule be an arithmet progress on the number of 
shape along the columns. In (b) there be an xor relat on the 
shape posit along the row (panel 3 = xor(panel 1, panel 2)). 
other featur such a shape type do not factor in. A be the correct 
choic for both. 

surprisingli effect use simpl visual IQ tests, such 
a raven’ progress matric (rpms) (figur 1) (raven 
et al., 1938). the premis behind rpm be simple: one must 
reason about the relationship between perceptu obviou 
visual featur – such a shape posit or line color – to 
choos an imag that complet the matrix. for example, 
perhap the size of squar increas along the rows, and 
the correct imag be that which adher to thi size relation. 
rpm be strongli diagnost of abstract verbal, spatial and 
mathemat reason ability, discrimin even among 
popul of highli educ subject (snow et al., 1984). 

sinc one of the goal of AI be to develop machin with 
similar abstract reason capabl to humans, to aid sci- 
entif discoveri for instance, it make sens to ask whether 
visual IQ test can help to understand learn machines. 
unfortunately, even in the case of human such test can be 
invalid if subject prepar too much, sinc test-specif 
heurist can be learn that shortcut the need for generally- 
applic reason (te nijenhui et al., 2001; flynn, 1987). 
thi potenti pitfal be even more acut in the case of neural 
networks, give their strike capac for memor 

ar 
X 

iv 
:1 

80 
7. 

04 
22 

5v 
1 

[ 
c 

.L 
G 

] 
1 

1 
Ju 

l 2 
01 

8 



measur abstract reason in neural network 

(zhang et al., 2016) and abil to exploit superfici statisti- 
cal cue (jo & bengio, 2017; szegedi et al., 2013). 

nonetheless, we contend that visual intellig test can 
help to good understand learn and reason in ma- 
chine (fleuret et al., 2011), provid they be coupl with 
a principl treatment of generalisation. suppos we be 
concern with whether a model can robustli infer the no- 
tion of ‘monoton increasing’. In it most abstract form, 
thi principl can appli to the quantiti of shape or lines, or 
even the intens of their colour. We can construct train 
data that instanti thi notion for increas quantiti 
or size and we can construct test data that onli involv 
increas colour intensities. generalis to the test set 
would then be evid of an abstract and flexibl applica- 
tion of what it mean to monoton increase. In thi way, 
a dataset with explicitli defin abstract semant (e.g., 
relations, attributes, pixels, etc.), allow u to curat train- 
ing and test set that precis probe the generalis 
dimens of abstract reason in which we be interested. 

To thi end, we have develop a larg dataset of abstract 
visual reason question where the underli abstract se- 
mantic can be precis controlled. thi approach allow u 
to address the follow questions: (1) can state-of-the-art 
neural network find solut – ani solut – to complex, 
human-challeng abstract reason task if train with 
plenti train data? (2) If so, how well do thi capac- 
iti generalis when the abstract content of train data be 
specif control for? 

To begin, we describ and motiv our dataset, outlin a 
procedur for automat gener of data, and detail the 
generalis regim we chose to explore. next, we estab- 
lish a number of strong baselines, and show that well know 
architectur that use onli convolutions, such a resnet-50 
(he et al., 2016), struggle. We design a novel variant of 
the relat network (santoro et al., 2017; raposo et al., 
2017), a neural network with specif structur design to 
encourag relation-level comparison and reasoning. We 
found that thi model substanti outperform other well- 
know architectures. We then studi thi top-perform 
model on our propos generalis test and find that 
it generalis well in certain test regim (e.g. appli 
know abstract relationship in novel combinations), but 
fail notabl in other (such a appli know abstract 
relationship to unfamiliar entities). finally, we propos a 
mean to improv generalisation: the use of auxiliari train- 
ing to encourag our model to provid an explan for it 
solutions. 

2. procedur gener matrix 
In 1936 the psychologist john raven introduc the now 
famou human IQ test: raven’ progress matric (rpm) 

(a) (b) 

unari (progress on shape number) 

binari (xor on line type) 

ternari (consist union on shape type) 

{ {, , 

figur 2. A difficult pgm and a depict of relat types. (a) 
a challeng puzzl with multipl relat and distractor infor- 
mation. (b) a possibl categor of relat type base on 
how the panel be consid when comput the relation: for 
unary, a function be comput on one panel to produc the sub- 
sequent panel; for binary, two independ sampl panel be 
consid in conjunct to produc a third panel; and for ternary, 
all three panel adher to some rule, such a all contain shape 
from some common set, regardless of order. 

(raven et al., 1938). rpm consist of an incomplet 3× 3 
matrix of context imag (see figur 1), and some (typic 
8) candid answer images. the subject must decid which 
of the candid imag be the most appropri choic to 
complet the matrix. 

It be thought that much of the power of rpm a diagnos- 
tic of human intellig deriv from the way they probe 
educt or fluid reason (jaeggi et al., 2008). sinc no 
definit of an ‘appropriate” choic be provided, it be in pos- 
sibl in principl to come up with a reason support ani 
of the candid answers. To succeed, however, the subject 
must ass all candid answers, all plausibl justif 
for those answers, and identifi the answer with the strong 
justification. In practice, the right answer tend to be the one 
that can be explain with the simplest justif use 
the basic relat underli the matrices. 

although raven hand-design each of the matrix in hi 
tests, late research typic employ some structur 
gener model to creat larg number of questions. In 
thi setting, a potenti answer be correct if it be consist 
with the underli gener model, and success rest on 
the abil to invert the model. 

2.1. automat gener of pgm 

here we describ our process for creat rpm-like matri- 
ces. We call our dataset the procedur gener matri- 
ce (pgm) dataset. To gener pgms, we take inspir 
from carpent et al. (1990), who identifi and catalogu 



measur abstract reason in neural network 

the relat that commonli underli rpms, a well a 
wang & Su (2015), who outlin one process for creat 
an automat generator. 

the first step be to build an abstract structur for the matrices. 
thi be do by randomli sampl from the follow 
primit sets: 

• relat type (r, with element r): progression, 
xor, or, and, consist union1 

• object type (o, with element o): shape, line 
• attribut type (a, with element a): size, type, 
colour, position, number 

the structur S of a pgm be a set of triples, S = {[r, o, a] : 
r ∈ R, o ∈ O, a ∈ a}. these tripl determin the chal- 
leng pose by a particular matrix. for instance, if S con- 
tain the tripl [progression, shape, colour], 
the pgm will exhibit a progress relation, instanti 
on the colour (greyscal intensity) of shapes. challeng 
pgm exhibit relat govern by multipl such triples: 
we permit up to four relat per matrix (1 ≤ |s| ≤ 4). 

each attribut type a ∈ A (e.g. colour) can take one 
of a finit number of discret valu v ∈ V (e.g. 10 inte- 
ger between [0, 255] denot greyscal intensity). So a 
give structur have multipl realis depend on the 
randomli chosen valu for the attribut types, but all of 
these realis share the same underli abstract chal- 
lenge. the choic of r constrain the valu of v that can be 
realized. for instance, if r be progression, the valu 
of v must strictli increas along row or column in the 
matrix, but can vari randomli within thi constraint. see 
the appendix for the full list of relations, attribut types, 
values, their hierarch organisation, and other statist 
of the dataset. 

We use Sa to denot the set of attribut among the tripl 
in S . after set valu for the colour attribute, we then 
choos valu for all other attribut a 6∈ Sa in one of two 
ways. In the distract setting, we allow these valu to 
vari at random provid that they do not induc ani further 
meaning relations. otherwise, the a 6∈ Sa take a singl 
valu that remain consist across the matrix (for example, 
perhap all the shape be the exact same size). randomli 
vari valu across the matrix be a type of distract 
common to raven’ more difficult progress matrices. 

thus, the gener process consist of: (1) sampl 1- 
4 triples, (2) sampl valu v ∈ V for each a ∈ sa, 
adher to the associ relat r, (3) sampl valu 
v ∈ V for each a 6∈ sa, ensur no spuriou relat be 
induced, (4) render the symbol form into pixels. 

1consist union be a relat wherein the three panel contain 
element from some common set, e.g., shape type {square, circle, 
triangl }. the order of the panel contain the element do 
not matter. 

2.2. generalis regim 

generalis in neural network have be subject of lot of 
recent debate, with some emphasis the success (lecun 
et al., 2015) and other the failur (garnelo et al., 2016; 
lake & baroni, 2017; marcus, 2018). our choic of regim 
be inform by this, but be in no way exhaustive. 

(1) neutral In both train and test sets, the structur 
S can contain ani tripl [r, o, a] for r ∈ R, o ∈ O and 
a ∈ A. the train and test set be disjoint, but thi 
separ be at the level of the input variabl (i.e., the 
pixel manifest of the matrices). 

(2) interpolation; (3) extrapol As in the neutral 
split, S consist of ani tripl [r, o, a]. for interpolation, 
in the train set, when a = colour or a = size (the 
order attributes), the valu of a be restrict to even- 
index member of the discret set va, wherea in the test 
set onli odd-index valu be permitted. for extrapo- 
lation, the valu of a be restrict to the low half of 
their discret set of valu Va dure training, wherea in 
the test set they take valu in the upper half. note that all 
S contain some tripl [r, o, a] with a = colour or a = 
size. thus, generalis be requir for everi question 
in the test set. 

(4) held-out attribut shape-colour or (5) 
line-typ S in the train set contain no 
tripl with o = shape and a = colour. all structur 
govern puzzl in the test set contain at least one tripl 
with o = shape and a = colour. for comparison, we 
includ a similar split in which tripl be held-out if 
o = line and a = type. 

6: held-out tripl In our dataset, there be 29 possibl 
uniqu tripl [r, o, a]. We alloc seven of these for the 
test set, at random, but such that each of the a ∈ A be 
repres exactli onc in thi set. these held-out tripl 
never occur in question in the train set, and everi S 
in the test set contain at least one of them. 

7: held-out pair of tripl all S contain at least two 
triples, of which 400 be viable2 ([r1, o1, a1], [r2, o2, a2]) = 
(t1, t2). We randomli alloc 360 to the train set and 
40 to the test set. member (t1, t2) of the 40 held-out pair 
do not occur togeth in structur S in the train set, 
and all structur S have at least one such pair (t1, t2) a a 
subset. 

2certain triples, such a [progression, shape, 
number] and [progression, shape, xor] cannot 
occur togeth in the same pgm 



measur abstract reason in neural network 

8: held-out attribut pair S contain at least two 
triples. there be 20 (unordered) viabl pair of attribut 
(a1, a2) such that for some ri, oi, ([r1, o1, a1], [r2, o2, a2]) 
be a viabl tripl pair. ([r1, o1, a1], [r2, o2, a2]) = (t1, t2). 
We alloc 16 of these pair for train and four for 
testing. for a pair (a1, a2) in the test set, S in the train 
set contain tripl with a1 and a2. In the test set, all S 
contain tripl with a1 and a2. 

3. model and experiment setup 
We first compar the perform of sever standard deep 
neural network on the neutral split of the pgm dataset. 
We also develop a novel architectur base on relat 
network (santoro et al., 2017), that we call the wild rela- 
tion network (wren), name in recognit of mari wild 
who contribut to the develop of raven’ progress 
matrix along with her husband john raven. 

the input consist of the eight context panel and eight 
multiple-choic panels. each panel be an 80 × 80 pixel 
image; so, the panel be present a a set of 16 featur 
maps. 

model be train to produc the label of the correct 
miss panel a an output answer by optimis a softmax 
cross entropi loss. We train all network by stochast 
gradient descent use the adam optimis (kingma & ba, 
2014). for each model, hyper-paramet be chosen use 
a grid sweep to select the model with small loss estim 
on a held-out valid set. We use the valid loss 
for early-stop and we report perform valu on a 
held-out test set. for hyper-paramet set and further 
detail on all model see appendix A. 

cnn-mlp: We implement a standard four layer convo- 
lution neural network with batch normal and relu 
non-linear (lecun et al., 2015). the set of pgm input 
panel be treat a a set of separ greyscal input fea- 
ture map for the cnn. the convolv output be pass 
through a two-layer, fulli connect mlp use a relu 
non-linear between linear layer and dropout of 0.5 on the 
penultim layer. note that thi be the type of model appli 
to raven-styl sequenti reason question by hoshen & 
werman (2017). 

resnet: We use a standard implement of the 
resnet-50 architectur a describ in He et al. (2016). 
As before, each of the context panel and multiple-choic 
panel be treat a an input featur map. We also train a 
select of resnet variants, includ resnet-101, resnet- 
152, and sever custom-built small resnets. the best 
perform model be resnet-50. 

lstm: We implement a standard lstm modul 
(hochreit & schmidhuber, 1997), base on zaremba et al. 
(2014). sinc lstm be design to process input sequen- 
tially, we first pass each panel (context panel and multi- 
ple choic panels) sequenti and independ through 
a small 4-layer cnn, tag the cnn’ output with a one- 
hot label indic the panel’ posit (the top left pgm 
panel be tag with label 1, the top-middl pgm panel be 
tag with label 2 etc.), and pass the result sequenc 
of label embed to the lstm. the final hidden state 
of the lstm be pass through a linear layer to produc 
logit for the softmax cross entropi loss. the network be 
train use batch normal after each convolut 
layer and drop-out be appli to the lstm hidden state. 

wild relat network (wren): our novel wren 
model (fig. 3) appli a relat network modul (san- 
toro et al., 2017) multipl time to infer the inter-panel 
relationships. 

the model output a 1-d score sk for a give candid 
multiple-choic panel, with label k ∈ [1, 8]. the choic 
with the high score be select a the answer a use a 
softmax function σ across all scores: a = σ([s1, . . . , s8]). 
the score of a give multiple-choic panel be evalu 
use a relat network (rn): 

sk = rn(xk) 

= fφ 

( ∑ 
y,z∈xk 

gθ(y, z) 
) 
, (1) 

where Xk = {x1, x2, ..., x8} 
⋃ 
{ck}, ck be the vector repre- 

sentat of the multipl choic panel k, and xi the repre- 
sentat of context panel i. the input vector representa- 
tion be produc by process each panel independ 
through a small cnn and tag it with a panel label, sim- 
ilar to the lstm process describ above, follow by a 
linear projection. the function fφ and gθ be mlps. 

the structur of the wren model be well match to the 
problem of abstract reasoning, becaus it form representa- 
tion of pair-wis relat (use gθ), in thi case, between 
each context panel and a give multipl choic candidate, 
and between context panel themselves. the function fφ 
integr inform about context-context relat and 
context-multiple-choic relat to provid a score. also 
the wren model calcul a score for each multiple-choic 
candid independently, allow the network to exploit 
weight-shar across multiple-choic candidates. 

wild-resnet: We also implement a novel variant of the 
resnet architectur in which one multiple-choic candid 
panel, along with the eight context panel be provid a 
input, instead of provid all eight multiple-choic and 
eight context panel a input a in the standard resnet. In 



measur abstract reason in neural network 

Co 
nte 

xt 
Pa 

ne 
l 

choic panel B 

score-b 

... + 

Co 
nte 

xt 
Pa 

ne 
l 

choic panel A 

cnn 

RN 

score-a 

panel embed 

... 

panel 
embed 

pair 

+ 

softmax answer: A 

meta-target 
predict 

.64 

.22 

+ sigmoid 

figur 3. wren model A cnn process each context panel and an individu answer choic panel independ to produc 9 vector 
embeddings. thi set of embed be then pass to an rn, whose output be a singl sigmoid unit encod the “score” for the associ 
answer choic panel. 8 such pass be make through thi network (here we onli depict 2 for clarity), one for each answer choice, and the 
score be put through a softmax function to determin the model’ predict answer. 

thi way, the wild-resnet be design to provid a score for 
each candid panel, independ of the other candidates. 
the candid with the high score be the output answer. 
thi be similar to the wren model describ above, but 
use a resnet instead of a relat network for comput 
a candid score. 

context-blind resnet: A fully-blind model should be at 
chanc perform level, which for the pgm task be 12.5%. 
however, suffici strong model can learn to exploit 
statist regular in multiple-choic problem use 
the choic input alone, without consid the context 
(johnson et al., 2017). To understand the extent to which 
thi be possible, we train a resnet-50 model with onli 
the eight multiple-choic panel a input. 

3.1. train on auxiliari inform 

We explor auxiliari train a a mean to improv 
generalis performance. We hypothes that a model 
train to predict the relev relation, object and attribut 
type involv in each pgm might develop represent 
that be more amen to generalisation. To test this, we 
construct “meta-targets” encod the relation, object 
and attribut type present in pgm a a binari string. 
the string be of length 12, with element follow 
the syntax: (shape, line, color, number, 
position, size, type, progression, xor, 
or, and, consist union). We encod each 
tripl in thi binari form, then perform an OR oper 
across all binary-encod tripl to produc the meta- 
target. that is, or([101000010000], [100100010000]) = 
[101100010000]. the model then predict these label 

use a sigmoid unit for each element, train with cross 
entropy. A scale factor β determin the influenc 
of thi loss rel to the loss comput for the answer 
panel targets: ltotal = ltarget + βlmeta-target. We set β to 
a non-zero valu when we wish to explor the impact of 
auxiliari meta-target training. 

4. experi 
4.1. compar model on pgm question 

We first compar all model on the neutral train/test split, 
which correspond most close to tradit supervis 
learn regimes. perhap surprisingli give their effec- 
tive a power imag processors, cnn model fail 
almost complet at pgm reason problem (tabl 1), 
achiev perform margin good than our baselin - 
the context-blind resnet model which be blind to the con- 
text and train on onli the eight candid answers. the 
abil of the lstm to consid individu candid panel 
in sequenc yield a small improv rel to the 
cnn. the best perform resnet variant be resnet-50, 
which outperform the lstm. resnet-50 have significantli 
more convolut layer than our simpl cnn model, and 
henc have a great capac for reason about it input 
features. 

the best perform model be the wren model. thi 
strong perform may be partli due to the relat net- 
work module, which be be design explicitli for rea- 
sone about the relat between objects, and partli due 
to the score structure. note that the score structur 
be not suffici to explain the improv perform a 



measur abstract reason in neural network 

(a) (b) 

figur 4. the effect of distraction. In both pgms, the un- 
derli structur S be [[shape, colour, consist 
union]], but (b) includ distract on shape-number, 
shape-type, line-color, and line-type. 

the wren model substanti outperform the best wild- 
resnet model, which also have a score structure. 

4.2. perform on differ question type 

question involv a singl [r, o, a] tripl be easi 
than those involv multipl triples. interestingly, pgm 
with three tripl prove more difficult than those with 
four. although the problem be appar more complex 
with four triples, there be also more avail evid for 
ani solution. among pgm involv a singl triple, OR 
(64.7%) prove to be an easi relat than xor (53.2%). 
pgm with structur involv line (78.3%) be easi 
than those involv shape (46.2%) and those involv 
shape-numb be much easi (80.1%) than those 
involv shape-s (26.4%).thi suggest that the 
model struggl to discern fine-grain differ in size 
compar to more salient chang such a the absenc or 
presenc of lines, or the quantiti of shapes. for more detail 
of perform by question type, see appendix tabl 7, 8. 

4.3. effect of distractor 

the result report thu far be on question that includ 
distractor attribut valu (see fig. 4). the wren model 
perform notabl good when these distractor be re- 
move (79.3% on the valid and 78.3% on the test set, 
compar with 63.0% and 62.6% with distractors). 

4.4. generalis 

We compar the best perform wren model on each of 
the generalis regim (tabl 1), and observ notabl 
differ in the abil of the model to generalise. interpo- 

lation be the least problemat regim (generalis error 
14.6%). note that perform on both the interpol 
and extrapol train set be high than on the neu- 
tral train set becaus certain attribut (size, colour) 
have half a mani valu in those cases, which reduc the 
complex of the task.3 

after interpolation, the model generalis best in regim 
where the test question involv novel combin of oth- 
erwis familiar [r, o, a] tripl (held-out attribut pair and 
held-out tripl pairs). thi indic that the model learn 
to combin relat and attributes, and do not simpli mem- 
oriz combin of tripl a distinct structur in their 
own right. however, bad generalis in the case of 
held-out tripl suggest that the model be less abl to in- 
duce the mean of unfamiliar tripl from it knowledg of 
their constitu components. moreover, it could not under- 
stand relat instanti on entir novel attribut (held- 
out line-typ , held-out shape-colour). the bad 
generalis be observ on the extrapol regime. 
given that these question have the same abstract semant 
structur a interpol questions, the failur to generalis 
may stem from the model’ failur to perceiv input outsid 
of the rang of it prior experience. 

4.5. effect of auxiliari train 

We then explor the impact of auxiliari train on ab- 
stract reason and generalis by train our model 
with symbol meta target a describ in section 3.1. In 
the neutral regime, we found that auxiliari train lead to 
a 13.9% improv in test accuracy. critically, thi im- 
provement in the overal abil of the model to captur 
the data also appli to other generalis regimes. the 
differ be clearest in the case where the model be 
requir to recombin familiar tripl into novel combina- 
tions: (56.3% accuraci on held-out tripl pairs, up from 
41.9%, and 51.7% accuraci on held-out attribut pairs, up 
from 27.2%). thus, the pressur to repres abstract se- 
mantic principl such that they can be decod simpli into 
discret symbol explan seem to improv the abil 
of the model to product compos it knowledge. thi 
find align with previou observ about the benefit 
of discret channel for knowledg represent (andrea 
et al., 2016) and the benefit of induc explan or 
rational (ling et al., 2017). 

4.6. analysi of auxiliari train 

In addit to improv performance, train with meta- 
target provid a mean to measur which shapes, attributes, 

3sinc test question focu on held-out phenomena, test set in 
differ regim may have differ underli complexity. ab- 
solut perform cannot therefor be compar across differ 
regimes. 



measur abstract reason in neural network 

β = 0 β = 10 

model test (%) regim val. (%) test (%) diff. val. (%) test (%) diff. 
wren 62.6 neutral 63.0 62.6 -0.6 77.2 76.9 -0.3 

wild-resnet 48.0 interpol 79.0 64.4 -14.6 92.3 67.4 -24.9 
resnet-50 42.0 h.o. attribut pair 46.7 27.2 -19.5 73.4 51.7 -21.7 

lstm 35.8 h.o. tripl pair 63.9 41.9 -22.0 74.5 56.3 -18.2 
cnn + mlp 33.0 h.o. tripl 63.4 19.0 -44.4 80.0 20.1 -59.9 
blind resnet 22.4 h.o. line-typ 59.5 14.4 -45.1 78.1 16.4 -61.7 

h.o. shape-colour 59.1 12.5 -46.6 85.2 13.0 -72.2 
extrapol 69.3 17.2 -52.1 93.6 15.5 -78.1 

tabl 1. perform of all model on the neutral split (left), and generalis perform of the wren model (right) with generalis 
regim order accord to generalis error for β = 0. context-blind resnet generalis test perform for all regim be 
give in tabl 9 of the appendix. (diff: differ between test and valid performance, h.o:“held-out”) 

and relat the model believ be present in a give pgm, 
provid insight into the model’ decisions. use these 
predictions, we ask how the wren model’ accuraci var- 
i a a function of it meta-target predictions. unsurpris- 
ingly, the wren model achiev a test accuraci of 87.4% 
when it meta-target predict be correct, compar to 
onli 34.8% when it predict be incorrect. 

the meta-target predict can be broken down into pre- 
diction of object, attribute, and relat types. We lever- 
age these fine-grain predict to ask how the wren 
model’ accuraci vari a a function of it predict on 
each of these properti independently. the model accu- 
raci increas somewhat when the shape meta-target pre- 
diction be correct (78.2%) compar to be incorrect 
(62.2%), and when attribut meta-target predict be cor- 
rect (79.5%) compar to be incorrect (49.0%). how- 
ever, for the relat property, the differ between a 
correct and incorrect meta-target predict be substanti 
(86.8% vs. 32.1%). thi result suggest that predict the 
relat properti correctli be most critic to task success. 

the model’ predict certainty, defin a the mean ab- 
solut differ of the meta-target predict from 0.5, 
be predict of the model’ performance, suggest that 
the meta-target predict certainti be an accur measur 
of the model’ confid in an answer choic (figur 5; 
qualit similar for sub-targets; appendix figur 6-8). 

5. relat work 
variou comput model for solv rpm have be 
propos in the cognit scienc literatur (see (lovett & 
forbus, 2017) for a thorough review). the emphasi in these 
studi be on understand the oper and comparison 
commonli appli by humans. they typic factor out 
raw percept in favour of symbol inputs, and hard-cod 
strategi describ by cognit theories. In contrast, we 

figur 5. relationship between answer accuraci and meta- 
target predict certainti for the wren model (β = 10). the 
wren model be more accur when it be more confid about 
it meta-target predictions. certainti be defin a the mean 
absolut differ of the meta-target predict from 0.5. 

consid model that process input from raw pixel and 
studi how they infer, from knowledg of the correct answer, 
the process and represent necessari to resolv the 
task. much a we do, hoshen & werman (2017) train 
neural network to complet the row or column of raven- 
style matrix from raw pixels. they found that a cnn- 
base model induc visual relat such a rotat or 
reflection, but they do not address the problem of resolv 
complet rpms. our experi show that such model 
perform poorli on full rpm questions. moreover, hoshen 
& werman (2017) do not studi generalis to question 
that differ substant from their train data. wang & Su 
(2015) present a method for automat gener raven- 
style matrix and verifi their gener on humans, but do 
not attempt ani modelling. our method for automat 
gener rpm-style question borrow extens from 
the insight in that work. 

there be prior work emphasis both the advantag (clark 
& etzioni, 2016) and limit (davis, 2014) of apply- 



measur abstract reason in neural network 

ing standard test in AI (see marcu et al. (2016) and 
contribut articl for a review). approach base on stan- 
dardiz test gener focu on measur the gener 
knowledg of systems, while we focu on models’ abil 
to gener learn information. 

6. discuss 
one of the long-stand goal of artifici intellig be to 
develop machin with abstract reason capabl that 
equal or good those of humans. though there have also be 
substanti progress in both reason and abstract represen- 
tation learn in neural net (botvinick et al., 2017; lecun 
et al., 2015; higgin et al., 2016; 2017), the extent to which 
these model exhibit anyth like gener abstract reason- 
ing be the subject of much debat (garnelo et al., 2016; lake 
& baroni, 2017; marcus, 2018). the research present 
here be therefor motiv by two main goals. (1) To 
understand whether, and (2) to understand how, deep neural 
network might be abl to solv abstract visual reason 
problems. 

our answer to (1) be that, with import caveats, neural 
network can inde learn to infer and appli abstract reason- 
ing principles. our best perform model learn to solv 
complex visual reason questions, and to do so, it need 
to induc and detect from raw pixel input the presenc of 
abstract notion such a logic oper and arithmet 
progressions, and appli these principl to never-befor 
observ stimuli. importantly, we found that the architec- 
ture of the model make a critic differ to it abil to 
learn and execut such processes. while standard visual- 
process model such a cnn and resnet perform 
poorly, a model that promot the represent of, and 
comparison between part of the stimulu perform veri 
well. We found way to improv thi perform via addi- 
tional supervision: the train outcom and the model’ 
abil to generalis be improv if it be requir to 
decod it represent into symbol correspond to 
the reason behind the correct answer. 

when consid (2), it be import to note that our model 
be solv a veri differ problem from that solv by 
human subject take raven-styl IQ tests. the model’ 
world be highli constrained, and it experi consist 
of a small number of possibl relat instanti in finit 
set of attribut and valu across hundr of thousand 
of examples. It be highli unlik that the model’ solut 
match those appli by success humans. thi differ 
becom clear when we studi the abil of the model to 
generalise. unlik humans, who must transfer knowledg 
distil from their experi in everyday life to the un- 
familiar set of visual reason problems, our model 
exhibit transfer across question set with a high degre 
of perceptu and structur uniformity. when requir to 

interpol between know attribut values, and also when 
appli know abstract content in unfamiliar combina- 
tions, the model generalis notabl well. even within 
thi constrain domain, however, they perform strikingli 
poorli when requir to extrapol to input beyond their 
experience, or to deal with entir unfamiliar attributes. 

In thi latter behaviour, the model differ in a crucial way 
from humans; a human that could appli a relat such a 
xor to the colour of line would almost certainli have no 
troubl appli it to the colour of shapes. On the other 
hand, even the human abil to extend appar well- 
defin principl to novel object have limits; thi be pre- 
cise whi rpm be such an effect discrimin of 
human iq. for instance, a human subject might be uncertain 
what it mean to appli xor to the size or shape of set of 
objects, even if he or she have learn to do so perfectli in 
the case of colors. 

An import contribut of thi work be the introduct 
of the pgm dataset, a a tool for studi both abstract 
reason and generalis in models. generalis be 
a multi-facet phenomenon; there be no single, object 
way in which model can or should generalis beyond their 
experience. the pgm dataset provid a mean to measur 
the gener abil of model in differ ways, each 
of which may be more or less interest to research 
depend on their intend train setup and applications. 

design and instanti meaning train/test distinc- 
tion to studi generalis in the pgm dataset be simpli- 
fie by the object semant of the underli gener 
model. similar principl could be appli to more natural- 
istic data, particularli with crowdsourc human input. for 
instance, imag process model could be train to iden- 
tifi black hors and test on whether they can detect white 
horses, or train to detect fli seagulls, fli sparrow 
and nest seagulls, and test on the detect of nest 
sparrows. thi approach be take for one particular gener- 
alis regim by ramakrishnan et al. (2017), who test 
vqa model on imag contain object that be not 
observ in the train data. the pgm dataset extend and 
formalis thi approach, with regim that focu not onli 
on how model could respond to novel factor or class in 
the data, but also novel combin of know factor etc. 

In the next stage of thi research, we will explor strategi 
for improv generalisation, such a meta-learning, and 
will further explor the use of richli structured, yet gener- 
alli applicable, induct biases. We also hope to develop a 
deeper understand of the solut learn by the wren 
model when solv raven-styl matrices. finally, we wish 
to end by invit our colleagu across the machin learn- 
ing commun to particip in our new abstract reason 
challenge. 



measur abstract reason in neural network 

acknowledg 

We would like to thank david raposo, daniel zoran, murray 
shanahan, sergio gomez, yee whye teh and daan wierstra 
for help discuss and all the deepmind team for their 
support. 

refer 
andreas, j., klein, d., and levine, S. modular multitask re- 

inforc learn with polici sketches. arxiv preprint 
arxiv:1611.01796, 2016. 

botvinick, m., barrett, d., battaglia, p., de freitas, n., 
kumaran, d., leibo, j., lillicrap, t., modayil, j., mo- 
hamed, s., rabinowitz, n., et al. build machin that 
learn and think for themselves: commentari on lake et 
al., behavior and brain sciences, 2017. arxiv preprint 
arxiv:1711.08378, 2017. 

carpenter, P. a., just, M. a., and shell, P. what one intelli- 
genc test measures: a theoret account of the process- 
ing in the raven progress matrix test. psycholog 
review, 97(3):404, 1990. 

clark, P. and etzioni, O. My comput be an honor student- 
but how intellig be it? standard test a a measur 
of ai. AI magazine, 37(1):5–12, 2016. 

davis, E. the limit of standard scienc test a 
benchmark for artifici intellig research: posit 
paper. arxiv preprint arxiv:1411.1629, 2014. 

fleuret, f., li, t., dubout, c., wampler, E. k., yantis, s., 
and geman, D. compar machin and human on a 
visual categor test. proceed of the nation 
academi of sciences, 108(43):17621–17625, 2011. 

flynn, J. R. massiv iq gain in 14 nations: what iq test 
realli measure. psycholog bulletin, 101(2):171, 1987. 

garnelo, m., arulkumaran, k., and shanahan, M. toward 
deep symbol reinforc learning. arxiv preprint 
arxiv:1609.05518, 2016. 

he, k., zhang, x., ren, s., and sun, J. deep residu learn- 
ing for imag recognition. In proceed of the ieee 
confer on comput vision and pattern recognition, 
pp. 770–778, 2016. 

higgins, i., matthey, l., pal, a., burgess, c., glorot, x., 
botvinick, m., mohamed, s., and lerchner, A. beta- 
vae: learn basic visual concept with a constrain 
variat framework. 2016. 

higgins, i., sonnerat, n., matthey, l., pal, a., burgess, 
C. p., botvinick, m., hassabis, d., and lerchner, A. scan: 
learn abstract hierarch composit visual con- 
cepts. arxiv preprint arxiv:1707.03389, 2017. 

hochreiter, S. and schmidhuber, J. long short-term memory. 
neural comput., 9(8):1735–1780, novemb 1997. issn 
0899-7667. doi: 10.1162/neco.1997.9.8.1735. 

hoshen, D. and werman, M. Iq of neural networks. arxiv 
preprint arxiv:1710.01692, 2017. 

jaeggi, S. m., buschkuehl, m., jonides, j., and perrig, W. J. 
improv fluid intellig with train on work 
memory. proceed of the nation academi of sci- 
ences, 105(19):6829–6833, 2008. 

jo, J. and bengio, Y. measur the tendenc of cnn 
to learn surfac statist regularities. arxiv preprint 
arxiv:1711.11561, 2017. 

johnson, j., hariharan, b., van der maaten, l., fei-fei, l., 
zitnick, C. l., and girshick, R. clevr: A diagnost 
dataset for composit languag and elementari visual 
reasoning. In comput vision and pattern recogni- 
tion (cvpr), 2017 ieee confer on, pp. 1988–1997. 
ieee, 2017. 

kingma, D. P. and ba, J. adam: A method for stochast 
optimization. arxiv preprint arxiv:1412.6980, 2014. 

lake, B. M. and baroni, M. still not systemat af- 
ter all these years: On the composit skill of 
sequence-to-sequ recurr networks. arxiv preprint 
arxiv:1711.00350, 2017. 

lecun, y., bengio, y., and hinton, G. deep learning. nature, 
521(7553):436, 2015. 

ling, w., yogatama, d., dyer, c., and blunsom, P. pro- 
gram induct by rational generation: learn to solv 
and explain algebra word problems. arxiv preprint 
arxiv:1705.04146, 2017. 

lovett, A. and forbus, K. model visual problem solv 
a analog reasoning. psycholog review, 124(1):60, 
2017. 

marcus, G. deep learning: A critic appraisal. arxiv 
preprint arxiv:1801.00631, 2018. 

marcus, g., rossi, f., and veloso, M. beyond the ture 
test. Ai magazine, 37(1):3–4, 2016. 

ramakrishnan, S. k., pal, a., sharma, g., and mittal, A. 
An empir evalu of visual question answer for 
novel objects. arxiv preprint arxiv:1704.02516, 2017. 

raposo, d., santoro, a., barrett, d., pascanu, r., lilli- 
crap, t., and battaglia, P. discov object and their 
relat from entangl scene representations. 2017. 

raven, J. C. et al. raven’ progress matrices. western 
psycholog services, 1938. 



measur abstract reason in neural network 

santoro, a., raposo, d., barrett, D. g., malinowski, m., 
pascanu, r., battaglia, p., and lillicrap, T. A simpl neu- 
ral network modul for relat reasoning. In advanc 
in neural inform process systems, pp. 4974–4983, 
2017. 

snow, R. e., kyllonen, P. c., and marshalek, B. the topog- 
raphi of abil and learn correlations. advanc in 
the psycholog of human intelligence, 2( 47):103, 1984. 

szegedy, c., zaremba, w., sutskever, i., bruna, j., erhan, 
d., goodfellow, i., and fergus, R. intrigu properti of 
neural networks. arxiv preprint arxiv:1312.6199, 2013. 

Te nijenhuis, j., voskuijl, O. f., and schijve, N. B. practic 
and coach on iq tests: quit a lot of g. intern 
journal of select and assessment, 9(4):302–308, 2001. 

wang, K. and su, Z. automat gener of raven pro- 
gressiv matrices. In twenty-fourth intern joint 
confer on artifici intelligence, 2015. 

zaremba, w., sutskever, i., and vinyals, O. recurr neural 
network regularization. arxiv preprint arxiv:1409.2329, 
2014. 

zhang, c., bengio, s., hardt, m., recht, b., and vinyals, O. 
understand deep learn requir rethink general- 
ization. arxiv preprint arxiv:1611.03530, 2016. 



measur abstract reason in neural network 

A. appendix 
a.1. pgm dataset 

altogeth there be 1.2m train set questions, 20k vali- 
dation set questions, and 200k test set questions. 

when creat the matrix we aim to use the full carte- 
sian productr×a for construct structur S . however, 
some relation-attribut combin be problematic, such 
a a progress on line type, and some attribut interact in 
interest way (such a number and position, which be 
in some sens tied), restrict the type of relat we can 
appli to these attributes. the final list of relev relat 
per attribut type, broken down by object type (shape vs. 
line) is: 

shape: 
size: progression, xor, or, and, consist union 
color: progression, xor, or, and, consist union 
number: progression, consist union 
position: xor, or, and 
type: progression, xor, or, and, consist union 

line: 
color: progression, xor, or, and, consist union 
type: xor, or, and, consist union 

sinc the number and posit attribut type be tie (for 
example, have an arithmet progress on number whilst 
have an xor relat on posit be not possible), we for- 
bid number and posit from co-occur in the same ma- 
trix. otherwise, all other ((r, o, a), (r, o, a)) combin 
occur unless specif control for in the generalisa- 
tion regime. 

We creat a similar list for possibl valu for a give 
attribute: 

shape: 
color: 10 evenli space greyscal intens in [0, 1] 
size: 10 scale factor evenli space in [0, 1] 4 
number: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 
posit ((x, y) coordin in a (0, 1) plot): 

(0.25, 0.75), 
(0.75, 0.75), 
(0.75, 0.25), 
(0.25, 0.25), 
(0.5, 0.5), 
(0.5, 0.25), 
(0.5, 0.75), 
(0.25, 0.5), 
(0.75, 0.5) 

type: circle, triangle, square, pentagon, hexagon, 
4the actual specif valu use for size be number particular 

to the matplotlib implement of the plots, and henc depend 
on the scale of the plot and axes, etc. 

octagon, star 

line: 
color: 10 evenli space greyscal intens in [0, 1] 
type: diagon down, diagon up, vertical, horizontal, 

diamond, circl 

a.2. exampl of raven-styl pgm 

given the radic differ way in which visual reason 
test be appli to human (no prior experience) and to our 
model (control train and test splits), we believ it 
would be mislead to provid a human baselin for our 
results. however, for a sens of the difficulti of the task, 
we present here a set of 18 question gener from the 
neutral splits. note that the valu be filter for human 
readability. In the dataset there be 10 greyscal intens 
valu for shape and line colour and 10 size for each shape. 
In the following, we restrict to 4 clearly-distinct valu for 
each of these attributes. best view on a digit monitor, 
zoom in (see next page). inform human test reveal 
wide variability: particip with a lot of experi with 
the test could score well (> 80%), while other who come 
to the test blind would often fail to answer all the questions. 



measur abstract reason in neural network 

A B C D 

E F G H 

A B C D 

E F G H 

A B C D 

E F G H 

(1) (2) (3) 

A B C D 

E F G H 

A B C D 

E F G H 

A B C D 

E F G H 

(4) (5) (6) 

A B C D 

E F G H 

A B C D 

E F G H 

A B C D 

E F G H 

(7) (8) (9) 



measur abstract reason in neural network 

A B C D 

E F G H 

A B C D 

E F G H 

A B C D 

E F G H 

(10) (11) (12) 

A B C D 

E F G H 

A B C D 

E F G H 

A B C D 

E F G H 

(13) (14) (15) 

A B C D 

E F G H 

A B C D 

E F G H 

(16) (17) (18) 

A B C D 

E F G H 



measur abstract reason in neural network 

B. model detail 
here we provid addit detail for all our models, includ- 
ing the exact hyper-paramet set that we considered. 
throughout thi section, we will use the notat [x, y, z, w] 
to describ cnn and mlp size. for a cnn, thi notat 
refer to the number of kernel per layer: x kernel in the 
first layer, y kernel in the second layer, z kernel in the 
third layer and w kernel in the fourth layer. for the mlp, it 
refer to the number of unit per layer: x unit in the first 
layer, y unit in the second layer, z unit in the third layer 
and w unit in the fourth layer. 

all model be train use the adam optimiser, with 
expoenti decay rate paramet β1 = 0.9, β2 = 0.999, � = 
10−8. We also use a distribut train setup, use 4 
gpu-work per model. 

hyper-paramet 
cnn kernel [64, 64, 64, 64] 

cnn kernel size 3× 3 
cnn kernel stride 2 

mlp hidden-lay size 1500 
mlp drop-out fraction 0.5 

batch size 16 
learn rate 0.0003 

tabl 2. cnn-mlp hyper-paramet 

hyper-paramet 
batch size 32 

learn rate 0.0003 

tabl 3. resnet-50 and context-blind resnet hyper-paramet 

hyper-paramet 
cnn kernel [8, 8, 8, 8] 

cnn kernel size 3× 3 
cnn kernel stride 2 

lstm hidden layer size 96 
drop-out fraction 0.5 

batch size 16 
learn rate 0.0001 

tabl 4. lstm hyper-paramet 

hyper-paramet 
cnn kernel [32, 32, 32, 32] 

cnn kernel size 3× 3 
cnn kernel stride 2 

RN emb size 256 
RN gθ mlp [512, 512, 512, 512] 
RN fφ mlp [256, 256, 13] 

drop-out fraction 0.5 
batch size 32 

learn rate 0.0001 

tabl 5. wren hyper-paramet 

hyper-paramet 
batch size 16 

learn rate 0.0003 

tabl 6. wild-resnet hyper-paramet 



measur abstract reason in neural network 

C. result 

# relat wren (%) blind (%) 
one 68.5 23.6 
two 51.1 21.2 

three 44.5 22.1 
four 48.4 23.5 
all 62.6 22.8 

tabl 7. wren test perform and context-blind resnet per- 
formanc after train on the neutral pgm dataset, broken down 
accord to the number of relat per matrix. 

wren (%) blind (%) 
OR 64.7 30.1 

and 63.2 17.2 
consist union 60.1 28.0 

progress 55.4 15.7 
xor 53.2 20.2 

number 80.1 18.1 
posit 77.3 27.5 

type 61.0 28.1 
color 58.9 18.7 
size 26.4 16.3 
line 78.3 27.5 

shape 46.2 18.6 
all singl relat 68.5 23.6 

tabl 8. wren test perform and context-blind resnet per- 
formanc for single-rel pgm question after train on the 
neutral pgm dataset, broken down accord to the relat type, 
attribut type and object type in a give matrix. 

figur 6. relationship between answer accuraci and shape 
meta-target predict certainty. the wren model (β = 10) 
be more accur when confid about it meta-target predictions. 
certainti be defin a the mean absolut differ of the pre- 
diction from 0.5. 

figur 7. relationship between answer accuraci and attribut 
meta-target predict certainti 

figur 8. relationship between answer accuraci and relat 
meta-target predict certainti 



measur abstract reason in neural network 

test (%) 

regim β = 0 β = 10 
neutral 22.4 13.5 

interpol 18.4 12.2 
h.o. attribut pair 12.7 12.3 

h.o. tripl pair 15.0 12.6 
h.o. tripl 11.6 12.4 

h.o. line-typ 14.4 12.6 
h.o. shape-colour 12.5 12.3 

extrapol 14.1 13.0 

tabl 9. perform of the context-blind resnet model for all 
the gener regimes, in the case where there be an addit 
auxiliari meta-target (β = 10) and in the case where there be no 
auxiliari meta-target (β = 0). note that most of these valu be 
either close to chanc or slightli abov chance, indic that 
thi baselin model struggl to learn solut that generalis 
good than a random guess solution. for sever generalis 
regim such a interplolation, h.o attribut pairs, h.o. tripl 
and h.o tripl pair the generalis perform of the wren 
model report in tabl 1 be far great than the generalis per- 
formanc of our context-blind baseline, indic that the wren 
generalis cannot be account for with a context-blind solu- 
tion. 



measur abstract reason in neural network 

figur 9. answer key to puzzl in section a.2 


