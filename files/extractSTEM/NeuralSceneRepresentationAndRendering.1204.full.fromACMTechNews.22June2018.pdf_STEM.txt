




































scienc journal — aaa 


research articl 
◥ 

machin learn 

neural scene represent 
and render 
S. M. ali eslami*†, danilo jimenez rezende†, freder besse, fabio viola, 
ari S. morcos, marta garnelo, avraham ruderman, andrei A. rusu, ivo danihelka, 
karol gregor, david P. reichert, lar buesing, theophan weber, oriol vinyals, 
dan rosenbaum, neil rabinowitz, helen king, chloe hillier, matt botvinick, 
daan wierstra, koray kavukcuoglu, demi hassabi 

scene representation—th process of convert visual sensori data into concis 
descriptions—i a requir for intellig behavior. recent work have show that neural 
network excel at thi task when provid with large, label datasets. however, remov 
the relianc on human label remain an import open problem. To thi end, we 
introduc the gener queri network (gqn), a framework within which machin learn 
to repres scene use onli their own sensors. the gqn take a input imag of a 
scene take from differ viewpoints, construct an intern representation, and us thi 
represent to predict the appear of that scene from previous unobserv 
viewpoints. the gqn demonstr represent learn without human label or 
domain knowledge, pave the way toward machin that autonom learn to 
understand the world around them. 

M 
odern artifici vision system be base 
on deep neural network that consum 
large, label dataset to learn function 
that map imag to human-gener 
scene descriptions. they do so by, for ex- 

ample, categor the domin object in the 
imag (1), classifi the scene type (2), detect 
object-bound box (3), or label individu 
pixel into predetermin categori (4, 5). In 
contrast, intellig agent in the natur world 
appear to requir littl to no explicit supervis 
for percept (6). higher mammals, includ 
human infants, learn to form represent 
that support motor control, memory, planning, 
imagination, and rapid skill acquisit without 
ani social communication, and gener pro- 

ce have be hypothes to be instrumen- 
tal for thi abil (7–10). It be thu desir to 
creat artifici system that learn to repres 
scene by model data [e.g., two-dimension 
(2d) imag and the agent’ posit in space] 
that agent can directli obtain while process 
the scene themselves, without recours to se- 
mantic label (e.g., object classes, object loca- 
tions, scene types, or part labels) provid by a 
human (11). 
To that end, we present the gener queri 

network (gqn). In thi framework, a an agent 
navig a 3D scene i, it collect K imag xki 
from 2D viewpoint vki , which we collect 
refer to a it observationsoi ¼ fðxki ; vki þgk¼1;…;k. 
the agent pass these observ to a gqn 

compos of two main parts: a represent 
network f and a generationnetwork g (fig. 1). the 
representationnetwork take a input the agent’ 
observ and produc a neural scene rep- 
resent r, which encod inform about 
the underli scene (we omit scene subscript i 
where possible, for clarity). each addit ob- 
servat accumul further evid about 
the content of the scene in the same represen- 
tation. the gener network then predict 
the scene from an arbitrari queri viewpoint vq, 
use stochast latent variabl z to creat vari- 
abil in it output where necessary. the two 
network be train jointly, in an end-to-end 
fashion, to maxim the likelihood of generat- 
ing the ground-truth imag that would be ob- 
serv from the queri viewpoint. more formally, 
(i) r ¼ fqðoiþ, (ii) the deep gener network 
g defin a probabl densiti gqðxjvq; rÞ ¼ 
∫gqðx; zjvq; rþdzof an imag x be observ at 
queri viewpoint vq for a scene represent r 
use latent variabl z, and (iii) the learnabl 
paramet be denot by q. although the gqn 
train object be intractable, owe to the 
presenc of latent variables, we can employ var- 
iation approxim and optim with sto- 
chastic gradient descent. 
the represent network be unawar of the 

viewpoint that the gener network will be 
queri to predict. As a result, it will produc 
scene represent that contain all informa- 
tion (e.g., object identities, positions, colors, 
counts, and room layout) necessari for the gen- 
erat to make accur imag predictions. In 
other words, the gqn will learn by itself what 
these factor are, a well a how to extract them 
from pixels. moreover, the gener intern 
ani statist regular (e.g., typic color 
of the sky, a well a object shape regular 
and symmetries, patterns, and textures) that 
be common across differ scenes. thi allow 

research 

eslami et al., scienc 360, 1204–1210 (2018) 15 june 2018 1 of 7 

deepmind, 5 new street square, london ec4a 3tw, uk. 
*correspond author. email: aeslami@google.com 
†these author contribut equal to thi work. 

fig. 1. schemat illustr of 
the gener queri network. 
(a) the agent observ train 
scene i from differ viewpoint 

(in thi example, from v1i , v 
2 
i , and v 

3 
i ). 

(b) the input to the representa- 
tion network f be observ 

make from viewpoint v1i and v 
2 
i , 

and the output be the scene repre- 
sentat r, which be obtain by 
element-wis sum of the 
observations’ representations. the 
gener network, a recurr 
latent variabl model, us the 
represent to predict what the 

scene would look like from a differ viewpoint v3i . the gener can succeed onli if r contain accur and complet inform about the content of 
the scene (e.g., the identities, positions, colors, and count of the objects, a well a the room’ colors). train via back-propag across mani 
scenes, random the number of observations, lead to learn scene represent that captur thi inform in a concis manner. onli a hand 
of observ need to be record from ani singl scene to train the gqn. h1, h2,…hl be the L layer of the gener network. 

on june 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nload from 


http://science.sciencemag.org/ 


the gqn to reserv it represent capac 
for a concise, abstract descript of the scene, 
with the gener fill in the detail where 
necessary. for instance, instead of specifi 
the precis shape of a robot arm, the represen- 
tation network can succinctli commun the 
configur of it joints, and the gener 
know how thi high-level represent mani- 
fest itself a a fulli render arm with it pre- 
cise shape and colors. In contrast, voxel (12–15) 
or point-cloud (16) method (a typic obtain 
by classic structure-from-motion) employ lit- 
eral represent and therefor typic scale 
poorli with scene complex and size and be 
also difficult to appli to nonrigid object (e.g., 
animals, vegetation, or cloth). 

room with multipl object 

To evalu the feasibl of the framework, we 
experimentedwith a collect of environ 
in a simul 3D environment. In the first set 

of experiments, we consid scene in a squar 
roomcontain a varieti of objects.wal textures— 
aswel a the shapes, positions, and color of the 
object and lights—ar randomized, allow 
for an effect infinit number of total scene 
configurations; however, we use finit dataset 
to train and test the model [see section 4 of (17) 
for details]. after training, the gqn comput 
it scene represent by observ one ormor 
imag of a previous unencountered, held-out 
test scene. with thi representation, which can 
be a small a 256 dimensions, the generator’ 
predict at queri viewpoint be highli accu- 
rate and mostli indistinguish from ground 
truth (fig. 2a). the onlyway inwhich themodel 
can succeed at thi task be by perceiv and com- 
pactli encod in the scene represent vector 
r the number of object present in each scene, 
their posit in the room, the color in which 
they appear, the color of the walls, and the in- 
directli observ posit of the light source. 

unlik in tradit supervis learning, gqn 
learn to make these infer from imag 
without ani explicit human label of the con- 
tent of scenes. moreover, the gqn’ gener 
learn an approxim 3D render (in other 
words, a program that can gener an imag 
when give a scene represent and camera 
viewpoint) without ani prior specif of 
the law of perspective, occlusion, or light 
(fig. 2b). when the content of the scene be 
not explicitli specifi by the observ (e.g., 
becaus of heavi occlusion), the model’ un- 
certainti be reflect in the variabl of the 
generator’ sampl (fig. 2c). these properti 
be best observ in real-time, interact query- 
ing of the gener (movi s1). 
notably, themodel observ onli a small num- 

ber of imag (in thi experiment, few than five) 
from each scene dure training, yet it be capa- 
ble of render unseen train or test scene 
from arbitrari viewpoints. We also monitor 

eslami et al., scienc 360, 1204–1210 (2018) 15 june 2018 2 of 7 

fig. 2. neural scene represent and rendering. (a) after have 
make a singl observ of a previous unencount test scene, the 
represent network produc a neural descript of that scene. 
given thi neural description, the gener be capabl of predict accur 
imag from arbitrari queri viewpoints. thi impli that the scene 
descript captur the identities, positions, colors, and count of the 
objects, a well a the posit of the light and the color of the room. (b) the 
generator’ predict be consist with law of perspective, occlusion, 

and light (e.g., cast object shadow consistently).when observ 
provid view of differ part of the scene, the gqn correctli aggreg 
thi inform (scene two and three). (c) sampl variabl indic 
uncertainti over scene content (in thi instance, owe to heavi occlusion). 
sampl depict plausibl scenes, with complet object render in 
vari posit and color (see fig. S7 for further examples).th model’ 
behavior be best visual in movi format; see movi S1 for real-time, 
interact queri of gqn’ represent of test scenes. 

research | research articl 
on june 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nload from 


http://science.sciencemag.org/ 


the likelihood of predict observ of train- 
ing and test scene (fig. s3) and found no notice- 
abl differ between valu of the two. taken 
together, these point rule out the possibl of 
model overfitting. 
analysi of the train gqn highlight sev- 

eral desir properti of it scene represen- 
tation network. two-dimension t-distribut 
stochast neighbor emb (t-sne) (18) vi- 
sualiz of gqn scene represent vector 
show clear cluster of imag of the same 
scene, despit mark chang in viewpoint 
(fig. 3a). In contrast, represent produc 
by autoencod densiti model such a var- 
iation autoencod (vaes) (19) appar 
fail to captur the content of the underli 
scene [section 5 of (17)]; they appear to be rep- 
resent of the observ imag instead. 
furthermore, when prompt to reconstruct a 
target image, gqn exhibit composit be- 
havior, a it be capabl of both repres and 
render combin of scene element it have 

never encount dure train (fig. 3b), de- 
spite learn that these composit be un- 
likely. To test whether the gqn learn a factor 
representation, we investig whether chang- 
ing a singl scene properti (e.g., object color) 
while keep other (e.g., object shape and po- 
sition) fix lead to similar chang in the 
scene represent (a defin by mean cosin 
similar across scenes). We found that object 
color, shape, and size; light position; and, to a 
lesser extent, object posit be inde factor- 
ize [fig. 3C and section 5.3 and 5.4 of (17)]. 
We also found that the gqn be abl to carri out 
“scene algebra” [akin to word emb algebra 
(20)]. By add and subtract represent 
of relat scenes, we found that object and scene 
properti can be controlled, even across object 
posit [fig. 4A and section 5.5 of (17)]. finally, 
becaus it be a probabilist model, gqn also 
learn to integr inform from differ 
viewpoint in an effici and consist manner, 
a demonstr by a reduct in it bayesian 

“surprise” at observ a held-out imag of a 
scene a the number of view increas [fig. 4B 
and section 3 of (17)]. We includ analysi on the 
gqn’ abil to gener to out-of-distribut 
scenes, a well a further result on model 
of shepard-metzl objects, in section 5.6 and 
4.2 of (17). 

control of a robot arm 

represent that succinctli reflect the true 
state of the environ should also allow agent 
to learn to act in those environ more ro- 
bustli and with few interactions. therefore, we 
consid the canon task of move a robot 
arm to reach a color object, to test the gqn 
representation’ suitabl for control. the end- 
goal of deep reinforc learn be to learn 
the control polici directli from pixels; however, 
such method requir a larg amount of expe- 
rienc to learn from spars rewards. instead, we 
first train a gqn and use it to succinctli rep- 
resent the observations. A polici be then train 

eslami et al., scienc 360, 1204–1210 (2018) 15 june 2018 3 of 7 

fig. 3. viewpoint invariance, compositionality, and factor of 
the learn scene representations. (a) t-sne embeddings. t-sne be a 
method for nonlinear dimension reduct that approxim preserv 
the metric properti of the origin high-dimension data. each dot 
repres a differ view of a differ scene, with color indic scene 
identity.wherea the vae cluster imag mostli on the basi of wall angles, 
gqn cluster imag of the same scene, independ of view (scene 
represent comput from each imag individually).two scene with 

the same object (repres by asterisk and dagger symbols) but in 
differ posit be clearli separated. (b) composition demonstr 
by reconstruct of holdout shape-color combinations. (c) gqn factor 
object and scene properti becaus the effect of chang a specif 
properti be similar across divers scene (a defin bymean cosin similar 
of the chang in the represent across scenes). for comparison, we 
plot chanc factorization, a well a the factor of the image-spac and 
vae representations. see section 5.3 of (17) for details. 

research | research articl 
on june 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nload from 


http://science.sciencemag.org/ 


to control the arm directli from these representa- 
tions. In thi setting, the represent network 
must learn to commun onli the arm’ joint 
angles, the posit and color of the object, and 
the color of the wall for the gener to be 
abl to predict new views. becaus thi vector 
have much low dimension than the raw 
input images, we observ substanti more ro- 
bust and data-effici polici learning, obtain 
convergence-level control perform with ap- 
proxim one-fourth a mani interact with 
the environ a a standard method use raw 

pixel [fig. 5 and section 4.4 of (17)]. the 3D 
natur of the gqn represent allow u to 
train a polici from ani viewpoint around the 
arm and be suffici stabl to allow for arm- 
joint veloc control from a freeli move camera. 

partial observ maze environ 

finally, we consideredmor complex, procedur- 
al maze-lik environ to test gqn’ scale 
properties. themaz consist of multipl room 
connect via corridors, and the layout of each 
maze and the color of the wall be random 

in each scene. In thi setting, ani singl obser- 
vation provid a small amount of inform 
about the current maze. As before, the train 
object for gqn be to predict maze from new 
viewpoints, which be possibl onli if gqn suc- 
cess aggreg multipl observ to 
determin the maze layout (i.e., the wall and 
floor colors, the number of rooms, their posit 
in space, and how they connect to one anoth 
via corridors). We observ that gqn be abl to 
make correct predict from new first-person 
viewpoint (fig. 6a). We queri the gqn’ 

eslami et al., scienc 360, 1204–1210 (2018) 15 june 2018 4 of 7 

fig. 4. scene algebra and bayesian surprise. (a) ad and subtract- 
ing represent of relat scene enabl control of object and scene 
properti via “scene algebra” and indic factor of shapes, colors, 
and positions. pred, prediction. (b) bayesian surpris at a new observ 

after have make observ 1 to k for k = 1 to 5. when the model 
observ imag that contain inform about the layout of the scene, it 
surpris (defin a the kullback-leibl diverg between condit 
prior and posterior) at observ the held-out imag decreases. 

fig. 5. gqn represent enabl more robust and data-effici 
control. (a) the goal be to learn to control a robot arm to reach a 
randomli posit color object. the control polici observ the 
scene from a fix or move camera (gray). We pretrain a gqn 
represent network by observ random configur from random 
viewpoint insid a dome around the arm (light blue). (b) the gqn 
infer a scene represent that can accur reconstruct the scene. 
(c) (left) for a fix camera, an asynchron advantag actor-crit 
reinforc learn (rl) agent (44) learn to control the arm use 
roughli one-fourth a mani experi when use the gqn representa- 
tion, a oppos to a standard method use raw pixel (line correspond 

to differ hyperparameters; same hyperparamet explor for both 
standard and gqn agents; both agent also receiv viewpoint coordin 
a inputs). the final perform achiev by learn from raw pixel 
can be slightli high for some hyperparameters, becaus some task- 
specif inform might be lose when learn a compress represen- 
tation independ from the RL task a gqn does. (right) the benefit 
of gqn be most pronounc when the polici network’ view on the 
scene move from frame to frame, suggest viewpoint invari in 
it representation. We normal score such that a random agent 
achiev 0 and an agent train on “oracle” ground-truth state inform 
achiev 100. 

research | research articl 
on june 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nload from 


http://science.sciencemag.org/ 


represent more directli by train a sep- 
arat gener to predict a top-down view of the 
maze and found that it yield highli accur 
predict (fig. 6b). themodel’ uncertainty, a 
measur by the entropi of it first-person and 
top-down samples, decreas a more observa- 
tion aremad [fig. 6B and section 3 of (17)]. after 
about onli five observations, the gqn’ uncer- 
tainti disappear almost entirely. 

relat work 

gqn offer key advantag over prior work. 
tradit structure-from-motion, structure- 

from-depth, and multiview geometri techniqu 
(12–16, 21) prescrib the way in which the 3D 
structur of the environ be repres 
(for instance, a point clouds, mesh clouds, or a 
collect of predefin primitives). gqn, by 
contrast, learn thi represent space, al- 
low it to express the presenc of textures, 
parts, objects, lights, and scene concis and 
at a suitabl high level of abstraction. further- 
more, it neural formul enabl task-specif 
fine-tun of the represent via back- 
propag (e.g., via further supervis or re- 
inforc deep learning). 

classic neural approach to thi learn 
problem—e.g., autoencod and densiti model 
(22–27)—are requir to captur onli the dis- 
tribut of observ images, and there be no 
explicit mechan to encourag learn of how 
differ view of the same 3D scene relat to 
one another. the expect be that statist 
compress principl will be suffici to en- 
abl network to discov the 3D structur of 
the environment; however, in practice, they 
fall short of achiev thi kind of meaning 
represent and instead focu on regular- 
iti of color and patch in the imag space. 

eslami et al., scienc 360, 1204–1210 (2018) 15 june 2018 5 of 7 

fig. 6. partial observ and uncertainty. (a) the agent (gqn) 
record sever observ of a previous unencount test maze 
(indic by gray triangles). It be then capabl of accur predict the 
imag that would be observ at a queri viewpoint (yellow triangle). It 
can accomplish thi task onli by aggreg inform across multipl 
observations. (b) In the kth column, we condit gqn on observ 
1 to k and show gqn’ predict uncertainty, a well a two of gqn’ 

sampl predict of the top-down view of the maze. predict 
uncertainti be measur by comput the model’ bayesian surpris at 
each location, averag over three differ head directions. the 
model’ uncertainti decreas a more observ be made. As the 
number of observ increases, the model predict the top-down view 
with increas accuracy. see section 3 of (17), fig. s8, and movi S1 for 
further detail and results. nats, natur unit of information. 

research | research articl 
on june 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nload from 


http://science.sciencemag.org/ 


viewpoint transform network do explic- 
itli learn thi relationship; however, they have 
thu far be nonprobabilist and limit in 
scale—e.g., restrict to rotat around indi- 
vidual object for which a singl view be suffi- 
cient for predict (15, 28–33) or to small camera 
displac between stereo camera (34–36). 
By employ state-of-the-art deep, iterative, 

latent variabl densiti model (25), gqn be ca- 
pabl of handl free agent movement around 
scene contain multipl objects. In addition, 
owe to it probabilist formulation, gqn can 
account for uncertainti in it understand 
about a scene’ content in the face of sever 
occlus and partial observability. notably, 
the gqn framework be not specif to the par- 
ticular choic of architectur of the gener 
network, and altern such a gener 
adversari network (37) or autoregress mod- 
el (38) could be employed. 
A close relat bodi of work be that of dis- 

crimin pose estim (39–41), in which 
network be train to predict camera motion 
between consecut frames. the gqn formu- 
lation be advantageous, a it allow for aggre- 
gation of inform from multipl imag of 
a scene (see maze experiments); it be explicitli 
probabilistic, allow for applic such a 
explor through bayesian inform gain; 
and, unlik the aforement method where 
scene represent and pose predict be in- 
tertwined, the gqn architectur admit a clear 
architectur separ between the represen- 
tation and gener networks. the idea of 
pose estim be complementary, however— 
the gqn can be augment with a second “gen- 
erator” that, give an imag of a scene, predict 
the viewpoint from which it be taken, provid- 
ing a new sourc of gradient with which to 
train the represent network. 

outlook 

In thi work, we have show that a singl neural 
architectur can learn to perceive, interpret, and 
repres synthet scene without ani human 
label of the content of these scenes. It can 
also learn a power neural render that be 
capabl of produc accur and consist 
imag of scene from new queri viewpoints. 
the gqn learn represent that adapt to 
and compactli captur the import detail of 
it environ (e.g., the positions, identities, 
and color ofmultipl objects; the configur 
of the joint angl of a robot arm; and the layout 
of amaze), without ani of these semant be 
built into the architectur of the networks. gqn 
us analysis-by-synthesi to perform “invers 
graphics,” but unlik exist method (42), 
which requir problem-specif engin in 
the design of their generators, gqn learn thi 
behavior by itself and in a gener applic 
manner. however, the result represent 
be no longer directli interpret by humans. 
our experi have thu far be restrict 

to synthet environ for three reasons: (i) a 
need for control analysis, (ii) limit availa- 
biliti of suitabl real datasets, and (iii) limit 

of gener model with current hardware. 
although the environ be rel con- 
strain in term of their visual fidelity, they 
captur mani of the fundament difficulti of 
vision—namely, sever partial observ and 
occlusion—a well a the combinatorial, multi- 
object natur of scenes. As new sourc of data 
becom avail (41) and advanc be make in 
gener model capabl (37, 43), we 
expect to be abl to investig applic of 
the gqn framework to imag of naturalist 
scenes. 
total scene understand involv more than 

just represent of the scene’ 3D structure. In 
the future, it will be import to consid broader 
aspect of scene understanding—e.g., by queri 
across both space and time for model of dy- 
namic and interact scenes—a well a appli- 
cation in virtual and augment realiti and 
explor of simultan scene represent 
and local of observations, which relat to 
the notion of simultan local and map- 
ping in comput vision. 
our work illustr a power approach to 

machin learn of ground represent of 
physic scenes, a well a of the associ per- 
ception system that holist extract these 
represent from images, pave the way 
toward fulli unsupervis scene understand- 
ing, imagination, planning, and behavior. 

refer and note 

1. A. krizhevsky, I. sutskever, G. E. hinton, in advanc in neural 
inform process system 25 (nip 2012), F. pereira, 
C. J. C. burges, L. bottou, K. Q. weinberger, eds. (curran 
associates, 2012), pp. 1097–1105. 

2. B. zhou, A. lapedriza, J. xiao, A. torralba, A. oliva, in advanc 
in neural inform process system 27 (nip 2014), 
Z. ghahramani, M. welling, C. cortes, N. D. lawrence, 
K. Q. weinberger, eds. (curran associates, 2014), pp. 487–495. 

3. S. ren, K. he, R. girshick, J. sun, in advanc in neural 
inform process system 28 (nip 2015), C. cortes, 
N. D. lawrence, D. D. lee, M. sugiyama, R. garnett, eds. 
(curran associates, 2015), pp. 91–99. 

4. R. girshick, J. donahue, T. darrell, J. malik, in proceed of 
the 2014 ieee confer on comput vision and pattern 
recognit (cvpr) (ieee, 2014), pp. 580–587. 

5. M. C. mozer, R. S. zemel, M. behrmann, in advanc in neural 
inform process system 4 (nip 1991), J. E. moody, 
S. J. hanson, R. P. lippmann, eds. (morgan-kaufmann, 1992), 
pp. 436–443. 

6. J. konorski, scienc 160, 652–653 (1968). 
7. D. marr, vision: A comput investig into the human 

represent and process of visual inform 
(henri holt and co., 1982). 

8. D. hassabis, E. A. maguire, trend cogn. sci. 11, 299–306 
(2007). 

9. D. kumaran, D. hassabis, J. L. mcclelland, trend cogn. sci. 
20, 512–534 (2016). 

10. B. M. lake, R. salakhutdinov, J. B. tenenbaum, scienc 350, 
1332–1338 (2015). 

11. S. becker, G. E. hinton, natur 355, 161–163 (1992). 
12. Z. Wu et al., in proceed of the 2015 ieee confer on 

comput vision and pattern recognit (cvpr) (ieee, 2015), 
pp. 1912–1920. 

13. J. wu, C. zhang, T. xue, W. freeman, J. tenenbaum, in advanc 
in neural inform process system 29 (nip 2016), 
D. D. lee, M. sugiyama, U. V. luxburg, I. guyon, R. garnett, 
eds. (curran associates, 2016), pp. 82–90. 

14. D. J. rezend et al., in advanc in neural inform 
process system 29 (nip 2016), D. D. lee, M. sugiyama, 
U. V. luxburg, I. guyon, R. garnett, eds. (curran associates, 
2016), pp. 4996–5004. 

15. X. yan, J. yang, E. yumer, Y. guo, H. lee, in advanc in neural 
inform process system 29 (nip 2016), D. D. lee, 

M. sugiyama, U. V. luxburg, I. guyon, R. garnett, eds. (curran 
associates, 2016), pp. 1696–1704. 

16. M. pollefey et al., int. J. comput. vision 59, 207–232 (2004). 
17. see supplementari materials. 
18. L. van der maaten, J. mach. learn. res. 9, 2579–2605 (2008). 
19. I. higgin et al., at intern confer on learn 

represent (iclr) (2017). 
20. T. mikolov et al., in advanc in neural inform process 

system 26 (nip 2013), C. J. C. burges, L. bottou, M. welling, 
Z. ghahramani, K. Q. weinberger, eds. (curran associates, 2013), 
pp. 3111–3119. 

21. Y. zhang, W. xu, Y. tong, K. zhou, acm trans. graph. 34, 159 
(2015). 

22. D. P. kingma, M. welling, arxiv:1312.6114 [stat.ml] 
(20 decemb 2013). 

23. D. J. rezende, S. mohamed, D. wierstra, in proceed of the 
31st intern confer on machin learn (icml 2014) 
(jmlr, 2014), vol. 32, pp. 1278–1286. 

24. I. goodfellow et al., in advanc in neural inform process 
system 27 (nip 2014), Z. ghahramani, M. welling, C. cortes, 
N. D. lawrence, K. Q. weinberger, eds. (curran associates, 2014), 
pp. 2672–2680. 

25. K. gregor, F. besse, D. J. rezende, I. danihelka, D. wierstra, 
in advanc in neural inform process system 29 (nip 
2016), D. D. lee, M. sugiyama, U. V. luxburg, I. guyon, 
R. garnett, eds. (curran associates, 2016), pp. 3549–3557 

26. P. vincent, H. larochelle, Y. bengio, p.-a. manzagol, in 
proceed of the 25th intern confer on machin 
learn (icml 2008) (acm, 2008), pp. 1096–1103. 

27. P. dayan, G. E. hinton, R. M. neal, R. S. zemel, neural comput. 
7, 889–904 (1995). 

28. G. E. hinton, A. krizhevsky, S. D. wang, in proceed of the 
21st intern confer on artifici neural network 
and machin learn (icann 2011), T. honkela, W. duch, 
M. girolami, S. kaski, eds. (lectur note in comput scienc 
series, springer, 2011), vol. 6791, pp. 44–51. 

29. C. B. choy, D. xu, J. gwak, K. chen, S. savarese, in 
proceed of the 2016 european confer on comput 
vision (eccv) (lectur note in comput scienc series, 
springer, 2016), vol. 1, pp. 628–644. 

30. M. tatarchenko, A. dosovitskiy, T. brox, in proceed of the 
2016 european confer on comput vision (eccv) 
(lectur note in comput scienc series, springer, 2016), 
vol. 9911, pp. 322–337. 

31. F. anselmi et al., theor. comput. sci. 633, 112–121 (2016). 
32. D. F. fouhey, A. gupta, A. zisserman, in proceed of the 2016 

ieee confer on comput vision and pattern recognit 
(cvpr) (ieee, 2016), pp. 1516–1524. 

33. A. dosovitskiy, J. T. springenberg, M. tatarchenko, T. brox, 
ieee trans. pattern anal. mach. intell. 39, 692–705 (2017). 

34. C. godard, O. mac aodha, G. J. brostow, in proceed of 
the 2017 ieee confer on comput vision and pattern 
recognit (cvpr) (ieee, 2017), pp. 6602–6611. 

35. T. zhou, S. tulsiani, W. sun, J. malik, A. A. efros, in 
proceed of the 2016 european confer on comput 
vision (eccv) (lectur note in comput scienc series, 
springer, 2016), pp. 286–301. 

36. J. flynn, I. neulander, J. philbin, N. snavely, in proceed of 
the 2016 ieee confer on comput vision and pattern 
recognit (cvpr) (ieee, 2016), pp. 5515–5524. 

37. T. karras, T. aila, S. laine, J. lehtinen, arxiv:1710.10196 [cs.ne] 
(27 octob 2017). 

38. A. van den oord et al., in advanc in neural inform 
process system 29 (nip 2016), D. D. lee, M. sugiyama, 
U. V. luxburg, I. guyon, R. garnett, eds. (curran associates, 
2016), pp. 4790–4798. 

39. D. jayaraman, K. grauman, in proceed of the 2015 ieee 
intern confer on comput vision (iccv) (ieee, 
2015), pp. 1413–1421. 

40. P. agrawal, J. carreira, J. malik, arxiv:1505.01596 [cs.cv] 
(7 may 2015). 

41. A. R. zamir et al., in proceed of the 2016 european 
confer on comput vision (eccv) (lectur note in 
comput scienc series, springer, 2016), pp. 535–553. 

42. T. D. kulkarni, P. kohli, J. B. tenenbaum, V. mansinghka, 
in proceed of the 2015 ieee confer on comput 
vision and pattern recognit (cvpr) (ieee, 2015), 
pp. 4390–4399. 

43. Q. chen, V. koltun, in proceed of the 2017 ieee 
intern confer on comput vision (iccv) (ieee, 
2017), pp. 1511–1520. 

44. A. A. rusu et al., arxiv:1610.04286 [cs.ro] (13 octob 2016). 

eslami et al., scienc 360, 1204–1210 (2018) 15 june 2018 6 of 7 

research | research articl 
on june 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nload from 


https://arxiv.org/abs/1312.6114 
https://arxiv.org/abs/1710.10196 
https://arxiv.org/abs/1505.01596 
https://arxiv.org/abs/1610.04286 
http://science.sciencemag.org/ 


acknowledg 
We thank M. shanahan, A. zisserman, P. dayan, J. leibo, 
P. battaglia, and G. wayn for help discuss and advice; 
G. ostrovski, N. heess, D. zoran, V. nair, and D. silver for 
review the paper; K. anderson for help creat 
environments; and the rest of the deepmind team for 
support and ideas. funding: thi research be fund by 
deepmind. author contributions: s.m.a.e. and d.j.r. conceiv 
of the model. s.m.a.e., d.j.r., f.b., and f.v. design and 
implement the model, datasets, visualizations, figures, and 
videos. a.s.m. and a.r. design and perform analysi 

experiments. m.g. and a.a.r. perform robot arm experiments. 
i.d., d.p.r., o.v., and d.r. assist with maze navig 
experiments. l.b. and t.w. assist with shepard-metzl 
experiments. h.k., c.h., k.g., m.b., d.w., n.r., k.k., and d.h. 
managed, advised, and contribut idea to the project. 
s.m.a.e. and d.j.r. write the paper. compet interests: the 
author declar no compet financi interests. deepmind 
have file a u.k. patent applic (gp-201495-00-pct) relat 
to thi work. data and materi availability: dataset use 
in the experi have be make avail to download at 
https://github.com/deepmind/gqn-datasets. 

supplementari materi 

www.sciencemag.org/content/360/6394/1204/suppl/dc1 
supplementari text 
figs. S1 to s16 
algorithm S1 to S3 
tabl S1 
refer (45–52) 
movi S1 

29 novemb 2017; accept 10 april 2018 
10.1126/science.aar6170 

eslami et al., scienc 360, 1204–1210 (2018) 15 june 2018 7 of 7 

research | research articl 
on june 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nload from 


https://github.com/deepmind/gqn-dataset 
http://www.sciencemag.org/content/360/6394/1204/suppl/dc1 
http://science.sciencemag.org/ 


neural scene represent and render 

hassabi 
rosenbaum, neil rabinowitz, helen king, chloe hillier, matt botvinick, daan wierstra, koray kavukcuoglu and demi 
andrei A. rusu, ivo danihelka, karol gregor, david P. reichert, lar buesing, theophan weber, oriol vinyals, dan 
S. M. ali eslami, danilo jimenez rezende, freder besse, fabio viola, ari S. morcos, marta garnelo, avraham ruderman, 

doi: 10.1126/science.aar6170 
(6394), 1204-1210.360scienc 

, thi issu p. 1204scienc 
the basi of thi representation, the network predict what the scene would look like from a new, arbitrari viewpoint. 
imag take from differ viewpoint and creat an abstract descript of the scene, learn it essentials. next, on 
dub the gener queri network (gqn), that have no need for such label data. instead, the gqn first us 

develop an artifici vision system,et al.typ use million of imag painstakingli label by humans. eslami 
To train a comput to ''recognize'' element of a scene suppli by it visual sensors, comput scientist 

A scene-intern comput program 

articl tool http://science.sciencemag.org/content/360/6394/1204 

materi 
supplementari http://science.sciencemag.org/content/suppl/2018/06/13/360.6394.1204.dc1 

content 
relat http://science.sciencemag.org/content/sci/360/6394/1188.ful 

refer 

http://science.sciencemag.org/content/360/6394/1204#bibl 
thi articl cite 15 articles, 3 of which you can access for free 

permiss http://www.sciencemag.org/help/reprints-and-permiss 

term of serviceus of thi articl be subject to the 

be a regist trademark of aaas.scienc 
license american associ for the advanc of science. No claim to origin u.s. govern works. the titl 
science, 1200 new york avenu nw, washington, DC 20005. 2017 © the authors, some right reserved; exclus 

(print issn 0036-8075; onlin issn 1095-9203) be publish by the american associ for the advanc ofscienc 

on june 27, 2018 


http://science.sciencem 
ag.org/ 

D 
ow 

nload from 


http://science.sciencemag.org/content/360/6394/1204 
http://science.sciencemag.org/content/suppl/2018/06/13/360.6394.1204.dc1 
http://science.sciencemag.org/content/sci/360/6394/1188.ful 
http://science.sciencemag.org/content/360/6394/1204#bibl 
http://www.sciencemag.org/help/reprints-and-permiss 
http://www.sciencemag.org/about/terms-servic 
http://science.sciencemag.org/ 

