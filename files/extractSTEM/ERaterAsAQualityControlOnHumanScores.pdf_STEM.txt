













































et r&d connections: e-rat a a qualiti control on human score (april 2005) 


connect 
april 2005 



listening. learning. leading. 

e-rat a a 
qualiti control on 
human score 
william monaghan and brent bridgeman 

can natur languag process evalu the 
qualiti of writing? 

should comput replac human in analyz 
student essays? 

the answer depend on whom you ask. 

oppon of autom essay evalu 
system claim that comput lack the intrins 
human capac to determin good write from 
bad. however, test organ see such 
capabl a be a necess to effici 
score essay test (flam, 2004). A suitabl 
compromis would be to have human reader 
score essay in tandem with an autom essay 
evalu system, such a the ets-develop 
e-rater®. the approach benefit those in the 
test industri by creat less relianc on 
expens read and lessen the concern of 
critics, a human reader be an integr element 
in the system. 

the debat over the efficaci of use an essay 
format in test have a long histori (cooper, 1984). 
test programs, such a the graduat record 
examinations® (gre®) program, have come to 
recogn that essay can play an import role 
a indic of student abil and have add 
essay section (powers, fowles, & welsh, 1999). 
the advanc placement program® have alway 
util essays, and the colleg board® have add 
an essay section to the sat®. 

for those in the test industry, however, essay 
present a practic problem—how to effici 


develop, administer, and score test with essay 
sections. thi paper focu on the score of 
essay and the role autom essay evalu 
system can play in the process. 

whi autom essay score 

et make it mark by standard and then 
autom much of the test process. thi be 
do out of necess a much a for creat 
system in which all test taker can demonstr 
their profici in a common, fair way. few 
reason option be avail to administ 
test to million of student and complet the 
report of score in a time manner. while 
et have focu primarili on multiple-choic 
test in these efforts, the organ have be a 
pioneer in way to use essay in such testing. 

when use essay for assess purposes, 
et have found that have a singl essay 
question or prompt and a singl reader per essay 
do not produc reliabl score (breland, 
bridgeman, & fowles, 1999). the remedi be to 
have test taker write two essay if not more and 
to have at least two peopl read and rate each 
essay. score cost for such a test be 
substantial. et have held annual massiv 
read for some of it test administr 
involv essays. thi have meant bring 
togeth a small armi of educ to a singl 
locat and have them read through and score 
essay after essay. even move such a system 
onlin requir hour of train and logist 
support for each rater. recruit for each of 
these system alon can be a daunt task a 
qualifi individu be rel few, and they 
usual have press schedules. compens 
for the raters' time and possibl travel be a huge 
expens that be pass along to test taker a 
addit to their registr fees. 

that be whi the organ have invest in 
and develop autom essay evalu 





page 2 of 4 

capabl such a e-rater. In the e-rat system, 
the comput be fed thousand of essay that 
human rater have scored. the essay rang from 
those deem to be high-qual respons to 
one see to be less than adequate. To score an 
essay, the system be set up to look for pattern 
that be evid in good essays. the system 
accomplish thi task in seconds. studi show 
a high level of agreement between the score 
human rater assign to an essay and what e-rat 
award (attali & burstein, 2005). 

text vs. context 
even with thi high-level of agreement and 
e-rater' appar efficiency, a number of peopl 
still object to the idea of autom essay 
evaluation. they argue, and rightli so, that such 
system can be fool by clever nonsens or the 
inclus of well-construct sentenc that 
togeth make no sens at all. thi assum that a 
human reader, who would detect such cases, be 
not in the score model at all. the opposit fear 
be to have brilliant write construct in such a 
nonconformist manner that the machin assign a 
poor score. again, a reader should be an effect 
guard against such a situation. Of course, student 
seek instruct would have littl to gain in 
use e-rat outsid of it intend function. 

anoth worri be that autom essay system 
might be less valid for use in the score of 
essay write by english languag learners. will 
a machin that be train on the write of nativ 
english speaker work in a situat where the 
major of the test popul doesn't speak 
english a a nativ language? will system like 
e-rat have the same kind of valid in such 
instances? 

bridgeman (2004) say that a possibl solut be 
to use e-rat to check the score assign by 
human raters. By have e-rat run in the 
background, the score e-rat provid can be 
compar to the one assign by a singl human 
rater. If there be no discrepancy, the score stands. 
If the score be discrepant, a second human 
reader receiv the essay to see if a factor such a 
fatigu affect the score the first rater assign 
or if the essay have element that be unduli 
influenc the autom system. In thi system, 
the essay score would alway be base sole on 
human raters. 

the approach allow test organ to 
streamlin the essay evalu process while still 
provid valid score reporting. 

test e-rat a qualiti control 

To test hi model, bridgeman (2004) turn to 
the gre analyt write section, which have 
each test taker write two essays—on on an issu 
prompt and the other on an argument prompt. jill 
burstein, the lead scientist on the e-rat system 
and a comput linguist, develop e-rat 
score model for more than 100 prompt of 
each type (issu and argument). for the issu 
prompts, the e-rat score agre with the score 
assign by a human rater at the same rate that 
one human agre with another. for the argument 
prompts, agreement of e-rat and human rater 
be slightli lower, but still quit high. the 
correl between the score assign by two 
human be .81, and the correl of a human 
score and e-rat score be .76. 

To evalu the effect of use e-rat a 
an addit score or a a check on the score 
from one human rater per prompt, bridgeman 
studi 5,950 examine who have take the gre 
analyt write section twice. He use the final 
score base on at least four human rate (two 
for each prompt) from one administr a 
the criterion.1 thi criterion provid an estim 
of write abil that be total independ of 
the estim make from use e-rat either a 
an addit rater or a a check. the criterion 
be predict from score on a differ 
administr that be base on two human 


1 A singl score be report for the analyt write 

section. each essay receiv a score from two 
train reader use a 6-point holist scale. In 
holist scoring, reader be train to assign score 
on the basi of the overal qualiti of an essay in 
respons to the assign task. If the two assign 
score differ by more than 1 point on the scale, the 
discrep be adjud by a third gre reader. 
otherwise, the score from the two read of 
an essay be averaged. the final score be base on 
two essay (one a respons to an issu prompt and 
the other a respons to an argument prompt) that be 
then averag and round up to the near half- 
point interv (e.g., 3.0, 3.5). 





page 3 of 4 

per prompt, one human per prompt, one human 
with the e-rat check procedur result in a 
second human rate about 15% of the time, or 
one human plu e-rater. result be summar 
in tabl 1. 

the high agreement, even high than two 
human reader per prompt, be found when the 
score assign from one human reader be 
combin with the e-rat score. but if test user 
be uncomfort with have a score assign 
by a machin be part of a person’ score, the 
check human approach result in agreement 
rate that be nearli a high. 

summari 

autom essay evalu systems, such a 
e-rater, have a veri high threshold to meet to 
gain people' full confid a a valid score 
approach. thi skeptic be healthy, and until 
these system reach a level of sophist to 
make such concern unwarranted, educ 
measur organ should be judici 
in the use of these systems, especi in 
assess that help in make high-stak 
decisions, such a those use in admissions. 

however, autom essay evalu system 
do have valu if properli used. one such valid 
application, a thi paper establishes, be a a 
qualiti control check on human rate essay 
prompts. To produc reliabl score when use 
essay in assessment, multipl topic and 
multipl reader be necessary. arrang for 
human reader be a time-consum and costli 
task and one for which educ measur 

organ can consider lessen the burden 
with e-rater. the result describ here show that 
the high reliabl be obtain by combin 
a human reader' score with that gener by 
e-rater. 

et have and continu to explor other us for 
e-rat a it work to perfect the system. even 
thi seemingli limit usag of thi capabl 
can reap award by make essay score more 
effici and less costly. Of course, test taker 
be the ultim beneficiaries, a they will have 
anoth avenu besid multiple-choic test to 
demonstr their true abilities. 

refer 
attali, y., & burstein, J. (2005). autom essay score 

with e-rat v.2.0 (et rr-04-45). princeton, nj: ets. 

breland, H. m., bridgeman, b., & fowles, M. E. (1999). 
write assess in admiss to high education: 
review and framework (colleg board research rep. 
no. 99-03, gre board research rep. no. 96-12r, 
et rr-99-03). new york: colleg entranc 
examin board. 

bridgeman, B. (2004, december). e-rat a a qualiti 
control on human scorers. present in the et 
research colloquium series, princeton, nj. 

cooper, P. L. (1984) the assess of write ability: A 
review of research (gre board research report no. 
82-15r, et rr-84-12). princeton, nj: ets. 

flam, F. (2004, august 30). An appl for the computer. the 
philadelphia inquirer, p. d-01. 

powers, D. e., fowles, M. e., & welsh, C. K. (1999). 
further valid of a write assess for graduat 
admiss (gre board research rep. no. 96-13r, 
et rr-99-18). princeton, nj: ets. 



tabl 1 

agreement when criterion Is analyt write total from a differ administr 

reader per prompt within ½ point within 1 point 

2 human 76.6% 94.0% 

1 human 72.9% 92.5% 

check human 75.5% 93.9% 

1 human + e-rat 77.7% 94.2% 





page 4 of 4 

how e-rat work 

earlier version of e-rat have some 50 features, 
and a subset of these featur would be select 
to score the particular set of essays. the newer 
version of e-rat us a fix set of about 
10 featur in seven categori from which it 
deriv the final score. 

explan of the seven score categori 

• grammar score – base on error such a those 
in subject-verb agreement among other 

• mechan score – deriv from error in 
spell and other like error 

• usag score – base on such error a articl 
error and confus word (an exampl would 
be an instanc in which the essay writer us a 
word that although phonet similar have a 
differ mean from the intend word; 
use "to" where it would have be proper to 
use "too") 

• style score – base on instanc of overli 
repeat word and the number of veri long or 
veri short sentenc a well a other such 
featur 

• lexic complex score – drawn from 
inform such a the level of vocabulari the 
essay writer us in the essay 











• organization/develop score – base on the 
identif of sentenc that correspond to 
the background, thesis, main idea, support 
idea, and conclus 

• prompt-specif vocabulari usag score – 
deriv from e-rater' evalu of the word 
choic in an essay and the similar to the 
word choic in sampl of low- to high-qual 
essay write on the same topic 

In addit to these seven score categories, 
essay length also may be consid and 
weight in a control way. 


r&d connect be publish by 

et research & develop 
educ test servic 
rosedal road, 19-t 
princeton, NJ 08541-0001 

send comment about thi public to the abov address or via the 
web at: 

http://www.ets.org/research/contact.html 

copyright © 2005 by educ test service. all right reserved. 
educ test servic be an affirm action/equ opportun 
employer. 

educ test service, ets, and the et logo, e-rater, graduat 
record examinations, and gre be regist trademark of 
educ test service. 

colleg board, advanc placement program, and sat be regist 
trademark of the colleg entranc examin board. 

L i s t e n i n g . 
L ea rn i n g . 
L e a d i n g . 


