






















































curiosity-driven explor by self-supervis predict 


curiosity-driven explor by self-supervis predict 

deepak pathak 1 pulkit agraw 1 alexei A. efro 1 trevor darrel 1 

abstract 

In mani real-world scenarios, reward extrins 
to the agent be extrem sparse, or absent al- 
together. In such cases, curios can serv a 
an intrins reward signal to enabl the agent 
to explor it environ and learn skill that 
might be use late in it life. We formul 
curios a the error in an agent’ abil to pre- 
dict the consequ of it own action in a vi- 
sual featur space learn by a self-supervis 
invers dynam model. our formul scale 
to high-dimension continu state space like 
images, bypass the difficulti of directli pre- 
dict pixels, and, critically, ignor the aspect 
of the environ that cannot affect the agent. 
the propos approach be evalu in two en- 
vironments: vizdoom and super mario bros. 
three broad set be investigated: 1) spars 
extrins reward, where curios allow for far 
few interact with the environ to reach 
the goal; 2) explor with no extrins reward, 
where curios push the agent to explor more 
efficiently; and 3) gener to unseen sce- 
nario (e.g. new level of the same game) where 
the knowledg gain from earli experi 
help the agent explor new place much faster 
than start from scratch. 

1. introduct 
reinforc learn algorithm aim at learn polici 
for achiev target task by maxim reward provid 
by the environment. In some scenarios, these reward be 
suppli to the agent continuously, e.g. the run score 
in an atari game (mnih et al., 2015), or the distanc be- 
tween a robot arm and an object in a reach task (lilli- 
crap et al., 2016). however, in mani real-world scenarios, 
reward extrins to the agent be extrem spars or miss- 

1univers of california, berkeley. correspond to: 
deepak pathak <pathak@berkeley.edu>. 

proceed of the 34 th intern confer on machin 
learning, sydney, australia, 2017. jmlr: w&cp. copyright 
2017 by the author(s). 

(a) learn to explor in level-1 (b) explor faster in level-2 

figur 1. discov how to play super mario bro without re- 
wards. (a) use onli curiosity-driven exploration, the agent 
make signific progress in level-1. (b) the gain knowledg 
help the agent explor subsequ level much faster than when 
start from scratch. watch the video at http://pathak22. 
github.io/noreward-rl/ 

ing altogether, and it be not possibl to construct a shape 
reward function. thi be a problem a the agent receiv 
reinforc for updat it polici onli if it succeed in 
reach a pre-specifi goal state. hope to stumbl into 
a goal state by chanc (i.e. random exploration) be like to 
be futil for all but the simplest of environments. 

As human agents, we be accustom to oper with re- 
ward that be so spars that we onli experi them onc 
or twice in a lifetime, if at all. To a three-year-old enjoy- 
ing a sunni sunday afternoon on a playground, most trap- 
ping of modern life – college, good job, a house, a famili – 
be so far into the future, they provid no use reinforce- 
ment signal. yet, the three-year-old have no troubl enter- 
tain herself in that playground use what psychologist 
call intrins motiv (ryan, 2000) or curios (silvia, 
2012). motivation/curios have be use to explain the 
need to explor the environ and discov novel states. 
the french word flâneur perfectli captur the notion of a 
curiosity-driven observer, the “deliber aimless pedes- 
trian, unencumb by ani oblig or sens of urgency” 
(cornelia oti skinner). more generally, curios be a way 
of learn new skill which might come handi for pursu- 
ing reward in the future. 

similarly, in reinforc learning, intrins motiva- 
tion/reward becom critic whenev extrins reward 
be sparse. most formul of intrins reward can be 
group into two broad classes: 1) encourag the agent 
to explor “novel” state (bellemar et al., 2016; lope 

ar 
X 

iv 
:1 

70 
5. 

05 
36 

3v 
1 

[ 
c 

.L 
G 

] 
1 

5 
M 

ay 
2 

01 
7 

http://pathak22.github.io/noreward-rl/ 
http://pathak22.github.io/noreward-rl/ 


curiosity-driven explor by self-supervis predict 

et al., 2012; poupart et al., 2006) or, 2) encourag the agent 
to perform action that reduc the error/uncertainti in the 
agent’ abil to predict the consequ of it own ac- 
tion (i.e. it knowledg about the environment) (houthooft 
et al., 2016; moham & rezende, 2015; schmidhuber, 
1991; 2010; singh et al., 2005; stadi et al., 2015). 

measur “novelty” requir a statist model of the dis- 
tribut of the environment states, wherea measur 
predict error/uncertainti requir build a model of 
environment dynam that predict the next state (st+1) 
give the current state (st) and the action (at) execut 
at time t. both these model be hard to build in high- 
dimension continu state space such a images. An 
addit challeng lie in deal with the stochast of 
the agent-environ system, both due to the nois in the 
agent’ actuation, which caus it end-effector to move 
in a stochast manner, and, more fundamentally, due to 
the inher stochast in the environment. To give the 
exampl from (schmidhuber, 2010), if the agent receiv 
imag a state input be observ a televis screen dis- 
play white noise, everi state will be novel and it would 
be imposs to predict the valu of ani pixel in the fu- 
ture. other exampl of such stochast includ appear- 
anc chang due to shadow from other move entities, 
presenc of distractor objects, or other agent in the envi- 
ronment whose motion be not onli hard to predict but be 
also irrelev to the agent’ goals. somewhat different, 
but related, be the challeng of gener across phys- 
ical (and perhap also visually) distinct but function 
similar part of an environment, which be crucial for large- 
scale problems. one propos solut to all these prob- 
lem be to onli reward the agent when it encount state 
that be hard to predict but be “learnable” (schmidhuber, 
1991). however, estim learnabl be a non-trivi 
problem (lope et al., 2012). 

thi work belong to the broad categori of method that 
gener an intrins reward signal base on how hard it be 
for the agent to predict the consequ of it own actions, 
i.e. predict the next state give the current state and the ex- 
ecut action. however, we manag to escap most pitfal 
of previou predict approach with the follow key 
insight: we onli predict those chang in the environ 
that could possibl be due to the action of our agent or 
affect the agent, and ignor the rest. that is, instead of 
make predict in the raw sensori space (e.g. pixels), 
we transform the sensori input into a featur space where 
onli the inform relev to the action perform by 
the agent be represented. We learn thi featur space use 
self-supervis – train a neural network on a proxi in- 
vers dynam task of predict the agent’ action give 
it current and next states. sinc the neural network be onli 
requir to predict the action, it have no incent to repre- 
sent within it featur emb space the factor of vari- 

ation in the environ that do not affect the agent itself. 
We then use thi featur space to train a forward dynam 
model that predict the featur represent of the next 
state, give the featur represent of the current state 
and the action. We provid the predict error of the for- 
ward dynam model to the agent a an intrins reward to 
encourag it curiosity. 

the role of curios have be wide studi in the context 
of solv task with spars rewards. In our opinion, cu- 
riositi have two other fundament uses. curios help an 
agent explor it environ in the quest for new knowl- 
edg (a desir characterist of exploratori behavior be 
that it should improv a the agent gain more knowledge). 
further, curios be a mechan for an agent to learn skill 
that might be help in futur scenarios. In thi paper, we 
evalu the effect of our curios formul in all 
three of these roles. 

We first compar the perform of an a3c agent (mnih 
et al., 2016) with and without the curios signal on 3-d 
navig task with spars extrins reward in the viz- 
doom environment. We show that a curiosity-driven in- 
trinsic reward be crucial in accomplish these task (see 
section 4.1). next, we show that even in the absenc of 
ani extrins rewards, a curiou agent learn good explo- 
ration policies. for instance, an agent train onli with 
curios a it reward be abl to cross a signific portion 
of level-1 in super mario bros. similarli in vizdoom, 
the agent learn to walk intellig along the corridor in- 
stead of bump into wall or get stuck in corner (see 
section 4.2). A question that natur follow be whether 
the learn exploratori behavior be specif to the physic 
space that the agent train itself on, or if it enabl the 
agent to perform good in unseen scenario too? We show 
that the explor polici learn in the first level of mario 
help the agent explor subsequ level faster (shown in 
figur 1), while the intellig walk behavior learn by 
the curiou vizdoom agent transfer to a complet new 
map with new textur (see section 4.3). these result 
suggest that the propos method enabl an agent to learn 
generaliz skill even in the absenc of an explicit goal. 

2. curiosity-driven explor 
our agent be compos of two subsystems: a reward gener- 
ator that output a curiosity-driven intrins reward signal 
and a polici that output a sequenc of action to maxi- 
mize that reward signal. In addit to intrins rewards, 
the agent option may also receiv some extrins reward 
from the environment. let the intrins curios reward 
gener by the agent at time t be rit and the extrins re- 
ward be ret . the polici sub-system be train to maxim 
the sum of these two reward rt = rit + r 

e 
t , with r 

e 
t mostli 

(if not always) zero. 



curiosity-driven explor by self-supervis predict 

forward 
model 

invers 
model 

fe 
at 

ur 
e 



fe 
at 

ur 
e 



E 

icm 


st st+1 

ritr 
i 
t 

st+1stat 

at at+1 

�(st) �(st+1) 

�̂(st+1) ât 

icm 


ret+1 + r 
i 
t+1r 

e 
t + r 

i 
t 

figur 2. the agent in state st interact with the environ by execut an action at sampl from it current polici π and end up in 
the state st+1. the polici π be train to optim the sum of the extrins reward (ret ) provid by the environ E and the curios 
base intrins reward signal (rit) gener by our propos intrins curios modul (icm). icm encod the state st, st+1 into the 
featur φ(st), φ(st+1) that be train to predict at (i.e. invers dynam model). the forward model take a input φ(st) and at 
and predict the featur represent φ̂(st+1) of st+1. the predict error in the featur space be use a the curios base intrins 
reward signal. As there be no incent for φ(st) to encod ani environment featur that can not influenc or be not influenc by the 
agent’ actions, the learn explor strategi of our agent be robust to uncontrol aspect of the environment. 

We repres the polici π(st; θP ) by a deep neural network 
with paramet θP . given the agent in state st, it execut 
the action at ∼ π(st; θP ) sampl from the policy. θP be 
optim to maxim the expect sum of rewards, 

max 
θP 

eπ(st;θp )[σtrt] (1) 

unless specifi otherwise, we use the notat π(s) to de- 
note the parameter polici π(s; θP ). our curios re- 
ward model can potenti be use with a rang of polici 
learn methods; in the experi discuss here, we 
use the asynchron advantag actor critic polici gradient 
(a3c) (mnih et al., 2016) for polici learning. our main 
contribut be in design an intrins reward signal base 
on predict error of the agent’ knowledg about it en- 
viron that scale to high-dimension continu state 
space like images, bypass the hard problem of predict- 
ing pixel and be unaffect by the unpredict aspect of 
the environ that do not affect the agent. 

2.1. predict error a curios reward 

make predict in the raw sensori space (e.g. when 
st correspond to images) be undesir not onli becaus 
it be hard to predict pixel directly, but also becaus it be 
unclear if predict pixel be even the right object to 
optimize. To see why, consid use predict error in 
the pixel space a the curios reward. imagin a scenario 
where the agent be observ the movement of tree leaf 
in a breeze. sinc it be inher hard to model breeze, 
it be even harder to predict the pixel locat of each leaf. 

thi impli that the pixel predict error will remain high 
and the agent will alway remain curiou about the leaves. 
but the motion of the leaf be inconsequenti to the agent 
and therefor it continu curios about them be undesir- 
able. the underli problem be that the agent be unawar 
that some part of the state space simpli cannot be mod- 
ele and thu the agent can fall into an artifici curios 
trap and stall it exploration. novelty-seek explor 
scheme that record the count of visit state in a tabular 
form (or their extens to continu state spaces) also 
suffer from thi issue. measur learn progress instead 
of predict error have be propos in the past a one so- 
lution (schmidhuber, 1991). unfortunately, there be cur- 
rentli no know comput feasibl mechan for 
measur learn progress. 

If not the raw observ space, then what be the right fea- 
ture space for make predict so that the predict 
error provid a good measur of curiosity? To answer 
thi question, let u divid all sourc that can modifi the 
agent’ observ into three cases: (1) thing that can 
be control by the agent; (2) thing that the agent cannot 
control but that can affect the agent (e.g. a vehicl driven 
by anoth agent), and (3) thing out of the agent’ control 
and not affect the agent (e.g. move leaves). A good 
featur space for curios should model (1) and (2) and be 
unaffect by (3). thi latter be because, if there be a sourc 
of variat that be inconsequenti for the agent, then the 
agent have no incent to know about it. 



curiosity-driven explor by self-supervis predict 

2.2. self-supervis predict for explor 

instead of hand-design a featur represent for everi 
environment, our aim be to come up with a gener mecha- 
nism for learn featur represent such that the pre- 
diction error in the learn featur space provid a good 
intrins reward signal. We propos that such a featur 
space can be learn by train a deep neural network with 
two sub-modules: the first sub-modul encod the raw 
state (st) into a featur vector φ(st) and the second sub- 
modul take a input the featur encod φ(st), φ(st+1) 
of two consequ state and predict the action (at) take 
by the agent to move from state st to st+1. train thi 
neural network amount to learn function g defin as: 

ât = g 
( 
st, st+1; θI 

) 
(2) 

where, ât be the predict estim of the action at and the 
the neural network paramet θI be train to optimize, 

min 
θI 

li(ât, at) (3) 

where, LI be the loss function that measur the discrep- 
anci between the predict and actual actions. In case at 
be discrete, the output of g be a soft-max distribut across 
all possibl action and minim LI amount to maxi- 
mum likelihood estim of θI under a multinomi dis- 
tribution. the learn function g be also know a the in- 
vers dynam model and the tupl (st, at, st+1) requir 
to learn g be obtain while the agent interact with the en- 
viron use it current polici π(s). 

In addit to invers dynam model, we train anoth 
neural network that take a input at and φ(st) and pre- 
dict the featur encod of the state at time step t+ 1, 

φ̂(st+1) = f 
( 
φ(st), at; θF 

) 
(4) 

where φ̂(st+1) be the predict estim of φ(st+1) and the 
neural network paramet θF be optim by minim 
the loss function LF : 

LF 

( 
φ(st), φ̂(st+1) 

) 
= 

1 

2 
‖φ̂(st+1)− φ(st+1)‖22 (5) 

the learn function f be also know a the forward dy- 
namic model. the intrins reward signal rit be comput 
as, 

rit = 
η 

2 
‖φ̂(st+1)− φ(st+1)‖22 (6) 

where η > 0 be a scale factor. In order to gener the 
curios base intrins reward signal, we jointli optim 
the forward and invers dynam loss describ in equa- 
tion 3 and 5 respectively. the invers model learn a fea- 
ture space that encod inform relev for predict 
the agent’ action onli and the forward model make pre- 
diction in thi featur space. We refer to thi propos 

curios formul a intrins curios modul (icm). 
As there be no incent for thi featur space to encod 
ani environment featur that be not influenc by the 
agent’ actions, our agent will receiv no reward for reach- 
ing environment state that be inher unpredict 
and it explor strategi will be robust to the presenc 
of distractor objects, chang in illumination, or other nui- 
sanc sourc of variat in the environment. see figur 2 
for illustr of the formulation. 

the use of invers model have be investig to learn 
featur for recognit task (agraw et al., 2015; jayara- 
man & grauman, 2015). agraw et al. (2016) construct 
a joint inverse-forward model to learn featur representa- 
tion for the task of push objects. however, they onli 
use the forward model a a regular for train the in- 
vers model features, while we make use of the error in 
the forward model predict a the curios reward for 
train our agent’ policy. 

the overal optim problem that be solv for learn 
the agent be a composit of equat 1, 3 and 5 and can 
be write as, 

min 
θP ,θi ,θf 

[ 
− λeπ(st;θp )[σtrt] + (1− β)li + βlf 

] 
(7) 

where 0 ≤ β ≤ 1 be a scalar that weigh the invers 
model loss against the forward model loss and λ > 0 be 
a scalar that weigh the import of the polici gradient 
loss against the import of learn the intrins reward 
signal. 

3. experiment setup 
To evalu our curios modul on it abil to improv 
explor and provid gener to novel scenarios, 
we will use two simul environments. thi section de- 
scribe the detail of the environ and the experiment 
setup. 

environ the first environ we evalu on be 
the vizdoom (kempka et al., 2016) game. We consid 
the doom 3-d navig task where the action space of 
the agent consist of four discret action – move forward, 
move left, move right and no-action. our test setup in all 
the experi be the ‘doommywayhome-v0’ environ- 
ment which be avail a part of openai gym (brockman 
et al., 2016). episod be termin either when the agent 
find the vest or if the agent exce a maximum of 2100 
time steps. the map consist of 9 room connect by cor- 
ridor and the agent be task to reach some fix goal loca- 
tion from it spawn location. the agent be onli provid 
a spars termin reward of +1 if it find the vest and zero 
otherwise. for gener experiments, we pre-train on 



curiosity-driven explor by self-supervis predict 

(a) input snapshot in vizdoom (b) input w/ nois 
figur 3. frame from vizdoom 3-d environ which agent 
take a input: (a) usual 3-d navig setup; (b) setup when 
uncontrol nois be add to the input. 

a differ map with differ random textur from (doso- 
vitskiy & koltun, 2016) and each episod last for 2100 
time steps. sampl frame from vizdoom be show in 
figur 3a, and map be explain in figur 4. It take ap- 
proxim 350 step for an optim polici to reach the 
vest locat from the farthest room in thi map (spars re- 
ward). 

our second environ be the classic nintendo game su- 
per mario bro (paquette, 2016). We consid four level 
of the game: pre-train on the first level and show 
gener on the subsequ levels. In thi setup, we 
reparametr the action space of the agent into 14 uniqu 
action follow (paquette, 2016). thi game be play 
use a joystick allow for multipl simultan button 
presses, where the durat of the press affect what action 
be be taken. thi properti make the game particularli 
hard, e.g. to make a long jump over tall pipe or wide gaps, 
the agent need to predict the same action up to 12 time 
in a row, introduc long-rang dependencies. all our ex- 
periment on mario be train use curios signal only, 
without ani reward from the game. 

train detail all agent in thi work be train us- 
ing visual input that be pre-process in manner similar 
to (mnih et al., 2016). the input rgb imag be con- 
vert into gray-scal and re-siz to 42 × 42. In order to 
model tempor dependencies, the state represent (st) 
of the environ be construct by concaten the cur- 
rent frame with the three previou frames. close follow- 
ing (mnih et al., 2015; 2016), we use action repeat of four 
dure train time in vizdoom and action repeat of six 
in mario. however, we sampl the polici without ani ac- 
tion repeat dure inference. follow the asynchron 
train protocol in a3c, all the agent be train asyn- 
chronous with twenti worker use stochast gradient 
descent. We use adam optim with it paramet not 
share across the workers. 

a3c architectur the input state st be pass through 
a sequenc of four convolut layer with 32 filter each, 

S 

(a) train map scenario 

S 

S 

room: 13 
(“sparse”) 

room: 17 
(“veri sparse”) 

goal 

(b) test map scenario 
figur 4. map for vizdoom 3-d environment: (a) for general- 
izat experi (c.f. section 4.3), map of the environ 
where agent be pre-train onli use curios signal without ani 
reward from environment. ‘s’ denot the start position. (b) 
test map for vizdoom experiments. green star denot goal 
location. blue dot refer to 17 agent spawn locat in the 
map in the “dense” case. room 13, 17 be the fix start loca- 
tion of agent in “sparse” and “veri sparse” reward case respec- 
tively. note that textur be also differ in train and test maps. 

kernel size of 3x3, stride of 2 and pad of 1. An expo- 
nential linear unit (elu; (clevert et al., 2015)) be use after 
each convolut layer. the output of the last convolut 
layer be fed into a lstm with 256 units. two seper fulli 
connect layer be use to predict the valu function and 
the action from the lstm featur representation. 

intrins curios modul (icm) architectur the in- 
trinsic curios modul consist of the forward and the in- 
vers model. the invers model first map the input state 
(st) into a featur vector φ(st) use a seri of four con- 
volut layers, each with 32 filters, kernel size 3x3, stride 
of 2 and pad of 1. elu non-linear be use after 
each convolut layer. the dimension of φ(st) (i.e. 
the output of the fourth convolut layer) be 288. for the 
invers model, φ(st) and φ(st+1) be concaten into a 
singl featur vector and pass a input into a fulli con- 
nect layer of 256 unit follow by an output fulli con- 
nect layer with 4 unit to predict one of the four possibl 
actions. the forward model be construct by concatenat- 
ing φ(st) with at and pass it into a sequenc of two fulli 
connect layer with 256 and 288 unit respectively. the 
valu of β be 0.2, and λ be 0.1. the equat (7) be mini- 
mize with learn rate of 1e− 3. 

baselin method ‘icm + a3c’ denot our full algo- 
rithm which combin intrins curios model with a3c. 
across differ experiments, we compar our approach 
with three baselines. first be the vanilla ‘a3c’ algorithm 
with �-greedi exploration. second be ‘icm-pixel + a3c’, 
which be a variant of our icm without the invers model, 
and have curios reward depend onli on the forward 
model loss in predict next observ in pixel space. To 
design this, we remov the invers model layer and append 



curiosity-driven explor by self-supervis predict 

0 1 2 3 4 5 6 7 8 9 
number of train step (in millions) 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

1.2 

Ex 
tri 

n 
ic 

Re 
wa 

rd 
s p 

er 
E 

pi 
so 

de 

icm + a3c 
icm (pixels) + a3c 
a3c 

(a) “dens reward” set (b) “spars reward” set (c) “veri spars reward” set 
figur 5. compar the perform of the a3c agent with no curios (blue) against the curios in pixel space agent (green) and the 
propos curiou icm-a3c agent (orange) a the hard of the explor task be gradual increas from left to right. explor 
becom harder with larg distanc between the initi and goal locations: “dense”, “sparse” and “veri sparse”. the result depict that 
succeed on harder explor task becom progress harder for the baselin a3c, wherea the curiou a3c be abl to achiev 
good score in all the scenarios. pixel base curios work in dens and spars but fail in veri spars reward setting. the protocol 
follow in the plot involv run three independ run of each algorithm. darker line repres mean and shade area repres 
mean ± standard error of mean. We do not perform ani tune of random seeds. 

deconvolut layer to the forward model. icm-pixel be 
close to icm in architectur but incap of learn em- 
bed that be invari to the uncontrol part of envi- 
ronment. note that icm-pixel be repres of previ- 
ou method which comput inform gain by directli 
use the observ space (schmidhuber, 2010; stadi 
et al., 2015). We show that directli use observ space 
for comput curios be significantli bad than learn 
an emb a in icm. finally, we includ comparison 
with state-of-the-art explor method base on varia- 
tional inform maxim (vime) (houthooft et al., 
2016) which be train with trpo. 

4. experi 
We qualit and quantit evalu the perfor- 
manc of the learn polici with and without the propos 
intrins curios signal in two environments, vizdoom 
and super mario bros. three broad set be evaluated: 
a) spars extrins reward on reach a goal (section 4.1); 
b) explor with no extrins reward (section 4.2); and c) 
gener to novel scenario (section 4.3). In vizdoom 
gener be evalu on a novel map with novel tex- 
tures, while in mario it be evalu on subsequ game 
levels. 

4.1. spars extrins reward set 

We perform extrins reward experi on vizdoom us- 
ing ‘doommywayhome-v0’ setup describ in section 3. 
the extrins reward be spars and onli provid when the 
agent find the goal (a vest) locat at a fix locat in the 
map. We systemat vari the difficulti of thi goal- 
direct explor task by vari the distanc between 

the initi spawn locat of the agent and the locat 
of the goal. A larg distanc mean that the chanc of 
reach the goal locat by random explor be low 
and consequ the reward be say to be sparser. 

vari the degre of reward sparsity: We consid 
three setup with “dense”, “sparse” and “very-sparse” re- 
ward (see figur 4b). In these settings, the reward be al- 
way termin and the episod termin upon reach 
goal or after a maximum of 2100 steps. In the “dense” re- 
ward case, the agent be randomli spawn in ani of the 17 
possibl spawn locat uniformli distribut across 
the map. thi be not a hard explor task becaus some- 
time the agent be randomli initi close to the goal and 
therefor by random �-greedi explor it can reach the 
goal with reason high probability. In the “sparse” and 
“veri sparse” reward cases, the agent be alway spawn 
in room-13 and room-17 respect which be 270 and 
350 step away from the goal under an optim policy. A 
long sequenc of direct action be requir to reach the 
goal from these rooms, make these set hard goal 
direct explor problems. 

result show in figur 5 indic that while the perfor- 
manc of the baselin a3c degrad with sparser rewards, 
curiou a3c agent be superior in all cases. In the “dense” 
reward case, curiou agent learn much faster indic 
more effici explor of the environ a compar 
to �-greedi explor of the baselin agent. one possi- 
ble explan of the inferior perform of icm-pixel 
in comparison to icm be that in everi episod the agent be 
spawn in one out of seventeen room with differ tex- 
tures. It be hard to learn a pixel-predict model a the 
number of textur increases. 



curiosity-driven explor by self-supervis predict 

0 2 4 6 8 10 12 14 16 18 20 
number of train step (in millions) 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

1.2 
Ex 

tri 
n 

ic 
Re 

wa 
rd 

s p 
er 

E 
pi 

so 
de 

icm + a3c 
icm (pixels) + a3c 

figur 6. evalu the robust of icm to the presenc of un- 
control distractor in the environment. We creat such a 
distractor by replac 40% of the visual observ of the agent 
by white nois (see figur 3b). the result show that while icm 
succeed most of the times, the pixel predict model struggles. 

In the “sparse” reward case, a expected, the baselin a3c 
agent fail to solv the task, while the curiou a3c agent 
be abl to learn the task quickly. note that icm-pixel 
and icm have similar converg because, with a fix 
spawn locat of the agent, the icm-pixel encoun- 
ter the same textur at the start of each episod which 
make learn the pixel-predict model easi a com- 
par to the “dense” reward case. finally, in the “veri 
sparse” reward case, both the a3c agent and icm-pixel 
never succeed, while the icm agent achiev a perfect 
score in 66% of the random runs. thi indic that icm 
be good suit than icm-pixel and vanilla a3c for hard 
goal direct explor tasks. 

robust to uncontrol dynam for test the 
robust of the propos icm formul to chang in 
the environ that do not affect the agent, we augment 
the agent’ observ with a fix region of white nois 
which make up 40% of the imag (see figur 3b). In viz- 
doom 3-d navigation, ideal the agent should be unaf- 
fect by thi nois a the nois do not affect the agent 
in anyway and be mere a nuisance. figur 6 compar 
the perform of icm against some baselin method on 
the “sparse” reward setup describ above. while, the pro- 
pose icm agent achiev a perfect score, icm-pixel suf- 
fer significantli despit have succeed at the “spars 
reward” task when the input be not augment with ani 
nois (see figur 5b). thi indic that in contrast to 
icm-pixels, icm be insensit to nuisanc chang in the 
environment. 

comparison to trpo-vim We now compar our cu- 
riou agent against variat inform maxim 
agent train with trpo (houthooft et al., 2016) for the 

vizdoom “sparse” reward setup describ above. trpo be 
in gener more sampl effici than a3c but take a lot 
more wall-clock time. We do not show these result in plot 
becaus trpo and a3c have differ setups. the hyper- 
paramet and accuraci for the trpo and vime result 
follow from the concurr work (fu et al., 2017). despit 
the sampl effici of trpo, we see that our icm agent 
work significantli good than trpo and trpo-vime, 
both in term of converg rate and accuracy. result 
be show in the tabl below: 

method mean (median) score 
(“sparse” reward setup) (at convergence) 

trpo 26.0 % ( 0.0 %) 
a3c 0.0 % ( 0.0 %) 

vime + trpo 46.1 % ( 27.1 %) 

icm + a3c 100.0 % (100.0 %) 

As a saniti check, we replac the curios network with 
random nois sourc and use them a the curios re- 
ward. We perform systemat sweep across differ 
distribut paramet in the “sparse” reward case: uni- 
form, gaussian and laplacian. We found that none of these 
agent be abl to reach the goal show that our curios- 
iti modul do not learn degener solutions. 

4.2. No reward set 

A good explor polici be one which allow the agent to 
visit a mani state a possibl even without ani goals. In 
the case of 3-d navigation, we expect a good explor 
polici to cover a much of the map a possible; in the case 
of play a game, we expect it to visit a mani game state 
a possible. In order to test if our agent can learn a good 
explor policy, we train it on vizdoom and mario 
without ani reward from the environment. We then eval- 
uat what portion of the map be explor (for vizdoom), 
and how much progress it make (for mario) in thi setting. 
To our surprise, we have found that in both cases, the no- 
reward agent be abl to perform quot well (see video at 
http://pathak22.github.io/noreward_rl/). 

vizdoom: coverag dure exploration. An agent 
train with no extrins reward be abl to learn to nav- 
igat corridors, walk between room and explor mani 
room in the 3-d doom environment. On mani occa- 
sion the agent travers the entir map and reach room 
that be farthest away from the room it be initi in. 
given that the episod termin in 2100 step and farthest 
room be over 250 step away (for an optimally-mov 
agent), thi result be quit remarkable, demonstr that it 
be possibl to learn use skill without the requir of 
ani extern supervis of rewards. exampl explor 
be show in figur 7. the first 3 map show our agent ex- 

http://pathak22.github.io/noreward_rl/ 


curiosity-driven explor by self-supervis predict 

S 1 2 

3 4 5 

6 S 1 2 

3 
4 5 

S 1 

5 

2 

3 

4 S 
1 2 S 1 

2 

figur 7. each column in the figur show the visit pattern of an agent explor the environment. the red arrow show the initi 
locat and orient of the agent at the start of the episode. each room that the agent visit dure it explor of maximum 2100 
step have be colored. the first three column (with map color in yellow) show the explor strategi of an agent train with 
curios driven intern reward signal only. the last two column show the room visit by an agent conduct random exploration. 
the result clearli show that the curiou agent train with intrins reward explor a significantli larg number of room a compar 
to a randomli explor agent. 

plore a much larg state space without ani extrins signal, 
compar to a random explor agent (last two maps), 
which often have hard time get around local minimum of 
state spaces, e.g. get stuck against a wall and not abl 
to move (see video). 

mario: learn to play with no rewards. We train our 
agent in the super mario world use onli curios base 
signal. without ani extrins reward from environment, our 
mario agent can learn to cross over 30% of level-1. the 
agent receiv no reward for kill or dodg enemi or 
avoid fatal events, yet it automat discov these 
behavior (see video). one possibl reason be becaus get- 
ting kill by the enemi will result in onli see a small 
part of the game space, make it curios saturate. In 
order to remain curious, it be in the agent’ interest to learn 
how to kill and dodg enemi so that it can reach new part 
of the game space. thi suggest that curios provid in- 
direct supervis for learn interest behaviors. 

To the best of our knowledge, thi be the first demonstr 
where the agent learn to navig in a 3D environ and 
discov how to play a game by make use of rel 
complex visual imageri directli from pixels, without ani 
extrins rewards. there be sever prior work that use re- 
inforc learn to navig in 3D environ from 
pixel input or play atari game such a (mirowski 
et al., 2017; mnih et al., 2015; 2016), but they reli on in- 
termedi extern reward provid by the environment. 

4.3. gener to novel scenario 

In the previou section we show that our agent learn to 
explor larg part of the space where it curiosity-driven 
explor polici be trained. however, it remain un- 
clear whether the agent have do thi by learn “gener- 
aliz skills” for effici explor it environment, or 
if it simpli memor the train set. In other word we 
would like to know, when explor a space, how much of 
the learn behavior be specif to that particular space and 
how much be gener enough to be use in novel scenar- 

ios? To investig thi question, we train a no reward ex- 
ploratori behavior in one scenario (e.g. level-1 of mario) 
and then evalu the result explor polici in three 
differ ways: a) appli the learn polici “a is” to a new 
scenario; b) adapt the polici by fine-tun with curios 
reward only; c) adapt the polici to maxim some extrin- 
sic reward. happily, in all three cases, we observ some 
promis gener results: 

evalu “a is”: We evalu the polici train by max- 
imiz curios on level-1 of mario on subsequ lev- 
el without adapt the learn polici in ani way. We 
measur the distanc cover by the agent a a result of 
execut thi polici on level 1, 2, and 3, a show in 
tabl 1. We note that the polici perform surprisingli well 
on level 3, suggest good generalization, despit the fact 
that level-3 have differ structur and enemi compar 
to level-1. however, note that the run “a is” on level- 
2 do not do well. At first, thi seem to contradict the gen- 
eral result on level-3. however, note that level-3 
have similar global visual appear (day world with sun- 
light) to level-1, wherea level-2 be significantli differ 
(night world). If thi be inde the issue, then it should be 
possibl to quickli adapt the explor polici to level-2 
with a littl bit of “fine-tuning”. We will explor thi below. 

fine-tun with curios only: from tabl 1 we see 
that when the agent pre-train (use onli curios a 
reward) on level-1 be fine-tun (use onli curios a 
reward) on level-2 it quickli overcom the mismatch in 
global visual appear and achiev a high score than 
train from scratch with the same number of iterations. 
interestingly, train “from scratch” on level-2 be bad 
than the fine-tun policy, even when train for more 
iter than pre-train + fine-tun combined. one 
possibl reason be that level-2 be more difficult than level- 
1, so learn the basic skill such a moving, jumping, 
and kill enemi from scratch be much more danger 
than in the rel “safety” of level-1. thi result, there- 
fore might suggest that first pre-train on an earli level 



curiosity-driven explor by self-supervis predict 

level id level-1 level-2 level-3 

accuraci scratch run a be fine-tun scratch scratch run a be fine-tun scratch scratch 
iter 1.5m 0 1.5m 1.5m 3.5m 0 1.5m 1.5m 5.0m 

mean ± stderr 711 ± 59.3 31.9 ± 4.2 466 ± 37.9 399.7 ± 22.5 455.5 ± 33.4 319.3 ± 9.7 97.5 ± 17.4 11.8 ± 3.3 42.2 ± 6.4 
% distanc > 200 50.0 ± 0.0 0 64.2 ± 5.6 88.2 ± 3.3 69.6 ± 5.7 50.0 ± 0.0 1.5 ± 1.4 0 0 
% distanc > 400 35.0 ± 4.1 0 63.6 ± 6.6 33.2 ± 7.1 51.9 ± 5.7 8.4 ± 2.8 0 0 0 
% distanc > 600 35.8 ± 4.5 0 42.6 ± 6.1 14.9 ± 4.4 28.1 ± 5.4 0 0 0 0 

tabl 1. quantit evalu of the agent train to play super mario bros. use onli curios signal without ani reward from the 
game. our agent be train with no reward in level-1. We then evalu the agent’ polici both when it be run “a is”, and further 
fine-tun on subsequ levels. the result be compar to set when mario agent be train from scratch in level-2,3 use onli 
curios without ani extrins rewards. evalu metric be base on the distanc cover by the mario agent. 

0 2 4 6 8 10 12 14 
number of train step (in millions) 

0.0 

0.2 

0.4 

0.6 

0.8 

1.0 

1.2 

Ex 
tri 

n 
ic 

Re 
wa 

rd 
s p 

er 
E 

pi 
so 

de 

finetuned: icm + a3c 
scratch: icm + a3c 
finetuned: icm (pixels) + a3c 
scratch: icm (pixels) + a3c 

figur 8. perform of icm + a3c agent on the test set of viz- 
doom in the “veri sparse” reward case. fine-tun model learn 
the explor polici without ani extern reward on the train- 
ing map and be then fine-tun on the test map. the scratch 
model be directli train on the test map. the fine-tun icm + 
a3c significantli outperform icm + a3c indic that our cu- 
riositi formul be abl to learn generaliz explor poli- 
cies. the pixel predict base icm agent complet fail. note 
that textur be also differ in train and test. 

and then fine-tun on a late one produc a form of cur- 
riculum which aid learn and generalization. In other 
words, the agent be abl to use the knowledg it acquir 
by play level-1 to good explor the subsequ levels. 
Of course, the game design do thi on purpos to allow 
the human player to gradual learn to play the game. 

however, interestingly, fine-tun the explor polici 
pre-train on level-1 to level-3 deterior the perfor- 
mance, compar to run “a is”. thi be becaus level- 
3 be veri hard for the agent to cross beyond a certain point 
– the agent hit a curios blockad and be unabl to make 
ani progress. As the agent have alreadi learn about part 
of the environ befor the hard point, it receiv almost 
no curios reward and a a result it attempt to updat 
it polici with almost zero intrins reward and the polici 
slowli degenerates. thi behavior be vagu analog to 
boredom, where if the agent be unabl to make progress it 
get bore and stop exploring. 

fine-tun with extrins rewards: If it be the case 
that the agent have actual learn use exploratori be- 
havior, then it should be abl to learn quicker than start- 
ing from scratch even when extern reward be provid 
by environment. We perform thi evalu on vizdoom 
where we pre-train the agent use curios reward on 
a map show in figur 4a. We then test on the “veri 
sparse” reward set of ‘doommywayhome-v0’ envi- 
ronment which us a differ map with novel textur 
(see figur 4b) a describ earli in section 4.1. 

result in figur 8 show that the icm agent pre-train 
onli with curios and then fine-tun with extern re- 
ward learn faster and achiev high reward than an icm 
agent train from scratch to jointli maxim curios 
and the extern rewards. thi result confirm that the 
learn exploratori behavior be also use when the agent 
be requir to achiev goal specifi by the environment. 
It be also worth note that icm-pixel do not gener 
to thi test environment. thi indic that the propos 
mechan of measur curios be significantli good for 
learn skill that gener a compar to measur 
curios in the raw sensori space. 

5. relat work 
curiosity-driven explor be a well studi topic in the 
reinforc learn literatur and a good summari can 
be found in (oudey & kaplan, 2009; oudey et al., 
2007). schmidhub (1991; 2010) and sun et al. (2011) 
use surpris and compress progress a intrins rewards. 
classic work of kearn et al. (1999) and brafman et 
al. (2002) propos explor algorithm polynomi in 
the number of state space parameters. other have use 
empowerment, which be the inform gain base on en- 
tropi of actions, a intrins reward (klyubin et al., 2005; 
moham & rezende, 2015). stadi et al. (2015) use pre- 
diction error in the featur space of an auto-encod a a 
measur of interest state to explore. state visit 
count have also be investig for explor (belle- 
mare et al., 2016; Oh et al., 2015; tang et al., 2016). os- 
band et al. (2016) train multipl valu function and make 



curiosity-driven explor by self-supervis predict 

use of bootstrap and thompson sampl for explo- 
ration. mani approach measur inform gain for ex- 
plorat (littl & sommer, 2014; still & precup, 2012; 
storck et al., 1995). houthooft et al. (2016) use an ex- 
plorat strategi that maxim inform gain about 
the agent’ belief of the environment’ dynamics. our ap- 
proach of jointli train forward and invers model for 
learn a featur space have similar to (agraw et al., 
2016; jordan & rumelhart, 1992; wolpert et al., 1995), but 
these work use the learn model of dynam for plan- 
ning a sequenc of action instead of exploration. the idea 
of use a proxi task to learn a semant featur embed- 
ding have be use in a number of work on self-supervis 
learn in comput vision (agraw et al., 2015; doersch 
et al., 2015; goroshin et al., 2015; jayaraman & grauman, 
2015; pathak et al., 2016; wang & gupta, 2015). 

concurr work: A number of interest relat pa- 
per have appear on arxiv while the present work be 
in submission. sukhbaatar et al. (2017) gener supervi- 
sion for pre-train via asymmetr self-play between two 
agent to improv data effici dure fine-tuning. sev- 
eral method propos improv data effici of RL al- 
gorithm use self-supervis predict base auxiliari 
task (jaderberg et al., 2017; shelham et al., 2017). Fu 
et al. (2017) learn discrimin models, and gregor et 
al. (2017) use empower base measur to tackl ex- 
plorat in spars reward setups. 

6. discuss 
In thi work we propos a mechan for gener 
curiosity-driven intrins reward signal that scale to high 
dimension visual inputs, bypass the difficult problem 
of predict pixel and ensur that the explor strat- 
egi of the agent be unaffect by nuisanc factor in the 
environment. We demonstr that our agent significantli 
outperform the baselin a3c with no curiosity, a recent 
propos vime (houthooft et al., 2016) formul for 
exploration, and a baselin pixel-predict formulation. 

In vizdoom our agent learn the explor behavior of 
move along corridor and across room without ani re- 
ward from the environment. In mario our agent cross 
more than 30% of level-1 without ani reward from the 
game. one reason whi our agent be unabl to go beyond 
thi limit be the presenc of a pit at 38% of the game that 
requir a veri specif sequenc of 15-20 key press in 
order to jump across it. If the agent be unabl to execut 
thi sequence, it fall in the pit and dies, receiv no fur- 
ther reward from the environment. therefor it receiv 
no gradient inform indic that there be a world be- 
yond the pit that could potenti be explored. thi issu 
be somewhat orthogon to develop model of curiosity, 
but present a challeng problem for polici learning. 

It be common practic to evalu reinforc learn 
approach in the same environ that be use for 
training. however, we feel that it be also import to eval- 
uat on a separ “test set” a well. thi allow u to 
gaug how much of what have be learn be specif to 
the train environ (i.e. memorized), and how much 
might constitut “generaliz skills” that could be ap- 
pli to new settings. In thi paper, we evalu general- 
izat in two ways: 1) by appli the learn polici to a 
new scenario “a is” (no further learning), and 2) by fine- 
tune the learn polici on a new scenario (we borrow the 
pre-training/fine-tun nomenclatur from the deep fea- 
ture learn literature). We believ that evalu gen- 
eral be a valuabl tool and will allow the commun 
to good understand the perform of variou reinforce- 
ment learn algorithms. To further aid in thi effort, we 
will make the code for our algorithm, a well a test and 
environ setup freeli avail online. 

An interest direct of futur research be to use the 
learn explor behavior/skil a a motor primitive/low- 
level polici in a more complex, hierarch system. for 
example, our vizdoom agent learn to walk along corridor 
instead of bump into walls. thi could be a use prim- 
itiv for a navig system. 

while the rich and divers real world provid ampl op- 
portun for interaction, reward signal be sparse. our 
approach excel in thi set and convert unexpect in- 
teract that affect the agent into intrins rewards. how- 
ever our approach do not directli extend to the scenario 
where “opportun for interactions” be also rare. In the- 
ory, one could save such event in a replay memori and use 
them to guid exploration. however, we leav thi exten- 
sion for futur work. 

acknowledgements: We would like to thank sergey 
levine, evan shelhamer, saurabh gupta, phillip isola and 
other member of the bair lab for fruit discuss and 
comments. We thank jacob huh for help with figur 2 and 
alexey dosovitskiy for vizdoom maps. thi work be 
support in part by nsf iis-1212798, iis-1427425, iis- 
1536003, iis-1633310, onr muri n00014-14-1-0671, 
berkeley deepdrive, equip grant from nvidia, and the 
valrhona reinforc learn fellowship. 

refer 
agrawal, pulkit, carreira, joao, and malik, jitendra. 

learn to see by moving. In iccv, 2015. 

agrawal, pulkit, nair, ashvin, abbeel, pieter, malik, ji- 
tendra, and levine, sergey. learn to poke by poking: 
experienti learn of intuit physics. nips, 2016. 

bellemare, marc, srinivasan, sriram, ostrovski, georg, 



curiosity-driven explor by self-supervis predict 

schaul, tom, saxton, david, and munos, remi. uni- 
fy count-bas explor and intrins motivation. 
In nips, 2016. 

brafman, ronen I and tennenholtz, moshe. r-max-a gen- 
eral polynomi time algorithm for near-optim rein- 
forcement learning. jmlr, 2002. 

brockman, greg, cheung, vicki, pettersson, ludwig, 
schneider, jonas, schulman, john, tang, jie, and 
zaremba, wojciech. openai gym. arxiv:1606.01540, 
2016. 

clevert, djork-arné, unterthiner, thomas, and hochreiter, 
sepp. fast and accur deep network learn by expo- 
nential linear unit (elus). arxiv:1511.07289, 2015. 

doersch, carl, gupta, abhinav, and efros, alexei A. un- 
supervis visual represent learn by context pre- 
diction. In iccv, 2015. 

dosovitskiy, alexey and koltun, vladlen. learn to act 
by predict the future. iclr, 2016. 

fu, justin, co-reyes, john D, and levine, sergey. ex2: 
explor with exemplar model for deep reinforce- 
ment learning. arxiv:1703.01260, 2017. 

goroshin, ross, bruna, joan, tompson, jonathan, eigen, 
david, and lecun, yann. unsupervis featur learn 
from tempor data. arxiv:1504.02518, 2015. 

gregor, karol, rezende, danilo jimenez, and wierstra, 
daan. variat intrins control. iclr workshop, 
2017. 

houthooft, rein, chen, xi, duan, yan, schulman, john, 
De turck, filip, and abbeel, pieter. vime: variat 
inform maxim exploration. In nips, 2016. 

jaderberg, max, mnih, volodymyr, czarnecki, woj- 
ciech marian, schaul, tom, leibo, joel Z, silver, david, 
and kavukcuoglu, koray. reinforc learn with 
unsupervis auxiliari tasks. iclr, 2017. 

jayaraman, dinesh and grauman, kristen. learn imag 
represent tie to ego-motion. In iccv, 2015. 

jordan, michael I and rumelhart, david E. forward mod- 
els: supervis learn with a distal teacher. cognit 
science, 1992. 

kearns, michael and koller, daphne. effici reinforce- 
ment learn in factor mdps. In ijcai, 1999. 

kempka, michał, wydmuch, marek, runc, grzegorz, 
toczek, jakub, and jaśkowski, wojciech. vizdoom: A 
doom-bas ai research platform for visual reinforce- 
ment learning. arxiv:1605.02097, 2016. 

klyubin, alexand S, polani, daniel, and nehaniv, 
chrystoph L. empowerment: A univers agent- 
centric measur of control. In evolutionari computa- 
tion, 2005. 

lillicrap, timothi P, hunt, jonathan J, pritzel, alexander, 
heess, nicolas, erez, tom, tassa, yuval, silver, david, 
and wierstra, daan. continu control with deep rein- 
forcement learning. iclr, 2016. 

little, daniel Y and sommer, friedrich T. learn and 
explor in action-percept loops. close the loop 
around neural systems, 2014. 

lopes, manuel, lang, tobias, toussaint, marc, and 
oudeyer, pierre-yves. explor in model-bas re- 
inforc learn by empir estim learn 
progress. In nips, 2012. 

mirowski, piotr, pascanu, razvan, viola, fabio, soyer, 
hubert, ballard, andy, banino, andrea, denil, misha, 
goroshin, ross, sifre, laurent, kavukcuoglu, koray, 
et al. learn to navig in complex environments. 
iclr, 2017. 

mnih, volodymyr, kavukcuoglu, koray, silver, david, 
rusu, andrei A, veness, joel, bellemare, marc G, 
graves, alex, riedmiller, martin, fidjeland, andrea K, 
ostrovski, georg, et al. human-level control through 
deep reinforc learning. nature, 2015. 

mnih, volodymyr, badia, adria puigdomenech, mirza, 
mehdi, graves, alex, lillicrap, timothi P, harley, tim, 
silver, david, and kavukcuoglu, koray. asynchron 
method for deep reinforc learning. In icml, 
2016. 

mohamed, shakir and rezende, danilo jimenez. varia- 
tional inform maximis for intrins moti- 
vate reinforc learning. In nips, 2015. 

oh, junhyuk, guo, xiaoxiao, lee, honglak, lewis, 
richard L, and singh, satinder. action-condit 
video predict use deep network in atari games. In 
nips, 2015. 

osband, ian, blundell, charles, pritzel, alexander, and 
van roy, benjamin. deep explor via bootstrap 
dqn. In nips, 2016. 

oudeyer, pierre-yv and kaplan, frederic. what be intrin- 
sic motivation? a typolog of comput approaches. 
frontier in neurorobotics, 2009. 

oudeyer, pierre-yves, kaplan, frdric, and hafner, ver- 
ena V. intrins motiv system for autonom 
mental development. evolutionari computation, 2007. 



curiosity-driven explor by self-supervis predict 

paquette, philip. super mario bros. in openai gym. 
github:ppaquette/gym-super-mario, 2016. 

pathak, deepak, krahenbuhl, philipp, donahue, jeff, dar- 
rell, trevor, and efros, alexei A. context encoders: fea- 
ture learn by inpainting. In cvpr, 2016. 

poupart, pascal, vlassis, nikos, hoey, jesse, and regan, 
kevin. An analyt solut to discret bayesian rein- 
forcement learning. In icml, 2006. 

ryan, richard; deci, edward L. intrins and extrins mo- 
tivations: classic definit and new directions. con- 
temporari educ psychology, 2000. 

schmidhuber, jurgen. A possibl for implement 
curios and boredom in model-build neural con- 
trollers. In from anim to animats: proceed of the 
first intern confer on simul of adapt 
behavior, 1991. 

schmidhuber, jürgen. formal theori of creativity, fun, and 
intrins motiv (1990–2010). ieee transact on 
autonom mental development, 2010. 

shelhamer, evan, mahmoudieh, parsa, argus, max, and 
darrell, trevor. loss be it own reward: self-supervis 
for reinforc learning. arxiv:1612.07307, 2017. 

silvia, paul J. curios and motivation. In the oxford 
handbook of human motivation, 2012. 

singh, satind P, barto, andrew G, and chentanez, nut- 
tapong. intrins motiv reinforc learning. 
In nips, 2005. 

stadie, bradli C, levine, sergey, and abbeel, pieter. in- 
centiv explor in reinforc learn with 
deep predict models. nip workshop, 2015. 

still, susann and precup, doina. An information-theoret 
approach to curiosity-driven reinforc learning. 
theori in biosciences, 2012. 

storck, jan, hochreiter, sepp, and schmidhuber, jürgen. 
reinforc driven inform acquisit in non- 
determinist environments. In icann, 1995. 

sukhbaatar, sainbayar, kostrikov, ilya, szlam, arthur, and 
fergus, rob. intrins motiv and automat curric- 
ula via asymmetr self-play. arxiv:1703.05407, 2017. 

sun, yi, gomez, faustino, and schmidhuber, jürgen. plan- 
ning to be surprised: optim bayesian explor in 
dynam environments. In agi, 2011. 

tang, haoran, houthooft, rein, foote, davis, stooke, 
adam, chen, xi, duan, yan, schulman, john, De turck, 
filip, and abbeel, pieter. # exploration: A studi of 

count-bas explor for deep reinforc learn- 
ing. arxiv:1611.04717, 2016. 

wang, xiaolong and gupta, abhinav. unsupervis learn- 
ing of visual represent use videos. In iccv, 
2015. 

wolpert, daniel M, ghahramani, zoubin, and jordan, 
michael I. An intern model for sensorimotor integra- 
tion. science-aaas-weekli paper edition, 1995. 


