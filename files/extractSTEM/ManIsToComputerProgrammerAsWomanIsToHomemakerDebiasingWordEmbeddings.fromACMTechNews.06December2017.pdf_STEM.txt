






































































man be to comput programm a woman be to homemaker? debias word embed 


man be to comput programm a woman be to 
homemaker? debias word embed 

tolga bolukbasi1, kai-wei chang2, jame zou2, venkatesh saligrama1,2, adam kalai2 
1boston university, 8 saint mary’ street, boston, MA 

2microsoft research new england, 1 memori drive, cambridge, MA 
tolgab@bu.edu, kw@kwchang.net, jamesyzou@gmail.com, srv@bu.edu, adam.kalai@microsoft.com 

abstract 
the blind applic of machin learn run the risk of amplifi bia present 
in data. such a danger be face u with word embedding, a popular framework to 
repres text data a vector which have be use in mani machin learn and 
natur languag process tasks. We show that even word embed train on 
googl new articl exhibit female/mal gender stereotyp to a disturb extent. 
thi rais concern becaus their widespread use, a we describe, often tend to 
amplifi these biases. geometrically, gender bia be first show to be captur by 
a direct in the word embedding. second, gender neutral word be show to 
be linearli separ from gender definit word in the word embedding. use 
these properties, we provid a methodolog for modifi an emb to remov 
gender stereotypes, such a the associ between the word receptionist and 
female, while maintain desir associ such a between the word queen 
and female. use crowd-work evalu a well a standard benchmarks, we 
empir demonstr that our algorithm significantli reduc gender bia in 
embed while preserv the it use properti such a the abil to cluster 
relat concept and to solv analog tasks. the result embed can be use 
in applic without amplifi gender bias. 

1 introduct 
research on word embed have drawn signific interest in machin learn and natur languag 
processing. there have be hundr of paper write about word embed and their applications, 
from web search [22] to pars curriculum vita [12]. however, none of these paper have 
recogn how blatantli sexist the embed be and henc risk introduc bia of variou 
type into real-world systems. 

A word embedding, train on word co-occurr in text corpora, repres each word (or common 
phrase) w a a d-dimension word vector ~w 2 rd. It serf a a dictionari of sort for comput 
program that would like to use word meaning. first, word with similar semant mean tend to 
have vector that be close together. second, the vector differ between word in embed 
have be show to repres relationship between word [27, 21]. for exampl give an analog 
puzzle, “man be to king a woman be to x” (denot a man:k :: woman:x), simpl arithmet of 
the emb vector find that x=queen be the best answer becaus ��!man �����!woman ⇡ ��!king ����!queen. 
similarly, x=japan be return for paris:fr :: tokyo:x. It be surpris that a simpl vector 
arithmet can simultan captur a varieti of relationships. It have also excit practition 
becaus such a tool could be use across applic involv natur language. indeed, they 
be be studi and use in a varieti of downstream applic (e.g., document rank [22], 
sentiment analysi [14], and question retriev [17]). 

however, the embed also pinpoint sexism implicit in text. for instance, it be also the case that: 
��!man �����!woman ⇡ ���������������!comput programm ��������!homemaker. 

In other words, the same system that solv the abov reason analog will offens answer 
“man be to comput programm a woman be to x” with x=homemaker. similarly, it output that a 

30th confer on neural inform process system (nip 2016), barcelona, spain. 



extrem she 
1. homemak 
2. nurs 
3. receptionist 
4. librarian 
5. socialit 
6. hairdress 
7. nanni 
8. bookkeep 
9. stylist 
10. housekeep 

extrem he 
1. maestro 
2. skipper 
3. proteg 
4. philosoph 
5. captain 
6. architect 
7. financi 
8. warrior 
9. broadcast 
10. magician 

gender stereotyp she-h analog 
sewing-carpentri regist nurse-physician housewife-shopkeep 
nurse-surgeon interior designer-architect softball-basebal 
blond-burli feminism-conservat cosmetics-pharmaceut 
giggle-chuckl vocalist-guitarist petite-lanki 
sassy-snappi diva-superstar charming-aff 
volleyball-footbal cupcakes-pizza lovely-brilli 

gender appropri she-h analog 
queen-k sister-broth mother-fath 
waitress-wait ovarian cancer-prost cancer convent-monasteri 

figur 1: left the most extrem occup a project on to the she�h gender direct on 
w2vnews. occup such a businesswoman, where gender be suggest by the orthography, 
be excluded. right automat gener analog for the pair she-h use the procedur 
describ in text. each automat gener analog be evalu by 10 crowd-work to whether 
or not it reflect gender stereotype. 

father be to a doctor a a mother be to a nurse. the primari emb studi in thi paper be the 
popular publicly-avail word2vec [19, 20] 300 dimension emb train on a corpu of 
googl new text consist of 3 million english words, which we refer to here a the w2vnews. 
one might have hop that the googl new emb would exhibit littl gender bia becaus 
mani of it author be profession journalists. We also analyz other publicli avail embed 
train via other algorithm and find similar bia (appendix b). 

In thi paper, we quantit demonstr that word-embed contain bia in their geometri 
that reflect gender stereotyp present in broader society.1 due to their wide-spread usag a basic 
features, word embed not onli reflect such stereotyp but can also amplifi them. thi pose a 
signific risk and challeng for machin learn and it applications. the analog gener from 
these embed spell out the bia implicit in the data on which they be trained. hence, word 
embed may serv a a mean to extract implicit gender associ from a larg text corpu 
similar to how implicit associ test [11] detect automat gender associ possess by 
people, which often do not align with self reports. 

To quantifi bias, we will compar a word vector to the vector of a pair of gender-specif words. for 
instance, the fact that ���!nurs be close to ����!woman be not in itself necessarili biased(it be also somewhat 
close to ��!man – all be humans), but the fact that these distanc be unequ suggest bias. To make 
thi rigorous, consid the distinct between gender specif word that be associ with a gender 
by definition, and the remain gender neutral words. standard exampl of gender specif word 
includ brother, sister, businessman and businesswoman. We will use the gender specif word to 
learn a gender subspac in the embedding, and our debias algorithm remov the bia onli from 
the gender neutral word while respect the definit of these gender specif words. 

We propos approach to reduc gender bia in the word emb while preserv the use 
properti of the embedding. surprisingly, not onli do the emb captur bias, but it also 
contain suffici inform to reduc thi bias.w will leverag the fact that there exist a low 
dimension subspac in the emb that empir captur much of the gender bias. 

2 relat work and preliminari 
gender bia and stereotyp in english. It be import to quantifi and understand bia in languag 
a such bia can reinforc the psycholog statu of differ group [28]. gender bia in languag 
have be studi over a number of decad in a varieti of context (see, e.g., [13]) and we onli 
highlight some of the find here. bias differ across peopl though common can be detected. 
implicit associ test [11] have uncov gender-word bia that peopl do not self-report and 
may not even be awar of. common bia link femal term with liber art and famili and male 
term with scienc and career [23]. bia be see in word morphology, i.e., the fact that word such a 

1 stereotyp be bia that be wide held among a group of people. We show that the bia in the word 
emb be in fact close align with social concept of gender stereotype, a evalu by u.s.-bas 
crowd worker on amazon’ mechan turk. the crowd agre that the bia reflect both in the locat of 
vector (e.g. 

���! 
doctor closer to ��!man than to ����!woman) a well a in analog (e.g., he:coward :: she:whore.) exhibit 

common gender stereotypes. 

2 



actor are, by default, associ with the domin class [15], and femal version of these words, 
e.g., actress, be marked. there be also an imbal in the number of word with f-m with variou 
associations. for instance, while there be more word refer to males, there be mani more word 
that sexual femal than male [30]. consist bia have be studi within onlin context 
and specif relat to the context we studi such a onlin news (e.g., [26]), web search (e.g., 
[16]), and wikipedia (e.g., [34]). 

bia within algorithms. A number of onlin system have be show to exhibit variou biases, 
such a racial discrimin and gender bia in the ad present to user [31, 4]. A recent studi 
found that algorithm use to predict repeat offend exhibit indirect racial bia [1]. differ 
demograph and geograph group also use differ dialect and word-choic in social medium 
[6]. An implic of thi effect be that languag use by minor group might not be abl to be 
process by natur languag tool that be train on “standard” data-sets. bias in the curat of 
machin learn data-set have explor in [32, 3]. 

independ from our work, schmidt [29] identifi the bia present in word embed and 
propos debias by entir remov multipl gender dimensions, one for each gender pair. hi 
goal and approach, similar but simpler than ours, be to entir remov gender from the embedding. 
there be also an intens research agenda focu on improv the qualiti of word embed from 
differ angl (e.g., [18, 25, 35, 7]), and the difficulti of evalu emb qualiti (a compar 
to supervis learning) parallel the difficulti of defin bia in an embedding. 

within machin learning, a bodi of notabl work have focu on “fair” binari classif in 
particular. A definit of fair base on legal tradit be present by baroca and selbst [2]. 
approach to modifi classif algorithm to defin and achiev variou notion of fair 
have be describ in a number of works, see, e.g., [2, 5, 8] and a recent survey [36]. the prior 
work on algorithm fair be larg for supervis learning. fair classif be defin base 
on the fact that algorithm be classifi a set of individu use a set of featur with a 
distinguish sensit feature. In word embeddings, there be no clear individu and no a priori 
defin classif problem. however, similar issu arise, such a direct and indirect bia [24]. 

word embedding. An emb consist of a unit vector ~w 2 rd, with k~wk = 1, for each word 
(or term) w 2 W . We assum there be a set of gender neutral word N ⇢ W , such a flight attend 
or shoes, which, by definition, be not specif to ani gender. We denot the size of a set S by |s|. We 
also assum we be give a set of f-m gender pair P ⇢ W ⇥W , such a she-h or mother-fath 
whose definit differ mainli in gender. section 5 discu how N and P can be found within 
the emb itself, but until then we take them a given. As be common, similar between two 
vector u and v can be measur by their cosin similar : cos(u, v) = u·vkukkvk . thi normal 
similar between vector u and v be the cosin of the angl between the two vectors. sinc word 
be normal cos(~w1, ~w2) = ~w1 · ~w2.2 

unless otherwis stated, the emb we refer to be the aforement w2vnew embedding, a 
d = 300-dimension word2vec [19, 20] embedding, which have proven to be immens use sinc 
it be high quality, publicli available, and easi to incorpor into ani application. In particular, we 
download the pre-train emb on the googl new corpus,3 and normal each word to 
unit length a be common. start with the 50,000 most frequent words, we select onli lower-cas 
word and phrase consist of few than 20 lower-cas charact (word with upper-cas letters, 
digits, or punctuat be discarded). after thi filtering, 26,377 word remained. while we focu 
on w2vnews, we show late that gender stereotyp be also present in other emb data-sets. 
crowd experiments.4 two type of experi be performed: one where we solicit word 
from the crowd (to see if the emb bia contain those of the crowd) and one where we 
solicit rate on word or analog gener from our emb (to see if the crowd’ bia 
contain those from the embedding). these two type of experi be analog to experi 
perform in rate result in inform retriev to evalu precis and recall. when we speak 
of the major of 10 crowd judgments, we mean those annot make by 5 or more independ 
workers. the appendix contain the questionnair that be give to the crowd-workers. 

2we will abus terminolog and refer to the emb of a word and the word interchangeably. for example, 
the statement cat be more similar to dog than to cow mean �!cat ·�!dog � �!cat ·��!cow. 

3 
https://code.google.com/archive/p/word2vec/ 

4all human experi be perform on the amazon mechan turk platform. We select for 
u.s.-bas worker to maintain homogen and reproduc to the extent possibl with crowdsourcing. 

3 



3 geometri of gender and bia in word embed 
our first task be to understand the bia present in the word-embed (i.e. which word be closer 
to she than to he, etc.) and the extent to which these geometr bia agre with human notion of 
gender stereotypes. We use two simpl method to approach thi problem: 1) evalu whether the 
emb have stereotyp on occup word and 2) evalu whether the emb produc 
analog that be judg to reflect stereotyp by humans. the exploratori analysi of thi section 
will motiv the more rigor metric use in the next two sections. 

occup stereotypes. figur 1 list the occup that be closest to she and to he in the 
w2vnew embeddings. We ask the crowdwork to evalu whether an occup be consid 
female-stereotypic, male-stereotypic, or neutral. the project of the occup word onto the she- 
he axi be strongli correl with the stereotyp estim of these word (spearman ⇢ = 0.51), 
suggest that the geometr bia of emb vector be align with crowd judgment. We 
project each of the occup onto the she-h direct in the w2vnew emb a well a a 
differ emb gener by the glove algorithm on a web-crawl corpu [25]. the result be 
highli consist (appendix figur 6), suggest that gender stereotyp be preval across differ 
embed and be not an artifact of the particular train corpu or methodolog of word2vec. 

analog exhibit stereotypes. analog be a use way to both evalu the qualiti of a word 
emb and also it stereotypes. We first briefli describ how the emb gener analog 
and then discu how we use analog to quantifi gender stereotyp in the embedding. A more 
detail discuss of our algorithm and prior analog solver be give in appendix C. 

In the standard analog tasks, we be give three words, for exampl he, she, king, and look for the 
4th word to solv he to king be a she to x. here we modifi the analog task so that give two words, 
e.g. he, she, we want to gener a pair of words, x and y, such that he to x a she to y be a good 
analogy. thi modif allow u to systemat gener pair of word that the emb 
believ it analog to he, she (or ani other pair of seed words). the input into our analog gener 
be a seed pair of word (a, b) determin a seed direct ~a �~b correspond to the normal 
differ between the two seed words. In the task below, we use (a, b) = (she, he). We then score 
all pair of word x, y by the follow metric: 

s(a,b)(x, y) = co 
⇣ 
~a�~b, ~x� ~y 

⌘ 
if k~x� ~yk  �, 0 els (1) 

where � be a threshold for similarity. the intuit of the score metric be that we want a good 
analog pair to be close to parallel to the seed direct while the two word be not too far apart in 
order to be semant coherent. the paramet � set the threshold for semant similarity. In all 
the experiments, we take � = 1 a we find that thi choic often work well in practice. sinc all 
embed be normalized, thi threshold correspond to an angl  ⇡/3, indic that the two 
word be closer to each other than they be to the origin. In practice, it mean that the two word 
form the analog be significantli closer togeth than two random emb vectors. given the 
emb and seed words, we output the top analog pair with the larg posit s(a,b) scores. 
To reduc redundancy, we do not output multipl analog share the same word x. 

We employ u.s. base crowd-work to evalu the analog output by the aforement 
algorithm. for each analogy, we ask the worker two yes/no questions: (a) whether the pair 
make sens a an analogy, and (b) whether it reflect a gender stereotype. overall, 72 out of 150 
analog be rat a gender-appropri by five or more out of 10 crowd-workers, and 29 analog 
be rat a exhibit gender stereotyp by five or more crowd-work (figur 4). exampl of 
analog gener from w2vnew be show at figur 1. the full list be in appendix J. 

identifi the gender subspace. next, we studi the bia present in the emb geometrically, 
identifi the gender direct and quantifi the bia independ of the extent to which it be 
align with the crowd bias. languag use be “messy” and therefor individu word pair do not 
alway behav a expected. for instance, the word man have sever differ usages: it may be use 
a an exclam a in oh man! or to refer to peopl of either gender or a a verb, e.g., man the 
station. To more robustli estim bias, we shall aggreg across multipl pair comparisons. By 
combin sever directions, such a 

�! 
she ��!he and ����!woman ���!man, we identifi a gender direct 

g 2 Rd that larg captur gender in the embedding. thi direct help u to quantifi direct and 
indirect bia in word and associations. 

In english a in mani languages, there be numer gender pair terms, and for each we can 
consid the differ between their embeddings. befor look at the data, one might imagin 

4 



def. stereo. 
�! 
she��!h 92% 89% 
�! 
her��!hi 84% 87% 

����!woman���!man 90% 83% 
���! 
mary���!john 75% 87% 

����! 
herself�����!himself 93% 89% 

�����! 
daughter��!son 93% 91% 
����! 
mother����!fath 91% 85% 

�! 
gal��!guy 85% 85% 
�! 
girl��!boy 90% 86% 

����! 
female���!mal 84% 75% 

RG WS analog 

befor 62.3 54.5 57.0 
hard-debias 62.4 54.1 57.0 
soft-debias 62.4 54.2 56.8 

figur 2: left: ten word pair to defin gender, along with agreement with set of definit 
and stereotyp word solicit from the crowd. the accuraci be show for the correspond 
gender classifi base on which word be closer to a target word, e.g., the she-h classifi predict a 
word be femal if it be closer to she than he. middle: the bar plot show the percentag of varianc 
explain in the pca of the 10 pair of gender words. the top compon explain significantli more 
varianc than ani other; the correspond percentag for random word show a more gradual decay 
(figur creat by averag over 1,000 draw of ten random unit vector in 300 dimensions). right: 
the tabl show perform of the origin w2vnew emb (“before”) and the debias 
w2vnew on standard evalu metric measur coher and analogy-solv abilities: RG 
[27], WS [10], msr-analog [21]. higher be better. the result show that the perform do not 
degrad after debiasing. note that we use a subset of vocabulari in the experiments. therefore, the 
perform be low than the previous publish results. see appendix for full results. 

that they all have roughli the same vector differences, a in the follow caricature: 
��������! 
grandmoth =��! 

wise+ 
�! 
gal, 

�������! 
grandfath = 

��! 
wise+�!guy,��������!grandmother��������!grandfath = �!gal��!guy = g however, gender 

pair differ be not parallel in practice, for multipl reasons. first, there be differ bia 
associ with with differ gender pairs. second be polysemy, a mentioned, which in thi case 
occur due to the other use of grandfath a in to grandfath a regulation. finally, random in 
the word count in ani finit sampl will also lead to differences. figur 2 illustr ten possibl 
gender pairs, 

� 
(x 

i 

, y 

i 

) 

10 
i=1 

. 

To identifi the gender subspace, we take the ten gender pair differ vector and comput it 
princip compon (pcs). As figur 2 shows, there be a singl direct that explain the major 
of varianc in these vectors. the first eigenvalu be significantli larg than the rest. note that, 
from the random in a finit sampl of ten noisi vectors, one expect a decreas in eigenvalues. 
however, a also illustr in 2, the decreas one observ due to random sampl be much more 
gradual and uniform. therefor we hypothes that the top pc, denot by the unit vector g, captur 
the gender subspace. In general, the gender subspac could be high dimension and all of our 
analysi and algorithm (describ below) work with gener subspaces. 

direct bias. To measur direct bias, we first identifi word that should be gender-neutr for the 
applic in question. how to gener thi set of gender-neutr word be describ in section 5. 
given the gender neutral words, denot by N , and the gender direct learn from above, g, we 
defin the direct gender bia of an emb to be 1|n | 

P 
w2n |cos(~w, g)| 

c, where c be a paramet 
that determin how strict do we want to in measur bias. If c be 0, then |cos(~w � g)|c = 0 
onli if ~w have no overlap with g and otherwis it be 1. such strict measur of bia might be 
desir in set such a the colleg admiss exampl from the introduction, where it would 
be unaccept for the emb to introduc a slight prefer for one candid over anoth 
by gender. A more gradual bia would be set c = 1. the present we have chosen favor 
simplic – it would be natur to extend our definit to weight word by frequency. for example, 
in w2vnews, if we take N to be the set of 327 occupations, then directbias1 = 0.08, which 
confirm that mani occup word have substanti compon along the gender direction. 

4 debias algorithm 
the debias algorithm be defin in term of set of word rather than just pairs, for generality, so 
that we can consid other bia such a racial or religi biases. We also assum that we have a set 
of word to neutralize, which can come from a list or from the emb a describ in section 5. 
(in mani case it may be easi to list the gender specif word not to neutral a thi set can be 
much smaller.) 

5 



bia 

okay 

he 

figur 3: select word project along two axes: x be a project onto the differ between 
the embed of the word he and she, and y be a direct learn in the emb that captur 
gender neutrality, with gender neutral word abov the line and gender specif word below the line. 
our hard debias algorithm remov the gender pair associ for gender neutral words. In thi 
figure, the word abov the horizont line would all be collaps to the vertic line. 
the first step, call identifi gender subspace, be to identifi a direct (or, more generally, a 
subspace) of the emb that captur the bias. for the second step, we defin two options: 
neutral and equal or soften. neutral ensur that gender neutral word be zero in the 
gender subspace. equal perfectli equal set of word outsid the subspac and therebi 
enforc the properti that ani neutral word be equidist to all word in each equal set. for 
instance, if {grandmother, grandfather} and {guy, gal} be two equal sets, then after equal 
babysit would be equidist to grandmoth and grandfath and also equidist to gal and guy, 
but presum closer to the grandpar and further from the gal and guy. thi be suitabl for 
applic where one do not want ani such pair to display ani bia with respect to neutral words. 

the disadvantag of equal be that it remov certain distinct that be valuabl in certain 
applications. for instance, one may wish a languag model to assign a high probabl to the phrase 
to grandfath a regulation) than to grandmoth a regul sinc grandfath have a mean that 
grandmoth do not – equal the two remov thi distinction. the soften algorithm reduc 
the differ between these set while maintain a much similar to the origin emb a 
possible, with a paramet that control thi trade-off. 

To defin the algorithms, it will be conveni to introduc some further notation. A subspac B be 
defin by k orthogon unit vector B = {b1, . . . , bk} ⇢ rd. In the case k = 1, the subspac be 
simpli a direction. We denot the project of a vector v onto B by, v 

B 

= 

P 
k 

j=1(v · bj)bj . thi 
also mean that v � v 

B 

be the project onto the orthogon subspace. 

step 1: identifi gender subspace. inputs: word set W , defin set d1, d2, . . . , Dn ⇢ W 
a well a emb 

� 
~w 2 Rd 


w2w and integ paramet k � 1. let µi := 

P 
w2di ~w/|di| 

be the mean of the defin sets. let the bia subspac B be the first k row of svd(c) where 
C := 

P 
n 

i=1 

P 
w2di(~w � µi) 

T 

(~w � µ 
i 

) 

� 
|D 

i 

|. 
step 2a: hard de-bias (neutral and equalize). addit inputs: word to neutral 
N ✓ W , famili of equal set E = {e1, e2, . . . , em} where each Ei ✓ W . for each word 
w 2 N , let ~w be re-embed to ~w := (~w � ~w 

B 

) 

� 
k~w � ~w 

B 

k. for each set E 2 E , let 

µ := 

P 
w2e w/|e| and ⌫ := µ� µB . for each w 2 E, ~w := ⌫ + 

p 
1� k⌫k2 ~wb�µbk~wb�µbk . finally, 

output the subspac B and the new emb 
� 
~w 2 Rd 


w2w . 

equal equat each set of word outsid of B to their simpl averag ⌫ and then adjust vector 
so that they be unit length. It be perhap easi to understand by think separ of the two 
compon ~w 

B 

and ~w?b = ~w� ~wb . the latter ~w?b be all simpli equat to their average. within 
B, they be center (move to mean 0) and then scale so that each ~w be unit length. To motiv 
whi we center, beyond the fact that it be common in machin learning, consid the bia direct 
be the gender direct (k = 1) and a gender pair such a E = {male, female}. As discussed, it 

6 



figur 4: number of stereotyp (left) and appropri (right) analog gener by word 
embed befor and after debiasing. 

so happen that both word be posit (female) in the gender direction, though femal have a great 
projection. one can onli specul a to whi thi be the case, e.g., perhap the frequenc of text 
such a male nurs or male escort or she be assault by the male. however, becaus femal have a 
great gender component, after center the two will be symmetr balanc across the origin. 
If instead, we simpli scale each vector’ compon in the bia direciton without centering, male 
and femal would have exactli the same emb and we would lose analog such a father:mal 
:: mother:female. We note that neutral and equal complet remov pair bias. 

observ 1. after step 1 and 2a, for ani gender neutral word w ani equal set E, and ani two 
word e1, e2 2 E, ~w·~e1 = w·~e2 and k~w�~e1k = k~w�~e2k. furthermore, if E = 

� 
{x, y}|(x, y) 2 P 



be the set of pair defin pairbias, then pairbia = 0. 

step 2b: soft bia correction. overload the notation, we let W 2 rd⇥|vocab| denot the matrix 
of all emb vector and N denot the matrix of the emb vector correspond to gender 
neutral words. W and N be learn from some corpu and be input to the algorithm. the 
desir debias transform T 2 rd⇥d be a linear transform that seek to preserv pairwis 
inner product between all the word vector while minim the project of the gender neutral 
word onto the gender subspace. thi can be formal a min 

T 

k(tw )T (tw ) � wtwk2 
F 

+ 

�k(tn)t (tb)k2 
F 

, where B be the gender subspac learn in step 1 and � be a tune paramet 
that balanc the object of preserv the origin emb inner product with the goal of 
reduc gender bias. for � large, T would remov the project onto B from all the vector in N , 
which correspond exactli to step 2a. In the experiment, we use � = 0.2. the optim problem 
be a semi-definit program and can be solv efficiently. the output emb be normal to have 
unit length, ˆW = {tw/ktwk2, w 2 w}. 

5 determin gender neutral word 
for practic purposes, sinc there be mani few gender specif words, it be more effici to 
enumer the set of gender specif word S and take the gender neutral word to be the compliment, 
N = W \ S. use dictionari definitions, we deriv a subset S0 of 218 word out of the word in 
w2vnews. recal that thi emb be a subset of 26,377 word out of the full 3 million word 
in the embedding, a describ in section 2. thi base list S0 be give in appendix F. note that the 
choic of word be subject and ideal should be custom to the applic at hand. 

We gener thi list to the entir 3 million word in the googl new emb use a linear 
classifier, result in the set S of 6,449 gender-specif words. more specifically, we train a linear 
support vector machin (svm) with regular paramet of C = 1.0. We then ran thi classifi 
on the remain words, take S = S0 [ s1, where S1 be the word label a gender specif by 
our classifi among the word in the entir emb that be not in the 26,377 word of w2vnews. 
use 10-fold cross-valid to evalu the accuracy, we find an F -score of .627± .102. 
figur 3 illustr the result of the classifi for separ gender-specif word from gender- 
neutral words. To make the figur legible, we show a subset of the words. the x-axi correspond to 
project of word onto the 

�! 
she ��!he direct and the y-axi correspond to the distanc from the 

decis boundari of the train svm. 

7 



6 debias result 
We evalu our debias algorithm to ensur that they preserv the desir properti of the 
origin emb while reduc both direct and indirect gender biases. first we use the same 
analog gener task a before: for both the hard-debias and the soft-debias embeddings, 
we automat gener pair of word that be analog to she-h and ask crowd-work 
to evalu whether these pair reflect gender stereotypes. figur 4 show the results. On the initi 
w2vnew embedding, 19% of the top 150 analog be judg a show gender stereotyp 
by a major of the ten workers. after appli our hard debias algorithm, onli 6% of the new 
emb be judg a stereotypical. 

As an example, consid the analog puzzle, he to doctor be a she to X . the origin emb 
return X = nurs while the hard-debias emb find X = physician. moreov the hard- 
debias algorithm preserv gender appropri analog such a she to ovarian cancer be a he 
to prostat cancer. thi demonstr that the hard-debias have effect reduc the gender 
stereotyp in the word embedding. figur 4 also show that the number of appropri analog 
remain similar a in the origin emb after execut hard-debiasing. thi demonstr that 
that the qualiti of the embed be preserved. the detail result be in appendix J. soft-debias 
be less effect in remov gender bias. To further confirm the qualiti of embed after 
debiasing, we test the debias emb on sever standard benchmark that measur whether 
relat word have similar embed a well a how well the emb perform in analog tasks. 
appendix tabl 2 show the result on the origin and the new embed and the transform 
do not neg impact the performance. In appendix A, we show how our algorithm also reduc 
indirect gender bias. 

7 discuss 
word embed help u further our understand of bia in language. We find a singl direct 
that larg captur gender, that help u captur associ between gender neutral word and 
gender a well a indirect inequality. the project of gender neutral word on thi direct enabl 
u to quantifi their degre of female- or male-bias. 

To reduc the bia in an embedding, we chang the embed of gender neutral words, by remov 
their gender associations. for instance, nurs be move to to be equal male and femal in the 
direct g. In addition, we find that gender-specif word have addit bia beyond g. for 
instance, grandmoth and grandfath be both closer to wisdom than gal and guy are, which do not 
reflect a gender difference. On the other hand, the fact that babysit be so much closer to grandmoth 
than grandfath (more than for other gender pairs) be a gender bia specif to grandmother. By 
equat grandmoth and grandfath outsid of gender, and sinc we’v remov g from babysit, 
both grandmoth and grandfath and equal close to babysit after debiasing. By retain the 
gender compon for gender-specif words, we maintain analog such a she:grandmoth 
:: he:grandfather. through empir evaluations, we show that our hard-debias algorithm 
significantli reduc both direct and indirect gender bia while preserv the util of the embedding. 
We have also develop a soft-embed algorithm which balanc reduc bia with preserv 
the origin distances, and could be appropri in specif settings. 

one perspect on bia in word embed be that it mere reflect bia in society, and therefor 
one should attempt to debia societi rather than word embeddings. however, by reduc the bia in 
today’ comput system (or at least not amplifi the bias), which be increasingli reliant on word 
embeddings, in a small way debias word embed can hope contribut to reduc gender 
bia in society. At the veri least, machin learn should not be use to inadvert amplifi these 
biases, a we have see can natur happen. 

In specif applications, one might argu that gender bia in the emb (e.g. comput 
programm be closer to he) could captur use statist and that, in these special cases, the origin 
bia embed could be used. howev give the potenti risk of have machin learn 
algorithm that amplifi gender stereotyp and discriminations, we recommend that we should err on 
the side of neutral and use the debias embed provid here a much a possible. 

acknowledgments. the author thank tarleton gillespi and nanci baym for numer help 
discussions.5 

5 thi materi be base upon work support in part by nsf grant cns-1330008, ccf-1527618, by onr 
grant 50202168, nga grant hm1582-09-1-0037 and dh 2013-st-061-ed0001 

8 



refer 
[1] J. angwin, J. larson, S. mattu, and L. kirchner. machin bias: there’ softwar use across the countri 

to predict futur criminals. and it’ bia against blacks., 2016. 
[2] S. baroca and A. D. selbst. big data’ dispar impact. avail at ssrn 2477899, 2014. 
[3] E. beigman and B. B. klebanov. learn with annot noise. In acl, 2009. 
[4] A. datta, M. C. tschantz, and A. datta. autom experi on ad privaci settings. proceed on 

privaci enhanc technologies, 2015. 
[5] C. dwork, M. hardt, T. pitassi, O. reingold, and R. zemel. fair through awareness. In innov in 

theoret comput scienc conference, 2012. 
[6] J. eisenstein, B. o’connor, N. A. smith, and E. P. xing. diffus of lexic chang in social media. plo 

one, page 1–13, 2014. 
[7] M. faruqui, J. dodge, S. K. jauhar, C. dyer, E. hovy, and N. A. smith. retrofit word vector to 

semant lexicons. In naacl, 2015. 
[8] M. feldman, S. A. friedler, J. moeller, C. scheidegger, and S. venkatasubramanian. certifi and 

remov dispar impact. In kdd, 2015. 
[9] C. fellbaum, editor. wordnet: An electron lexic database. the mit press, cambridge, ma, 1998. 

[10] L. finkelstein, E. gabrilovich, Y. matias, E. rivlin, Z. solan, G. wolfman, and E. ruppin. place search 
in context: the concept revisited. In www. acm, 2001. 

[11] A. G. greenwald, D. E. mcghee, and J. L. schwartz. measur individu differ in implicit cognition: 
the implicit associ test. journal of person and social psychology, 74(6):1464, 1998. 

[12] C. hansen, M. tosik, G. goossen, C. li, L. bayeva, F. berbain, and M. rotaru. how to get the best word 
vector for resum parsing. In snn adapt intellig / symposium: machin learn 2015, nijmegen. 

[13] J. holm and M. meyerhoff. the handbook of languag and gender, volum 25. john wiley & sons, 
2008. 

[14] O. i̇rsoy and C. cardie. deep recurs neural network for composition in language. In nips. 2014. 
[15] R. jakobson, L. R. waugh, and M. monville-burston. On language. harvard univ pr, 1990. 
[16] M. kay, C. matuszek, and S. A. munson. unequ represent and gender stereotyp in imag search 

result for occupations. In human factor in comput systems. acm, 2015. 
[17] T. lei, H. joshi, R. barzilay, T. jaakkola, A. M. katerina tymoshenko, and L. marquez. semi-supervis 

question retriev with gate convolutions. In naacl. 2016. 
[18] O. levi and Y. goldberg. linguist regular in spars and explicit word representations. In conll, 

2014. 
[19] T. mikolov, K. chen, G. corrado, and J. dean. effici estim of word represent in vector space. 

In iclr, 2013. 
[20] T. mikolov, I. sutskever, K. chen, G. S. corrado, and J. dean. distribut represent of word and 

phrase and their compositionality. In nips. 
[21] T. mikolov, w.-t. yih, and G. zweig. linguist regular in continu space word representations. In 

hlt-naacl, page 746–751, 2013. 
[22] E. nalisnick, B. mitra, N. craswell, and R. caruana. improv document rank with dual word 

embeddings. In www, april 2016. 
[23] B. A. nosek, M. banaji, and A. G. greenwald. harvest implicit group attitud and belief from a 

demonstr web site. group dynamics: theory, research, and practice, 6(1):101, 2002. 
[24] D. pedreshi, S. ruggieri, and F. turini. discrimination-awar data mining. In sigkdd, page 560–568. 

acm, 2008. 
[25] J. pennington, R. socher, and C. D. manning. glove: global vector for word representation. In emnlp, 

2014. 
[26] K. ross and C. carter. women and news: A long and wind road. media, cultur & society, 33(8):1148– 

1165, 2011. 
[27] H. rubenstein and J. B. goodenough. contextu correl of synonymy. commun of the acm, 

8(10):627–633, 1965. 
[28] E. sapir. select write of edward sapir in language, cultur and personality, volum 342. univ of 

california press, 1985. 
[29] B. schmidt. reject the gender binary: a vector-spac operation. http://bookworm.benschmidt. 

org/posts/2015-10-30-rejecting-the-gender-binary.html, 2015. 
[30] J. P. stanley. paradigmat woman: the prostitute. paper in languag variation, page 303–321, 1977. 
[31] L. sweeney. discrimin in onlin ad delivery. queue, 11(3):10, 2013. 
[32] A. torralba and A. efros. unbias look at dataset bias. In cvpr, 2012. 
[33] P. D. turney. domain and function: A dual-spac model of semant relat and compositions. journal 

of artifici intellig research, page 533–585, 2012. 
[34] C. wagner, D. garcia, M. jadidi, and M. strohmaier. it’ a man’ wikipedia? assess gender inequ 

in an onlin encyclopedia. In ninth intern aaai confer on web and social media, 2015. 
[35] D. yogatama, M. faruqui, C. dyer, and N. A. smith. learn word represent with hierarch 

spars coding. In icml, 2015. 
[36] I. zliobaite. A survey on measur indirect discrimin in machin learning. arxiv preprint 

arxiv:1511.00148, 2015. 

9 


