






















































deep neuroevolution: genet algorithm be a competit altern for train deep neural network for reinforc learn 


deep neuroevolution: genet algorithm be a competit altern for 
train deep neural network for reinforc learn 

felip petroski such vashisht madhavan edoardo conti joel lehman kenneth O. stanley jeff clune 

uber AI lab 
{felipe.such, jeffclune}@uber.com 

abstract 
deep artifici neural network (dnns) be typ- 
ical train via gradient-bas learn al- 
gorithms, name backpropagation. evolut 
strategi (es) can rival backprop-bas algo- 
rithm such a q-learn and polici gradient 
on challeng deep reinforc learn (rl) 
problems. however, ES can be consid a 
gradient-bas algorithm becaus it perform 
stochast gradient descent via an oper sim- 
ilar to a finite-differ approxim of the 
gradient. that rais the question of whether 
non-gradient-bas evolutionari algorithm can 
work at dnn scales. here we demonstr they 
can: we evolv the weight of a dnn with a sim- 
ple, gradient-free, population-bas genet al- 
gorithm (ga) and it perform well on hard deep 
RL problems, includ atari and humanoid lo- 
comotion. the deep GA success evolv 
network with over four million free parameters, 
the larg neural network ever evolv with 
a tradit evolutionari algorithm. these re- 
sult (1) expand our sens of the scale at which 
ga can operate, (2) suggest intriguingli that 
in some case follow the gradient be not the 
best choic for optim performance, and (3) 
make immedi avail the multitud of 
techniqu that have be develop in the neu- 
roevolut commun to improv perform 
on RL problems. To demonstr the latter, we 
show that combin dnn with novelti search, 
which be design to encourag explor on 
task with decept or spars reward functions, 
can solv a high-dimension problem on which 
reward-maxim algorithm (e.g. dqn, a3c, 
es, and the ga) fail. additionally, the deep 
GA parallel good than es, a3c, and dqn, 
and enabl a state-of-the-art compact encod 
techniqu that can repres million-paramet 
dnn in thousand of bytes. 

1. introduct 
A recent trend in machin learn and AI research be that 
old algorithm work remark well when combin with 
suffici comput resourc and data. that have be 
the stori for (1) backpropag appli to deep neu- 
ral network in supervis learn task such a com- 
puter vision (krizhevski et al., 2012) and voic recog- 
nition (seid et al., 2011), (2) backpropag for deep 
neural network combin with tradit reinforc 
learn algorithms, such a q-learn (watkin & dayan, 
1992; mnih et al., 2015) or polici gradient (pg) method 
(sehnk et al., 2010; mnih et al., 2016), and (3) evolut 
strategi (es) appli to reinforc learn bench- 
mark (saliman et al., 2017). one common theme be 
that all of these method be gradient-based, includ es, 
which involv a gradient approxim similar to finit 
differ (williams, 1992; wierstra et al., 2008; sali- 
man et al., 2017). thi histor trend rais the question 
of whether a similar stori will play out for gradient-fre 
methods, such a population-bas gas. 

thi paper investig that question by test the perfor- 
manc of a simpl GA on hard deep reinforc learn 
(rl) benchmarks, includ atari 2600 (bellemar et al., 
2013; brockman et al., 2016; mnih et al., 2015) and hu- 
manoid locomot in the mujoco simul (todorov 
et al., 2012; schulman et al., 2015; 2017; brockman et al., 
2016). We compar the perform of the GA with that 
of contemporari algorithm appli to deep RL (i.e. dqn 
(mnih et al., 2015), a q-learn method, a3c (mnih et al., 
2016), a polici gradient method, and es). one might ex- 
pect ga to perform far bad than other method becaus 
they be so simpl and do not follow gradients. surpris- 
ingly, we found that ga turn out to be a competit al- 
gorithm for RL – perform good on some domain and 
bad on other – add a new famili of algorithm to the 
toolbox for deep RL problems. We also valid the effec- 
tive of learn with ga by compar their perfor- 
manc to that of random search. while the GA alway out- 
perform random search, interestingli we discov that 
in some atari game random search outperform power- 

ar 
X 

iv 
:1 

71 
2. 

06 
56 

7v 
2 

[ 
c 

.N 
E 

] 
4 

J 
an 

2 
01 

8 



ful deep RL algorithm (dqn on 4/13 game and a3c on 
5/13) and ES (3/13), suggest that these domain may 
be easi than previous thought, at least for some algo- 
rithms, and that local optima, saddl points, noisi gradi- 
ent estimates, or some other forc be imped progress on 
these problem for gradient-bas methods. note that al- 
though deep neural network often do not struggl with lo- 
cal optimum in supervis learn (pascanu et al., 2014), 
local optimum remain an issu in RL becaus the reward sig- 
nal may decept encourag the agent to perform action 
that prevent it from discov the global optim behav- 
ior. 

like ES and the deep RL algorithms, the GA have uniqu 
benefits. one be that, through a new techniqu we intro- 
duce, ga turn out to be faster than ES due to their be- 
ing even more amen to parallelization. the GA and ES 
be both faster than q-learn and polici gradient method 
when substanti distribut comput be avail (here, 
720 cpu core across dozen of machines). anoth bene- 
fit be that, via thi same new technique, even multi-million- 
paramet network train by ga can be encod with 
veri few (thousand of) bytes, yield what we believ to 
be a state-of-the-art compact encod method. 

In general, the unexpectedli competit perform of 
the GA (and even random search) suggest that the struc- 
ture of the search space in some of these domain be not 
alway amen to gradient-bas search. that realiza- 
tion can open up new research directions, focu on when 
and how to exploit the region where a gradient-fre search 
might be more appropriate. It also expand the toolbox of 
idea and method applic for RL and may lead to new 
kind of hybrid algorithms. 

2. background 
At a high level, an RL problem challeng an agent to max- 
imiz some notion of cumul reward (e.g. total, or dis- 
counted) for a give problem without supervis a to how 
to accomplish that goal (sutton & barto, 1998). A host 
of tradit RL algorithm perform well on small, tab- 
ular state space (sutton & barto, 1998). however, scal- 
ing to high-dimension problem (e.g. learn to act di- 
rectli from pixels) be challeng until RL algorithm 
har the represent power of deep neural net- 
work (dnns), thu catalyz the field of deep reinforce- 
ment learn (deep rl) (mnih et al., 2015). three broad 
famili of deep learn algorithm have show promis 
on RL problem so far: q-learn method such a dqn 
(mnih et al., 2015), polici gradient method (sehnk et al., 
2010) (e.g. a3c (mnih et al., 2016), trpo (schulman 
et al., 2015), ppo (schulman et al., 2017)), and more re- 
centli evolut strategi (es) (saliman et al., 2017). 

deep q-learn algorithm approxim the optim Q 
function with dnns, yield polici that, for a give 
state, choos the action that maxim the q-valu 
(watkin & dayan, 1992; mnih et al., 2015; hessel et al., 
2017). polici gradient method directli learn the parame- 
ter of a dnn polici that output the probabl of take 
each action in each state. A team from openai recent 
experi with a simplifi version of natur evolu- 
tion strategi (wierstra et al., 2008), specif one that 
learn the mean of a distribut of parameters, but not it 
variance. they found that thi algorithm, which we will 
refer to simpli a evolut strategi (es), be competit 
with dqn and a3c on difficult RL benchmark problems, 
with much faster train time (i.e. faster wall-clock time 
when mani cpu be available) due to good paralleliza- 
tion (saliman et al., 2017). 

all of these method can be consid gradient-bas 
methods, a they all calcul or approxim gradient in a 
dnn and optim those paramet via stochast gradient 
descent/asc (although they do not requir differenti 
through the reward function, such a a simulator). dqn 
calcul the gradient of the loss of the dnn q-valu func- 
tion approxim via backpropagation. polici gradient 
sampl behavior stochast from the current polici and 
then reinforc those that perform well via stochast gradi- 
ent ascent. ES do not calcul gradient analytically, but 
instead approxim the gradient of the reward function in 
the paramet space (saliman et al., 2017; wierstra et al., 
2008). 

here we test whether a truli gradient-fre method – a sim- 
ple GA – can perform well on challeng deep RL tasks. 
We find ga perform surprisingli well and thu can be 
consid a new addit to the set of algorithm for deep 
RL problems. We first describ our genet algorithm and 
other contemporari methods, and then report the experi- 
mental result that lead to our conclusions. 

3. method 
the next section describ the method use in thi paper’ 
experiments, namely, the GA that be appli across all ex- 
periments, and the novelti search algorithm with which the 
GA be combin in one set of experiments. 

3.1. genet algorithm 

We purpos test with an extrem simpl GA to set a 
baselin for how well gradient-fre evolutionari algorithm 
work for RL problems. We expect futur work to reveal 
that add the legion of enhanc that exist for ga 
(fogel & stayton, 1994; haupt & haupt, 2004; caponetto 
et al., 2003; clune et al., 2011; mouret & doncieux, 2009; 
lehman & stanley, 2011b; pugh et al., 2016; stanley et al., 



2009; stanley, 2007; mouret & clune, 2015) will improv 
their perform on deep RL tasks. 

A genet algorithm (holland, 1992; eiben et al., 2003) 
evolv a popul P of N individu (here, neural net- 
work paramet vector θ, often call genotypes). At ev- 
eri generation, each θi be evaluated, produc a fit 
score (aka reward) F (θi). our GA variant perform trun- 
cation selection, wherein the top T individu becom the 
parent of the next generation. To produc the next genera- 
tion, the follow process be repeat N − 1 times: A par- 
ent be select uniformli at random with replac and be 
mutat by appli addit gaussian nois to the param- 
eter vector: θ′ = θ + σ� where � ∼ N (0, i). the appro- 
priat valu of σ be determin empir for each ex- 
periment, a describ in supplementari inform (si) 
tabl 3. the N th individu be an unmodifi copi of the 
best individu from the previou generation, a techniqu 
call elitism. historically, ga often involv crossov 
(i.e. combin paramet from multipl parent to pro- 
duce an offspring), but for simplic we do not includ 
it. the new popul be then evalu and the process 
repeat for G gener or until some other stop cri- 
terion be met. algorithm 1 outlin pseudo-cod for thi 
approach. 

We plan to releas open sourc code and paramet config- 
urat for all of our experi a soon a possible. hy- 
perparamet be fix for all atari games, chosen onc 
at the outset befor ani experiment base on our intu- 
ition for which one would work well for thi architectur 
size. the result on atari would thu like improv with 
differ hyperparameters. We do run an extens hyper- 
paramet search for the humanoid locomot task. the 
hyperparamet for all experi be list in si. 

algorithm 1 simpl genet algorithm 
input: mutat power σ, popul size N , number of 
select individu T , polici initi routin φ. 
for g = 1, 2...g gener do 

for i = 1, ..., N in next generation’ popul do 
if g = 1 then 
pgi = φ(n (0, i)) {initi random dnn} 
F gi = F (P 

g 
i ) {assess it fitness} 

els 
if i = 1 then 
pgi = P 

g−1 
i ;F 

g 
i = F 

g−1 
i {copi the elite} 

els 
k = uniformrandom(1, T ) {select parent} 
sampl � ∼ N (0, I) 
pgi = P 

g−1 
k + σ� {mutat parent} 

F gi = F (P 
g 
i ) {assess it fitness} 

sort Pg and F g with descend order by F g 
return: high perform policy, pg1 

GA implement tradit store each individu a 
a paramet vector θ, but thi approach scale poorli in 
memori and network transmiss cost with larg popu- 
lation and paramet vector (e.g. deeper and wider neural 
networks). We propos a novel method to store larg pa- 
ramet vector compactli by repres each paramet 
vector a an initi seed plu the list of random seed 
that produc the seri of mutat appli to θ. thi inno- 
vation be essenti to enabl ga to work at the scale of 
deep neural networks, and we thu call it a deep ga. thi 
techniqu also have the benefit of offer a state-of-the-art 
compress method (section 5). 

one motiv for choos ES versu q-learn and pol- 
ici gradient method be it faster wall-clock time with dis- 
tribut computation, owe to good parallel (sal- 
iman et al., 2017). We found that the deep GA not onli 
preserv thi benefit, but slightli improv upon it. the 
GA be faster than ES for two main reasons: (1) for everi 
generation, ES must calcul how to updat it neural net- 
work paramet vector θ. It do so via a weight aver- 
age across mani (10,000 in saliman et al. 2017) pseudo- 
offspr (random θ perturbations) weight by their fit- 
ness. thi averag oper be slow for larg neural net- 
work and larg number of pseudo-offspr (the latter be 
requir for healthi optimization), and be not requir for 
the deep ga. (2) the ES requir requir virtual batch nor- 
maliz to gener divers polici amongst the pseudo- 
offspring, which be necessari for accur finit differ 
approxim (saliman et al., 2016). virtual batch nor- 
maliz requir addit forward pass for a refer- 
enc batch–a random set of observ chosen at the start 
of training–to comput layer normal statist that 
be then use in the same manner a batch normal 
(ioff & szegedy, 2015). We found that the random GA pa- 
ramet perturb gener suffici divers polici 
without virtual batch normal and thu avoid these 
addit forward pass through the network. 

3.2. novelti search 

one benefit of train deep neural network with simpl 
ga be that do so enabl u to immedi take advan- 
tage of algorithm previous develop in the neuroevolu- 
tion community. As a demonstration, we experi with 
novelti search (ns) (lehman & stanley, 2011a), which 
be design for decept domain in which reward-bas 
optim mechan converg to local optima. NS 
avoid these local optimum by ignor the reward function 
dure evolut and instead reward agent for perform- 
ing behavior that have never be perform befor (i.e. 
that be novel). surprisingly, it can often outperform algo- 
rithm that util the reward signal, a result demonstr 
on maze navig and simul bipe locomot task 
(lehman & stanley, 2011a). here we appli NS to see 



how it perform when combin with dnn on a decep- 
tive image-bas RL problem (that we call the imag hard 
maze). We refer to the GA that optim for novelti a 
ga-ns. 

NS requir a behavior characterist (bc) that describ 
the behavior of a polici bc(π) and a behavior dis- 
tanc function between the bc of ani two policies: 
dist(bc(πi), bc(πj)). after each generation, member 
of the popul have a probabl p (here, 0.01) of hav- 
ing their BC store in an archive. the novelti of a polici 
be defin a the averag distanc to the k (here, 25) near- 
est neighbor (sort by behavior distance) in the pop- 
ulat or archive. novel individu be thu determin 
base on their behavior distanc to current or previous 
see individuals. the GA otherwis proce a normal, 
substitut novelti for fit (reward). for report and 
plot purpos only, we identifi the individu with the 
high reward per generation. the algorithm be present 
in SI sec. 8. 

4. experi 
our experi focu on the perform of the GA on 
the same challeng problem that have valid the ef- 
fectiv of state-of-the-art deep RL algorithm and ES 
(saliman et al., 2017). they includ learn to play atari 
directli from pixel (mnih et al., 2015; schulman et al., 
2017; mnih et al., 2016; bellemar et al., 2013) and a con- 
tinuou control problem involv a simul humanoid 
robot learn to walk (brockman et al., 2016; schulman 
et al., 2017; saliman et al., 2017; todorov et al., 2012). We 
also test on an atari-scal maze domain that have a clear 
local optimum (imag hard maze) to studi how well these 
algorithm avoid decept (lehman & stanley, 2011a). 

for all our experi we record the best agent found 
in each of multiple, independent, randomli initi GA 
runs: 5 for the atari domains, 5 for humanoid locomotion, 
and 10 for the imag hard maze. dure evolution, each 
agent be evalu n time (n=1 for atari and imag hard 
maze domain and n=5 for the humanoid locomot do- 
main) and fit be the mean reward. however, becaus 
fals posit can aris with so few evaluations, for report- 
ing (only) at everi generation, the individu with high 
mean fit (the elite) be re-evalu 30 time to good 
estim it true mean fitness. the high 30-evalu 
reward across all gener be consid the final reward 
of an individu run. for each treatment, we then report 
the median valu across run of those final per-run reward 
values. 

4.1. atari 

train deep neural network to play atari – map di- 
rectli from pixel to action – be a celebr feat that 
arguabl launch the deep RL era and expand our un- 
derstand of the difficulti of RL domain that machin 
learn could tackl (mnih et al., 2015). here we test how 
the perform of dnn evolv by a simpl GA com- 
pare to dnn train by the major famili of deep RL al- 
gorithm and es. due to limit comput resources, 
we compar result on 13 atari games. some be chosen 
becaus they be game on which ES perform well (frost- 
bite, gravitar, kangaroo, venture, zaxxon) or poorli (ami- 
dar, enduro, skiing, seaquest) and the remain game 
be chosen from the ale (bellemar et al., 2013) set in 
alphabet order (assault, asterix, asteroids, atlantis). 
To facilit comparison with result report in saliman 
et al. (2017), we keep the number of game frame agent 
experi over the cours of a GA run constant (at one bil- 
lion frames). the frame limit result in a differ number 
of gener per independ GA run (si sec. tabl 2), a 
polici of differ qualiti in differ run may see more 
frame in some game (e.g. if the agent life longer). 

dure training, each agent be evalu on a full episod 
(cap at 20k frames), which can includ multipl lives, 
and fit be the final episod reward. the follow be 
ident to dqn (mnih et al., 2015): (1) data preprocess- 
ing, (2) network architecture, and (3) the stochast envi- 
ronment that start each episod with up to 30 random, ini- 
tial no-op operations. We use the larg dqn architectur 
from mnih et al. (2015) consist of 3 convolut lay- 
er with 32, 64, and 64 channel follow by a hidden layer 
with 512 units. the convolut layer use 8 × 8, 4 × 4, 
and 3 × 3 filter with stride of 4, 2, and 1, respectively. 
all hidden layer be follow by a rectifi nonlinear 
(relu). the network contain over 4M parameters. 

compar our result with those from other algorithm 
fairli be extrem difficult, a such comparison be in- 
herent appl and orang in mani differ ways. one 
import consider be whether agent be evalu on 
random start (a random number of no-op actions), which 
be the regim they be train on, or start randomli sam- 
plead from human play, which test for gener (nair 
et al., 2015). becaus we do not have a databas of hu- 
man start to sampl from, our agent be evalu with 
random starts. where possible, we compar our result to 
those for other algorithm for which such random start re- 
sult be available. that be true for dqn and es, but not 
true for a3c, where we have to includ result on human 
starts. 

We also attempt to control for the number of frame see 
dure training, but becaus dqn be far slow to run, we 
present result from the literatur that train on few frame 



(200m, which requir 7-10 day of comput vs. hour 
of comput need for ES and the GA to train on 1B 
frames). there be mani variant of dqn that we could 
compar to, includ the rainbow (hessel et al., 2017) al- 
gorithm that combin mani differ recent improv 
to dqn (van hasselt et al., 2016; wang et al., 2015; schaul 
et al., 2015; sutton & barto, 1998; bellemar et al., 2017; 
fortunato et al., 2017). however, we choos to compar 
the GA to the original, vanilla dqn algorithm, partli be- 
caus we also introduc a vanilla ga, without the mani 
modif and improv that have be previous 
develop (haupt & haupt, 2004). 

In what will like be a surpris to many, the simpl GA 
be abl to train deep neural network to play mani atari 
game roughli a well a dqn, a3c, and ES (tabl 1). 
among the 13 game we tried, dqn, es, and the GA 
each produc the best score on 3 games, while a3c pro- 
duce the best score on 4. On skiing, the GA produc 
a score high than ani other algorithm to date that we 
be awar of, includ all the dqn variant in the rain- 
bow dqn paper (hessel et al., 2017). On some games, 
the GA perform advantag over dqn, a3c, and ES be 
consider (e.g. frostbite, venture, skiing). video of 
polici evolv by the GA can be view here: https: 
//goo.gl/qbhdj9. In a head-to-head comparison on 
these 13 games, the GA perform good on 6 vs. es, 6 vs. 
a3c, and 5 vs. dqn. 

there be also mani game in which the GA perform 
worse, continu a theme in deep RL where differ fam- 
ili of algorithm perform differ across differ do- 
main (saliman et al., 2017). We note that all such com- 
parison be preliminari becaus we do not have the com- 
putat resourc to gather suffici sampl size (and 
test on enough games) to see if the algorithm be signif- 
icantli different; instead the key takeaway be that they all 
tend to perform roughli similarli in that each do well on 
differ games. 

becaus perform do not plateau in the GA runs, we 
test whether the GA improv further give addit com- 
putation. We thu run the GA four time longer (4b frames) 
and in all game but one, it score improv (tabl 1). with 
these post-4b-fram scores, the GA outperform each of 
the other algorithm (a3c, es, and dqn) on 7 of the 13 
game in head-to-head comparisons. In most games, the 
ga’ perform still have not converg at 4B frames, 
leav open the question of to how well the GA will ul- 
timat perform when run even longer. To our knowledge, 
thi 4m+ paramet neural network be the larg neural net- 
work ever evolv with a simpl ga. 

one remark fact be how quickli the GA find high- 
perform individuals. becaus we employ a larg popu- 
lation size (5,000), each run last rel few gener 

(min 72, max 409, SI sec. tabl 2). In fact, in mani games, 
the GA find a solut good than dqn in onli one or ten 
of generations! specifically, the median GA perform 
be high than the final dqn perform in 1, 1, 1, 29, 
and 58 gener for frostbite, venture, skiing, gravitar, 
and kangaroo, respectively. similar result hold for es, 
where 1, 1, 3, and 14 gener of the GA be need 
to obtain high perform than ES for frostbite, skiing, 
amidar, and venture, respectively. the number of gener- 
ation requir to beat a3c be 1, 1, 1, 1, 1, 16, and 52 
for enduro, frostbite, kangaroo, skiing, venture, gravitar, 
and amidar, respectively. 

each generation, the GA tend to make small-magnitud 
chang to the paramet vector control by σ (see meth- 
ods). that the GA outperform dqn, a3c, and ES in so 
few gener – especi when it do so in the first 
gener (which be befor a round of selection) – suggest 
that mani high-qual polici exist near the origin (to be 
precise, in or near the region in which the random initializa- 
tion function gener policies). that rais the question: 
be the GA do anyth more than random search? 

To answer thi question, we evalu mani polici ran- 
domli gener by the ga’ initi function φ and 
report the best score. We give random search approx- 
imat the same amount of frame and comput a 
the GA and compar their perform (tabl 1). In ev- 
eri case, the GA significantli outperform random search 
(fig. 1, p < 0.05, thi and all futur p valu be via a 
wilcoxon rank-sum test). the improv perform sug- 
gest the GA be perform healthi optim over gen- 
erations. 

surprisingly, give how celebr and impress dqn, 
ES and a3c are, random search actual outperform dqn 
on 4 out of 13 game (asteroids, frostbite, skiing, & ven- 
ture), ES on 3 (amidar, frostbite, & skiing), and a3c on 
5 (enduro, frostbite, kangaroo, skiing, & venture). inter- 
estingly, some of these polici produc by random search 
be not trivial, degener policies. instead, they appear 
quit sophisticated. consid the follow exampl from 
the game frostbite, which requir an agent to perform a 
long sequenc of jump up and down row of iceberg mov- 
ing in differ direct (while avoid enemi and op- 
tional collect food) to build an igloo brick by brick 
(fig. 2). onli after the igloo be built can the agent en- 
ter the igloo to receiv a larg payoff. over it first two 
lives, a polici found by random search complet a seri 
of 17 actions, jump down 4 row of iceberg move in 
differ direct (while avoid enemies) and back up 
again three time to construct an igloo. then, onli onc 
the igloo be built, the agent immedi move toward 
it and enter it, at which point it get a larg reward. It 
then repeat the entir process on a harder level, thi time 

https://goo.gl/qbhdj9 
https://goo.gl/qbhdj9 


dqn evolut strategi random search GA GA a3c 
frames, time 200m, ∼7-10d 1b, ∼ 1h 1b, ∼ 1h 1b, ∼ 1h 4b, ∼ 4h 1.28b, 4d 
forward pass 450m 250m 250m 250m 1B 960m 
backward pass 400m 0 0 0 0 640m 
oper 1.25b U 250m U 250m U 250m U 1B U 2.24b U 

amidar 978 112 151 216 294 264 
assault 4,280 1,674 642 819 1,006 5,475 
asterix 4,359 1,440 1,175 1,885 2,392 22,140 
asteroid 1,365 1,562 1,404 2,056 2,056 4,475 
atlanti 279,987 1,267,410 45,257 79,793 125,883 911,091 
enduro 729 95 32 39 50 -82 
frostbit 797 370 1,379 4,801 5,623 191 
gravitar 473 805 290 462 637 304 
kangaroo 7,259 11,200 1,773 8,667 10,920 94 
seaquest 5,861 1,390 559 807 1,241 2,355 
ski -13,062 -15,442 -8,816 -6,995 -6,522 -10,911 
ventur 163 760 547 810 1,093 23 
zaxxon 5,363 6,380 2,943 5,183 6,827 24,622 

tabl 1. the atari result reveal a simpl genet algorithm be competit with q-learn (dqn), polici gradient (a3c), and 
evolut strategi (es). shown be game score (higher be better). compar perform between algorithm be inher chal- 
leng (see main text), but we attempt to facilit comparison by show estim for the amount of comput (operations, the 
sum of forward and backward neural network passes), data effici (the number of game frame from train episodes), and how 
long in wall-clock time the algorithm take to run. the ga, dqn, and es, perform best on 3 game each, while a3c win on 4 games. 
surprisingly, random search often find polici superior to those of dqn, a3c, and ES (see text for discussion). note the dramat 
differ in the speed of the algorithm, which be much faster for the GA and es, and data efficiency, which favor dqn. the score 
for dqn be from hessel et al. (2017) while those for a3c and ES be from saliman et al. (2017). for a3c, dqn, and es, we cannot 
provid error bar becaus they be not report in the origin literature; GA and random search error bar be visual in (fig. 1). 
the wall-clock time be approxim becaus they depend on a varieti of hard-to-control-for factors. We found the GA run slightli 
faster than ES on average. 

also gather food and thu earn bonu point (video: 
https://youtu.be/cghgenv1hii). that polici re- 
sult in a veri high score of 3,620 in less than 1 hour of 
random search, vs. an averag score of 797 produc by 
dqn after 7-10 day of optimization. one may think that 
random search found a lucki open loop sequenc of ac- 
tion overfit to that particular stochast environment. re- 
markably, we found that thi polici actual gener to 
other initi condit too, achiev a median score of 
3,170 (with 95% bootstrap median confid interv 
of 2,580 - 3,170) on 200 differ test environ (each 
with up to 30 random initi no-ops, a standard test pro- 
cedur (hessel et al., 2017; mnih et al., 2015)). 

these exampl and the success of RS versu dqn, a3c, 
and ES suggest that mani atari game that seem hard base 
on the low perform of lead deep RL algorithm may 
not be a hard a we think, and instead that these algorithm 
for some reason be perform extrem poorli on task 
that be actual quit easy. they further suggest that some- 
time the best search strategi be not to follow the gradient, 
but instead to conduct a dens search in a local neighbor- 
hood and select the best point found, a subject we return to 

in the discuss (sec. 6). 

4.2. humanoid locomot 

We next test the GA on a challeng continu control 
problem, specif humanoid locomot (fig. 3a). We 
test with the mujoco humanoid-v1 environ in ope- 
nai gym (todorov et al., 2012; brockman et al., 2016), 
which involv a simul humanoid robot learn to 
walk. solv thi problem have valid modern, pow- 
er algorithm such a a3c (mnih et al., 2016), trpo 
(schulman et al., 2015), and ES (saliman et al., 2017). 

thi problem involv map a vector of 376 scalar that 
describ the state of the humanoid (e.g. it position, ve- 
locity, angle) to 17 joint torques. the robot receiv a 
scalar reward that be a combin of four compon 
each timestep. It get posit reward for stand and it 
veloc in the posit x direction, and neg reward the 
more energi it expend and for how hard it impact the 
ground. these four term be sum over everi timestep 
in an episod to calcul the total reward. 

To stabil training, we normal each dimens of the 

https://youtu.be/cghgenv1hii 


figur 1. GA and random search perform across gener on atari 2600 games. the GA significantli outperform random 
search in everi game (p < 0.05). the perform of the GA and random search to dqn, a3c, and ES depend on the game. We plot 
final score (a dash lines) for dqn, a3c, and ES becaus we do not have their perform valu across train and becaus they 
train on differ number of game frame (see tabl 1). for GA and rs, we report the median and 95% bootstrap confid 
interv of the median across 5 experi of the best mean evalu score (over 30 stochast rollouts) see up to that point in 
training. 

input space separ by subtract the mean and divid 
by the varianc of data for that input gather in the domain 
by 10,000 random policies. We also appli anneal to 
the mutat power σ, decreas it to 0.001 after 1,000 
generations, which result in a small perform boost 
at the end of training. 

the architectur have two 256-unit hidden layer with tanh 
activ functions. thi architectur be the one in the 
configur file includ in the sourc code releas by 
saliman et al. (2017). the architectur describ in their 
paper be similar, but smaller, have 64 neuron per layer 
(saliman et al., 2017). although rel shallow by 
deep learn standards, and much small than the atari 
dnns, thi architectur still contain ∼167k parameters, 
which be order of magnitud great than the larg neu- 
ral network evolv for robot task that we be awar 

of, which contain 1,560 (huizinga et al., 2016) and be- 
fore that 800 paramet (clune et al., 2011). mani as- 
sum evolut would fail at larg scale (e.g. network 
with hundr of thousand or million of weights, a in 
thi paper). previou work have call the problem solv 
with a score around 6,000 (saliman et al., 2017). the GA 
achiev a median abov that level after ∼1,500 genera- 
tions. however, it requir far more comput than ES 
to do so (e requir ∼100 gener for median per- 
formanc to surpass the 6,000 threshold). It be not clear 
whi the GA requir so much more comput and hy- 
perparamet tune on thi problem, especi give how 
quickli the GA found high-perform polici in the atari 
domain. while the GA need far more comput in thi 
domain, it be interest nevertheless that it do eventu- 
alli solv it by produc an agent that can walk and score 
over 6,000. consid it veri fast discoveri of high- 



figur 2. exampl of high-perform individu on frostbit 
found through random search. see text for a descript of the 
behavior of thi policy. it final score be 3,620 in thi episode, 
which be high than the score produc by dqn, a3c and es, 
although not a high a the score found by the GA (tabl 1). 

(a) (b) 

figur 3. two differ test domains. left: the human loco- 
motion domain. the humanoid robot have to learn to walk effi- 
ciently. shown be an exampl polici evolv with a ga. right: 
the imag hard maze domain. A small wheel robot must nav- 
igat to the goal with thi bird’s-ey view a pixel inputs. shown 
be an exampl imag frame a see by the robot’ neural network. 
the text annot and arrow be not visibl to the robot. the 
robot start in the bottom left corner face right. 

perform solut in atari, clearli the ga’ advantag 
versu other method depend on the domain, and under- 
stand thi depend be an import target for futur 
research. 

4.3. imag hard maze 

the hard maze domain be a stapl in the neuroevolut 
community, where it demonstr the problem of local op- 
tima (aka deception) in reinforc learn (lehman & 
stanley, 2011a). In it, a robot receiv more reward the 
closer it get to the goal. specifically, a singl reward be 
provid at the end of an episod consist of the nega- 
tive of the straight-lin distanc between the final posit 
of the agent and the goal. the problem be decept be- 
caus greedili get closer to the goal lead an agent to 
perman get stuck in one of the decept trap in the 
maze (fig. 3b). optim algorithm that do not con- 
duct suffici explor suffer thi fate. novelti search 
(ns) solv thi problem easili becaus it ignor the re- 
ward entir and encourag agent to visit new places, 
which ultim lead to some reach the goal (lehman 

& stanley, 2011a). 

the origin version of thi problem involv onli a few 
input (radar sensor to sens walls) and two continu 
outputs, one that control speed (forward or backward) and 
anoth that control rotation, make it solvabl by small 
neural network (on the order of ten of connections). be- 
caus here we want to demonstr the benefit of NS at 
the scale of deep neural networks, we introduc a new ver- 
sion of the domain call imag hard maze. like mani 
atari games, it show a bird’s-ey view of the world to 
the agent in the form of an 84 × 84 pixel image. thi 
chang make the problem easi in some way (e.g. now 
it be fulli observable), but harder becaus it be much higher- 
dimensional: the neural network must learn to process thi 
pixel input and take actions. for tempor context, the cur- 
rent frame and previou three frame be all input at each 
timestep, follow mnih et al. (2015). An exampl frame 
be show in fig. 3b. the output remain the same a in the 
origin problem formulation. 

follow previou work in the origin hard maze 
(lehman & stanley, 2011a), the BC be the (x, y) posit 
of the robot at the end of the episod (400 timesteps), and 
the behavior distanc function be the squar euclidean 
distanc between these final (x, y) positions. both the en- 
viron and the agent be deterministic. the simpl sim- 
ulat reject forward or backward motion that result in 
the robot penetr wall (i.e. the posit of the robot 
remain unchang from the previou timestep in such 
cases); these dynam prohibit a robot from slide along 
a wall, although rotat motor command still have their 
usual effect in such situations. 

We confirm that the result that held for small neural net- 
work on the original, radar-bas version of thi task also 
hold for the high-dimensional, visual version of thi task 
with deep neural networks. with a 4m+ paramet net- 
work process pixels, the ga-bas novelti search (ga- 
ns) be abl to solv the task by find the goal (fig. 4). 
the GA optim for reward onli and, a expected, get 
stuck in the local optimum of trap 2 (fig. 5) and thu fail 
to solv the problem (fig. 4), significantli underperform- 
ing ga-n (p < 0.001). these result thu confirm that 
intuit gain in small neural network about local op- 
tima in reward function hold for much larger, deep neural 
networks. while local optimum may not be a problem with 
deep neural network in supervis learn where the cor- 
rect answer be alway give (pascanu et al., 2014), the same 
be not true in reinforc learn problem with spars 
or decept rewards. our result confirm that we be abl 
to use explor method such a novelti search to solv 
thi sort of deception, even in high-dimension problem 
such a those involv learn directli from pixels. 

thi be the larg neural network optim by novelti 



search to date by three order of magnitude. In an com- 
panion paper (conti et al., 2017), we also demonstr a 
similar finding, by hybrid novelti search with ES to 
creat ns-es, and show that it too can help deep neural net- 
work avoid decept in challeng RL benchmark do- 
mains. We hope these result encourag more investig 
into combin deep neural network with novelti search 
and similar methods, such a qualiti divers algorithms, 
which seek to collect a set of high-performing, yet interest- 
ingli differ polici (lehman & stanley, 2011b; culli 
et al., 2015; pugh et al., 2016). 

figur 4. imag hard maze result reveal that novelti search 
can train deep neural network to avoid local optimum that 
stymi other algorithms. the ga, which sole optim for 
reward and have no incent to explore, get stuck on the local 
optimum of trap 2 (the goal and trap be visual in fig. 3b). 
the GA optim for novelti (ga-ns) be encourag to ignor 
reward and explor the whole map, enabl it to eventu find 
the goal. ES perform even bad than the ga, a discuss in the 
main text. dqn and a2c also fail to solv thi task. for es, the 
perform of the mean θ polici each iter be plotted. for 
GA and ga-ns, the perform of the highest-scor individ- 
ual per gener be plotted. becaus dqn and a2c do not have 
the same number of evalu per iter a the evolutionari 
algorithms, we plot their final median reward a dash lines. fig. 
5 show the behavior of these algorithm dure training. 

As expected, ES also fail to solv the task becaus it fo- 
cu sole on maxim reward (fig. 5). It be surpris- 
ing, however, that it significantli underperform the GA 
(p < 0.001). In 8 of 10 run it get stuck near trap 1, 
not becaus of deception, but instead seemingli becaus 
it cannot reliabl learn to pa through a small bottleneck 
corridor. thi phenomenon have never be observ with 
population-bas gas, suggest the ES (at least with 
these hyperparameters) be qualit differ than ga 
in thi regard (lehman et al., 2017). We believ thi differ- 

enc occur becaus ES optim for the averag reward 
of the popul sampl from a probabl distribution. 
even if the maximum fit of agent sampl from that 
distribut be high further along a corridor, ES will not 
move in that direct if the averag popul be low 
(e.g. if other polici sampl from the distribut crash 
into the walls, or experi other low-reward fates). In a 
companion paper, we investig thi interest differ 
between ES and ga (lehman et al., 2017). note, how- 
ever, that even when ES move through thi bottleneck (2 
out of 10 runs), becaus it be sole reward-driven, it get 
stuck in trap 2. 

We also test q-learn (dqn) and polici gradient on thi 
problem. We do not have sourc code for a3c, but be 
abl to obtain sourc code for a2c, which have similar per- 
formanc (wu et al., 2017): the onli differ (explain 
whi it have one few ‘a’) be that it be synchron instead 
of asynchronous. for these experi we modifi the 
reward of the domain to step-by-step reward (the nega- 
tive chang in distanc to goal sinc the last time-step), but 
for plot purposes, we record the final distanc to the 
goal. have per-step reward be more standard for these 
algorithm and give them more information, but do not 
remov the deception. becaus dqn requir discret out- 
puts, for it we discret each of the two continu output 
into to five equal size bins. becaus it need to be abl 
to specifi all possibl output combinations, it thu learn 
52 = 25 q-values. 

also a expected, dqn and a2c fail to solv thi problem 
(fig. 4, fig. 5). their default explor mechan be 
not enough to find the global optimum give the decept 
reward function in thi domain. dqn be drawn into the 
expect trap 2. for reason that be not clear to us, even 
though a2c visit trap 2 frequent earli in training, it 
converg on get stuck in a differ part of the maze. 

overal the result for the imag hard maze domain con- 
firm that the deep GA allow algorithm develop for 
small-scal neural network to oper at dnn scale, and 
can thu be har on hard, high-dimension problem 
that requir dnns. In futur work, it will be interest- 
ing to test the benefit that novelti search provid when 
combin with a GA on more domains, includ atari 
and robot domains. more importantly, our demonstra- 
tion suggest that other algorithm that enhanc ga can 
now be combin with dnns. perhap most promis 
be those that combin a notion of divers (e.g. nov- 
elty) and qualiti (i.e. be high performing) (mouret & 
clune, 2015; mouret & doncieux, 2009; lehman & stan- 
ley, 2011b; culli et al., 2015; pugh et al., 2016). 



figur 5. how differ algorithm explor the decept imag hard maze over time. tradit reward-maxim algorithm 
do not exhibit suffici explor to avoid the local optimum (go up). In contrast, a GA optim for novelti onli (ga-ns) 
explor the entir environ and ultim find the goal. for the evolutionari algorithm (ga-ns, ga, es), blue cross repres 
the popul (pseudo-offspr for es), red cross repres the top T GA offspring, orang dot repres the final posit of GA 
elit and the current mean ES policy, and the black cross be entri in the ga-n archive. all 3 evolutionari algorithm have the same 
number of evaluations, but ES and the GA have mani overlap point becaus they revisit locat due to poor exploration, give 
the illus of few evaluations. for dqn and a2c, we plot the end-of-episod posit of the agent for each of the 20k episod prior 
to the checkpoint list abov the plot. 

5. compact network encod 
As mention before, the deep GA method enabl com- 
pactli store dnn by repres them a the seri of 
mutat requir to reconstruct their parameters. thi 
techniqu be advantag becaus the size of the com- 
press paramet increas with the number of genera- 
tion instead of with the size of the network (the latter be 
often much larg than the former). for example, a previ- 
ousli discussed, we be abl to evolv competit atari- 
play agent in some game in a littl a ten of gen- 
erations, enabl u to compress the represent of a 
4m+ paramet neural network to just thousand of byte 
(a factor of 10,000-fold smaller). As far a we know, thi 
repres the state of the art in encod larg network 
compactly. however, it do not count a a gener net- 
work compress techniqu becaus it cannot take an ar- 
bitrari network and compress it, and instead onli work for 

network evolv with a ga. Of course, one could har 
thi approach to creat a gener compress techniqu by 
evolv a network to match a target network, which be an 
interest area for futur research. 

the compress of a network be entir depend on 
the number of mutat need to achiev a certain per- 
formance. for humanoid locomotion, thi translat to 
encod 160,000 paramet in just 6kb (27-fold com- 
pression). thi amount be the larg number of muta- 
tion need for ani of the network we evolved. all of the 
solut on the imag hard maze domain could be repre- 
sent with 1kb or less (16,000 time smaller). the atari 
compress benefit chang depend on the game due 
varianc in gener between experi (tabl 2), but 
be alway substantial: all atari final network be com- 
pressibl to 300-2,000 byte (8,000-50,000-fold compres- 
sion). It does, of course, requir comput to reconstruct 



the dnn weight vector from thi compact encoding. 

6. discuss 
the surpris success of the GA in domain thought to 
requir at least some degre of gradient estim sug- 
gest some heretofor under-appreci aspect of high- 
dimension search spaces. the random search result 
suggest that dens sampl in a region around the ori- 
gin be suffici in some case to find far good solut 
than those found by state-of-the-art, gradient-bas meth- 
od even with far more comput or wall-clock time, 
suggest that gradient do not point to these solutions, or 
that other optim issu interfer with find them, 
such a saddl point or noisi gradient estimates. the GA 
result further suggest that sampl in the region around 
good solut be often suffici to find even good solu- 
tions, and that a sequenc of such discoveri be possibl in 
mani challeng domains. that result in turn impli that 
the distribut of solut of increas qualiti be unex- 
pectedli dense, and that you do not need to follow a gradi- 
ent to find them. 

another, non-mutu exclus hypothesis, be that ga 
have improv perform due to tempor extend ex- 
plorat (osband et al., 2016). that mean they explor 
consist sinc all action in an episod be a function of 
the same set of mutat parameters, which have be show 
to improv explor (plappert et al., 2017). such consis- 
tenci help with explor for two reasons, (1) an agent 
take the same action (or have the same distribut over ac- 
tions) each time it visit the same state, which make it eas- 
ier to learn whether the polici in that state be advantageous, 
and (2) the agent be also more like to have correl ac- 
tion across state (e.g. alway go up) becaus mutat 
to it intern represent can affect the action take in 
mani state similarly. 

perhap more interest be the result that sometim it be 
actual bad to follow the gradient than sampl local 
in the paramet space for good solutions. thi scenario 
probabl do not hold in all domains, or even in all the 
region of a domain where it sometim holds, but that it 
hold at all expand our conceptu understand of the 
viabil of differ kind of search operators. A reason 
GA might outperform gradient-bas method be if local 
optimum be present, a it can jump over them in the parame- 
ter space, wherea a gradient method cannot (without addi- 
tional optim trick such a momentum, although we 
note that ES util the modern adam optim in these 
experi (kingma & ba, 2014), which includ mo- 
mentum). one unknown question be whether ga-styl lo- 
cal, gradient-fre search be good earli on in the search pro- 
cess, but switch to a gradient-bas search late allow 
further progress that would be impossible, or prohibit 

comput expensive, for a GA to make. anoth un- 
know question be the promis of simultan hybridiz- 
ing GA method with modern algorithm for deep rl, such 
a q-learning, polici gradients, or evolut strategies. 

We emphas that we still know veri littl about the ul- 
timat promis of ga versu compet algorithm for 
train deep neural network on reinforc learn 
problems. On the atari domain, we do not perform hy- 
perparamet search, so the best GA result could be much 
high a it be well know that hyperparamet can have 
massiv effect on the perform of optim algo- 
rithms, includ ga (haupt & haupt, 2004; clune et al., 
2008). We also have not yet see the algorithm con- 
verg in most of these domains, so it ceil be unknown. 
additionally, here we use an extrem simpl ga, but 
mani techniqu have be invent to improv GA perfor- 
manc (eiben et al., 2003; haupt & haupt, 2004), includ- 
ing crossov (holland, 1992; deb & myburgh, 2016), in- 
direct encod (stanley, 2007; stanley et al., 2009; clune 
et al., 2011), and encourag divers (lehman & stan- 
ley, 2011a; mouret & clune, 2015; pugh et al., 2016) (a 
subject we do take an initi look at here with our nov- 
elti search experiments), just to name a few. moreover, 
mani techniqu have be invent that dramat im- 
prove the train of dnn with backpropagation, such a 
residu network (he et al., 2015), virtual batch normal- 
izat (saliman et al., 2016), selu or relu activ 
function (krizhevski et al., 2012; klambauer et al., 2017), 
lstm or gru (hochreit & schmidhuber, 1997; cho 
et al., 2014), regular (hoerl & kennard, 1970), 
dropout (srivastava et al., 2014), and anneal learn 
rate schedul (robbin & monro, 1951). We hypothes 
that mani of these techniqu will also improv neuroevo- 
lution for larg dnns, a subject we be current investi- 
gating. 

It be also possibl that some of these enhanc may 
remedi the poor data effici the GA show on the hu- 
manoid locomot problem. for example, indirect en- 
coding, which allow genom paramet to affect multi- 
ple weight in the final neural network (in a way similar 
to convolution’ tie weights, but with far more flexibil- 
ity), have be show to dramat improv perform 
and data effici when evolv robot gait (clune et al., 
2011). those result be found with the hyperneat al- 
gorithm (stanley et al., 2009), which have an indirect en- 
cod that abstract the power of development biolog 
(stanley, 2007), and be a particularli promis direct 
for humanoid locomot and atari that we will investi- 
gate in futur work. more generally, it will be interest 
to learn on which domain deep GA tend to perform well 
or poorli and understand why. for example, ga could 
perform well in other non-differenti domains, such a 
architectur search (liu et al., 2017; miikkulainen et al., 



2017) and for train limit precis (includ binary) 
neural networks. 

finally, it be worth note that the GA (like ES befor it) 
benefit greatli from large-scal parallel comput in 
these studies. In our experiments, each GA run be dis- 
tribut across hundr or thousand of cpus, depend 
on avail computation. that the avail of such re- 
sourc so significantli chang what be possibl with such 
a simpl algorithm motiv further invest in large- 
scale parallel comput infrastructure. while the depen- 
denc of the result on hundr or thousand of cpu in 
parallel could be view a an obstacl for some, it could 
also be interpret a an excit opportunity: a price 
continu to declin and the avail of such resourc 
becom more mainstream, more and more research 
will have the opportun to investig an entir new 
paradigm of opportunities, not unlik the transform 
gpu have enabl in deep learning. 

7. conclus 
our work introduc a deep ga, which involv a sim- 
ple parallel trick that allow u to train deep neu- 
ral network with gas. We then document that ga be 
surprisingli competit with popular algorithm for deep 
reinforc learn problems, such a dqn, a3c, and 
es, especi in the challeng atari domain. We also 
show that interest algorithm develop in the neu- 
roevolut commun can now immedi be test 
with deep neural networks, by show that a deep ga- 
power novelti search can solv a decept atari-scal 
game. It will be interest to see futur research investi- 
gate the potenti and limit of gas, especi when com- 
bin with other techniqu know to improv GA perfor- 
mance. more generally, our result continu the stori – 
start by backprop and extend with ES – that old, sim- 
ple algorithm plu modern amount of comput can 
perform amazingli well. that rais the question of what 
other old algorithm should be revisited. 

acknowledg 
We thank all of the member of uber AI lab for help- 
ful suggest throughout the cours of thi work, in par- 
ticular zoubin ghahramani, peter dayan, noah goodman, 
thoma miconi, and theofani karaletsos. We also thank 
justin pinkul, mike deats, codi yancey, and the entir 
opusstack team at uber for provid resourc and tech- 
nical support. 

refer 
bellemare, marc G, naddaf, yavar, veness, joel, and 

bowling, michael. the arcad learn environment: 
An evalu platform for gener agents. J. artif. in- 
tell. res.(jair), 47:253–279, 2013. 

bellemare, marc G, dabney, will, and munos, rémi. 
A distribut perspect on reinforc learning. 
arxiv preprint arxiv:1707.06887, 2017. 

brockman, greg, cheung, vicki, pettersson, ludwig, 
schneider, jonas, schulman, john, tang, jie, and 
zaremba, wojciech. openai gym, 2016. 

caponetto, riccardo, fortuna, luigi, fazzino, stefano, 
and xibilia, maria gabriella. chaotic sequenc to 
improv the perform of evolutionari algorithms. 
ieee transact on evolutionari computation, 7(3): 
289–304, 2003. 

cho, kyunghyun, van merriënboer, bart, bahdanau, 
dzmitry, and bengio, yoshua. On the properti of neu- 
ral machin translation: encoder-decod approaches. 
arxiv preprint arxiv:1409.1259, 2014. 

clune, jeff, misevic, dusan, ofria, charles, lenski, 
richard E, elena, santiago F, and sanjuán, rafael. nat- 
ural select fail to optim mutat rate for long- 
term adapt on rug fit landscapes. plo 
comput biology, 4(9):e1000187, 2008. 

clune, jeff, stanley, kenneth o., pennock, robert t., and 
ofria, charles. On the perform of indirect encod 
across the continuum of regularity. ieee transact 
on evolutionari computation, 2011. 

conti, edoardo, madhavan, vashisht, petroski such, fe- 
lipe, lehman, joel, stanley, kenneth o., and clune, 
jeff. improv explor in evolut strategi for 
deep reinforc learn via a popul of novelty- 
seek agents. arxiv preprint to appear, 2017. 

cully, a., clune, j., tarapore, d., and mouret, j.-b. robot 
that can adapt like animals. nature, 521:503–507, 2015. 
doi: 10.1038/nature14422. 

deb, kalyanmoy and myburgh, christie. break the 
billion-vari barrier in real-world optim use 
a custom evolutionari algorithm. In gecco ’16, 
pp. 653–660. acm, 2016. 

dhariwal, prafulla, hesse, christopher, plappert, matthias, 
radford, alec, schulman, john, sidor, szymon, and wu, 
yuhuai. openai baselines. https://github.com/ 
openai/baselines, 2017. 

eiben, agoston E, smith, jame E, et al. introduct to 
evolutionari computing, volum 53. springer, 2003. 

https://github.com/openai/baselin 
https://github.com/openai/baselin 


fogel, david B and stayton, lauren C. On the effective- 
ness of crossov in simul evolutionari optimization. 
biosystems, 32(3):171–182, 1994. 

fortunato, meire, azar, mohammad gheshlaghi, piot, bi- 
lal, menick, jacob, osband, ian, graves, alex, mnih, 
vlad, munos, remi, hassabis, demis, pietquin, olivier, 
et al. noisi network for exploration. arxiv preprint 
arxiv:1706.10295, 2017. 

glorot, xavier and bengio, yoshua. understand the dif- 
ficulti of train deep feedforward neural networks. In 
icai, pp. 249–256, 2010. 

haupt, randi L and haupt, sue ellen. practic genet 
algorithms. john wiley & sons, 2004. 

he, kaiming, zhang, xiangyu, ren, shaoqing, and sun, 
jian. deep residu learn for imag recognition. arxiv 
preprint arxiv:1512.03385, 2015. 

hessel, matteo, modayil, joseph, van hasselt, hado, 
schaul, tom, ostrovski, georg, dabney, will, horgan, 
dan, piot, bilal, azar, mohammad, and silver, david. 
rainbow: combin improv in deep reinforce- 
ment learning. arxiv preprint arxiv:1710.02298, 2017. 

hochreiter, sepp and schmidhuber, jürgen. long short- 
term memory. neural computation, 9(8):1735–1780, 
1997. 

hoerl, arthur E and kennard, robert W. ridg regression: 
bias estim for nonorthogon problems. techno- 
metrics, 12(1):55–67, 1970. 

holland, john H. genet algorithms. scientif american, 
267(1):66–73, 1992. 

huizinga, joost, mouret, jean-baptiste, and clune, jeff. 
doe align phenotyp and genotyp modular im- 
prove the evolut of neural networks? In gecco ’16, 
pp. 125–132. acm, 2016. 

ioffe, sergey and szegedy, christian. batch normalization: 
acceler deep network train by reduc intern 
covari shift. icml’15, pp. 448–456. jmlr.org, 2015. 

kingma, diederik and ba, jimmy. adam: A 
method for stochast optimization. arxiv preprint 
arxiv:1412.6980, 2014. 

klambauer, günter, unterthiner, thomas, mayr, andreas, 
and hochreiter, sepp. self-norm neural networks. 
arxiv preprint arxiv:1706.02515, 2017. 

krizhevsky, alex, sutskever, ilya, and hinton, geoffrey E. 
imagenet classif with deep convolut neural 
networks. In nips, pp. 1097–1105, 2012. 

lehman, joel and stanley, kenneth O. abandon ob- 
jectives: evolut through the search for novelti alone. 
evolutionari computation, 19(2):189–223, 2011a. 

lehman, joel and stanley, kenneth O. evolv a di- 
versiti of virtual creatur through novelti search and 
local competition. In gecco ’11: proceed of 
the 13th annual confer on genet and evolution- 
ari computation, pp. 211–218, dublin, ireland, 12-16 
juli 2011b. acm. isbn 978-1-4503-0557-0. doi: 
doi:10.1145/2001576.2001606. 

lehman, joel, chen, jay, clune, jeff, and stanley, ken- 
neth O. ES be more than just a tradit finite-differ 
approximator. arxiv preprint to appear, 2017. 

liu, hanxiao, simonyan, karen, vinyals, oriol, fernando, 
chrisantha, and kavukcuoglu, koray. hierarch rep- 
resent for effici architectur search. arxiv 
preprint arxiv:1711.00436, 2017. 

miikkulainen, risto, liang, jason, meyerson, elliot, 
rawal, aditya, fink, dan, francon, olivier, raju, 
bala, navruzyan, arshak, duffy, nigel, and hodjat, 
babak. evolv deep neural networks. arxiv preprint 
arxiv:1703.00548, 2017. 

mnih, volodymyr, kavukcuoglu, koray, silver, david, 
rusu, andrei A, veness, joel, bellemare, marc G, 
graves, alex, riedmiller, martin, fidjeland, andrea K, 
ostrovski, georg, et al. human-level control through 
deep reinforc learning. nature, 518(7540):529– 
533, 2015. 

mnih, volodymyr, badia, adria puigdomenech, mirza, 
mehdi, graves, alex, lillicrap, timothy, harley, tim, 
silver, david, and kavukcuoglu, koray. asynchron 
method for deep reinforc learning. In icml, pp. 
1928–1937, 2016. 

mouret, jean-baptist and clune, jeff. illumin 
search space by map elites. arxiv e-prints, 
abs/1504.04909, 2015. 

mouret, jean-baptist and doncieux, stephane. overcom- 
ing the bootstrap problem in evolutionari robot us- 
ing behavior diversity. In proceed of the ieee 
congress on evolutionari comput (cec-2009), 
pp. 1161–1168. ieee, 2009. 

nair, arun, srinivasan, praveen, blackwell, sam, alci- 
cek, cagdas, fearon, rory, De maria, alessandro, pan- 
neershelvam, vedavyas, suleyman, mustafa, beattie, 
charles, petersen, stig, et al. massiv parallel meth- 
od for deep reinforc learning. arxiv preprint 
arxiv:1507.04296, 2015. 



osband, ian, blundell, charles, pritzel, alexander, and 
van roy, benjamin. deep explor via bootstrap 
dqn. In nips, pp. 4026–4034, 2016. 

pascanu, razvan, dauphin, yann N, ganguli, surya, and 
bengio, yoshua. On the saddl point problem for non- 
convex optimization. arxiv preprint arxiv:1405.4604, 
2014. 

plappert, matthias, houthooft, rein, dhariwal, prafulla, 
sidor, szymon, chen, richard Y, chen, xi, asfour, 
tamim, abbeel, pieter, and andrychowicz, marcin. pa- 
ramet space nois for exploration. arxiv preprint 
arxiv:1706.01905, 2017. 

pugh, justin K, soros, lisa b., and stanley, kenneth O. 
qualiti diversity: A new frontier for evolutionari com- 
putation. 3(40), 2016. issn 2296-9144. 

robbins, herbert and monro, sutton. A stochast approx- 
imat method. the annal of mathemat statistics, 
pp. 400–407, 1951. 

salimans, t., ho, j., chen, x., sidor, s., and sutskever, I. 
evolut strategi a a scalabl altern to rein- 
forcement learning. arxiv e-prints, 1703.03864, march 
2017. 

salimans, tim, goodfellow, ian, zaremba, wojciech, che- 
ung, vicki, radford, alec, and chen, xi. improv tech- 
niqu for train gans. In nips, pp. 2234–2242, 2016. 

salimans, tim, ho, jonathan, chen, xi, and sutskever, 
ilya. evolut strategi a a scalabl altern to re- 
inforc learning. arxiv preprint arxiv:1703.03864, 
2017. 

schaul, tom, quan, john, antonoglou, ioannis, and sil- 
ver, david. priorit experi replay. arxiv preprint 
arxiv:1511.05952, 2015. 

schulman, john, levine, sergey, abbeel, pieter, jordan, 
michael, and moritz, philipp. trust region polici opti- 
mization. In icml ’15, pp. 1889–1897, 2015. 

schulman, john, wolski, filip, dhariwal, prafulla, rad- 
ford, alec, and klimov, oleg. proxim polici optimiza- 
tion algorithms. arxiv preprint arxiv:1707.06347, 2017. 

sehnke, frank, osendorfer, christian, rückstieß, thomas, 
graves, alex, peters, jan, and schmidhuber, jürgen. 
parameter-explor polici gradients. neural networks, 
23(4):551–559, 2010. 

seide, frank, li, gang, and yu, dong. convers 
speech transcript use context-depend deep neu- 
ral networks. In interspeech 2011. intern speech 
commun association, august 2011. 

srivastava, nitish, hinton, geoffrey, krizhevsky, alex, 
sutskever, ilya, and salakhutdinov, ruslan. dropout: 
A simpl way to prevent neural network from overfit- 
ting. the journal of machin learn research, 15(1): 
1929–1958, 2014. 

stanley, kenneth O. composit pattern produc net- 
works: A novel abstract of development. genet 
program and evolv machin special issu on 
development systems, 8(2):131–162, 2007. 

stanley, kenneth o., d’ambrosio, david b., and gauci, 
jason. A hypercube-bas indirect encod for evolv 
large-scal neural networks. artifici life, 15(2):185– 
212, 2009. 

sutton, richard S and barto, andrew G. reinforc 
learning: An introduction, volum 1. mit press cam- 
bridge, 1998. 

todorov, emanuel, erez, tom, and tassa, yuval. mujoco: 
A physic engin for model-bas control. In intelli- 
gent robot and system (iros), 2012 ieee/rsj inter- 
nation confer on, pp. 5026–5033. ieee, 2012. 

van hasselt, hado, guez, arthur, and silver, david. deep 
reinforc learn with doubl q-learning. In aaai, 
pp. 2094–2100, 2016. 

wang, ziyu, schaul, tom, hessel, matteo, van hasselt, 
hado, lanctot, marc, and De freitas, nando. duel 
network architectur for deep reinforc learning. 
arxiv preprint arxiv:1511.06581, 2015. 

watkins, christoph jch and dayan, peter. q-learning. 
machin learning, 8(3-4):279–292, 1992. 

wierstra, daan, schaul, tom, peters, jan, and schmid- 
huber, juergen. natur evolut strategies. In 
evolutionari computation, 2008. cec 2008.(ieee 
world congress on comput intelligence). ieee 
congress on, pp. 3381–3387. ieee, 2008. 

williams, ronald J. simpl statist gradient-follow 
algorithm for connectionist reinforc learning. 
machin learning, 8(3-4):229–256, 1992. 

wu, yuhuai, mansimov, elman, grosse, roger B, liao, 
shun, and ba, jimmy. scalabl trust-region method for 
deep reinforc learn use kronecker-factor 
approximation. In nips, pp. 5285–5294, 2017. 

8. supplementari inform 
8.1. hyperparamet 

the polici initi function φ gener an initi pa- 
ramet vector. bia weight for each neuron be set to 



zero, and connect weight be drawn from a standard 
normal distribution; these weight be then rescal so that 
the vector of incom weight for each neuron have unit 
magnitude. thi procedur be know a normal col- 
umn weight initi and the implement here be 
take from the openai baselin packag (dhariw et al., 
2017). without thi procedure, the varianc of the distribu- 
tion of a neuron’ activ depend on the number of it 
inputs, which can complic optim of dnn (glo- 
rot & bengio, 2010). other principl initi rule 
could like be substitut in it place. 

game minimum median maximum 
gener gener gener 

amidar 226 258 269 
enduro 121 121 121 
frostbit 348 409 494 
gravitar 283 292 304 
kangaroo 242 253 322 
seaquest 145 167 171 
ski 81 86 88 
ventur 71 72 75 
zaxxon 335 342 349 

tabl 2. the number of gener the GA need to reach 
4B frames. 

hyperparamet humanoid imag atari 
locomot hard maze 

popul size (n) 12,500+1 20,000+1 5,000+1 
mutat power (σ) 0.00224 0.005 0.005 
truncat size (t) 625 61 10 
number of trial 5 1 1 
archiv probabl 0.01 

tabl 3. hyperparameters. popul size be increment to 
account for elit (+1). mani of the unusu number be found 
via preliminari hyperparamet search in other domains. 

algorithm 2 novelti search (ga-ns) 
input: mutat power σ, popul size N , number 
of individu select to reproduc per gener T , 
polici initi routin φ, empti archiv A, archiv 
insert probabl p 
for g = 1, 2...g gener do 

for i = 1, ..., N in next generation’ popul do 
if g = 1 then 
pgi = φ(n (0, i)) {initi random dnn} 
bcgi = bc(p 

g 
i ) 

F gi = F (P 
g 
i ) 

els 
if i = 1 then 
pgi = P 

g−1 
i ;F 

g 
i = F 

g−1 
i {copi most novel} 

bcgi = BC 
g−1 
i 

els 
k = uniformrandom(1, T ) {select parent} 
sampl � ∼ N (0, I) 
pgi = P 

g−1 
k + σ� {mutat parent} 

bcgi = bc(p 
g 
i ) 

F gi = F (P 
g 
i ) 

for i = 1, ..., N in next gener popul do 
N gi = dist(bc 

g 
i ,A ∪bcg) 

insert bcgi into A with probabl p 
sort (pg , bcg , F g) in descend order by N g 

return: high perform polici 


