






















































learn to optim combinatori functions: supplementari materi 


learn to optim combinatori functions: 
supplementari materi 

nir rosenfeld 1 eric balkanski 1 amir globerson 2 yaron singer 1 

theorem 1. for all α > 1 and �, δ > 0, for m suffici 
large, there exist a famili of function F and a function 
mpmac(·) such that 

• for all �′, δ′ > 0: F be α-pmac-learn with sampl 
complex mpmac(n, δ′, �′, α), and 

• give strictli less than mpmac(n, δ, 1− (1− �)1/m, α) 
samples, F be not α-dops, i.e., 

mdops(n,m, δ, �, α) ≥mpmac(n, δ, 1−(1−�)1/m, α). 

proof. fixα > 1 and � > 0. defin p := 1−(1−�)1/m+�s, 
for some small constant �s > 0, and let s1, . . . , s1/p be 
1/p arbitrari distinct sets. the hard class of function be 
F = {fi}i∈[1/p] where 

fi(s) = 

{ 
α if S = Si 
1 
2 otherwis 

consid the distributiond which be the uniform distribut 
over set s1, . . . , s1/p, so Sj be drawn with probabl p 
for all j ∈ [1/p]. We first argu that the sampl complex 
for pmac-learn f over D be at most 

mpmac(n, δ 
′, �′, α) = 

{ 
0 if �′ ≥ p 

log(1/δ′) 
log(1/(1−p)) if � 

′ < p 

note that if �′ ≥ p, f̃(s) = 1/2 for all S be correct with 
probabl 1− p ≥ 1− �′ over S ∼ D and with probabl 
1 over the samples. If �′ < p, if there exist sampl Si such 
that f(si) = α, then f̃(si) = α, and f̃(s) = 1/2 for all 
other S. note that that thi be correct with probabl 1 over 
S ∼ D if Si be in the samples. the probabl that Si be in 

1harvard univers 2tel aviv university. correspond to: 
nir rosenfeld <nir.rosenfeld@g.harvard.edu>. 

proceed of the 35 th intern confer on machin 
learning, stockholm, sweden, pmlr 80, 2018. copyright 2018 
by the author(s). 

the sampl 

1− (1− p)m = 1− (1− p) 
log(1/δ′) 

log(1/(1−p)) 

= 1− e 
log(δ′) 

log(1−p) log(1−p) 

= 1− δ′. 

thus, f̃ be correct with probabl 1− δ′ over the samples. 
next, we argu that for all δ > 0 and m suffici large, 
the sampl complex for dop be at least 

mpmac(n, δ, 1− (1− �)1/m, α) = 

mpmac(n, δ, p− �s, α) = 
log(1/δ) 

log(1/(1− p)) 
. 

consid the random function fi where i ∈ [1/p] be uni- 
formli random. let F ′ be the random collect of 
function fi such that Si be in the test set but not in the 
train set. sinc Si be not in the test set, we have that 
for all fi ∈ F ′ and for all set S in the test set, 

fi(s) = 1. 

thus, the function in F ′ be indistinguish from the 
sampl in the train set. thi impli that the decis 
of the algorithm be independ of the random variabl i, 
condit on fi ∈ F ′. let S be the set in the test set 
that be return by the algorithm, we obtain that 

E 
i:fi∈f ′ 

[fi(s)] = Pr 
i:fi∈f ′ 

[S = si] · α+ Pr 
i:fi∈f ′ 

[S 6= si] · 
1 

2 

≤ α 
|F ′| 

+ 
1 

2 

sinc S be independ of i condit on fi ∈ F ′. con- 
sider the case where Si be not in the train set with prob- 
abil strictli great than δ. the probabl that Si be in 
the test set be 1− (1− p)m = �+ �s. thu a function be 
in F ′ with probabl at least δ(� + �s). note that 1/p be 
arbitrarili larg if m be arbitrarili large. thus, |F ′| > 2α 
with arbitrarili larg probabl if m be arbitrarili larg for 
fix �, δ, and α. combin with the previou inequality, 



distribut optim from samples: supplementari materi 

thi impli that 

E 
i:fi∈f ′ 

[fi(s)] < 1 = 
1 

α 
· fi(si) 

= 
1 

α 
· E 
i:fi∈f ′ 

[ max 
s∈ste 

fi(s)] 

where the last equal be sinc Si ∈ ste for all i ∈ F ′. 
thus, there exist at least one function fi ∈ F such that the 
algorithm do not obtain an α-approxim when Si be 
in the test set and not in the train set. 

the probabl that Si be in the test set be 1 − (1 − 
p)m = �+ �s. thus, Si need to be in the train set with 
probabl at least 1− δ, otherwis we don’t get an α-apx 
with probabl 1− �. the probabl that Si be not in the 
train set be (1− p)m. thus, we need δ > (1− p)m, or 

m > 
log(1/δ) 

log(1/(1− p)) 
= mpmac(n, δ, p− �s, α) 
= mpmac(n, δ, 1− (1− �)1/m, α). 


