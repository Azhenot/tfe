






















































moodexplorer: toward compound emot detect via smartphon sens 


176 
moodexplorer: toward compound emot detect via 
smartphon sens 

xiao zhang, nanj university, china 
wenzhong li∗, nanj university, china 
XU chen, sun yat-sen university, china 
sanglu lu, nanj university, china 

social psycholog and neurosci have confirm that emot state exert a signific effect on human 
communication, perception, social behavior and decis making. with the wide avail of smartphon 
equip with microphone, accelerometer, gps, and other sourc of sensors, it be worthwhil to explor the 
possibl of automat emot detect via smartphon sensing. particularly, we focu on a novel research 
problem that tri to detect the compound emot (a set of multipl dimension basic emotions) of smartphon 
users. We observ that users’ self-report emot state have high correl with their smartphon usag 
pattern and sens data. base on the observations, we exploit a featur extract and select algorithm 
to find the most signific features. We further adopt a factor graph model to tackl the correl between 
featur and emot labels, and propos a machin learn algorithm for compound emot detect base 
on the smartphon sens data. the propos mechan be implement a an app call moodexplor in 
android platform. extens experi conduct on the smartphon data collect from 30 univers student 
show that moodexplor can recogn users’ compound emot with 76.0% exact match on average. 

cc concepts: • human-cent comput → smartphones; ubiquit and mobil comput system 
and tools; 

addit key word and phrases: emot detection, compound emotion, smartphon sensing, factor graph 

acm refer format: 
xiao zhang, wenzhong li, Xu chen, and sanglu lu. 2017. moodexplorer: toward compound emot detect 
via smartphon sensing. proc. acm interact. mob. wearabl ubiquit technol. 1, 4, articl 176 (decemb 2017), 
30 pages. https://doi.org/10.1145/3161414 

1 introduct 

mood sens be receiv widespread attent from the social psychology, neuroscience, and comput 
scienc in the past year [14][30][52]. the mood or emot state play an import role in human daili 
lives, which have great influenc on people’ communication, perception, social behavior, and decis 

∗the correspond author be wenzhong li, email lwz@nju.edu.cn. 

authors’ addresses: xiao zhang, nanj university, state key laboratori for novel softwar technology, nanjing, jiangsu, 
210023, china; wenzhong li, nanj university, state key laboratori for novel softwar technology, nanjing, jiangsu, 
210023, china; Xu chen, sun yat-sen university, school of data and comput science, guangzhou, guangdong, china; 
sanglu lu, nanj university, state key laboratori for novel softwar technology, nanjing, jiangsu, 210023, china. 

permiss to make digit or hard copi of all or part of thi work for person or classroom use be grant without fee 

provid that copi be not make or distribut for profit or commerci advantag and that copi bear thi notic and 
the full citat on the first page. copyright for compon of thi work own by other than the author(s) must be 

honored. abstract with credit be permitted. To copi otherwise, or republish, to post on server or to redistribut to lists, 
requir prior specif permiss and/or a fee. request permiss from permissions@acm.org. 

© 2017 copyright held by the owner/author(s). public right licens to associ for comput machinery. 
2474-9567/2017/12-art176 $15.00 
https://doi.org/10.1145/3161414 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:2 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

mobil sensor 

emot state 

app usag 

wi-fi 

accelerometerlight 

microphon 

compass 

gp 

fig. 1. emot detect via smartphon sensing. 

making. automat mood detect be a challeng task, which envis a wide rang of new mood- 
awar applic scenarios. for instance, peopl enjoy differ style of music and movie, which not 
onli depend on their preference, but also relat to their mood and personality. therefor a smart 
recommend system should take into account the mood to enhanc user experience. anoth exampl 
be advertisement, which be found interest or annoy for differ person with differ emot states. 
So advertis can be more effect and person if peoples’ feel can be considered. with the 
preval of social network applications, share the mood among famili and close friend can help 
peopl to strengthen their bond and improv the way of social communication. furthermore, robot will 
be wide use in the near futur in differ aspect of our lives. the robot can be more intellig and 
human if they can “read the mood” of the human they work for. last but not least, understand 
the emot state and it evolut be import to evalu individual’ psycholog health and mental 
well-b [10][19]. 

mood detect have be report use bodi physiolog signal such a heart beat, blood pressure, 
breath rate, etc [17][26][53]. however, monitor such signal reli on expens dedic devices, which 
be infeas for pervas device-fre detection. some exist work sought to recogn emot by audio 
and video signal [49]. for example, ang et al. explor speech-bas recognit for the emot of 
annoy and frustrat [1]. ashraf et al. detect pain express by recognit of facial signal [3]. 
the moodmet propos the recognit of smile face via campu video camera [22]. however, visual 
featur recognit onli reflect people’ express in a snapshot. besides, collect and analyz audio 
and video data be a high-comput task, and such signal cannot be captur everywher without 
wide deploy of cameras. 

nowaday smartphon be wide use in people’ daili life for business, social and entertain 
purpos [4][30]. As show in fig. 1, there be mani sensor emb in modern smartphones: microphone, 
accelerometer, electron compass, gps, proximity, etc. the data captur from the sensor poss 
profound inform and can be exploit to infer user’ social behavior such a physic movement, 
social communications, location, etc. intuitively, the usag of smartphon and the context inform be 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:3 

correl to users’ emotions. for example, peopl may play game with cellphon when they be happy, 
or they may feel depress if they live in a noisi environment. motiv by this, sever work employ 
smartphon sens data for emot detection. bogomolov et al. propos a multifactori statist 
model to recogn daili stress by comprehens analyz mobil phone data and weather condit 
[6]. sun et al. employ sensor data, app usag information, and sm content for cold-start emot 
predict use transfer learn [43]. however, their approach be content-based, which requir users’ 
highli privacy-sensit inform such a sm content. likamwa et al. propos a system for mood 
detect util email, sms, location, and app usag durat a featur [30]. what need to be 
stress be that the literatur onli consid singl emot detection, which assum that peopl be in 
onli one emot state in a period. differ from the exist works, we weaken such assumpt and 
studi the co-exist of multipl emot call compound emotion. accord to plutchik’ theori 
[34], emot be not necessari in a pure state and could be the mixtur of basic emotions. the studi 
of [11][55] also show compound facial emot in human facial expressions. for instance, “happili 
surprised” be a compound emot express that combin basic emot of happi and surprise. 
differ from the compound facial emot that individu should experi at the same time, the 
compound emot studi in our paper refer to a set of basic emot that an individu experienc 
in a duration, which can occur simultan or alternately. In thi paper, we provid formal express 
of compound emot a a vector of multipl basic emot with discret levels, and propos a machin 
learn algorithm to detect compound emotion. To the best of our knowledge, thi be the first work of 
compound emot detect base on smartphon sensing, which have not be address in the past. 

specially, we propos a system call moodexplor to enabl compound emot detect via cellphone. 
We first develop an android app to allow peopl to report their emot state and collect the data of 
smartphon sensor and usag patterns. the app be instal and test by 30 univers student for 
a month, which form the dataset for model train and testing. It be observ in the dataset that about 
60% report emot be compound emot consist of more than two basic emotions, which verifi 
the motiv of our work. base on the dataset, we extract differ type of featur in respect of 
environment, contact, app usage, and human activities, which be use for automat emot detection. 
To best tune the performance, we present a featur select algorithm use the problem-transform 
approach and relieff measur to choos the most signific features. use the select featur a 
input, we adopt the factor graph model to repres the correl between featur and multipl 
emot labels, and propos a learn algorithm for compound emot detection. We conduct extens 
experi on the collect dataset, which demonstr that the exact match of compound emot 
achiev 76.0% on average. 
the main contribut of the paper be summar a follows. 

• We identifi the compound emot detect problem. unlik exist work that assum human 
emot be exclusive, we observ that peopl usual report their emot state a the combin 
of sever basic emotions. the compound emot be found highli correl with users’ smartphon 
usag pattern and sens data, therefor compound emot detect be possibl without the need 
of know people’ bodi physiolog signal or facial expressions. To the best of our knowledge, 
smartphone-bas compound emot detect have not be address in the literature. 

• We propos a novel compound emot detect method use smartphon sensing. specifically, 
we extract differ type of featur from the sens data to show the environment, social contact, 
app usag and activ of individuals, and appli featur select algorithm to find the most 
signific features. use the select featur a input, we propos a machin learn algorithm 
to deriv the probabl emot label by maxim a posteriori probability. the propos machin 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:4 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

learn algorithm us a factor graph to depict the correl between featur and emot label 
and the correl across differ basic emotions, which be show to be suitabl for compound 
emot detection. 

• We develop and implement the moodexplor system for automat emot detection. the propos 
system be implement a an android app, which be test by 30 univers students. base on 
the sens data collect by the smartphon and the emot state report by the participants, 
we train the machin learn model and test it performance. It be show that compound emot 
can be correctli detect by our system with 76.0% exact match on average. 

the rest of the paper be organ a follows. section 2 provid an introduct to the relat work 
about affect measur model and emot recognit mechanisms. section 3 propos the compound 
emot model and formal the compound emot detect problem. section 4 introduc the system 
design and data collect process. section 5 propos data process method includ featur extract 
and featur selection, and conduct correl analysi of the select features. section 6 develop an 
effici factor graph model for compound emot detection. section 7 report experiment results, and 
demonstr the perform of the propos factor graph method. section 8 conclud the work. 

2 relat work 

2.1 model for emot and mood measur 

emot and mood have be wide studi in psychology, sociology, and neurosci [35]. generally, 
“emotion” refer to the current instantan feeling, and “mood” refer to the averag feel over a 
longer period of time. A varieti of model have be propos to measur and quantifi emot and 
mood. such model can be appli to measur both instantan and long-term feelings, and they can 
be use for emot and mood measur without restrict distinction. 
one frequent use measur for gener affect state be the posit and neg affect schedul 

(panas) model [9]. particip complet the pana be ask to rate the extent to which they 
experienc each out of 20 emot on a 5-point likert scale rang from ”veri slightly” to ”veri 
much”. particip may be ask how they feel right now or dure longer period of time (e.g. dure 
the past month) accord to the purpos of the measur of emot or mood. 
the discret categori model [12][45] describ emot through a set of categories. one of the most 

popular model be ekman’ six basic categori [12]: happiness, sadness, anger, surprise, fear, disgust. the 
ekman’ model be intuit and understand to normal users, and it allow the co-exist of multipl 
emot with differ intens levels. with it high applicability, the ekman’ emot model be 
wide adopt by mani studi [36] [55]. 

the circumplex mood model [35] employ a two-dimension circumplex to repres the emot 
state of the participants: the pleasur dimens measur the degre of posit and neg feelings, 
and the activ dimens measur the likelihood for a user to take action under the mood state. 
each dimens be quantifi use a score, henc the circumplex mood model provid continu 
measur of the mood, and have be adopt in mani studi [30][43]. 

2.2 emot recognit 

emot recognit be a fine-grain sentiment analysis, which aim to identifi emot from variou 
inform resources, such a video, image, text and so on. In the recent years, a larg number of work 
have be do on thi area. In the work of [22], the research identifi smile face through the camera 
in the campus. the studi found that the users’ emot state have a cyclic pattern and be highli 
correl with extern events. In the tradit emot classification, the emot state be gener 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:5 

classifi into one discret category. some work adopt the continu probabl distribut to depict 
the emot state of an image, and propos to use the gaussian mixtur model [54] for emot 
recognition. As for emot analysi base on text [28], the author classifi sentence-level emot 
consid label context dependence, and formal the problem a a multi-label classif problem, 
which allow the detect of sever emot in a singl sentence. recently, the eq-radio propos 
the usag of wireless signal to monitor individual’ heartbeats, which be further use a featur for 
emot detect [53]. however, their work reli on dedic wifi devic and on-bodi sensor such a 
ecg monitors. 

2.3 detect of human mental well-b base on smartphon 

nowadays, with the rapid adopt of smartphones, research have show that it be possibl to adopt 
smartphon sens data to infer and detect human mental well-b such a stress, anxiety, depression, 
emotion, and mood [8][31][37][41][42][51]. 
deepmood [8] detect bipolar affect disord util type dynam and acceleromet sensor 

data in the smartphon base on multi-view neural network. herdem et al. [21] aim to help mobil 
individu to interact offlin with friend when they need emot support. bogomolov et al. propos 
a multifactori statist model to recogn daili stress by comprehens analyz mobil phone 
data and weather condit [6]. canzian et al. monitor the depress state of user by mean of 
smartphon mobil trace analysi [7]. mottelson et al. propos the detect of posit and neg 
affect use mobil commod sensor in the wild [32]. the iself system [43] employ sensor data, 
app usag information, and sm content for emot predict use transfer learning. moodscop 
[30] util email, sm contact information, websit visit information, location, and app usag 
durat to detect users’ mood. however, the exist work onli consid singl emot detection, 
which ignor the fact that multipl emot may co-exist in a period. In our paper, we make the first 
attempt to solv the compound emot detect problem in smartphon sens environment, which 
correspond to the deriv of a multi-dimension emot vector with discret level leverag a set 
of smartphone-gener sens information. 

3 compound emot model 

3.1 definit 

the phrase mood and emot be highli relat but have slight differ in sever aspect [5]. gener 
speaking, emot be instantan intens feel react to some events, while mood can be view 
a long-last intern emot state of an individu [16]. sinc the state-of-the-art smartphon can 
not captur everi human instantan activ and event so far, detect instantan emot be 
unlik and impracticable. therefor our studi be interest in the measur of short-term mood in 
a time interv of sever hours. unlik the convent definit of mood, we particularli focu on the 
detect of compound emotion, which be defin a follows. 

definit 3.1 (compound emotion). the compound emot be a set of emot that an individu 
experienc in a short duration. the durat be measur by a time interv typic within a few hours, 
dure which the multipl emot can occur simultan or successively. 

accord to the definition, compound emot be differ from the convent concept of emot 
sinc it allow the co-exist of multipl emotions. compound emot be also differ from the 
convent concept of mood sinc it less emphas on the long-last emot state but more focu 
on the measur of affect in a short duration. 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:6 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

���� ��� 

���� � 
������� 
����� � 
�� 
��� 
� � 

� ����� ��� 

���� 
� 
�� 
� ��� 
�� 

������� ��������� 

������� �� 
����� 

����� ��� 
� 
�� 

������ 
���� ���� 
���� ���������� �� 
� 
�� 
�� 

���� 

fig. 2. moodexplor system framework. 

the “compound emotion” be not a standard term in the literature, so we introduc our own definit 
in the paper. some previou work also show the co-exist of multipl emot in human facial 
express [11]. for example, the studi of [11] show 21 combin facial emot includ “happili 
surprised”, “angrili surprised”, “sadli feared”, “sadli disgusted”, etc, which be identifi from the 
human facial express accord to their images. To avoid confusion, we refer to their find a 
“compound facial emotions”. the compound facial emot describ multipl emot occur at the 
same time, while our definit of compound emot measur multipl emot in a duration, which 
can occur simultan or at differ times. 
the detect of compound emot use smartphon sens be base on an implicit assumpt 

that individuals’ feel be correl to their smartphon usage. such correl be rather intuitive. 
for example, one may feel happi when she shop online, or one may call a close friend when she feel 
sad. although the cause-effect of emotion-smartphon correl be hard to depict, we can exploit the 
correl to infer individual’ compound emot by explor the smartphon sens data. the 
correl of the compound emot and the smartphon sens data will be discuss in section 5. 

the widespread ekman’ discret categori model [12] consist of six basic emot categories: happiness, 
sadness, anger, surprise, fear, and disgust. In thi paper, we adopt the idea of the combin of ekman’ 
six basic emot categori to express compound emotions. specifically, we assign a discret score for 
each basic emot category, and repres the compound emot state of an individu by a 6-tupl 
vector 

Y =< shappy, ssad, sanger, ssurprise, sfear, sdisgust >, (1) 

where S∗ be the score of the correspond basic emot category, which be quantifi to 5 level in 
{1, 2, 3, 4, 5} repres null, slight, moderate, strong, and extrem respectively. 

3.2 compound emot detect problem 

We address the problem of compound emot detect base on the sens data collect from the 
smartphone. given the massiv sourc of sens inform from variou sensor includ microphone, 
accelerometer, electron compass, light sensor, etc., a well a the log app usag history, it be a 
challeng task to infer compound emot with the multimod data. 

To achiev effici compound emot detection, we first need to extract the most use inform 
know a featur from the raw data. given the obtain vector of featur denot by X, the task be to 
identifi the user’ compound emotion. particulary, we want to find a function to map the featur vector 
X to the compound emot present Y : 

f : (x) → Y, (2) 
where Y be a multi-dimension emot vector a defin in eq. (1). 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:7 

icollector 

audiocollector 

gpscollector 

lightcollector 

stepcollector 

usagecollector 

wificollector 

collectingservic 

notificationservic 

gatheralarmreceiv 

feedbackalarmreceiv 

mainact 

fig. 3. class structur of the moodexplor app. 

In the follow sections, we will propos solut framework and methodolog to solv the compound 
emot detect problem. 

4 system design and data collect 

4.1 system framework 

We propos the system framework a show in fig. 2 for compound emot detect via smartphon 
sensing. first, we develop an android app to collect smartphon data from the users. the app allow 
user to report their compound emot to the server periodically, which form the ground truth and 
be use to build train set and test set. then we extract featur from the collect data. sinc the 
number of the extract featur be large, we further appli a featur select method to choos the most 
signific featur to form the featur vector. use the featur vector and the label instanc a 
input, we train a compound emot detect model base on factor graph. finally, we use the test set 
with users’ report emot to verifi the perform of the propos system. 

the detail techniqu be introduc in the follow subsections. 

4.2 implement of moodexplor 

We implement the app on android platform for collect sens data and compound emot reports. 
fig. 4 show the screen shot of the moodexplor app. It enabl sever function such a report 
the compound emotions, search the histori of reports, and show the statist of the cellphon 
sens data and app usages. the app send three notif per day, which be at 10am, 3pm, 
and 8pm respectively, at least 5 hour apart from one another. user can report their emot state 
experienc in the past few hour either launch the notif or open the app directly. the 
app adopt the ekman’ six class basic emot model and ask user to evalu the intens level for 
each of the basic emot categori in the scale of 1 to 5. 
In detail, the class structur of the app be show in fig. 3. the data collect process icollector 

run in the background and it be invok by the system alarmmanag periodically. To save energy, 
the data collect interv be set to 5 minutes. each time the icollector be invoked, it will read the 
smartphon sensors. particularly, it read the gp locat of the smartphone; check the on/off state 
of the smartphon screen; scan the wifi signal nearbi to log the id of scan access point (aps) 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:8 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

fig. 4. screenshot of moodexplorer. 

0 

1000 

2000 

3000 

4000 

5000 

6000 

20 60 100 140 180 220 260 300 

E 
ne 

rg 
y( 

J) 

time(mins) 

new 
music 
weibo 

taobao 
nike+ 

m.e. 

fig. 5. energi consumpt of differ apps. 

and the receiv signal strength (rss); and then it read the microphone, the light sensor and the 
accelerometer, electron compass, gyroscop measur for 15 second and record the data in a local 
database. the users’ app usag inform and social activities, which includ opening/clos an app, 
making/answ a phone call, sending/receiv a sms, be also log a event with timestamp 
in the system. the record data be store in the smartphone’ file system and submit to a remot 
cloud server daili for analysis. To save the commun cost, the data be upload onli when there be 
avail wifi connection. 

the app do not record privacy-sensit inform such a the content of the sm and the voic 
dure a call. all cellphon number and user name be anonym by map them to random ids. 
the ssid of the ap be also map to random id to conceal their real name. the gp coordin of 
the user be collect under the users’ permiss (the app will ask for the author to use the gp 
data when the app be installed). If a user concern about her locat privacy, she can simpli turn-off 
the gp or unauthoris gp to moodexplor to avoid be track by our app. for the app usage, 
we onli record the categori (e.g., shopping, entertainment, social networks, etc) of the app without 
record their actual names. for the sensor data such a microphon sound and light, we comput their 
mean, variance, and other measur in a durat a introduc in the featur extraction, and submit 
the featur measur to the server without keep the origin data. 

4.3 energi consumpt of moodexplor 

energi consumpt be one import issu for smartphone. To evalu the energi consumption, we 
compar the energi consumpt of moodexplor with sever other wide use applications: a news app 
(today’ tops), a music app (kugou music), a social network app (weibo), a shop app (taobao), 
and a fit app (nike+). We run the app for 5 hour and compar their energi consumpt accord 
to the batteri logs. the result be show in fig. 5. As show in the figure, the energi consumpt of 
moodexplor be not severe, which be veri close to the shop app (taobao) and the social network 
app (weibo). It consum onli half energi of the music app (kugou music). the news app (today’ 
tops) and fit app (nike+) consum much more energi than moodexplor sinc they may acquir 
sensor data and network commun frequently. the experi show that sampl sensor data 
everi 5 minut in moodexplor do not affect the usag of smartphon and the energi consumpt 
be in an accept level. 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:9 

������ ���� 
� 

!"�""� #� 
� 

(a) gender. 

���������� 
�� 
����������� 
�� 
����������� 
� 
����������� 
�� 
����������� 
� 
����������� 
�� 

(b) age. 

�"�""� ����� 
�� 
""�""� #����� 
$"�""� ����� 

(c) education. 

fig. 6. demograph of the 30 participants. 

4.4 data collect 

To test the moodexplor app, we recruit 42 student volunt to particip in the experi of 
data collection. the student be ask to instal the moodexplor app in their smartphones, and to 
util the app to report their emot three time per day around 10am, 3pm, and 8pm. We do 
not restrict the time to report their emotions, but we requir that the interv of consecut report 
should be longer than 5 hours. the student volunt be studi comput science, engineering, and 
busi in our university, who have adequ comput skill and util smartphon actively. they 
be told about the research purpos of the experi and be awar of that their data will be use 
for emot study. To encourag the student to involv in the experiments, we sent them a thank-you 
gift (e.g., a usb flash drive, a mobil microsd memori card, a t-shirt, etc) after they submit enough 
number of emot reports. the data collect last for about one month, from june. 15 to july. 14, 
2016. accord to the return results, we have 30 student that submit emot report more than 
50 times, which be use in our analysis. 
among the 30 students, there be about 57% femal and 43% males. their age be from 18 to 30, 

and be mostli concentr in the rang of 21-24 and 27-30. there be about 53% undergraduates, 33% 
master students, and 14% phd students. the demograph of the particip be show in fig. 6. 

5 featur extract and select 

5.1 data observ 

after data collection, we need to preprocess the dataset. We mainli remov the unqualifi submiss 
which caus by miss sens data or compound emot labels. If a user fail to submit the report 
in an interval, then the correspond data will not be use for the model due to the lack of label. If 
part of the sens data be miss in an instance, for example, a user may disabl the gp in some 
time intervals, then the correspond featur will be set to “null”. the null featur will not affect the 
model and detect accuraci much sinc they be unlik to be chosen to build the model after featur 
selection. 
We make some observ to the collect dataset after data preprocessing. We first observ the 

distribut of number of qualifi submissions, which be show in fig. 7. As show in the figure, there 
be about 57% (17) user submit qualifi compound emot report more than 80 times, and 2 
user continu to submit report after one month. most of the particip submit more than 60 
qualifi reports, which be use to form the train set and test set for our model. 

then we studi the number of emot label report by the individuals, which distribut be show 
in fig. 8. As show in the figure, about 41% report have singl kind of emotion; about 40% report 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:10 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

contain two kind of emotions, and about 13% contain 3 kind of emotions. In anoth words, there 
be near 60% report instanc that be compound (multi-dimensional) emotions. thi illustr that 
compound emot be veri common in daili life, which verifi the key motiv of our study. 

remov the “null” emotions, we also show the distribut of differ level of the report compound 
emot in fig. 10. As show in the figure, happi be the most common emot that contain in the 
user reports, where about 14% be moderate, about 11% be slight, about 7% be strong and about 4% 
be extreme. sad be the second common emotion, which yield about 10.5% be slight and about 5% 
be moderate. other emot such a anger, surprise, fear and disgust be less common, and most of 
them be less than 10% in the total user reports. 
We further investig the combin of basic emotions. If two basic emot co-exist in the same 

report, we consid them correlated. We calcul the fraction of correl emot in the users’ reports, 
which be illustr in fig. 9, where thicker edg correspond to high frequenc of co-existence. As 
show in the figure, sever emot pairs, such a sadness&disgust, sadness&fear, happiness&surprise, 
frequent appear together. thi suggest that some combin of basic emot be veri common in 
the experiments. obviously, not all combin be meaning for humans. some emot pair such a 
happyness&anger, happiness&disgust, anger&surprise, rare appear togeth in our observations. 

In summary, we observ that compound emot be veri common in the user reports, and some of 
the emot pair be highli correl in the collect dataset. such inform will be exploit to 
design a machin learn algorithm for emot detection. 

5.2 featur extract 

with the data collect from the mobil users, we need to process them into featur presentation. 
specifically, the raw data be process to differ type of featur includ the situat of environment, 
contact, app usage, and activity, which be discuss below. 

5.2.1 environment. environment situat be know to influenc the feel of users. the environ- 
ment featur can be repres by the microphon and the light sensor inform from the environment. 
the microphon log the sound “heard” by the smartphone, and we take the mean and varianc to indi- 
cate the volum and dispalcements. accord to [40], a sound be consid to be nois when the volum 

exce some threshold: 85-90 dba. We further defin the nois ratio NR = number of nois samplestot number of audio sampl , 

silenc ratio SR = 1−nr and noise-sil ratio nsr = nrsr to repres the nois condit of the 
environment. 
similarly, we take the mean and varianc of light sensor a features, and use the dark ratio (dr), 

bright ratio (br), dark-bright ratio (dbr) to repres the illumin and to infer the indoor/outdoor 
durat of an individual. 

In term of locat information, gp can be use to infer the locat of a user outdoor. however, it 
cannot be use for indoor environment. On the other hand, wifi have be use a a mean of indoor 
posit [13], which be also an import locat information. In our system, we record the ssid of 
wifi access point scan by the smartphon everi 5 minutes, and count the frequenc each ssid 
appear in the wifi log. then we choos the frequenc of the top N occur ssid of each user a the 
indoor locat feature, which approxim indic the user’ often visit indoor locations. 

5.2.2 contact. accord to the studi of [2], peoples’ emot state be influenc by their friend or 
the social contact persons. contact featur repres the users’ social connect and behaviors. from 
the call/sm logs, we extract the call duration, and call/sm frequenc to repres the contact features. 
call durat measur how much time the user spent on commun with a specif contact through 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:11 

0 

5 

10 

15 

20 

50 60 70 80 90 100 

N 
um 

be 
r 

of 
u 

se 
r 

number of qualifi mood submiss 

fig. 7. distribut of the number of 
qualifi submiss 

0 

0.05 

0.1 

0.15 

0.2 

0.25 

0.3 

0.35 

0.4 

0.45 

0 1 2 3 4 5 

Fr 
ac 

tio 
n 

number of emot label 

fig. 8. distribut of the number of 
emot labels. 

����� 

��� 

���� 

�� 
� 
�� 

��� 

� 
����� 

fig. 9. correl of basic emot 
categories. 

0 

0.02 

0.04 

0.06 

0.08 

0.1 

0.12 

0.14 

0.16 

happi sad anger surpris fear disgust 

Fr 
ac 

tio 
n 

emot label 

slight 
moder 

strong 
extrem 

fig. 10. distribut of emot levels. 

0 

5 

10 

15 

20 

25 

30 

acc. gy. audio light screen app wifi contact 

N 
um 

be 
r 

of 
a 

pp 
ea 

ra 
nc 

e 

featur 

fig. 11. freq. of the categori of the 
select features. 

0 

5 

10 

15 

20 

0.5 0.6 0.7 0.8 0.9 1 

N 
um 

be 
r 

of 
U 

se 
r 

averag jaccard similar 

fig. 12. distribut of jaccard simi- 
lariti 

tabl 1. descript of the extract features. the number in bracket indic the number of the extract featur 
for that category. the star mean that the number of extract featur be not fix and it vari from person to person. 

type data sourc categori extract featur descript 

environ 

microphon audio (5) mean, variance, nr, sr, nsr 
light sensor (5) mean, variance, dr, br, dbr 

gp (3) gp longitude, altitude, latitud 
wifi (20) frequenc of ssid of the top 20 ap for an user 

contact 
phone call (*) call frequenc and durat of each contact person 

sm (*) sm frequenc of each contact person 
app usag app log (18) durat of 18 app categori 

activ 

acceleromet (7) mean and varianc of three axi (x, Y, z), step count 
compass (6) mean and varianc of three axi (x, Y, Z) 
gyroscop (6) mean and varianc of three axi (x, Y, Z) 
screen (4) screen on ratio, off ratio, sleep duration, usag amount 

phonecal in a time interval. similarly, call/sm frequenc measur how mani time the user have call 
or sent messag to a contact dure the interval. 

5.2.3 app usage. intuitively, peopl tend to use differ kind of app under differ emo- 
tions/moods. due to the larg amount of apps, we do not use a concret app a a feature. instead, we 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:12 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

algorithm 1: featur select algorithm 

input: multi-label featur matrix M 
output: select featur set S 
1: transform M to k singl label featur matrix 
2: use relieff to measur import of featur in everi singl label dataset 
3: output a weight vector W for all the featur 
4: sum up W of all the k singl label dataset 
5: sort the featur accord to the sum up weight 
6: choos the top N feauter a the select featur set S 

classifi the app into 18 categories. such categori can be directli obtain from the android market 
such a googl play1. the 18 categori are: mother and child, traffic navigation, image, offic efficiency, 
education, news, travel, life tools, life services, telephon communication, system tools, smartphon 
beautification, social chat, vedio, shopping, health, financi management, and music. for each app 
launch by the user, we use a tupl < category, durat > to denot the app usage. 

5.2.4 activity. activ such a go for sport or lie on the bed may impli peoples’ differ 
emotions/moods. the users’ activ can somehow be deriv by the accelerometer, electron compass, 
and gyroscop sensor that record the human movement condit when carri a smartphone. the 
acceleromet record user movement in three dimensions: X (the direct of front and back), Y (the 
direct of left and right), and Z (the direct of up and down) . the electron compass record the 
orient in the form of an angl with respect to magnet north, and the gyroscop sensor record the 
posit of the smartphone. all of them can reflect the movement state of the mobil users. 
sinc the acceleromet data have clear period pattern when the user be walk and running, such 

properti have be use by mani work to count the step of a user [29]. In our paper, we also take step 
count a a featur of user activities. 
explor the on/off pattern of cellphon screen lead to sever interest features, such a the 

sleep duration, which can be estim by the long off-screen interval, and the phone usag amount, 
defin by the proport of screen on-to-off duration, which indic the time that a user spent on 
play with the smartphone. 

the overal featur extract from the dataset be summar in tabl 1. the integ in the bracket 
indic the number of the extract featur from the type of data. note that we do not put a specif 
number on the phone call and sm data sinc the number of contact of differ user be variant. 

5.3 featur select 

after featur extraction, we get 100 more featur from the raw data. however, not all featur play an 
equal role in compound emot detection. some featur may show high correl tp the labels; some 
may be less relevant; some of them may be redundant; and some may be noisy. therefor we appli the 
featur select techniqu to reduc the number of featur to improv the effici of the machin 
learn model. 

featur select be an import data pre-process step in machin learning. It aim to find a subset 
of featur X∗ ⊆ X to describ the dataset a well a X does, where X be the origin featur space. when 
learn from the high-dimension data, featur select provid a good way to reduc the dimensions. 

1play.google.com 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:13 

tabl 2. top 6 select featur of 4 randomli users. 

user first sel. fea. sec. sel. fea. third sel. fea. fourth sel. fea. fifth sel. fea. sixth sel. fea. 

1 
bright dark mean of light varianc of freq. of call durat of 

ratio (br) ratio (dr) sensor valu audio volum ssid #1873 contact #1436 

2 
step count mean of mean of varianc of durat of call freq. of 

gyro. Z axi gyro. X axi gyro. Y axi shop app contact #1037 

3 
screen On screen off call freq. of call durat of freq. of freq. of one 

ratio ratio contact #1686 contact #1686 ssid #1074 ssid #2244 

4 
freq. of freq. of freq. of durat of social durat of durat of 

ssid #3541 ssid #5541 ssid #1712 chat app video app music app 

sinc the report compound emot consist multipl basic emotions, we take each basic emot a 
a label, and the compound emot dataset can be view a a multi-label dataset. We adopt the problem 
transform approach [38] for featur selection, which work a follows. firstly, we use the binari 
relev (br) [46] method to transform the multi-label dataset to k singl label datasets, where k be 
the number of label (which equal 6 in our system). secondly, we exploit relieff (rf) [39] a featur 
evalu measur for each singl label dataset. relieff output a weight w for each featur to repres 
it significance. the larg weight w is, the more import the featur is. thirdly, for each featur we 
sum up it RF weight in all singl label datasets, which repres it overal importance. finally, we 
sort the featur accord to their total RF weights, and select the top N featur to be use by the 
learn model. the detail be show in algorithm 1. 

In appendix A, we show the RF weight of all the extract featur of a user. It show that there be 
110 featur extract from the algorithm. the weight of differ featur be quit different. onli veri 
few featur have weight larg than 1, while a larg number of featur have small weights. It suggest 
that a few number of featur could be suffici to build a learn model. In thi paper, the default 
number of featur use for the learn model be 6, and the influenc of the number of select featur 
on the system perform be discuss in section 7.4. 
tabl 2 show the chosen featur for 4 random users. It be interest to see that differ user 

have differ type of featur to indic their emot states. three of the users’ emot state 
be correl to their contacts’ calls, which impli that social connect be influenti to human 
emotions. some of them be correl to differ app they use for shopping, entertainment, and 
social communications. user 1’ emot state be sensit to the ambient light and sounds. the emot 
state of user 2 be more relat to hi motion such a step count and accelerometer. user 3’ emot 
state seem to be more correl with smartphon usag durat and location. while the emot tate 
of user 4 be heavili relat to app usag and indoor locations. base on the observations, differ user 
have differ set of featur that correl to their compound emotions. thi suggest that person 
model should be built individu to infer the users’ compound emotions. the select featur of all the 
30 particip be list in appendix B. 

fig. 11 show the frequenc of the categori of the select featur in the collect dataset. As show 
in the result, the contact inform (the social connection), the wifi inform (the indoor locat 
information) and the app usag be the top three most signific data sourc relat to users’ emot 
labels. thi indic that who the user contacts, where the user have been, and which app the user 
util daili have great influenc on the user’ emot states. other import featur includ the 
light sensor (the ambient brightness), the acceleromet (individuals’ physic movement status), and the 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:14 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

0 

2 

4 

6 

8 

10 

12 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

absolut correl 

(a) absolut correl of the 

first select feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

p-valu 

(b) p-valu of the first select 

feature. 

0 

2 

4 

6 

8 

10 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

absolut correl 

(c) absolut correl of the 

second select feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

p-valu 

(d) p-valu of the second se- 

lect feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

absolut valu of correl 

(e) absolut correl of the 
third select feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

p valu 

(f) p-valu of the third se- 
lect feature. 

0 

2 

4 

6 

8 

10 

12 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

absolut correl 

(g) absolut correl of the 
fourth select feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

p-valu 

(h) p-valu of the fourth se- 
lect feature. 

0 

2 

4 

6 

8 

10 

12 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

absolut correl 

(i) absolut correl of the 
fifth select feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

p-valu 

(j) p-valu of the fifth select 
feature. 

0 

2 

4 

6 

8 

10 

12 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

absolut correl 

(k) absolut correl of the 
sixth select feature. 

0 

5 

10 

15 

20 

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

N 
um 

be 
r 

of 
u 

se 
r 

p-valu 

(l) p-valu of the sixth select 
feature. 

fig. 13. histogram of the correl and the p-valu for top 6 select featur (the number of the select featur 
be 6 in default, the influenc of the number of select featur be discuss in section 7.4). 

audio (the environment noise), which also reflect differ aspect of human activ and behavior 
that have an effect on users’ compound emotions. 

To test whether the select featur chang over time, we make observ on the individuals’ featur 
set select from differ time periods. specifically, for each user, we appli the featur select to 
select the top-six featur in the first 20 day to form a featur set f1, and then we use the same method 
to select a featur set F2 in 30 days’ long. We adopt the jaccard coeffici [27] to measur the similar 
of the two featur set F1 and f2, which be defin a j(f1,f2) = |f1∩f2||f1∪f2| . the high j(f1,f2) valu 
mean the more similar between F1 and F2 and the less vari of featur sets. We show the distribut 
of jaccard similar of the 30 user in fig. 12. As show in the figure, half of the user have jaccard 
similar larg than 0.9, which mean their featur set be almost unchang in a month. about 6 
users’ jaccard similar be between 0.8 and 0.9, and veri few users’ jaccard similar be less than 
0.7, which impli that the featur set of a few user chang slowli over time. In summary, all the users’ 
jaccard similar be larg than 0.5, which mean the chosen top-six featur be stabl and they will 
not chang over time dramatically. It impli that our learn model can be appli for a rel long 
time and it do not need to be updat frequently. 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:15 

5.4 correl analysi 

after select featur base on alg. 1, we conduct correl analysi between the select featur and 
the emot label for each user. We comput the pearson correl coeffici [47] use the similar 
method mention in [7][18]. We also comput the p-valu associ to each correl value. the 
p-valu come from the hypothesi test, in which the null hypothesi be that the correl valu equal 
0. the p-valu repres the probabl that the null hypothesi be true. A small p-valu (such a 0.1) 
indic that the null hypothesi can be reject and the correl valu be significant. 

sinc the emot label be multipl in our scenario, we calcul the correl valu between each 
select featur and each emot label respectively. A select featur can be consid signific 
if it have signific correl with at least one emot label (i.e., p < 0.1). the histogram of the 
correl coeffici and the p-valu for the users’ top six select featur be show in fig. 13. As 
show in the result, the distribut of the absolut correl valu be uneven, which be dynam 
variant between 0 to 0.6. however, one thing in common be that the p-valu of the select featur be 
mostli concentr on p < 0.1. for example, for the first select featur show in fig. 13(b), about 
57% users’ p-valu be less than 0.1. similarly, for the second to the sixth select feature, there be 
major of user with p-valu in the rang (0, 0.1). sinc p < 0.1 mean signific correl accord 
to the theori of signific test, it confirm that the select top six featur be significantli correl 
to the users’ emot states, which be reason to be use for compound emot detection. 

6 factor graph model for compound emot detect 

6.1 intuit 

accord to the observ in section 5.1, compound emot be the combin of the six basic 
emot categories, and there exist high correl among some pair of basic emot such a happi- 
ness&surprise. A machin learn algorithm should take into account such correl for compound 
emot detection. In thi paper, we adopt the factor graph model [24] to describ the correl between 
basic emot and the correl between featur and emot labels. base on the propos factor 
graph model, we can formul the condit probabl of the compound emot vector give the 
observ featur vector, and appli the maximum a posteriori (map) [33] principl to deriv the most 
probabl emot label for compound emotion. the detail be introduc in the follow subsections. 

6.2 model descript 

In the factor graph model, each node in the graph repres a variable, and the edg repres the 
correl between variables, which be call factor function. fig. 14 show the factor graph use for 
compound emot detection, which describ varieti of correl among featur and emot labels. 
specially, a vector Xt be use to denot the input featur vector in the t-th time interv of the user’ 
data trace, which contain N featur attribut after featur selection. and a vector Y t = (yt1, ..., y 

t 
K) 

be use to denot the compound emot vector at the t-th time interval, where yti be the ith emot 
label and K be the number of basic emot catalogu (k=6 for the ekman’ model). To describ the 
correl between featur and emot labels, similar to the tradit multi-label learn [28], we 
adopt the problem transform approach [38] to replic the featur vector Xt by k copies, denot 
a {xtk}kk=1, such that we associ each copi xtk with each emot label ytk. with such expansion, the 
factor graph model of our problem be show in fig. 14, where each node in the upper layer correspond 
to an emot label; and each node in the low layer correspond to a copi of the featur vector. 

the goal of the propos factor graph model be to describ the condit probabl of users’ emot 
state give the featur of the smartphon usag trace. particularly, the factor graph factor the 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:16 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

$ $% & ' 
� � 

� � � 

� 

�� 

� 

�� 
$ 

�� 
( 

�� 

� 

�� 

! 

�� 
" 

��$�� ( 
�� 

� 

�� 
! 

�� 
" 

�� 

! � 
% & ' 

� �� � � 
( "% & ' 
� �� � � " ! 

% & ' 
� �� � � 

" "% & ' 
� � 

� � � ! !% & ' 
� � 

� � � 

" � 
% & ' 

� �� � � 
$ (% & ' 
� �� � � 

fig. 14. the factor graph model. 

global probabl distribut a a product of local factor functions, each of which depend on a subset 
of the variabl (nodes) [24][48]. therefore, base on the observ in section 5.1, we introduc two 
kind of factor function (correspond to the edg in the factor graph) to account for the correl 
between featur and emot label a well a the correl across differ emot labels: 

• feature-label factor function: f(xtj , ytj) repres the correl between the featur vector 
xtj and the emot label y 

t 
j . the red rectangl in fig. 14 repres the feature-label factor function. 

• label-label factor function: w(yti , ytj) repres the correl between the emot label yti 
and the emot label ytj . the yellow rectangl in fig. 14 repres the label-label factor function. 

the factor function can be defin in mani way to reflect the correl factors. In thi paper, we 
follow the fundament hammersley-clifford theorem [20], which model the correl factor by the 
exponential-linear function in a markov random field. We elabor the factor function in detail a 
follows. 
1) the feature-label correl factor function. follow the hammersley-clifford theorem [20], we 

defin the feature-label factor correl function a an exponential-linear function: 

f(xtk, y 
t 
k) = 

1 

Z1 
exp{ 

N∑ 

n=1 

αnkφ(x 
t 
nk, y 

t 
k)}. (3) 

the notat of the equat be explain a follows. the notat xtnk (1 ≤ n ≤ N) indic the 
n-th featur of the featur vector xtk. the function φ(x 

t 
nk, y 

t 
k) be a binari indicator. for example, 

φ(xtnk = “true”, y 
t 
k = “strong”) mean that if the n-th attribut in the featur vector (repres by 

xtnk) be “true” and the level of the user’ k-th compon in the emot vector (repres by y 
t 
k) be 

”strong”, then the indic function valu be 1, otherwis 0. the weight αnk describ how strong the 
correl between xtnk and y 

t 
k is, which be the model paramet to be learn by the machin learn 

algorithm. Z1 be a normal term to ensur the sum of the probabl equal to 1 [44]. 
2) the label-label correl factor function. sinc there be multipl emot labels, if we take into 

account all the possibl correl among ani two labels, it will result in the explos growth of the 
learn parameters, hinder the perform of our model. moreover, account all emot label pair 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:17 

algorithm 2: construct of label correl tree 

input: label matirx M , number of label K 
output: label correl tree G = (v,e) 
1: count the time tij that label pair < li, lj > coexist in each row of M , 

T = {< li, lj >, tij}ki,j=1&i �=j 
2: sort T accord to the time tij 
3: for each label pair < li, lj > in T do 
4: if |e| < K − 1 then 
5: if li ∈ V & lj ∈ V then 
6: if |e| = |V | − 2 then 
7: V = V ∪ {li, lj}; 
8: E = E ∪ {(li, lj)}; 
9: els 

10: continue; 
11: end if 
12: els 
13: V = V ∪ {li, lj}; 
14: E = E ∪ {(li, lj)}; 
15: end if 
16: els 
17: break; 
18: end if 
19: end for 

may caus a cycl structur in the factor graph, which make exact infer difficult [44]. therefore, 
we design an algorithm to elicit the strong correl label pair and meanwhil avoid the cyclic structur 
in the factor graph. 

the propos algorithm work a follows. first, we sort the time that two emot label coexist in the 
same interv in a descend order. then, we select the emot label pair one by one in accord 
with the order. each label pair correspond to an edg in the factor graph. If an edg do not caus 
cyclic structure, it will be add into the factor graph; otherwis it will be discarded. the process be 
repeat until a tree structur call label correl tree be formed. An exampl of the construct 
label correl tree be show in the upper layer of the factor graph in fig. 14. the detail algorithm be 
illustr in algorithm 2. 

with the obtain label correl tree, and follow the hammersley-clifford theorem [20], we defin 
the label-label correl factor function w(yti , y 

t 
j) for each edg in the tree as: 

w(yti , y 
t 
j) = 

1 

Z2 
exp{βijφ(yti , ytj)}, (4) 

where yti and y 
t 
j be emot labels; φ(i 

t 
i , y 

t 
j) be a binari indic to show whether two emot label 

be correl in the factor graph (1 mean correl and 0 otherwise); the weight βij quantifi the 
influenc degre between two differ emot labels, which be the model paramet to be learn by the 
machin learn algorithm; and Z2 be a normal term to ensur the sum of the probabl equal 
to 1. 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:18 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

algorithm 3: learn algorithm 

input: learn rate η 
output: model paramet θ 
1: initi θ ← 0 
2: repeat 
3: calcul ed[φ(x 

t 
nk, y 

t 
k)] and ed[φ(i 

t 
i , y 

t 
j)] use the train dataset 

4: calcul eθ[φ(x 
t 
nk, y 

t 
k)] and eθ[φ(i 

t 
i , y 

t 
j)] use BP algorithm 

5: calcul the gradient ∂l(θ) 
∂θ 

accord to eqs.(7) and (8) 

6: updat paramet θ a θ = θ + η ∂l(θ) 
∂θ 

7: until converg 

6.3 object function 

assum there be T total time interv in the experiment. let X = (x1, ..., XT ) be the sequenc 
of observ featur vector over T time intervals, and Y = (Y 1, ..., Y T ) be the sequenc of the users’ 
compound emot vector accordingly. 

follow the common principl of factor graph [23][25][44], give the observationsx and the correl 
factor functions, the condit probabl of Y can be describ by the product of all factor function 
in the factor graph, which be express a 

P (Y |x, θ) = 
T∏ 

t=1 

K∏ 

k=1 

∏ 

∀ytj∈δ(ytk) 
f(ytk, X 

t 
k)w(i 

t 
k, y 

t 
j), (5) 

where δ(ytk) be the set of emot label node have an edg with y 
t 
k in the factor graph; and θ = 

({αnk}, {βij}) be model paramet to be learned. 
the condit probabl give in eq. 5 form the object function to be maximized, which be 

discuss in the next subsection. 

6.4 model learn 

given eq. 5, we would like to determin the optim system paramet θ = ({αnk}, {βij}) to maxim 
the object function (which correspond to find an optim map from the featur vector to the 
compound emot vector with maximum probability). To achiev thi task, the maxim problem 
can be transform to minim the follow neg log-likelihood function: 

l(θ) = − logp (Y |x, θ) + λ 
2 
( 

N∑ 

n=1 

K∑ 

k=1 

αnk 
2 + 

K∑ 

i=1 

K∑ 

j=1 

βij 
2), (6) 

where the last term be a l2-regular penalty, which be commonli use in data mine to prevent 
overfit [23]; and λ be the weight of penalti factor. 

We appli the gradient decent method [23] to learn the paramet θ. We obtain the gradient for our 
factor graph model a follows: 

∂l(θ) 

∂αnk 
= eθ[φ(x 

t 
nk, y 

t 
k)]− ed[φ(xtnk, ytk)] + λαnk, (7) 

and 
∂l(θ) 

∂βij 
= eθ[φ(i 

t 
i , y 

t 
j)]− ed[φ(yti , ytj)] + λβij . (8) 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:19 

where eθ[·] be the expect of featur valu with respect to the model paramet θ, and ed[·] be the 
averag valu by count the give pattern over the give train dataset. the learn algorithm be 
summar in algorithm 3, wherein we appli the belief propag (bp) algorithm [15] to infer the 
expect valu eθ[·]. 
given the learn paramet θ, we can infer user’ compound emot base on the principl of 

maximum a posteriori (map), i.e., find the compound emot vector that maxim the likelihood 
give the learn paramet θ a 

argmax 
y 

P (Y = y|x, θ). (9) 

similarly, we use the belief propag (bp) algorithm to calcul the margin probabilities. hence, 
the emot vector with the high probabl will be obtain a the output of compound emot 
detection. 

7 perform evalu 

In thi section, we conduct experi on the collect dataset to analyz the perform of the 
propos factor graph base compound emot detect method. 

7.1 experi setup 

the experi be base on the smartphon data collect from 30 students. sinc differ user may 
have differ set of features, we run the featur select algorithm and construct the factor graph 
model for each user individually. By default, we use 70% data a train set to build the model, and use 
the remain 30% data a the test set for perform evaluation. 
We compar the perform of the propos algorithm with three baselin classif algorithms: 

decis tree (dt), support vector machin (svm), and logist regress (lr). We adopt the problem 
transform approach that transform the compound emot detect problem into the detect of 
each dimens of basic emot independently. To do that, we transform the obtain multi-label dataset 
to k single-label datasets, appli the classif algorithm on each single-label dataset separately, and 
combin the result to form the compound emotions. We do not util the convent multi-label 
classifi such a ml-knn and ml-dt due to the fact that form a larg label space for multi-label 
classif will lead to the short of input instanc for model training. 

7.2 perform metric 

let m be the size of the test set, and k be the number of basic emot catalogu (k=6 in the ekman’ 
model). denot by R = {r1, ..., rm} be the set of emot report in the test set (ground truth), 
where Ri = (ri1, ..., rik) (1 ≤ i ≤ m) be the emot vector of the i-th instanc in the set. denot by 
S = {s1, ..., sm} be the set of emot vector deriv from our compound emot detect algorithm, 
where Si = (si1, ..., sik) (1 ≤ i ≤ m) be the i-th infer emot vector. We use the follow metric for 
perform evalu in the paper. 

• exact match: It evalu the percentag of emot label that be correctli detect by the 
algorithm, which be calcul by 

exactmatch = 
1 

mk 

m∑ 

i=1 

k∑ 

j=1 

i(rij , sij), (10) 

where i(·) be an indic function, i(rij , sij) = 1 when rij = sij , and 0 otherwise. 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:20 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

tabl 3. mean valu of differ metrics. 

mean valu DT svm LR factor graph 

exact match(%) 71.9 71.4 72.3 76.0 
mae 0.380 0.395 0.380 0.322 

accuracy(%) 54.4 55.9 54.7 62.9 
precision(%) 68.0 65.5 67.9 80.2 
recall(%) 60.0 64.2 60.2 66.2 
f1-score(%) 64.1 65.8 64.1 72.5 

• mean absolut error (mae): mae be defin a the averag differ between the infer 
emot vector and the ground truth, which be give by: 

mae = 
1 

m 

m∑ 

i=1 

m(ri, si), (11) 

where m(ri, si) = 
∑k 

j=1 |rij − sij | be the absolut error between two vectors. 
like tradit multi-label learn classification, we also adopt four wide use perform metric 

[50] includ accuracy, precision, recal and f1-score. 

• accuracy: It evalu the proport of correctli infer emot label to the total number of 
label for each instance. the overal correct be the averag on all the instances, which be give by 

accuraci = 
1 

m 

m∑ 

i=1 

|si ∩ri| 
|si ∪ri| , (12) 

where Si ∩ri be the set of non-nul common emot label of Si and ri, while Si ∪ri be the set of 
non-nul distinct emot label respectively. 

• precision: It be the the proport of the correctli infer emot label to the total number of 
infer emot label for each instance. the overal precis be the averag on all the instances: 

precis = 
1 

m 

m∑ 

i=1 

|si ∩ri| 
|si| . (13) 

• recall: It be the the proport of the correctli infer emot label to the number of actual 
emot label for one instance. the overal recal be the averag on all the instances: 

recal = 
1 

m 

m∑ 

i=1 

|si ∩ri| 
|ri| . (14) 

• f1-score: It be a weight harmon mean between the precis and the recall: 
F1 = 

2 ∗ recal ∗ Pr ecis 
recal + Pr ecis 

. (15) 

7.3 numer result 

We run the factor graph base compound emot detect algorithm and the baselin algorithm for 
each user in the dataset, and comput their perform metrics. the mean valu of the perform 
metric be compar in tabl 3. It be show that the propos algorithm outperform the baselin 
algorithm in all perform metrics. for instance, the propos compound emot detect method 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:21 

0 

0.2 

0.4 

0.6 

0.8 

1 

0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 

C 
D 

F 

exact match 

FG 
DT 

svm 
LR 

(a) cdf of exact match. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

C 
D 

F 

mae 

FG 
DT 

svm 
LR 

(b) cdf of mae. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0 0.2 0.4 0.6 0.8 1 

C 
D 

F 

accuraci 

FG 
DT 

svm 
LR 

(c) cdf of accuracy. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0.2 0.4 0.6 0.8 1 

C 
D 

F 

precis 

FG 
DT 

svm 
LR 

(d) cdf of precision. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 

C 
D 

F 

recal 

FG 
DT 

svm 
LR 

(e) cdf of recall. 

0 

0.2 

0.4 

0.6 

0.8 

1 

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 

C 
D 

F 

f1-score 

FG 
DT 

svm 
LR 

(f) cdf of f1-score. 

fig. 15. cdf of metric of differ methods. 

achiev 76.0% exact match on average, which have +4.1%, +4.6% and +3.7% improv compar 
with dt, svm and LR respectively. the propos algorithm have mae 0.322, which be much low than 
that of the other algorithm (abov 0.380). the accuracy, precision, recall, and f1-score of the propos 
algorithm all show signific improv compar with the baselines. the reason lie in that the 
propos factor graph model take into account the correl between emot labels, while the baselin 
algorithm predict the emot label individu without consid their correlations. 

the cdf (cumul distribut function) of exact match be compar in fig. 15(a). As show in 
the figure, for the factor graph algorithm, over 80% user achiev exact match larg than 70%; but for 
dt, svm, and LR models, onli about 56% user have exact match larg than 70%. 
fig. 15(b) compar the cdf of mae for differ approach in compound emot detection. As 

show in the result, for the factor graph algorithm, more than 80% user have mae small than 0.4, 
which be much good than dt, svm and lr. the averag mae of differ approach be show in 
tabl 3. the propos factor graph method have the low mae, which achiev 15%, 18% and 15% 
improv compar with dt, svm, and LR respectively. 
accuracy, precision, recal and f1-score be four commonli use perform metric in tradit 

muti-label learning, and their cdf be show in fig. 15(c), 15(d), 15(e), 15(f) respectively. As show in 
the figures, the distribut curv of the factor graph algorithm be more concentr to the right part 
of the figures, which mean that more user achiev high accuracy, high precision, high recall, and 
high f1-score compar with the baselines. the averag valu of the perform metric be show in 
tabl 3. compar with dt, svm, and lr, the propos factor graph algorithm gain +8.5%, +7.0% 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:22 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

68 

70 

72 

74 

76 

78 

80 

82 

40 45 50 55 60 65 70 75 80 

M 
ea 

n 
E 

xa 
ct 

m 
at 

ch 
(% 

) 

train dataset size (%) 

FG 
DT 

svm 
LR 

(a) mean exact match under differ train 

dataset size. 

0.3 

0.32 

0.34 

0.36 

0.38 

0.4 

0.42 

0.44 

40 45 50 55 60 65 70 75 80 

M 
ea 

n 
M 

A 
E 

train dataset size (%) 

FG 
DT 

svm 
LR 

(b) mean mae under differ train dataset size. 

65 

70 

75 

80 

4 8 12 16 20 

M 
ea 

n 
E 

xa 
ct 

m 
at 

ch 
(% 

) 

number of select featur 

FG 
DT 

svm 
LR 

(c) mean exact match under differ number of 

select features. 

0.3 

0.35 

0.4 

0.45 

0.5 

0.55 

0.6 

4 8 12 16 20 

M 
ea 

n 
M 

A 
E 

number of select featur 

FG 
DT 

svm 
LR 

(d) mean mae under differ number of select 

features. 

fig. 16. mean exact match and mae under differ train dataset size and number of select features. 

and +8.2% in averag accuracy; gain +12.2%, +14.7% and +12.3% in averag precision; gain +6.2%, 
+2.0% and +6.0% in recall; and gain +8.4%, +6.7% and +8.4% in f1-score respectively. 

7.4 paramet analysi 

It should be awar that the perform of the propos train algorithm should be influenc by 
sever system paramet such a the chosen of train set and features. In thi section, we analyz the 
influenc of train dataset size and the number of select featur on compound emot detect 
performance. 
the averag exact match and mae under differ train dataset size be illustr in fig. 16(a) 

and fig. 16(b). As show in the result, when the size of the train dataset size increas from 40% to 
80%, the exact match of propos factor graph model have +2.3% improv and the mae have 7% 
improvement. thi be due to the fact that when the size of train dataset size be too small, there be not 
enough instanc for training, which lead to underfitting. when the train dataset size be larg enough, 
the exact match tend to be stabl and could not be improv further by increas the size of train 
set. 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:23 

To explor the influenc of the select features, we fix the train set size to be 70% (about 40+ 
instanc over the all collect instances), and run the algorithm by vari the number of select 
featur from top-4 to top-22. the averag exact match and mae under differ number of select 
featur be illustr in fig. 16(c) and fig. 16(d). As show in the figure, when the number of featur 
increases, the perform decreases. when the number of select featur continu to becom larg 
(¿12), there be a signific downward trend about the exact match and upward trend about the mae. 
the reason be explain a follows. when the featur space becom larger, it normal requir more 
train data to “feed” the model. In our experiments, the number of instanc be about 40+, which be 
rel small. increas the number of featur will easili caus over-fit of the model, which will 
decreas the performance. accord to the figure, the perform be almost the same when the number 
of featur from 4 to 6, which impli that a small number of signific featur be enough for the 
detect task. note that too few featur will also caus under-fit or lack of gener in machin 
learn theories. We choos 6 a the default number of featur in our experi sinc it perform well 
under differ condit and can avoid over-fit and under-fit issues. 
In summary, the propos compound emot detect method yield the high exact match and 

the low mae compar with the baselin approaches. 

8 conclus 

automat emot detect be import to build intellig system and to evalu individual’ mental 
well-being. In thi paper, we propos moodexplorer, a system to detect people’ compound emot 
base on smartphon sens data. We first present the compound emot model that model people’ 
compound emot by the combin of basic emot categories, and formul the compound emot 
detect problem a a multi-label classif problem. then we conduct featur extraction, featur 
selection, and correl analysi on the user report dataset, which show that compound emot 
frequent appear and have high correl with some smartphon usag featur such a the sensor 
data and app usag patterns. We further introduc a factor graph model to describ the correl 
among emot label and features, and propos a machin learn algorithm for compound emot 
detection. the propos compound emot detect mechan be implement in an android app 
call moodexplorer. the app ran on 30 univers student for one month, and their smartphon 
sens data be collect for analysis. We conduct extens experi on the collect dataset, which 
show that moodexplor can infer user’ compound emot with exact match of 76.0% on average. 
the studi of smartphone-bas compound emot detect in our paper achiev sever cheer 

results, which includ an interest research problem of compound emot detection, a well-develop 
machin learn model, and a decent accuraci for compound emot detection. however, we shall also 
mention some limit of the work and the possibl futur directions. first, smartphone-bas sens be 
unobtrus but intermitt and coarse-grained. sinc the user may not carri the smartphon all day 
long, the sens data onli reflect part of the human activities. therefor it can roughli estim user’ 
compound emot in a coars time interval. toward fine-grain short-term emot detect be still 
a challeng task. second, the propos model be onli test on a small scale dataset. the result be 
verifi by 30 univers students, and it be unclear whether it can be gener to peopl under differ 
occupancy, differ age, differ culture, and differ races. the experi could be biased, and the 
collect data and emot report could be distort sinc we could not tell whether a subject report 
his/her emot truth and thoroughly. third, the propos detect model be person-specif and 
need to be train individually. It requir a rel long period to collect data from the user to build 
the model, which prevent it from be wide deploy in practice. In the next step, we may seek a 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:24 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

gener model for user group by cluster the featur and users, and we may appli the semi-supervis 
learn and transfer learn techniqu to reduc the startup time. 

the research on emot detect base on smartphon sens be a veri young area, and it enabl a 
new way to sense, understand, interact, and interven on human emot feel and mental wellbeing. 
It need the interdisciplinari effort from sociology, psychology, neuroscience, and comput science. 
togeth they will make more progress toward automat emot recognit and prediction. 

acknowledg 

the author sincer acknowledg the review and editor who provid valuabl comment and 
feedback to help to improv the qualiti of the paper. thi work be partial support by the nation 
key r&d program of china (grant no. 2017yfb1001800), the nation natur scienc foundat of 
china (grant nos. 61672278, 61373128, 61321491), the collabor innov center of novel softwar 
technolog and industrialization, and the sino-german institut of social computing. 

refer 
[1] J. ang, R. dhillon, A. krupski, E. shriberg, and A. stolcke. prosody-bas automat detect of annoy and 

frustrat in human-comput dialog. In interspeech. citeseer, 2002. 
[2] S. aral and D. walker. identifi influenti and suscept member of social networks. science, 337(6092):337–341, 

2012. 
[3] A. B. ashraf, S. lucey, J. F. cohn, T. chen, Z. ambadar, K. M. prkachin, and P. E. solomon. the pain face–pain 

express recognit use activ appear models. imag and vision computing, 27(12):1788–1796, 2009. 
[4] X. bao, S. fan, A. varshavsky, K. li, and R. roy choudhury. your reaction suggest you like the movie: automat 

content rate via reaction sensing. In proceed of the 2013 acm intern joint confer on pervas and 

ubiquit computing, page 197–206. acm, 2013. 
[5] C. beedie, P. terry, and A. lane. distinct between emot and mood. cognit & emotion, 19(6):847–878, 2005. 
[6] A. bogomolov, B. lepri, M. ferron, F. pianesi, and A. S. pentland. daili stress recognit from mobil phone data, 

weather condit and individu traits. In proceed of the 22nd acm intern confer on multimedia, 

page 477–486. acm, 2014. 
[7] L. canzian and M. musolesi. trajectori of depression: unobtrus monitor of depress state by mean of 

smartphon mobil trace analysis. In proceed of the 2015 acm intern joint confer on pervas and 
ubiquit computing, page 1293–1304. acm, 2015. 

[8] B. cao, L. zheng, C. zhang, P. S. yu, A. piscitello, J. zulueta, O. ajilore, K. ryan, and A. D. leow. deepmood: 

model mobil phone type dynam for mood detection. 2017. 
[9] J. R. crawford and J. D. henry. the posit and neg affect schedul (panas): construct validity, measur 

properti and norm data in a larg non-clin sample. british journal of clinic psychology, 43(3):245–265, 

2004. 
[10] E. diener, S. oishi, and R. E. lucas. personality, culture, and subject well-being: emot and cognit evalu 

of life. annual review of psychology, 54(1):403–425, 2003. 
[11] S. du, Y. tao, and A. M. martinez. compound facial express of emotion. proceed of the nation academi of 

sciences, 111(15):e1454–e1462, 2014. 
[12] P. ekman, W. V. friesen, M. o’sullivan, A. chan, I. diacoyanni-tarlatzis, K. heider, R. krause, W. A. lecompte, 

T. pitcairn, P. E. ricci-bitti, et al. univers and cultur differ in the judgment of facial express of emotion. 
journal of person and social psychology, 53(4):712, 1987. 

[13] A. exler, M. urschel, A. schankin, and M. beigl. smartphone-bas detect of locat chang use wifi data. In 
intern confer on wireless mobil commun and healthcare, page 164–167. springer, 2016. 

[14] J. H. fowler, N. A. christakis, et al. dynam spread of happi in a larg social network: longitudin analysi over 

20 year in the framingham heart study. british medic journal, 337:a2338, 2008. 
[15] B. J. frey and D. J. mackay. A revolution: belief propag in graph with cycles. advanc in neural inform 

process systems, page 479–485, 1998. 

[16] T. geller. how do you feel?: your comput knows. commun of the acm, 57(1):24–26, 2014. 
[17] A. gluhak, M. presser, L. zhu, S. esfandiyari, and S. kupschick. toward mood base mobil servic and applications. 

In european confer on smart sens and context, page 159–174. springer, 2007. 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:25 

[18] J. golbeck, C. robles, and K. turner. predict person with social media. In chi’11 extend abstract on 

human factor in comput systems, page 253–262. acm, 2011. 
[19] J. J. gross and O. P. john. individu differ in two emot regul processes: implic for affect, 

relationships, and well-being. journal of person and social psychology, 85(2):348, 2003. 
[20] J. M. hammersley and P. clifford. markov field on finit graph and lattices. 1971. 
[21] K. C. herdem. reactions: twitter base mobil applic for awar of friends’ emotions. In proceed of the 

2012 acm confer on ubiquit computing, page 796–797. acm, 2012. 
[22] J. hernandez, M. E. hoque, W. drevo, and R. W. picard. mood meter: count smile in the wild. In proceed of 

the 2012 acm confer on ubiquit computing, page 301–310. acm, 2012. 
[23] D. koller and N. friedman. probabilist graphic models: principl and techniques. mit press, 2009. 
[24] F. R. kschischang, B. J. frey, and h.-a. loeliger. factor graph and the sum-product algorithm. ieee transact 

on inform theory, 47(2):498–519, 2001. 
[25] J. lafferty, A. mccallum, and F. pereira. condit random fields: probabilist model for segment and label 

sequenc data. In proceed of the eighteenth intern confer on machin learn (icml ’01), volum 1, 
page 282–289, 2001. 

[26] H. leng, Y. lin, and L. zanzi. An experiment studi on physiolog paramet toward driver emot recognition. 

In intern confer on ergonom and health aspect of work with computers, page 237–246. springer, 
2007. 

[27] M. levandowski and D. winter. distanc between sets. nature, 234(5323):34–35, 1971. 
[28] S. li, L. huang, R. wang, and G. zhou. sentence-level emot classif with label and context dependence. 

proceed of acl-2015, page 1045–1053, 2013. 
[29] W. li, Y. hu, X. fu, S. lu, and D. chen. cooper posit and track in disrupt toler networks. ieee 

transact on parallel and distribut systems, 26(2):382–391, 2015. 
[30] R. likamwa, Y. liu, N. D. lane, and L. zhong. moodscope: build a mood sensor from smartphon usag patterns. 

In proceed of the 11th annual intern confer on mobil systems, applications, and services, page 389–402. 
acm, 2013. 

[31] G. mackerron and S. mourato. happi be great in natur environments. global environment change, 

23(5):992–1000, 2013. 
[32] A. mottelson and K. hornbæk. An affect detect techniqu use mobil commod sensor in the wild. In proceed 

of the 2016 acm intern joint confer on pervas and ubiquit computing, page 781–792. acm, 2016. 

[33] K. P. murphy. machin learning: a probabilist perspective. mit press, 2012. 
[34] R. plutchik. A gener psychoevolutionari theori of emotion. theori of emotion, 1(3-31):4, 1980. 
[35] J. posner, J. A. russell, and B. S. peterson. the circumplex model of affect: An integr approach to affect 

neuroscience, cognit development, and psychopathology. develop and psychopathology, 17(03):715–734, 2005. 
[36] R. reisenzein, M. studtmann, and G. horstmann. coher between emot and facial expression: evid from 

laboratori experiments. emot review, 5(1):16–23, 2013. 
[37] S. servia-rodŕıguez, K. K. rachuri, C. mascolo, P. J. rentfrow, N. lathia, and G. M. sandstrom. mobil sens at 

the servic of mental well-being: a large-scal longitudin study. In proceed of the 26th intern confer 

on world wide web, page 103–112. intern world wide web confer steer committee, 2017. 
[38] N. spolaôr, E. A. cherman, M. C. monard, and H. D. lee. A comparison of multi-label featur select method 

use the problem transform approach. electron note in theoret comput science, 292:135–151, 2013. 
[39] N. spolaôr, E. A. cherman, M. C. monard, and H. D. lee. relieff for multi-label featur selection. In intellig 

system (bracis), 2013 brazilian confer on, page 6–11. ieee, 2013. 
[40] S. A. stansfeld and M. P. matheson. nois pollution: non-auditori effect on health. british medic bulletin, 

68(1):243–257, 2003. 
[41] T. stütz, T. kowar, M. kager, M. tiefengrabner, M. stuppner, J. blechert, F. H. wilhelm, and S. ginzinger. smartphon 

base stress prediction. In intern confer on user modeling, adaptation, and personalization, page 240–251. 
springer, 2015. 

[42] Y. suhara, Y. xu, and A. pentland. deepmood: forecast depress mood base on self-report histori via 

recurr neural networks. In proceed of the 26th intern confer on world wide web, page 715–724. 
intern world wide web confer steer committee, 2017. 

[43] B. sun, Q. ma, S. zhang, K. liu, and Y. liu. iself: toward cold-start emot label use transfer learn with 

smartphones. In 2015 ieee confer on comput commun (infocom), page 1203–1211. ieee, 2015. 
[44] C. sutton and A. mccallum. An introduct to condit random fields. arxiv preprint arxiv:1011.4088, 2010. 
[45] S. S. tomkins. affect, imagery, consciousness: vol. i. the posit affects. 1962. 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:26 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

[46] G. tsoumakas, I. katakis, and I. vlahavas. mine multi-label data. In data mine and knowledg discoveri handbook, 

page 667–685. springer, 2009. 
[47] L. wasserman. all of statistics: a concis cours in statist inference. springer scienc & busi media, 2013. 
[48] Y. yang, J. jia, B. wu, and J. tang. social role-awar emot contagion in imag social networks. In thirtieth aaai 

confer on artifici intelligence, 2016. 

[49] Z. zeng, M. pantic, G. I. roisman, and T. S. huang. A survey of affect recognit methods: audio, visual, and 
spontan expressions. ieee transact on pattern analysi and machin intelligence, 31(1):39–58, 2009. 

[50] m.-l. zhang and z.-h. zhou. A review on multi-label learn algorithms. ieee transact on knowledg and data 
engineering, 26(8):1819–1837, 2014. 

[51] S. zhang and P. hui. A survey on mobil affect computing. arxiv prepr. arxiv14101648, 2014. 
[52] Y. zhang, J. tang, J. sun, Y. chen, and J. rao. moodcast: emot predict via dynam continu factor graph 

model. In proceed of the 10th intern confer on data mining, page 1193–1198. ieee, 2010. 
[53] M. zhao, F. adib, and D. katabi. emot recognit use wireless signals. In proceed of the 22nd annual 

intern confer on mobil comput and network (mobicom ’16), page 95–108, 2016. 
[54] S. zhao, H. yao, and X. jiang. predict continu probabl distribut of imag emot in valence-arous 

space. In proceed of the 23rd acm intern confer on multimedia, page 879–882. acm, 2015. 

[55] Y. zhou, H. xue, and X. geng. emot distribut recognit from facial expressions. In proceed of the 23rd 
acm intern confer on multimedia, page 1247–1250. acm, 2015. 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:27 

appendix 

A the RF weight OF all featur OF A user 

fea. ID featur weight fea. ID featur weight 
1 dark ratio (dr) 1.35 2 var. of acc. X axi 1.23 
3 var. of acc. Y axi 1.15 4 dur. of travel app 1.13 
5 dur. of life servic app 1.13 6 mean of acc. Z axi 1.10 
7 bright ratio (br) 1.01 8 dark bright ratio (dbr) 0.99 
9 freq. of ssid #2009 0.93 10 freq. of ssid #5455 0.93 
11 freq. of ssid #2060 0.92 12 call freq. of contact #1870 0.83 
13 silenc ratio (sr) 0.75 14 mean of gyro. X axi 0.73 
15 var. of audio volum 0.64 16 nois ratio (nr) 0.62 
17 nsr 0.58 18 call durat of contact #1870 0.58 
19 call freq. of contact #2021 0.56 20 var. of acc. Z axi 0.48 
21 var. of gyro. Z axi 0.45 22 mean of gyro. Y axi 0.45 
23 var. of gyro. X axi 0.44 24 mean of audio volum 0.40 
25 mean of gyro. Z axi 0.32 26 mean of acc. X axi 0.30 
27 call durat of contact #6387 0.29 28 call durat of contact #1577 0.29 
29 call durat of contact #2243 0.29 30 call freq. of contact #1577 0.29 
31 call durat of contact #3367 0.28 32 call freq. of contact #3367 0.28 
33 call freq. of contact #1713 0.28 34 call durat of contact #1713 0.28 
35 call freq. of contact #2243 0.28 36 call freq. of contact #6387 0.28 
37 call freq. of contact #2241 0.28 38 call durat of contact #5426 0.28 
39 call freq. of contact #5426 0.28 40 call durat of contact #2021 0.27 
41 mean of acc. Y axi 0.27 42 var. of gyro. Y axi 0.18 
43 call freq. of contact #1905 0.17 44 call durat of contact #4127 0.14 
45 call freq. of contact #1683 0.13 46 var. of light sensor valu 0.13 
47 dur. of mother&child app 0.13 48 dur. of smartphon beautif app 0.13 
49 freq. of ssid #1712 0.13 50 freq. of ssid #2244 0.13 
51 freq. of ssid #5135 0.13 52 freq. of ssid #1131 0.13 
53 dur. of imag app 0.13 54 dur. of vedio app 0.13 
55 dur. of offic effici app 0.11 56 dur. of shop app 0.11 
57 step count 0.11 58 gp altitud 0.09 
59 gp latitud 0.08 60 gp longitud 0.08 
61 dur. of system tool app 0.08 62 call durat of contact #1683 0.08 
63 call durat of contact #2241 0.03 64 dur. of life tool app 0.02 
65 dur. of financi manag app 0.02 66 dur. of social chat app 0.00 
67 dur. of health app 0.00 68 sm freq. of contact #6387 0.00 
69 sm freq. of contact #1577 0.00 70 sm freq. of contact #3367 0.00 
71 freq. of ssid #2116 0.00 72 freq. of ssid #1478 0.00 
73 freq. of ssid #9034 0.00 74 freq. of ssid #5458 0.00 
75 freq. of ssid #3546 0.00 76 dur. of telephon commun app 0.00 
77 call durat of contact #1905 0.00 78 sm freq. of contact #1683 0.00 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:28 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

fea. ID featur weight fea. ID featur weight 
79 sm freq. of contact #1870 0.00 80 sm freq. of contact #2021 0.00 
81 dur. of new app 0.00 82 dur. of traffic navig app 0.00 
83 dur. of educ app 0.00 84 dur. of music app 0.00 
85 sm freq. of contact #2243 -0.06 86 sm freq. of contact #2241 -0.06 
87 sm freq. of contact #5426 -0.06 88 sm freq. of contact #1905 -0.06 
89 mean of magnet Z axi -0.09 90 mean of magnet Y axi -0.09 
91 var. of magnet Y axi -0.09 92 call freq. of contact #4127 -0.11 
93 mean of light sensor valu -0.11 94 freq. of ssid #1618 -0.11 
95 freq. of ssid #1627 -0.11 96 freq. of ssid #1875 -0.11 
97 freq. of ssid #1751 -0.12 98 var. of magnet X axi -0.12 
99 mean of magnet X axi -0.12 100 var. of magnet Z axi -0.12 
101 sm freq. of contact #1713 -0.12 102 sm freq. of contact #4127 -0.13 
103 freq. of ssid #9673 -0.14 104 freq. of ssid #6470 -0.14 
105 freq. of ssid #4707 -0.14 106 freq. of ssid #3907 -0.16 
107 sleep durat -0.16 108 screen on ratio -0.16 
109 usag amount -0.17 110 screen off ratio -0.19 

tabl 4. example: the weight of an individual’ all features. 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



moodexplorer: toward compound emot detect via smartphon sens • 176:29 

B the select featur for all 30 user 

user first sel. fea. sec. sel. fea. third sel. fea. fourth sel. fea. fifth sel. fea. sixth sel. fea. 

1 
bright dark mean of light var. of freq. of call dur. of 

ratio (br) ratio (dr) sensor valu audio volum ssid #1873 contact #1436 

2 
step count mean of mean of varianc of durat of call freq. of 

gyro. Z axi gyro. X axi gyro. Y axi shop app contact #1037 

3 
screen On screen off call freq. of call dur. of freq. of freq. of one 

ratio ratio contact #1686 contact #1686 ssid #1074 ssid #2244 

4 
freq. of freq. of freq. of dur. of social durat of durat of 

ssid #3541 ssid #5541 ssid #1712 chat app video app music app 

5 
call dur. of call freq. of call dur. of call freq. of call dur. of sm freq. of 

contact #4890 contact #1682 contact #1133 contact #1803 contact #3769 contact #2110 

6 
bright freq. of freq. of freq. of nois dark 

ratio (br) ssid #1074 ssid #4392 ssid #2244 ratio (nr) ratio (dr) 

7 
dark bright durat of mean of gyro. mean of acc. dark screen On 
ratio (dbr) educ app Y axi Y axi ratio (dr) ratio 

8 
silenc call freq. of dark sm freq. of mean of varianc of 

ratio (sr) contact #1199 ratio (dr) contact #1086 audio volum acc. X axi 

9 
step count mean of mean of varianc of sm freq. of dark 

gyro. Y axi acc. X axi audio volum contact #1829 ratio (dr) 

10 
freq. of freq. of freq. of dur. of system screen On sleep 

ssid #2242 ssid #1904 ssid #7676 tool app ratio durat 

11 
silenc mean of bright mean of sm freq. of freq. of 

ratio (sr) gyro. Z axi ratio (br) audio volum contact #1776 ssid #1576 

12 
dark varianc of varianc of durat of dur. of life mean of 

ratio (dr) acc. X axi acc. Y axi travel app servic app acc. Z axi 

13 
freq. of mean of light call dur. of bright freq. of one varianc of 

ssid #6388 sensor valu contact #2112 ratio (br) ssid #2244 audio volum 

14 
mean of mean of dark mean of mean of freq. of 

acc. Z axi acc. Y axi ratio (dr) acc. X axi gyro. X axi ssid #1869 

15 
freq. of freq. of varianc of step count usag amount durat of 

ssid #5427 ssid #8244 acc. Y axi educ app 

16 
durat of durat of durat of varianc of durat of varianc of 
vedio app music app social chat app acc. X axi new app acc. Z axi 

17 
freq. of screen On sleep call freq. of sm freq. of call durat 

ssid #8224 ratio durat contact #2076 contact #2076 of contact #1598 

18 
mean of freq. of one sm freq. of varianc of screen On mean of 

acc. Z axi ssid #1712 contact #3889 acc. Z axi ratio acc. Y axi 

19 
silenc bright durat of dur. of traffic durat of nois 

ratio (sr) ratio (br) shop app navig app new app ratio (nr) 

20 
nois mean of varianc of freq. of screen off screen On 

ratio (nr) audio volum audio volum ssid #2022 ratio ratio 

21 
audio freq. of nois durat of screen On silenc 
mean ssid #1025 ratio (nr) educ app ratio ratio (sr) 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 



176:30 • xiao zhang, wenzhong li, Xu chen, and sanglu Lu 

22 
freq. of mean of dark mean of varianc of dur. of offic 

ssid #3368 gyro. X axi ratio (dr) gyro. Y axi gyro. X axi effici app 

23 
durat of durat of durat of screen off screen On mean of 
m&c app shop app vedio app ratio ratio gyro. Y axi 

24 
varianc of durat of durat dur. of system freq. of durat of 
acc. Y axi social chat app financi app tool app ssid #1684 educ app 

25 
screen off screen On freq. of mean of mean of call freq. of 

ratio ratio ssid #5427 gyro. Z axi acc. Z axi contact #8274 

26 
freq. of call freq. of call dur. of durat of bright var. of light 

ssid #4126 contact #4348 contact #1118 social chat app ratio (br) sensor valu 

27 
dark call freq. of sm freq. of sm freq. of mean of light varianc of 

ratio (dr) contact #1367 contact #1367 contact #1595 sensor valu audio volum 

28 
nois silenc bright mean of call freq. of varianc of dark 
ratio (nsr) ratio (br) acc. Z axi contact #1897 audio volum ratio (dr) 

29 
freq. of freq. of mean of bright dark mean of 

ssid #2242 ssid #6388 gyro. X axi ratio (br) ratio (dr) gyro. Y axi 

30 
nois dark durat of sm freq. of call dur. of call dur. of 

ratio (nr) ratio (dr) system tool app contact #4122 contact #4122 contact #6463 
tabl 5. select featur of anoth 20 users. 

receiv may 2017; revis august 2017; accept octob 2017 

proceed of the acm on interactive, mobile, wearabl and ubiquit technologies, vol. 1, no. 4, articl 176. 

public date: decemb 2017. 
















<< 
/ascii85encodepag fals 
/allowtranspar fals 
/autopositionepsfil fals 
/autorotatepag /none 
/bind /left 
/calgrayprofil (gray gamma 2.2) 
/calrgbprofil (srgb iec61966-2.1) 
/calcmykprofil (u.s. web coat \050swop\051 v2) 
/srgbprofil (srgb iec61966-2.1) 
/cannotembedfontpolici /error 
/compatibilitylevel 1.6 
/compressobject /off 
/compresspag true 
/convertimagestoindex true 
/passthroughjpegimag true 
/createjobticket fals 
/defaultrenderingint /default 
/detectblend true 
/detectcurv 0.0000 
/colorconversionstrategi /leavecolorunchang 
/dothumbnail true 
/embedallfont true 
/embedopentyp true 
/parseiccprofilesincom true 
/embedjobopt true 
/dscreportinglevel 0 
/emitdscwarn fals 
/endpag -1 
/imagememori 524288 
/lockdistillerparam true 
/maxsubsetpct 100 
/optim true 
/opm 0 
/parsedsccom fals 
/parsedsccommentsfordocinfo fals 
/preservecopypag true 
/preservedicmykvalu true 
/preserveepsinfo fals 
/preserveflat true 
/preservehalftoneinfo true 
/preserveopicom fals 
/preserveoverprintset true 
/startpag 1 
/subsetfont true 
/transferfunctioninfo /remov 
/ucrandbginfo /preserv 
/useprologu fals 
/colorsettingsfil (none) 
/alwaysemb [ true 
] 
/neveremb [ true 
] 
/antialiascolorimag fals 
/cropcolorimag true 
/colorimageminresolut 150 
/colorimageminresolutionpolici /ok 
/downsamplecolorimag true 
/colorimagedownsampletyp /bicub 
/colorimageresolut 300 
/colorimagedepth -1 
/colorimagemindownsampledepth 1 
/colorimagedownsamplethreshold 1.50000 
/encodecolorimag true 
/colorimagefilt /dctencod 
/autofiltercolorimag true 
/colorimageautofilterstrategi /jpeg 
/coloracsimagedict << 
/qfactor 0.15 
/hsampl [1 1 1 1] /vsampl [1 1 1 1] 
>> 
/colorimagedict << 
/qfactor 0.15 
/hsampl [1 1 1 1] /vsampl [1 1 1 1] 
>> 
/jpeg2000coloracsimagedict << 
/tilewidth 256 
/tileheight 256 
/qualiti 30 
>> 
/jpeg2000colorimagedict << 
/tilewidth 256 
/tileheight 256 
/qualiti 30 
>> 
/antialiasgrayimag fals 
/cropgrayimag true 
/grayimageminresolut 150 
/grayimageminresolutionpolici /ok 
/downsamplegrayimag true 
/grayimagedownsampletyp /bicub 
/grayimageresolut 300 
/grayimagedepth -1 
/grayimagemindownsampledepth 2 
/grayimagedownsamplethreshold 1.50000 
/encodegrayimag true 
/grayimagefilt /dctencod 
/autofiltergrayimag true 
/grayimageautofilterstrategi /jpeg 
/grayacsimagedict << 
/qfactor 0.15 
/hsampl [1 1 1 1] /vsampl [1 1 1 1] 
>> 
/grayimagedict << 
/qfactor 0.15 
/hsampl [1 1 1 1] /vsampl [1 1 1 1] 
>> 
/jpeg2000grayacsimagedict << 
/tilewidth 256 
/tileheight 256 
/qualiti 30 
>> 
/jpeg2000grayimagedict << 
/tilewidth 256 
/tileheight 256 
/qualiti 30 
>> 
/antialiasmonoimag fals 
/cropmonoimag true 
/monoimageminresolut 1200 
/monoimageminresolutionpolici /ok 
/downsamplemonoimag true 
/monoimagedownsampletyp /bicub 
/monoimageresolut 600 
/monoimagedepth -1 
/monoimagedownsamplethreshold 1.00167 
/encodemonoimag true 
/monoimagefilt /ccittfaxencod 
/monoimagedict << 
/K -1 
>> 
/allowpsxobject fals 
/checkcompli [ 
/none 
] 
/pdfx1acheck fals 
/pdfx3check fals 
/pdfxcompliantpdfonli fals 
/pdfxnotrimboxerror true 
/pdfxtrimboxtomediaboxoffset [ 
0.00000 
0.00000 
0.00000 
0.00000 
] 
/pdfxsetbleedboxtomediabox true 
/pdfxbleedboxtotrimboxoffset [ 
0.00000 
0.00000 
0.00000 
0.00000 
] 
/pdfxoutputintentprofil (none) 
/pdfxoutputconditionidentifi () 
/pdfxoutputcondit () 
/pdfxregistrynam () 
/pdfxtrap /fals 

/createjdffil fals 
/descript << 
/enu () 
>> 
>> setdistillerparam 
<< 
/hwresolut [600 600] 
/pages [612.000 792.000] 
>> setpagedevic 

