








from tika import parser 
import os, sy 
from nltk.stem import wordnetlemmat 
from nltk.stem import porterstemm 
from nltk.corpu import wordnet a wn 
import nltk 

pos_to_wornet_dict = { 

'jj': wn.adj, 
'jjr': wn.adj, 
'jjs': wn.adj, 
'rb': 'a', 
'rbr': 'a', 
'rbs': 'a', 
'nn': 'n', 
'nnp': 'n', 
'nns': 'n', 
'nnps': 'n', 
'vb': 'v', 
'vbg': 'v', 
'vbd': 'v', 
'vbn': 'v', 
'vbp': 'v', 
'vbz': 'v', 
'dt' : 'n', 
'in' : 'n', 
'prp$' : 'n', 
'md' : 'n', 
'to' : 'n', 
'cc' : 'n', 
'prp' : 'n' 

} 


lemmat = wordnetlemmatizer() 
stemmer = porterstemmer() 

file = os.listdir(os.getcwd()) 
for x in rang (0, len(files)): 
if os.path.isfile(files[x]): 
raw = parser.from_file(files[x]) 
#print(raw['content']) 
f = open("extract\\"+files[x]+".txt", "wb") 
f.write(raw['content'].encode('utf-8')) 
f.close() 
text = "" 
textlem = "" 

with open("extract\\"+files[x]+".txt",encoding="utf8") a f: 
for line in f: 
for word in line.split(): 
a = nltk.pos_tag(nltk.word_tokenize(word))[0][1] 
tag = 'n' 
if a in pos_to_wornet_dict: 
tag = pos_to_wornet_dict[a] 
newword = lemmatizer.lemmatize(word, tag) 

textlem = textlem + newword 
textlem = textlem + " " 

textlem = textlem + "\n" 

file = open("extractlem\\"+files[x]+"_lem.txt", "wb") 
file.write(textlem.encode('utf-8')) 
file.close() 
print(files[x]+"lem done") 

with open("extractlem\\"+files[x]+"_lem.txt",encoding="utf8") a f: 
for line in f: 
for word in line.split(): 
#print(word) 
newword = stemmer.stem(word) 

text = text + newword 
text = text + " " 

text = text + "\n" 

file = open("extractstem\\"+files[x]+"_stem.txt", "wb") 
file.write(text.encode('utf-8')) 
file.close() 

print(files[x]+"stem done") 


print(files[x]+" done") 






# a = nltk.pos_tag(nltk.word_tokenize(word))[0][1] 
# tag = 'n' 
# if a in pos_to_wornet_dict: 
# tag = pos_to_wornet_dict[a] 
# newword = lemmatizer.lemmatize(word, tag) 
# print(lemmatizer.lemmatize(word, tag)) 




