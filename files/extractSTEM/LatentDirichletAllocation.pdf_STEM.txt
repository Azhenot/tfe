




































blei03a.dvi 


journal of machin learn research 3 (2003) 993-1022 submit 2/02; publish 1/03 

latent dirichlet alloc 

david M. blei blei@cs.berkeley.edu 
comput scienc divis 
univers of california 
berkeley, CA 94720, usa 

andrew Y. Ng ang@cs.stanford.edu 
comput scienc depart 
stanford univers 
stanford, CA 94305, usa 

michael I. jordan jordan@cs.berkeley.edu 
comput scienc divis and depart of statist 
univers of california 
berkeley, CA 94720, usa 

editor: john lafferti 

abstract 

We describ latent dirichlet alloc (lda), a gener probabilist model for collect of 
discret data such a text corpora. lda be a three-level hierarch bayesian model, in which each 
item of a collect be model a a finit mixtur over an underli set of topics. each topic is, in 
turn, model a an infinit mixtur over an underli set of topic probabilities. In the context of 
text modeling, the topic probabl provid an explicit represent of a document. We present 
effici approxim infer techniqu base on variat method and an EM algorithm for 
empir bay paramet estimation. We report result in document modeling, text classification, 
and collabor filtering, compar to a mixtur of unigram model and the probabilist lsi 
model. 

1. introduct 

In thi paper we consid the problem of model text corpu and other collect of discret 
data. the goal be to find short descript of the member of a collect that enabl effici 
process of larg collect while preserv the essenti statist relationship that be use 
for basic task such a classification, novelti detection, summarization, and similar and relev 
judgments. 

signific progress have be make on thi problem by research in the field of informa- 
tion retriev (ir) (baeza-y and ribeiro-neto, 1999). the basic methodolog propos by 
IR research for text corpora—a methodolog success deploy in modern internet search 
engines—reduc each document in the corpu to a vector of real numbers, each of which repre- 
sent ratio of counts. In the popular tf-idf scheme (salton and mcgill, 1983), a basic vocabulari 
of “words” or “terms” be chosen, and, for each document in the corpus, a count be form of the 
number of occurr of each word. after suitabl normalization, thi term frequenc count be 
compar to an invers document frequenc count, which measur the number of occurr of a 

c©2003 david M. blei, andrew Y. Ng and michael I. jordan. 



blei, ng, and jordan 

word in the entir corpu (gener on a log scale, and again suitabl normalized). the end result 
be a term-by-docu matrix X whose column contain the tf-idf valu for each of the document 
in the corpus. thu the tf-idf scheme reduc document of arbitrari length to fixed-length list of 
numbers. 

while the tf-idf reduct have some appeal features—not in it basic identif of set 
of word that be discrimin for document in the collection—th approach also provid a rela- 
tive small amount of reduct in descript length and reveal littl in the way of inter- or intra- 
document statist structure. To address these shortcomings, IR research have propos sever 
other dimension reduct techniques, most notabl latent semant index (lsi) (deerwest 
et al., 1990). lsi us a singular valu decomposit of the X matrix to identifi a linear subspac 
in the space of tf-idf featur that captur most of the varianc in the collection. thi approach can 
achiev signific compress in larg collections. furthermore, deerwest et al. argu that the 
deriv featur of lsi, which be linear combin of the origin tf-idf features, can captur 
some aspect of basic linguist notion such a synonymi and polysemy. 

To substanti the claim regard lsi, and to studi it rel strength and weaknesses, it be 
use to develop a gener probabilist model of text corpu and to studi the abil of lsi to 
recov aspect of the gener model from data (papadimitri et al., 1998). given a gener 
model of text, however, it be not clear whi one should adopt the lsi methodology—on can attempt 
to proceed more directly, fit the model to data use maximum likelihood or bayesian methods. 

A signific step forward in thi regard be make by hofmann (1999), who present the 
probabilist lsi (plsi) model, also know a the aspect model, a an altern to lsi. the plsi 
approach, which we describ in detail in section 4.3, model each word in a document a a sampl 
from a mixtur model, where the mixtur compon be multinomi random variabl that can be 
view a represent of “topics.” thu each word be gener from a singl topic, and differ 
word in a document may be gener from differ topics. each document be repres a 
a list of mix proport for these mixtur compon and therebi reduc to a probabl 
distribut on a fix set of topics. thi distribut be the “reduc description” associ with 
the document. 

while hofmann’ work be a use step toward probabilist model of text, it be incomplet 
in that it provid no probabilist model at the level of documents. In plsi, each document be 
repres a a list of number (the mix proport for topics), and there be no gener 
probabilist model for these numbers. thi lead to sever problems: (1) the number of parame- 
ter in the model grow linearli with the size of the corpus, which lead to seriou problem with 
overfitting, and (2) it be not clear how to assign probabl to a document outsid of the train set. 

To see how to proceed beyond plsi, let u consid the fundament probabilist assumpt 
underli the class of dimension reduct method that includ lsi and plsi. all of these 
method be base on the “bag-of-words” assumption—that the order of word in a document can 
be neglected. In the languag of probabl theory, thi be an assumpt of exchang for the 
word in a document (aldous, 1985). moreover, although less often state formally, these method 
also assum that document be exchangeable; the specif order of the document in a corpu 
can also be neglected. 

A classic represent theorem due to de finetti (1990) establish that ani collect of ex- 
changeabl random variabl have a represent a a mixtur distribution—in gener an infinit 
mixture. thus, if we wish to consid exchang represent for document and words, we 
need to consid mixtur model that captur the exchang of both word and documents. 

994 



latent dirichlet alloc 

thi line of think lead to the latent dirichlet alloc (lda) model that we present in the 
current paper. 

It be import to emphas that an assumpt of exchang be not equival to an as- 
sumption that the random variabl be independ and ident distributed. rather, exchange- 
abil essenti can be interpret a mean “condit independ and ident dis- 
tributed,” where the condit be with respect to an underli latent paramet of a probabl 
distribution. conditionally, the joint distribut of the random variabl be simpl and factor 
while margin over the latent parameter, the joint distribut can be quit complex. thus, while 
an assumpt of exchang be clearli a major simplifi assumpt in the domain of text 
modeling, and it princip justif be that it lead to method that be comput efficient, 
the exchang assumpt do not necessarili lead to method that be restrict to simpl 
frequenc count or linear operations. We aim to demonstr in the current paper that, by take 
the de finetti theorem seriously, we can captur signific intra-docu statist structur via 
the mix distribution. 

It be also worth note that there be a larg number of gener of the basic notion of 
exchangeability, includ variou form of partial exchangeability, and that represent theo- 
rem be avail for these case a well (diaconis, 1988). thus, while the work that we discu in 
the current paper focu on simpl “bag-of-words” models, which lead to mixtur distribut for 
singl word (unigrams), our method be also applic to richer model that involv mixtur for 
larg structur unit such a n-gram or paragraphs. 

the paper be organ a follows. In section 2 we introduc basic notat and terminology. 
the lda model be present in section 3 and be compar to relat latent variabl model in 
section 4. We discu infer and paramet estim for lda in section 5. An illustr 
exampl of fit lda to data be provid in section 6. empir result in text modeling, text 
classif and collabor filter be present in section 7. finally, section 8 present our 
conclusions. 

2. notat and terminolog 

We use the languag of text collect throughout the paper, refer to entiti such a “words,” 
“documents,” and “corpora.” thi be use in that it help to guid intuition, particularli when 
we introduc latent variabl which aim to captur abstract notion such a topics. It be import 
to note, however, that the lda model be not necessarili tie to text, and have applic to other 
problem involv collect of data, includ data from domain such a collabor filtering, 
content-bas imag retriev and bioinformatics. indeed, in section 7.3, we present experiment 
result in the collabor filter domain. 

formally, we defin the follow terms: 
• A word be the basic unit of discret data, defin to be an item from a vocabulari index by 
{1, . . . ,v}. We repres word use unit-basi vector that have a singl compon equal to 
one and all other compon equal to zero. thus, use superscript to denot components, 
the vth word in the vocabulari be repres by a V -vector w such that wv = 1 and wu = 0 for 
u 6= v. 

• A document be a sequenc of N word denot by w = (w1,w2, . . . ,wn), where wn be the nth 
word in the sequence. 

• A corpu be a collect of M document denot by D = {w1,w2, . . . ,wm}. 

995 



blei, ng, and jordan 

We wish to find a probabilist model of a corpu that not onli assign high probabl to 
member of the corpus, but also assign high probabl to other “similar” documents. 

3. latent dirichlet alloc 

latent dirichlet alloc (lda) be a gener probabilist model of a corpus. the basic idea be 
that document be repres a random mixtur over latent topics, where each topic be charac- 
teriz by a distribut over words.1 

lda assum the follow gener process for each document w in a corpu D: 

1. choos N ∼ poisson(ξ). 
2. choos θ ∼ dir(α). 
3. for each of the N word wn: 

(a) choos a topic zn ∼ multinomial(θ). 
(b) choos a word wn from p(wn |zn,β), a multinomi probabl condit on the topic 

zn. 

sever simplifi assumpt be make in thi basic model, some of which we remov in subse- 
quent sections. first, the dimension k of the dirichlet distribut (and thu the dimension 
of the topic variabl z) be assum know and fixed. second, the word probabl be parameter- 
ize by a k×v matrix β where βi j = p(w j = 1 |zi = 1), which for now we treat a a fix quantiti 
that be to be estimated. finally, the poisson assumpt be not critic to anyth that follow and 
more realist document length distribut can be use a needed. furthermore, note that N be 
independ of all the other data gener variabl (θ and z). It be thu an ancillari variabl and 
we will gener ignor it random in the subsequ development. 

A k-dimension dirichlet random variabl θ can take valu in the (k−1)-simplex (a k-vector 
θ lie in the (k−1)-simplex if θi ≥ 0, ∑ki=1 θi = 1), and have the follow probabl densiti on thi 
simplex: 

p(θ |α) = Γ 
( 
∑ki=1 αi 

) 
∏ki=1 γ(αi) 

θα1−11 · · ·θαk−1k , (1) 

where the paramet α be a k-vector with compon αi > 0, and where γ(x) be the gamma function. 
the dirichlet be a conveni distribut on the simplex — it be in the exponenti family, have finit 
dimension suffici statistics, and be conjug to the multinomi distribution. In section 5, these 
properti will facilit the develop of infer and paramet estim algorithm for lda. 

given the paramet α and β, the joint distribut of a topic mixtur θ, a set of N topic z, and 
a set of N word w be give by: 

p(θ,z,w |α,β) = p(θ |α) 
N 

∏ 
n=1 

p(zn |θ)p(wn |zn,β), (2) 

1. We refer to the latent multinomi variabl in the lda model a topics, so a to exploit text-ori intuitions, but 
we make no epistemolog claim regard these latent variabl beyond their util in repres probabl 
distribut on set of words. 

996 



latent dirichlet alloc 

α z wθ 

β 

M 
N 

figur 1: graphic model represent of lda. the box be “plates” repres replicates. 
the outer plate repres documents, while the inner plate repres the repeat choic 
of topic and word within a document. 

where p(zn |θ) be simpli θi for the uniqu i such that zin = 1. integr over θ and sum over 
z, we obtain the margin distribut of a document: 

p(w |α,β) = 
∫ 

p(θ |α) 
( 

N 

∏ 
n=1 

∑ 
zn 

p(zn |θ)p(wn |zn,β) 
) 

dθ. (3) 

finally, take the product of the margin probabl of singl documents, we obtain the proba- 
biliti of a corpus: 

p(d |α,β) = 
M 

∏ 
d=1 

∫ 
p(θd |α) 

( 
Nd 

∏ 
n=1 

∑ 
zdn 

p(zdn |θd)p(wdn |zdn,β) 
) 

dθd . 

the lda model be repres a a probabilist graphic model in figur 1. As the figur 
make clear, there be three level to the lda representation. the paramet α and β be corpus- 
level parameters, assum to be sampl onc in the process of gener a corpus. the variabl 
θd be document-level variables, sampl onc per document. finally, the variabl zdn and wdn be 
word-level variabl and be sampl onc for each word in each document. 

It be import to distinguish lda from a simpl dirichlet-multinomi cluster model. A 
classic cluster model would involv a two-level model in which a dirichlet be sampl onc 
for a corpus, a multinomi cluster variabl be select onc for each document in the corpus, 
and a set of word be select for the document condit on the cluster variable. As with mani 
cluster models, such a model restrict a document to be associ with a singl topic. lda, 
on the other hand, involv three levels, and notabl the topic node be sampl repeatedli within the 
document. under thi model, document can be associ with multipl topics. 

structur similar to that show in figur 1 be often studi in bayesian statist modeling, 
where they be refer to a hierarch model (gelman et al., 1995), or more precis a con- 
dition independ hierarch model (kass and steffey, 1989). such model be also often 
refer to a parametr empir bay models, a term that refer not onli to a particular model 
structure, but also to the method use for estim paramet in the model (morris, 1983). in- 
deed, a we discu in section 5, we adopt the empir bay approach to estim paramet 
such a α and β in simpl implement of lda, but we also consid fuller bayesian approach 
a well. 

997 



blei, ng, and jordan 

3.1 lda and exchang 

A finit set of random variabl {z1, . . . ,zn} be say to be exchang if the joint distribut be 
invari to permutation. If π be a permut of the integ from 1 to N: 

p(z1, . . . ,zn) = p(zπ(1), . . . ,zπ(n)). 

An infinit sequenc of random variabl be infinit exchang if everi finit subsequ be 
exchangeable. 

De finetti’ represent theorem state that the joint distribut of an infinit exchang 
sequenc of random variabl be a if a random paramet be drawn from some distribut and 
then the random variabl in question be independ and ident distributed, condit on 
that parameter. 

In lda, we assum that word be gener by topic (bi fix condit distributions) and 
that those topic be infinit exchang within a document. By de finetti’ theorem, the prob- 
abil of a sequenc of word and topic must therefor have the form: 

p(w,z) = 
∫ 

p(θ) 

( 
N 

∏ 
n=1 

p(zn |θ)p(wn |zn) 
) 

dθ, 

where θ be the random paramet of a multinomi over topics. We obtain the lda distribut 
on document in eq. (3) by margin out the topic variabl and endow θ with a dirichlet 
distribution. 

3.2 A continu mixtur of unigram 

the lda model show in figur 1 be somewhat more elabor than the two-level model often 
studi in the classic hierarch bayesian literature. By margin over the hidden topic 
variabl z, however, we can understand lda a a two-level model. 

In particular, let u form the word distribut p(w |θ,β): 
p(w |θ,β) = ∑ 

z 
p(w |z,β)p(z |θ). 

note that thi be a random quantiti sinc it depend on θ. 
We now defin the follow gener process for a document w: 

1. choos θ ∼ dir(α). 
2. for each of the N word wn: 

(a) choos a word wn from p(wn |θ,β). 
thi process defin the margin distribut of a document a a continu mixtur distribution: 

p(w |α,β) = 
∫ 

p(θ |α) 
( 

N 

∏ 
n=1 

p(wn |θ,β) 
) 

dθ, 

where p(wn |θ,β) be the mixtur compon and p(θ |α) be the mixtur weights. 
figur 2 illustr thi interpret of lda. It depict the distribut on p(w |θ,β) which be 

induc from a particular instanc of an lda model. note that thi distribut on the (V − 1)- 
simplex be attain with onli k+kv paramet yet exhibit a veri interest multimod structure. 

998 



latent dirichlet alloc 

































figur 2: An exampl densiti on unigram distribut p(w |θ,β) under lda for three word and 
four topics. the triangl emb in the x-i plane be the 2-d simplex repres all 
possibl multinomi distribut over three words. each of the vertex of the trian- 
gle correspond to a determinist distribut that assign probabl one to one of the 
words; the midpoint of an edg give probabl 0.5 to two of the words; and the centroid 
of the triangl be the uniform distribut over all three words. the four point mark 
with an x be the locat of the multinomi distribut p(w |z) for each of the four 
topics, and the surfac show on top of the simplex be an exampl of a densiti over the 
(V −1)-simplex (multinomi distribut of words) give by lda. 

4. relationship with other latent variabl model 

In thi section we compar lda to simpler latent variabl model for text—th unigram model, a 
mixtur of unigrams, and the plsi model. furthermore, we present a unifi geometr interpreta- 
tion of these model which highlight their key differ and similarities. 

4.1 unigram model 

under the unigram model, the word of everi document be drawn independ from a singl 
multinomi distribution: 

p(w) = 
N 

∏ 
n=1 

p(wn). 

thi be illustr in the graphic model in figur 3a. 

999 



blei, ng, and jordan 

w 
M 

N 

(a) unigram 

z w 
M 

N 

(b) mixtur of unigram 

z w 
M 

Nd 

(c) plsi/aspect model 

figur 3: graphic model represent of differ model of discret data. 

4.2 mixtur of unigram 

If we augment the unigram model with a discret random topic variabl z (figur 3b), we obtain a 
mixtur of unigram model (nigam et al., 2000). under thi mixtur model, each document be gen- 
erat by first choos a topic z and then gener N word independ from the condit 
multinomi p(w |z). the probabl of a document is: 

p(w) = ∑ 
z 

p(z) 
N 

∏ 
n=1 

p(wn |z). 

when estim from a corpus, the word distribut can be view a represent of topic 
under the assumpt that each document exhibit exactli one topic. As the empir result in 
section 7 illustrate, thi assumpt be often too limit to effect model a larg collect of 
documents. 

In contrast, the lda model allow document to exhibit multipl topic to differ degrees. 
thi be achiev at a cost of just one addit parameter: there be k− 1 paramet associ 
with p(z) in the mixtur of unigrams, versu the k paramet associ with p(θ |α) in lda. 

4.3 probabilist latent semant index 

probabilist latent semant index (plsi) be anoth wide use document model (hofmann, 
1999). the plsi model, illustr in figur 3c, posit that a document label d and a word wn be 

1000 



latent dirichlet alloc 

condit independ give an unobserv topic z: 

p(d,wn) = p(d)∑ 
z 

p(wn |z)p(z |d). 

the plsi model attempt to relax the simplifi assumpt make in the mixtur of unigram 
model that each document be gener from onli one topic. In a sense, it do captur the possibl 
that a document may contain multipl topic sinc p(z |d) serf a the mixtur weight of the topic 
for a particular document d. however, it be import to note that d be a dummi index into the list 
of document in the train set. thus, d be a multinomi random variabl with a mani possibl 
valu a there be train document and the model learn the topic mixtur p(z |d) onli for those 
document on which it be trained. for thi reason, plsi be not a well-defin gener model of 
documents; there be no natur way to use it to assign probabl to a previous unseen document. 

A further difficulti with plsi, which also stem from the use of a distribut index by train- 
ing documents, be that the number of paramet which must be estim grow linearli with the 
number of train documents. the paramet for a k-topic plsi model be k multinomi distri- 
bution of size V and M mixtur over the k hidden topics. thi give kV + kM paramet and 
therefor linear growth in M. the linear growth in paramet suggest that the model be prone 
to overfit and, empirically, overfit be inde a seriou problem (see section 7.1). In prac- 
tice, a temper heurist be use to smooth the paramet of the model for accept predic- 
tive performance. It have be shown, however, that overfit can occur even when temper be 
use (popescul et al., 2001). 

lda overcom both of these problem by treat the topic mixtur weight a a k-paramet 
hidden random variabl rather than a larg set of individu paramet which be explicitli link to 
the train set. As describ in section 3, lda be a well-defin gener model and gener 
easili to new documents. furthermore, the k + kV paramet in a k-topic lda model do not grow 
with the size of the train corpus. We will see in section 7.1 that lda do not suffer from the 
same overfit issu a plsi. 

4.4 A geometr interpret 

A good way of illustr the differ between lda and the other latent topic model be by 
consid the geometri of the latent space, and see how a document be repres in that 
geometri under each model. 

all four of the model describ above—unigram, mixtur of unigrams, plsi, and lda— 
oper in the space of distribut over words. each such distribut can be view a a point on 
the (V −1)-simplex, which we call the word simplex. 

the unigram model find a singl point on the word simplex and posit that all word in the 
corpu come from the correspond distribution. the latent variabl model consid k point on 
the word simplex and form a sub-simplex base on those points, which we call the topic simplex. 
note that ani point on the topic simplex be also a point on the word simplex. the differ latent 
variabl model use the topic simplex in differ way to gener a document. 

• the mixtur of unigram model posit that for each document, one of the k point on the word 
simplex (that is, one of the corner of the topic simplex) be chosen randomli and all the word 
of the document be drawn from the distribut correspond to that point. 

1001 



blei, ng, and jordan 

x 

x x 

x 

x 

x 

x x 

x 

x 

x 

x 

x 

x 

x 

x 

xx 

x 

x 

x 

x 

xx 

x 

x 

x 

��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 

��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 
��������������������� 

����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 

����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 
����������������������������� 

�� 
�� 
�� 

�� 
�� 
�� 

�� 
�� 
�� 
�� 

� 
� 
� 
� 

topic 2 

topic 1 

topic 3 

topic simplex 

word simplex 

figur 4: the topic simplex for three topic emb in the word simplex for three words. the 
corner of the word simplex correspond to the three distribut where each word (re- 
spectively) have probabl one. the three point of the topic simplex correspond to three 
differ distribut over words. the mixtur of unigram place each document at one 
of the corner of the topic simplex. the plsi model induc an empir distribut on 
the topic simplex denot by x. lda place a smooth distribut on the topic simplex 
denot by the contour lines. 

• the plsi model posit that each word of a train document come from a randomli chosen 
topic. the topic be themselv drawn from a document-specif distribut over topics, 
i.e., a point on the topic simplex. there be one such distribut for each document; the set of 
train document thu defin an empir distribut on the topic simplex. 

• lda posit that each word of both the observ and unseen document be gener by a 
randomli chosen topic which be drawn from a distribut with a randomli chosen parameter. 
thi paramet be sampl onc per document from a smooth distribut on the topic simplex. 

these differ be highlight in figur 4. 

5. infer and paramet estim 

We have describ the motiv behind lda and illustr it conceptu advantag over other 
latent topic models. In thi section, we turn our attent to procedur for infer and paramet 
estim under lda. 

1002 



latent dirichlet alloc 

β 

α z wθ N 
M 

zθ 

φγ 

N M 

figur 5: (left) graphic model represent of lda. (right) graphic model represent 
of the variat distribut use to approxim the posterior in lda. 

5.1 infer 

the key inferenti problem that we need to solv in order to use lda be that of comput the 
posterior distribut of the hidden variabl give a document: 

p(θ,z |w,α,β) = p(θ,z,w |α,β) 
p(w |α,β) . 

unfortunately, thi distribut be intract to comput in general. indeed, to normal the distri- 
bution we margin over the hidden variabl and write eq. (3) in term of the model parameters: 

p(w |α,β) = γ(∑i αi) 
∏i γ(αi) 

∫ ( k 
∏ 
i=1 

θαi−1i 

)( 
N 

∏ 
n=1 

k 

∑ 
i=1 

V 

∏ 
j=1 

(θiβi j)w 
j 
n 

) 
dθ, 

a function which be intract due to the coupl between θ and β in the summat over latent 
topic (dickey, 1983). dickey show that thi function be an expect under a particular extens 
to the dirichlet distribut which can be repres with special hypergeometr functions. It have 
be use in a bayesian context for censor discret data to repres the posterior on θ which, in 
that setting, be a random paramet (dickey et al., 1987). 

although the posterior distribut be intract for exact inference, a wide varieti of approxi- 
mate infer algorithm can be consid for lda, includ laplac approximation, variat 
approximation, and markov chain mont carlo (jordan, 1999). In thi section we describ a simpl 
convexity-bas variat algorithm for infer in lda, and discu some of the altern in 
section 8. 

5.2 variat infer 

the basic idea of convexity-bas variat infer be to make use of jensen’ inequ to ob- 
tain an adjust low bound on the log likelihood (jordan et al., 1999). essentially, one consid 
a famili of low bounds, index by a set of variat parameters. the variat paramet 
be chosen by an optim procedur that attempt to find the tightest possibl low bound. 

A simpl way to obtain a tractabl famili of low bound be to consid simpl modif 
of the origin graphic model in which some of the edg and node be removed. consid in 
particular the lda model show in figur 5 (left). the problemat coupl between θ and β 

1003 



blei, ng, and jordan 

aris due to the edg between θ, z, and w. By drop these edg and the w nodes, and endow- 
ing the result simplifi graphic model with free variat parameters, we obtain a famili 
of distribut on the latent variables. thi famili be character by the follow variat 
distribution: 

q(θ,z |γ,φ) = q(θ |γ) 
N 

∏ 
n=1 

q(zn |φn), (4) 

where the dirichlet paramet γ and the multinomi paramet (φ1, . . . ,φn) be the free variat 
parameters. 

have specifi a simplifi famili of probabl distributions, the next step be to set up an 
optim problem that determin the valu of the variat paramet γ and φ. As we show 
in appendix A, the desideratum of find a tight low bound on the log likelihood translat 
directli into the follow optim problem: 

(γ∗,φ∗) = argmin 
(γ,φ) 

d(q(θ,z |γ,φ) ‖ p(θ,z |w,α,β)). (5) 

thu the optim valu of the variat paramet be found by minim the kullback- 
leibler (kl) diverg between the variat distribut and the true posterior p(θ,z |w,α,β). 
thi minim can be achiev via an iter fixed-point method. In particular, we show in 
appendix a.3 that by comput the deriv of the KL diverg and set them equal to 
zero, we obtain the follow pair of updat equations: 

φni ∝ βiwn exp{eq[log(θi) |γ]} (6) 
γi = αi +∑nn=1 φni. (7) 

As we show in appendix a.1, the expect in the multinomi updat can be comput a follows: 

eq[log(θi) |γ] = ψ(γi)−ψ 
( 
∑kj=1 γ j 

) 
, (8) 

where Ψ be the first deriv of the logγ function which be comput via taylor approxima- 
tion (abramowitz and stegun, 1970). 

eqs. (6) and (7) have an appeal intuit interpretation. the dirichlet updat be a poste- 
rior dirichlet give expect observ take under the variat distribution, e[zn |φn]. the 
multinomi updat be akin to use bayes’ theorem, p(zn |wn) ∝ p(wn |zn)p(zn), where p(zn) be 
approxim by the exponenti of the expect valu of it logarithm under the variat distri- 
bution. 

It be import to note that the variat distribut be actual a condit distribution, 
vari a a function of w. thi occur becaus the optim problem in eq. (5) be conduct 
for fix w, and thu yield optim paramet (γ∗,φ∗) that be a function of w. We can write 
the result variat distribut a q(θ,z |γ∗(w),φ∗(w)), where we have make the depend 
on w explicit. thu the variat distribut can be view a an approxim to the posterior 
distribut p(θ,z |w,α,β). 

In the languag of text, the optim paramet (γ∗(w),φ∗(w)) be document-specific. In 
particular, we view the dirichlet paramet γ∗(w) a provid a represent of a document in 
the topic simplex. 

1004 



latent dirichlet alloc 

(1) initi φ0ni := 1/k for all i and n 
(2) initi γi := αi +n/k for all i 
(3) repeat 
(4) for n = 1 to N 
(5) for i = 1 to k 
(6) φt+1ni := βiwn exp(ψ(γti)) 
(7) normal φt+1n to sum to 1. 
(8) γt+1 := α+∑nn=1 φt+1n 
(9) until converg 

figur 6: A variat infer algorithm for lda. 

We summar the variat infer procedur in figur 6, with appropri start point 
for γ and φn. from the pseudocod it be clear that each iter of variat infer for lda 
requir o((n + 1)k) operations. empirically, we find that the number of iter requir for a 
singl document be on the order of the number of word in the document. thi yield a total number 
of oper roughli on the order of n2k. 

5.3 paramet estim 

In thi section we present an empir bay method for paramet estim in the lda model 
(see section 5.4 for a fuller bayesian approach). In particular, give a corpu of document D = 
{w1,w2, . . . ,wm}, we wish to find paramet α and β that maxim the (marginal) log likelihood 
of the data: 

`(α,β) = 
M 

∑ 
d=1 

log p(wd |α,β). 

As we have describ above, the quantiti p(w |α,β) cannot be comput tractably. however, 
variat infer provid u with a tractabl low bound on the log likelihood, a bound which 
we can maxim with respect to α and β. We can thu find approxim empir bay estim 
for the lda model via an altern variat EM procedur that maxim a low bound with 
respect to the variat paramet γ and φ, and then, for fix valu of the variat parameters, 
maxim the low bound with respect to the model paramet α and β. 

We provid a detail deriv of the variat EM algorithm for lda in appendix a.4. 
the deriv yield the follow iter algorithm: 

1. (e-step) for each document, find the optim valu of the variat paramet {γ∗d ,φ∗d : 
d ∈ d}. thi be do a describ in the previou section. 

2. (m-step) maxim the result low bound on the log likelihood with respect to the model 
paramet α and β. thi correspond to find maximum likelihood estim with expect 
suffici statist for each document under the approxim posterior which be comput in 
the e-step. 

1005 



blei, ng, and jordan 

α z wθ 
M 

N 

k 
βη 

figur 7: graphic model represent of the smooth lda model. 

these two step be repeat until the low bound on the log likelihood converges. 
In appendix a.4, we show that the m-step updat for the condit multinomi paramet β 

can be write out analytically: 

βi j ∝ 
M 

∑ 
d=1 

Nd 

∑ 
n=1 

φ∗dniw 
j 
dn. (9) 

We further show that the m-step updat for dirichlet paramet α can be implement use an 
effici newton-raphson method in which the hessian be invert in linear time. 

5.4 smooth 

the larg vocabulari size that be characterist of mani document corpu creat seriou problem 
of sparsity. A new document be veri like to contain word that do not appear in ani of the 
document in a train corpus. maximum likelihood estim of the multinomi paramet 
assign zero probabl to such words, and thu zero probabl to new documents. the standard 
approach to cop with thi problem be to “smooth” the multinomi parameters, assign posit 
probabl to all vocabulari item whether or not they be observ in the train set (jelinek, 
1997). laplac smooth be commonli used; thi essenti yield the mean of the posterior 
distribut under a uniform dirichlet prior on the multinomi parameters. 

unfortunately, in the mixtur model setting, simpl laplac smooth be no longer justifi a a 
maximum a posteriori method (although it be often implement in practice; cf. nigam et al., 1999). 
In fact, by place a dirichlet prior on the multinomi paramet we obtain an intract posterior 
in the mixtur model setting, for much the same reason that one obtain an intract posterior in 
the basic lda model. our propos solut to thi problem be to simpli appli variat infer 
method to the extend model that includ dirichlet smooth on the multinomi parameter. 

In the lda setting, we obtain the extend graphic model show in figur 7. We treat β a 
a k×v random matrix (one row for each mixtur component), where we assum that each row 
be independ drawn from an exchang dirichlet distribution.2 We now extend our infer- 
enc procedur to treat the βi a random variabl that be endow with a posterior distribution, 

2. An exchang dirichlet be simpli a dirichlet distribut with a singl scalar paramet η. the densiti be the same 
a a dirichlet (eq. 1) where αi = η for each component. 

1006 



latent dirichlet alloc 

condit on the data. thu we move beyond the empir bay procedur of section 5.3 and 
consid a fuller bayesian approach to lda. 

We consid a variat approach to bayesian infer that place a separ distribut on 
the random variabl β, θ, and z (attias, 2000): 

q(β1:k,z1:m,θ1:m |λ,φ,γ) = 
k 

∏ 
i=1 

dir(βi |λi) 
M 

∏ 
d=1 

qd(θd,zd |φd,γd), 

where qd(θ,z |φ,γ) be the variat distribut defin for lda in eq. (4). As be easili verified, 
the result variat infer procedur again yield eqs. (6) and (7) a the updat equat 
for the variat paramet φ and γ, respectively, a well a an addit updat for the new 
variat paramet λ: 

λi j = η+ 
M 

∑ 
d=1 

Nd 

∑ 
n=1 

φ∗dniw 
j 
dn. 

iter these equat to converg yield an approxim posterior distribut on β, θ, and z. 
We be now left with the hyperparamet η on the exchang dirichlet, a well a the hy- 

perparamet α from before. our approach to set these hyperparamet be again (approximate) 
empir bayes—w use variat EM to find maximum likelihood estim of these paramet 
base on the margin likelihood. these procedur be describ in appendix a.4. 

6. exampl 

In thi section, we provid an illustr exampl of the use of an lda model on real data. our 
data be 16,000 document from a subset of the trec AP corpu (harman, 1992). after remov 
a standard list of stop words, we use the EM algorithm describ in section 5.3 to find the dirichlet 
and condit multinomi paramet for a 100-topic lda model. the top word from some of 
the result multinomi distribut p(w |z) be illustr in figur 8 (top). As we have hoped, 
these distribut seem to captur some of the underli topic in the corpu (and we have name 
them accord to these topics). 

As we emphas in section 4, one of the advantag of lda over relat latent variabl mod- 
el be that it provid well-defin infer procedur for previous unseen documents. indeed, 
we can illustr how lda work by perform infer on a held-out document and examin 
the result variat posterior parameters. 

figur 8 (bottom) be a document from the trec AP corpu which be not use for paramet 
estimation. use the algorithm in section 5.1, we comput the variat posterior dirichlet 
paramet γ for the articl and variat posterior multinomi paramet φn for each word in the 
article. 

recal that the ith posterior dirichlet paramet γi be approxim the ith prior dirichlet pa- 
ramet αi plu the expect number of word which be gener by the ith topic (see eq. 7). 
therefore, the prior dirichlet paramet subtract from the posterior dirichlet paramet indic 
the expect number of word which be alloc to each topic for a particular document. for 
the exampl articl in figur 8 (bottom), most of the γi be close to αi. four topics, however, be 
significantli larg (bi this, we mean γi −αi ≥ 1). look at the correspond distribut over 
word identifi the topic which mix to form thi document (figur 8, top). 

1007 



blei, ng, and jordan 

further insight come from examin the φn parameters. these distribut approxim 
p(zn |w) and tend to peak toward one of the k possibl topic values. In the articl text in figur 8, 
the word be color cod accord to these valu (i.e., the ith color be use if qn(zin = 1) > 0.9). 
with thi illustration, one can identifi how the differ topic mix in the document text. 

while demonstr the power of lda, the posterior analysi also highlight some of it lim- 
itations. In particular, the bag-of-word assumpt allow word that should be gener by the 
same topic (e.g., “william randolph hearst foundation”) to be alloc to sever differ top- 
ics. overcom thi limit would requir some form of extens of the basic lda model; 
in particular, we might relax the bag-of-word assumpt by assum partial exchang or 
markovian of word sequences. 

7. applic and empir result 

In thi section, we discu our empir evalu of lda in sever problem domains—docu 
modeling, document classification, and collabor filtering. 

In all of the mixtur models, the expect complet log likelihood of the data have local max- 
ima at the point where all or some of the mixtur compon be equal to each other. To avoid 
these local maxima, it be import to initi the EM algorithm appropriately. In our experiments, 
we initi EM by seed each condit multinomi distribut with five documents, reduc- 
ing their effect total length to two words, and smooth across the whole vocabulary. thi be 
essenti an approxim to the scheme describ in heckerman and meila (2001). 

7.1 document model 

We train a number of latent variabl models, includ lda, on two text corpu to compar the 
gener perform of these models. the document in the corpu be treat a unlabeled; 
thus, our goal be densiti estimation—w wish to achiev high likelihood on a held-out test set. In 
particular, we comput the perplex of a held-out test set to evalu the models. the perplexity, 
use by convent in languag modeling, be monoton decreas in the likelihood of the test 
data, and be algebraicli equival to the invers of the geometr mean per-word likelihood. A 
low perplex score indic good gener performance.3 more formally, for a test set of 
M documents, the perplex is: 

perplexity(dtest) = exp 
{ 
−∑ 

M 
d=1 log p(wd) 

∑md=1 Nd 

} 
. 

In our experiments, we use a corpu of scientif abstract from the C. elegan commun (av- 
ery, 2002) contain 5,225 abstract with 28,414 uniqu terms, and a subset of the trec AP corpu 
contain 16,333 newswir articl with 23,075 uniqu terms. In both cases, we held out 10% of 
the data for test purpos and train the model on the remain 90%. In preprocess the data, 

3. note that we simpli use perplex a a figur of merit for compar models. the model that we compar be all 
unigram (“bag-of-words”) models, which—a we have discuss in the introduction—ar of interest in the informa- 
tion retriev context. We be not attempt to do languag model in thi paper—an enterpris that would requir 
u to examin trigram or other higher-ord models. We note in passing, however, that extens of lda could be 
consid that involv dirichlet-multinomi over trigram instead of unigrams. We leav the explor of such 
extens to languag model to futur work. 

1008 



latent dirichlet alloc 

\arts" \budgets" \children" \education" 

new million children school 

film tax women student 

show program peopl school 

music budget child educ 

movi billion year teacher 

play feder famili high 

music year work public 

best spend parent teacher 

actor new say bennett 

first state famili manigat 

york plan welfar namphi 

opera money men state 

theater program percent presid 

actress govern care elementari 

love congress life haiti 

the william randolph hearst foundat will give $1.25 million to lincoln center, metropoli- 
tan opera co., new york philharmon and juilliard school. “our board felt that we have a 
real opportun to make a mark on the futur of the perform art with these grant an act 
everi bit a import a our tradit area of support in health, medic research, educ 
and the social services,” hearst foundat presid randolph A. hearst say monday in 

announc the grants. lincoln center’ share will be $200,000 for it new building, which 
will hous young artist and provid new public facilities. the metropolitan opera co. and 
new york philharmon will receiv $400,000 each. the juilliard school, where music and 
the perform art be taught, will get $250,000. the hearst foundation, a lead support 
of the lincoln center consolid corpor fund, will make it usual annual $100,000 

donation, too. 

figur 8: An exampl articl from the AP corpus. each color code a differ factor from which 
the word be put generated. 

1009 



blei, ng, and jordan 

0 10 20 30 40 50 60 70 80 90 100 
1400 

1600 

1800 

2000 

2200 

2400 

2600 

2800 

3000 

3200 

3400 

number of topic 

P 
er 

pl 
ex 

iti 
smooth unigram 
smooth mixt. unigram 
lda 
fold in plsi 

0 20 40 60 80 100 120 140 160 180 200 
2500 

3000 

3500 

4000 

4500 

5000 

5500 

6000 

6500 

7000 

number of topic 

P 
er 

pl 
ex 

iti 

smooth unigram 
smooth mixt. unigram 
lda 
fold in plsi 

figur 9: perplex result on the nematod (top) and AP (bottom) corpu for lda, the unigram 
model, mixtur of unigrams, and plsi. 

1010 



latent dirichlet alloc 

num. topic (k) perplex (mult. mixt.) perplex (plsi) 

2 22,266 7,052 
5 2.20×108 17,588 
10 1.93×1017 63,800 
20 1.20×1022 2.52×105 
50 4.19×10106 5.04×106 
100 2.39×10150 1.72×107 
200 3.51×10264 1.31×107 

tabl 1: overfit in the mixtur of unigram and plsi model for the AP corpus. similar behav- 
ior be observ in the nematod corpu (not reported). 

we remov a standard list of 50 stop word from each corpus. from the AP data, we further 
remov word that occur onli once. 

We compar lda with the unigram, mixtur of unigrams, and plsi model describ in sec- 
tion 4. We train all the hidden variabl model use EM with exactli the same stop criteria, 
that the averag chang in expect log likelihood be less than 0.001%. 

both the plsi model and the mixtur of unigram suffer from seriou overfit issues, though 
for differ reasons. thi phenomenon be illustr in tabl 1. In the mixtur of unigram model, 
overfit be a result of peak posterior in the train set; a phenomenon familiar in the super- 
vise setting, where thi model be know a the naiv bay model (rennie, 2001). thi lead to a 
nearli determinist cluster of the train document (in the e-step) which be use to determin 
the word probabl in each mixtur compon (in the m-step). A previous unseen document 
may best fit one of the result mixtur components, but will probabl contain at least one word 
which do not occur in the train document that be assign to that component. such word 
will have a veri small probability, which caus the perplex of the new document to explode. 
As k increases, the document of the train corpu be partit into finer collect and thu 
induc more word with small probabilities. 

In the mixtur of unigrams, we can allevi overfit through the variat bayesian smooth- 
ing scheme present in section 5.4. thi ensur that all word will have some probabl under 
everi mixtur component. 

In the plsi case, the hard cluster problem be allevi by the fact that each document be 
allow to exhibit a differ proport of topics. however, plsi onli refer to the train doc- 
ument and a differ overfit problem aris that be due to the dimension of the p(z|d) 
parameter. one reason approach to assign probabl to a previous unseen document be by 
margin over d: 

p(w) = ∑ 
d 

N 

∏ 
n=1 

∑ 
z 

p(wn |z)p(z |d)p(d). 

essentially, we be integr over the empir distribut on the topic simplex (see figur 4). 
thi method of inference, though theoret sound, caus the model to overfit. the document- 

specif topic distribut have some compon which be close to zero for those topic that do not 
appear in the document. thus, certain word will have veri small probabl in the estim of 

1011 



blei, ng, and jordan 

each mixtur component. when determin the probabl of a new document through marginal- 
ization, onli those train document which exhibit a similar proport of topic will contribut 
to the likelihood. for a give train document’ topic proportions, ani word which have small 
probabl in all the constitu topic will caus the perplex to explode. As k get larger, the 
chanc that a train document will exhibit topic that cover all the word in the new document 
decreas and thu the perplex grows. note that plsi do not overfit a quickli (with respect to 
k) a the mixtur of unigrams. 

thi overfit problem essenti stem from the restrict that each futur document exhibit 
the same topic proport a be see in one or more of the train documents. given thi 
constraint, we be not free to choos the most like proport of topic for the new document. An 
altern approach be the “folding-in” heurist suggest by hofmann (1999), where one ignor 
the p(z|d) paramet and refit p(z|dnew). note that thi give the plsi model an unfair advantag 
by allow it to refit k−1 paramet to the test data. 

lda suffer from neither of these problems. As in plsi, each document can exhibit a differ 
proport of underli topics. however, lda can easili assign probabl to a new document; 
no heurist be need for a new document to be endow with a differ set of topic proport 
than be associ with document in the train corpus. 

figur 9 present the perplex for each model on both corpu for differ valu of k. the 
plsi model and mixtur of unigram be suitabl correct for overfitting. the latent variabl 
model perform good than the simpl unigram model. lda consist perform good than the 
other models. 

7.2 document classif 

In the text classif problem, we wish to classifi a document into two or more mutual ex- 
clusiv classes. As in ani classif problem, we may wish to consid gener approach 
or discrimin approaches. In particular, by use one lda modul for each class, we obtain a 
gener model for classification. It be also of interest to use lda in the discrimin framework, 
and thi be our focu in thi section. 

A challeng aspect of the document classif problem be the choic of features. treat 
individu word a featur yield a rich but veri larg featur set (joachims, 1999). one way to 
reduc thi featur set be to use an lda model for dimension reduction. In particular, lda 
reduc ani document to a fix set of real-valu features—th posterior dirichlet paramet 
γ∗(w) associ with the document. It be of interest to see how much discriminatori inform 
we lose in reduc the document descript to these parameters. 

We conduct two binari classif experi use the reuters-21578 dataset. the 
dataset contain 8000 document and 15,818 words. 

In these experiments, we estim the paramet of an lda model on all the documents, 
without refer to their true class label. We then train a support vector machin (svm) on the 
low-dimension represent provid by lda and compar thi svm to an svm train on 
all the word features. 

use the svmlight softwar packag (joachims, 1999), we compar an svm train on all 
the word featur with those train on featur induc by a 50-topic lda model. note that we 
reduc the featur space by 99.6 percent in thi case. 

1012 



latent dirichlet alloc 

0 0.05 0.1 0.15 0.2 0.25 
85 

90 

95 

proport of data use for train 

A 
c 
c 
u 

ra 
c 
y 

0 0.05 0.1 0.15 0.2 0.25 
93 

94 

95 

96 

97 

98 

proport of data use for train 

A 
c 
c 
u 

ra 
c 
y 

word featur 

lda featur 

word featur 

lda featur 

(b)(a) 

figur 10: classif result on two binari classif problem from the reuters-21578 
dataset for differ proport of train data. graph (a) be earn vs. not earn. 
graph (b) be grain vs. not grain. 

0 10 20 30 40 50 
200 

250 

300 

350 

400 

450 

500 

550 

600 

number of topic 

P 
re 

di 
ct 

iv 
e 

P 
er 

pl 
ex 

iti 

lda 
fold in plsi 
smooth mixt. unigram 

figur 11: result for collabor filter on the eachmovi data. 

figur 10 show our results. We see that there be littl reduct in classif perform 
in use the lda-bas features; indeed, in almost all case the perform be improv with the 
lda features. although these result need further substantiation, they suggest that the topic-bas 
represent provid by lda may be use a a fast filter algorithm for featur select in 
text classification. 

1013 



blei, ng, and jordan 

7.3 collabor filter 

our final experi us the eachmovi collabor filter data. In thi data set, a collect 
of user indic their prefer movi choices. A user and the movi chosen be analog to a 
document and the word in the document (respectively). 

the collabor filter task be a follows. We train a model on a fulli observ set of users. 
then, for each unobserv user, we be show all but one of the movi prefer by that user and 
be ask to predict what the held-out movi is. the differ algorithm be evalu accord to 
the likelihood they assign to the held-out movie. more precisely, defin the predict perplex on 
M test user to be: 

predictive-perplexity(dtest) = exp 
{ 
−∑ 

M 
d=1 log p(wd,nd |wd,1:nd−1) 

M 
) 
} 

. 

We restrict the eachmovi dataset to user that posit rat at least 100 movi (a posit 
rate be at least four out of five stars). We divid thi set of user into 3300 train user and 390 
test users. 

under the mixtur of unigram model, the probabl of a movi give a set of observ movi 
be obtain from the posterior distribut over topics: 

p(w|wobs) = ∑ 
z 

p(w|z)p(z|wobs). 

In the plsi model, the probabl of a held-out movi be give by the same equat except that 
p(z|wobs) be comput by fold in the previous see movies. finally, in the lda model, the 
probabl of a held-out movi be give by integr over the posterior dirichlet: 

p(w|wobs) = 
∫ 

∑ 
z 

p(w|z)p(z|θ)p(θ|wobs)dθ, 

where p(θ|wobs) be give by the variat infer method describ in section 5.2. note that 
thi quantiti be effici to compute. We can interchang the sum and integr sign, and comput a 
linear combin of k dirichlet expectations. 

with a vocabulari of 1600 movies, we find the predict perplex illustr in figur 11. 
again, the mixtur of unigram model and plsi be correct for overfitting, but the best predict 
perplex be obtain by the lda model. 

8. discuss 

We have describ latent dirichlet allocation, a flexibl gener probabilist model for collec- 
tion of discret data. lda be base on a simpl exchang assumpt for the word and 
topic in a document; it be therefor realiz by a straightforward applic of de finetti’ repre- 
sentat theorem. We can view lda a a dimension reduct technique, in the spirit of lsi, 
but with proper underli gener probabilist semant that make sens for the type of data 
that it models. 

exact infer be intract for lda, but ani of a larg suit of approxim infer algo- 
rithm can be use for infer and paramet estim within the lda framework. We have 
present a simpl convexity-bas variat approach for inference, show that it yield a fast 

1014 



latent dirichlet alloc 

algorithm result in reason compar perform in term of test set likelihood. other 
approach that might be consid includ laplac approximation, higher-ord variat tech- 
niques, and mont carlo methods. In particular, leisink and kappen (2002) have present a 
gener methodolog for convert low-ord variat low bound into higher-ord varia- 
tional bounds. It be also possibl to achiev high accuraci by dispens with the requir of 
maintain a bound, and inde minka and lafferti (2002) have show that improv inferenti 
accuraci can be obtain for the lda model via a higher-ord variat techniqu know a ex- 
pectat propagation. finally, griffith and steyver (2002) have present a markov chain mont 
carlo algorithm for lda. 

lda be a simpl model, and although we view it a a competitor to method such a lsi and 
plsi in the set of dimension reduct for document collect and other discret cor- 
pora, it be also intend to be illustr of the way in which probabilist model can be scale 
up to provid use inferenti machineri in domain involv multipl level of structure. in- 
deed, the princip advantag of gener model such a lda includ their modular and their 
extensibility. As a probabilist module, lda can be readili emb in a more complex model— 
a properti that be not possess by lsi. In recent work we have use pair of lda modul to 
model relationship between imag and their correspond descript caption (blei and jordan, 
2002). moreover, there be numer possibl extens of lda. for example, lda be readili 
extend to continu data or other non-multinomi data. As be the case for other mixtur models, 
includ finit mixtur model and hidden markov models, the “emission” probabl p(wn |zn) 
contribut onli a likelihood valu to the infer procedur for lda, and other likelihood be 
readili substitut in it place. In particular, it be straightforward to develop a continu variant of 
lda in which gaussian observ be use in place of multinomials. anoth simpl extens 
of lda come from allow mixtur of dirichlet distribut in the place of the singl dirichlet 
of lda. thi allow a richer structur in the latent topic space and in particular allow a form of 
document cluster that be differ from the cluster that be achiev via share topics. finally, 
a varieti of extens of lda can be consid in which the distribut on the topic variabl 
be elaborated. for example, we could arrang the topic in a time series, essenti relax the 
full exchang assumpt to one of partial exchangeability. We could also consid partial 
exchang model in which we condit on exogen variables; thus, for example, the topic 
distribut could be condit on featur such a “paragraph” or “sentence,” provid a more 
power text model that make use of inform obtain from a parser. 

acknowledg 

thi work be support by the nation scienc foundat (nsf grant iis-9988642) and the 
multidisciplinari research program of the depart of defens (muri n00014-00-1-0637). 
andrew Y. Ng and david M. blei be addit support by fellowship from the microsoft 
corporation. 

refer 

M. abramowitz and I. stegun, editors. handbook of mathemat functions. dover, new york, 
1970. 

1015 



blei, ng, and jordan 

D. aldous. exchang and relat topics. In écol d’été de probabilité de saint-flour, xiii— 
1983, page 1–198. springer, berlin, 1985. 

H. attias. A variat bayesian framework for graphic models. In advanc in neural informa- 
tion process system 12, 2000. 

L. avery. caenorrhabd genet center bibliography. 2002. url 
http://elegans.swmed.edu/wli/cgcbib. 

R. baeza-y and B. ribeiro-neto. modern inform retrieval. acm press, new york, 1999. 

D. blei and M. jordan. model annot data. technic report ucb//csd-02-1202, u.c. 
berkeley comput scienc division, 2002. 

B. de finetti. theori of probability. vol. 1-2. john wiley & son ltd., chichester, 1990. reprint 
of the 1975 translation. 

S. deerwester, S. dumais, T. landauer, G. furnas, and R. harshman. index by latent semant 
analysis. journal of the american societi of inform science, 41(6):391–407, 1990. 

P. diaconis. recent progress on de finetti’ notion of exchangeability. In bayesian statistics, 3 
(valencia, 1987), page 111–125. oxford univ. press, new york, 1988. 

J. dickey. multipl hypergeometr functions: probabilist interpret and statist uses. 
journal of the american statist association, 78:628–637, 1983. 

J. dickey, J. jiang, and J. kadane. bayesian method for censor categor data. journal of the 
american statist association, 82:773–781, 1987. 

A. gelman, J. carlin, H. stern, and D. rubin. bayesian data analysis. chapman & hall, london, 
1995. 

T. griffith and M. steyvers. A probabilist approach to semant representation. In proceed 
of the 24th annual confer of the cognit scienc society, 2002. 

D. harman. overview of the first text retriev confer (trec-1). In proceed of the first 
text retriev confer (trec-1), page 1–20, 1992. 

D. heckerman and M. meila. An experiment comparison of sever cluster and initi 
methods. machin learning, 42:9–29, 2001. 

T. hofmann. probabilist latent semant indexing. proceed of the twenty-second annual 
intern sigir conference, 1999. 

F. jelinek. statist method for speech recognition. mit press, cambridge, ma, 1997. 

T. joachims. make large-scal svm learn practical. In advanc in kernel method - support 
vector learning. m.i.t. press, 1999. 

M. jordan, editor. learn in graphic models. mit press, cambridge, ma, 1999. 

1016 



latent dirichlet alloc 

M. jordan, Z. ghahramani, T. jaakkola, and L. saul. introduct to variat method for graph- 
ical models. machin learning, 37:183–233, 1999. 

R. kass and D. steffey. approxim bayesian infer in condit independ hierarch 
model (parametr empir bay models). journal of the american statist association, 84 
(407):717–726, 1989. 

M. leisink and H. kappen. gener low bound base on comput gener high order ex- 
pansions. In uncertainti in artifici intelligence, proceed of the eighteenth conference, 
2002. 

T. minka. estim a dirichlet distribution. technic report, m.i.t., 2000. 

T. P. minka and J. lafferty. expectation-propag for the gener aspect model. In uncertainti 
in artifici intellig (uai), 2002. 

C. morris. parametr empir bay inference: theori and applications. journal of the american 
statist association, 78(381):47–65, 1983. with discussion. 

K. nigam, J. lafferty, and A. mccallum. use maximum entropi for text classification. ijcai-99 
workshop on machin learn for inform filtering, page 61–67, 1999. 

K. nigam, A. mccallum, S. thrun, and T. mitchell. text classif from label and unlabel 
document use em. machin learning, 39(2/3):103–134, 2000. 

C. papadimitriou, H. tamaki, P. raghavan, and S. vempala. latent semant indexing: A proba- 
bilist analysis. page 159–168, 1998. 

A. popescul, L. ungar, D. pennock, and S. lawrence. probabilist model for unifi collabor 
and content-bas recommend in sparse-data environments. In uncertainti in artifici 
intelligence, proceed of the seventeenth conference, 2001. 

J. rennie. improv multi-class text classif with naiv bayes. technic report aitr-2001- 
004, m.i.t., 2001. 

G. ronning. maximum likelihood estim of dirichlet distributions. journal of statistc com- 
putat and simulation, 34(4):215–221, 1989. 

G. salton and M. mcgill, editors. introduct to modern inform retrieval. mcgraw-hill, 
1983. 

appendix A. infer and paramet estim 

In thi appendix, we deriv the variat infer procedur (eqs. 6 and 7) and the paramet 
maxim procedur for the condit multinomi (eq. 9) and for the dirichlet. We begin by 
deriv a use properti of the dirichlet distribution. 

1017 



blei, ng, and jordan 

a.1 comput e[log(θi |α)] 
the need to comput the expect valu of the log of a singl probabl compon under the 
dirichlet aris repeatedli in deriv the infer and paramet estim procedur for lda. 
thi valu can be easili comput from the natur parameter of the exponenti famili 
represent of the dirichlet distribution. 

recal that a distribut be in the exponenti famili if it can be write in the form: 

p(x |η) = h(x)exp{ηt T (x)−a(η)} , 
where η be the natur parameter, T (x) be the suffici statistic, and a(η) be the log of the normal- 
izat factor. 

We can write the dirichlet in thi form by exponenti the log of eq. (1): 

p(θ |α) = exp{(∑ki=1(αi−1) logθi)+ logγ(∑ki=1 αi)−∑ki=1 logγ(αi)} . 
from thi form, we immedi see that the natur paramet of the dirichlet be ηi = αi − 1 and 
the suffici statist be T (θi) = logθi. furthermore, use the gener fact that the deriv of 
the log normal factor with respect to the natur paramet be equal to the expect of the 
suffici statistic, we obtain: 

e[logθi |α] = ψ(αi)−ψ 
( 
∑kj=1 α j 

) 
where Ψ be the digamma function, the first deriv of the log gamma function. 

a.2 newton-raphson method for a hessian with special structur 

In thi section we describ a linear algorithm for the usual cubic newton-raphson optim 
method. thi method be use for maximum likelihood estim of the dirichlet distribut (ron- 
ning, 1989, minka, 2000). 

the newton-raphson optim techniqu find a stationari point of a function by iterating: 

αnew = αold−h(αold)−1g(αold) 
where h(α) and g(α) be the hessian matrix and gradient respect at the point α. In general, 
thi algorithm scale a o(n3) due to the matrix inversion. 

If the hessian matrix be of the form: 

H = diag(h)+1z1t, (10) 

where diag(h) be defin to be a diagon matrix with the element of the vector h along the diagonal, 
then we can appli the matrix invers lemma and obtain: 

h−1 = diag(h)−1− diag(h) 
−111tdiag(h)−1 

z−1 +∑kj=1 h 
−1 
j 

multipli by the gradient, we obtain the ith component: 

(h−1g)i = 
gi− c 

hi 

1018 



latent dirichlet alloc 

where 

c = 
∑kj=1 g j/h j 

z−1 +∑kj=1 h 
−1 
j 

. 

observ that thi express depend onli on the 2k valu hi and gi and thu yield a newton- 
raphson algorithm that have linear time complexity. 

a.3 variat infer 

In thi section we deriv the variat infer algorithm describ in section 5.1. recal that 
thi involv use the follow variat distribution: 

q(θ,z |γ,φ) = q(θ |γ) 
N 

∏ 
n=1 

q(zn |φn) (11) 

a a surrog for the posterior distribut p(θ,z,w |α,β), where the variat paramet γ and 
φ be set via an optim procedur that we now describe. 

follow jordan et al. (1999), we begin by bound the log likelihood of a document use 
jensen’ inequality. omit the paramet γ and φ for simplicity, we have: 

log p(w |α,β) = log 
∫ 

∑ 
z 

p(θ,z,w |α,β)dθ 

= log 
∫ 

∑ 
z 

p(θ,z,w |α,β)q(θ,z) 
q(θ,z) 

dθ 

≥ 
∫ 

∑ 
z 

q(θ,z) log p(θ,z,w |α,β)dθ− 
∫ 

∑ 
z 

q(θ,z) logq(θ,z)dθ 

= eq[log p(θ,z,w |α,β)]−eq[logq(θ,z)]. (12) 

thu we see that jensen’ inequ provid u with a low bound on the log likelihood for an 
arbitrari variat distribut q(θ,z |γ,φ). 

It can be easili verifi that the differ between the left-hand side and the right-hand side 
of the eq. (12) be the KL diverg between the variat posterior probabl and the true 
posterior probability. that is, let l(γ,φ;α,β) denot the right-hand side of eq. (12) (where we 
have restor the depend on the variat paramet γ and φ in our notation), we have: 

log p(w |α,β) = l(γ,φ;α,β)+d(q(θ,z |γ,φ) ‖ p(θ,z |w,α,β)). (13) 

thi show that maxim the low bound l(γ,φ;α,β) with respect to γ and φ be equival to 
minim the KL diverg between the variat posterior probabl and the true posterior 
probability, the optim problem present earli in eq. (5). 

We now expand the low bound by use the factor of p and q: 

l(γ,φ;α,β) = eq[log p(θ |α)]+eq[log p(z |θ)]+eq[log p(w |z,β)] 
−eq[logq(θ)]−eq[logq(z)]. 

(14) 

1019 



blei, ng, and jordan 

finally, we expand eq. (14) in term of the model paramet (α,β) and the variat paramet 
(γ,φ). each of the five line below expand one of the five term in the bound: 

l(γ,φ;α,β) = logγ 
( 
∑kj=1 α j 

)− k∑ 
i=1 

logγ(αi)+ 
k 

∑ 
i=1 

(αi−1) 
( 
ψ(γi)−ψ 

( 
∑kj=1 γ j 

)) 

+ 
N 

∑ 
n=1 

k 

∑ 
i=1 

φni 
( 
ψ(γi)−ψ 

( 
∑kj=1 γ j 

)) 

+ 
N 

∑ 
n=1 

k 

∑ 
i=1 

V 

∑ 
j=1 

φniw jn logβi j 

− logγ(∑kj=1 γ j)+ k∑ 
i=1 

logγ(γi)− 
k 

∑ 
i=1 

(γi−1) 
( 
ψ(γi)−ψ 

( 
∑kj=1 γ j 

)) 

− 
N 

∑ 
n=1 

k 

∑ 
i=1 

φni logφni, 

(15) 

where we have make use of eq. (8). 

In the follow two sections, we show how to maxim thi low bound with respect to the 
variat paramet φ and γ. 

a.3.1 variat multinomi 

We first maxim eq. (15) with respect to φni, the probabl that the nth word be gener by 
latent topic i. observ that thi be a constrain maxim sinc ∑ki=1 φni = 1. 

We form the lagrangian by isol the term which contain φni and add the appropri 
lagrang multipliers. let βiv be p(wvn = 1 |zi = 1) for the appropri v. (recal that each wn be 
a vector of size V with exactli one compon equal to one; we can select the uniqu v such that 
wvn = 1): 

l[φni] = φni 
( 
ψ(γi)−ψ 

( 
∑kj=1 γ j 

)) 
+φni logβiv−φni logφni +λn 

( 
∑kj=1 φni−1 

) 
, 

where we have drop the argument of L for simplicity, and where the subscript φni denot that 
we have retain onli those term in L that be a function of φni. take deriv with respect to 
φni, we obtain: 

∂L 
∂φni 

= ψ(γi)−ψ 
( 
∑kj=1 γ j 

) 
+ logβiv− logφni−1+λ. 

set thi deriv to zero yield the maxim valu of the variat paramet φni (cf. eq. 6): 

φni ∝ βiv exp 
( 
ψ(γi)−ψ 

( 
∑kj=1 γ j 

)) 
. (16) 

1020 



latent dirichlet alloc 

a.3.2 variat dirichlet 

next, we maxim eq. (15) with respect to γi, the ith compon of the posterior dirichlet param- 
eter. the term contain γi are: 

l[γ] = 
k 

∑ 
i=1 

(αi−1) 
( 
ψ(γi)−ψ 

( 
∑kj=1 γ j 

)) 
+ 

N 

∑ 
n=1 

φni 
( 
ψ(γi)−ψ 

( 
∑kj=1 γ j 

)) 

− logγ(∑kj=1 γ j)+ logγ(γi)− k∑ 
i=1 

(γi−1) 
( 
ψ(γi)−ψ 

( 
∑kj=1 γ j 

)) 
. 

thi simplifi to: 

l[γ] = 
k 

∑ 
i=1 

( 
ψ(γi)−ψ 

( 
∑kj=1 γ j 

))( 
αi +∑nn=1 φni− γi 

)− logγ(∑kj=1 γ j)+ logγ(γi). 
We take the deriv with respect to γi: 

∂L 
∂γi 

= ψ′(γi) 
( 
αi +∑nn=1 φni− γi 

)−ψ′ (∑kj=1 γ j) k∑ 
j=1 

( 
α j +∑nn=1 φn j − γ j 

) 
. 

set thi equat to zero yield a maximum at: 

γi = αi +∑nn=1 φni. (17) 

sinc eq. (17) depend on the variat multinomi φ, full variat infer requir 
altern between eqs. (16) and (17) until the bound converges. 

a.4 paramet estim 

In thi final section, we consid the problem of obtain empir bay estim of the model 
paramet α and β. We solv thi problem by use the variat low bound a a surrog 
for the (intractable) margin log likelihood, with the variat paramet φ and γ fix to the 
valu found by variat inference. We then obtain (approximate) empir bay estim by 
maxim thi low bound with respect to the model parameters. 

We have thu far consid the log likelihood for a singl document. given our assumpt 
of exchang for the documents, the overal log likelihood of a corpu D = {w1,w2, . . . ,wm} 
be the sum of the log likelihood for individu documents; moreover, the overal variat low 
bound be the sum of the individu variat bounds. In the remaind of thi section, we abus 
notat by use L for the total variat bound, index the document-specif term in the 
individu bound by d, and sum over all the documents. 

recal from section 5.3 that our overal approach to find empir bay estim be base 
on a variat EM procedure. In the variat e-step, discuss in appendix a.3, we maxim 
the bound l(γ,φ;α,β) with respect to the variat paramet γ and φ. In the m-step, which we 
describ in thi section, we maxim the bound with respect to the model paramet α and β. the 
overal procedur can thu be view a coordin ascent in L . 

1021 



blei, ng, and jordan 

a.4.1 condit multinomi 

To maxim with respect to β, we isol term and add lagrang multipliers: 

l[β] = 
M 

∑ 
d=1 

Nd 

∑ 
n=1 

k 

∑ 
i=1 

V 

∑ 
j=1 

φdniw 
j 
dn logβi j + 

k 

∑ 
i=1 

λi 
( 

∑vj=1 βi j −1 
) 

. 

We take the deriv with respect to βi j, set it to zero, and find: 

βi j ∝ 
M 

∑ 
d=1 

Nd 

∑ 
n=1 

φdniw 
j 
dn. 

a.4.2 dirichlet 

the term which contain α are: 

l[α] = 
M 

∑ 
d=1 

( 
logγ 

( 
∑kj=1 α j 

)− k∑ 
i=1 

logγ(αi)+ 
k 

∑ 
i=1 

( 
(αi−1) 

( 
ψ(γdi)−ψ 

( 
∑kj=1 γd j 

)))) 

take the deriv with respect to αi gives: 

∂L 
∂αi 

= M 
( 
Ψ 
( 
∑kj=1 α j 

)−ψ(αi))+ M∑ 
d=1 

( 
ψ(γdi)−ψ 

( 
∑kj=1 γd j 

)) 

thi deriv depend on α j, where j 6= i, and we therefor must use an iter method to find 
the maxim α. In particular, the hessian be in the form found in eq. (10): 

∂L 
∂αiα j 

= δ(i, j)mψ′(αi)−ψ′ 
( 
∑kj=1 α j 

) 
, 

and thu we can invok the linear-tim newton-raphson algorithm describ in appendix a.2. 
finally, note that we can use the same algorithm to find an empir bay point estim of η, 

the scalar paramet for the exchang dirichlet in the smooth lda model in section 5.4. 

1022 


