


















































learn TO reinforc learn 

JX wang1, Z kurth-nelson1, D tirumala1, H soyer1, JZ leibo1, 
R munos1, C blundell1, D kumaran1,3, M botvinick1,2 
1deepmind, london, UK 
2gatsbi comput neurosci unit, ucl, london, UK 
3institut of cognit neuroscience, ucl, london, UK 

{wangjane, zebk, dhruvat, soyer, jzl, munos, cblundell, 

dkumaran, botvinick} @google.com 

abstract 

In recent year deep reinforc learn (rl) system have attain superhuman 
perform in a number of challeng task domains. however, a major limit 
of such applic be their demand for massiv amount of train data. A critic 
present object be thu to develop deep RL method that can adapt rapidli to new 
tasks. In the present work we introduc a novel approach to thi challenge, which 
we refer to a deep meta-reinforc learning. previou work have show that 
recurr network can support meta-learn in a fulli supervis context. We 
extend thi approach to the RL setting. what emerg be a system that be train 
use one RL algorithm, but whose recurr dynam implement a second, quit 
separ RL procedure. thi second, learn RL algorithm can differ from the 
origin one in arbitrari ways. importantly, becaus it be learned, it be configur 
to exploit structur in the train domain. We unpack these point in a seri of 
seven proof-of-concept experiments, each of which examin a key aspect of deep 
meta-rl. We consid prospect for extend and scale up the approach, and 
also point out some potenti import implic for neuroscience. 

1 introduct 

recent advanc have allow long-stand method for reinforc learn (rl) to be newli 
extend to such complex and large-scal task environ a atari (mnih et al., 2015) and Go 
(silver et al., 2016). the key enabl breakthrough have be the develop of techniqu allow 
the stabl integr of RL with non-linear function approxim through deep learn (lecun 
et al., 2015; mnih et al., 2015). the result deep RL method be attain human- and often 
superhuman-level perform in an expand list of domain (jaderberg et al., 2016; mnih et al., 
2015; silver et al., 2016). however, there be at least two aspect of human perform that they 
starkli lack. first, deep RL typic requir a massiv volum of train data, wherea human 
learner can attain reason perform on ani of a wide rang of task with compar littl 
experience. second, deep RL system typic special on one restrict task domain, wherea 
human learner can flexibl adapt to chang task conditions. recent critiqu (e.g., lake et al., 
2016) have invok these differ a pose a direct challeng to current deep RL research. 

In the present work, we outlin a framework for meet these challenges, which we refer to a 
deep meta-reinforc learning, a label that be intend to both link it with and distinguish it 
from previou work employ the term “meta-reinforc learning” (e.g. schmidhub et al., 
1996; schweighof and doya, 2003, discuss later). the key concept be to use standard deep RL 
techniqu to train a recurr neural network in such a way that the recurr network come to 
implement it own, free-stand RL procedure. As we shall illustrate, under the right circumstances, 
the secondari learn RL procedur can display an adapt and sampl effici that the 
origin RL procedur lacks. 

the follow section review previou work employ recurr neural network in the context of 
meta-learn and describ the gener approach for extend such method to the RL setting. We 

1 

ar 
X 

iv 
:1 

61 
1. 

05 
76 

3v 
3 

[ 
c 

.L 
G 

] 
2 

3 
Ja 

n 
20 

17 



then present seven proof-of-concept experiments, each of which highlight an import ramif 
of the deep meta-rl setup by character agent perform in light of thi framework. We close 
with a discuss of key challeng for next-step research, a well a some potenti implic for 
neuroscience. 

2 method 

2.1 background: meta-learn IN recurr neural network 

flexible, data-effici learn natur requir the oper of prior biases. In gener terms, 
such bia can deriv from two sources; they can either be engin into the learn system (as, 
for example, in convolut networks), or they can themselv be acquir through learning. the 
second case have be explor in the machin learn literatur under the rubric of meta-learn 
(schmidhub et al., 1996; thrun and pratt, 1998). 

In one standard setup, the learn agent be confront with a seri of task that differ from one 
anoth but also share some underli set of regularities. meta-learn be then defin a an 
effect wherebi the agent improv it perform in each new task more rapidly, on average, than 
in past task (thrun and pratt, 1998). At an architectur level, meta-learn have gener be 
conceptu a involv two learn systems: one lower-level system that learn rel 
quickly, and which be primarili respons for adapt to each new task; and a slow higher-level 
system that work across task to tune and improv the lower-level system. 

A varieti of method have be pursu to implement thi basic meta-learn setup, both within 
the deep learn commun and beyond (thrun and pratt, 1998). Of particular relev here be 
an approach introduc by hochreit and colleagu (hochreit et al., 2001), in which a recurr 
neural network be train on a seri of interrel task use standard backpropagation. A critic 
aspect of their setup be that the network receives, on each step within a task, an auxiliari input 
indic the target output for the preced step. for example, in a regress task, on each step 
the network receiv a input an x valu for which it be desir to output the correspond y, but the 
network also receiv an input disclos the target y valu for the preced step (see hochreit 
et al., 2001; santoro et al., 2016). In thi scenario, a differ function be use to gener the data 
in each train episode, but if the function be all drawn from a singl parametr family, then the 
system gradual tune into thi consist structure, converg on accur output more and more 
rapidli across episodes. 

one interest aspect of hochreiter’ method be that the process that underli learn within each 
new task inher entir in the dynam of the recurr network, rather than in the backpropag 
procedur use to tune that network’ weights. indeed, after an initi train period, the network 
can improv it perform on new task even if the weight be held constant (see also cotter 
and conwell, 1990; prokhorov et al., 2002; younger et al., 1999). A second import aspect of the 
approach be that the learn procedur implement in the recurr network be fit to the structur 
that span the famili of task on which the network be trained, emb bia that allow it to learn 
effici when deal with task from that family. 

2.2 deep meta-rl: definit and key featur 

importantly, hochreiter’ origin work (hochreit et al., 2001), a well a it subsequ extens 
(cotter and conwell, 1990; prokhorov et al., 2002; santoro et al., 2016; younger et al., 1999) onli 
address supervis learn (i.e. the auxiliari input provid on each step explicitli indic the 
target output on the previou step, and the network be train use explicit targets). In the present 
work we consid the implic of appli the same approach in the context of reinforc 
learning. here, the task that make up the train seri be interrel RL problems, for example, 
a seri of bandit problem vari onli in their parameterization. rather than present target 
output a auxiliari inputs, the agent receiv input indic the action output on the previou step 
and, critically, the quantiti of reward result from that action. the same reward inform be fed 
in parallel to a deep RL procedure, which tune the weight of the recurr network. 

It be thi setup, a well a it result, that we refer to a deep meta-rl (although from here on, for 
brevity, we will often simpli call it meta-rl, with apolog to author who have use that term 

2 



previously). As in the supervis case, when the approach be successful, the dynam of the recurr 
network come to implement a learn algorithm entir separ from the one use to train the 
network weights. onc again, after suffici training, learn can occur within each task even if the 
weight be held constant. however, here the procedur the recurr network implement be itself a 
full-fledg reinforc learn algorithm, which negoti the exploration-exploit tradeoff 
and improv the agent’ polici base on reward outcomes. A key point, which we will emphas in 
what follows, be that thi learn RL procedur can differ starkli from the algorithm use to train the 
network’ weights. In particular, it polici updat procedur (includ featur such a the effect 
learn rate of that procedure), can differ dramat from those involv in tune the network 
weights, and the learn RL procedur can implement it own approach to exploration. critically, a 
in the supervis case, the learn RL procedur will be fit to the statist span the multi-task 
environment, allow it to adapt rapidli to new task instances. 

2.3 formal 

let u write a D a distribut (the prior) over markov decis process (mdps). We want to 
demonstr that meta-rl be abl to learn a prior-depend RL algorithm, in the sens that it will 
perform well on averag on mdp drawn from D or slight modif of D. An appropri 
structur agent, emb a recurr neural network, be train by interact with a sequenc of 
mdp environ (also call tasks) through episodes. At the start of a new episode, a new mdp 
task m ∼ D and an initi state for thi task be sampled, and the intern state of the agent (i.e., the 
pattern of activ over it recurr units) be reset. the agent then execut it action-select 
strategi in thi environ for a certain number of discret time-steps. At each step t an action 
at ∈ A be execut a a function of the whole histori Ht = {x0, a0, r0, . . . , xt−1, at−1, rt−1, xt} 
of the agent interact in the mdp m dure the current episod (set of state {xs}0≤s≤t, action 
{as}0≤s<t, and reward {rs}0≤s<t observ sinc the begin of the episode, when the recurr 
unit be reset). the network weight be train to maxim the sum of observ reward over all 
step and episodes. 

after training, the agent’ polici be fix (i.e. the weight be frozen, but the activ be chang 
due to input from the environ and the hidden state of the recurr layer), and it be evalu 
on a set of mdp that be drawn either from the same distribut D or slight modif of that 
distribut (to test the gener capac of the agent). the intern state be reset at the begin 
of the evalu of ani new episode. sinc the polici learn by the agent be history-depend (a it 
make us of a recurr network), when expos to ani new mdp environment, it be abl to adapt 
and deploy a strategi that optim reward for that task. 

3 experi 

In order to evalu the approach to learn that we have just described, we conduct a seri of 
six proof-of-concept experiments, which we present here along with a seventh experi origin 
report in a relat paper (mirowski et al., 2016). one particular point of interest in these experi 
be to see whether meta-rl could be use to learn an adapt balanc between explor and 
exploitation, a demand of ani fully-fledg RL procedure. A second and still more import 
focu be on the question of whether meta-rl can give rise to learn that gain effici by 
capit on task structure. 

In order to examin these questions, we perform four experi focu on bandit task and two 
addit experi focu on markov decis problems. all of our experi (a well a the 
addit experi we report) employ a common set of methods, with minor implement 
variations. In all experiments, the agent architectur center on a recurr neural network (lstm; 
hochreit and schmidhuber, 1997) feed into a soft-max output repres discret actions. 
As detail below, the paramet of thi network core, a well a some other architectur details, 
vari across experi (see figur 1 and tabl 1). however, it be import to emphas that 
comparison between specif architectur be outsid the scope of thi paper. our main aim be to 
illustr and valid the meta-rl framework in a more gener way. To thi end, all experi 
use the high-level task setup previous described: both train and test be organ into 
fixed-length episodes, each involv a task randomli sampl from a predetermin task distribution, 
with the lstm hidden state initi at the begin of each episode. task-specif input and 

3 



paramet exps. 1 & 2 exp. 3 exp. 4 exp. 5 exp. 6 
no. thread 1 1 1 1 32 
no. lstm 1 1 1 1 2 
no. hidden 48 48 48 48 256/64 
step unrol 100 5 150 20 100 
βe anneal anneal anneal 0.05 0.001 
βv 0.05 0.05 0.05 0.05 0.4 
learn rate tune 0.001 0.001 tune tune 
discount factor tune 0.8 0.8 tune tune 
input a, r, t a, r, t a, r, t a, r, t, x a, r, x 
observ n/a n/a n/a 1-hot rgb (84x84) 
no. trials/episod 100 5 150 10 10 
episod length 100 5 150 20 ≤3600 

tabl 1: list of hyperparameters. βe = coeffici of entropi regular loss; in exps. 1-4, βe be anneal 
from 1.0 to 0.0 over the cours of training. βv = coeffici of valu function loss (mirowski et al., 2016). r = 
reward, a = last action, t = current time step, x = current observation. exp. 1: bandit with independ arm 
(section 3.1.1); exp. 2: bandit with depend arm I (section 3.1.2); exp. 3: bandit with depend arm II 
(section 3.1.3); exp. 4: restless bandit (section 3.1.4); exp. 5: the “two-step task” (section 3.2.1); exp. 6: 
learn abstract task structur (section 3.2.2). 

action output be describ in conjunct with individu experiments. In all experi except 
where specified, the input includ a scalar indic the reward receiv on the preced time-step 
a well a a one-hot represent of the action sampl on that time-step. 

all reinforc learn be conduct use the advantag actor-crit algorithm, a detail 
in mnih et al. (2016) and mirowski et al. (2016) (see also figur 1). detail of training, includ 
the use of entropi regular and a combin polici and valu estim loss, close follow the 
method detail in mirowski et al. (2016), with the except that our experi use a singl 
thread unless otherwis noted. for a full list of paramet refer to tabl 1. 

figur 1: advantag actor-crit with recurrence. In all architectures, reward and last action be addit input 
to the lstm. for non-bandit environments, observ be also fed into the lstm either a a one-hot or pass 
through an encod model [3-layer encoder: two convolut layer (first layer: 16 8x8 filter appli with 
stride 4, second layer: 32 4x4 filter with stride 2) follow by a fulli connect layer with 256 unit and then a 
relu non-linearity. see for detail mirowski et al. (2016)]. for bandit experiments, current time step be also 
fed in a input. π = policy; v = valu function. a3c be the distribut multi-thread asynchron version 
of the advantag actor-crit algorithm (mnih et al., 2016); a2c be singl threaded. (a) architectur use in 
experi 1-5. (b) convolutional-lstm architectur use in experi 6. (c) stack lstm architectur 
with convolut encod use in experi 6 and 7. 

4 



3.1 bandit problem 

As an initi set for evalu meta-rl, we studi a seri of bandit problems. except for a veri 
limit set of bandit environments, it be intract to comput the (prior-dependent) bayesian-optim 
strategy. here we demonstr that a recurr system train on a set of bandit environ drawn 
i.i.d. from a give distribut of environ produc a bandit algorithm which perform well on 
problem drawn from that distribution, and to a certain extent gener to relat distributions. 
thus, meta-rl learn a prior-depend bandit algorithm. 

the specif bandit instanti of the gener meta-rl procedur describ in section 2.3 be defin 
a follows. let D be a train distribut over bandit environments. the meta-rl system be train 
on a sequenc of bandit environ through episodes. At the start of a new episode, it lstm state 
be reset and a bandit task b ∼ D be sampled. A bandit task be defin a a set of distribut – one for 
each arm – from which reward be sampled. the agent play in thi bandit environ for a certain 
number of trial and be train to maxim observ rewards. after training, the agent’ polici be 
evalu on a set of bandit task that be drawn from a test distribut d′, which can either be the 
same a D or a slight modif of it. 
We evalu the result perform of the learn bandit algorithm by the cumul regret, 
a measur of the loss (in expect rewards) suffer when play sub-optim arms. write 
µa(b) the expect reward of arm a in bandit environ b, and µ∗(b) = maxa µa(b) = µa∗(b)(b) 
(where a∗(b) be one optim arm) the optim expect reward, we defin the cumul regret (in 
environ b) a RT (b) = 

∑T 
t=1 µ 

∗(b)− µat(b), where at be the arm (action) chosen at time t. In 
experi 4 (restless bandits; section 3.1.4), µ∗ also depend on t. We report the perform 
(averag over bandit environ drawn from the test distribution) either in term of the cumul 
regret: eb∼d′ [rt (b)] or in term of number of sub-optim pulls: eb∼d′ [ 

∑T 
t=1 i{at 6= a∗(b)}]. 

3.1.1 bandit with independ arm 

We first consid a simpl two-arm bandit task to examin the behavior of meta-rl under condit 
where theoret guarante exist and gener purpos algorithm apply. the arm distribut be 
independ bernoulli distribut (reward be 1 with probabl p and 0 with probabl 1− p), 
where the paramet of each arm (p1 and p2) be sampl independ and uniformli over [0, 1]. 
We denot by Di the correspond distribut over these independ bandit environ (where 
the subscript i stand for independ arms). 

At the begin of each episode, a new bandit task be sampl and held constant for 100 trials. 
train last for 20,000 episodes. the network be give a input the last reward, last action taken, 
and the trial number t, subsequ produc the action for the next trial t+ 1 (figur 1). after 
training, we evalu on 300 new episod with the learn rate set to zero (the learn polici be 
fixed). 

across model instances, we randomli sampl learn rate and discount, follow mnih et al. (2016). 
for all figures, we plot the averag of the top 5 run of 100 randomli sampl hyperparamet 
settings, where the top agent be select from the first half of the 300 evalu episod and 
perform be plot for the second half. We measur the cumul expect regret across the 
episode, compar with sever algorithm tailor for thi independ bandit setting: gittin index 
(gittins, 1979) (which be bayesian optim in the finite-horizon case), ucb (auer et al., 2002) (which 
come with theoret finite-tim regret guarantees), and thompson sampl (thompson, 1933) 
(which be asymptot optim in thi setting: see kaufmann et al., 2012b). model simul 
be conduct with the pymabandit toolbox from (kaufmann et al., 2012a) and custom matlab 
scripts. 

As show in figur 2a (green line; “independent”), meta-rl outperform both thompson sampl 
(gray dash line) and ucb (light gray dash line), although it perform less well compar to 
gittin (black dash line). To verifi the critic import of provid reward inform to the 
lstm, we remov thi input, leav all other input a before. As expected, perform be at 
chanc level on all bandit tasks. 

5 



Ep 
be 

od 
e 

300 

0 

sub-optim arm pull 

0 100 

(a) 

Tr 
ai 

ni 
ng 

C 
on 

di 
tio 

n 

test condit 

cumul regret 

indep. 

unif. 

easi 

med. 

hard 

indep. unif. easi med. hard 

(b) 

(c) (d) 

(e) 

trial # 

(f ) 

trial # 
20 40 60 80 100 

1 

2 

3 

0 

testing: depend uniform 

Cu 
m 

ul 
at 

iv 
e 

Re 
gr 

et 

ucb 

lstm a2c “depend uniform” 

thompson 
gittin 

trial # 
20 40 60 80 100 

1 

2 

3 

0 

testing: independ 

Cu 
m 

ul 
at 

iv 
e 

Re 
gr 

et 

ucb 

lstm a2c “independent” 

thompson 
gittin 

trial # 
20 40 60 80 100 

1 

2 

3 

0 

testing: hard 

Cu 
m 

ul 
at 

iv 
e 

Re 
gr 

et 

4 

ucb 

lstm a2c “medium” 

thompson 
gittin 

trial # 
20 40 60 80 100 

1 

2 

3 

0 

testing: easi 

Cu 
m 

ul 
at 

iv 
e 

Re 
gr 

et 

ucb 

lstm a2c “medium” 

thompson 
gittin 

figur 2: perform on independent- and correlated-arm bandits. We report perform a the cumul 
expect regret RT for 150 test episodes, averag over the top 5 hyperparamet for each agent-task con- 
figuration, where the top 5 be determin base on perform on a separ set of 150 test episodes. (a) 
lstm a2c train and evalu on bandit with independ arm (distribut di; see text), and compar 
with theoret optim models. (b) A singl agent play the medium difficulti task with distribut dm. 
suboptim arm pull over trial be depict for 300 episodes. (c) lstm a2c train and evalu on bandit 
with depend uniform arm (distribut du), (d) train on medium bandit task (dm) and test on easi 
(de), and (e) train on medium (dm) and test on hard task (dh). (f) cumul regret for all possibl 
combin of train and test environ (di, du, de, dm, dh). 

3.1.2 bandit with depend arm (i) 

As we have emphasized, a key properti of meta-rl be that it give rise to a learn RL algorithm that 
exploit consist structur in the train distribution. In order to garner empir evid for thi 
point, we test the agent from our first experi in a more structur bandit task. specifically, 
we train the system on two-arm bandit in which arm reward distribut be correlated. In 
thi setting, unlik the one studi in the previou section, experi with either arm provid 
inform about the other. standard bandit algorithms, includ ucb and thompson sampling, 
perform suboptim in thi setting, a they be not design to exploit such correlations. In some 
case it be possibl to tailor algorithm for specif arm structur (see for exampl lattimor and 
munos, 2014), but extens problem-specif analysi be typic required. our approach aim to 
learn a structure-depend bandit algorithm directli from experi with the target bandit domain. 

We consid bernoulli distribut where the paramet (p1, p2) of the two arm be correl 
in the sens that p1 = 1 − p2. We consid sever train and test distributions. the uniform 
mean that p1 ∼ u([0, 1]) (uniform distribut over the unit interval). the easi mean that 
p1 ∼ u({0.1, 0.9}) (uniform distribut over those two possibl values), and similarli we call 
medium when p1 ∼ u({0.25, 0.75}) and hard when p1 ∼ u({0.4, 0.6}). We denot by du, 
de, dm, and Dh the correspond induc distribut over bandit environments. In addit 

6 



we also consid the independ uniform distribut (a in the previou section, di) where 
p1, p2 ∼ u([0, 1]) independently. agent be both train and test on those five distribut over 
bandit environ (among which four correspond to correl distributions: du, de, Dm and dh; 
and one to the independ case: di). As a valid of the name give to the task distribut 
(de, dm, dh), result show that the easi task be easi to learn than the medium which itself be easi 
than the hard one (figur 2f). thi be compat with the gener notion that the hard of a bandit 
problem be invers proport to the differ between the expect reward of the optim and 
sub-optim arms. We again note that withhold the reward input to the lstm result in chanc 
perform on even the easi bandit task, a should be expected. 

figur 2f report the result of all possibl training-test regimes. from observ the cumul 
expect regrets, we make the follow observations: i) agent train in structur environ 
(du,de,dm, anddh) develop prior knowledg that can be use effect when test on structur 
distribut – perform compar to gittin (figur 2c-f), and superiorli compar to agent 
train on independ arm (di) in all structur task at test (figur 2f). thi be becaus an agent 
train on independ reward (di) have not learn to exploit the reward correl that be use 
in those structur tasks. ii) conversely, previou train on ani structur distribut (du, de, 
dm, or dh) hurt perform when agent be test on an independ distribut (di; figur 2f). 
thi make sense, a train on correl arm may produc a polici that reli on specif reward 
structure, therebi impact perform in problem where no such structur exists. iii) whilst 
the previou result emphas the point that meta-rl give rise to a separ learnt RL algorithm 
that implement prior-depend bandit strategies, result also provid evid that there be some 
gener beyond the exact train distribut encount (figur 2f). for example, agent 
train on the distribut De and Dm perform well when test over a much wider structur 
distribut (i.e. du). further, our evid suggest that there be gener from train 
on the easi task (de,dm) to test on the hardest task (dh; figur 2e), with similar or even 
margin superior perform a compar to train on the hard distribut Dh itself(figur 
2f). In contrast, train on the hard distribut Dh result in rel poor gener to other 
structur distribut (du, de, dm), suggest that train pure on hard instanc may result in 
a learn RL algorithm that be more constrain by prior knowledge, perhap due to the difficulti of 
solv the origin problem. 

3.1.3 bandit with depend arm (ii) 

In the previou experiment, the agent could outperform standard bandit algorithm by make use 
of learn depend between arms. however, it could do thi while alway choos what it 
believ to be the highest-pay arm. We next examin a problem where inform can be gain 
by pay a short-term reward cost. similar problem have be examin befor a provid a 
challeng to standard bandit algorithm (see e.g. russo and van roy, 2014). In contrast, human and 
anim make decis that sacrific immedi reward for inform gain (e.g. bromberg-martin 
and hikosaka, 2009). 

In thi experiment, the agent be train on 11-arm bandit with strong depend between 
arms. all arm have determinist payouts. nine “non-target” arm have reward = 1, and one “target” 
arm have reward = 5. meanwhile, arm a11 be alway “informative”, in that the target arm be 
index by 10 time a11’ reward (e.g. a reward of 0.2 on a11 indic that a2 be the target arm). 
thus, a11’ payout rang from 0.1 to 1. In each episode, the index of the target arm be randomli 
assigned. On the first trial of each episode, the agent could not know which arm be the target, so the 
inform arm return expect reward 0.55 and everi target arm return expect reward 1.4. 
choos the inform arm thu meant forego immedi reward, but with the compens 
of valuabl information. episod be five step long. again, the reward on the previou trial be 
provid a an addit observ to the agent. To facilit learning, thi be encod in 1-hot 
format. 

result be show in figur 3. the agent learn the optim long-run strategi of sampl the 
inform arm once, despit the short-term cost, and then use the result inform to exploit 
the high-valu target arm. thompson sampling, if suppli the true prior, search potenti target 
arm and exploit the target if found. ucb perform bad becaus it sampl everi arm onc 
even if the target arm be found early. 

7 



figur 3: learn RL procedur pay immedi cost to gain inform to improv long-run returns. In thi task, 
one arm be lower-pay but provid perfect inform about which of the other ten arm be highest-paying. 
the remain nine arm be intermedi in reward. the index of the inform arm be fix between episodes, 
but the index of the highest-pay arm be random between episodes. On the first trial, the train agent 
sampl the inform arm. On subsequ trials, the agent us the inform it gain to determinist 
exploit the highest-pay arm. thompson sampl and ucb be not abl to take advantag of the depend 
between arms. 

3.1.4 restless bandit 

In previou experi we consid stationari problem where the agent’ action yield in- 
format about task paramet that remain fix throughout each episode. next, we consid a 
bandit problem in which reward probabl chang over the cours of an episode, with differ 
rate of chang (volatilities) in differ episodes. To perform well, the agent must not onli track 
the best arm, but also infer the volatil of the episod and adjust it own learn rate accordingly. 
In such an environment, learn rate should be high when the environ be chang rapidly, 
becaus past inform becom irrelev more quickli (behren et al., 2007; sutton and barto, 
1998). 

We test whether meta-rl would learn such a flexibl RL polici use a two-arm bernoulli bandit 
task with reward probabl p1 and 1-p1. the valu of p1 chang slowli in “low vol” episod 
and quickli in “high vol” episodes. the agent have no way of know which type of episod it 
be in, except for it reward histori within the episode. figur 4a show exampl “low vol” and 
“high vol” episodes. reward magnitud be fix at 1, and episod be 100 step long. ucb and 

thompson sampl be again implement for comparison. the confid bound term 
√ 

χ logn 
ni 

in ucb have paramet χ which be set to 1, select empir for good perform on our data 
set. thompson sampling’ posterior updat includ knowledg of the gaussian random walk, but 
with a fix volatil for all episodes. 

As in the previou experiment, meta-rl achiev low regret in test than thompson sampling, 
ucb, or the rescorla-wagn (r-w) learn rule (figur 4b; rescorla et al., 1972) with the best 
fix learn rate (α=0.5). To test whether the agent adjust it effect learn rate to match 
environ with differ volatil levels, we fit r-w model to the agent’ behavior, concaten 
episod into block of 10, where each block consist of onli “low vol” or onli “high vol” episodes. 
We consid four differ model encompass differ combin of three parameters: 
learn rate α, softmax invers temperatur β, and a laps rate � to account for unexplain choic 
varianc not relat to estim valu economid et al. (2015). model “b” includ onli β, “ab” 
includ α and β, “be” includ β and �, and “abe” includ all three. all paramet be estim 
separ on each block of 10 episodes. In model where � and α be not free, they be fix 
to 0 and 0.5, respectively. model comparison by bayesian inform criterion (bic) indic 
that meta-rl’ behavior be good describ by a model with differ learn rate for each block 
than a model with a fix learn rate across blocks. As a control, we perform the same model 
comparison on the behavior produc by the best r-w agent, find no benefit of allow differ 
learn rate across episod (model “abe” and “ab” v “be” and “b”; figur 4c-d). In these models, 
the paramet estim for meta-rl’ behavior be strongli relat to the volatil of the episodes, 
indic that meta-rl adjust it learn rate to the volatil of the episode, wherea model 
fit the r-w behavior simpli recov the fix paramet (figur 4e-f). 

8 



figur 4: learn RL procedur adapt it own learn rate to the environment. (a) agent be train on 
two-arm bandit with perfectli anti-correl bernoulli reward probabilities, p1 and 1-p1. two exampl 
episod be shown. p1 chang within an episod (solid black line), with a fast poisson jump rate in “high vol” 
episod and a slow rate in “low vol” episodes. (b) the train lstm agent outperform ucb, thompson 
sampling, and a rescorla-wagn (r-w) learner with fix learn rate α=0.5 (select for be optim on 
averag in thi distribut of environments). (c,d) We fit r-w model by maximum likelihood both to the 
behavior of r-w (a a control) and to the behavior of lstm. model includ a learn rate that could vari 
between episod (“ab” and “abe”) outperform model without these free paramet on lstm’ data, but not 
on r-w’ data. addit of a laps paramet further improv model fit on lstm’ data (“be” and “abe”), 
suggest that the algorithm implement by lstm be not exactli rescorla-wagner. (e,f) the lstm’s, but not 
r-w’s, estim learn rate be high in volatil episodes. small jitter add to visual overlap points. 

3.2 markov decis problem 

the forego experi focu on bandit task in which action do not affect the task’ underli 
state. We turn now to mdp where action do influenc state. We begin with a task deriv from the 
neurosci literatur and then turn to a task, origin studi in the context of anim learning, 
which requir learn of abstract task structure. As in the previou experiments, our focu be 
on examin how meta-rl adapt to invari in task structure. We wrap up by review an 
experi recent report in a relat paper (mirowski et al., 2016), which demonstr how 
meta-rl can scale to large-scal navig task with rich visual inputs. 

3.2.1 the “two-step task” 

here we examin meta-rl in a set that have be wide use in the neurosci literatur to 
distinguish the contribut of differ system view to support decis make (daw et al., 
2005). specifically, thi paradigm – know a the “two-step task” (daw et al., 2011) – be develop 
to dissoci a model-fre system that cach valu of action in state (e.g. td(1) q-learning; 
see sutton and barto, 1998), from a model-bas system which learn an intern model of the 
environ and evalu the valu of action at the time of decision-mak through look-ahead 
plan (daw et al., 2005). our interest be in whether meta-rl would give rise to behavior 
emul a model-bas strategy, despit the use of a model-fre algorithm (in thi case a2c) to 
train the system weights. 

9 



We use a modifi version of the two-step task, design to bolster the util of model-bas over 
model-fre control (see kool et al., 2016). the task’ structur be diagram in figur 5a. from the 
first-stag state s1, action a1 lead to second-stag state S2 and S3 with probabl 0.75 and 0.25, 
respectively, while action a2 lead to S2 and S3 with probabl 0.25 and 0.75. one second-stag 
state yield a reward of 1.0 with probabl 0.9 (and otherwis zero); the other yield the same 
reward with probabl 0.1. the ident of the higher-valu state be assign randomli for each 
episode. thus, the expect valu for the two first-stag action be either ra = 0.9 and rb = 0.1, or 
ra = 0.1 and rb = 0.9. all three state be repres by one-hot vectors, with the transit model 
held constant across episodes: i.e. onli the expect valu of the second stage state chang from 
episod to episode. 

We appli the convent analysi use in the neurosci literatur to dissoci model-fre 
from model-bas control (daw et al., 2011). thi focu on the “stay probability,” that is, the 
probabl with which a first-stag action be select at trial t+ 1 follow a second-stag reward 
at trial t, a a function of whether trial t involv a common transit (e.g. action a1 at state S1 
lead to s2) or rare transit (action a2 at state S1 lead to s3). under the standard interpret (see 
daw et al., 2011), model-fre control – à la td(1) – predict that there should be a main effect of 
reward: first-stag action will tend to be repeat if follow by reward, regardless of transit 
type, and such action will tend not to be repeat (choic switch) if follow by non-reward (figur 
5b). In contrast, model-bas control predict an interact between the reward and transit type, 
reflect a more goal-direct strategy, which take the transit structur into account. intuitively, 
if you receiv a second-stag reward (e.g. at s2) follow a rare transit (i.e. have take action 
a2 at state s1), to maxim your chanc of get to thi reward on the next trial base on your 
knowledg of the transit structure, the optim first stage action be a1 (i.e. switch). 

the result of the stay-prob analysi perform on the agent’ choic show a pattern conven- 
tional interpret a impli the oper of model-bas control (figur 5c). As in previou 
experiments, when reward inform be withheld at the level of network input, perform be 
at chanc levels. 

If interpret follow standard practic in neuroscience, the behavior of the model in thi experi 
reflect a surpris effect: train with model-fre RL give rise to behavior reflect model-bas 
control. We hasten to note that differ interpret of the observ pattern of behavior be 
avail (akam et al., 2015), a point to which we will return below. however, notwithstand thi 
caveat, the result of the present experi provid a further illustr of the point that the learn 
procedur that emerg from meta-rl can differ starkli from the origin RL algorithm use to train 
the network weights, and take a form that exploit consist task structure. 

3.2.2 learn abstract task structur 

In the final experi we conducted, we take a step toward examin the scalabilti of meta-rl, by 
studi a task that involv rich visual inputs, longer time horizon and spars rewards. additionally, 
in thi experi we studi a meta-learn task that requir the system to tune into an abstract 
task structure, in which a seri of object play defin role which the system must infer. 

the task be adapt from a classic studi of anim behavior, conduct by harlow (1949). On each 
trial in the origin task, harlow present a monkey with two visual contrast objects. one of 
these cover a small well contain a morsel of food; the other cover an empti well. the anim 
chose freeli between the two object and could retriev the food reward if present. the stage be 
then hidden and the left-right posit of the object be randomli reset. A new trial then began, 
with the anim again choos freely. thi process continu for a set number of trial use the 
same two objects. At complet of thi set of trials, two entir new and unfamiliar object be 
substitut for the origin two, and the process begin again. importantly, within each block of trials, 
one object be chosen to be consist reward (regardless of it left-right position), with the other 
be consist unrewarded. what harlow (harlow, 1949) observ be that, after substanti 
practice, monkey display behavior that reflect an understand of the task’ rules. when two 
new object be presented, the monkey’ first choic between them be necessarili arbitrary. but 
after observ the outcom of thi first choice, the monkey be at ceil thereafter, alway choos 
the reward object. 

10 



(a) two-step task (b) model predict 

(c) lstm a2c with reward input 

figur 5: three-stat mdp model after the “two-step task” from daw et al. (2011). (a) mdp with 3 state and 
2 actions. all trial start in state s1, with transit probabl after take action a1 or a2 depict in the 
graph. S2 and S3 result in expect reward ra and rb (see text). (b) predict of choic probabl give 
either a model-bas strategi or a model-fre strategi (daw et al., 2011). specifically, model-bas strategi 
take into account transit probabl and would predict an interact between the amount of reward receiv 
on the last trial and the transit (common or uncommon) observed. (c) agent display a perfectli model-bas 
profil when give the reward a input. 

We anticip that meta-rl should give rise to the same pattern of abstract one-shot learning. In 
order to test this, we adapt harlow’ paradigm into a visual fixat task, a follows. A 84x84 pixel 
input repres a simul comput screen (see figur 6a-c). At the begin of each trial, thi 
display be blank except for a small central fixat cross (red crosshairs). the agent select discret 
left-right action which shift it view approxim 4.4 degre in the correspond direction, 
with a small momentum effect (alternatively, a no-op action could be selected). the complet of a 
trial requir perform two tasks: saccad to the central fixat cross, follow by saccad 
to the correct image. If the agent held the fixat cross in the center of the field of view (within a 
toler of 3.5 degre visual angle) for a minimum of four time steps, it receiv a reward of 0.2. 
the fixat cross then disappear and two imag – drawn randomli from the imagenet dataset 
(deng et al., 2009) and resiz to 34x34 – appear on the left and right side of the display (figur 
6b). the agent’ task be then to “select” one of the imag by rotat until the center of the imag 
align with the center of the visual field of view (within a toler of 7 degre visual angle). 
onc one of the imag be selected, both imag disappear and, after an intertri interv of 10 
time-steps, the fixat cross reappeared, initi the next trial. each episod contain a maximum 
of 10 trial or 3600 steps. follow mirowski et al. (2016), we implement an action repeat of 4, 
mean that select an imag take a minimum of three independ decis (twelv primit 
actions) after have complet the fixation. It should be noted, however, that the rotat posit 
of the agent be not limited; that is, 360 degre rotat could occur, while the simul comput 
screen onli subtend 65 degrees. 

although new imagenet imag be chosen at the begin of each episod (sampl with 
replac from a set of 1000 images), the same imag be re-us across all trial within 
an episode, though in randomli vari left-right placement, similar to the object in harlow’ 
experiment. and a in that experiment, one imag be arbitrarili chosen to be the “rewarded” imag 
throughout the episode. select of thi imag yield a reward of 1.0, while the other imag yield 
a reward of -1.0. dure test, the a3c learn rate be set to zero and imagenet imag be drawn 
from a separ held-out set of 1000, never present dure training. 

A grid search be conduct for optim hyperparameters. At perfect performance, agent can 
complet one trial per 20-30 step and achiev a maximum expect reward of 9 per 10 trials. given 

11 



(a) fixat (b) imag display (c) right saccad and select 

(d) train perform (e) robust over random seed (f) one-shot learn 

figur 6: learn abstract task structur in visual rich 3D environment. a-c) exampl of a singl trial, 
begin with a central fixation, follow by two imag with random left-right placement. d) averag 
perform (measur in averag reward per trial) of top 40 out of 100 seed dure training. maximum 
expect perform be indic with black dash line. e) perform at episod 100,000 for 100 random 
seeds, in decreas order of performance. f) probabl of select the reward image, a a function of trial 
number for a singl a3c stack lstm agent for a rang of train durat (episod per thread, 32 threads). 

the natur of the task – which requir one-shot image-reward memori togeth with mainten of 
thi inform over a rel long timescal (i.e. over fixation-cross select and across trials) – 
we assess the perform of not onli a convolutional-lstm architectur which receiv reward 
and action a addit input (see figur 1b and tabl 1), but also a convolutional-stack lstm 
architectur use in a navig task discuss below (see figur 1c). 

agent perform be illustr in figur 6d-f. whilst the singl lstm agent be rel 
success at solv the task, the stacked-lstm variant exhibit much good robustness. that is, 
43% of random seed of the best hyperparamet set perform at ceil (figur 6e), compar to 
26% of the singl lstm. 

like the monkey in harlow’ experi (harlow, 1949), the network converg on an optim 
policy: not onli do the agent success fixat to begin each trial, but start on the second trial 
of each episod it invari select the reward image, regardless of which imag it select on the 
first trial(figur 6f). thi reflect an impress form of one-shot learning, which reflect an implicit 
understand of the task structure: after observ one trial outcome, the agent bind a complex, 
unfamiliar imag to a specif task role. 

further experiments, report elsewher (wang et al., 2017), confirm that the same recurr 
a3c system be also abl to solv a substanti more difficult version of the task. In thi task, onli 
one imag – which be randomli design to be either the reward item to be selected, or the 
unreward item to be avoid – be present on everi trial dure an episode, with the other 
imag present be novel on everi trial. 

3.2.3 one-shot navig 

the experi use the harlow task demonstr the capac of meta-rl to oper effect 
within a visual rich environment, with rel long time horizons. here we consid relat 
experi recent report within the navig domain (mirowski et al., 2016) (see also jaderberg 
et al., 2016), and discu how these can be recast a exampl of meta-rl – attest to the scaleabl 
of thi principl to more typic mdp set that pose challeng RL problem due to dynam 
chang spars rewards. 

12 



(a) labryinth i-maz (b) illustr episod 

(c) perform (d) valu function 

figur 7: a) view of i-maz show goal object in one of the 4 alcov b) follow initi explor 
(light trajectories), agent repeatedli go to goal (blue trajectories) c) perform of stack lstm (term 
“nav a3c”) and feedforward (“ff a3c”) architectures, per episod (goal = 10 points) averag across top 5 
hyperparameters. e) follow initi goal discoveri (goal hit mark in red), valu function occur well in 
advanc of the agent see the goal which be hidden in an alcove. figur use with permiss from mirowski 
et al. (2016). 

specifically, we consid a set where the environ layout be fix but the goal chang locat 
randomli each episod (figur 7; mirowski et al., 2016). although the layout be rel simple, 
the labyrinth environ (see for detail mirowski et al., 2016) be richer and more fine discret 
(cf vizdoom), result in long time horizons; a train agent take approxim 100 step (10 
seconds) to reach the goal for the first time in a give episode. result show that a stack lstm 
architectur (figur 1c), that receiv reward and action a addit input equival to that use 
in our harlow experi achiev near-optim behavior – show one-shot memori for the goal 
locat after an initi exploratori period, follow by repeat exploit (see figur 7c). thi be 
evid by a substanti decreas in latenc to reach the goal for the first time (~100 timesteps) 
compar to subsequ visit (~30 timesteps). notably, a feedforward network (see figur 7c), 
that receiv onli a singl imag a observation, be unabl to solv the task (i.e. no decreas in 
latenc between success goal rewards). whilst not interpret a such in mirowski et al. (2016), 
thi provid a clear demonstr of the effect of meta-rl: a separ RL algorithm with 
the capabl of one-shot learn emerg through train with a fix and more increment RL 
algorithm (i.e. polici gradient). meta-rl can be view a allow the agent to infer the optim 
valu function follow initi explor (see figur 7d) – with the addit lstm provid 
inform about the current relev goal locat to the lstm that output the polici over the 
extend timefram of the episode. taken together, meta-rl allow a base model-fre RL algorithm 
to solv a challeng RL problem that might otherwis requir fundament differ approach 
(e.g. base on successor represent or fulli model-bas rl). 

4 relat work 

We have alreadi touch on the relationship between deep meta-rl and pioneer work by hochre- 
iter et al. (2001) use recurr network to perform meta-learn in the set of full supervis 

13 



(see also cotter and conwell, 1990; prokhorov et al., 2002; younger et al., 1999). that approach be 
recent extend in santoro et al. (2016), which demonstr the util of leverag an extern 
memori structure. the idea of cross meta-learn with reinforc learn have be previ- 
ousli discuss by schmidhub et al. (1996). that work, which appear to have introduc the term 
“meta-rl,” differ from our in that it do not involv a neural network implementation. more recently, 
however, there have be a surg of interest in use neural network to learn optim procedures, 
use a rang of innov meta-learn techniqu (andrychowicz et al., 2016; chen et al., 2016; 
Li and malik, 2016; zoph and le, 2016). recent work by chen et al. (2016) be particularli close in 
spirit to the work we have present here, and can be view a treat the case of “infinit bandits” 
use a meta-learn strategi broadli analog to the one we have pursued. 

the present research also bear a close relationship with a differ bodi of recent work that have not 
be frame in term of meta-learning. A number of studi have use deep RL to train recurr 
neural network on navig tasks, where the structur of the task (e.g., goal locat or maze 
configuration) vari across episod (jaderberg et al., 2016; mirowski et al., 2016). the final 
experi that we present above, drawn from (mirowski et al., 2016), be one example. To the 
extent that such experi involv the key ingredi of deep meta-rl – a neural network with 
memory, train through RL on a seri of interrel task – they be almost certain to involv the 
kind of meta-learn we have describ in the present work. thi relat work provid an indic 
that meta-rl can be fruit appli to larg scale problem than the one we have studi in our 
own experiments. importantly, it indic that a key ingredi in scale the approach may be to 
incorpor memori mechan beyond those inher in unstructur recurr neural network 
(see grave et al., 2016; mirowski et al., 2016; santoro et al., 2016; weston et al., 2014). our work, 
for it part, suggest that there be untap potenti in deep recurr RL agent to meta-learn quit 
abstract aspect of task structure, and to discov strategi that exploit such structur toward rapid, 
flexibl adaptation. 

dure complet of the present research, close relat work be report by duan et al. (2016). 
like us, duan and colleagu use deep RL to train a recurr network on a seri of interrel tasks, 
with the result that the network dynam learn a second RL procedur which oper on a faster 
time-scal than the origin algorithm. they compar the perform of these learn procedur 
against convent RL algorithm in a number of domains, includ bandit and navigation. 
An import differ between thi parallel work and our own be the former’ primari focu on 
rel unstructur task distribut (e.g., uniformli distribut bandit problem and random 
mdps); our main interest, in contrast, have be in structur task distribut (e.g., depend 
bandit and the task introduc by harlow, 1949), becaus it be in thi set where the system can 
learn a bia – and therefor effici – RL procedur that exploit regular task structure. the two 
perspect are, in thi regard, nice complementary. 

5 conclus 

A current challeng in artifici intellig be to design agent that can adapt rapidli to new task by 
leverag knowledg acquir through previou experi with relat activities. In the present 
work we have report initi explor of what we believ be one promis avenu toward thi 
goal. deep meta-rl involv a combin of three ingredients: (1) use of a deep RL algorithm 
to train a recurr neural network, (2) a train set that includ a seri of interrel tasks, (3) 
network input that includ the action select and reward receiv in the previou time interval. 
the key result, which emerg natur from the setup rather than be special engineered, be 
that the recurr network dynam learn to implement a second RL procedure, independ from 
and potenti veri differ from the algorithm use to train the network weights. critically, thi 
learn RL algorithm be tune to the share structur of the train tasks. In thi sense, the learn 
algorithm build in domain-appropri biases, which can allow it to oper with great effici 
than a general-purpos algorithm. thi bia effect be particularli evid in the result of our 
experi involv depend bandit (section 3.1.2 and 3.1.3), where the system learn to 
take advantag of the task’ covari structure; and in our studi of harlow’ anim learn task 
(section 3.2.2), where the recurr network learn to exploit the task’ structur in order to display 
one-shot learn with complex novel stimuli. 

14 



one of our experi (section 3.2.1) illustr the point that a system train use a model-fre 
RL algorithm can develop behavior that emul model-bas control. A few further comment on 
thi result be warranted. As note in our present of the simul results, the pattern of choic 
behavior display by the network have be consid in the cognit and neurosci literatur 
a reflect model-bas control or tree search. however, a have be remark in veri recent work, 
the same pattern can aris from a model-fre system with an appropri state represent (akam 
et al., 2015). indeed, we suspect thi may be how our network in fact operates. however, other 
find suggest that a more explicitli model-bas control mechan can emerg when a similar 
system be train on a more divers set of tasks. In particular, ilin et al. (2007) show that recurr 
network train on random maze can approxim dynam program procedur (see also 
silver et al., 2017; tamar et al., 2016). At the same time, a we have stressed, we consid it an 
import aspect of deep meta-rl that it yield a learn RL algorithm that capit on invari 
in task structure. As a result, when face with wide vari but still structur environments, deep 
meta-rl seem like to gener RL procedur that occupi a grey area between model-fre and 
model-bas rl. 

the two-step decis problem studi in section 3.2.1 be deriv from neuroscience, and we 
believ deep meta-rl may have import implic in that arena (wang et al., 2017). the notion 
of meta-rl have be discuss previous in neurosci but onli in a narrow sense, accord 
to which meta-learn adjust scalar hyperparamet such a the learn rate or softmax invers 
temperatur (khamassi et al., 2011; 2013; kobayashi et al., 2009; lee and wang, 2009; schweighof 
and doya, 2003; soltani et al., 2006). In recent work (wang et al., 2017) we have show that 
deep meta-rl can account for a wider rang of experiment observations, provid an integr 
framework for understand the respect role of dopamin and the prefront cortex in biolog 
reinforc learning. 

acknowledg 

We would like the thank the follow colleagu for use discuss and feedback: nando de 
freitas, david silver, koray kavukcuoglu, daan wierstra, demi hassabis, matt hoffman, piotr 
mirowski, andrea banino, sam ritter, neil rabinowitz, peter dayan, peter battaglia, alex lerchner, 
tim lillicrap and greg wayne. 

refer 
thoma akam, rui costa, and peter dayan. simpl plan or sophist habits? state, transit and learn 

interact in the two-step task. plo comput biol, 11(12):e1004648, 2015. 

marcin andrychowicz, misha denil, sergio gomez, matthew W hoffman, david pfau, tom schaul, and nando 
de freitas. learn to learn by gradient descent by gradient descent. arxiv preprint arxiv:1606.04474, 2016. 

peter auer, nicolo cesa-bianchi, and paul fischer. finite-tim analysi of the multiarm bandit problem. 
machin learning, 47(2-3):235–256, 2002. 

timothi EJ behrens, mark W woolrich, mark E walton, and matthew FS rushworth. learn the valu of 
inform in an uncertain world. natur neuroscience, 10(9):1214–1221, 2007. 

ethan S bromberg-martin and okihid hikosaka. midbrain dopamin neuron signal prefer for advanc 
inform about upcom rewards. neuron, 63(1):119–126, 2009. 

yutian chen, matthew W hoffman, sergio gomez, misha denil, timothi P lillicrap, and nando de freitas. 
learn to learn for global optim of black box functions. arxiv preprint arxiv:1611.03824, 2016. 

NE cotter and PR conwell. fixed-weight network can learn. In 1990 ijcnn intern joint confer on 
neural networks, page 553–559, 1990. 

nathaniel D daw, yael niv, and peter dayan. uncertainty-bas competit between prefront and dorsolater 
striatal system for behavior control. natur neuroscience, 8(12):1704–1711, 2005. 

nathaniel D daw, samuel J gershman, ben seymour, peter dayan, and raymond J dolan. model-bas 
influenc on humans’ choic and striatal predict errors. neuron, 69(6):1204–1215, 2011. 

jia deng, wei dong, richard socher, li-jia li, kai li, and Li fei-fei. imagenet: A large-scal hierarch 
imag database. In comput vision and pattern recognition, 2009. cvpr 2009. ieee confer on, page 
248–255. ieee, 2009. 

yan duan, john schulman, Xi chen, peter L. bartlett, ilya sutskever, and pieter abbeel. rl2: fast reinforc 
learn via slow reinforc learning. arxiv preprint arxiv:1611.02779, 2016. url http://arxiv. 

15 

http://arxiv.org/abs/1611.02779 
http://arxiv.org/abs/1611.02779 
http://arxiv.org/abs/1611.02779 


org/abs/1611.02779. 

marco economides, zeb kurth-nelson, annika lübbert, marc guitart-masip, and raymond dolan. model- 
base reason in human becom automat with training. plo comput biology, 11(9):e1004463, 
2015. 

john C gittins. bandit process and dynam alloc indices. journal of the royal statist society. seri 
B (methodological), page 148–177, 1979. 

alex graves, greg wayne, malcolm reynolds, tim harley, ivo danihelka, agnieszka grabska-barwińska, 
sergio gómez colmenarejo, edward grefenstette, tiago ramalho, john agapiou, et al. hybrid comput 
use a neural network with dynam extern memory. nature, 2016. 

harri F harlow. the format of learn sets. psycholog review, 56(1):51, 1949. 

sepp hochreit and jürgen schmidhuber. long short-term memory. neural computation, 9(8):1735–1780, 
1997. 

sepp hochreiter, A steven younger, and peter R conwell. learn to learn use gradient descent. In 
intern confer on artifici neural networks, page 87–94. springer, 2001. 

roman ilin, robert kozma, and paul J werbos. effici learn in cellular simultan recurr neural 
networks-th case of maze navig problem. In 2007 ieee intern symposium on approxim 
dynam program and reinforc learning, page 324–329. ieee, 2007. 

max jaderberg, volodymir mnih, wojciech czarnecki, tom schaul, joel Z. leibo, david silver, and koray 
kavukcuoglu. reinforc learn with unsupervis auxiliari tasks. arxiv preprint arxiv:1611.05397, 
2016. url http://arxiv.org/abs/1611.05397. 

emili kaufmann, olivi cappé, and aurélien garivier. On bayesian upper confid bound for bandit 
problems. In proc. of int’l conf. on artifici intellig and statistics, aistats, 2012a. 

emili kaufmann, nathaniel korda, and rémi munos. thompson sampling: An asymptot optim 
finite-tim analysis. In algorithm learn theori - 23rd intern conference, page 199–213, 2012b. 

mehdi khamassi, stéphane lallée, pierr enel, emmanuel procyk, and peter F dominey. robot cognit 
control with a neurophysiolog inspir reinforc learn model. frontier in neurorobotics, 5:1, 
2011. 

mehdi khamassi, pierr enel, peter ford dominey, and emmanuel procyk. medial prefront cortex and the 
adapt regul of reinforc learn parameters. prog brain res, 202:441–464, 2013. 

kunikazu kobayashi, hiroyuki mizoue, takashi kuremoto, and masanao obayashi. A meta-learn method 
base on tempor differ error. In intern confer on neural inform processing, page 
530–537. springer, 2009. 

wouter kool, fieri A cushman, and samuel J gershman. when do model-bas control pay off? plo 
comput biol, 12(8):e1005090, 2016. 

brenden M lake, tomer D ullman, joshua B tenenbaum, and samuel J gershman. build machin that 
learn and think like people. arxiv preprint arxiv:1604.00289, 2016. 

tor lattimor and rémi munos. bound regret for finite-arm structur bandits. In advanc in neural 
inform process system 27, page 550–558, 2014. 

yann lecun, yoshua bengio, and geoffrey hinton. deep learning. nature, 521(7553):436–444, 2015. 

daeyeol lee and xiao-j wang. mechan for stochast decis make in the primat frontal cortex: 
single-neuron record and circuit modeling. neuroeconomics: decis make and the brain, page 
481–501, 2009. 

Ke Li and jitendra malik. learn to optimize. arxiv preprint arxiv:1606.01885, 2016. 

piotr mirowski, razvan pascanu, fabio viola, hubert soyer, andi ballard, andrea banino, misha denil, ross 
goroshin, laurent sifre, koray kavukcuoglu, dharshan kumaran, and raia hadsell. learn to navig in 
complex environments. arxiv preprint arxiv:1611.03673, 2016. url http://arxiv.org/abs/1611. 
03673. 

volodymyr mnih, koray kavukcuoglu, david silver, andrei A. rusu, joel veness, et al. human-level control 
through deep reinforc learning. nature, 518:529–533, 2015. 

volodymyr mnih, adrià puigdomènech badia, mehdi mirza, alex graves, timothi P. lillicrap, tim harley, 
david silver, and koray kavukcuoglu. asynchron method for deep reinforc learning. In proc. of 
int’l conf. on machin learning, icml, 2016. 

16 

http://arxiv.org/abs/1611.02779 
http://arxiv.org/abs/1611.02779 
http://arxiv.org/abs/1611.02779 
http://arxiv.org/abs/1611.05397 
http://arxiv.org/abs/1611.03673 
http://arxiv.org/abs/1611.03673 


danil V prokhorov, lee A feldkamp, and ivan Yu tyukin. adapt behavior with fix weight in rnn: an 
overview. In proceed of the ieee intern joint confer on neural network (ijcnn), page 
2018–2023, 2002. 

robert A rescorla, allan R wagner, et al. A theori of pavlovian conditioning: variat in the effect of 
reinforc and nonreinforcement. classic condit ii: current research and theory, 2:64–99, 1972. 

dan russo and benjamin van roy. learn to optim via information-direct sampling. In advanc in 
neural inform process system 27, page 1583–1591, 2014. 

adam santoro, sergey bartunov, matthew botvinick, daan wierstra, and timothi lillicrap. meta-learn 
with memory-aug neural networks. In proceed of the 33rd intern confer on machin 
learning, page 1842–1850, 2016. 

jurgen schmidhuber, jieyu zhao, and marco wiering. simpl principl of metalearning. technic report, see, 
1996. 

nicola schweighof and kenji doya. meta-learn in reinforc learning. neural networks, 16(1):5–9, 
2003. 

david silver, aja huang, chri J maddison, arthur guez, laurent sifre, georg van den driessche, julian 
schrittwieser, et al. master the game of go with deep neural network and tree search. nature, 529(7587): 
484–489, 2016. 

david silver, hado van hasselt, matteo hessel, tom schaul, arthur guez, tim harley, gabriel dulac-arnold, 
david reichert, neil rabinowitz, andr barreto, and thoma degris. the predictron: end-to-end learn 
and planning. submit to int’l confer on learn representations, iclr, 2017. 

alireza soltani, daeyeol lee, and xiao-j wang. neural mechan for stochast behaviour dure a 
competit game. neural networks, 19(8):1075–1090, 2006. 

richard S sutton and andrew G barto. reinforc learning: An introduction, volum 1. mit press 
cambridge, 1998. 

aviv tamar, Yi wu, garrett thomas, sergey levine, and pieter abbeel. valu iter networks. arxiv preprint 
arxiv:1602.02867v2, 2016. 

william R thompson. On the likelihood that one unknown probabl exce anoth in view of the evid 
of two samples. biometrika, 25:285–294, 1933. 

sebastian thrun and lorien pratt. learn to learn: introduct and overview. In learn to learn, page 
3–17. springer, 1998. 

jane X wang, zeb kurth-nelson, dhruva tirumala, joel leibo, hubert soyer, dharshan kumaran, and matthew 
botvinick. meta-reinforc learning: a bridg between prefront and dopaminerg function. In cosyn 
abstracts, 2017. 

jason weston, sumit chopra, and antoin bordes. memori networks. arxiv preprint arxiv:1410.3916, 2014. 

A steven younger, peter R conwell, and neil E cotter. fixed-weight on-lin learning. ieee transact on 
neural networks, 10(2):272–283, 1999. 

barret zoph and quoc V le. neural architectur search with reinforc learning. arxiv preprint 
arxiv:1611.01578, 2016. 

17 


1 introduct 
2 method 
2.1 background: meta-learn in recurr neural network 
2.2 deep meta-rl: definit and key featur 
2.3 formal 

3 experi 
3.1 bandit problem 
3.1.1 bandit with independ arm 
3.1.2 bandit with depend arm (i) 
3.1.3 bandit with depend arm (ii) 
3.1.4 restless bandit 

3.2 markov decis problem 
3.2.1 the ``two-step task'' 
3.2.2 learn abstract task structur 
3.2.3 one-shot navig 


4 relat work 
5 conclus 

