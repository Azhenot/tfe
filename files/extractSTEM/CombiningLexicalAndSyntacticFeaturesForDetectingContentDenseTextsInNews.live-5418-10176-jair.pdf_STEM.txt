









































journal of artifici intellig research 60 (2017) 179-219 submit 12/16; publish 09/17 

combin lexic and syntact featur for 
detect content-dens text in new 

yinfei yang yangyin7@gmail.com 
1600 amphitheatr pkwi 
mountain view, CA 94043 

ani nenkova nenkova@seas.upenn.edu 
univers of pennsylvania 

3330 walnut street 

philadelphia, pa, 19103 usa 

abstract 

content-dens news report import factual inform about an event in direct, suc- 
cinct manner. inform seek applic such a inform extraction, question 
answer and summar normal assum all text they deal with be content-dense. 
here we empir test thi assumpt on news articl from the business, u.s. inter- 
nation relations, sport and scienc journal domains. our find clearli indic 
that about half of the news text in our studi be in fact not content-dens and motiv 
the develop of a supervis content-dens detector. We heurist label a larg 
train corpu for the task and train a two-lay classifi model base on lexic and un- 
lexic syntact features. On manual annot data, we compar the perform 
of domain-specif classifiers, train on data onli from a give news domain and a gener 
classifi in which data from all four domain be pool together. our annot and pre- 
diction experi demonstr that the concept of content densiti vari depend on 
the domain and that naiv annot provid judgement bia toward the stereotyp 
domain label. domain-specif classifi be more accur for domain in which content- 
dens text be typic fewer. domain independ classifi reproduc good naiv 
crowdsourc judgements. classif predict be high across all conditions, around 
80%. 

1. introduct 

new articl be write with differ goal in mind. some aim to inform the reader 
about an import event, focu on specif detail such a who do what to whom where 
and when. other aim to provid background information, fact relat to an event and 
necessari to understand an event but not newsworthi by themselves. yet other seek to 
entertain the reader, or to showcas the brilliant masteri of languag and the wit of the 
author. 

In thi paper we introduc the task of detect if a text be content-dens or not. content- 
dens news report import factual inform about an event, in direct and succinct 
manner. prototyp exampl of content-dens text be newswir articles, which be 
usual perfect answer to a “what happened?” question, ground in a specif event. In 
gener news, however, newswire-like, content-dens text be not the norm. 

c©2017 AI access foundation. all right reserved. 



yang & nenkova 

We base our analysi on the open paragraph, call the lead or lede, of news articl 
drawn from the new york times. new report often adher to the invert pyramid struc- 
ture, in which the lead convey what happened, when and where, follow by more detail 
in the body. inform that be not essenti be includ in the final tail. when writer 
adher to thi style of writing, the lead be inform and provid posit exampl of 
content-dens texts. alternatively, the lead may be creative, provoc or entertain 
rather than informative, provid exampl of non content-dens texts. 

consid the lead below, from the polit and sport section of the new york times. 
the first two be content-dens leads. the other two be non content-dens lead that do 
not focu on events; and which be much richer stylistically. 

content-dense: 

[politics] evo morales, a candid for presid who have pledg to revers a campaign 
financ by the unit state to wipe out coca growing, score a decis victori in gener 
elect in bolivia on sunday. 

mr. morales, 46, an aymara indian and former coca farmer who also promis to 
roll back american-prescrib econom changes, have garner up to 51 percent of the vote, 
accord to televis quick-count polls, which talli a sampl of vote at poll place and 
be consid highli accurate. 

[sports] north carolina (29-1) and duke (26-3) of the atlant coast confer re- 
ceiv no. 1 seed yesterday in the 64-team women’ n.c.a.a. tournament, along with 
ohio state (28-2) and louisiana state (27-3). 

the top-rank tar heel receiv the no. 1 overal seeding, but be place in what 
appear to be the most difficult regional. 

non content-dense: 

[politics] when the definit histori of the iraq war be written, futur historian will 
sure want to ask saddam hussein and georg W. bush each one big question. To saddam, 
the question would be: what be you thinking? If you have no weapon of mass destruction, 
whi do you keep act a though you did? for mr. bush, the question would be: what 
be you thinking? If you bet your whole presid on succeed in iraq, whi do you 
let donald rumsfeld run the war with just enough troop to lose? whi didn’t you establish 
secur insid iraq and along it borders? how could you ever have thought thi would be 
easy? 

the answer to these question can be found in what be america’ great intellig 
failur in iraq – and that be not about w.m.d. 

[sports] with hi silver pant and dark blue jersey cover by a mottl mix of grass 
stains, paint and mud, new england patriot run back corey dillon sat on an aluminum 
bench on the sidelin at gillett field on sunday, look exhaust and frozen. 

onli a few minut remain in the patriots’ 20-3 victori over the indianapoli colts, 
and dillon be resting. He star at the field, snowflak swirl around hi head a the 
realiz of hi first playoff victori swirl insid it. 

below we propos an approach for label short news text a content-dens or not. our 
analysi of manual annot reveal that uninform articl lead be common. We 
investig sever type of lexic and non-lexic syntact featur for distinguish 

180 



detect content-dens new text 

content-dens text from other more gener or creativ write texts. We present a 
two-lay classifi model which significantli outperform a baselin assum that all news 
lead be content-dense. We also studi the robust of the definit of content densiti 
across domains, a well a the perform of domain-depend and domain-independ 
(general) classifiers. 

2. motiv 

traditionally, natur languag process practition work under the assumpt that 
the direct goal of text analysi be to ultim deriv a semant interpret of text. our 
work deviat from thi tradit and instead focu on detect style differ first, 
defer or entir forego semant interpretation. thi “style, then semant if need 
be” approach to understand reflect typic human behavior (kahneman, 2011). 

under style we hope to captur how content be convey rather than exactli what fact 
be be commun (queneau, 1947) or what truth valu one ought to assign to the 
express statements. thi definit be remark close to decades-old attempt to defin 
style a part of text typology: 

style be use here to mean the way text be intern differenti other than by topic; 
mainli by the choic of the presenc or absenc of some of a larg rang of structur 
and lexic features. 

(sinclair & ball, 1996) 

In the articl we also investig how broad topic (our news domains) interact with 
style, both in the way domain inform influenc people’ style judgement and in the 
chang of the structur and lexic indic of style across domains. 

In spirit, our work belong to a grow bodi of research concern with develop 
method for deduc how inform in longer text1 be convey and how inform will 
be perceiv by reader (yu & hatzivassiloglou, 2003; danescu-niculescu-mizil, kossinets, 
kleinberg, & lee, 2009; jurafsky, ranganath, & mcfarland, 2009; ashok, feng, & choi, 
2013; cook & hirst, 2013; loui & nenkova, 2014). our effort be complementary, and 
cannot be compar directly, to work concern with proposit mean directly, such 
a event detect (peng, song, & roth, 2016; feng, huang, tang, ji, qin, & liu, 2016; 
nguyen & grishman, 2016), verid (sauŕı & pustejovsky, 2009; de marneffe, manning, 
& potts, 2012) or fact-check (vlacho & riedel, 2014). 

3. corpu 

the data for our experi come from the new york time (nyt) annot corpu 
(ldc catalog no. ldc2008t19). the corpu contain 20 year worthi of nyt editions, 
along with rich meta-data about the newspap section in which the articl appear and 
summari produc by inform scientist for mani of the articles. the lead of articl 
be explicitli mark in the corpus, so extract the relev text for further analysi be 
straightforward. 

1. rather than individu word and sentenc 

181 



yang & nenkova 

In our previou proof-of-concept work (yang & nenkova, 2014), we select a subcorpu 
of articl publish in 2005 or 2006 from four differ genr (business, u.s. intern 
relations, scienc and sports). given the select criteria, the data in that prior work 
contain consider few articl from the scienc and the sport domain compar to 
the other two domains. moreover, the perform of the content-dens classifi in the 
scienc and sport domain be notabl bad than the other two domains, which could 
be explain either by the fact that these classifi be train on small dataset or by 
the intrins difficult of predict content densiti in these two domains. To definit 
resolv thi question, and to benefit from the larg train dataset possible, we extend 
the corpu to the full nyt corpu in the experi report in thi manuscript. 

We also expect that the degre to which a text would be judg to be content-dense, 
report on import event in a direct manner, be influenc by the domain of the article. 
It be reason to expect that typic event in scienc or sport would not be consid of 
the same import a intern polit or busi events. To studi the cross-domain 
differences, we analyz four news domains: business, sports, science2 and US intern 
relat (or polit for short). 

3.1 train set heurist 

To automat label lead a content-dens or not, we make use of the manual summari 
which accompani mani articl in the nyt corpus. for the articl with content-dens 
leads, the manual summari will be veri similar to the lead itself, a thi type of lead by 
definit provid a fact-focus summari of the article. for lead that simpli seek to 
engag the reader via more creativ devices, the manual summari will differ consider 
from the lead. overall, the similar between the lead and the manual summari provid 
a strong indic of the import and factual, event-oriented, natur of the inform 
express in the lead. 

for articl with manual summari of at least 25 words, we calcul a content-dens 
score. for each word in the summary, a tupl t(w, pos) be creat contain the word and 
it part of speech. the score be comput as: 

score = 
# of t(w, pos) also in lead 

# of t(w, pos) in sum 
(1) 

3.2 label analysi 

tabl 1 show detail about the number of all nyt articl from each of the four domains. 
the first column show the number of articl in the nyt from the give domain. the 
second column show the number of articl use for train domain-depend classifi 
(we explain the select procedur below). overal onli about one third of articl have 
associ manual summaries. 

the distribut of content-dens score assign a a function of the overlap with the 
human summari be show in figur 1. In the busi domain the distribut of score 

2. the scienc articl be from the cat corpu (loui & nenkova, 2013), which onli contain articl 
publish after 1999. 

182 



detect content-dens new text 

tabl 1: number of articl in the corpus. 

total number of articl articl use in train (percentage) 

busi 149,113 21,224 (14.2%) 

scienc 23,240 7,737 (33.%) 

sport 134,925 10,670 (7.9%) 

polit 45,926 10,503 (22.8%) 

overal 353,204 50,134 (14.2%) 

be almost uniform, reflect the fact that in that section there be articl about impor- 
tant events—compani mergers, unexpect stock price changes, product announc 
and lawsuits—but also non-ev specif analysi of current trends, minor event such a 
auction and people-cent piec about promin busi men and women. 

In sport and science, the distribut of content-dens score be clearli skew toward 
the non content-dens end of the spectrum. In these domain writer more often resort to 
the use of creativ and indirect languag meant to provok readers’ interest. 

the content-dens score in polit be almost normal distributed, with mean roughli 
in the middl of the possibl range, and much high than ani of the other domains. the 
non content-dens lead in thi domain usual provid a commentari on an ongo event 
rather than report of a specif new development. 

In the rest of the paper, we focu on the binari classif task of predict if a 
lead be content-dens or not. however, it be reason to expect that our indirect label 
score be noisy. To obtain cleaner data for train our model, we label onli the lead with 
most extrem scores: we assign the label non content-dens to the lead with score that 
fall below the 20th percentil and label content-dens to lead that score abov the 80th 
percentil for their domain. the 20th/80th percentil set be color red in figur 1. In 
the gener (domain-independent) model, the data be pool togeth and again the lead 
with low score be assign to the non content-dens class and the lead with high 
score be consid content-dense. 

4. methodolog 

In thi section, we introduc the featur and model we use in our experiments. In our 
prior experi (yang & nenkova, 2014), we found that lexic featur be well-suit 
for the task, particularli lexic represent determin independ of the train 
data. along with these, unlexic syntact represent also lead to remark 
good results. A number of other represent we experi with do not appear to 
be that benefici for the task. motiv by these findings, here we studi in depth the 
lexic represent and the unlexic syntact representation, and explor way to 
combin the predict of these model to achiev even good accuracy. 

4.1 featur 

We compar and combin two lexic and one syntact representation. for the lexic 
representation, we use the vocabulari from the mrc database(mrc), which be inde- 

183 



yang & nenkova 

figur 1: score histogram for the four genres: [top left] business, [top right] science, 
[bottom left] sports, [bottom right] politics. 20th and 80th percentil be color 
red. red star indic the averag content-dens score for each genre. 

pendent of our train set and a vocabulari deriv from the train set and weight by 
mutual information(mi). the syntact represent be simpli the list of product 
rules(pr) from the constitu pars of the sentenc in the lead. 

4.1.1 mrc database(mrc) 

the mrc psycholinguist databas (wilson, 1988) be an electron dictionari contain 
150,837 words, differ subset of which be annot for 26 linguist and psycholinguis- 
tic attributes. We select a subset of 4,923 word norm for age of acquisition, imagery, 
concreteness, familiar and ambiguity. In (wilson, 1988), the word be chosen among 
those with medium frequenc in a larg corpu and experi subject be ask to rate 
on a scale the degre to which each word have one of these properties. the mrc dictionari 
be a compil of result from differ studies, run by differ research groups, with 
differ criterion for select the list of word for which to solicit norms. We use the list 
of word which have at least one of abov ratings. the valu of each featur be equal to the 
number of time it appear in the lead, divid by the number of word in the lead. 

184 



detect content-dens new text 

about 90% of the mrc vocabulari (4,647 words) appear at least onc in the train 
data. about 4,300 appear more than five times.3 

4.1.2 mutual information(mi) 

the lexic represent describ abov be domain independent, determin without ani 
knowledg about the data which will be use for train and test of our classif 
models. We also introduc a domain-depend lexic representation, deriv from the 
train data for the classifi and use mutual inform to measur the associ 
between particular word and the content-dens and non content-dens write styles. for 
each genre, we comput the mutual inform between word and lead type in the train 
data as: 

mic = log 
p(word, c) 

p(word)p(c) 
(2) 

here c be either the content-dens or the non-cont dens class. We onli comput the 
MI score for word that appear at least 5 time in the train set. We select the top 500 
word with high associ with each of the write styles, for a total of 1,000 features. 
the valu of the featur be 1 if the word occur in the lead and 0 otherwise. 

the word with high mutual information4 with the content-dens class and non 
content-dens class be list in tabl 2. 

the word with high mutual inform with the content-dens class be distinctli 
domain specific. content-dens lead in the busi domain be more like to talk about 
compani and their executives, deals, agreement and offers. content-dens lead in scienc 
be more like to discu a specif studi or drug, sinc they be overwhelm bia 
toward health-rel topics. sport content-dens lead be associ with specif sport 
event or deals. In politics, content-dens lead discu american involv and attacks. 

In addition, the word “yesterday” and “today” also appear among those associ with 
content-dens leads, provid a strong indic that the news be focu on a specif recent 
event rather than a gener discuss or person aspect story. the word associ with 
the non content-dens class in contrast tend to be relat to non-specif activ (find, 
feel, hear, smile, remember, sit, wait) and focu on person aspect () rather than on the 
profession role (man, people, friend, husband, guy, kid, child, friend). 

onli about half of the word in the mutual inform represent also appear in 
the mrc. 

3. As we mention in the open of thi section, in our earli work (yang & nenkova, 2014) we also 
experi with other dictionaries, includ liwc and the gener inquirer. result consist 
confirm that the mrc lead to best predict results. thi be also the larg resource, guarante 
the best coverag of featur for new texts. for these reasons, we includ onli mrc featur in the work 
present here, to focu the present on the cross-domain differ and classifi combination, 
which be novel with respect to our prior work. 

4. We ran 10 fold cross valid in the experiments. the mutual inform be comput separ base 
on the train set of each fold. the word list in tabl 2 be from fold 0. high mutual inform 
word from other fold be veri similar. 

185 



yang & nenkova 

tabl 2: top 30 select word for each domain and overal data 

content-dens non content-dens 

busi 

company, yesterday, million, billion, 
today, percent, group, announce, 

executive, plan, share, corporation, 
york, part, deal, agree, largest, unit, 

court, agency, inc., commission, bank, 
include, firm, chief, agreement, 

chairman, offer, servic 

day, stock, ago, work, thing, good, 
investor, year, find, turn, long, man, 
economy, job, people, home, street, 

room, time, rate, lot, index, city, sit, 
mr., market, wall, money, ms., life 

scienc 

study, health, today, yesterday, report, 
drug, official, research, federal, state, 

scientist, administration, disease, 
researcher, company, government, 
accord, human, virus, university, 

group, million, expert, announce, cell, 
include, cancer, united, agency, issu 

day, mr., ms., ago, feel, hear, room, 
sit, walk, home, eye, life, friend, thing, 

run, talk, live, game, stand, back, 
family, hand, foot, good, morning, 
husband, hour, night, town, son 

sport 

yesterday, today, league, team, 
national, million, season, association, 
official, die, cup, contract, race, year, 

deal, tonight, game, conference, major, 
president, round, lead, charge, 

announce, committee, victory, win, 
woman, world, seri 

fan, ago, watch, back, stand, ball, day, 
question, good, turn, moment, room, 
smile, feel, hand, time, wear, people, 
knicks, hear, remember, n.b.a., net, 

guy, sit, thing, stadium, shot, kid, walk 

polit 

official, united, today, american, 
states, administration, mr., clinton, 

military, government, weapon, 
international, effort, attack, security, 

nuclear, force, report, intelligence, 
group, court, defense, nations, 

program, include, china, agency, 
secretary, nato, plan 

man, day, world, war, people, time, 
u.s., ago, back, sit, thing, front, live, 

city, child, street, room, stand, 
saddam, morning, america, word, 

year, wait, car, kerry, young, friend, 
watch, hour 

overal 

yesterday, today, company, million, 
official, billion, group, united, percent, 
announce, states, plan, administration, 

york, include, american, agency, 
government, federal, report, accord, 

court, executive, national, drug, part, 
state, international, corporation, deal 

day, ago, thing, man, good, stock, 
time, room, sit, back, stand, turn, 

watch, street, hear, home, feel, people, 
long, life, lot, ms., walk, town, wall, 

word, friend, live, moment, eye 

4.1.3 product rules(pr) 

finally, we use product rule a the syntact represent (loui & nenkova, 2012; 
ganjigunt ashok, feng, & choi, 2013; post & bergsma, 2013; malmasi & dras, 2014). 

We view each sentenc a the set of grammat productions, lh → rhs, which 
appear in the syntact pars tree of the sentence. We keep onli non-termin nodes, 
exclud all lexic information, so the lexic and syntact represent captur non- 
overlap aspect of write style. all product rule from the train set be use in 

186 



detect content-dens new text 

the representation. the number of product rule vari for the four domains, from 16,000 
rule (science) to 32,000 rule (business).5 

4.2 classifi combin 

the three featur represent we introduc captur domain independ lexic clue 
for content-density, domain-depend indic for import event and gener style of 
write captur by the structur of sentenc in the text. We train a logist regress 
classifi with each class of featur individually. furthermor in thi section, we examin 
two approach for combin the predict from the three class of features. 

4.2.1 feature-level combination(c1) 

first we examin the perform of feature-level combin to develop a system that 
make use of all three type of indic of content density. We concaten the three 
featur represent togeth in a featur vector. the number of entri in the featur 
vector be equal to the sum of the number of featur of the mrc, mutual inform and 
product rule representations. then we train a logist regress model base on the 
concaten featur representation. thi way of combin evid lead to overal im- 
provement in our earli work. howev much work on ensembl learn have demonstr 
that for varieti of task thi method of combin be not a power a decision-level 
combin (for exampl see raaijmakers, truong, & wilson, 2008; van halteren, zavrel, 
& daelemans, 1998; metallinou, lee, & narayanan, 2010; bertolami & bunke, 2006). We 
treat the feature-level combin a the baselin for our experiments. figur 2 (a) show 
the structur of feature-level combin classifier. 

4.2.2 decision-level combination(c2) 

classifi combin have be show to outperform featur combin in a singl classi- 
fier (tulyakov, jaeger, govindaraju, & doermann, 2008). there be multipl reason whi 
thi may be the case, especi for a linear classifi like the one we use. concaten all 
featur in a singl represent make the system prone to over-fitting, a the number 
of featur becom closer to the number of train examples. If the number of featur 
of a give type be consider small (for exampl there be mani more featur in the 
product rule represent compar to the mutual inform representation), the 
signal contribut to the final decis may be domin by the larg class, defeat the 
purpos of evid combination. It could also lead to the presenc of correl features, 
for exampl in the combin of the two type of lexic features. 

We propos a two layer classifi combin system. We first train a logist regress 
classifi with each of the three featur represent individually. then anoth model 
be trained, in which the featur be the probabl of the content-dens class from the 
first layer classifiers. In the experiment, the corpu be split into train set, develop 
set and test set. the first layer classifi be train on the train set, and the second 

5. stanford corenlp packag (manning, surdeanu, bauer, finkel, bethard, & mcclosky, 2014) be use to 
extract product rules. 

187 



yang & nenkova 

layer classifi be train on develop set. figur 2 (b) illustr the structur of the 
decision-level combin system. 

(a) feature-level combin (b) decision-level combin 

figur 2: illustr of feature-level combin and decision-level combin 

5. evalu on automat annot 

In thi section we will evalu the effect of each of the featur a well a the two 
combin systems. 

5.1 classifi evalu 

In the feature-level combin system, we train the binari classifi use liblinear (r.- 
E. fan & lin, 2008) with l2-regular logist regress model setting. In the decision- 
level combin experiments, we first train binari classifi base on each featur rep- 
resent use liblinear with the same settings. use the probabl output (for the 
content-dens class) of the first stage classifi a features, we then train a final binari 
classifi use libsvm (chang & lin, 2011) with linear kernel. grid search be use on 
train and develop set to find the best hyper-paramet in all models. 

We perform 10-fold cross-valid experi on the entir heurist label data. 
the entir dataset be split into 10 partitions. At each run, five partit be use for train 
first-stag classifi and the feature-level combin classifier. four partit be use 
for train the second-stag combin classifier, which us onli the probabl of the 
content-dens class from the first stage classifiers. one partit be use for test the 
classifiers. We evalu the two combin model on the automat label data but 
also analyz the perform when onli a singl class of featur be used. 6 

the result be present in tabl 3. becaus of the way the data be labeled, the two 
class be of equal size, with 50% accuraci a the random baseline. the top three row 
in the tabl correspond to a system train with onli one class of features. the last two 
row show the result for the two combin systems. the column correspond to the 
domain we study—business, science, sport and politics. the domain-specif model be 

6. We train the first- and second-stag classifi on differ portion of the train data in the fold in 
order to obtain realist predict from the first-stag classifier. If we be to use all nine partit in 
the current fold, the second-stag classifi would be train on the predict of the first-stag classifi 
on it own train data which would be unrealist accurate. the train protocol we adopt reflect 
good realist usag of the combin classifier. 

188 



detect content-dens new text 

train and test onli on the data from the give domain and the result be show in the 
first four columns. the general, domain-independ model be train and test on the 
combin dataset and the last column show it performance. 

precision, recall, f-score and accuraci be show in the table. depend on the 
domain, accuraci be high, rang between 87.2% for busi and 83.6% for politics. 
the precis and recal be veri balanc accord the numbers, which lead f-score veri 
close to accuraci in all experiments. here we mostli focu our discuss on accuracy. 

Of the individu featur classes, the product rule represent lead to the best 
overal accuracy. combin the represent at the featur level lead to improv 
over the product rule classifi for the busi and polit domain, a well a in the 
gener domain-independ classifi but not for scienc and sport where perform 
use all featur be in fact bad than use product rule alone. 

In line with our previou work, all singl featur classifi have veri good performances. 
the product rule (pr) syntact represent lead to the best perform for all 
domains, with accuraci over or close to 80% for all domains. the most import rule 
be quit differ in each genre, but the discov pattern be mostli align with our 
intuition. for example, VP ->vb NP prt advp be often associ with content-dens 
lead in business, the exampl text like VP ->vb[push] np[the czech currency] prt[up] 
advp[sharply]. the rule NP ->jj CD nns, however, be usual associ with non 
content-dens leads, e.g. NP ->jj[pre-april] cd[15] nns[blues]. the product rule 
with high weight be list in appendix B. 

Of the lexic representations, the mrc represent lead to good results, with 
accuraci vari from 82.7% for the busi domain to 79.4% for the sport domain. 
the corpus-depend lexic represent base on mutual inform have a slightli 
low performance: the accuraci rang between 81.9% for busi and 78.1% for politics. 

the result for the gener classifier—which be train and test on data from the four 
domain pool together—ar similar. for thi classifi lead may chang their labels, for 
exampl a sport articl whose content-dens score be in the 80th percentil of score for 
sport may fall below the 20th percentil when all data be combined. 

the fact that the represent design independ of the train data can lead 
to such good result be a posit finding, indic that the result be like to be robust. 

for all the domain and gener domain-independ data, decision-level combin 
consider improv the perform compar to classifi train with onli one of the 
representations. It be the most accur among the five classifi that we compare, with up 
to 3.8% perform gain in polit compar to the best singl featur classifier. 

the baselin combin system, feature-level combination, perform bad than the 
decision-level combination. one of the possibl reason be that give the increas number 
of features, thi model may requir more train data to reach it perform potential. 
We studi thi aspect of model develop in section 5.3. 

5.2 combin classifi with differ represent 

here we evalu differ possibl combin of featur types. We compar these pos- 
sibil for decision-level combination, which we alreadi establish work good than 
feature-level combination. 

189 



yang & nenkova 

tabl 3: binari classif result of 10-fold cross valid on the automat label 
set for differ class of featur and two fusion model for all domains: [p]recis / 
[r]ecal / [f]score / [a]ccuraci (%) 

busi scienc 
P R F A P R F A 

mrc 82.0 84.4 83.1 82.7 79.4 84.1 81.6 81.1 
MI 80.0 85.1 82.5 81.9 76.9 85.0 80.7 79.8 
PR 83.8 83.9 83.8 83.8 83.2 84.7 83.9 83.8 

C1 85.8 85.3 85.5 85.5 80.4 83.7 82.0 81.4 
C2 87.9 86.4 87.1 87.2 87.5 87.0 87.2 87.3 

sport polit 
P R F A P R F A 

mrc 78.6 80.9 79.7 79.4 76.8 82.3 79.4 78.5 
MI 76.0 84.4 80.0 78.8 74.7 84.8 79.4 78.1 
PR 82.1 83.3 82.7 82.6 79.4 80.4 79.9 79.8 

C1 80.8 83.0 81.9 81.5 77.0 83.6 80.1 80.8 
C2 86.0 85.0 85.5 85.6 83.1 84.2 83.6 83.6 

tabl 4: binari classif result of 10-fold cross valid on the automat label 
set for differ class of featur and two fusion model for general: [p]recis / [r]ecal 
/ [f]score / [a]ccuraci (%) 

gener 
P R F A 

mrc 81.2 83.4 82.3 82.0 
MI 79.4 82.2 80.8 80.6 
PR 83.5 83.5 83.5 83.5 

C1 84.4 85.8 85.1 85.0 
C2 86.8 86.4 86.6 86.7 

the motiv to examin combin of featur be that not all featur be avail 
in all applications. moreov concern about run time may make syntact featur unde- 
sirabl in certain settings, where syntact pars may not be feasible. mutual inform 
represent also requir larg train data for each domain of interest, to comput 
the mutual weight for each feature. So we examin the effect of combin differ 
featur classes. the multilay structur make the decision-level fusion easi to add or 
remov features. develop can simpli train a classifi base on new features, then add 
them to the second layer without affect exist singl featur classifiers. 

We show the result from evalu three differ classifi combinations: mrc+mi 
(lexic featur only), mrc+pr (domain independ featur only) and mrc+mi+pr 
(all featur together). 

the result be show in tabl 5. the top row in the tabl correspond to the baseline, 
feature-level combin model with all three class of features. row 2-4 correspond to 

190 



detect content-dens new text 

tabl 5: binari classif result of 10-fold cross valid on the automat label 
set for differ combin of featur for all domains: [p]recis / [r]ecal / [f]1 / 
[a]ccuraci (%) 

busi scienc 
P R F A P R F A 

C1 85.8 85.3 85.5 85.5 80.4 83.7 82.0 81.4 

mrc+mi 84.8 84.8 84.8 84.8 83.7 82.7 83.1 83.2 
mrc+pr 87.2 86.1 86.6 86.7 86.5 86.6 86.6 86.6 

mrc+mi+pr 87.9 86.4 87.1 87.2 87.5 87.0 87.2 87.3 

sport polit 
P R F A P R F A 

C1 80.8 83.0 81.9 81.5 77.0 83.6 80.1 80.8 

mrc+mi 82.2 81.8 82.0 82.0 79.5 82.3 80.9 80.6 
mrc+pr 84.6 84.7 84.6 84.7 82.2 83.5 82.8 82.7 

mrc+mi+pr 86.0 85.0 85.5 85.6 83.1 84.2 83.6 83.6 

tabl 6: binari classif result of 10-fold cross valid on the automat label 
set for differ combin of featur for general: [p]recis / [r]ecal / [f]1 / [a]ccuraci 
(%) 

gener 
P R F A 

C1 84.4 85.8 85.1 85.0 

mrc+mi 83.7 83.3 83.4 83.5 
mrc+pr 86.5 85.4 85.9 86.1 

mrc+mi+pr 86.8 86.4 86.6 86.7 

decision-level model with the three differ classifi combinations. As in previou tables, 
the first four column correspond to domain-specif models, and the last column show 
the result for the general, domain-independ model. combin classifi base on 
all three featur in decision-level combin still have the high accuracy, show that 
each of the three represent contribut to the improv perform of the classifier. 
the domain independ features, mrc+pr with decision-level combin show a com- 
petit result too, suggest that the mutual inform represent be the one that 
could be remov with least degrad in performance. the accuraci be just slightli 
low than the best, 0.5% low for the busi domain for example. 

the decision-level combin of lexic represent have low perform then 
the other two decision-level combin models. the accuraci rang between 84.5% 
for the busi domain and 80.3% for the polit domain. the combin of the two 
lexic represent lead to good perform than use either of the individu featur 
classes, suggest that mrc+mi combin at the decision-level be a good altern 
when syntact featur be not available. 

191 



yang & nenkova 

5.3 Is the train data enough? 

We now discu the impact of the train set size on classifi performance. We evalu 
the relationship between classifi accuraci and the increas of the number of train 
instanc for each domain. We start with a train set of 100 articles, grow to 6,500 
instanc in the train data, increas the train set with 100 randomli select articl 
in each step. accuraci be comput on the same test set for each domain. As in 
our previou experiments, 10-fold cross valid be performed. for each fold, there be 
a dedic test set, which mean all cross-valid iter use the same test set. the 
report result be an averag of the accuraci on the fix test set in each fold. 

figur 3 show the accuracy/s curv for each domain. among the four genres, 
decision-level combin of all three featur have the high accuracy. the accuraci 
increas rapidli with the increas of train data when the number of train articl be 
less than 2,000. when the size be larg than 2,000, it continu to increase, but veri slowly. 
the decision-level combin of mrc+pr features, which be the second best model for 
all domains, behav similarly. the accuraci of the mrc+mi decision-level combin be 
the bad of the combin system and exhibit the slowest increase. 

the accuraci of decision-level combin with 6,500 train articl be alreadi veri 
close to the final number with full train set (shown in tabl 5). increas the number 
of train instanc bare chang the perform after thi point. 

the baseline, feature-level combination, have the low accuracies. yet we still see 
increas in accuraci a the train set size increases. for three of the domains, it perfor- 
manc becom the same a that of the mrc+mi combin with a larg enough train 
set. 

the result also indic that decision-level combin be abl to achiev good per- 
formanc with less train data. 

the graph suggest that the differ in perform of the content-dens predictor 
in the four domain like reflect the difficulti of the domain rather than the differ in 
train data size. 

6. evalu on human annot 

So far we have establish that recognit of content-dens text can be do veri ac- 
curat when the label for the lead be determin by intuit heurist on the avail 
article/summari resources. We would like howev to test the model on manual anno- 
tat data a well, in order to verifi that the predict inde conform to reader percept 
of the style of the article. 

6.1 human annot dataset 

We select a total of 1,000 articl and split them into two sets. for the first set of 400 
articles, the author of the paper annot the content-dens label and provid a real- 
valu score for the domain-depend content-dens of each text. then a second set of 
600 articl be select and annot on amazon mechan turk (amt). all annot 
articl be randomli pick from the nyt data and do not appear in the train data 
for the classifi that we evalu here. 

192 



detect content-dens new text 

figur 3: accuraci by chang size of train set for the four genres: [top left] business, 
[top right] science, [bottom left] sports, [bottom right] polit 

6.1.1 basic set 

In the basic human annot set, the author of the paper annot 400 nyt articles, 100 
from each domain, with judgement of their perceiv informativeness. similar to prior work 
on grammat judgement (bard, robertson, & sorace, 1996), the annot be do 
with respect to a refer lead that fell around the middl of the content-dens spectrum. 
lead be label by domain: the question be if a specif articl from domain D be 
content-dens compar to the refer lead for that domain. all 100 lead from the same 
domain be group togeth and display in random order, with the annot see 
lead onli from the same domain until they complet the annot for that domain. the 
refer lead in each case be drawn from the respect domain. the annot give both 
a categor label for the lead (less content-dens or more content-dens than the reference) 
and a real valu score (rang between 0 to 100) via a slide bar. the categor label 
be use to test the binari classifiers. the real-valu annot be use to comput 
correl with classif score produc by the classifier. 

inter-annot agreement all 400 test lead be annot a be content-dens 
or not and with a real-valu indic of the extent to which they be content-dense. tabl 
7 show the percent agreement between the two annot on the binari level task, a well 
a the correl of the real-valu annotation. for the binari annot we also report 
the kappa statistic. 

193 



yang & nenkova 

tabl 7: inter-annot agreement on manual annotations. percent agreement be comput 
on the binari annotation, correl be comput on the real-valu degre of content-dens 
of the leads. all correl be highli significant, with p < 0.001. 

agreement kappa correl 

busi 0.70 0.405 0.608 

scienc 0.74 0.455 0.523 

sport 0.73 0.460 0.522 

polit 0.78 0.550 0.711 

As tabl 7 shows, the agreement for all domain be consider high but not perfect. 
agreement be highest—almost 80%— for the polit domain. the agreement be low 
in the busi domain, 70%. the correl of content-dens score exceed 0.5 and 
be highli signific (p < 0.001) for all domains. the high correl of real-valu 
scores, especi for the polit and busi domains, suggest that the task may be more 
amen to annot and autom a a real-valu predict task rather than a a 
binari distinction. 

kappa howev be rel low, indic that the annot task be rather difficult. 
To refin our instruct for annotation, we adjud all lead for which there be no 
initi agreement on the label. both author sat together, read the refer lead and 
each of the lead to be annotated, discuss the reason whi the lead should be label 
content-dens or not. In mani cases, the final decis be make by take into account 
the domain from which the lead be drawn (i.e. “there isn’t much import inform 
in a sport lead, but it could be consid content-dens in the context of sport news 
reporting”), a well a the refer lead for the specif genr (i.e. “the lead be not that 
content-dens but appear to contain more import fact or report the news in a more 
direct style than the refer lead”). We studi further the way domain and percept of 
content densiti interact in the next section, where independ annot rat content- 
densiti both in in-domain and in domain-independ gener settings.7 

below be an exampl on whose label the author initi disagreed. In thi lead, the 
first paragraph be non-inform and the second paragraph be informative, provid partial 
justif for either overal label. 

[exampl of label disagreement] mani elderli peopl be alreadi distress by 
the increas number of drug they be taking, includ painkil and heart medication. 
now, those who be also battl depress may be wonder where it all will end. 

last week, research at the univers of pittsburgh present find from a larg 
government-financ studi suggest that antidepress be more effect in ward 
off a recurr of late-lif depress than period session of interperson therapy, a 
standard form of talk treatment. 

7. As we will shortli see, the classifi be impress accur on instanc in which the annot agre 
in their initi annot and quit poor on the lead that requir adjudication. these find suggest 
that in futur work in may be benefici to develop a classifi for sentence-level predict (yang, bao, & 
nenkova, 2017) of content-density, which would be help for character lead that mix inform 
and entertain sentences. anoth clear altern be to develop a classifi to predict that a text be 
ambigu in term of it content-dens status. 

194 



detect content-dens new text 

6.1.2 amt annot set 

We also compil a second set of 600 nyt articles, 150 for each domain. In an attempt 
to provid more guidanc to the annotators, we give four refer lead for each domain, 
two a exampl of prototyp content-dens lead and two a exampl of lead that be 
clearli not content dense. the refer lead for each domain be show in appendix A. 
the annot saw the four prototyp leads, a well a a group of lead that they have 
to annotate. they provid both a categor label for each target lead (content-dens or 
not) and a real valu score for the degre to which it can be consid content-dens (rang 
between 0 and 100). 

the annot be partit into group of five leads—an annot have to label at 
least five lead and then request more data for annotation, in group of five. To emb some 
qualiti control, one of the five lead in each group be a lead from the dataset annot by 
the authors, for which they agre in independ annot befor the adjud step. 
thi data allow u to ass the qualiti of annot after problemat annot be 
filter out. 

here we also studi the differ in how the content-dens of a text would be perceiv 
in-domain and in gener setting. for each lead text, two task be publish separ 
for label content-dens in-domain or in general. for the in-domain task, annot 
be give domain inform (i.e. “here be articl drawn from the sport section of a 
newspaper...”) and the refer lead be select from that domain. In the gener task, 
worker be not told the domain of the lead and the refer lead be select without 
regard to domain.8 

ten annot annot each lead in each of the two conditions. 

We use two rule to filter out unqualifi annotators. We filter out all annot by 
annot who annot too quickli or be inconsistent. the first rule be that annotator’ 
averag annot time per task should be longer than 40 seconds. for reference, the 
averag annot time per task among all annot be around two minutes. the second 
rule be that label categori and score should be consist for each lead text. If an annot 
label a lead a content-dens but give a veri low content-dens score or vice versa, we 
know someth in their understand of the task be amiss. 

there be on averag 8 annot for each item after filter out unqualifi words. 
for each lead, we use the major categori a the final categori label and the averag score 
a the final score label. If there be a tie for a lead, we label it content-dense. 

tabl 8 show the agreement and kappa between the major label from amt worker 
and the authors’ agre labels. We comput these onli for the in-domain label becaus 
our initi annot be domain dependent. 

agreement for the busi and sport categori be high but onli moder for scienc 
and politics. We be unsur about the exact reason whi thi be the case. 

amt worker annot lead in two conditions: in-domain, where the judgement be 
specif to the domain from which the lead be drawn and gener (domain-independent), 
where a domain be not specifi and text from all four domain be randomli mix in the 
annot tasks. tabl 9 show the number of content-dens lead for each domain for both 

8. the content-dens exampl be from busi and science, and the non content-dens from busi 
and politics. 

195 



yang & nenkova 

tabl 8: agreement of emb baselin lead between amt worker and author of the 
paper. 

agreement(%) kappa 

busi 92.1 0.841 

scienc 86.8 0.622 

sport 97.3 0.947 

polit 79.0 0.574 

tabl 9: number (and percentage) of content-dens lead annot by amt worker for 
each domain. the same data be annot with respect to in-domain and gener criterion 
and the statist for each condit be show in the first and last column respectively. the 
two middl column show the number of lead that chang label from content-dense(cd) 
to non content-dense(non-cd) or vice versa between the in-domain and gener condition, 
broken down accord to the direct of the change. 

in-domain 
label chang 

gener 
CD → non-cd non-cd → CD 

busi 93 (62.0%) 8 11 96 (66.0%) 

scienc 64 (42.7%) 16 25 73 (48.7%) 

sport 76 (51.1%) 38 2 40 (26.7%) 

polit 72 (48.0%) 2 53 123 (82.0%) 

overal 305 (50.8%) 64 91 332 (55.3%) 

conditions, along with the number of lead whose label chang across conditions. the first 
and the forth column correspond respect to the number (percentage) of content-dens 
lead among all in-domain and gener label for the same data. the second and third 
column show the number of label that chang their label from content-dens (cd) to 
non content-dens (non-cd) or vice versa, between the domain-depend and the domain- 
independ labelling. 

clearly, the domain context play a larg role in the percept of content density. 
the chang be most clear for the polit and sport domain: in the domain-independ 
label a larg number of sport leads, which appear content-dens for their domain, 
be consid non content-dens in general. for sports, dure in-domain annot we 
have about half of the lead mark a content-dense, while just under 30% of the same 
lead be mark a content-dens in domain independ annotation. similarli mani of 
the polit lead consid non content-dens for the standard of the polit domain 
be consid a such in the domain-independ setting. there be virtual no chang 
in label in the opposit direction, which conform to our expect and provid an 
addit confirm of the reason qualiti of the crowdsourc annotations. 

the polit domain appear most stable, with veri similar percentag of lead judg 
a content-dens in in-domain and gener annotation. We also get some addit evi- 
denc that thi domain be harder to annotate, possibl becaus lead there often mix both 
direct fact and non-liter content. We discuss thi trend in our analysi of the author 

196 



detect content-dens new text 

annot of the domain. In the busi domain, the ratio of lead that chang label 
between the in-domain and gener set be closest to 1, show least bia in perception. 
thi be in stark contrast with polit for example, which be consid more content dens 
in general, attest by both the number of lead that chang label and the percentag of 
lead in the gener set (82%). 

overal the in-domain annot have a more balanc number of content-dens and 
non content dens labels. 

6.2 are lead informative? 

In automat summar research, the articl lead be gener consid to be infor- 
mative, or content-dense. the begin of the articl be know to be a strong summari 
baselin (mani, klein, house, hirschman, firmin, & sundheim, 2002; nenkova, 2005) and 
mani featur for identifi import content in articl be base on overlap with the 
open paragraph. our annot allow u to directli examin to what extent thi 
gener intuit hold across domain of journalist write in the new york times. 

tabl 9 show the number of lead in each domain label a content-dens in the 
manual annot dataset describ above. It be clear that the prevail assumpt 
that the lead of the articl be alway content-dens be not support in the data we analyz 
here. 

the major of articl in the polit domain, which be repres of the data 
on which large-scal evalu of summar system tend to be perform and which 
focu on specif current events, be inde content-dense. more than 60% of lead in thi 
domain be label a content-dens in the authors’ annotation. the trend be similar in the 
amt annotations. 

conform to intuition, the second larg proport of content-dens lead be in the 
busi domain. there the articl be often trigger by current event but here be more 
analysis, humor and creativity. In these lead import inform can often be infer 
but be not directli state in factual form. busi lead also tend to have the same labels, 
regardless of whether they be annot with respect to the domain standard or in general. 
for the busi domain, onli 19 out of 150 label chang across condit (cf. first line 
in tabl 9), which correspond to at least half the rate of label chang for ani of the other 
domains. 

In sport the factual inform in the lead that have to be convey be not much and 
it be embellish and present in a verbos and entertain manner. particularli amt 
annot consid less than a third of the sport lead to be content-dens across domains. 
In the scienc journal section mani lead onli establish a gener topic or an issue, or 
includ a human interest story. overal there be onli a small partit of scienc lead 
label a content-dense. 

the percept of content densiti be certainli influenc by the context of the domain. 
there be 55 polit lead that chang label from in-domain to the gener condition, and 
53 of them be chang from non content-dens to content-dense, indic that in that 
set annot follow their domain bia in decid the label. similarli 38 sport in- 
domain-content-dens lead be non content-dens across domains, but onli 2 lead chang 
in the opposit direction. 

197 



yang & nenkova 

these find have two import implic for languag process applic and 
summar in particular. 

It be unrealist to expect that all newspap text have high inform value. find- 
ing valuabl content have be address a a standalon problem in social medium (becker, 
naaman, & gravano, 2011) and user gener data (agichtein, castillo, donato, gionis, 
& mishne, 2008) but gener have be ignor in news analysis. 

In addition, our analysi cast doubt on the practic of requir summar 
system to produc summari of fix length. mani of the articl with lead that be not 
content-dens do not discu even in the bodi of the articl an event reader would consid 
important. An appropri summari should simpli indic this, or a summari should not 
be even attempted. automat system be anyhow not particularli good at summar 
articl that deal with opinion or discuss rather than a specif event (nenkova & louis, 
2008). In inform access applications, tag the genr of the articl a event-cent 
or not (similar to earli work in distinguish opinion piec from factual reporting, see Yu 
& hatzivassiloglou, 2003) may be most helpful, with preview snippet summari produc 
onli for the event-cent articles. 

6.3 classifi evalu 

here we evalu the combin two-lay classifi train on heurist label data on 
the manual annotations. note that the manual annot lead be use for evalu 
only, no addit train be perform at thi stage. 

follow the assumpt prevail in summar research that the lead of the 
articl be alway content-dense, the first baselin (baseline-1) alway consid the lead of 
the articl content-dense. 

the second baselin (baseline-2) be establish base on the length of the entir news 
article, not onli the lead. the intuit be that longer articl may have uninform lead 
design to draw the reader into the subject while short articl need to start out with a 
more focu present of the event so be like to have an content-dens lead. We train 
a l2-regular logist regress model base on thi singl feature. As tabl 10 shows, 
the singl featur classifi achiev reason accuraci of 68% for the scienc domain. 

6.3.1 classif result on the basic set 

tabl 10 show the result from appli the domain-depend and the gener domain- 
independ model on the basic human annot set. accuraci comput against each 
of the two individu annot be show in the last two columns. sport and polit 
domain have high predict accuraci on the data label by the first annotator, and 
busi and scienc domain have high predict accuraci for the second annotator’ 
labels. also the predict accuraci have small varianc on the data label by the first 
annotator, between 78% for the polit domain and 74% for scienc the domain, compar 
with the accuraci on data label by the second annotator, between 87% for busi 
and 71% for sports. overal howev the predict accuraci on the final combin data, 
after disagr have be adjudicated, be highest, demonstr that the adjud 
procedur do lead to more intern consist labels. As in the heurist label data, 

198 



detect content-dens new text 

recognit accuraci be high for the busi and scienc domain (83%) and low for 
the sport and polit domain (around 80%). 

We also evalu the predict accuraci separ on the subset of the data for 
which the two annot agre on the label in the first stage of independ annotation, 
correspond to the presum clear-cut cases, and those for which adjud be 
needed. clearly, the classifi captur characterist of content-dens lead quit well. 
the accuraci on the subset of the data for which the annot agre be much high 
than that for individu annotators, indic that when the text have mix characterist 
lead to disagr in annotation, it be more like that the classifi make more error 
a well. 

On the agre subset—mark with the same label by both annot dure indepen- 
dent annotation—accuraci be around 90% for the busi and scienc domains, 80% for 
sport and polit domains. 

the classifi accuraci be much high than the baselin for all domains. 

We also calcul the precision, recal and f-score for the content-dens lead class for 
the combin dataset. the result be show in tabl 11. the domain model perform 
best in three of four genres, while the overal gener model lead in politics. thi find 
be again align with what we observ on accuracy. although both model can achiev 
good accuraci on sport leads, the F score in that domain be not a good a in the other 
domains. here both the domain model and overal model can achiev a veri high precis 
but a rel low recall. 

tabl 12 show the correl between the classif score from the final classifi 
and the real-valu score of content-dens by the two annotators. all correl be highli 
statist significant. In line with what we have see in the analysi of other results, the 
correl be the high for the busi domain. 

similarli we comput the predict accuraci stratifi accord to the classifi con- 
fidenc in that prediction. figur 4 show the plot on all four genres. the accuraci of 
high confid predict be much high than the overal accuracy. the ”articl length” 
baseline, however, have low accuraci in it high confid predictions. 

6.3.2 classif result on the amt annot 

tabl 13 show the accuraci and f-score of the domain-depend and the gener domain- 
independ model on the amt annotations. As in previou tables, row 1 and 2 repres 
the result from domain model and the domain-independ model respectively. row 
3 to 4 show result for the two baselines. our classifi outperform the baselin by a 
larg margin except for polit in the domain-independ labels, where the baselin that 
consid all lead to be content dens work best. overal however, the result show that 
the baselin of assum all lead be content-dens perform poorli and the propos 
approach significantli improv the accuracies. 

compar the accuraci of predict for data drawn from the same newspap section, 
it be evid that busi and scienc have the most stabl predict and the accuraci 
of the domain-depend and the domain-independ classifi do not differ much on 
these subset of the test data. the classifi train on domain-independ label achiev 
78.0% accuraci on the domain-specif labels, in which the annot be explicitli told 

199 



yang & nenkova 

tabl 10: binari classif accuracies(%) on basic human annot dataset for model 
train on heurist label data. 

busi combin agre adjud anno 1 anno 2 

domain model 83 94.3 56.7 75 87 
overal model 79 91.4 50.0 75 83 

baseline-1 53 52.8 53.3 47 57 
baseline-2 60 65.7 46.7 58 64 

scienc combin agre adjud anno 1 anno 2 

domain model 83 89.2 65.4 77 80 
overal model 81 89.2 57.7 71 86 

baseline-1 37 31.1 53.8 45 27 
baseline-2 68 69 65.4 62 65 

sport combin agre adjud anno 1 anno 2 

domain model 78 80.8 70.3 74 71 
overal model 75 75.3 74.1 69 68 

baseline-1 49 46.5 55.6 45 50 
baseline-2 65 70 51.9 63 66 

polit combin agre adjud anno 1 anno 2 

domain model 78 83.3 59.1 78 74 
overal model 80 83.3 68.2 76 76 

baseline-1 61 60.3 63.6 55 61 
baseline-2 51 55 36.4 55 53 

tabl 11: [p]recision, [r]ecal and [f]score (%) on basic human annot dataset for mod- 
el train on heurist label data. [d]omain model, [o]veral model, and [b]aseline-2. 

busi scienc sport polit 
P R F P R F P R F P R F 

D 81 88.7 84.7 95.7 57.9 72.1 90.9 50 64.5 95.4 67.7 79.2 
O 76.8 84.3 80.4 91.3 55.2 68.8 86.3 50 63.3 87.3 78.7 82.7 

b-2 61 67.9 64.3 61.5 42.1 50 66.7 57.1 61.5 63 47.5 54.2 

tabl 12: correl between predict probabl and human annot scores. all 
correl be highli signific with p < 0.001. 

annot 1 annot 2 
domain model overal model domain model overal model 

busi 0.621 0.647 0.797 0.810 
scienc 0.575 0.546 0.711 0.758 
sport 0.590 0.575 0.588 0.582 
polit 0.658 0.629 0.609 0.592 

the news section from which the articl be drawn and use thi inform in judg if 
the lead be content-dens or not. thi accuraci be less than 2% low than the predict on 

200 



detect content-dens new text 

figur 4: predict accuraci base on probabl rank on basic human annot 
set. [top left] business, [top right] science, [bottom left] sports, [bottom right] 
polit 

domain-independ labels. similarli in the scienc domain the differ in perform 
for the two type of label be a low a 2.0%. In stark contrast, there be larg differ 
in perform for the in-domain and domain-independ label for sport and politics, 
where the differ between the two reach 10%. 

the crowdsourc annot be perform both in-domain (judg the content 
densiti with respect to the expect for the give domain, polit for example) and in 
domain independ setting. here domain model be on averag bad than the domain 
independ models. for the sport domain, train a domain-specif classifi help 
most in improv the detect of content-dens sport lead but for the other domain 
the advantag be less clear. thi result be reassuring. If the domain model be clearli 
superior, one would have need accur domain predictor for practic applications. the 
analysi present here demonstr that a domain-independ classifi may be suffici 
for mani applications. 

the accuraci of the domain model drop consider compar to their respect 
accuraci on the author-annot set. for example, there be around 8.0% drop in the 
polit domain. there be sever possibl reason for thi difference. the articl in the 
initi set that the author annot be select onli from the articl publish in 2005 
and 2006 while the amt set be select from the entir nyt dataset from 1987 to 2007. the 
annot instruct also differ for the two sets. the amt annot be present 
with prototyp content-dens and non content-dens lead a references, while the author 

201 



yang & nenkova 

have onli one lead in the middl of the rang of content-dens a reference. finally, the 
gener domain-independ classifi on averag work best, predict both the in-domain 
and gener label in the test set good than the domain-depend classifiers. thi trend 
indic that amt worker be like more influenc by gener domain expect 
when label the data. It be plausibl that domain-depend annot requir more 
detail instruct that be not a readili pass on in the crowdsourc setting. 

tabl 13: binari classif result on amt annot dataset for model train on 
heurist label data: [a]ccuracies(%) and [f]scores(%) 

in-domain busi scienc sport polit averag 
A F A F A F A F A F 

domain model 76.0 78.3 72.7 70.9 73.3 69.5 70.7 73.5 73.2 73.5 
gener model 78.0 80.4 76.7 74.8 70.0 63.1 70.0 73.3 73.7 73.9 

baseline-1 62.0 – 42.7 – 51.1 – 48.0 – 55.0 – 
baseline-2 58.0 62.7 64.7 40.0 62.7 51.2 52.7 48.5 59.5 61.4 

domain indep. busi scienc sport polit averag 
A F A F A F A F A F 

domain model 79.3 81.4 73.3 74.0 80.0 62.8 76.7 83.8 77.3 75.5 
gener model 77.3 80.4 78.7 78.4 82.0 67.6 77.3 84.5 78.8 77.7 

baseline-1 66.0 – 48.7 – 26.7 – 82.0 – 55.9 – 
baseline-2 62.7 68.6 66.7 62.8 68.0 53.3 50.7 61.1 62.0 61.7 

We further comput the correl coeffici between predict probabl and av- 
erag score annot by amt workers. the result be show in tabl 14. domain model 
have good correl than gener model in three of domain for domain depend (in- 
domain) labels, but with small absolut differ in correlation. the domain-independ 
model be much good in predict content-dens in the general, domain-independ 
condition. all correl be highli significant, rang from 0.577 to 0.661 against in- 
domain label and from 0.602 to 0.730 against domain-independ labels. As in the binari 
predict task, the domain-independ label appear to be easi for the system to predict. 
the correl coeffici be in line with our intuit and much closer to the number we 
have see base on the basic author-annot set (shown in tabl 12). thi trend impli 
that predict content-dens in term of real-valu score may be more suitabl for thi 
task. 

tabl 14: correl between predict probabl and averag score annot by amt 
worker in the domain specif and gener condition. all correl be highli signific 
with p < 0.001. 

in-domain label domain independ label 
domain model gener model domain model gener model 

busi 0.602 0.614 0.713 0.730 
scienc 0.661 0.646 0.652 0.690 
sport 0.600 0.577 0.619 0.602 
polit 0.616 0.615 0.652 0.668 

202 



detect content-dens new text 

for the amt annot test set, we also comput the predict accuraci stratifi 
accord to percentil of data rank by the classifi confid in that prediction. fig- 
ure 5 and 6 show the plot on all four domain for the two type of annot label 
(domain-specif or domain-independent). again, the accuraci of high confid predic- 
tion be much high than the overal accuracy. the articl length baseline, however, have 
much low accuracies. 

figur 5: predict accuraci base on probabl rank on amt annot data. the 
x axi repres the percentil of data use to calcul the accuraci accord to the 
predic confidence. (in-domain): [top left] business, [top right] science, [bottom 
left] sports, [bottom right] polit 

7. recogn better summari 

So far we have demonstr that detector of content-dens can be develop use heuris- 
tical label data and that it can achiev respect accuraci in intrins evalu on 
human-label leads. ultim howev the goal would be to integar the content-dens 
predict in inform seek applic such a summar and news browsing. 
test the impact of the content-dens predict in such extrins evalu will be 
the main focu of futur work. 

here, however, we show a feasibl studi to verifi the potenti for develop of more 
inform summar method that exploit the concept of content density. specifically, 
we demonstr that the content-dens detector be abl to recogn when an automat 
summari of a singl news articl be good than the lead of the article. thi be an import 
open problem in summarization, where the lead paragraph baselin be veri strong and few 

203 



yang & nenkova 

figur 6: predict accuraci base on probabl rank on amt annot data. the 
x axi repres the percentil of data use to calcul the accuraci accord to the 
predic confidence. (domain independent): [top left] business, [top right] science, 
[bottom left] sports, [bottom right] polit 

system outperform it (over, dang, & harman, 2007). moreover, all of the proof-of-concept 
experi from the previou section be perform on data drawn from the nyt. We 
would like to verifi that the use of the predict remain when data from other 
sourc be considered. 

motiv by these goals, we perform our experi on detect if a machin sum- 
mari be more inform than the lead on two datasets: nyt and on data from the 
document understand conference, which have data from a varieti of sources. 

We randomli select 400 articl with manual summari from the nyt, 100 for each 
genre. We gener automat summari for the articl use two systems. the first 
system, leadsumm, be the strong lead baselin which pick the first 100 word a the 
summary. the second system be icsisumm (gillick & favre, 2009), which be one of the 
state-of-the-art multi-docu summar system (gillick, riedhammer, favre, & 
hakkani-tur, 2009; berg-kirkpatrick, gillick, & klein, 2011). 

We perform human evalu to determin which of the two summari in the pair 
(leadsumm and icsisumm) be better. We ask annot to first read the manual sum- 
mari from nyt, then read the two summari gener by the systems. then we ask 
the annot to indic which of the two system summari cover good the inform 
express in the nyt goldstandrd. they be also provid with an option to indic that 
the two system summari cover the inform express in the goldstandard equal well. 

204 



detect content-dens new text 

the flow between the sentenc in the leadsumm summari be good than those in the 
automat summari becaus thi be a snippet of profession write discourse. here our 
goal be to studi the content-dens of the two summaries, independ of the linguist 
qualiti of the summari which we know favor the lead system. for thi reason, we random- 
ize the order of the sentenc in both of the leadsumm and icsisumm summaries. the 
order of present the leadsumm and icsisumm to annot be also random dure 
judgement collection. 

the task be publish on amazon mechan turk (amt) and each task be assign 
to 10 annotators, with one summari per task/hit. icsisumm gener empti summari 
for 77 out of the 400 randomli select articl and two of it summari be ident to 
the lead baseline. We remov those task so there be total 323 task be published. the 
major vote of the 10 annot be use a the final label. the human annot be 
use a ground truth in the follow steps. 

next we appli the content-dens detector on the gener icsisumm and leadsumm 
to get content-dens probabl score for each. the summari with high content-dens 
score be predict to be the good summary. As expect from prior manual share task 
evaluations, leadsumm be good then icsisumm for most of the articles. 

the confid in the predict that one summari be good than anoth be control 
by the content-dens score difference. the larg the differ between the content-dens 
score of the icsisumm and leadsumm is, the more confid we can be that the summari 
be inde better. We track how the summar perform vari with the differ in 
content-dens scores. In case when the differ between the two content-dens score 
be low than a set value, we consid that the lead summari be better. By defin the 
score differ = scoreicsisumm − scoreleadsumm, we cutoff the evalu sampl by 
score differ comput the metric for differ cutoff levels. 

tabl 15 show the result of detect when icsisumm produc good summari than 
the lead baselin on the nyt articles. thi be equival to a combin system which 
us lead summari unless it be confid that the automat summari be better, in which 
case it us the icsisumm summary. the first column repres the cutoff valu and the 
second column show the number of total sampl within thi cutoff. column 3 to 5 be 
the statist of human judgements. the last column show the number (percentage) of 
correctli predict sampl of the combin system. each row show the result of 
the system with a cutoff value. the last row show the statist for the entir dataset 
and two baselines, one that pick icsisumm summari onli and anoth that pick lead 
summari only. the lead be good for 59% of the test articles; the icsisumm produc 
the more inform summari for 34% of the test articles. the two summari be 
consid equal inform in the rest of the cases. clearly, a expect from past duc 
evaluations, the lead baselin summari be good than the automat summarizer. howev 
even an extract summar can significantli outperform the lead baselin if we have a 
reliabl way in which to predict when an altern summari would be more informative; 
thi could improv one out of each three summari produc by the summarizer. 

last column show the perform of a combin system use content-dens score 
to decid which of the two avail summari be superior. whenev at least some threshold 
be use to decid when the automat summari be better, the combin system’ output 
be prefer by the assessor consider more often than the output for the lead baseline. 

205 



yang & nenkova 

particularli for threshold between 0.1 and 0.4, the output of the combin system be 
prefer between 64 and 66% of the time, compar to the 58.5% for the lead baseline. 
these improv be statist signific (with p < 0.05) accord to a binomi 
test with expect probabl of produc good summari of 0.585, correspond to the 
human prefer for the baselin lead summaries. 

tabl 15: perform of combin system with differ cutoff on nyt articles. the 
last column show the number (percentage) of correct predict sampl of the combin 
system. 

cutoff # of sampl 
human judgement 

icsisumm tie leadsumm combin system 

0.5 18 14 0 4 199 (61.6%) 
0.4 29 23 1 5 207 (64.1%) 
0.3 42 32 2 8 213 (65.9%) 

nyt 0.2 54 39 2 13 215 (66.6%) 
0.1 79 47 4 28 208 (64.4%) 
0 179 78 7 94 173 (53.6%) 

all 323 109 (33.7%) 25 189 (58.5%) n/a 

next we verifi that the propos model work with reason accuraci on sourc 
other than the nyt. We run the same leadsumm and icsisumm system on the duc 
dataset (over et al., 2007). We onli perform the experi on the data from duc2002, 
which be the last year nist provid single-docu human summaries. 

there be total of 533 articl from variou sourc in duc2002, includ associ 
press (ap), wall street journal (wsj), lo angel magazin (la), FT magazin (ft), 
san jose mercuri new (sjmn) and foreign broadcast inform servic daili report 
(fbis). AP be a newswirs service, provid high-qual news report use by mani 
medium outlets. By the natur of newswir services, the AP articl (and leads) be expect 
to be contain a larg propartit of content-dens texts. the other articl sourc be 
drawn from newspapers, so be much more like to includ lead that be not content- 
dense. 

again, we filter out articl for which icsisumm summari could not be gener or 
for which the automat and lead summari be identical. again, we exclud from consid- 
erat articl for which icsisumm do not gener a summari or for which icsisumm 
produc a summari consist of the lead of the article. after filter these, we obtain 
493 articl for the evaluation. 

As with the nyt experiment, we use amt to obtain judgement about which of the 
two summari of the articl be better. all the annot set be exactli the same 
with the nyt annot describ above. 

again, we then appli content-dens detector on the gener icsisumm and leadsumm 
to detect which summari be good base on their content-dens scores. tabl 16 show the 
result of detect good icsisumm on the duc2002 articles. 

the judgement on data drawn from sourc differ from the nyt allow u to get a 
sens about the extent to which the content-dens detector we develop to the news genr 
in gener rather than specif to the nyt. the observ that bear special mention be 

206 



detect content-dens new text 

tabl 16: perform of combin system with differ cutoff on duc2002 articles. 
the last column show the number (percentage) of correct predict sampl of the combi- 
nation system. 

cutoff # of sampl 
human judgement 

icsisumm tie leadsumm combin system 

0.5 5 4 0 1 246 (78.6%) 
0.4 13 9 0 4 248 (79.2%) 

duc 0.3 22 11 1 10 244 (78.0%) 
AP 0.2 38 15 1 22 236 (75.4%) 

0.1 72 21 2 49 215 (68.7%) 
0 117 30 5 82 191 (61.0%) 

all 313 51 (16.3%) 19 243 (77.6%) n/a 

0.5 4 3 0 1 134 (74.4%) 
0.4 6 5 0 1 135 (75.0%) 

duc 0.3 8 6 1 1 136 (75.6%) 
other 0.2 9 6 2 1 136 (75.6%) 

0.1 18 9 4 5 135 (75.0%) 
0 48 19 5 27 123 (68.3%) 

all 180 41 (22.7%) 8 131 (72.8%) n/a 

that here, the percentag of articl for which the icsisumm be abl to produc more infor- 
mativ summari than the lead summar be consider small than in the randomli 
select sampl of nyt articles. for the AP articles, the automat summari be judg 
a good than the lead baselin for 16% of the test articles. thi conform with expect 
that the AP articl and lead will be overal more content-dens than regular newspap 
sources. for the other sourc (newspaper), the percentag of automat summari that 
be good than the lead be 23%. for comparison, in the nyt sampl the system produc 
a more inform summari in 34% of the cases. thi larg percentag may reflect the 
style of the new york time or the fact that the articl from nyt be randomli drawn, 
so cover a broader rang of domain than the duc data. 

the number indic that the style of AP be the most typic inform while the 
nyt be the most stylist rich, with lead that be often not content-dense. If thi be the 
case, the room for expedit news search and brows via automat summar have 
be underestim in duc evaluations. 

the last column show the perform of the combin system. the perform 
on duc other be still similar to the perform on nyt. just pick ani cutoff ≥ 0.1 
will lead a perform improvement. however, the combin system be onli good than 
lead baselin for AP when the cutoff ≥ 0.3 and reach the best perform when set 
cutoff to 0.4, which be align with previou find that AP be typic informational. 

overall, the combin system perform good than baselin system when we pick 
the right cutoff. the choic of cutoff depend on the sourc type and the reason be the 
write style can be veri differ in differ sourc a discuss above. for nyt and 
duc newspap sourc (exclud ap), the combin system be abl the achiev good 

207 



yang & nenkova 

perform when set cutoff a 0.1 and achiev high accuraci when set cutoff 
a 0.2. for the ap, however, it be much harder to find a cut off in which the combin 
system would outperform the lead baseline. the cutoff have to be set to 0.4 to get the best 
performance, so the gener abil to produc a good summari with AP data be dubious. 

thi analysi of cut-off at which predict of content densiti would improv summa- 
rizat be onli a pilot analysis. ideally, we would need suffici data to have a dedic 
develop set on which to determin the cut-off and an independ test set on which 
to verifi it utility. such in-depth studi be left for futur work. here howev we note that 
there be a clear differ in the perform for the stylist differ newswir and 
newspap and that there be suffici evid for potenti of use the content-dens 
stylist distinct for improv singl document summarization. 

8. conclus and futur work 

In thi paper we introduc the task of detect content-dens news articl leads. We use 
article/summari pair from the nyt corpu to heurist label a larg set of articl a 
content-dens when the lead of the articl overlap highli with the human summari and a 
non content-dens when the overlap be low. 

We present experi with two lexic represent and one syntact representa- 
tion. the product rule syntact represent be the best predictor of lead content- 
densiti among the three. the corpus-independ lexic represent from a vocabulari 
defin by the mrc lexicon prove to be the more use lexic representation. We com- 
par a feature-level combin model and a two-lay decision-level combin model. 
the latter perform best in all our experiments. 

our analysi reveal that there be a larg variat across news domain in the fraction 
of content-dens lead and in the predict accuraci that can be achieved. contrari to 
popular assumpt in news summarization, we find that a larg fraction of lead be in 
fact not content-dens and thu do not provid a satisfactori summary. 

overal domain-specif model be more accur than in-domain label from train 
annotators. the gener model train on all data pool togeth achiev good perfor- 
manc on crowdsourc annot in both domain-depend and domain independ 
annot conditions. our experi indic that predict content-dens in term of 
real-valu score may be more accur and benefici for applic than simpli classify- 
ing a lead a content-dens or not. 

In thi work, we have establish the feasibl of the task of detect content-dens 
texts. We have confirm that the automat annot of data captur distinct in 
inform a perceiv by people. We also show proof-of-concept experi that 
show how the approach can be use to improv single-docu summar of news 
and the gener of summari snippet in news-brows applications. In futur work the 
task can be extend to more fine-grain levels, with predict on sentenc level and the 
predictor will be intergar in a fulli function summar system. 

all data for the work present in thi paper and the domain-depend and gener 
classifi will be make publicli with the public of thi article. 

208 



detect content-dens new text 

acknowledg 

We report our initi work on detect of information-dens text in a paper publish at 
aaai 2014 (yang & nenkova, 2014) In thi manuscript we have further extend the work 
by use much larg sampl of new york time articl for train and by analyz 
the perform of a larg number of two-lay classifiers. here we also introduc a new 
collect of manual annot test data, for both domain-depend and gener content- 
densiti of texts. We make use of thi collect for detail evalu of the content-dens 
classifiers. In our aaai 2014 paper, we use the term information-dens to describ the 
type of text we wish to detect. here we switch the terminolog to content-dense, to avoid 
confus with work in the intersect of cognit scienc and comput linguist 
that us the term inform densiti in information-theoret sens to describ the chang 
in surpris in the linear process of sentenc (jaeger, 2010; pate & goldwater, 2015). 

appendix A. refer lead use in amt annot 

here we present all the refer lead annot saw in amt human intellig task 
(hits). 

a.1 in-domain refer lead 

In in-domain annotations, annot label a group of five lead from same domain in 
each hit. two content-dens lead and two non content-dens lead from the same domain 
be display at the beginning. 

a.1.1 refer lead for busi domain 

[content-dens ref 1] secur regul charg one of the richest men in mexico, 
ricardo B. salina pliego, with fraud yesterday, in a lawsuit that seek to have him bar 
a a director or offic of ani compani whose share trade on an american exchange. 

the secur and exchang commiss also sought to have mr. salina pliego, the 
chairman of TV azteca, the second-biggest spanish-languag broadcaster, give up more 
than $110 million he make from trade in the company’ stock and debt. 

[content-dens ref 2] In a rare move, microsoft say yesterday that it have agre 
to pay a percentag of the sale of it new portabl medium player to the univers music 
group. 

univers music, a unit of vivendi, will receiv a royalti on the zune player in exchang 
for licens it record for microsoft’ new digit music service, the compani said. 

[non content-dens ref 1] look for some thong underwear or perhap a leather 
jacket and don’t know where to find them? tri log on to a restaur web site. 

small restaurateur be increasingli use the internet to sell good that go far beyond 
the usual array of brand t-shirt and hats, in hope of not just build the bottom line, 
but also cultiv possibl new market for expansion. 

209 



yang & nenkova 

[non content-dens ref 2] ”what stress me most,” the chief execut of novartis, 
daniel L. vasella, said, ”i that we be get new regul from abroad without ani 
consultation.“ 

thi have be the world econom forum that the unit state govern larg 
pass by. In a world that both respect and fear american power, there be worri that the 
unit state do not care what other think. 

a.1.2 refer lead for scienc domain 

[content-dens ref 1] scientist have decod the chimp genom and compar it 
with that of humans, a major step toward defin what make peopl human and develop 
a deep insight into the evolut of human sexual behavior. 

the comparison pinpoint the genet differ that have arisen in the two speci 
sinc they split from a common ancestor some six million year ago. 

[content-dens ref 2] A popular class of drug for high blood pressure, ace in- 
hibitors, may caus birth defect if take dure the first three month of pregnancy, doc- 
tor be reporting. pregnant woman and those who be plan to becom pregnant should 
avoid the drugs, the research and offici at the food and drug administr warn. 

ace inhibitor have long be know to caus birth defect if take late in pregnancy, 
but until now be consid safe if take in the first trimester. 

[non content-dens ref 1] To gaug the potenti consum impact of the consoli- 
dation sweep the telephon industry, look no further than the silver-ton plastic phone 
gather dust on the desk in justin martikovic’ studio apartment. 

mr. martikovic, 30, a junior architect who reli on a cellphon for hi normal calling, 
say he never us the desk phone – but he pay $360 a year to keep it hook up. 

[non content-dens ref 2] As the horror of the south asian tsunami spread and 
peopl gather onlin to discu the disast on site know a web logs, or blogs, those 
of a polit bent natur turn the discuss to their favorit topics. 

To some in the blogosphere, it simpli have to be the government’ fault. 

a.1.3 refer lead for sport domain 

[content-dens ref 1] ivor G. balding, one of three british brother who gain 
intern fame a polo star in the 1930’s, when the sport attract larg crowd and 
wide press coverage, die on thursday at hi home in camden, s.c. He be 96. 

hi death be announc by hi family. 

[content-dens ref 2] finally, the deal be done. 

laveranu coles, the wide receiv from the washington redskins, pass a physic 
examin by the jets’ medic staff yesterday, clear the way for the team to reacquir 
him in a trade for wide receiv santana moss. 

[non content-dens ref 1] nearli 36 year ago, when it be hi turn to interview 
the prospect employee, the estim jame reston, onetim travel secretari for the 

210 



detect content-dens new text 

cincinnati red but then the execut editor of thi newspaper, ask how a polit 
scienc major have wound up write about sports. 

I answer the question, but I have a good answer now. the polit scienc class 
prepar me for the nonsens that will pa for a hear about steroid use in basebal 
next thursday in washington. 

[non content-dens ref 2] three year ago, a he stood in the rubbl of the st. 
bonaventur basketbal program, ahmad smith have a decis to make. 

one of hi teammates, center jamil terrell, have be declar inelig after it be 
learn that he have be admit to the franciscan univers in the hill of southwestern 
new york with a weld certif – and the approv of st. bonaventure’ president. 

a.1.4 refer lead for polit domain 

[content-dens ref 1] At least 844 american servic member be kill in iraq in 
2005, nearli match 2004’ total of 848, accord to inform releas by the unit 
state govern and a nonprofit organ that track casualti in iraq. 

the death of two american announc by the unit state militari on friday – a 
marin kill by gunfir in falluja and a soldier kill by a roadsid bomb in baghdad – 
brought the total kill sinc the war in iraq begin in march 2003 to 2,178. the total 
wound sinc the war begin be 15,955. 

[content-dens ref 2] seventeen peopl die in two separ violent incid on 
sunday and monday that underscor an increas sens of lawless in mexico. 

A former soldier go on a rampag in a pacif coast town on sunday, kill 12 peopl 
befor local resid chase him down and the polic shot him in the town square. thirteen 
hour later, gunman attack gambler at an illeg cockfight at a guadalajara racetrack, 
kill 4 and wound 27 when they toss two grenad into the crowd. 

[non content-dens ref 1] presid bush on tuesday press senat republican 
leader to continu fight to confirm john R. bolton a ambassador to the unit nations, 
even though senat bill frist, the major leader, say hi option have be exhaust 
and some republican urg the appoint of mr. bolton when congress recesses. 

”the presid make it veri clear that he expect an up-or-down vote,“ dr. frist told 
report after meet with the president. back in the capitol, he added, ”I don’t want to 
close that door yet.“ 

[non content-dens ref 2] are thing get good or bad in iraq? that be the 
basic question, on which much hing for the unit state and the world. here be some 
impressionist answers. 

just over a year ago, on my last visit to the country, I be abl to drive north to tikrit, 
saddam hussein’ home town, and south to the shiit holi citi of najaf. these be not 
excurs for sit back and enjoy the scenery. but they be feasible, at high speed 
and with some risk. 

211 



yang & nenkova 

a.2 domain-independ annot 

In domain-independ annotations, annot be give a group of five lead randomli 
select from all domains. two inform lead and two uninform lead be give a 
references. 

[content-dens ref 1] secur regul charg one of the richest men in mexico, 
ricardo B. salina pliego, with fraud yesterday, in a lawsuit that seek to have him bar 
a a director or offic of ani compani whose share trade on an american exchange. 

the secur and exchang commiss also sought to have mr. salina pliego, the 
chairman of TV azteca, the second-biggest spanish-languag broadcaster, give up more 
than $110 million he make from trade in the company’ stock and debt. 

[content-dens ref 2] In a rare move, microsoft say yesterday that it have agre 
to pay a percentag of the sale of it new portabl medium player to the univers music 
group. 

univers music, a unit of vivendi, will receiv a royalti on the zune player in exchang 
for licens it record for microsoft’ new digit music service, the compani said. 

[non content-dens ref 1] look for some thong underwear or perhap a leather 
jacket and don’t know where to find them? tri log on to a restaur web site. 

small restaurateur be increasingli use the internet to sell good that go far beyond 
the usual array of brand t-shirt and hats, in hope of not just build the bottom line, 
but also cultiv possibl new market for expansion. 

[non content-dens ref 2] As the horror of the south asian tsunami spread and 
peopl gather onlin to discu the disast on site know a web logs, or blogs, those 
of a polit bent natur turn the discuss to their favorit topics. 

To some in the blogosphere, it simpli have to be the government’ fault. 

appendix B. product rule with highest weight 

In thi section we list the product rule with high weight for each genre. We also 
show two exampl for each product rule. the exampl be extract from lead text 
use stanford corenlp package. 

212 



detect content-dens new text 

tabl 17: top 10 product rule with exampl for busi 

posit rule 
+vp->vb NP prt advp 

1) VP ->vb[scare] np[them] prt[away] advp[al over again] 
2) VP ->vb[push] np[the czech currency] prt[up] advp[sharply] 
+vp->vbg PP S 

1) VP ->vbg[boasting] pp[on line about their incent packages] s[to attract compani to reloc to 
their areas] 
2) VP ->vbg[looking] pp[for fact about differ regions] s[to get inform that onli use to be 
avail , if at all , through the mail and in-person visits] 
+np->nn 

1) NP ->nn[response] 
2) NP ->nn[overdrive] 
+vp->adjp vbg NP PP 

1) VP ->adjp[tough] vbg[protecting] np[american industry] pp[from unfair trade practices] 
2) VP ->adjp[sometim heated] vbg[questioning] np[tuesday] pp[from member of a hous 
subcommittee] 
+ np->dt nnp 

1) NP ->dt[the] nnp[i.m.f.] 
2) NP ->dt[the] nnp[f.d.a.] 

neg rule 
− np->jj CD nn 

1) NP ->jj[pre-april] cd[15] nns[blues] 
2) NP ->jj[past] cd[150] nns[degrees] 
− vp->vbn PP np-tmp PP 

1) VP ->vbn[injured] pp[in a car crash in peru , a third weathered] np-tmp[a summer] pp[in 
pakistan in brutal 117-degre heat] 
2) VP ->vbn[swayed] pp[down the wet black runway at the alexand mcqueen fashion show last 
thursday] np-tmp[night] pp[to an omin disco] 
− advp->rbr RB PP 

1) advp ->rbr[more] rb[often] pp[than not] 
2) advp ->rbr[more] rb[often] pp[than not] 
− vp->vbz : NP 

1) VP ->vbz[war] :[:] np[ha newsweek ’s time final come] 
2) VP ->vbz[is] :[:] np[now what] 
− vp->vbd advp np-tmp , NP 

1) VP ->vbd[fell] advp[sharply] np-tmp[yesterday] ,[,] np[the fourth consecut declin , a 
concern about inflat and interest rate grow befor today ’s report on produc prices] 
2) VP ->vbd[opened] advp[here] np-tmp[friday] ,[,] np[anoth sign of how compani all over the 
world be still rush to do busi in china] 

213 



yang & nenkova 

tabl 18: top 10 product rule with exampl for scienc 

posit rule 
− qp->jjr IN NP 

1) QP ->jjr[more] in[than] np[the vast majority] 
2) QP ->jjr[more] in[than] np[a jubil return] 
− adjp->adjp sbar 

1) adjp ->adjp[less like than other to have child , and those who do give birth run an 
increas risk of bear a child with the same birth defect] sbar[that they themselv have] 
2) adjp ->adjp[far less successful] sbar[than expected] 
− np->dt nnp nn NN 
1) NP ->dt[a] nnp[federal] nns[appeals] nn[court] 
2) NP ->dt[a] nnp[texas] nns[appeals] nn[court] 
− s->frag NP VP . 

1) S ->frag[in] np[old] vp[soul : the scientif evid for past live , ” -lrb- simon 
schuster , 1999 -rrb- tom shroder , a washington post editor , review the 80-year-old clinic 
psychiatrist ’s research on reincarn and find it hard to refute] .[.] 
2) S ->frag[tonight , when] np[live from lincoln center ”] vp[broadcast a concert by the new 
york philharmon on pb station across the countri , the announc will not be say anyth 
about the person stori of the bass-bariton thoma quasthoff , who will sing four concert aria by 
mozart] .[.] 
− np->prp$ nn NN 

1) NP ->prp$[their] nns[doctors] nn[charge] 
2) NP ->prp$[their] nns[employees] nn[home] 

neg rule 
− vp->vbg NP PP PP 

1) VP ->vbg[ordering] np[a cup of coffee] pp[at starbucks] pp[into an olymp challenge] 
2) VP ->vbg[taking] np[a crack] pp[at hi plays] pp[in the form of faith reviv or loos 
interpretations] 
− vp->vb NP advp , sbar 

1) VP ->vb[get] np[both hi legs] advp[amputated] ,[,] sbar[even though they have be perfectli 
healthy] 
2) VP ->vb[use] np[her niec ’s card] advp[here] ,[,] sbar[sinc she do n’t live in westchester] 
− vp->vbn NP , advp PP 

1) VP ->vbn[triggered] np[copycats] ,[,] advp[sometimes] pp[bi the dozens] 
2) VP ->vbn[been] np[7,000 case of leprosi in thi countri over the previou three years] ,[,] 
advp[far more than] pp[in the past] 
− np->np , NP CC NP 

1) NP ->np[social x-rays] ,[,] np[those rail-thin woman who have attain the exalt statu that 
come from be marri to a master-of-the-univers invest banker] cc[or] np[lawyer] 
2) NP ->np[your new book] ,[,] np[evolut ’s rainbow : divers , gender and sexual in nature] 
cc[and] np[people] 
− sbar->sbar , RB sbar 

1) sbar ->sbar[al about whom we could persuad to hire us] ,[,] rb[not] sbar[whom we would 
deign to work for] 
2) sbar ->sbar[when they be fine] ,[,] rb[only] sbar[when they be muck up or obscure] 

214 



detect content-dens new text 

tabl 19: top 10 product rule with exampl for sport 

posit rule 
+ whnp->wp$ NN NN 
1) whnp ->wp$[whose] nn[baseball] nn[career] 
2) whnp ->wp$[whose] nn[return] nn[date] 
+ vp->vbd prt , S 

1) VP ->vbd[left] prt[off] ,[,] s[decis win the featur copley cup race of the 27th annual 
san diego crew classic yesterday for the second consecut year] 
2) VP ->vbd[lashed] prt[out] ,[,] s[accus the leagu of racism] 
+ np->cd JJ JJ NN NN 

1) NP ->cd[one] jj[infamous] jj[dining] nn[hall] nn[brawl] 
2) NP ->cd[seven] jj[consecutive] jj[first-round] nn[playoff] nn[series] 
+ vp->advp vbd NP PP sbar 

1) VP ->advp[out 95 second into the first round and golota] vbd[left] np[the arena] pp[in an 
ambulance] sbar[aft he lose conscious in hi locker room after the fight] 
2) VP ->advp[quickly] vbd[switched] np[him] pp[to second base] sbar[becaus chuck knoblauch 
could not throw straight] 
+ advp->jj 

1) advp ->jj[next] 
2) advp ->jj[free] 

neg rule 
− np->np , CC NP , PP 

1) NP ->np[bob brenli ’s use] ,[,] cc[or] np[overuse] ,[,] pp[of curt schilling] 
2) NP ->np[vt.] ,[,] cc[minus] np[a number of player still particip in the world cup] ,[,] 
pp[includ gretzki , who have be team canada ’s best player but be also the ranger ’ big 
question mark] 
− np->dt MD CD NN 

1) NP ->dt[a] md[may] cd[31] nn[deadline] 
2) NP ->dt[a] md[march] cd[4] nn[night] 
− np->np JJ nnp NN NN 

1) NP ->np[maryland ’s] jj[first] nnp[a.c.c.] nn[tournament] nn[championship] 
2) NP ->np[the year ’s] jj[first] nnp[grand] nn[slam] nn[tournament] 
− np->prp$ 

1) NP ->prp$[his] 
2) NP ->prp$[its] 
− xs->jj IN 

1) XS ->jj[much] in[over] 
2) XS ->jj[further] in[than] 

215 



yang & nenkova 

tabl 20: top 10 product rule with exampl for polit 

posit rule 
+ np->dt nnp : nnp nnp 
1) NP ->dt[the] nnp[editor] :[:] nnp[philip] nnp[gourevitch] 
2) NP ->dt[the] nnp[editor] :[:] nnp[henry] nnp[siegman] 
+ np->dt vbg nnp nnp 

1) NP ->dt[the] vbg[collapsing] nnp[soviet] nnp[union] 
2) NP ->dt[the] vbg[ruling] nnp[communist] nnp[party] 
+ vp->vbg NP prt sbar 

1) VP ->vbg[propelling] np[a civic debate] prt[over] sbar[wheth to chang the way american 
experi and ultim build urban public spaces] 
2) VP ->vbg[provoking] np[a debate] prt[about] sbar[wheth american court would repeat the 
kind of rule that restrict the civil right of japanese-american dure world war ii] 
+ vp->vp CC VP S 

1) VP ->vp[are be held in banco delta asia in macao] cc[and] vp[are] s[to be transfer to a 
north korean account at the bank of china] 
2) VP ->vp[said the contact be informal] cc[and] vp[had no bear on the efforts] s[to help him 
settl in panama] 
+ advp->advp CC advp 

1) advp ->advp[at least anoth week] cc[and] advp[perhap longer] 
2) advp ->advp[far enough] cc[and] advp[wel enough] 

neg rule 
− adjp->jj CC RB JJ 

adjp ->jj[important] cc[but] rb[relatively] jj[routine] 
adjp ->jj[tragic] cc[but] rb[not] jj[surprising] 
− advp->dt RP 

advp ->dt[all] rp[over] 
advp ->dt[all] rp[around] 
− np->np NN PP S 
NP ->np[asmat ali janbaz ’s] nn[explanation] pp[for the american militari helicopters] s[fli over 
thi isol mountain valley last thursday afternoon] 
NP ->np[the chines govern ’s] nn[use] pp[of militari force] s[to suppress the 1989 tiananmen 
demonstrations] 
− sbar->whadjp S 

sbar ->whadjp[exactli what] s[you be do when you heard that franklin D. roosevelt have 
die , or that john F. kennedi have be shot , or that martin luther king jr. be dead] 
sbar ->whadjp[how delightful] s[it must be these day to be a member of the chines communist 
politburo] 
− np->vbn nnp nn 

NP ->vbn[suspected] nnp[qaeda] nns[members] 
NP ->vbn[suspected] nnp[taliban] nns[fighters] 

216 



detect content-dens new text 

refer 

agichtein, e., castillo, c., donato, d., gionis, a., & mishne, G. (2008). find high- 
qualiti content in social media. In proceed of the 2008 intern confer 
on web search and data mining, pp. 183–194. acm. 

ashok, V. g., feng, s., & choi, Y. (2013). success with style: use write style to predict 
the success of novels. In proceed of the 2013 confer on empir method 
in natur languag processing, emnlp 2013, 18-21 octob 2013, grand hyatt 
seattle, seattle, washington, usa, A meet of sigdat, a special interest group 
of the acl, pp. 1753–1764. 

bard, E. g., robertson, d., & sorace, A. (1996). magnitud estim of linguist ac- 
ceptability. language, 72 (1), pp. 32–68. 

becker, h., naaman, m., & gravano, L. (2011). beyond trend topics: real-world event 
identif on twitter.. icwsm, 11, 438–441. 

berg-kirkpatrick, t., gillick, d., & klein, D. (2011). jointli learn to extract and com- 
press. In proceed of the 49th annual meet of the associ for comput 
linguistics: human languag technologies-volum 1, pp. 481–490. associ for 
comput linguistics. 

bertolami, r., & bunke, H. (2006). earli featur stream integr versu decis level 
combin in a multipl classifi system for text line recognition. In pattern recog- 
nition, 2006. icpr 2006. 18th intern confer on, vol. 2, pp. 845–848. 

chang, c.-c., & lin, c.-j. (2011). libsvm: A librari for support vector machines. acm 
transact on intellig system and technology, 2, 27:1–27:27. softwar avail 
at http://www.csie.ntu.edu.tw/ cjlin/libsvm. 

cook, p., & hirst, G. (2013). automat assess whether a text be clichéd, with 
applic to literari analysis. proceed of naacl hlt 2013. 

danescu-niculescu-mizil, c., kossinets, g., kleinberg, j., & lee, L. (2009). how opinion 
be receiv by onlin communities: A case studi on amazon.com help votes. 
In proceed of www, pp. 141–150. 

de marneffe, m., manning, C. d., & potts, C. (2012). did it happen? the pragmat 
complex of verid assessment. comput linguistics, 38 (2), 301–333. 

feng, x., huang, l., tang, d., ji, h., qin, b., & liu, T. (2016). A language-independ 
neural network for event detection. In proceed of the 54th annual meet of 
the associ for comput linguistics, acl 2016, august 7-12, 2016, berlin, 
germany, volum 2: short papers. 

ganjigunt ashok, v., feng, s., & choi, Y. (2013). success with style: use write style 
to predict the success of novels. In proceed of the 2013 confer on empir 
method in natur languag processing, pp. 1753–1764. 

gillick, d., & favre, B. (2009). A scalabl global model for summarization. In proceed 
of the workshop on integ linear program for natur langaug processing, 
pp. 10–18. associ for comput linguistics. 

217 



yang & nenkova 

gillick, d., riedhammer, k., favre, b., & hakkani-tur, D. (2009). A global optim 
framework for meet summarization. In 2009 ieee intern confer on 
acoustics, speech and signal processing, pp. 4769–4772. ieee. 

jaeger, T. F. (2010). redund and reduction: speaker manag syntact inform 
density. cognit psychology, 61 (1), 23–62. 

jurafsky, d., ranganath, r., & mcfarland, D. A. (2009). extract social meaning: iden- 
tifi interact style in spoken conversation. In human languag technologies: 
confer of the north american chapter of the associ of comput lin- 
guistics, proceedings, may 31 - june 5, 2009, boulder, colorado, usa, pp. 638–646. 

kahneman, D. (2011). thinking, fast and slow. 

louis, a., & nenkova, A. (2012). A coher model base on syntact patterns. In proceed- 
ing of 2012 joint confer on empir method in natur languag processing, 
pp. 1157–1168. associ for comput linguistics. 

louis, a., & nenkova, A. (2013). what make write great? first experi on articl 
qualiti predict in the scienc journal domain. tacl. 

louis, a., & nenkova, A. (2014). verbose, lacon or just right: A simpl comput 
model of content appropri under length constraints. In proceed of the 14th 
confer of the european chapter of the associ for comput linguistics, 
eacl 2014, april 26-30, 2014, gothenburg, sweden, pp. 636–644. 

malmasi, s., & dras, M. (2014). chines nativ languag identification. In proceed of 
eacl, vol. 2, pp. 95–99. 

mani, i., klein, g., house, d., hirschman, l., firmin, t., & sundheim, B. (2002). summac: 
a text summar evaluation. natur languag engineering, 8 (1), 43–68. 

manning, C. d., surdeanu, m., bauer, j., finkel, j., bethard, S. j., & mcclosky, D. (2014). 
the stanford corenlp natur languag process toolkit. In associ for com- 
putat linguist (acl) system demonstrations, pp. 55–60. 

metallinou, a., lee, s., & narayanan, S. (2010). decis level combin of multipl 
modal for recognit and analysi of emot expression. In proceed of the 
ieee intern confer on acoustics, speech, and signal processing, icassp 
2010, 14-19 march 2010, sheraton dalla hotel, dallas, texas, usa, pp. 2462–2465. 

nenkova, A. (2005). automat text summar of newswire: lesson learn rom the 
document understand conference. In aaai, pp. 1436–1441. 

nenkova, a., & louis, A. (2008). can you summar this? identifi correl of input 
difficulti for multi-docu summarization. In acl 2008, proceed of the 46th 
annual meet of the associ for comput linguistics, pp. 825–833. 

nguyen, T. h., & grishman, R. (2016). model skip-gram for event detect with convo- 
lution neural networks. In emnlp, pp. 886–891. the associ for comput 
linguistics. 

over, p., dang, h., & harman, D. (2007). duc in context. inf. process. manage., 43 (6), 
1506–1520. 

218 



detect content-dens new text 

pate, J. k., & goldwater, S. (2015). talker account for listen and channel characterist 
to commun efficiently. journal of memori and language, 78, 1–17. 

peng, h., song, y., & roth, D. (2016). event detect and co-refer with minim 
supervision. In proceed of the 2016 confer on empir method in natur 
languag processing, emnlp 2016, austin, texas, usa, novemb 1-4, 2016, pp. 
392–402. 

post, m., & bergsma, S. (2013). explicit and implicit syntact featur for text classifica- 
tion. In proceed of the 51st annual meet of the associ for comput 
linguist (volum 2: short papers), pp. 866–872. 

queneau, R. (1947). exercis in style. 

r.-e. fan, k.-w. chang, c.-j. H. x.-r. w., & lin, c.-j. (2008). liblinear: A librari for 
larg linear classification.. 9, 1871–1874. 

raaijmakers, s., truong, k., & wilson, T. (2008). multimod subject analysi of 
multiparti conversation. In proceed of the confer on empir method in 
natur languag processing, emnlp ’08, pp. 466–474. 

sauŕı, r., & pustejovsky, J. (2009). factbank: a corpu annot with event factuality. 
languag resourc and evaluation, 43 (3), 227–268. 

sinclair, j., & ball, J. (1996). preliminari recommend on text typology. 

tulyakov, s., jaeger, s., govindaraju, v., & doermann, D. (2008). review of classifi 
combin methods. In In machin learn in document analysi and recognition. 
informatica 34 (2010) 111?118 S. vemulap et al. 

van halteren, h., zavrel, j., & daelemans, W. (1998). improv data driven wordclass 
tag by system combination. In proceed of the 36th annual meet of the as- 
sociat for comput linguist and 17th intern confer on com- 
putat linguist - volum 1, acl ’98, pp. 491–497. 

vlachos, a., & riedel, S. (2014). fact checking: task definit and dataset construction. In 
proceed of the acl 2014 workshop on languag technolog and comput 
social science. 

wilson, M. (1988). the mrc psycholinguist database: machin readabl dictionary. be- 
haviour research methods, instrument and computer, version 2, 20 (1), 6–11. 

yang, y., bao, f., & nenkova, A. (2017). detect (un)import content for single- 
document news summarization. In proceed of the 15th confer of the european 
chapter of the associ for comput linguistics: volum 2, short papers, 
pp. 707–712. 

yang, y., & nenkova, A. (2014). detect information-dens text in multipl news do- 
mains. In proceed of twenty-eighth aaai confer on artifici intelligence. 

yu, h., & hatzivassiloglou, V. (2003). toward answer opinion questions: separ 
fact from opinion and identifi the polar of opinion sentences. In proceed 
of emnlp, pp. 129–136. 

219 


