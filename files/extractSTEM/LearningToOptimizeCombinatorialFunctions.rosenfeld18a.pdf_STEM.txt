






















































learn to optim combinatori function 


learn to optim combinatori function 

nir rosenfeld 1 eric balkanski 1 amir globerson 2 yaron singer 1 

abstract 
submodular function have becom a ubiquit 
tool in machin learning. they be learnabl 
from data, and can be optim effici and 
with guarantees. nonetheless, recent neg 
result show that optim learn surrog 
of submodular function can result in arbitrarili 
bad approxim of the true optimum. our 
goal in thi paper be to highlight the sourc of 
thi hardness, and propos an altern criterion 
for optim gener combinatori function 
from sampl data. We prove a tight equival 
show that a class of function be optimiz if 
and onli if it can be learned. We provid effici 
and scalabl optim algorithm for sever 
function class of interest, and demonstr their 
util on the task of optim choos trend 
social medium items. 

1. introduct 
submodular optim be fast becom a primari tool in 
machin learning. the power of submodular a a model 
have be demonstr in numer applications, includ 
document summar (lin & bilmes, 2011), cluster- 
ing (gome & krause, 2010), activ learn (golovin & 
krause, 2011; guillori & bilmes, 2011; hoi et al., 2006), 
graph and network infer (gomez rodriguez et al., 2010; 
rodriguez & schölkopf, 2012; defazio & caetano, 2012), 
and inform diffus in network (kemp et al., 2003). 
crucial to the success of these method be the fact that op- 
timiz submodular function can be do effici and 
with provabl guarante (kraus & golovin, 2014). 

In mani cases, however, the true function cannot be ac- 
cessed, and instead a surrog function be learn from 
data (balkanski et al., 2017). To thi end, pmac learn 
(balcan & harvey, 2011) offer a framework for analyz 
the learnabl of submodular functions, a well a algo- 

1harvard univers 2tel aviv university. correspond to: 
nir rosenfeld <nirr@g.harvard.edu>. 

proceed of the 35 th intern confer on machin 
learning, stockholm, sweden, pmlr 80, 2018. copyright 2018 
by the author(s). 

rithm for learn in practice. encourag result show 
that in mani case submodular function can be effici 
learn from data (balcan & harvey, 2011; iyer et al., 2013; 
feldman & kothari, 2014; feldman & vondrak, 2016). A 
natur approach in thi set be to first learn a surrog 
function from samples, and then optim it, hop that the 
estim optimum will be close to the true one. A recent 
line of work have be devot to thi set of optim 
from sampl (ops) (balkanski et al., 2016; 2017). 

the main result of op be unfortun discouraging: for 
maxim a submodular function under a cardin con- 
straint, no algorithm can obtain a constant factor approxi- 
mation guarante give polynomially-mani sampl from 
ani distribut (balkanski et al., 2017). thus, optim 
over learn surrog do not provid ani meaning 
guarante with respect to the true function. 

the hard of op is, however, a worst-cas result. the 
hard stem from the discrep between how the algo- 
rithm gain access to inform (via samples) and how it 
be evalu (globally). In contrast, machin learn objec- 
tive be typic concern with expect outcomes, and 
be evalu over the same distribut from which data be 
acquir (valiant, 1984). In thi paper, we build on thi moti- 
vation and propos an altern framework for optim 
from samples. the object we propose, call distribu- 
tional optim from sampl (dops), circumv the 
abov difficulti by consid a distribution-depend 
objective. In general, a function class F be in α-dop if an 
α-approxim of the empir argmax can be found with 
arbitrarili high probabl use polynomi mani sam- 
ples, for ani distribut D and for ani f ∈ F . formally: 
definit 1 (α-dops). let F = {f : 2[n] → r+} be a 
class of set function over n elements. We say that F be 
α-distribut optimiz from sampl if there be an 
algorithm A that, for everi distribut D over 2[n], everi 
f ∈ F , and everi �, δ ∈ [0, 1], when A be give a input 
a sampl set S = {(si, f(si))}mi=1 where Si 

iid∼ D, with 
probabl of at least 1− δ over S it hold that: 

PT ∼dm 
[ 
f 
( 
a(t ) 

) 
≥ αmax 

s∈t 
f(s) 

] 
≥ 1− � (1) 

where T = {(sj)}mj=1, a(t ) ∈ T be the output of the 
algorithm, and S be of size M ∈ poly(n,m, 1/�, 1/δ). 



learn to optim combinatori function 

the criterion in eq. (1) relax the op object to hold in 
expect over D. thi be achiev by replac the entir 
combinatori domain with a sampl subset T of size m, 
allow for a distribution-agnost notion of approximation. 
As m increases, satisfi eq. (1) be expect to be harder. 
when m→∞, dop recov ops. 

our first goal in thi paper be to establish the hard of 
dops. In general, classic approxim result do not nec- 
essarili transfer to statist set (balkanski et al., 2017). 
nonetheless, our main theoret result establish a tight 
equival between dop and pmac learn (balcan & 
harvey, 2011), mean that ani F that be learnabl be also 
optimizable, and vice versa. thi demonstr an intrigu 
link between learn and optim submodular functions, 
which be know to be pmac-learn (balcan & harvey, 
2011). the equival result be constructive, and give a 
gener optim algorithm which can util ani pmac 
learner a a black box for dops, and vice versa. while our 
main focu in thi paper be on submodular functions, these 
result hold for ani famili of combinatori functions. 

In practice, however, optim via pmac algorithm have 
sever drawback (balcan & harvey, 2011; feldman & 
kothari, 2014; feldman & vondrak, 2016). our second 
goal in thi paper be henc to design an effici and scal- 
abl dop algorithm for sever class of interest. our 
algorithm optim a loss function whose minim 
provid a suffici condit for dops. We prove that the 
minim of the empir loss can be use for recov 
an approxim argmax. In thi sense, the framework we 
propos be one in which the algorithm “learn to optimize”. 
We show how the loss can be minim effici and with 
guarante for sever submodular function classes, includ- 
ing coverag functions, cut functions, and unit demand. 

An addit benefit of our approach be that it provid guar- 
ante even when the output of the algorithm be restrict to 
a set of sampl alternatives. thi set be especi preva- 
lent in case where both set and their valu be gener 
by human users. for example, in the problem of influenc 
maxim (kemp et al., 2003), the goal be to choos 
a “seed” set of user such that, when expos to certain 
content, will maxim it expect propagation. however, 
target arbitrari subset of user be in most case impos- 
sible, and the algorithm must choos between the set of 
user share current trend items. In the last part of the 
paper we demonstr the empir util of our approach 
on thi task use real data from twitter. 

2. distribut optim and learn 
In thi section we give a tight character of function 
class in dop by show that a class F be in dop if 
and onli if it be pmac-learnable. thi involv two steps. In 

the first, we show that if F be α-pmac learnabl with sam- 
ple complex mpmac, then it be α-dops. We augment thi 
result with tight sampl complex bound for α-dops. In 
the second part, we show that pmac learnabl be not onli 
suffici but also necessari for distribut optim 
from samples. We show that if F be not α-pmac learnable, 
then it be not (α− �)-dop for ani constant � > 0, which be 
tight. thi result be obtain by construct a novel pmac 
algorithm base on a dop black-box, and may thu be of 
separ interest in pmac analysis. overall, our result de- 
termin the hard of dop by establish a connect 
between the approxim and learnabl of function 
classes. 

We begin by review the notion of pmac learnability: 

definit 2 (pmac, balcan & harvey (2011)). A class F 
be α-pmac-learn if there be an algorithm such that for 
everi distribut D, everi f ∈ F , and everi �, δ ∈ [0, 1], 

ps∼d 
[ 
f̃(s) ≤ f(s) ≤ αf̃(s) 

] 
≥ 1− � (2) 

where the input of the algorithm be a set S of size M ∈ 
poly(n, 1/�, 1/δ), the output be a map f̃ : 2[n] → r+, 
and eq. (2) hold w.p. at least 1− δ over S. 

intuitively, pmac gener the standard notion of pac 
learn by consid a loss which penal predict 
that be not within a factor of α of their true value. 

We be now readi to prove our main theoret results. 

2.1. If F be pmac-learn then F be in dop 

We show that if F be α-pmac learnabl with sampl com- 
plexiti mpmac(n, δ, �, α), then it be α-dop with sampl 
complex mpmac(n, δ, 1− (1− �)1/m, α), and thi sampl 
complex be tight. A pmac algorithm learn a surrog 
function f̃ . In our reduction, the correspond dop algo- 
rithm simpli output argmaxs∈t f̃(s). the technic part 
of thi result be in show the sampl complex tightness. 
intuitively, the sampl complex be exactli the number of 
sampl that be need so that, with high probability, f̃ 
obtain a good approxim on all S ∈ T . We begin by 
show that mpmac(n, δ, 1 − (1 − �)1/m, α) be sufficient, 
which follow from the definit of pmac. 

theorem 1. assum F be α-pmac-learn with sampl 
complex mpmac(n, δ, �, α), then F be α-dop with sam- 
ple complex at most mpmac(n, δ, 1− (1− �)1/m, α), i.e., 

mdops(n,m, δ, �, α) ≤mpmac(n, δ, 1− (1− �)1/m, α). 

proof. let f ∈ F , D be some distribution, S = 
{(si, f(si))}mi=1 and T = {si}mi=1 be the train and test 
sets, anda be an algorithm that construct f̃ which α-pmac 
learn f with sampl complex mpmac(n, δ, �, α). 



learn to optim combinatori function 

the dop algorithm that we analyz construct f̃ with 
algorithm A use S and return 

s̃? = argmax 
s∈t 

f̃(s). 

fix �, δ > 0 and α > 1 and consid M = mpmac(n, δ, 1− 
(1 − �)1/m, α). By the definit of α-pmac, we get that 
with probabl 1− δ over S, 

Pr 
s∼d 

[ 
f̃(s) ≤ f(s) ≤ α · f̃(s) 

] 
≥ (1− �)1/m. 

next, we obtain 

Pr 
T 

[ 
f̃(s) ≤ f(s) ≤ α · f̃(s) : ∀S ∈ T 

] 
= 
( 

Pr 
s∼d 

[ 
f̃(s) ≤ f(s) ≤ α · f̃(s) 

])m 
≥ 1− �. 

where the equal be due to the set S ∈ T be drawn 
i.i.d. fromd, and the inequ hold with probabl 1−δ 
over S. We defin S? = argmaxs∈t f(s) and obtain that 
with probabl 1− � over T and 1− δ over S, 

f(s̃?) ≥ f̃(s̃?) ≥ f̃(s?) ≥ α−1f(s?). 

We conclud that withm = mpmac(n, δ, 1−(1−�)1/m, α), 

f(s̃?) ≥ 1 
α 
·max 
s∈t 

f(s) 

with probabl 1− � over T and 1− δ over S. 

for tightness, we give an information-theoret low bound 
by construct a difficult class F that cannot be in α-dop 
with less than mpmac(n, δ, 1− (1− �)1/m, α) samples. 
theorem 2. for all α > 1 and �, δ > 0, for m suffici 
large, there exist a famili of function F and a function 
mpmac(·) such that 

• for all �′, δ′ > 0: F be α-pmac-learn with sampl 
complex mpmac(n, δ′, �′, α), and 

• give strictli less than mpmac(n, δ, 1− (1− �)1/m, α) 
samples, F be not α-dops, i.e., 

mdops(n,m, δ, �, α) ≥mpmac(n, δ, 1−(1−�)1/m, α). 

proof sketch (see supp. materi for full proof). for each 
f in the difficult F , onli a singl set S? have a high value, 
while all other have low values. We consid a uniformli 
random function f ∈ F and the correspond random 
subclass F ′ ⊆ F which consist of all function f ′ such 
that S? be in the test set but not in the train set. 

informally, an algorithm which aim to optim f ∈ F ′ 
cannot use the train set to learn which S ∈ T be s?. more 

precisely, if f ∈ F ′, the decis of the algorithm be 
independ of the random of f , condit on f ∈ 
F ′. thus, if f ∈ F ′, the algorithm do not obtain an α- 
approxim becaus of the gap between the valu of S? 

and the other sets. 

We construct F and D such that S? be in the test set w.p. 
great than 1 − �. thi impli that to satisfi dops, the 
algorithm must observ enough sampl so that S? be in the 
train set w.p. at least 1− δ. We then argu that thi number 
of sampl be at least mpmac(n, δ, 1− (1− �)1/m, α). 

2.2. If F be not pmac-learn then F be not in dop 

A simpl intuit for theorem 1 be that if one can accu- 
rate predict the valu of all S ∈ T , then it be possibl 
to find the empir argmax. the main result in thi sec- 
tion, which be perhap less intuitive, show that the revers 
implic also holds. namely, if one can find the the em- 
piric argmax, then it be possibl to infer the valu of all 
set in T . the contraposit of thi result be that if F be 
not pmac-learnable, then F be not in dops. combin 
both result provid a full character of distribut 
optim in term of learnability. 

To construct a pmac learner from a dop algorithm, we 
first randomli partit S into “train” and “test” sets. We 
then train the dop algorithm on the train set, and use it 
to gener pairwis comparison with test elements. the 
learn valu for S be give by the maximum valu of a test 
sampl that S “beats” (via the infer comparisons). At 
a high level, the analysi us the dop guarante and a 
bucket argument to satisfi the pmac requirements. 
theorem 3. let µ = max f(s)/mins:f(s)>0 f(s), 
c be ani constant such that 1 ≤ α ≤ c, and 
Mµ = 

8 log µ 
� log c 

( 
1 
� + 2 log 

( 
1 
δ 

)) 
. If a class F be in α- 

dop with sampl complex mdops(n,m, �, δ, α), then 
it be α-pmac-learn with sampl complex Mµ + 
mdops(n, 2, �/mµ, δ/mµ, α/c), i.e., 

mpmac(n, �, δ, α) ≥mµ+mdops(n, 2, �/mµ, δ/mµ, α/c). 

proof. fix �, δ > 0 and α > 1. let S = {(si, f(si))}mi=1 
be the sampl from D that be give a input. We partit 
the sampl in S uniformli at random into S1 and S2 of 
size M1 and m2, respectively. for some S ∼ D, the goal 
be to predict f̃(s) such that f̃(s) ≤ f(s) ≤ α · f̃(s). 

for each Si ∈ s2, defin s2,i := {si, s}. sinc F be 
in dops, with M1 = mdops(n, 2, �/m2, δ/m2, α/c) sam- 
ples, the algorithm output s?i ∈ s2,i such that with proba- 
biliti 1− δ/m2 over S1 and 1− �/m2 over s2,i, 

f(s?i ) ≥ 
α 

c 
max(f(s), f(si)). 

By a union bound, thi hold for all i ∈m2 with probabl 
1− δ over S1 and probabl 1− � over S and s2. 



learn to optim combinatori function 

We say that S “beats” Si if the α-dop algorithm output 
S when give s2,i. let s−2 be the collect of set Si in 
S2 such that S beat si. the learn algorithm be 

f̃(s) = 
c 

α 
· max 
si∈s−2 

f(si). 

let fmin = min f(s) and fmax = max f(s). We parti- 
tion the set into bucket defin a follows: 

Bi := {S : fmin · ci−1 ≤ f(s) < fminci} 

for i ≥ 1 and B0 = {S : f(s) = 0}. with β := 
logµ/ log c buckets, all set S be in a bucket sinc fmin ≤ 
f(s) ≤ fmax. We defin a bucket Bi to be dens if a ran- 
dom set S ∼ D have non-neglig probabl to be in 
bi, otherwis it be sparse. more precisely, Bi be dens if 
prs∼d [S ∈ bi] ≥ �/2β. 

the set S be in a dens bucket Bi with probabl at least 
1 − �2 sinc there be at most β bucket that be not dens 
and S be in each of them with probabl at most �2β by the 
definit of dens bucket. with m samples, the expect 
number of sampl in Bi be at least m �2β and by a standard 
concentr bound, 

Pr 

[ 
|bi| ≤ 

m 

2 

� 

2β 

] 
≤ e− 

m� 
16β 

We assum that |bi| ≥ m2 
� 

2β for the remaind of the proof. 
there be at most one set in bucket Bi that be beaten by all 
the other sets. sinc the set S have equal probabl to be 
ani of the set in bi,1 there be at least one other set S− in 
Bi which S beat with probabl 1/|bi| ≤ 4β/m�. 

with δ ≥ e− 
m� 
16β (and henc m ≥ log(1/δ)16β� ), with prob- 

abil of at least 1− δ, the number of sampl in Bi be at 
least m �4β . with �/2 ≥ 4β/m� (and henc m ≥ 8β/� 

2), 
with probabl of at least 1− � over S ∼ D, S be in a dens 
bucket and beat at least one other S− ∈ s−2 in that bucket. 

We get that: 

f̃(s) = 
c 

α 
· max 
si∈s−2 

f(si) ≥ 
c 

α 
· f(s−) ≥ 1 

α 
· f(s) 

where the equal be by the definit of f̃(s), the first 
inequ be sinc S− ∈ s−2 , and the last be sinc S and S− 
be in the same bucket. We also have 

f(s) ≥ α 
c 
· max 
si∈s−2 

f(si) = f̃(s) 

where the inequ be by the definit of s−2 and the equal- 
iti by definit of f̃(s). thus, f̃(s) ≤ f(s) ≤ αf̃(s) and 
with M2 = m ≥ 8 log µ� log c 

( 
1 
� + 2 log 

( 
1 
δ 

)) 
= mµ, the sampl 

complex be Mµ +mdops(n, 2, �/mµ, δ/mµ, α/c). 

1we assum that the dop algorithm break tie in a consist 
manner, i.e., it cannot be adversari and break tie depend on 
whether S be the set we wish to learn or if S ∈ s2. 

algorithm 1 dops( = {(si, zi)}mi=1, m, α) 
1: randomli partit [M ] into N = bmm c set a1, . . . , AN 
2: creat m-tupl sampl set S = {(si, zi)}ni=1 from S 

where Si = {sj}j∈ai and z 
i = {zj}j∈ai 

3: comput α(zi) = {y ∈ [m] : ziy ≥ αmax zi} ∀ i ∈ [N ] 

4: θ̂ = argmin 
θ∈θ 

N∑ 
i=1 

max 
y 

[1{i 6∈α(zi)} + fθ( 
i 
y)− ψθ(si, zi)]+ 

where ψθ(s, z) = 1|α(z)| 
∑ 
y∈α(z) fθ(sy) 

5: return h 
θ̂ 
(T ) = argmax 

s∈t 
f 
θ̂ 
(s) 

3. learn to optim at scale 
In thi section we give an effici dop algorithm that 
appli to sever interest parametr submodular sub- 
class FΘ = {fθ : θ ∈ θ}. our gener techniqu 
includ two steps. first, we identifi a loss function whose 
minim provid a suffici condit for dop (eq. 
(1)), but be in gener hard to optimize. then, we show that 
for the function class we consider, a transform of the 
input reveal structur which can be exploit for effici 
optim a convex surrog loss. note that in principle, 
due to thm. 1, ani pmac algorithm can be use for dops. 
this, however, have sever practic disadvantages, which 
we comment on in sec. 3.5. 

We begin by illustr our approach for coverag function 
with parametr weights. We then describ our algorithm, 
prove it correctness, and show how it can be appli to 
other class such a graph cuts, unit demand, and coverag 
function with parametr cover sets. 

3.1. learn to optim coverag function 

coverag function be a simpl but import class of sub- 
modular functions, and have be use in applic such 
a comput linguist (sipo et al., 2012), algorith- 
mic game theori (dughmi & vondrák, 2015), and influenc 
maxim in social network (kemp et al., 2003). let 
U be a ground set of d items, and C = {c1, . . . , cn} a col- 
lection of subset where Ci ⊆ U . for a set of non-neg 
item weight θ = {θ1, . . . , θd}, a function fθ : 2[n] → R be 
a coverag function if: 

fθ(s) = 
∑ 

u∈c(s) 

θu, c(s) = 
⋃ 
i∈ 

Ci (3) 

while appar simple, coverag function be quit ex- 
pressive, and optim them from sampl be know to 
be hard (balkanski et al., 2017). one reason be that, a a 



learn to optim combinatori function 

function of their input S, coverag function can be highli 
non-linear. meanwhile, a a function of their parameters, 
they becom linear via a simpl transform of the inputs: 

fθ(s) = 〈φ(s), θ〉, φu(s) = 1{∃ i∈ s.t. u∈ci} (4) 

thi structur allow our algorithm to effici find the 
approxim empir argmax of ani give T with high 
probability. the output of the algorithm be a function h ∈ H 
for choos one S out of the m candid in T , where: 

H = {hθ(t ) = argmax 
s∈t 

fθ(s) : θ ∈ Θ} (5) 

In thi sense, our method “learns” how to optim over 
collect of size m. 

3.2. algorithm 

pseudocod of our dop algorithm be give in algorithm 1. 
the follow theorem establish it correctness: 

theorem 4. let m ∈ N and �, δ ∈ [0, 1], and let f = fθ∗ 
with θ∗ ∈ Θ. for a give α > 0, let h be the output of 
algorithm 1 when give S = {(si, zi)}mi=1, m, and α a 
input, where z = fθ(s) and S 

iid∼ D. then, with probabl 
of at least 1− δ over S, it hold that: 

PT ∼dm 
[ 
f 
( 
h(t ) 

) 
≥ αmax 

s∈t 
f(s) 

] 
≥ 1− � (6) 

for M ≥ õ(m(kb/�)2) with k = max |s|, B = ‖θ∗‖2. 

the follow proof hold for ani class of function that 
can be make linear in their paramet under some represen- 
tation. thi includ the coverag function in sec. 3.1 a 
well a the class we consid in sec. 3.3. 

proof. We begin with some notation. let S = 
{s1, . . . , sm} be a set of m exampl with correspond 
valu z = {z1, . . . , zm} where zy = f(sy). algorithm 1 
return a function h that choos a set Sy ∈ S. It will be 
conveni to instead view h a a map from S to index 
y ∈ [m]. denot the set of α-approxim solut by: 

α(z) = {y ∈ [m] : zy ≥ αmax z} (7) 

our analysi make use of the follow loss function: 

∆α(z, y) = 1{i 6∈ α(z)} (8) 

eq. (8) be use sinc l(h) := e[∆α(z, h(s))] ≤ � im- 
pli that h satisfi eq. (6). We therefor focu on bound 
l(h). As we do not have access tod, our algorithm choos 
an h ∈ H which instead minim the empir loss. note 
that while ∆α be defin over m-tuples, S contain individ- 
ual sets. To ensur a consist empir loss, we randomli 

partit [M ] into N = m/m distinct set a1, . . . , AN , 
and defin anm-tupl sampl set S = {(si, zi)}ni=1, where 
Si = {sy}y∈ai and zi = {zy}y∈ai . the loss be now: 

l̂(h;s) = 1 
N 

N∑ 
i=1 

∆α(z 
i, ŷi), ŷi = h(si) (9) 

sinc ∆α be not convex, the algorithm instead optim a 
surrog convex upper bound. there be mani way to do 
this; here we use an averag hing surrogate: 

max 
y∈[m] 

[∆α(z 
i, y) + fθ( 

i 
y)− ψθ(si, zi)]+ (10) 

where [a]+ = max{0, a} and: 

ψθ(s, z) = 
1 

|α(z)| 
∑ 

y∈α(z) 
fθ(sy) (11) 

eq. (10) be similar in spirit to the loss in (lapin et al., 2015), 
and be tight w.r.t. eq. (9) whenev L̂ = 0, intuitively, 
minim eq. (10) push θ toward valu for which the 
true argmax be score high than all other by a margin. 
note that the averag in eq. (11) can be replac with a 
max to attain a tighter (though no longer convex) surrogate. 

sinc S be label by some fθ∗ ∈ fθ, we have that 
l(hθ∗) = 0. thi mean that there be some θ ∈ Θ such 
that with l̂(hθ;s) = 0, and due to the tight of eq. (10), 
l̃(hθ;s) = 0 a well. thi be suffici for appli the 
follow gener bound (collins, 2004): 

l(h) ≤ O 

(√ 
m 

M 

( 
(kb logm)2 + log 

1 

δ 

)) 
(12) 

plug in M give l(h) ≤ �, conclud the proof. 

eq. (10) be convex whenev fθ be linear in θ for some repre- 
sentat φ. thi hold for coverag function (eq. (4)) a 
well a for the other class we consid in sec. 3.3. eq. (10) 
can then be optim use standard convex solvers, or with 
highli effici and scalabl solver such a the cut plane 
method of joachim et al. (2009). 

3.3. other submodular class 

We now discu how our method can be extend to other 
submodular function classes. for each class, we give a 
transform φ of the input under which the function 
becom linear in it parameters. thm. 4 and algorithm 1 
can then be appli with the appropri fθ(s) = 〈φ(s), θ〉. 

graph k-cuts: let G = (v,e) be an undirect graph, 
and let θ ∈ r|e|+ be edg weights. for a partit P ∈ [k]|v | 
of the node into k groups, it valu be give by: 

fθ(p ) = 
1 

2 

∑ 
(u,v)∈ 
Pu 6=pv 

θuv 



learn to optim combinatori function 

while k-cut function be know to be hard to optim over 
P , they becom linear in θ with the transformation: 

φuv(p ) = 1{pu 6= pv} ∀ (u, v) ∈ E 

unit demand: let θ ∈ rn+ be a set of item weights. the 
valu of a subset S ⊆ [n] be give by: 

fθ(s) = max 
u∈ 

θu 

although it be possibl to write fθ = 〈θ, φ(s)〉 with 
φu(s) = 1{θu≥θv ∀v∈s}, thi represent requir θ, 
which be unknown. nonetheless, a similar data-depend 
construct can still be use to obtain some θ′ which min- 
imiz the loss. To see why, let S̄ ∈ S be the set with the 
high valu fθ(s̄) in S. for thi s̄, there must exist some 
u ∈ S̄ that be not in ani other S ∈ S with fθ(s) < fθ(s̄). 
By set φv(s̄) = 1{u=v} and θ′u = fθ(s̄), we ensur 
that fθ(s̄) = 〈θ′, φ(s̄)〉. note that thi do not necessarili 
impli that θ′u = θu. In a similar fashion, by setting: 

φu(si) = 1{u ∈ Si ∧ @ j 6= i s.t. u ∈ Sj ∧ zj < zi} 

for everi i ∈M , we get that fθ(si) = 〈θ′, φ(si)〉 for some 
θ′, which guarante L̂ = 0. note that gener here 
concern φ a appli to exampl in both S and T . 

coverag with parametr cover sets: let U = [N ] 
be a ground set of item with unit weights. the paramet 
be a collect item subset {c1, . . . , cn} with Ci ⊆ U . 
We use ξiu = 1{u ∈ ci} and denot the maxim overlap 
by d = maxu 

∑ 
i ξiu. for a subset S ∈ [n], it valu is: 

fc(s) = 
∣∣∣⋃ 

i∈ 
Ci 

∣∣∣ 
while fC be not linear over C, it can be linear over a 
differ parameterization. for xi = 1{i ∈ s}, we have: 

fc(s) = 
∑ 
u∈ω 

( 
1− 

n∏ 
i=1 

(1− xiξiu) 
) 

sinc fC be a polynomi of degre at most d, the explicit 
size of φ (and henc of the correspond θ) be nd. for 
comput efficiency, we can consid the dual form and 
implicitli defin φ via the kernal inner product: 

〈φ(s), φ(s′)〉 = 
( 
〈x , xs′〉+ 1 

)d 

3.4. reduc the sample-complex cost of m 

interestingly, at the cost of a small addit addit error, 
the depend of the gener bound on m can be 
remov by consid an altern loss function. fix 

some q ∈ [0, 1]. given S, defin Q to be the set of ex- 
ampl in the top q-quantile. the idea here be to learn θ 
so that fθ will score top-quantil exampl S ∈ Q abov 
low-quantil exampl S 6∈ Q. the correspond loss be 
therefor defin over exampl pairs: 

∆q(s, S 
′, fθ) = 

{ 
1{fθ(s)<fθ(s′)} if S ∈ Q ∧ S′ 6∈ Q 
0 otherwis 

(13) 
note that, in a similar fashion to ∆α, the empir loss 
l̂q over ∆q can be optim efficiently, and the optim θ 
give l̂q = 0. for ani S ∈ S, the probabl of have at 
least one S ∈ S ∩Q be 1− qm. appli the gener 
bound in agarw & niyogi (2009) gives: 

� ≤ qm + Õ 

( 
B 

λmq 
+ 

( 
B2 

λ 
+ Z 

)√ 
ln(1/δ) 

Mq 

) 
(14) 

where Z = sup f(s) and λ control an addit regular- 
izer. In sec. 4 we use a stricter variant of thi formulation, 
in which high-quantil item be bin separately. 

3.5. use pmac algorithm in practic 

In principle, the reduct in sec. 2.1 show that ani pmac 
algorithm can be use for dops. practically, however, thi 
approach have sever disadvantages. the root caus of thi 
be that most current pmac algorithm be design for gen- 
eral submodular functions.2 As such, they must adher to 
demand low bound (balcan & harvey, 2011; feldman 
& vondrak, 2016) which hold even for simpl distribut 
(e.g., uniform). when consid specif submodular sub- 
classes, these algorithm can therefor be suboptim (and 
in fact quit costly) in term of runtime, sampl complexity, 
and/or approxim ratio. additionally, virtual all cur- 
rent pmac algorithm provid guarante for either uniform 
or product distributions. even in thi setting, pmac algo- 
rithm either guarante a fix approxim ratio, or be 
exponenti in α (feldman & vondrak, 2016), make them 
difficult to use for α-dop with arbitrarili small α. the 
onli know result for arbitrari distribut be the 

√ 
n+ 1- 

pmac algorithm of balcan & harvey (2011), which give a 
match ω̃(n1/3) low bound on α. 

4. experi 
In thi section we evalu the perform of our method 
on the task of optim choos trend item in social 
medium platforms. Of the countless item that be continu- 
ousli creat and share by user in such platforms, onli a 
hand will becom widespread (goel et al., 2012). A key 

2 A notabl except to thi be feldman & kothari (2014) 
which specif consid pmac learn of coverag function 
with unknown cover sets. 



learn to optim combinatori function 

time 

n 
u 

m 
b 

e 
r 

o 
f 

a 
d 

o 
p 

ti 
o 

n 
s 

figur 1. demonstr the power of a coverag model. the 
true diffus curv of a focal hashtag ω (black) and an addit 
hashtag ω′ with an initi similar (but eventu veri different) 
diffus pattern (orange). diffusion-curv extrapol (bauck- 
hage & kersting, 2014) be gener base on Sω alon (dash 
blue) and on both Sω and c(sω) (dash green), with dash 
line mark the time of the correspond last observations. 
thi show how condit on c(sω) can boost perform by 
provid a probabilist “glimpse” into the near future. marker 
in the zoom inlaid plot indic activ users. 

challeng face daili by platform administr be that of 
identifi potenti trend content a earli a possible. 
trend item can then be marked, use for gener 
recommendations, or promot to the public front page. 

4.1. optim trend item 

for a give social platform, let n be the number of users, 
and Ω be the set of spread content items. when a user 
u ∈ [n] be observ to have be expos to an item ω ∈ Ω, 
we say that u adopt ω. thi can happen, for instance, 
when u views, shares, comments, or vote on ω. A crucial 
factor in the success spread of an item be the ident of 
it earli adopt (rogers, 1962; goldenberg et al., 2002). 
We therefor repres each content item ω at a certain time 
point by the set of user that have adopt it up to that time, 
which we denot by Sω ⊆ [n]. We will be interest in 
the final number of adopt zω a a function of the set 
of adopt users, name zω = f(sω). for simplic 
we assum that all item be consid at the time when 
adopt by exactli k users, so that |sω| = k for all ω ∈ Ω. 
under the abov representation, target a success item 
can be thought of a optim over the set of adopt 
user under a cardin constraint. the task be therefor to 
choos the set Sω for which f(sω) be maximal. 

the abov optim task have two clear restrictions. first, 
f cannot be access or queried, and ani inform re- 
gard the valu of subset be avail onli via samples, 
name past item and their adopt users. second, an algo- 
rithm cannot output ani user subset S ⊆ [n], but must rather 
choos from a set of current avail items. In addition, 

the task of choos the top trend item be perform re- 
peatedly, each time over a differ collect of content 
items. for example, for a front page that be updat hourly, a 
new trend item must be select from the set of current 
propag content item for each update. note that in such 
systems, the avail subset and their eventu valu be 
primarili determin by the system’ users. onlin social 
platform be therefor a prime exampl of a set where 
an optim algorithm have onli statist access to data. 

4.2. experiment setup 

We evalu the perform of our method on a benchmark 
dataset of propag twitter hashtag (weng et al., 2013). 
data be gather by monitor the share (tweet and 
retweeting) of hashtag across user over the cours of a 
month. the dataset includ 612,355 user who share 
226,488 distinct hashtags, with a total of 1,687,704 share 
activities. for each hashtag, the data describ the sequenc 
of adopt user and the correspond timestamps. these 
be use to construct a “retweet” social network G = (v,e) 
where (u, v) ∈ E if v retweet u. A user be consid to 
be activ if she share at least 20 hashtags. We focu on the 
11,815 activ user and on the 4,155 hashtag that includ 
at least one activ user. If a user retweet the same hashtag 
more than once, we consid onli the first tweet. 

sampl be gener in the follow manner. for each 
hashtag ω, the user set Sω be defin to includ the first 
k ∈ {5, . . . , 15} activ adopt users, and zω be set to 
be the number of eventu adopters. all pair (sω, zω) 
be randomli partit into a train set S and a global 
test set T ′ use a 90:10 split. all method be give 
S a input, and be evalu on 1,000 random subset 
T ⊆ T ′ of size m, where m ∈ {100, . . . , 500}. thi be 
repeat 100 times, and averag result be reported. all 
method we consid return an element Ŝ ∈ T by com- 
put argmaxs∈t g(s) for some score function g, which 
be typic learn from the data. hyper-paramet be 
tune use cross valid for all relev methods. 

dop model: We implement the dop algorithm use 
coverag function a the base class. specifically, give the 
social network graph G = (v,e), we use V a the ground 
set, and construct a cover set Cv = u : (v, u) ∈ E) for 
everi v ∈ V . the coverag function we learn is: 

fθ,η(s) = 
∑ 
v∈v 

θv + 
∑ 

u∈c(s) 

ηu (15) 

wherec(s) = 
⋃ 
v∈ Cv . the idea behind thi model be that, 

give that user v adopted, each of her neighbor can also 
adopt (with some probability). figur 1 illustr thi idea. 
thus, the two term in eq. (15) quantifi the contribut 
of the adopt node and of their neighbors, respectfully, 



learn to optim combinatori function 

100 200 300 400 500 
0 

50 

100 

150 

200 

a 
v 
g 

. 
n 

u 
m 

. 
a 

d 
o 

p 
te 

r 

slope 

op 

pmac 

linreg 

dop 

5 7 9 11 13 15 
0 

50 

100 

150 

200 

a 
v 
g 
. 
n 
u 
m 

. 
a 
d 
o 
p 
te 

r 

slope 

op 

pmac 

linreg 

dop 

figur 2. comparison differ method for the task of optim choos trend hashtag on twitter. 

to the overal score. the coverag formul take into ac- 
count the potenti overlap in neighbor nodes, which can 
often be consider (holland & leinhardt, 1971; watt & 
strogatz, 1998). We note that G be construct use train- 
ing data alone, and incom edg where onli consid 
for node with at least 10 shares. eq. (10) be optim 
use the cutting-plan method of joachim et al. (2009). 

baselines: We compar to the follow methods: 

• slope: A first-ord extrapol where we first esti- 
mate the slope of the diffus curve, and then choos 
the subset with the high value. 

• linreg: We first run linear regress with `2 regu- 
larization, and then choos the subset with the high 
predict value. 

• ops: A variant of the op (balkanski et al., 2016), 
where instead of return a global argmax, a give 
subset be score base on the sum of margin estimates. 
note that under certain conditions, thi algorithm be 
optim for the set of optim from samples. 

• pmac: A soft version of the distribution-independ 
pmac algorithm of balcan & harvey (2011). sinc the 
origin algorithm assum separ (which do 
not hold here), we instead use an agnost classifier. 

results: figur 2(a) and 2(b) compar the valu (number 
of adopters) for the chosen output of each method. As can be 
seen, dop clearli outperform other method by a margin. 
note that when k increases, averag output valu be like 
to increas a well, sinc the algorithm be give more 
inform a input. when m increases, however, it be not 
clear a-priori how the averag output valu should change. 
thi be becaus larg test set be more like to includ 
higher-valu items, but at the same time have more low- 
valu alternatives. interestingly, while the perform of 

most baselin do not improv (or even degrades) a m 
increases, the perform of dop improv steadily. 

5. conclus 
In thi work, we propos an optim criterion for 
set where the algorithm be limit to statist access of 
the object function. We argu that thi set be pervasive, 
and in fact, believ that in most applic it be the common 
rule rather than the exception. previou result have be 
gener negative, but mostli due to demand worst-cas 
requirements. draw inspir from learn theory, our 
solut relax these requir to hold in expectation. 

our main theoret result show an equival between 
optim in thi set and learning. thi highlight 
intrigu connect between the comput and statis- 
tical structur of function classes. An interest corollari be 
that analyz hard of comput and approxim 
can now be do use statist tools, and vice versa. 

sever of the function class we explor be notori 
hard to optimize, but have a surprisingli simpl structur 
a a function of their parameters. thi allow u to use 
simpl learn strategi to produc power optim 
mechanisms. We hypothes that there be mani other 
class that poss these properties. An addit avenu 
for further exploration, hint by our equival result, be 
the reverse: be there class that be seemingli hard-to- 
learn, but due to their optimiz properties, can actual 
be learn efficiently? We leav thi for futur work. 

acknowledg 
thi research be support by a googl phd fellowship, 
nsf grant career ccf-1452961, bsf grant 2014389, 
nsf usicc propos 1540428, isf center of excel 
grant, a googl research award, and a facebook research 
award. 



learn to optim combinatori function 

refer 
agarwal, shivani and niyogi, partha. gener bound 

for rank algorithm via algorithm stability. journal 
of machin learn research, 10(feb):441–474, 2009. 

balcan, maria-florina and harvey, nichola ja. learn 
submodular functions. In proceed of the forty-third 
annual acm symposium on theori of computing, pp. 
793–802. acm, 2011. 

balkanski, eric, rubinstein, aviad, and singer, yaron. the 
power of optim from samples. In advanc in 
neural inform process systems, pp. 4017–4025, 
2016. 

balkanski, eric, rubinstein, aviad, and singer, yaron. the 
limit of optim from samples. In proceed 
of the 49th annual acm sigact symposium on theori 
of computing, stoc 2017, montreal, qc, canada, june 
19-23, 2017, pp. 1016–1027, 2017. 

bauckhage, christian and kersting, kristian. strong regular- 
iti in growth and declin of popular of social medium 
services. arxiv preprint arxiv:1406.6529, 2014. 

collins, michael. paramet estim for statist pars- 
ing models: theori and practic of distribution-fre meth- 
ods. new develop in pars technology, 23:19–55, 
2004. 

defazio, aaron and caetano, tiberio S. A convex formu- 
lation for learn scale-fre network via submodular 
relaxation. In advanc in neural inform process 
systems, pp. 1250–1258, 2012. 

dughmi, shaddin and vondrák, jan. limit of random- 
ize mechan for combinatori auctions. game and 
econom behavior, 92:370–400, 2015. 

feldman, vitali and kothari, pravesh. learn coverag 
function and privat releas of marginals. In confer 
on learn theory, pp. 679–702, 2014. 

feldman, vitali and vondrak, jan. optim bound on ap- 
proxim of submodular and xo function by juntas. 
siam journal on computing, 45(3):1129–1170, 2016. 

goel, sharad, watts, duncan J, and goldstein, daniel G. 
the structur of onlin diffus networks. In proceed- 
ing of the 13th acm confer on electron commerce, 
pp. 623–638. acm, 2012. 

goldenberg, jacob, libai, barak, and muller, eitan. ride 
the saddle: how cross-market commun can creat 
a major slump in sales. journal of marketing, 66(2):1–16, 
2002. 

golovin, daniel and krause, andreas. adapt submod- 
ularity: theori and applic in activ learn and 
stochast optimization. journal of artifici intellig 
research, 42:427–486, 2011. 

gomes, ryan and krause, andreas. budget nonparamet- 
ric learn from data streams. In icml, pp. 391–398, 
2010. 

gomez rodriguez, manuel, leskovec, jure, and krause, 
andreas. infer network of diffus and influence. 
In proceed of the 16th acm sigkdd intern 
confer on knowledg discoveri and data mining, pp. 
1019–1028. acm, 2010. 

guillory, andrew and bilmes, jeff A. simultan learn 
and cover with adversari noise. In icml, volum 11, 
pp. 369–376, 2011. 

hoi, steven ch, jin, rong, zhu, jianke, and lyu, michael R. 
batch mode activ learn and it applic to medic 
imag classification. In proceed of the 23rd inter- 
nation confer on machin learning, pp. 417–424. 
acm, 2006. 

holland, paul W and leinhardt, samuel. transit in 
structur model of small groups. compar group 
studies, 2(2):107–124, 1971. 

iyer, rishabh K, jegelka, stefanie, and bilmes, jeff A. cur- 
vatur and optim algorithm for learn and minimiz- 
ing submodular functions. In advanc in neural infor- 
mation process systems, pp. 2742–2750, 2013. 

joachims, t., finley, t., and yu, chun-nam. cutting-plan 
train of structur svms. machin learning, 77(1): 
27–59, 2009. 

kempe, david, kleinberg, jon, and tardos, éva. maxi- 
mize the spread of influenc through a social network. 
In proceed of the ninth acm sigkdd intern 
confer on knowledg discoveri and data mining, pp. 
137–146. acm, 2003. 

krause, andrea and golovin, daniel. submodular function 
maximization., 2014. 

lapin, maksim, hein, matthias, and schiele, bernt. top- 
k multiclass svm. In advanc in neural inform 
process systems, pp. 325–333, 2015. 

lin, hui and bilmes, jeff. A class of submodular function 
for document summarization. In proceed of the 49th 
annual meet of the associ for comput 
linguistics: human languag technologies-volum 1, 
pp. 510–520. associ for comput linguistics, 
2011. 



learn to optim combinatori function 

rodriguez, manuel gomez and schölkopf, bernhard. sub- 
modular infer of diffus network from multipl 
trees. arxiv preprint arxiv:1205.1671, 2012. 

rogers, e.m. diffus of innovations. free press of glen- 
coe, 1962. 

sipos, ruben, shivaswamy, pannaga, and joachims, 
thorsten. large-margin learn of submodular summa- 
rizat models. In proceed of the 13th confer 
of the european chapter of the associ for computa- 
tional linguistics, pp. 224–233. associ for compu- 
tation linguistics, 2012. 

valiant, lesli G. A theori of the learnable. communica- 
tion of the acm, 27(11):1134–1142, 1984. 

watts, duncan J and strogatz, steven H. collect dy- 
namic of small-worldnetworks. nature, 393(6684):440, 
1998. 

weng, lilian, menczer, filippo, and ahn, yong-yeol. viral- 
iti predict and commun structur in social networks. 
scientif reports, 3:2522, 2013. 


