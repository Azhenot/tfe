


































untitl 


2015 ieee intern confer on big data (big data) 

978-1-4799-9926-2/15/$31.00 ©2015 ieee 184 

distribut frank-wolf under pipelin stale synchron parallel 

nam-luc tran, thoma peel, sabri skhiri 
eura nova 

email: {namluc.tran, thomas.peel, sabri.skhiri} 
@euranova.eu 

abstract—iterative-converg algorithm repres an im- 
portant famili of applic in big data analytics. these be 
typic run on distribut process framework deploy 
on a cluster of machines. On the other hand, we be wit 
the move toward data center oper system (os), where 
resourc be unifi by a resourc manag and process 
framework coexist with each other. In thi context, differ 
process framework job task can be schedul on the same 
machin and slow down a worker (straggler problem). exist 
work have show that an iter model with relax consis- 
tenci such a the stale synchron parallel (ssp) model, while 
still guarante convergence, be abl to cope with stragglers. 
In thi paper we propos a model for the integr of the 
ssp model on a pipelin distribut process framework. 
We then appli ssp on a distribut version of the frank- 
wolf algorithm. We theoret show it sparsiti bound 
and converg under ssp. finally, we experiment show 
that the frank-wolf algorithm appli on lasso regress 
under ssp be abl to converg faster than it bsp counterpart, 
especi under load condit similar to those encount 
in a data center os. 

keywords-stal synchron parallel; distribut convex op- 
timization; frank-wolfe; lasso regression; paramet server; 
big data 

I. introduct 

big data analyt have reach a certain level of matur 
in industri and scienc [1]. machin learn algorithm be 
one of the most import applic of big data distribut 
process frameworks. these algorithm be differ from 
tradit workload by their iterative-converg nature: 
they iter until they reach a threshold value, such a 
in minim a predict error. distribut process 
framework such a twister [2], haloop [3] or scalop [4] 
implement the bulk synchron parallel (bsp) paradigm 
[5] and be optim for thi type of iter workload. 

new approach have tri to push forward those ma- 
chine learn process optim by implement a 
pipelin architectur in which tupl of data be stream 
through a set of operators. thi be the case for dryad and 
naiad [6], but also for spark [7] that pipelin mini-batch 
of resili distribut dataset (rdd), or even more recent 
flink [8]–[10]. those pipelin architectur present inter- 
est advantag such a increas perform and the 
abil to implement within the same framework both the 
batch and speed layer of the lambda architectur [11]. all 

figur 1. within a data center oper system, all resourc be unifi 
by a resourc manag and differ framework compet with each other 
for the alloc of resourc with regard to the task they execute. 

those new approach be howev still base on the bsp 
paradigm. 

follow the recent evolut in data center toward the 
unif of comput resources, bsp-base framework 
could lead to neg impact in perform for such algo- 
rithms. indeed, the rise of resourc manag such a meso 
[12], yarn [13], omega [14] and corona [15] have defin 
the concept of data center oper system a illustr 
in figur 1. the comparison be often use to describ 
a modern data center make from a cluster of machines, 
see individu a resources, on which task workload be 
submit in a transpar way. the resourc manag auto- 
matic provis and execut the tasks, and dynam 
shrink (deallocation) or increas (allocation) the resourc 
dedic to the task of a framework. the whole system 
be optim in term of cpu, memory, disk i/o and data 
locality. thi bring the follow advantages: high rate of 
data center usage, optim autom and abstract of 
the resources. 

come back to iterative-converg tasks, thi mean that 
a bsp worker run on a machin be not isol anymor 
from other framework in the cluster. As a result, anoth 
set of task can be schedul on the same machin and 
slow down the worker. thi kind of worker be identifi a a 
straggler in [16] where it be show that bsp program 
dramat suffer in the presenc of stragglers. instead, 
the author of [16] propos the stale synchron parallel 



185 

(ssp) model of computation. 
In the ssp paradigm, the synchron barrier be re- 

lax in each superstep and faster worker continu to 
iter on stale version of the model, a show in figur 
2. converg and correct of the solut be still 
guarante for most of the iterative-converg algorithms. 
there be howev current no model for the implement 
of ssp on pipelin or stream architectures. 

We focu on the distribut frank-wolf algorithm a 
we think that it be an excel candid for asynchron 
process and especi within bound stale a 
be the case for ssp. the frank-wolf algorithm aim at 
solv convex optim problem over a convex set 
and have regain a lot of interest in the machin learn 
commun especi in large-scal applications. In it 
basic version [17] the algorithm consist of a first phase 
where parallel comput be perform with regard 
to a select criteria, follow by a synchron 
phase where the best candid from the previou phase 
be selected, in a pure bsp fashion. thi work follow the 
distribut frank-wolf algorithm investig in a fulli 
synchron manner by [18] and alreadi discuss in an 
asynchron set in [19]. 

our contribution: In thi paper, we propos (1) an 
ssp model base on the apach flink1 pipelin architecture, 
(2) an asynchron distribut frank-wolf optim 
algorithm adapt to our propos ssp model and (3) an 
empir evalu of an implement of the well-known 
lasso regress algorithm [20] use our optim 
scheme in the core of the algorithm. flink’ pipelin 
engin and stream api make it a first choic for pipelin 
ssp. our model howev be gener and can be appli a 
such on pure stream process architectur such a storm 
or samoa [21]. 

We show a few theoret result in a bad case scenario 
includ a converg analysi of the distribut frank- 
wolf algorithm under ssp and an upper bound on the 
sparsiti of the final solution. those result be then evalu 
in practic through an applic of our algorithm to solv 
a synthet lasso regress problem i.e. a l1 regular 
spars approxim problem. our choic of the lasso be 
motiv by it popular in the featur select domain 
make it a good candid to highlight our method. 

thi work be a first step toward a more detail theoret 
analysi of our algorithm and an extens to the on-lin 
convex learn setting. 

outline: section two discu our model for ssp 
on a pipelin architecture. section three describ the 
distribut franck-wolf algorithm in an asynchron 
environ leverag the ssp model. In thi section, we 

1the apach flink project: http://flink.apache.org 

figur 2. In bulk synchron parallel (left), worker synchron 
their updat to the model after each clock. under stale synchron 
parallel (right), worker access the updat of their co-work in a 
best-effort mode within the bound of staleness. 

also present our propos for the bound on the sparsiti of the 
solut a well a the theoret proof of converg of 
our algorithm under ssp. the applic of thi algorithm 
on lasso regress be detail in section four. finally, 
section five discu the experi and the results. 

notations: In the rest of the paper, we denot vector 
by bold low case letter x and matrix by bold upper 
case letter A. ‖x‖0 be the number of non-zero entri in 
the vector x. ∇f(α) denot the gradient of a differenti 
function f take at α. We do not distinguish local worker 
thread and worker thread locat on differ hosts: we 
call them all ”worker threads”. 

ii. stale synchron parallel iter IN 
pipelin process framework 

distribut process be one solut to the challeng 
of treat big and big volum of data. there be 
current numer framework that store and process data 
in a distribut and scalabl way. 

In a distribut process cluster, mani situat can 
lead to thread or process act slow than their peer for 
some time. these process be call straggler and have 
variou causes, includ the algorithm itself. other reason 
includ hardwar heterogeneity, skew data distribution, 
and garbag collect in the case of high-level languages. 
within data center oper system (figur 1), other caus 
for straggler includ concurr between task of differ 
frameworks. 

the bulk synchron parallel model (bsp), one of the 
most current use paradigm in distribut process 
framework for iter algorithms, have show it limit in 
term of scalabl [22]. the frequent and explicit natur 
of the synchron in bsp impli that each iter 
proce at the pace of the slowest thread. thi lead 
to poorer perform in the presenc of stragglers. the 
author of [23] have show that in some case the percentag 
of effect time spent on the comput repres 25 
percent of total time spent in the iteration, the rest of the time 
be spent in network commun and synchronization. 



186 

A. gener of the iter model 

within an iteration, a clock repres the amount of work 
a worker perform on it data partition. each worker do 
not see the adjust perform from the other worker 
on their partit dure a clock. thi lead to the notion of 
stale which defin the number of clock dure which a 
worker do not see the adjust from the other workers. 

In stale synchron parallel process (ssp), the stal- 
ene paramet can take arbitrari valu and differ 
worker can be at differ valu of clock, with the stale 
paramet defin the maximum number of clock the 
fast worker may be ahead of the slowest. stale be 
a paramet to be tune depend on the algorithm and the 
size of the cluster [16]. under thi generalization, bsp be the 
particular case where the stale paramet equal zero. 

As there be no explicit barrier forc everi worker to 
synchron their adjustments, there be a need for a share 
data structur that store the current algorithm state and that 
the worker be abl to updat individu on each clock. 
thi compon be call the paramet server. 

In ssp, each worker start with an intern clock equal 
to zero. the worker repeat the follow sequence: (1) 
perform comput use the share model store in the 
paramet server, (2) perform addit updat to the share 
model in the paramet server, (3) advanc it intern clock 
by 1. thi sequenc of oper result in a window 
bound by the slowest and the fast worker, and with a 
maximum length defin by the stale valu (figur 2). 
the ssp condit [24] be such that within that window, each 
worker see a noisi view of the system state, compos of, 
on one hand, the updat guarante until the low bound 
of the window, and on the other side, the best-effort updat 
make within the window. 

B. converg guarante 

To the best of our knowledge, the converg proof for 
the ssp model have be demonstr onli for the stochast 
gradient descent algorithm in [23] and [24]. although the 
author have use ssp for other algorithm such a latent 
dirichlet allocation, there be no formal demonstr for 
variant such a coordin descent algorithms. the dis- 
tribut frank-wolf algorithm belong to the latter cate- 
gory. In thi paper, we propos an ssp implement for 
the distribut frank-wolf algorithm a well a a theoret 
proof of it converg under ssp. 

C. integr model on pipelin process framework 

In a pipelin architecture, the distribut process 
framework process the data tupl one by one by stream 
them through the oper that compos the dataflow graph 
of operations. from an architectur viewpoint, thi can be 
see a a stream process environ on which the data 
tupl be stream to the workers. 

figur 3. ssp iter control model for a pipelin distribut process 
framework. (a) worker commun their intern clock to the sink, which 
commun the cluster-wid clock, use an event-driven architecture. (b) 
overview of the paramet server, built on top of a data grid. each worker 
store a partit of the grid and automat benefit from local read 
and replic of the grid. 

In thi context we have design ssp iter to work in 
such pipelin frameworks. instead of reli on a design 
center around the paramet server which handl both 
the clock synchron between worker through block 
call and the storag of the paramet a in [24], we break 
down the architectur in two components: the first part 
handl the stale synchron among worker and 
the second provid a share data structur between workers. 

As a result, the most import chang come from (1) the 
event-driven natur of the paramet server and (2) the adap- 
tation of the iter control model to be ssp-compliant. 
In flink, when a job be submitted, it be first compil into 
a dataflow graph of task includ the execut control 
structures, and then transform into a physic execut 
plan that defin where each task will be executed. the task 
that handl iter be control by special structur that 
we call the iter control model. 

In the current iter control model, all the task 
spawn by an iter job be connect to a clock 
synchron sink in an event-driven fashion. the 
clock synchron sink hold the valu of the current 
cluster-wid clock, defin a the minimum clock valu 
among the workers. each time a worker finish a clock 
within an iter task, it send it updat clock to the 
clock synchron task. the clock synchron 
task collect all the clock updat from the worker, and 
keep track of all the intern clock for each worker. 
each time the minimum clock valu changes, the clock 
synchron sink broadcast the valu to all the worker 
a the new cluster-wid clock value. thi process be 
formal on figur 1. 

dure the execution, a worker start work on it 
next clock onli if it own clock valu be not great than 
the cluster-wid clock plu the valu of the staleness. 
thi constraint guarante that the fast worker be never 
ahead of the slowest worker by a number of clock great 
than the valu of the staleness, and thu verifi the ssp 
condit [24]. algorithm 3 illustr the process of a 



187 

1: C = (0, 0, 0 . . .) where |c| = |V | and V be the set of 
all worker 

2: clock = 0 

3: loop 
4: ci = receiv clock for worker vi 
5: Ci = ci 

6: if min(c) > clock then 
7: clock = min(c) 

8: broadcast clock to all worker a the new cluster- 
wide clock 

9: end if 
10: end loop 

algorithm 1. process of the clock synchron task involv in the 
ssp iter control model. 

worker under ssp accord to our iter control model. 
the paramet server be built on top of a data grid 

distribut among all the worker in the cluster. thi have 
the advantag that write make by a worker to the grid be 
propag and replic to each of the worker in the 
background. valu write by a worker be local store 
on it partit of the grid. write to the model overwrit the 
version present on the paramet server and read alway 
return the late updat write to the paramet server. 
the latter have the effect of directli “pushing” the late 
version of the share model and allow slow worker to 
immedi benefit from the advanc of faster workers, 
while the former enabl the benefit of local access in a 
pipelin implement of ssp. 

finally, one can notic that thi implement be fulli 
applic on pure stream process system such a storm. 

relat work: peetuum be the implement of a 
paramet server use for ssp iter a describ in 
[23]. the ssp iter control model be implement a 
block call to the workers. In a more recent contribution, 
[24] describ eager ssp (essp) in which updat to the 
model be push immedi to the workers. In our ssp 
integr model, write to the paramet server overwrit 

1: let α(0) ∈ D 
2: for k = 0, 1, 2, . . . do 
3: s(k) = argmins∈d 〈s,∇f(α(k))〉 
4: α(k+1) = (1 − γ)α(k) + γs(k), where γ = 2k+2 or 

obtain via line-search 
5: end for 
6: stop criterion: 〈α(k) − s(k),∇f(α(k))〉 ≤ � 

algorithm 2. the frank-wolf algorithm. 

the valu alreadi present. thi lead to the same effect a 
essp in practice. 

the concept of microstep iter describ a setup 
where data be partit among worker and each iter 
take a singl element from a work set and updat an 
element in a solut set have be defin in [25]. when 
the data flow between the solut set and the work 
set do not cross partit boundaries, thi lead to fulli 
asynchron iterations. however, in distribut process 
frameworks, thi make iter control difficult a well 
a the checkpoint of intermedi result for recovery. 
these issu howev can be address with a paramet 
server approach. 

iii. definit OF the distribut frank-wolf 
algorithm under stale synchron 

parallel 

In thi section, we remind the reader of the basic frank- 
wolf algorithm and it distribut counterpart. then, we 
present our variant of the distribut version under the ssp 
paradigm. finally, we state some nice properti of our 
algorithm and compar it to relat works. 

A. the frank-wolf algorithm 

the frank-wolf algorithm [17] be a simpl yet power 
algorithm target the follow optim problem: 

min 
x∈d 

f(x), (1) 

where the f be a continu differenti convex function, 
and the domain D be a compact convex subset of R. 
algorithm 2 show the basic frank-wolf algorithm. 

despit it simplicity, the frank-wolf algorithm show 
nice converg properties: let α∗ be the optim solut 
of equat (1), theorem 1 state that the frank-wolf 
algorithm find an �-approxim solut α̃ after o(1/�) 
iter at most. 

theorem 1 (jaggi, 2013 [26]). let Cf be the curvatur of 
f . algorithm 2 output a feasibl point α̃ satisfi f(α̃)− 
f(α∗) ≤ � after at most (1+δ)×6.75cf/� iter where 
δ ≥ 0 be the accuraci to which the linear sub-problem be 
solved. 

the o(1/�) standard converg rate of the frank-wolf 
algorithm compar badli to optim first order methods.2 

however, the frank-wolf method iter have good proper- 
ties. for example, when the convex domain D = conv(s) be 
a convex hull of anoth set S then the iter be express 
a a linear combin of element from S . In such a 
setting, if the element (refer to a atom thereafter) 
expos a structur (sparsiti or low-rank) then the iter can 
inherit thi structure. moreover, onli one atom per iter 

2thi can be improv with addit assumpt (see [27] and 
refer therein). 



188 

can be add to the current solution. thus, the iter of 
the algorithm have a compact represent that can be 
leverag to reduc the memori usag of the algorithm. 
We now recal the distribut version of the frank-wolf 
algorithm. 

B. distribut frank-wolf algorithm 

In [28], the author propos a distribut version of thi 
algorithm to solv a slightli differ problem, name : 

min 
α∈rn 

f(α) s.t. ‖α‖1 ≤ β, (2) 

where f(α) = g(aα) for a matrix A = [a1, . . . ,an] ∈ 
rd×n with d << n. We name atom a column of the matrix 
A. We assum with no loss of gener that the atom be 
unit norm vectors: ‖ai‖2 = 1. We consid the column-wis 
partit of A across a set of N worker V = {vi}ni=1. 
A node vi be give a set of column denot by Ai such 
that 

⋃ 
i Ai = A and Ai 

⋂aj = ∅ ∀i �= j. In thi setting, 
one wish to find a weight vector α ∈ Rn under a sparsiti 
constraint. thi formul be tightli relat to optim 
over an atom set a mention in [26] and fit well to the 
distribut setting: on the one hand the linear sub-problem 
can be comput in parallel and on the other hand the iter 
be a spars linear combin of atom allow for an 
effici commun scheme. 

each iter of the algorithm propos in [28] take 
place in three steps: 

step 1 : each worker comput the best atom si local 
and broadcast it to all other workers, 
step 2 : each worker elect the best atom from all the 
atom it have received, 
step 3 : each worker updat it local version of pa- 
ramet α use the atom select dure the previou 
step. 

thi algorithm be part of the bsp iter algorithm 
paradigm and the author demonstr the converg and 
correct of their approach. In [28] the author focu on 
decreas the amount of commun and the wait 
cost of their procedur and propos an extens to case 
where worker be heterogeneous. basically, they propos 
to distribut less atom to slow worker and more to the 
fast one by run a cluster algorithm to group atom 
around centroid that they propos to use a proxi on 
which to run the frank-wolf algorithm. 

however, they notic that run their algorithm in 
a simul asynchron environ not onli converg 
but also have a high acceler potential. when straggler 
randomli appear among workers, a common scenario in a 
data center os, an unbalanc partit like the one pro- 
pose in [28] be not appropriate. moreover, in such situation, 
obtain a dynam load-balanc schedul polici can 
be costly. thi scenario lead u to explor an asynchron 
variant we describ in the follow subsection. 

C. distribut frank-wolf under ssp 

We propos an asynchron version of the distribut 
frank-wolf algorithm base on the ssp paradigm. more 
precisely, each worker can use a local optim atom in 
order to updat it current possibl out-of-date, but with 
bound staleness, version of the paramet vector α. our 
claim be that avoid the synchron updat step can help 
the algorithm be toler to the straggler problem without 
sacrific the converg rate. algorithm 3 formal 
the use of the ssp paradigm to reach our goal. At each 
iteration, each worker request the current valu of the 
paramet store in a paramet server. then, it process 
the sub-problem optim step with respect to it local 
atom set. finally, it updat the paramet and write the 
new paramet valu on the paramet server. 

implement details: the atom be in the sup- 
port of the current solut be store onli onc in a share 
replic cach and the coeffici be partit across 
worker in anoth cache. thi allow for communication- 
effici updat propagation. the solut be updat in 
a greedi fashion. thi can lead to lose improv 
make by worker upstream when a late worker updat the 
paramet vector α. To prevent thi kind of scenario, the 
function updateparamet check the improv between 
the current store solut and the solut to be inserted. 
hence, our algorithm be guarante to make improv 
at each iteration. thi lead to the follow theorem that 
state the converg of our algorithm. 

theorem 2 (converg of algorithm 3). let Cf be the 
curvatur of f . algorithm 3 output a feasibl point α̃ satis- 
fy f(α̃)−f(α∗) ≤ � after at most 2s×(1+δ)×6.75cf/� 
iter where δ ≥ 0 be the accuraci to which the linear 
sub-problem be solved. 

sketch of proof: the intuit behind the proof be 
the following. each worker can onli improv the current 
solution. moreover, becaus of the ssp paradigm, we be 
assur to see each atom at least onc everi 2 iterations. 
thus, we make an updat that be a good a frank-wolf 
updat everi 2 iter at least. 

thi converg rate be a bad case result that badli 
compar to the origin distribut frank-wolf algorithm. 
however, in practice, we show in section V that our 
algorithm converg more quickli to the optimum. A more 
detail analysi of thi properti be left for futur work. 

We shall now make two proposit relat to properti 
of the distribut frank-wolf algorithm under ssp. the first 
one be that our asynchron set do not increas too 
much the number of non-zero entri of each of the iter 
that remain upper bounded. 

proposit 1 (sparsiti of the iterates). At iter k, the 



189 

1: let α(0)i = 0, ci = 0 for all worker vi ∈ V , clock = 0 
and s the stale parameter. 

2: clock be updat onward from the clock synchroniza- 
tion sink 

3: for all worker vi ∈ V in parallel do 
4: repeat 
5: if ci ≤ clock + s then 
6: α(ci) = getparameter() 

7: s(ci) = argmins∈di 〈s,∇f(α(ci))i〉 
8: α(ci+1) = (1 − γ)α(ci) + γs(ci), where γ be 

obtain via line-search 
9: updateparamet 

( 
i, ci,α 

(ci+1) 
) 

10: ci = ci + 1 

11: send ci to the clock synchron sink 
12: els 
13: wait until ci ≤ clock + s 
14: end if 
15: until 〈α(ci) − s(ci),∇f(α(ci))〉 ≤ � for all node vi 
16: end for 

algorithm 3. stale synchron parallel distribut frank-wolf algo- 
rithm. the main contribut lie in the stale bound (line 5) in which 
at each clock iter the late paramet be obtain (line 6). the local 
optim valu be updat (line 7-8) and an updat to the paramet server 
be sent (line 9). finally, the clock be increment and propag (line 
10-11). 

number of non-zero element in the weight vector αk be at 
most : 

‖αk‖0 ≤ k ×N (3) 
proof: It be easi to prove the proposit by induct 

on k. At the first iteration, k = 1 and sinc each worker 
can onli add one atom per iter to the current solution, 
henc add onli one non-zero coefficient, we have that 
‖α1‖0 ≤ N . In the bad case scenario, worker sequen- 
tialli updat the paramet within each iter and sinc 
Ai 

⋂aj = ∅ ∀i �= j there be at most N new non-zero entri 
in αk+1 compar to αk. suppos that the assumpt hold 
at iter k and show that it hold for k + 1 : 

‖αk+1‖0 ≤ ‖αk‖0 +N 
≤ k ×N +N 
≤ (k + 1)×n, 

where the first line come from the preced observ 
and the second one be give by our induct hypothesis. 
thi conclud the proof. 

the second proposit deal with an interest metric 
which be the commun cost incur by our algorithm. 
It tell u that the commun cost be not bad than 

the one of the distribut frank-wolf algorithm [28] in a 
star network setting. actually, in practice, the cost be slightli 
low in our implement becaus we do not need to send 
over the network an atom that have alreadi be chosen in 
a previou iteration. indeed, we leverag the cach featur 
of the paramet server. note that the algorithm of [28] can 
also benefit from thi featur with a small change. 

proposit 2 (commun cost). We assum that send- 
ing a number over the network have a constant cost. then 
the total commun cost of our algorithm to obtain an 
� approxim solut be at most 

O (δ × (n2 +nd)× 6.75cf/�) . 
sketch of proof: thi result be a straightforward appli- 

cation of theorem 1 jointli with proposit (1). 

again, thi be a bad case result assum that none of 
the atom be select twice. In practice, we will see that 
atom be select multipl time and henc we can leverag 
the atom grid that cach the alreadi select atom at a 
worker level avoid unnecessari atom broadcast onc 
it be present in the cache. 

relat work: In [28], the author have alreadi 
point out that their algorithm be robust to asynchron 
updates. under random commun drop to simul 
asynchronism, they show that their algorithm empir 
converges. In [19], the author studi an asynchron 
version of the frank-wolf algorithm to solv a convex 
optim problem subject to block-separ constraint 
and they discu a distribut variant of the algorithm. In 
a nutshell, they sampl group of atom from the entir set 
and each of these be process asynchron by a worker 
lie in a worker pool. when the sub-sampl set be empty, 
they updat the paramet and repeat the procedur until 
a converg criterion be met. they propos an extens 
in order to leverag a distribut environment. each worker 
asynchron sampl one block of coordin at a time 
and give back it result to a paramet server. everi τ 
result received, the paramet server updat the paramet 
and broadcast the new valu the workers. thi work be 
relat to what we present in thi paper but instead of let 
worker randomli choos block of variabl we leverag 
the data local provid by our pipelin set such that 
each block be assign to a worker at the begin of the 
process. In such a set we be sure not to process the 
same block more than onc at each iteration, thu reduc 
the number of redund comput that can appear in 
[19]. moreover, a our updat take place at the worker- 
side there be no comput overhead at the task manager. 
finally, we propos a more detail view on how atom be 
store and sent to workers. 



190 

iv. application: lasso regress 

We choos the lasso algorithm [20] to empir 
valid the effect of our approach. lasso be a 
linear regress method for solv the follow spars 
approxim problem : 

min 
α∈rn 

1 
2 ‖y −aα‖22 s.t. ‖α‖1 ≤ β, (4) 

where we seek to approxim the target valu yi for the 
train point i by a spars linear combin of it featur 
aij , use the same small number of featur for all data 
points. 

A. dualiti gap 

the frank-wolf algorithm expos a nice certif for 
the current iter quality, the so-cal duality-gap : 

h(α) = max 
α∈d 

〈α− s,∇f(α)〉 ≥ f(α)− f(α∗). 
given that s be a minim of the linear problem 
at point α, the dualiti gap be give by the by-product 
〈α − s,∇f(α)〉. moreov in the special case of lasso, 
the duality-gap be fast to compute. thi quantity, a show 
below, onli depend on inform that be avail at each 
worker level : 

〈α− s,∇f(α)〉 = − (α− s)t AT (y −aα) 
= (A (s−α))t (y −aα) 
= (as−aα)t r 

the residu be easili comput by each worker (or it can 
be broadcast along with the current spars approximation) 
and s belong to the set of avail column of thi worker. 
moreover, the matrix-vector product onli involv spars 
vector : As be onli the multipl of a column of A by 
a scalar. 

B. line search 

for lasso, the line search problem can be obtain 
analyt with nearli no addit comput cost. 
indeed, the optim step-siz be obtain by solv the 
follow problem : 

γ∗ = argmin 
γ∈[0,1] 

f 
( 
α(k) + γ(s(k) −α(k)) 

) 
, (5) 

which be equival to find the minimum of a quadrat 
function. set the deriv with respect to γ to zero 
give : 

∂f 

∂γ 
= 0 ⇔ − (a(s−α))t (y −a(α+ γ∗(s−α)) = 0 

⇔ γ∗ = (s−α) 
T 
AT (y −aα) 

‖a(s−α)‖22 
⇔ γ∗ = 〈α− s,∇f(α)〉‖a(s−α)‖22 

thu 

γ∗ = max 
( 
0, min 

( 
1, 

〈α− s,∇f(α)〉 
‖a(s−α)‖22 

)) 
(6) 

the optim step-siz be obtain through the duality-gap and 
‖a(s−α)‖22 can be evalu effici (becaus a(s−α) 
be avail a it be involv in the duality-gap computation). 

V. experi 
We have implement the ssp model on an exist 

pipelin distribut process framework. the pipelin 
model we have describ do not reli on ani particular 
framework featur and be suit for most exist frame- 
work support a pipelin setup. We have chosen the 
apach flink project a the basi for our implementation. 
At it core flink defin each data process job a a 
dataflow graph with pipelin operators. the graph describ 
the transform of the data set dure the process. 

the platform support the execut of iter algo- 
rithm with a converg criterion. like mani other frame- 
works, it support the bulk synchron parallel model for 
iter algorithm among distribut workers. We have ex- 
tend the exist structur for the control of the iter 
to implement ssp and we have integr our own paramet 
server built on top of a data grid. 

We have run experi on spars random matrix of 
dimens 1.000 × 10.000 with a sparsiti ratio of 0.001 
and a random vector α∗ such that ‖α∗‖0 = 100. for each 
stale bounds, we have repeat the experi 5 times. 
the cluster be compos of five node on the same switch, 
each equip with a 2ghz single-cor equival and 3gb 
of memory. We use a plain bsp implement of the 
frank-wolf algorithm in flink with no use of the paramet 
server a the baselin for our comparisons. 

A. converg and qualiti of the solut 

figur 4 show the evolut of the residu over the 
iterations. We observ that, despit the stale introduc 
by ssp, the model converg well. under the valu for the 
stale that we have experi with, the converg 
with regard to the residu be even good than the baselin 
bsp counterpart with no staleness. thi can be explain 
by the fact that in ssp, a worker at a certain clock can read 
a model that have alreadi be updat by anoth worker at 
a more advanc clock. while be sever clock behind, 
a worker can alreadi work with a more converg model 
updat by other workers. 

B. perform under load 

In order to test our solut in a situat where differ 
framework share the same cluster in an environ similar 
to yarn or mesos, we have write a tool that gener 
load on the hosts. the gener load be independ from 
the process framework and be not predict from the 
schedul point of view of the process framework. 



191 

thi behaviour simul a concurr task deploy on 
the cluster by anoth framework involv comput or 
garbag collection. thi type of temporari load have an 
impact on the perform of a worker but due to it short 
duration, comput a dynam load-balanc polici and 
reloc the task from a worker to anoth host be costli 
and inefficient. 

figur 4 show the converg of the residu under 
gener load. our tool select run a compute-intens 
process on a random worker for 12 second continuously, 
one worker at a time. the result show that even under load 
our solut be abl to perform good than the baselin bsp 
implementation, with a speedup of up to two time faster. 

C. sparsiti of the final model 

figur 5 show the distribut of the αi coeffici at 
converg after 250 iter for our ssp implement 
and the baselin bsp implementation. the result show that 
the final model obtain with our ssp implement be 
nearli two time less spars than the model obtain with 
bsp, despit be asynchron with a stale paramet 
of 10 and have 5 worker in parallel. thi be in line with 
the bound state in proposit 1. 

these result howev show that the final model have a 
high proport of veri small αi coefficients. thi open 
the way to prune method that will clean up the model 
and render it more sparse. It be possibl to prune the model 
either dynamically, in which case the intermedi model 
will then be cleaner, either at the end of the job, which 
allow for a good control of the error. To do so, we think 
that incorpor the use of away-step [29] may be a good 
candid to remov those small coefficients. 

50 100 150 200 250 

iter 

0.00 

0.01 

0.02 

0.03 

0.04 

0.05 

0.06 

0.07 

0.08 

0.09 

O 
b 
je 

c 
ti 

v 
e 

bsp-load 

ssp0-load 

ssp5-load 

ssp10-load 

0 50 100 150 200 

time (s) 

bsp-load 

ssp0-load 

ssp5-load 

ssp10-load 

figur 4. evolut of the residu of the frank-wolf algorithm under 
variou valu of ssp stale and in bsp, in function of the iter 
(left) and over time (right). A load be gener on a random node in 
the cluster at ani moment dure twelv second for the durat the 
experiment. 

−0.005 0.000 0.005 0.010 0.015 0.020 

bsp 

0 

10 

20 

30 

40 

50 

60 

−0.005 0.000 0.005 0.010 0.015 0.020 

ssp-10 

figur 5. histogram of the distribut of the weight in the solut 
obtain after 250 iterations. left: bsp, right: ssp with stale 10. 

vi. conclus and futur direct 

In thi paper we have present a model for ssp iter 
suit for distribut and pipelin process frameworks. 
We have propos an implement of the frank-wolf 
algorithm under ssp to ass our ssp framework. We have 
theoret show the upper bound of the distribut and 
ssp variant of the algorithm, a well a it convergence. We 
show with experi that our solut be abl to converg 
faster than it bsp counterpart, even under a randomli 
load environment. thi result be particularli valuabl in 
the light of cluster manag by the recent resourc man- 
agers, in which sever distribut process framework 
compet for resourc over their respect tasks. 

our futur line of research includ the studi of the more 
gener context of optim over atom sets, the studi 
of the sparsiti of the iter (away step might be useful) 
and the comparison of our solut with other asynchron 
approach like [19], [30]. 

acknowledg 

the author would like to thank the apach flink team 
for their support dure the implementation. 

refer 

[1] applic power by hadoop. [online]. available: 
http://wiki.apache.org/hadoop/poweredbi 

[2] J. ekanayake, H. li, B. zhang, T. gunarathne, s.-h. bae, 
J. qiu, and G. fox, “twister: A runtim for iter mapre- 
duce,” in proceed of the 19th acm intern sym- 
posium on high perform distribut computing, ser. 
hpdc ’10. new york, ny, usa: acm, 2010, pp. 810– 
818. 

[3] Y. bu, B. howe, M. balazinska, and M. D. ernst, “haloop: 
effici iter data process on larg clusters,” proc. 
vldb endow., vol. 3, no. 1-2, pp. 285–296, sep. 2010. 

[4] M. weimer, T. condie, and R. ramakrishnan, “machin 
learn in scalops, a high order cloud comput lan- 
guage,” in nip 2011 workshop on parallel and large-scal 
machin learn (biglearn), decemb 2011. 

[5] L. G. valiant, “A bridg model for parallel computation,” 
commun. acm, vol. 33, no. 8, pp. 103–111, aug. 1990. 



192 

[6] D. G. murray, F. mcsherry, R. isaacs, M. isard, P. barham, 
and M. abadi, “naiad: A time dataflow system,” in pro- 
ceed of the 24th acm symposium on oper system 
principl (sosp). acm, novemb 2013. 

[7] M. zaharia, M. chowdhury, T. das, A. dave, J. ma, M. mc- 
cauley, M. J. franklin, S. shenker, and I. stoica, “re- 
silient distribut datasets: A fault-toler abstract for 
in-memori cluster computing,” in proceed of the 9th 
usenix confer on network system design and im- 
plementation, ser. nsdi’12. berkeley, ca, usa: usenix 
association, 2012, pp. 2–2. 

[8] S. dudoladov, C. xu, S. schelter, A. katsifodimos, S. ewen, 
K. tzoumas, and V. markl, “optimist recoveri for itera- 
tive dataflow in action,” in proceed of the 2015 acm 
sigmod intern confer on manag of data, 
melbourne, victoria, australia, may 31 - june 4, 2015, 2015, 
pp. 1439–1443. 

[9] S. ewen, S. schelter, K. tzoumas, D. warneke, and V. markl, 
“iter parallel data process with stratosphere: An insid 
look,” in proceed of the 2013 acm sigmod interna- 
tional confer on manag of data, ser. sigmod 
’13. new york, ny, usa: acm, 2013, pp. 1053–1056. 

[10] flink iter and delta-iteration. [online]. 
available: ci.apache.org/projects/flink/flink-docs-release-0.6/ 
iterations.html 

[11] N. marz. big data lambda architecture. [on- 
line]. available: http://www.databasetube.com/database/ 
big-data-lambda-architecture/ 

[12] B. hindman, A. konwinski, M. zaharia, A. ghodsi, A. D. 
joseph, R. katz, S. shenker, and I. stoica, “mesos: A 
platform for fine-grain resourc share in the data center,” 
in proceed of the 8th usenix confer on network 
system design and implementation, ser. nsdi’11. berkeley, 
ca, usa: usenix association, 2011, pp. 295–308. 

[13] V. K. vavilapalli, A. C. murthy, C. douglas, S. agarwal, 
M. konar, R. evans, T. graves, J. lowe, H. shah, S. seth, 
B. saha, C. curino, O. o’malley, S. radia, B. reed, and 
E. baldeschwieler, “apach hadoop yarn: yet anoth re- 
sourc negotiator,” in proceed of the 4th annual sym- 
posium on cloud computing, ser. socc ’13. new york, 
ny, usa: acm, 2013, pp. 5:1–5:16. 

[14] M. schwarzkopf, A. konwinski, M. abd-el-malek, and 
J. wilkes, “omega: flexible, scalabl schedul for larg 
comput clusters,” in proceed of the 8th acm european 
confer on comput systems, ser. eurosi ’13. new 
york, ny, usa: acm, 2013, pp. 351–364. 

[15] (2012) under the hood: schedul mapreduc job 
more effici with corona. [online]. available: http: 
//on.fb.me/txusyn 

[16] H. cui, J. cipar, Q. ho, J. K. kim, S. lee, A. kumar, J. wei, 
W. dai, G. R. ganger, P. B. gibbons, G. A. gibson, and 
E. P. xing, “exploit bound stale to speed up big data 
analytics,” in proceed of the 2014 usenix confer on 
usenix annual technic conference, ser. usenix atc’14. 
berkeley, ca, usa: usenix association, 2014, pp. 37–48. 

[17] M. frank and P. wolfe, “an algorithm for quadrat program- 
ming,” naval research logist quarterly, vol. 3, no. 1-2, pp. 
95–110, 1956. 

[18] A. bellet, Y. liang, A. B. garakani, M. balcan, and F. sha, 
“distribut frank-wolf algorithm: A unifi framework 
for communication-effici spars learning,” corr, vol. 
abs/1404.2644, 2014. 

[19] y.-x. wang, V. sadhanala, W. dai, W. neiswanger, S. sra, and 
E. P. xing, “asynchron parallel block-coordin frank- 
wolfe,” 2014. 

[20] R. tibshirani, “regress shrinkag and select via the 
lasso,” journal of the royal statist society, vol. 58, no. 1, 
pp. 267–288, 1996. 

[21] G. De francisci moral and A. bifet, “samoa: scalabl 
advanc massiv onlin analysis,” J. mach. learn. res., 
vol. 16, no. 1, pp. 149–153, jan. 2015. 

[22] J. cipar, Q. ho, J. K. kim, S. lee, G. R. ganger, G. gibson, 
K. keeton, and E. xing, “solv the straggler problem with 
bound staleness,” in proceed of the 14th usenix con- 
ferenc on hot topic in oper systems, ser. hotos’13. 
berkeley, ca, usa: usenix association, 2013, pp. 22–22. 

[23] Q. ho, J. cipar, H. cui, and S. lee, “more effect 
distribut ML via a stale synchron parallel paramet 
server,” nips, no. july, pp. 1–9, 2013. 

[24] W. dai, A. kumar, J. wei, Q. ho, G. gibson, and E. P. 
xing, “analysi of high-perform distribut ML at scale 
through paramet server consist models,” 2014. 

[25] S. ewen, K. tzoumas, M. kaufmann, and V. markl, “spin- 
ning fast iter data flows,” proc. vldb endow., vol. 5, 
no. 11, pp. 1268–1279, jul. 2012. 

[26] M. jaggi, “revisit frank-wolfe: projection-fre spars 
convex optimization,” proceed of the 30th intern 
confer on machin learning, vol. 28, pp. 427–435, 2013. 

[27] D. garber and E. hazan, “faster rate for the frank-wolf 
method over strongly-convex sets,” in the 32nd intern 
confer on machin learning, 2015. 

[28] A. bellet, Y. liang, A. B. garakani, F. sha, and m.-f. balcan, 
“A distribut frank-wolf algorithm for communication- 
effici spars learning,” 2015. 

[29] J. guélat and P. marcotte, “some comment of wolfe’ 
‘away step’,” math. program., vol. 35, no. 1, pp. 110–119, 
may 1986. [online]. available: http://dx.doi.org/10.1007/ 
bf01589445 

[30] J. liu, S. wright, C. ré, and V. bittorf, “an asynchron 
parallel stochast coordin descent algorithm,” 2014. 
















<< 
/ascii85encodepag fals 
/allowtranspar fals 
/autopositionepsfil fals 
/autorotatepag /none 
/bind /left 
/calgrayprofil (gray gamma 2.2) 
/calrgbprofil (srgb iec61966-2.1) 
/calcmykprofil (u.s. web coat \050swop\051 v2) 
/srgbprofil (srgb iec61966-2.1) 
/cannotembedfontpolici /warn 
/compatibilitylevel 1.4 
/compressobject /off 
/compresspag true 
/convertimagestoindex true 
/passthroughjpegimag true 
/createjobticket fals 
/defaultrenderingint /default 
/detectblend true 
/detectcurv 0.0000 
/colorconversionstrategi /leavecolorunchang 
/dothumbnail fals 
/embedallfont true 
/embedopentyp fals 
/parseiccprofilesincom true 
/embedjobopt true 
/dscreportinglevel 0 
/emitdscwarn fals 
/endpag -1 
/imagememori 1048576 
/lockdistillerparam true 
/maxsubsetpct 100 
/optim fals 
/opm 0 
/parsedsccom fals 
/parsedsccommentsfordocinfo fals 
/preservecopypag true 
/preservedicmykvalu true 
/preserveepsinfo fals 
/preserveflat true 
/preservehalftoneinfo true 
/preserveopicom fals 
/preserveoverprintset true 
/startpag 1 
/subsetfont fals 
/transferfunctioninfo /remov 
/ucrandbginfo /preserv 
/useprologu fals 
/colorsettingsfil () 
/alwaysemb [ true 
/arial-black 
/arial-bolditalicmt 
/arial-boldmt 
/arial-italicmt 
/arialmt 
/arialnarrow 
/arialnarrow-bold 
/arialnarrow-boldital 
/arialnarrow-ital 
/arialunicodem 
/bookantiqua 
/bookantiqua-bold 
/bookantiqua-boldital 
/bookantiqua-ital 
/bookmanoldstyl 
/bookmanoldstyle-bold 
/bookmanoldstyle-boldital 
/bookmanoldstyle-ital 
/bookshelfsymbolseven 
/centuri 
/centurygoth 
/centurygothic-bold 
/centurygothic-boldital 
/centurygothic-ital 
/centuryschoolbook 
/centuryschoolbook-bold 
/centuryschoolbook-boldital 
/centuryschoolbook-ital 
/comicsansm 
/comicsansms-bold 
/couriernewps-bolditalicmt 
/couriernewps-boldmt 
/couriernewps-italicmt 
/couriernewpsmt 
/estrangeloedessa 
/franklingothic-medium 
/franklingothic-mediumital 
/garamond 
/garamond-bold 
/garamond-ital 
/gautami 
/georgia 
/georgia-bold 
/georgia-boldital 
/georgia-ital 
/haettenschweil 
/impact 
/kartika 
/latha 
/lettergothicmt 
/lettergothicmt-bold 
/lettergothicmt-boldobliqu 
/lettergothicmt-obliqu 
/lucidaconsol 
/lucidasan 
/lucidasans-demi 
/lucidasans-demiital 
/lucidasans-ital 
/lucidasansunicod 
/mangal-regular 
/microsoftsansserif 
/monotypecorsiva 
/msreferencesansserif 
/msreferencespecialti 
/mvboli 
/palatinolinotype-bold 
/palatinolinotype-boldital 
/palatinolinotype-ital 
/palatinolinotype-roman 
/raavi 
/shruti 
/sylfaen 
/symbolmt 
/tahoma 
/tahoma-bold 
/timesnewromanmt-extrabold 
/timesnewromanps-bolditalicmt 
/timesnewromanps-boldmt 
/timesnewromanps-italicmt 
/timesnewromanpsmt 
/trebuchet-boldital 
/trebuchetm 
/trebuchetms-bold 
/trebuchetms-ital 
/tunga-regular 
/verdana 
/verdana-bold 
/verdana-boldital 
/verdana-ital 
/vrinda 
/webd 
/wingdings2 
/wingdings3 
/wingdings-regular 
/zwadobef 
] 
/neveremb [ true 
] 
/antialiascolorimag fals 
/cropcolorimag true 
/colorimageminresolut 200 
/colorimageminresolutionpolici /ok 
/downsamplecolorimag true 
/colorimagedownsampletyp /bicub 
/colorimageresolut 300 
/colorimagedepth -1 
/colorimagemindownsampledepth 1 
/colorimagedownsamplethreshold 1.50000 
/encodecolorimag true 
/colorimagefilt /dctencod 
/autofiltercolorimag fals 
/colorimageautofilterstrategi /jpeg 
/coloracsimagedict << 
/qfactor 0.76 
/hsampl [2 1 1 2] /vsampl [2 1 1 2] 
>> 
/colorimagedict << 
/qfactor 0.76 
/hsampl [2 1 1 2] /vsampl [2 1 1 2] 
>> 
/jpeg2000coloracsimagedict << 
/tilewidth 256 
/tileheight 256 
/qualiti 15 
>> 
/jpeg2000colorimagedict << 
/tilewidth 256 
/tileheight 256 
/qualiti 15 
>> 
/antialiasgrayimag fals 
/cropgrayimag true 
/grayimageminresolut 200 
/grayimageminresolutionpolici /ok 
/downsamplegrayimag true 
/grayimagedownsampletyp /bicub 
/grayimageresolut 300 
/grayimagedepth -1 
/grayimagemindownsampledepth 2 
/grayimagedownsamplethreshold 1.50000 
/encodegrayimag true 
/grayimagefilt /dctencod 
/autofiltergrayimag fals 
/grayimageautofilterstrategi /jpeg 
/grayacsimagedict << 
/qfactor 0.76 
/hsampl [2 1 1 2] /vsampl [2 1 1 2] 
>> 
/grayimagedict << 
/qfactor 0.76 
/hsampl [2 1 1 2] /vsampl [2 1 1 2] 
>> 
/jpeg2000grayacsimagedict << 
/tilewidth 256 
/tileheight 256 
/qualiti 15 
>> 
/jpeg2000grayimagedict << 
/tilewidth 256 
/tileheight 256 
/qualiti 15 
>> 
/antialiasmonoimag fals 
/cropmonoimag true 
/monoimageminresolut 400 
/monoimageminresolutionpolici /ok 
/downsamplemonoimag true 
/monoimagedownsampletyp /bicub 
/monoimageresolut 600 
/monoimagedepth -1 
/monoimagedownsamplethreshold 1.50000 
/encodemonoimag true 
/monoimagefilt /ccittfaxencod 
/monoimagedict << 
/K -1 
>> 
/allowpsxobject fals 
/checkcompli [ 
/none 
] 
/pdfx1acheck fals 
/pdfx3check fals 
/pdfxcompliantpdfonli fals 
/pdfxnotrimboxerror true 
/pdfxtrimboxtomediaboxoffset [ 
0.00000 
0.00000 
0.00000 
0.00000 
] 
/pdfxsetbleedboxtomediabox true 
/pdfxbleedboxtotrimboxoffset [ 
0.00000 
0.00000 
0.00000 
0.00000 
] 
/pdfxoutputintentprofil (none) 
/pdfxoutputconditionidentifi () 
/pdfxoutputcondit () 
/pdfxregistrynam () 
/pdfxtrap /fals 

/createjdffil fals 
/descript << 
/ch <feff4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002> 
/cht <feff4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002> 
/dan <feff004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e> 
/deu <feff00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e> 
/esp <feff005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e> 
/fra <feff005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e> 
/ita (utilizzar quest impostazioni per crear documenti adob pdf adatti per visualizzar e stampar documenti aziendali in modo affidabile. I documenti pdf creati possono esser aperti con acrobat e adob reader 5.0 e versioni successive.) 
/jpn <feff30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002> 
/kor <feffc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e> 
/nld (gebruik deze instellingen om adob pdf-documenten te maken waarme zakelijk documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakt pdf-documenten kunnen worden geopend met acrobat en adob reader 5.0 en hoger.) 
/nor <feff004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e> 
/ptb <feff005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e> 
/suo <feff004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e> 
/sve <feff0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e> 
/enu (use these set to creat pdf that match the "required" set for pdf specif 4.01) 
>> 
>> setdistillerparam 
<< 
/hwresolut [600 600] 
/pages [612.000 792.000] 
>> setpagedevic 

