


































































Mutual information, neural networks and the renormalization group


Articles
https://doi.org/10.1038/s41567-018-0081-4

1Institute for Theoretical Physics, ETH Zurich, Zurich, Switzerland. 2Racah Institute of Physics, Hebrew University of Jerusalem, Jerusalem, Israel.  
*e-mail: maciejk@ethz.ch

Machine learning has been captivating public attention lately due to groundbreaking advances in automated translation, image and speech recognition1, game-playing2 and achiev-
ing super-human performance in tasks in which humans excelled 
while more traditional algorithmic approaches struggled3. The 
applications of those techniques in physics are very recent, initially 
leveraging the trademark prowess of machine learning in classifica-
tion and pattern recognition and applying them to classify phases 
of matter4–8, study amorphous materials9,10, or exploiting the neural 
networks’ potential as efficient nonlinear approximators of arbitrary 
functions11,12 to introduce a new numerical simulation method for 
quantum systems13,14. However, the exciting possibility of employing 
machine learning not as a numerical simulator, or a hypothesis tester, 
but as an integral part of the physical reasoning process is still largely 
unexplored and, given the staggering pace of progress in the field of 
artificial intelligence, of fundamental importance and promise.

The renormalization group (RG) approach has been one of the 
conceptually most profound tools of theoretical physics since its 
inception. It underlies the seminal work on critical phenomena15, 
and the discovery of asymptotic freedom in quantum chromody-
namics16, and of the Kosterlitz–Thouless phase transition17,18. The 
RG is not a monolith, but rather a conceptual framework compris-
ing different techniques: real-space RG19, functional RG20 and den-
sity matrix RG21, among others. While all of those schemes differ 
quite substantially in their details, style and applicability, there is 
an underlying physical intuition that encompasses all of them—the 
essence of RG lies in identifying the ‘relevant’ degrees of freedom 
and integrating out the ‘irrelevant’ ones iteratively, thereby arriving 
at a universal, low-energy effective theory. However potent the RG 
idea, those relevant degrees of freedom need to be identified first22,23. 
This is often a challenging conceptual step, particularly for strongly 
interacting systems, and may involve a sequence of mathematical 
mappings to models, whose behaviour is better understood24,25.

Here we introduce an artificial neural network algorithm itera-
tively identifying the physically relevant degrees of freedom in 
a spatial region and performing an RG coarse-graining step. The 
input data are samples of the system configurations drawn from 

a Boltzmann distribution; no further knowledge about the micro-
scopic details of the system is provided. The internal parameters 
of the network, which ultimately encode the degrees of freedom 
of interest at each step, are optimized (‘learned’, in neural network 
parlance) by a training algorithm based on evaluating real-space 
mutual information (RSMI) between spatially separated regions. 
We validate our approach by studying the Ising and dimer models 
of classical statistical physics in two dimensions. We obtain the RG 
flow and extract the Ising critical exponent. The robustness of the 
RSMI algorithm to physically irrelevant noise is demonstrated.

The identification of the important degrees of freedom, and the 
ability to execute a real-space RG procedure19, has not only quanti-
tative but also conceptual significance: it allows one to gain insights 
into the correct way of thinking about the problem at hand, raising 
the prospect that machine-learning techniques may augment the 
scientific inquiry in a fundamental fashion.

The RSMI algorithm
Before going into more detail, let us provide a bird’s eye view of our 
method and results. We begin by phrasing the problem in probabi-
listic/information-theoretic terms, a language also used in refs 26–30.  
To this end, we consider a small ‘visible’ spatial area V , which 
together with its environment E  forms the system X , and we define 
a particular conditional probability distribution ∣Λ H VP ( ), which 
describes how the relevant degrees of freedom H (‘dubbed hiddens’) 
in V  depend on both V  and E . We then show that the sought-after 
conditional probability distribution is found by an algorithm maxi-
mizing an information-theoretic quantity, the mutual information, 
and that this algorithm lends itself to a natural implementation 
using artificial neural networks. We describe how RG is practically 
performed by coarse-graining with respect to ∣Λ H VP ( ) and iterating 
the procedure. Finally, we provide a verification of our claims by 
considering two paradigmatic models of statistical physics: the Ising 
model—for which the RG procedure yields the famous Kadanoff 
block spins—and the dimer model, whose relevant degrees of free-
dom are much less trivial. We reconstruct the RG flow of the Ising 
model and extract the critical exponent.

Mutual information, neural networks and the 
renormalization group
Maciej Koch-Janusz   1* and Zohar Ringel2

Physical systems differing in their microscopic details often display strikingly similar behaviour when probed at macroscopic 
scales. Those universal properties, largely determining their physical characteristics, are revealed by the powerful renormal-
ization group (RG) procedure, which systematically retains ‘slow’ degrees of freedom and integrates out the rest. However, 
the important degrees of freedom may be difficult to identify. Here we demonstrate a machine-learning algorithm capable of 
identifying the relevant degrees of freedom and executing RG steps iteratively without any prior knowledge about the system. 
We introduce an artificial neural network based on a model-independent, information-theoretic characterization of a real-space 
RG procedure, which performs this task. We apply the algorithm to classical statistical physics problems in one and two dimen-
sions. We demonstrate RG flow and extract the Ising critical exponent. Our results demonstrate that machine-learning tech-
niques can extract abstract physical concepts and consequently become an integral part of theory- and model-building.

© 2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.

NaTuRe PhySIcS | www.nature.com/naturephysics

mailto:maciejk@ethz.ch
http://orcid.org/0000-0002-2903-5202
http://www.nature.com/naturephysics


Articles NaTuRe PHysIcs

Consider then a classical system of local degrees of freedom 
= … ≡X x x x{ , , } { }N i1 , defined by a Hamiltonian energy function 

H({xi}) and associated statistical probabilities ∝ β−XP( ) e xH({ })i , 
where β is the inverse temperature. Alternatively (and sufficiently 
for our purposes), the system is given by Monte Carlo samples of the 
equilibrium distribution XP( ). We denote a small spatial region of 
interest by ≡V v{ }i  and the remainder of the system by ≡E e{ }i , so 
that =X V E( , ). We adopt a probabilistic point of view, and treat X E,  
and so on as random variables. Our goal is to extract the relevant 
degrees of freedom H from V .

‘Relevance’ is understood here in the following way: the degrees 
of freedom that RG captures govern the long-distance behaviour 
of the theory, and therefore the experimentally measurable physi-
cal properties; they carry the most information about the system 
at large, as opposed to local fluctuations. We thus formally define 
the random variable H as a composite function of degrees of free-
dom in V  maximizing the ‘mutual information’ between H and the 
environment E . This definition, as we discuss in the Supplementary 
Information, is related to the requirement that the effective coarse-
grained Hamiltonian be compact and short-ranged, which is a con-
dition any successful standard RG scheme should satisfy. As we also 
show, it is supported by numerical results.

Mutual information, denoted by Iλ, measures the total amount of 
information about one random variable contained in the other9,10,31 
(thus, it is more general than correlation coefficients). It is given in 
our setting by:

∑=Λ Λ Λ
Λ











H E E H
E H

H E
H E

I P
P

P P
( : ) ( , )log

( , )
( ) ( ) (1)

,

The unknown distribution Λ E HP ( , ) and its marginalization 
Λ HP ( ), depending on a set of parameters Λ (which we keep generic 

at this point), are functions of V EP( , ) and of ∣Λ H VP ( ), which is the 
central object of interest.

Finding ∣Λ H VP ( ) that maximizes IΛ under certain constraints is 
a well-posed mathematical question and has a formal solution32.  

However, since the space of probability distributions grows expo-
nentially with the number of local degrees of freedom, it is, in 
practice, impossible to use without further assumptions for any 
but the smallest physical systems. Our approach is to exploit the 
remarkable dimensionality reduction properties of artificial neural 
networks11. We use restricted Boltzmann machines (RBMs), a class 
of probabilistic networks well adapted to approximating arbitrary 
data probability distributions. An RBM is composed of two layers 
of nodes, the ‘visible’ layer, corresponding to local degrees of free-
dom in our setting, and a ‘hidden’ layer. The interactions between 
the layers are defined by an energy function ≡ θΘ V HE E ( , )a b, ,  =   
− ∑ b hj j j −  ∑ a vi i i −  θ∑ v hij i ij j, such that the joint probability distri-
bution for a particular configuration of visible and hidden degrees 
of freedom is given by a Boltzmann weight:

=Θ
− θV H

Z
V HP ( , ) 1 e (2)E ( , )a b, ,

where Z  is the normalization. The goal of the network training is 
to find parameters θij (‘weights’ or ‘filters’) and ai,bi optimizing a 
chosen objective function.

Three distinct RBMs are used. Two are trained as efficient 
approximators of the probability distributions V EP( , ) and VP( ), 
using the celebrated contrastive divergence (CD) algorithm33. Their 
trained parameters are used by the third network (see Fig. 1b),  
which has a different objective: to find ∣Λ H VP ( ) maximizing IΛ. To 
the end we introduce the real-space mutual information (RSMI) 
network, whose architecture is shown in Fig. 1a. The hidden units 
of RSMI correspond to coarse-grained variables H.

The parameters λΛ = a b( , , )i j i
j  of the RSMI network are trained 

by an iterative procedure. At each iteration, a Monte Carlo estimate 
of function Λ H EI ( : ) and its gradients is performed for the current 
values of parameters Λ. The gradients are then used to improve 
the values of weights in the next step, using a stochastic gradient 
descent procedure.

The trained weights Λ define the probability ∣Λ H VP ( ) of a 
Boltzmann form, which is used to generate MC samples of the coarse-
grained system. Those, in turn, become input to the next iteration of 
the RSMI algorithm. The estimates of mutual information, weights of 
the trained RBMs and sets of generated MC samples at every RG step 
can be used to extract quantitative information about the system in 
the form of correlation functions, critical exponents and so on, as we 
show below and in the Supplementary Information. We also empha-
size that the parameters Λ identifying relevant degrees of freedom are 
re-computed at every RG step. This potentially allows RSMI to capture 
the evolution of the degrees of freedom along the RG flow34.

Validation
To validate our approach, we consider two important classical mod-
els of statistical physics: the Ising model, whose coarse-grained 
degrees of freedom resemble the original ones, and the fully packed 
dimer model, where they are entirely different.

Ha

b

B

P( )

CD CD RSMI

PΛ(H∣ )
λ ji

θ

θ( )
),

( ),

P(

Fig. 1 | The RSMI algorithm. a, The RSMI neural network architecture. The 
hidden layer H is directly coupled to the visible layer V  via the weights λi

j 
(red arrows). However, the training algorithm for the weights estimates 
mutual information between H and the environment E . The buffer B is 
introduced to filter out local correlations within V  (see Supplementary 
Information). b, The workflow of the algorithm. The CD-algorithm-trained 
RBMs learn to approximate probability distributions V EP( , ) and VP( ). Their 
final parameters, denoted collectively by V EΘ( , ) and VΘ( ), are inputs for the 
main RSMI network learning to extract H V∣ΛP ( ) by maximizing IΛ. The final 
weights λi

j of the RSMI network identify the relevant degrees of freedom. 
They are shown in Figs. 2 and 4 for Ising and dimer problems.

0 1

0

a b

1
–2.5

0

2.5

0 1

0

1

0 1

0

1

0 1

0

1

0 1

0

1

Fig. 2 | The weights of the RSMI network trained on the Ising model. 
Visualization of the weights of the RSMI network trained on the Ising model 
for a visibile area V  of 2 ×  2 spins. The ANN couples strongly to areas with 
large absolute value of the weights. a, The weights for Nh =  1 hidden neuron: 
the ANN discovers Kadanoff blocking. b, The weights for Nh =  4 hidden 
neurons: each neuron tracks one original spin.

© 2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.

NaTuRe PhySIcS | www.nature.com/naturephysics

http://www.nature.com/naturephysics


ArticlesNaTuRe PHysIcs

The Ising Hamiltonian on a two-dimensional (2D) square lattice is:

∑=
⟨ ⟩

H s s (3)
i j

i jI
,

with si =  ± 1 and the summation over nearest neighbours. Real-space 
RG of the Ising model proceeds by the block-spin construction19, 
whereby each 2 ×  2 block of spins is coarse-grained into a single 
effective spin, whose orientation is decided by a ‘majority’ rule.

The results of the RSMI algorithm trained on Ising model sam-
ples are shown in Fig. 2. We vary the number of both hidden neu-
rons Nh and the visible units, which are arranged in a 2D area V  
of size L ×  L (see Fig. 1a). For a four-spin area, the network indeed 
rediscovers the famous Kadanoff block-spin: Fig. 2a shows a single 
hidden unit coupling uniformly to four visible spins (that is, the ori-
entation of the hidden unit is decided by the average magnetization 
in the area). Figure 2b is a trivial but important sanity check: given 
four hidden units to extract relevant degrees of freedom from an 
area of four spins, the networks couples each hidden unit to a dif-
ferent spin, as expected. In the Supplementary Information we also 
compare the weights for areas V  of different size, which are general-
izations of the Kadanoff procedure to larger blocks.

We next study the dimer model, given by an entropy-only parti-
tion function, which counts the number of dimer coverings of the 
lattice (that is, subsets of edges such that every vertex is the endpoint 
of exactly one edge). Figure 3a shows sample dimer configurations 
(and additional spin degrees of freedom added to generate noise). 
This deceptively simple description hides non-trivial physics35 and 
correspondingly, the RG procedure for the dimer model is more 
subtle, since—in contrast to the Ising case—the correct degrees of 
freedom to perform RG on are not dimers, but rather look like effec-
tive local electric fields. This is revealed by a mathematical mapping 
to a ‘height field’ h (see Fig. 3a,b and ref. 36), whose gradients behave 
like electric fields. The continuum limit of the dimer model is given 
by the following action:

∫ ∫= ∇ ≡S h x h xx E x[ ] d ( ( )) d ( ) (4)dim 2 2 2 2

and therefore the coarse-grained degrees of freedom are low-
momentum (Fourier) components of the electrical fields Ex,Ey in the 
x and y directions. They correspond to ‘staggered’ dimer configura-
tions shown in Fig. 3a.

Remarkably, the RSMI algorithm extracts the local electric fields 
from the dimer model samples without any knowledge of those 
mappings. In Fig. 4, the weights for Nh =  2 and Nh =  4 hidden neu-
rons, for an 8 ×  8 area (similar to Fig. 3a), are shown: the pattern 
of large negative (blue) weights couples strongly to a dimer pattern 

0 1 2 3 4 5 6 7

0

a

b

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7

0 1 2 3 4 5 6 7

0

1

2

3

4

5

6

7

0

1

2

3

4

5

6

7

0 1 2 3 4 5 6 7
–2

–1

0

1

2

Fig. 4 | The weights of the RSMI network trained on dimer model data. a, Nh =  2 hidden neurons for a visible area V  of 8 ×  8 spins. The two filters 
recognize Ey and Ex +  Ey electrical fields, respectively (compare with dimer patterns in Fig. 3a). b, The trained weights for Nh =  4 hidden neurons.

0

a

b

1 0 1

2 233

4 5 4 5

77 66

0

0 0

0

1

11

1

3 32 2

3 32 2

Fig. 3 | The dimer model. a, Two sample dimer configurations (blue links), 
corresponding to the Ey and Ex electrical fields, respectively. The coupled 
pairs of additional spin degrees of freedom on vertices and faces of the 
lattice (wiggly lines) are decoupled from the dimers and from each other. 
Their fluctuations constitute irrelevant noise. b, An example of mapping the 
dimer model to local electric fields. The so-called staggered configuration 
on the left maps to uniform non-vanishing field in the vertical direction: 
⟨ ⟩ ≠E 0y . The ‘columnar’ configuration on the right produces both Ex and Ey 
that are zero on average (see ref. 36 for details of the mapping).

© 2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.

NaTuRe PhySIcS | www.nature.com/naturephysics

http://www.nature.com/naturephysics


Articles NaTuRe PHysIcs
corresponding to local uniform Ey field (see left panels of Fig. 3a,b). 
The large positive (yellow) weights select an identical pattern, trans-
lated by one link. The remaining neurons extract linear superposi-
tions Ex +  Ey or Ex −  Ey of the fields.

To demonstrate the robustness of the RSMI, we added physically 
irrelevant noise, forming nevertheless a pronounced pattern, which 
we model by additional spin degrees of freedom, strongly coupled 
(ferromagnetically) in pairs (wiggly lines in Fig. 3a). Decoupled 
from the dimers, and from other pairs, they form a trivial system, 
whose fluctuations are short-range noise on top of the dimer model. 
Vanishing weights (green in Fig. 4a,b) on sites where pairs of spins 
reside prove that RSMI discards their fluctuations as irrelevant for 
long-range physics, despite their regular pattern.

Notably, the filters obtained using our approach for the dimer 
model, which match the analytical expectation, are orthogonal to 
those obtained using Kullback–Leibler divergence. As expanded on 
in the Supplementary Information, this shows that standard RBMs 
minimizing the Kullback–Leibler divergence do not generally per-
form RG, thereby contradicting prior claims37.

Finally, we demonstrate that by iterating the RSMI algorithm the 
qualitative insights into the nature of relevant degrees of freedom 
give rise to quantitative results. To this end, we revisit the 2D Ising 
model that (contrary to the dimer model) exhibits a non-trivial crit-
ical point at the temperature = + ∕ −T (log(1 2 ) 2)c

1, separating the 
paramagnetic and ferromagnetic phases. We generate Monte Carlo 
samples of the system of size 128 ×  128 at values T around the criti-
cal point, and for each one we perform up to four RG steps, by com-
puting the Λ filters using RSMI, coarse-graining the system with 
respect to those filters (effectively halving the linear dimensions) 
and reiterating the procedure. In addition to the set of Monte Carlo 
configurations for the coarse-grained system, estimates of mutual 
information as well as the filters of the CD-trained RBMs are gener-
ated and stored. The effective temperature T of the system at each 
RG step can be evaluated entirely intrinsically either from correla-
tions or the mutual information, as discussed in the Supplementary 
Information. Using the RBM filters, spin–spin correlations (for 
instance, next-nearest neighbour) can be computed. By comparing 
these with known analytical results38, an additional cross-check of 
the effective temperature can be obtained.

In Fig. 5, the effective T is plotted against ξ ξ∕log ( )2 128 , where ξ 
and ξ128 are the current and 128 ×  128 systems’ correlation lengths, 
respectively (this has the meaning of an RG step for integer values). 
The RG flow of the 2D Ising model is recovered: systems starting 
with T <  Tc flow towards ever-decreasing T (that is, an ordered 
state), while the ones with T >  Tc flow towards a paramagnet. In fact, 
the position of the critical point can be estimated with 1% accuracy 
just from the divergent flow. Furthermore, we evaluate the correla-
tion length exponent ν, defined by ξ ∝  τ−ν. Using the finite-size data 
collapse (see Supplementary Fig. 4), its value, equal to the negative 
slope, is estimated to be ν ≈  1.0 ±  0.15, consistent with the exact ana-
lytical result ν =  1.

Future directions
Artificial neural networks based on RSMI optimization have 
proved capable of extracting complex information about physi-
cally relevant degrees of freedom and using it to perform a real-
space RG procedure. The RSMI algorithm we propose allows for 
the study of the existence and location of critical points, and RG 
flow in their vicinity, as well as estimation of correlations func-
tions, critical exponents and so on. This approach is an example 
of a new paradigm in applying machine learning in physics: the 
internal data representations discovered by suitably designed algo-
rithms are not just technical means to an end, but instead are a 
clear reflection of the underlying structure of the physical system 
(see also ref. 39). Thus, in spite of their ‘black box’ reputation, the 
innards of such architectures may teach us fundamental lessons. 

This raises the prospect of employing machine learning in science 
in a collaborative fashion, exploiting the machines’ power to distil 
subtle information from vast data, and human creativity and back-
ground knowledge40.

Numerous further research directions can be pursued. Most 
directly, equilibrium systems with less understood relevant 
degrees of freedom—for example, disordered and glassy sys-
tems—can be investigated9,10. The ability of the RSMI algorithm 
to re-compute the relevant degrees of freedom at every RG step 
potentially allows one to study their evolution along the (more 
complicated) RG flow34. Furthermore, although we studied clas-
sical systems, the extension to the quantum domain is possible 
via the quantum-to-classical mapping of Euclidean path integral 
formalism. A more detailed analysis of the mutual-informa-
tion-based RG procedure may prove fruitful from a theoretical 
perspective. Finally, applications of RSMI beyond physics are 
possible, since it offers a neural network implementation of a 
variant of the information bottleneck method32, successful in 
compression and clustering analyses41; it can also be used as a 
local-noise-filtering pre-training stage for other machine-learn-
ing algorithms.

Data availability. The data that support the plots within this paper 
and other findings of this study are available from the correspond-
ing author upon request.

Received: 11 May 2017; Accepted: 13 February 2018;  
Published: xx xx xxxx

References
 1. LeCun, Y., Bengio, Y. & Hinton, G. E. Deep learning. Nature 521,  

436–444 (2015).
 2. Silver, D. et al. Mastering the game of Go with deep neural networks and tree 

search. Nature 529, 584–589 (2016).
 3. Hershey, J. R., Rennie, S. J., Olsen, P. A. & Kristjansson, T. T. Super-human 

multi-talker speech recognition: A graphical modeling approach. Comput. 
Speech Lang. 24, 45–66 (2010).

 4. Carrasquilla, J. & Melko, R. G. Machine learning phases of matter. Nat. Phys. 
13, 431–434 (2017).

 5. Torlai, G. & Melko, R. G. Learning thermodynamics with Boltzmann 
machines. Phys. Rev. B 94, 165134 (2016).

 6. van Nieuwenburg, E. P. L., Liu, Y.-H. & Huber, S. D. Learning phase 
transitions by confusion. Nat. Phys. 13, 435–439 (2017).

 7. Wang, L. Discovering phase transitions with unsupervised learning.  
Phys. Rev. B 94, 195105 (2016).

 8. Ohtsuki, T. & Ohtsuki, T. Deep learning the quantum phase transitions in 
random electron systems: applications to three dimensions. J. Phys. Soc. Jpn 
86, 044708 (2017).

T/Tc

R
G

 s
te

p

ParamagnetFerromagnet

0.94 0.96 0.98 1.00 1.02 1.04 1.06 1.08

0.0

0.5

1.0

1.5

2.0

2.5

3.0

1.03 1.02 1.01 1.007

1.002 0.99 0.98

Fig. 5 | RG flow for the 2D Ising model. The temperature T (in units 
of Tc) as a function of the RG step for systems with initial Monte Carlo 
temperatures (denoted by lines of different colour) below and above Tc.  
See Supplementary Information for details.

© 2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.

NaTuRe PhySIcS | www.nature.com/naturephysics

http://www.nature.com/naturephysics


ArticlesNaTuRe PHysIcs
 9. Ronhovde, P.et al Detecting hidden spatial and spatio-temporal structures in 

glasses and complex physical systems by multiresolution network clustering. 
Eur. Phys. J. E 34, 105 (2011).

 10. Ronhovde, P.et al Detection of hidden structures for arbitrary scales in 
complex physical systems. Sci. Rep. 2, 329 (2012).

 11. Hinton, G. E. & Salakhutdinov, R. R. Reducing the dimensionality of data 
with neural networks. Science 313, 504–507 (2006).

 12. Lin, H. W. & Tegmark, M. Why does deep and cheap learning work so well? 
J. Stat. Phys. 168, 1223–1247 (2017).

 13. Carleo, G. & Troyer, M. Solving the quantum many-body problem with 
artificial neural networks. Science 355, 602–606 (2017).

 14. Deng, D.-L., Li, X. & Sarma, S. D. Machine learning topological states.  
Phys. Rev. B 96, 195145 (2017).

 15. Wilson, K. G. The renormalization group: Critical phenomena and the Kondo 
problem. Rev. Mod. Phys. 47, 773–840 (1975).

 16. Politzer, H. D. Reliable perturbative results for strong interactions?  
Phys. Rev. Lett. 30, 1346–1349 (1973).

 17. Berezinskii, V. L. Destruction of long-range order in one-dimensional and 
two-dimensional systems having a continuous symmetry group I. Classical 
systems. Sov. J. Exp. Theor. Phys. 32, 493 (1971).

 18. Kosterlitz, J. M. & Thouless, D. Ordering, metastability and phase transitions 
in two-dimensional systems. J. Phys. C 6, 1181 (1973).

 19. Kadanoff, L. P. Scaling laws for Ising models near T(c). Physics 2,  
263–272 (1966).

 20. Wetterich, C. Exact evolution equation for the effective potential. Phys. Lett. B 
301, 90–94 (1993).

 21. White, S. R. Density matrix formulation for quantum renormalization groups. 
Phys. Rev. Lett. 69, 2863–2866 (1992).

 22. Ma, S.-k, Dasgupta, C. & Hu, C.-k Random antiferromagnetic chain.  
Phys. Rev. Lett. 43, 1434–1437 (1979).

 23. Corboz, P. & Mila, F. Tensor network study of the Shastry–Sutherland model 
in zero magnetic field. Phys. Rev. B 87, 115144 (2013).

 24. Capponi, S., Chandra, V. R., Auerbach, A. & Weinstein, M. p6 chiral 
resonating valence bonds in the kagome antiferromagnet. Phys. Rev. B 87, 
161118 (2013).

 25. Auerbach, A. Interacting Electrons and Quantum Magnetism  
(Springer, New York, NY, 1994).

 26. Gaite, J. & O’Connor, D. Field theory entropy, the h theorem, and the 
renormalization group. Phys. Rev. D 54, 5163–5173 (1996).

 27. Preskill, J. Quantum information and physics: some future directions.  
J. Mod. Opt. 47, 127–137 (2000).

 28. Apenko, S. M. Information theory and renormalization group flows. Phys. A 
391, 62–77 (2012).

 29. Machta, B. B., Chachra, R., Transtrum, M. K. & Sethna, J. P. Parameter space 
compression underlies emergent theories and predictive models. Science 342, 
604–607 (2013).

 30. Beny, C. & Osborne, T. J. The renormalization group via statistical inference. 
New. J. Phys. 17, 083005 (2015).

 31. Stephan, J.-M., Inglis, S., Fendley, P. & Melko, R. G. Geometric mutual 
information at classical critical points. Phys. Rev. Lett. 112, 127204 (2014).

 32. Tishby, N., Pereira, F. C. & Bialek, W. The information bottleneck method. In 
Proc. 37th Allerton Conf. on Communication, Control and Computation  
(eds Hajek, B. & Sreenivas, R. S.) 49, 368–377 (University of Illinois, 2001).

 33. Hinton, G. E. Training products of experts by minimizing contrastive 
divergence. Neural Comput. 14, 1771–1800 (2002).

 34. Ludwig, A. W. W. & Cardy, J. L. Perturbative evaluation of the conformal 
anomaly at new critical points with applications to random systems.  
Nucl. Phys. B 285, 687–718 (1987).

 35. Fisher, M. E. & Stephenson, J. Statistical mechanics of dimers on a plane 
lattice. II. Dimer correlations and monomers. Phys. Rev. 132,  
1411–1431 (1963).

 36. Fradkin, E. Field Theories of Condensed Matter Physics (Cambridge  
Univ. Press, Cambridge, 2013).

 37. Mehta, P. & Schwab, D. J. An exact mapping between the variational 
renormalization group and deep learning. Preprint at  
abs/1410.3831 (2014).

 38. McCoy, B. M. & Wu, T. T. The Two-Dimensional Ising Model (Harvard Univ. 
Press, Cambridge, MA, 1973).

 39. Schoenholz, S. S., Cubuk, E. D., Sussman, D. M., Kaxiras, E. & Liu, A. J. A 
structural approach to relaxation in glassy liquids. Nat. Phys. 12,  
469–471 (2016).

 40. Jordan, M. I. & Mitchell, T. M. Machine learning: Trends, perspectives, and 
prospects. Science 349, 255–260 (2015).

 41. Slonim, N. & Tishby, N. Document clustering using word clusters via the 
information bottleneck method. In Proc. 23rd Annual International ACM 
SIGIR Conf. on Research and Development in Information Retrieval, SIGIR ’00 
208–215 (ACM, 2000).

acknowledgements
We thank S. Huber and P. Fendley for discussions. M.K.-J. gratefully acknowledges 
the support of the Swiss National Science Foundation. Z.R. was supported by the 
European Union's Horizon 2020 research and innovation programme under the Marie 
Sklodowska-Curie grant agreement no. 657111.

author contributions
M.K.-J. and Z.R. contributed equally to this work.

competing interests
The authors declare no competing interests.

additional information
Supplementary information is available for this paper at https://doi.org/10.1038/
s41567-018-0081-4.

Reprints and permissions information is available at www.nature.com/reprints.

Correspondence and requests for materials should be addressed to M.K.

Publisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in 
published maps and institutional affiliations.

© 2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.

NaTuRe PhySIcS | www.nature.com/naturephysics

https://doi.org/10.1038/s41567-018-0081-4
https://doi.org/10.1038/s41567-018-0081-4
http://www.nature.com/reprints
http://www.nature.com/naturephysics

	Mutual information, neural networks and the renormalization group
	The RSMI algorithm
	Validation
	Future directions
	Data availability. 

	Acknowledgements
	Fig. 1 The RSMI algorithm.
	Fig. 2 The weights of the RSMI network trained on the Ising model.
	Fig. 3 The dimer model.
	Fig. 4 The weights of the RSMI network trained on dimer model data.
	Fig. 5 RG flow for the 2D Ising model.




