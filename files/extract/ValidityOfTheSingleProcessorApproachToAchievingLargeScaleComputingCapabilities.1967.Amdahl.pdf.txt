




































amdahl.dvi


AFIPS spring joint computer conference�����

Validity of the single processor approach to achieving large scale

computing capabilities�

Gene M� Amdahl

IBM Sunnyvale� California

� INTRODUCTION

For over a decade prophets have voiced the contention that the organization of a single computer

has reached its limits and that truly signi�cant advences can be made only by interconnection of a

multiplicity of computers in such a manner as to permit cooperative solution� Variously the proper

direction has been pointed out as general purpose computers with a generalized interconnection

of memories� or as specialized computers with geometrically related memory interconnections and

controlled by one or more instruction streams�

Demonstration is made of the continued validity of the single processor approach and of the

weaknesses of the multiple processor approach in terms of application to real problems and their

attendant irregularities�

The arguments presented are based on statistical characteristics of computation on computers

over the last decade and upon the operational requirements within problems of physical interest�

An additional reference will be one of the most thorough analyses of relative computer capabilities

currently published �Changes in Computer Performance�� Datamation� September ����� Professor

Kenneth F� Knight� Stanford School of Business Asministration�

The �rst characteristic of interest is the fraction of the computational load which is associated

with data management housekeeping� This fraction has been very nearly constant for about ten

years� and accounts for �	
 of the executed instructions in production runs� In an entirely dedicated

special purpose environment this might be reduced by a factor of two� but it is highly improbably

that it could be reduced by a factor of three� The nature of this overhead appears to be sequential

so that it is unlikely to be amenable to parallel processing techniques� Overhead alone would then

place an upper limit on throughput of �ve to seven times the sequential processing rate� even if the

�This paper is retyped as the present form by Guihai Chen� He wishes you would enjoy reading this historical

paper�

�



housekeeping were done in a separate processor� The non housekeeping part of the problem could

exploit at most a processor of performance three to four times the performance of the housekeeping

processor� A fairly obvious conclusion which can be drawn at this point is that the e�ort expended

on achieving high parallel processing rates is wasted unless it is accompanied by achievements in

sequential processing rates of very nearly the same magnitude�

Data management housekeeping is not the only problem to plague oversimpli�ed approaches to

high speed computation� The physical problems which are of practical interest tend to have rather

signi�cant complications� Examples of these complications are as follows� boundaries are likely to

be irregular interiors are inhomogeneous computations required may be dependent on the states

of the variables at each point propagation rates of di�erent physical e�ects may be quite di�erent

the rate of convergence� or convergence at all may be strongly dependent on sweeping through the

array along di�erent axes on succeeding passes� etc� The e�ect of each of these complications is

very severe on any computer organization based on geometrically related processors in a paralleled

processing system� Even the existence of regular rectangular boundaries has the interesting property

that for spatial dimension of N there are �N di�erent point geometries to be dealt with in a nearest

neighbor computation� If the second nearest neighbor were also involved� there would be �N

di�erent point geometries to contend with� An irregular boundary compounds this problem as does

an inhomogeneous interiors� Computations which are dependent on the states of variables would

require the processing at each point to consume approximately the same computational times as

the sum of computations of all physical e�ects within a large region� Di�erences of changes in

propagation rates may a�ect the mesh point relationships�

Ideally the computation of the action of the neighboring points upon the point under consid�

eration involves their values at a previous time proportional to the mesh spacing and inversely

proportional to the propagation rate� Since the time step is normally kept constant� a faster prop�

agation rate for some e�ects would imply interactions with more distant points� Finally the fairly

common practice of sweeping through the mesh along di�erent axes on succeeding passes posed

problems of data management which a�ects all processors� however it a�ects geometrically related

processors more severely by requiring transposing all points in storage in addition to the revised

input�output scheduling� A realistic assessment of the e�ect of these irregularities on a simpli�ed

and regularized abstraction of the problem yields a degradation in the vicinity of one�half to one

order of magnitude�

To sum up the e�ects of data management housekeeping and of problem irregularities� the

author has compared three di�erent machine organizations involving approximately equal amounts

�



of hardware� Machine A has thirty two arithmetic execution units controlled by a single instruction

stream� Machine B has pipelined arithmetic execution units with up to three overlapped operations

on vectors of eight elements� Machine C has the same pipelined execution units� but initiation of

individual operations at the same rate as Machine B permitted vector element operations� The

performance of these three machines are plotted in Figure � as a function of the fraction of the

number of instructions which permit parallelism� The probable region of operation is centered

around a point corresponding to ��
 data management overhead and �	
 of the problem operations

forced to be sequential�

Figure  1

The historic performance versus cost of computers has been explored very thoroughly by Profes�

sor Knight� The carefully analyzed data he presents re�ects not just execution times for arithmetic

operations and cost of minimum of recommended con�gurations� He includes memory capacity ef�

fects� input�output overlap experienced� and special functional capabilities� The best statistical �t

obtained corresponds to a performance proportional to the square of cost at any technological level�

This result very e�ectively supports the often invoked �Grosch�s Law�� Utilizing this analysis� one

can argue that if twice the amount of hardware were exploited in a single system� one could expect

to obtain four times the performance� The only di�culty is involved in knowing how to exploit this

additional hardware� At any point in time it is di�cult to foresee how the precious bottlenecks in

a sequential computer will be e�ectively overcome� If it were easy they would not have been left as

bottlenecks� It is true by historical example that the successive obstacles have been hurdled� so it is

appropriate to quote the Rev� Adam Clayton Powell��Keep the faith� baby�� If alternatively one

decided to improve the performance by putting two processors side by side with shared memory�

one would �nd approximately ��� times as much hardware� The additional two tenths in hardware

accomplished the crossbar switching for the sharing� The resulting performance achieved would

be about ���� The latter �gure is derived from the assumption of each processor utilizing half of

�



the memories about half of the time� The resulting memory con�icts in the shared system would

extend the execution of one of two operations by one quarter of the execution time� The net result

is a price performance degradation to 	�� rather than an improvement to ��	 for the single larger

processor�

Comparative analysis with associative processor is far less easy and obvious� Under certain

condition of regular formats there is a fairly direct approach� Consider an associative processor de�

signed for pattern recognition in which decisions within individual elements are forwarded to some

set of other elements� In the associative processor design the receiving elements would have a set of

source addresses which recognize by associative techniques whether or not it was to receive the de�

cision of the currently declaring element� To make a corresponding special purpose non�associative

processor one would consider a receiving element and its source addresses as an instruction� with

binary decision maintained in registers� Considering the use of the �lm memory� an associative

cycle would be longer than a non�destructive read cycle� In such a real analogy the special purpose

non�associative processor can be expected to take about one�fourth as many memory cycles as the

associative version and only about one sixth of the time� These �gures were computed on the full

recognition task with somewhat di�ering ratios in each phase� No blanket claim is intended here�

but rather that each requirement should be investigated from both approaches�

� Notes by Guihai Chen

� The very famous Amdahl�s Law� presented as in the following formula� is deprived from this

paper� However� Amdahl gave only a literal description which was paraphrased by latecomers

as follows�

Speedup �
�

rs �
rp
n

where rs � rp � � and rs represents the ratio of the sequential portion in one program�

� Only a small part of this paper� exactly the fourth paragraph� contributes to the Amdahl�s

Law� This paper also discussed some other important problems� For example� Amdahl had

forseen many negative factors plaguing the parallel computation of irregular problems� such

as �� boundaries are likely to be irregular �� interiors are inhomogeneous �� computations

required may be dependent on the states of the variables at each point �� propagation rates

of di�erent physical e�ects may be quite di�erent �� the rate of convergence� or convergence

at all may be strongly dependent on sweeping through the array along di�erent axes on

succeeding passes� etc�

�


