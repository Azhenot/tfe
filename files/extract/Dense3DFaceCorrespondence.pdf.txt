




































untitled


Dense 3D Face Correspondence
Syed Zulqarnain Gilani , Ajmal Mian, Faisal Shafait, and Ian Reid

Abstract—We present an algorithm that automatically establishes dense correspondences between a large number of 3D faces.

Starting from automatically detected sparse correspondences on the outer boundary of 3D faces, the algorithm triangulates existing

correspondences and expands them iteratively by matching points of distinctive surface curvature along the triangle edges. After

exhausting keypoint matches, further correspondences are established by generating evenly distributed points within triangles by

evolving level set geodesic curves from the centroids of large triangles. A deformable model (K3DM) is constructed from the dense

corresponded faces and an algorithm is proposed for morphing the K3DM to fit unseen faces. This algorithm iterates between rigid

alignment of an unseen face followed by regularized morphing of the deformable model. We have extensively evaluated the proposed

algorithms on synthetic data and real 3D faces from the FRGCv2, Bosphorus, BU3DFE and UND Ear databases using quantitative and

qualitative benchmarks. Our algorithm achieved dense correspondences with a mean localisation error of 1.28 mm on synthetic faces

and detected 14 anthropometric landmarks on unseen real faces from the FRGCv2 database with 3 mm precision. Furthermore, our

deformable model fitting algorithm achieved 98.5 percent face recognition accuracy on the FRGCv2 and 98.6 percent on Bosphorus

database. Our dense model is also able to generalize to unseen datasets.

Index Terms—Dense correspondence, 3D face, morphing, keypoint detection, level sets, geodesic curves, deformable model

Ç

1 INTRODUCTION

ONE of the canonical tasks in shape analysis is to find ameaningful mapping between two or more shapes [1].
The process, called shape correspondence, is a pre-requisite
for many computer vision, computer graphics and medical
image analysis applications. The requisite density of corre-
spondence is often dictated by the underlying shape and tar-
get application. Sometimes, sparse correspondence is
sufficient to infer shape semantics bymatching representative
points, for example the four corners of a rectangle or emblem-
atic points on key joints of a human body. However, sparse
correspondence is often inadequate in case of articulated
shapes [2], [3] where parts of the shape can bend indepen-
dently or in the correspondence of anatomical shapes which
can deform in an elastic manner [4]. In such circumstances,
dense correspondence is required to guarantee representation
of global shape changes, for instance in case of morphing or
attribute transfer. Furthermore, very subtle changes within a
class of shapes can be detected only if the correspondence
between these shapes is dense [6].

In this paper we are concerned with the task of finding
dense correspondences between a very large number of simi-
lar shapes; in our case 3D scans of human faces. We do so

because this further enables us to generate highly accurate 3D
morphable models that can be used for information transfer
between the training set and a test face or between two test
faces by morphing the 3D model to fit the test face(s). For
example, given the location of anthropometric landmarks [7]
on the 3Dmorphablemodel, these landmarks can be automat-
ically localized on previously unseen test faces [8]. Further-
more, dense correspondences and morphable models can be
used for 3D face recognition [9], [10]. Other applications
include facial morphometric measurements such as gender
scoring [5] and asymmetry for syndrome diagnosis [6], statis-
tical shape modelling [11], [12], shape interpolation [13], non-
rigid shape registration [3], [14], [15], deformation analy-
sis [16] and recognition [17], [18], [19].

While it is possible to manually annotate a small number
(�30) of correspondences for a few 3D faces, it is not feasible
to manually identify dense correspondences (�6,000)
between hundreds of 3D faces. The literature also proposes
computing dense correspondence by extending manually
annotated sparse ones [9], [20]. However, with the advent of
huge 3D face databases like the Facebase Consortium [21] or
Raine dataset [22], [23], this strategy too has become impracti-
cal and calls for fully automatic algorithms. Automatically
establishing dense correspondences between the 3D faces of
two different persons is an extremely challenging task
because the facial shape varies significantly amongst individ-
uals depending on their identity, gender, ethnicity and age [7]
as well as their facial expression and pose. The problem of
dense 3D point-to-point correspondences can be formulated
as follows. Given a set of N 3D faces, Fj ¼ ½xp; yp; zp�T ;
j ¼ 1; . . . ;N; p ¼ 1; . . . ; Pj, the aim is to establish a dense bijec-
tive mapping f : Fi ! Fjði 6¼ jÞ over k vertices where
1 < < k < minðPi; PjÞ. Correspondences should cover all
regions of the face for high fidelity and should follow the
same triangulation for shape consistency.

� S.Z. Gilani, A. Mian, and F. Shafait are with the School of Computer Sci-
ence and Software Engineering, The University of Western Australia, 35
Stirling Highway, Crawley, WA, 6009, Australia. E-mail: syedzulqarnain.
gilani@research.uwa.edu.au, {ajmal.mian, faisal.shafait}@uwa.edu.au.

� I. Reid is with the School of Computer Science, University of Adelaide,
Ingkarni Wardli, North Terrace Campus, Adelaide, SA 5005, Australia.
E-mail: ian.reid@adelaide.edu.au.

Manuscript received 15 June 2016; revised 2 May 2017; accepted 3 July 2017.
Date of publication 10 July 2017; date of current version 12 June 2018.
(Corresponding author: Syed Zulqarnain Gilani.)
Recommended for acceptance by R. Yang.
For information on obtaining reprints of this article, please send e-mail to:
reprints@ieee.org, and reference the Digital Object Identifier below.
Digital Object Identifier no. 10.1109/TPAMI.2017.2725279

1584 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018

0162-8828� 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

https://orcid.org/0000-0002-7448-2327
https://orcid.org/0000-0002-7448-2327
https://orcid.org/0000-0002-7448-2327
https://orcid.org/0000-0002-7448-2327
https://orcid.org/0000-0002-7448-2327
mailto:
mailto:
mailto:
mailto:


Existing dense correspondence techniques have one or
more of the following limitations: (1) They need manually
annotated landmarks on 3D faces for initialization. (2) They
use image texture matching to find 3D shape correspon-
dence. (3) They correspond all faces to a single reference face
neglecting the global proximity of the 3D faces. (4) They
have not been tested on complete benchmark databases such
as the FRGCv2 [24] or Bosphorus [25] datasets for face recog-
nition and landmark identification. (5) They have no explicit
mechanism of updating the dense correspondencemodel.

In this context, we propose a fully automatic algorithm
for establishing dense correspondences simultaneously
between a large number of 3D faces. Our algorithm does
not require any manual intervention and relies solely on 3D
shape matching to encode accurate facial morphology. We
organize the 3D faces into a minimum spanning tree based
on bending energy required to deform one shape into the
other so that correspondences can be propagated in a reli-
able way. We propose a mechanism for automatic initializa-
tion of a sparse set of correspondences on the outer
boundary of the 3D faces. We form a triangulation of these
correspondences, and iteratively add to the set of points by
matching points of distinctive surface curvature along (and
close to) the triangulated edges. After exhausting the possi-
bilities for such matches, we further expand the set of
matches by generating points distributed evenly within tri-
angles by evolving level set geodesic curves from the cent-
roids of large triangles. The outcome of our algorithm is a
Keypoint-based 3D Deformable Model (K3DM).

Our second major contribution is a deformable model fit-
ting algorithm where K3DM is used to morph into unseen
query faces. Starting from the mean face, the fitting algo-
rithm iterates between two steps. The query face is trans-
formed rigidly to align with the model and the model is
deformed using regularized least squares to fit the query
face. This algorithm converges in a few iterations and is
robust to noise, outlier points, missing points, pose and
expression variations.

Our final contribution is an algorithm for augmenting the
K3DM. Given the K3DM and a new batch of M faces, we
construct a minimum spanning tree using the nearest face
to the K3DM as the root node. The K3DM is augmented by
adding one face at a time, starting with the root node, and
each time updating the model and deforming the updated
model to better fit the next face in the spanning tree.

Evaluating dense correspondence techniques is challeng-
ing due to the inherent difficulty of obtaining ground-truth
data. In the existing literature, evaluations have mostly
been performed on a sparse set of anthropometric facial
landmarks [26], [27], [28] since these can be manually
labelled. However, evaluation on only a few (� 20) anthro-
pometric points does not show how well dense correspond-
ences have generalized to the whole face. Thus, subjective
evaluations are frequently performed [29] by visually
inspecting the quality of morphing between faces [4], [30].
In this paper, we show how synthetic 3D faces (Facegen
Modeller) can be used to quantitatively evaluate dense cor-
respondences on a large set of points (� 1; 000). Using the
presented deformable face model, we perform extensive
experiments for landmark localization (Section 6.1) and
face recognition (Section 6.2) using real faces from the

FRGCv2 [24] and BU3DFE [31] databases. Results show
that our algorithm outperforms state-of-the-art application-
specific algorithms in each of these areas.

2 RELATED WORK

Existing 3D correspondence techniques can be grouped into
descriptor based, model based and optimization based [1].

Descriptor Based Techniques. These techniques match local
3D point signatures derived from the curvatures, shape
index and normals. However, they are often highly sensitive
to surface noise and sampling density [33] of the underlying
geometry [34]. More significantly for our purpose, the den-
sity of corresponding points is typically low resulting in cor-
respondences between a very sparse set of anthropometric
landmarks.

One of the earliest works, in this category, for establishing
dense correspondence was proposed by Sun and Abidi [30],
[32] who projected geodesic contours around a 3D facial
point onto their tangential plane and used them as features
to match two surfaces. The approach, with minor modifica-
tions, was employed by Salazar et al. [35] to establish point
correspondence on 3D faces in BU3DFE database. Lu and
Jain [36] presented a multimodal approach for facial feature
extraction. Using a face landmark model, the authors
detected seven corresponding points on 3D faces using
shape index from range images and cornerness from inten-
sity images. Segundo et al. [37] combined surface curvature
and depth relief curves for landmark detection in 3D faces of
the FRGCv2 and BU3DFE databases. They extracted features
from the mean and Gaussian curvatures for detecting five
landmarks in the nose and eye (high curvature) regions.

Creusot et al. [27] presented a machine learning approach
to detect 14 corresponding landmarks on 3D faces. They
trained multiple LDA classifiers on a set of 200 faces and a
landmark model using a myriad of local descriptors. Each
landmark detection was treated as a two class classification
problem and the final results were fused. This method works
well for neutral expression faces of the FRGCv2 and Bospho-
rus databases. Perakis et al. [26] proposed a method to detect
landmarks under large pose variations using a statistical
Facial Landmark Model (FLM) for the full face and another
two FLMs for profile views of the face. Keypoints are
detected using Shape Index and Spin Images and then
matched on the basis ofminimumcombined normalized Pro-
crustes and Spin Image similarity distance from all three
FLMs. Thismethodwas used to detect eight correspondences
in the FRGCv2 and UND Ear databases. Later, the authors
proposed a technique [38] for fusing features from 2D and 3D
data to detect these landmarkswith better accuracy than [26].

Some methods have also been proposed for generating
sparse correspondence for 3D face recognition [44], [45], [46],
[47]. However, these methods are based on keypoint corre-
spondences that are repeatable only on the same identity.

Model Based Techniques. These approaches create a
morphable model using a sparse set of correspondences
and then extend them to dense correspondences.

Employing a Point Distribution Model coupled with 3D
point signature detection, Nair and Cavallaro [8] estimated
the location of 49 corresponding landmarks on faces. They
tested their algorithm on 2,350 faces of the BU3DFE [31]

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1585



database and reported a rather high mean landmark locali-
zation error.

Blanz and Vetter [29] proposed a dense correspondence
algorithm using optical flow on the texture and the 3D cylin-
drical coordinates of the face points assuming that the faces
are spatially aligned. They constructed a 3D morphable face
model from 100 male and female faces each. An arbitrary
face was chosen as a reference and the remaining scans were
registered to it by iterating between optical flow based corre-
spondence and morphable model fitting. One potential pit-
fall of the texture based dense correspondence [29] is that
facial texture is not always consistent with the underlying
3D facial morphology, e.g., the shape and location of eye-
brows. Moreover, this algorithm requires seven manually
annotated facial landmarks for initialization. Later, in [9],
[39] the authors used the 3Dmorphablemodel for face recog-
nition. Experiments were performed on only 150 pairs of 3D
faces [39] from FRGCv2 database, although the total number
of scans in the database are 4,007. The seminal work of Blanz
andVetter [29]was extended by Paysan et al. [40] in the Basel
Face Model (BFM) which used an improved mesh registra-
tion algorithm [41]. The authors havemade their dense corre-
spondence model publicly available which has enabled us to
draw comparisons with their model.

Passalis et al. [42] proposed an Annotated Face Model
(AFM) based on an average facial 3D mesh. The model was
created by manually annotating a sparse set of anthropo-
metric landmarks [7] on 3D face scans and then segmenting
it into different annotated areas. Later, Kakadiaris et al. [43]
proposed elastic registration using this AFM by shifting the
manually annotated facial points according to elastic con-
straints to match the corresponding points of 3D target
models in the gallery. Face recognition was performed by
comparing the wavelet coefficients of the deformed images
obtained from morphing. Passalis et al. [18] further
improved the AFM by incorporating facial symmetry to
perform pose invariant face recognition. However, the algo-
rithm depends on detection of at least five facial landmarks
on a side pose scan.

Level set curves were evolved in [28] to automatically
extract seed points and correspondences were established
by minimizing the bending energy between patches around
seed points of different faces. A morphable model based on
the dense corresponding points was then fitted to unseen
query faces for transfer of correspondences. The accuracy of
landmark localization in this method depends on the num-
ber and accuracy of initial seed points.

Optimization Based Techniques. These methods optimize
an objective function to find a mapping between fiducial
points. Non-rigid ICP (NICP) is one such technique which
formulates deformable registration as an optimization prob-
lem consisting of a mesh smoothness term and several data
fitting terms [41], [78]. These algorithms require accurate
global initialization points ranging from 14 points [41] to 68
points [77]. These points are either manually annotated [29],
[41] or detected automatically using texture [77]. An exten-
sion to this method removes the need for fiducial points but
assumes a partial overlap of facial regions [78], [79]. The
alignment between two faces is performed with a global
rigid transformation followed by per-vertex affine transfor-
mations that bring the non-rigid shapes into full alignment.

Such methods are more suited for time varying deforma-
tions of the same identity and often do not result in a bijec-
tive (one-to-one) mapping of the vertices. Booth et al. [77]
constructed a dense correspondence model of several faces
from a propriety dataset by registering the scans to a tem-
plate mesh using NICP algorithm [41] initialized by 68 fidu-
cial landmarks detected using texture. Bolkart et al. [80]
presented dense correspondence as an optimization prob-
lem and used the Minimum Description Length (MDL) [11]
as the objective function. The authors of methods that are
based on NICP [40], [41], [77], [78], [79] or other alternative
optimization techniques [80] have not reported facial land-
mark localization results. Hence, it is difficult to perform a
direct objective comparison with these methods.

3 DENSE 3D FACE CORRESPONDENCE

The overall idea of our system for dense correspondence
between 3D face scans, is to begin with a set of automati-
cally extracted seed points that represent points matched
across all faces in the dataset, and gradually densify the set
of matches. Fig. 1 depicts the overall flow of our system.
Here we give an overview of how this proceeds, and then
expand the details in the sections below.

We first organize the faces into a tree (Section 3.1) based on
similarity. We then seek a set of reliable seed matches
(Section 3.2) from which to begin an iterative densification
process. Each iteration of the densification process
(Section 3.3) begins by selecting the current best set of
matches (comprising nq of the full set of nmatches) and form-
ing a triangulation of these points. Taking each edge of
the resulting triangulation in turn, we extract a narrow patch

Fig. 1. Block diagram of the presented dense 3D face correspondence
algorithm.

1586 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018



centered on the edge from a pair of faces that are adjacent
in the tree. For each of these patches we find points of dis-
tinctive curvature (Section 3.4) - these will be new candi-
date matches, or keypoints - and compute a 38-dimensional
descriptor of the local surface around each keypoint. Using
constrained nearest neighbor we then determine points
that match well between the pair of patches (i.e., their
descriptors match and they are within a proximity thresh-
old in the patch). We repeat this process for all parent/
child pairs throughout the tree, and eliminate all keypoints
that are not consistently matched throughout the tree. The
remaining keypoints that are successfully matched across
all faces in the dataset are added to the current set of
matches. At the end of one iteration, when we have cycled
through all the triangulated edges, we choose a new best
set of nq matches and repeat the process.

Once the search for keypoints is exhausted, further corre-
spondences on facial areas devoid of discriminative points
are established by first evolving level set curves and sam-
pling equidistant vertices (See Section 3.6). Feature vectors
of these vertices on the reference face are then matched with
the remaining faces to establish correspondence as previ-
ously stated.

The outcome of this process is a set of densely corre-
sponding 3D faces which we call the Keypoint-based 3D
Deformable Model (K3DM).

3.1 Preprocessing and Organizing Faces

The nose tip of a 3D face is detected automatically following
Mian et al. [67]. Centering a sphere at the nose tip , the face
is cropped. The pose of the 3D face is iteratively corrected to
a canonical form using the Hotelling transform [53]. Next,
holes are filled and noise is removed using the gridfit algo-
rithm [76].

Next,we pre-organise the face dataset into a graph (in fact,
a tree) in which similar faces are “close” to one another. Let
G ¼ ðVg; EgÞ be a directed graph where each node Vg is a 3D
face F from the dataset and each edgeEg connects two nodes
ðvi; vjÞ of the graph. Each edge of the graph hasweightw

wðvi; vjÞ ¼
bij þ bji

2
; (1)

where bij is the amount of bending energy required to
deform face Fi to Fj and is measured using the 2D thin-plate

spline model [48]. Note that bij 6¼ bji and bii ¼ 0. Since, the
faces are already roughly aligned, their nearest neighbor
points are taken as approximate correspondences for the
purpose of calculating the bending energy. From G, we con-
struct a minimum spanning tree P ¼ ðVt; EtÞ using
Kruskal’s algorithm. The node with the maximum number
of children is taken as the root node.

The purpose of this pre-organisation is to increase the
likelihood of finding point matches between pairs of faces.
A naive approach would be to arbitrarily choose a single (or
average) face as reference and find its correspondences to
others in the dataset. But such an approach ignores the
proximity between the face instances and the global infor-
mation underlying the population. The process and a sam-
ple graph are shown in Fig. 2.

3.2 Sparse Correspondence Initialization

We initialize the correspondences by first automatically
establishing a sparse set of seed points. We restrict these
seed points to those that lie on the roughly ellipse-shaped
2D convex hull of the face, i.e., the 2D-hull when the 3D
mesh is projected into the x� y plane. We sample these
points at regular angular intervals of d ¼ p=36 (see
Fig. 3), where the angle d is measured at the nose tip.
There is of course no guarantee that in the finite resolu-
tion mesh of the face there will be a point at an exact
multiple of p=36, but for each face we choose the nearest
point. This yields a set of 72 3D seed points for each 3D
face in the dataset which are used in the first iteration of
the triangulation and densification process, as described
in the next section.

3.3 Triangulation and Geodesic Patch Extraction

The main part of our algorithm is an iteration that takes the
best set of matches that have been established to date, and
grows the number of correspondences. For the first itera-
tion, we use the sparse set of correspondences established
as in the previous section, while for subsequent iterations
we determine the best set of nq matches from the full set of
nmatches as described in Section 3.5.

In each iteration, given nq correspondences between N
faces, we perform a 2D Delaunay triangulation of the mean
x� y locations of the nq current best matches. This triangu-
lation is then used consistently across all faces. We then
pick a pair of parent/child nodes from the Minimum Span-
ning Tree P, Fj and Fk. For both faces in the pair, we extract

Fig. 2. The directed graphG ¼ ðVg; EgÞ (Left) and the Minimum Spanning
Tree (MST)P ¼ ðVt; EtÞ (Right) constructed from five example images of
FRGCv2.

Fig. 3. (a) Vertices of the 2D-convex hull of the projection. (b) Points
sampled at angular intervals of p=36. (c) Initial sparse correspondence
projected on four identities of the FRGCv2 dataset.

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1587



a narrow surface patch S ¼ f½xi; yi; zi�T ; i ¼ 1; . . . ;mg � F,
centered on a geodesic curve defined by each triangle
edge (see Fig. 4). For the sake of simplicity we call this a geo-
desic patch.

The (projected) length of the patch is the same as the
length of the edge. The “narrow” width is set with reference
to the scale of the original face mesh resolution. More specif-
ically, we set the width to be 5r where r is the average
mesh-edge length in the vicinity of the endpoint of the edge
(note that here the mesh-edges refer to the edges in the orig-
inal dataset, not the edges of the triangulation used for the
densification). This makes the extraction of the geodesic
patch scale invariant. The values of r for real 3D faces cap-
tured with the Minolta or the 3dMDface scanners typically
range from 1-3 mm.

Finally, we bring the patches Sj;Sk into approximate
alignment using non-rigid registration [49], [50]. The pro-
cess is shown in Fig. 4.

3.4 Keypoint Detection on Geodesic Patches

Our aim now is to establish accurate correspondences
between a patch on one face Sj and its corresponding patch
on the other face Sk. We do this in a fairly standard manner
by finding distinctive keypoints, generating a descriptor of
the local surface around each point, and establishing
matches between points on each patch whose descriptors
are sufficiently close.

More specifically, to find keypoints we consider the sur-
face distinctiveness at each point in the patch. We do so by
calculating the covariance of all the points within a neigh-
borhood of 5r of the current point, and marking as key-
points any points for which the ratio of the largest two
eigenvalues of the covariance exceeds a threshold. Note that
if the neighborhood is uniform these eigenvalues will be
equal, and therefore the point is unsuitable as a keypoint.

Fig. 5 shows keypoints detected by our algorithm in the
tenth iteration on four different identities of the FRGCv2
database.

We use the keypoints detected on surface patch Sj for
feature extraction and matching only if an adequate number
of keypoints are detected (we use a minimum of three), oth-
erwise, Sj is not considered to be sufficiently descriptive
and the matches are not sought within the patch.

3.5 Feature Extraction and Matching

We denote by CCj ¼ ½xi; yi; zi�T ; i ¼ 1; . . . ; nC the set of key-
points detected on the surface Sj, where nC is the number
of keypoints (likewise for CCk). For each keypoint we extract
a feature vectors x which describe the local surface (within
5r) using a set of 3D signature and histogram based descrip-
tors. These descriptors have been widely used in the litera-
ture [27], [51], [52] for automatic object recognition and for
landmark detection. We use a combination of many descrip-
tors since the surface patch is quite small and a single
descriptor may not capture sufficient information. The list
of descriptors is given below:

� The spatial location ½xi; yi; zi�T .
� The surface normal ½nx; ny; nz�T .
� The seven invariant moments [53] of the 3	 3 histo-

grams of theXY; YZ andXZ planes.
� The central moment mmn of order mþ n of the histo-

gram matrixH

mmn ¼
X’
i¼1

X’
j¼1

ði� �iÞmðj� �jÞnHði; jÞ; (2)

where ’ is the total number of points in H,
�i ¼ P’i¼1 P’j¼1 iHði; jÞ and �j ¼ P’i¼1 P’j¼1 jHði; jÞ.

� The mean of the two principle curvatures �k1 and �k2
calculated at each point on the extracted local surface

� The Gaussian CurvatureK ¼ k1k2
� The Mean Curvature H ¼ k1þk22
� The Shape Index. We use two variants of the shape

index which vary from 0 to 1 and �1 to 1
respectively,
sa ¼ 12 � 1p arctan k1þk2k1�k2 ; 0 � sa � 1 and
sb ¼ 2p arctan k1þk2k1�k2 ; �1 � sb � 1.

� The Curvedness c ¼
ffiffiffiffiffiffiffiffiffiffi
k2
1
þk2

2
2

q
� The Log-Curvedness

cl ¼ 2p log
ffiffiffiffiffiffiffiffiffiffi
k2
1
þk2

2
2

q
,

� The Willmore Energy ew ¼ H2 �K,
� The Shape Curvedness cs ¼ sb:cl
� The Log Difference Mapml ¼ lnðK �H þ 1Þ.
Using these descriptors, the dimensionality of the final

feature vector x is 38. These features are extracted over a
small enough local surface centered at the keypoint such
that they are repeatable across identities. In contrast, the fea-
ture vector extracted by Mian et al. [44], [54] takes the range
values of a larger surface (typically 20 mm radius) sur-
rounding each keypoint. Hence, their features are repeat-
able only over the same identity. One of the prerequisites of
the techniques that use depth values as features [44], [51],

Fig. 4. Illustration of geodesic patch extraction. (a) Two 3D faces with tri-
angulation over a few corresponding points from the 2nd iteration. Geo-
desic surface patch is extracted between two sample points shown in
red colour. (b) Pointclouds of the geodesic surface patches before and
after registration.

Fig. 5. Illustration of keypoints (not corresponding points) detected along
geodesic patches in the tenth iteration of our algorithm. Notice the
repeatability of keypoints across the identities.

1588 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018



[52], [54] is to define a local reference frame for pose invari-
ant matching. In our case, the features are quasi pose invari-
ant and hence do not require a local reference frame. This is
because the pose of each training faces has been iteratively
corrected to a canonical form during preprocessing and the
features are extracted from a very small patch.

Next, we perform constrained-NN search between the
feature vectors xj from Sj and xk from Sk, such that the cor-
responding points lie within a proximity of 2r to each other,
and their matching score dðxj; xkÞ - taken to be Euclidean
distance between their feature descriptors - is less than a
threshold kq. The quality of correspondence varies directly
with kq. Higher values of kq will result in poor correspond-
ing points with large errors, whereas lower values of kq may
reject valid correspondences and hence adversely effect the
correspondence density. Fig. 6 shows the effect of kq on the
correspondence found in our experiments on the synthetic
dataset. As we increase the value of kq, the mean localiza-
tion error and its standard deviation (SD) increases. Fig. 7
shows the outcome of this step on two identities.

This process is repeated for all surface patches in a pair
of faces, and for all pairs of faces in the MST. Only points
that are matched throughout the MST in the pairwise
scheme are retained and these are added to the set of cor-
respondences obtained for the previous iteration. We then
select from the full set of correspondences those which
have the smallest matching score dðxj; xkÞ. We denote the
number of selected correspondences by nq and use a value
of nq ¼ 80 in our experiments. In order to adequately cover

the whole face for the subsequent iteration, we add
the original seed points to the nq points. Next, we obtain
a triangulation of these points on the mean face of the
dataset and extract geodesic surface patches as described
in Section 3.3, repeating the process.

3.6 Densifying Matches in Uniform Regions

Keypoints, by their very definition concentrate around
regions of high curvature/discrimination, such as the
mouth, nose, and eyes. In this section we describe how we
establish correspondences in more uniform regions where
keypoints cannot be found. A simple approach to establish
dense correspondence in these areas would be to sample
them uniformly within triangles of the Delaunay triangula-
tion. This approach has been used in 2D by Munsell
et al. [55] who pre-organized the shape instances based on a
similarity measure and then established correspondence
between pairs of shapes by mapping the points from the
source instance to the target instance after minimizing a
bending energy. However, a uniform sampling in the trian-
gle only results in uniform sampling on the face in planar
regions. Instead, we adopt a sampling strategy that respects
the underlying surface distances (geodesics) on each face.

After triangulation of the final set of best quality corre-
sponding points, we select large triangles with area greater
than ta. We set ta to be the mean area of all triangles in the
connectivity, an effective and expeditious choice. From the
centroid of each triangle, we evolve a level-set curve, in
which the front speed is set to be uniform along a (radial)
geodesic. For convenience we refer to these curves as “level-
set geodesics”. We follow the Fast Marching Method [56]
and use the implementation given by Peyre [57]. We then
sample the points along the curve at regular intervals to
ensure equidistant points (see Fig. 8). Because the evolution

Fig. 6. The effect of correspondence quality threshold kq in the synthetic dataset in the first iteration. (Left) Graph of kq versus the mean and SD of
correspondence localization error. (Middle) kq versus the number of correspondences established. (Right) kq versus the maximum localization error.
For all our experiments we have set kq ¼ 2r shown in the graphs in a magenta circle.

Fig. 7. Correspondence established in 1st, 4th, 13th and 18th iteration of
our algorithm on the first two identities of FRGCv2. Notice how well the
points correspond across the identities.

Fig. 8. Correspondence establishment on smooth surfaces. Two faces
from an ordered pair with triangulation over nq best quality corresponding
points. Blue dots indicate the centroids of large triangles. Level set based
evolution of geodesic curves for the two sample triangles, magnified on
the right.

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1589



speed of the curve is uniform along geodesics, we obtain
a uniform sampling on the surface; this is in contrast to
uniform sampling within the triangle which would not nec-
essarily be uniform on the surface itself. Although these
points are not keypoints, they are repeatable on all 3D faces
across identities because they are extracted from triangles
whose vertices are corresponding to each other across the
dataset. Furthermore, these points are extracted at equal
intervals over a small regionwhich is smooth.

Given this set of points sampled uniformly on the sur-
face, we extract feature vectors and perform pairwise
matching as before. Points whose feature vectors are close
enough are retained as matches, with the rest discarded.
This is not an iterative process and points are sampled only
once from each triangle meeting the threshold criterion.
Fig. 8 visually illustrates the process.

An alternative method for densifying the matches in the
uniform regions could be to register the source and target
faces using the NICP algorithm [41], [78] initialized by the
correspondences established in the previous section. Once
the target face has deformed to the source face, densification
of correspondences is achieved by mapping the vertices in
uniform regions of the source face to that of the target face.
This approach requires tweaking the NICP parameters and
the iterative optimization process for non-rigid face defor-
mation tends to be computationally expensive. Our results
in Section 6.1 also show that our feature matching approach
achieves higher accuracy and therefore, we use this
approach for the remaining part of the paper.

4 K3DM FITTING AND AUGMENTATION

The output of the dense correspondence algorithm is the set of
N densely corresponding 3D faces eFj. Our objective now is to
develop a compact deformable model based on these densely
corresponding faces. To do so we take a standard PCA-based

approach, and we call the result our Keypoint-based 3D

deformable model. More formally, let �� ¼ ½ef1;ef2; . . . ; efN; �,
where ef ¼ ½x1; . . . ; xp; y1; . . . ; yp; z1; . . . ; zp�T and p ¼ 1; . . . ; P .
The rowmeanmm� of the K3DM is given by,

mm� ¼
1

N

XN
i¼1

efi: (3)
The row-normalized model m ¼ �� mm� can be modelled

by a multivariate Gaussian distribution and its eigenvalue
decomposition is given by,

USVT ¼ ��m; (4)
where US are the principal components (PCs), the columns
of V are their corresponding loadings, and S is a diagonal
matrix of eigenvalues. We use only the first n columns of U
which correspond to 98 percent of the energy.

We propose to deform the statistical model given in (4)
into a query face Q in a two step iterative process, i.e., regis-
tration and morphing. Algorithm 1 gives the details of fit-
ting the deformable model to a query face. Note that we use
Q and M for the point clouds of the query face and model
and use q and m for their vectorized versions respectively.
The query face after vectorization can be parametrized by
the statistical model such that mi ¼ Uaai þ mm�, where the

vector aai contains the parameters which are used to vary
the shape of the model in the ith iteration and mi is the vec-
torized form of the model representing the query face. In
the initialization step aai is set to zero and the deformable
model Mi is characterized by the mean face of the K3DM.
Each iteration begins with a registration step where the
input face Q is registered to the model Mi. This step essen-
tially entails finding an approximate correspondence
between the model and the query face and a rigid transfor-
mation. Correspondence is established by searching for the
Nearest Neighbor (NN) of each point of Mi in Q using the
k-d tree data structure [58]. Let d represent the NN euclid-
ean distance between the corresponded query face and the
model such that dj ¼ kfQji �Mijk2. We define outliers as
points on eQ whose NN distance with Mi is greater than a
threshold tc where tc ¼ dþ 3sd and exclude them from reg-
istration. This step ensures that the outliers do not affect the
registration process. Next, the query face is translated to the
mean of the model and is rotated to align with Mi. We
denote the corresponded and registered query face byQr.

In the next step, the modelMi is deformed to fit the regis-
tered query faceQr such that,

âai ( min
aai

U
aai þ mm� � qr
�� ��

2
þ�kaai � aai�1k2; (5)

andmi ¼ Uâai þ mm�. The 
 denotes that only those points (rows
ofU andmm�) are consideredwhich satisfy the threshold tc. The
second term in (5) puts a constraint on deforming the model.
The applied condition is intuitive because we want to partially
deform themodel in each iteration such that themodel approx-
imates the query face in small steps. The iterative procedure is
terminated when the residual error kmi � qrk2 � �f . In all of
our experiments � was set to 0.8 and �f ¼ 10�4. Fig. 9 shows
the K3DMfitting results on three datasets.

Algorithm 1. K3DM Fitting

Require: �m�m ¼ ½ef1;ef2; . . . ;efN � � mm� and Query Face
Q ¼ ½xp; yp; zp�T where p ¼ 1; . . . ; Pq.

Initialization:
1: Iteration: i ¼ 0 and �0 ¼ 1
2: USVT ¼ ��m
3: aai ¼ 0 andmi ¼ Uaai þ mm�
4: while �i > �f do
5: Update iteration: i ¼ iþ 1
6: eQ ¼ Q(Mi (NN using k-d tree)
7: eQ0 ¼ feQkeQi �Mik2 < dþ 3sdg
8: Qr ¼ eQ0Rþ t (Registration step)
9: U
( {Uk rows of U correspond to eQ0}
10: âai(minaai U
aai þ mm� � qr

�� ��
2
þ�kaai � aai�1k2

11: mi ¼ Uâai þ mm�
12: �i ¼ kmi � qrk2
13: end while
14: return Qr; âa;m

From a practical perspective, there is usually a need to
augment an existing dense correspondence model with new
3D faces. In the following, we present a K3DM augmenta-
tion algorithm to achieve this objective. Given the K3DM
and a batch of M new 3D faces, we compute the bending
energy required to deform the mean face of the K3DM to

1590 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018



each of the new faces. This information is employed to orga-
nize theM faces in a Minimum Spanning Tree as outlined in
Section 3.1. Traversing from the root node (mean face), the
K3DM is morphed into each child node using the model fit-
ting procedure given in Algorithm 1. The resulting corre-
sponded 3D face of the input identity is added to the
K3DM. Algorithm 2 gives the details of our model augmen-
tation technique.

Algorithm 2. K3DM Augmentation

Require: �� ¼ ½ef1;ef2; . . . ;efN � and a batch of input 3D faces
FM ¼ ff1; f2; . . . ; fMg,M � 1.

Initialization:
1: Pre-organize the M faces in a Minimum Spanning Tree

P ¼ ðVt; EtÞ
2: for each 3D face fi in P do
3: efi ¼ fit K3DMð��; fiÞ
4: �� ¼ ½ef1;ef2; . . . ;efN;efi�
5: Increment number of faces in the model
6: mm� ¼ 1N

PN
n¼1 n

7: end for
8: return �� ¼ ½ef1;ef2; . . . ;efNþM �

5 EXPERIMENTAL SETUP

We have carried out extensive experiments on synthetic and
real data. Below are the details of the datasets used, evalua-
tion criteria and the experiments performed.

5.1 Datasets Used

Our synthetic dataset consists of 100 3D faces generated
from the Facegen software.1 Facegen has been used by sci-
entists in the field of neuroscience and social cognition to
generate synthetic faces for replicating human stimuli [59],
[60]. The 100 faces are in perfect correspondence with each
other and hence provide the ground truth. Each face has
3,727 vertices and 7,179 triangles. For experiments on real
3D faces, we used the FRGCv2 [24], Bosphorus [25],
BU3DFE [31] and side pose scans of the UND Ear database
Collections F [61] and G [62]. Some sample images and
details of these datasets are given in Fig. 10. The purpose of
using such diverse datasets was to evaluate the perfor-
mance of our proposed technique for partial data, occlusion,
expression and pose invariance.

5.2 Evaluation Criteria

Fig. 11 shows qualitative results of our dense correspon-
dence algorithm. The smooth transition between different
faces is indicative of accurate correspondences [4], [20]. We
have included a video of morphings in the supplementary

material, which can be found on the Computer Society Digi-
tal Library at http://doi.ieeecomputersociety.org/10.1109/
TPAMI.2017.2725279.

Objective evaluation of dense correspondence algorithms
on real data is difficult due to the unavailability of the
ground-truth shape correspondences [63]. One solution is to
use synthetic data where correspondences are known a pri-
ori. We used the synthetic 3D face dataset as ground truth
for our evaluations. To the best of our knowledge, this is the
first time synthetic 3D face images have been used to evalu-
ate results of a dense correspondence algorithm in terms of
mean localization error of the correspondences. This dataset
and protocol was also used to evaluate the efficacy of indi-
vidual modules of our algorithm.

In the case of real data, the accuracy of the dense corre-
spondence can be measured together with the deformable
model fitting algorithm by measuring the accuracy of land-
mark localization and face recognition. Results are expected
to be better when the underlying models have accurate
dense correspondences. Hence, we used our dense corre-
spondence models and fitting algorithm in these applica-
tions and evaluated the results. In all tables, we have
highlighted the best and the second best result in that cate-
gory. Note that the main focus of this paper is to propose a
dense 3D face correspondence algorithm. Experiments on
landmark localization and face recognition have been car-
ried out to validate the accuracy of the correspondences.

We create separate dense correspondence models from
the FRGCv2, Bosphorus and BU3DFE datasets and denote

Fig. 9. K3DM fitting results on three datasets. The first scan for each
dataset is the raw input while the second scan is the fitted model. The 60
degree side pose scan has been rotated to highlight the partial data.

Fig. 10. Sample images and details of our four experimental datasets.

Fig. 11. Qualitative results of our dense correspondence algorithm on
the first three identities of FRGCv2. The first face in each row is the
source and the last face is the target.1. Singular Inversions, “Facegen Modeller”, www.facegen.com

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1591

http://doi.ieeecomputersociety.org/10.1109/TPAMI.2017.2725279
http://doi.ieeecomputersociety.org/10.1109/TPAMI.2017.2725279
www.facegen.com


them with K3DMFR, K3DMBO and K3DMBU respectively.
We compare our results to the Basel Face Model (BFM)
proposed by Paysan et al. [40]. We also establish dense cor-
respondence using our NICP variant for densifying the
initial keypoints based feature matches (see Section 3.6).
The algorithm is initialized by the correspondences found
in Section 3.5 and the variant is referred as K3DM-NICP.

6 RESULTS AND ANALYSIS

6.1 Landmark Localization

Synthetic Dataset. First, we present the evaluation of our
algorithm on synthetic data. We establish dense correspon-
dence on 100 synthetic 3D faces using our proposed algo-
rithm and report the mean and the standard deviation
(SD) of the localization error with respect to the ground
truth. The original synthetic dataset contains 3,727 vertices
for each 3D face. Our proposed method is successful in
establishing dense correspondence over 2,246 vertices (60
percent of the original) with a mean localization error of
1:28 mm and SD �2:2 mm. Correspondence within 10 mm
is established on 99:33 percent vertices. Fig. 12a shows a
plot of the cumulative distribution of correspondences
within a given error distance. We also establish dense cor-
respondence over 2,341 vertices of the synthetic dataset
using our K3DM-NICP variant (see Section 3.6). That
method results in a mean localization error of 1:30 mm
with �2:3 mm standard deviation.

To ascertain the contribution of different components, we
repeat our experiments by removing different components
(see Fig. 1) from our algorithm. The results in Table 1 show
that the combination of all components/modules gives the
best results.

FRGCv2 Dataset. We construct a dense correspondence
K3DM from the first neutral scan of the first 200 identi-
ties (100 males and females each) of this dataset. The

remaining 1,956 scans of 266 identities are used as test
data. Next, we construct a K3DM from the neutral scans
of the next 200 identities (100 male and female each) and
use the 2,051 scans corresponding to the first training set
for testing. This way, we are able to perform landmark
detection on all 4,007 scans of FRGCv2, each time ensur-
ing that the identity used for making the K3DM is not
present in the test data.

We establish dense correspondences between 9,309 verti-
ces on the FRGCv2 dataset (K3DMFR) and report the mean
and SD of the Landmark Localization Error (�L) on 14 fidu-
cial points considered to be biologically significant [65].
These anthropometric landmarks are annotated only on the
mean face and transferred to each densely corresponded
scan in the dataset. Manual annotations provided by
Szeptycki et al. [66] and Creusot et al. [27] were used as
ground truth for comparison.

A comparison of the mean and SD of landmark localiza-
tion error of our proposed algorithm with the state-of-the-
art in Table 2 shows that our results outperform them by a
significant margin. K3DMBU was constructed from 100 neu-
tral expression scans and 100 angry expression level-1
scans. K3DMBO was constructed from the first neutral scan
of 105 identities. The K3DMFR achieves the best perfor-
mance and even the cross domain K3DMs and the K3DM-
NICP variant outperform existing state-of-the-art. Cumula-
tive localization error plots using K3DMFR are shown
graphically in Fig. 12b.

Bosphorus Dataset. We construct two K3DMBO (100 faces
each) from the neutral scans of the Bosphorus dataset [25]
such that the model and test identities are mutually exclu-
sive. Note that there are 299 neutral expression scans in the
dataset. We manually annotate 14 fiducial landmarks on the
mean face of K3DM and transfer the information to other
scans after model fitting. Fig. 12c shows the cumulative
detection rate of the 14 landmarks. Table 3 details landmark
localization results on the three categories of the Bosphorus
dataset. It is evident that our algorithm performs signifi-
cantly better than the state-of-the-art under occlusions,
rotation and expression variation. Creusot et al. [27] and
Sukno et al. [64] trained their algorithms on 99 neutral
scans. They did not report results on these 99 scans and
the scans with yaw rotation of �90�. On the contrary we
report the landmarking results on all 4,666 scans of the
database including the scans with large yaw variation.
Landmark annotations provided by [25], [27] were used as
ground truth. For this experiment, K3DMFR was created
from the neutral scans of first 100 male and female (each)

Fig. 12. Results of dense correspondence: (a-d) Cumulative localization error distribution plots on the Synthetic (2,246 vertices), FRGCv2 (14 land-
marks), Bosphorus (14 landmarks) and BU3DFE (12 landmarks) datasets.

TABLE 1
Module Wise Mean and SD of Localization Error
(mm) on 2,246 Vertices of the Synthetic Dataset

Excluded Module(s) mean � std
Organising faces into a graph 2.16 � 2.8
Keypoint detection 3.06 � 5.1
Feature matching 3.61 � 6.8
Keypoint detection and feature matching 4.78 � 7.3
Selecting best matches in each iteration 2.61 � 3.4
No modules excluded 1.28 � 2.2

1592 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018



identities of FRGCv2 while K3DMBU was the same as used
in experiment on FRGCv2 dataset.

BU3DFE Dataset. We construct dense correspondence
models from the neutral as well as intensity level-1 anger
expression scans of 100 identities of the BU3DFE data-
set [31]. We ensure mutually exclusive test and training
identities while landmark localization using a K3DMBU .
Comparative results on 12 anthropometric landmarks [7] on
all 2,500 scans of the dataset are given in Table 4. Ground
truth landmark locations are provided with the dataset [31].

Fig. 12d shows the commutative error detection rate of the
12 landmarks. K3DMFR was created from the scans of the
first 100 male and 100 female identities of FRGC. Our results
are better than the state-of-the-art for both the models.

UND Ear Dataset. To evaluate the landmark localization
performance of our algorithm on side pose scans containing
self occlusions, we perform experiments on the UND Ear
Database [61], [62]. We follow the exact protocol outlined
by [18], [38] for a fair comparison. The dataset is divided into
45 and 60 degree left and right pose scans namely DB45L,

TABLE 2
Comparative Results of the Mean and SD (mm) of Landmark Localization Error on FRGCv2 Dataset

Author Images Ex(L) Ex( R ) En(L) En( R) N Prn Ac* Ch* Ls Li Pg Sn Mean

Lu [36] 676 9.5 � 17.1 10.3 � 18.1 8.2 �17.2 8.3 � 17.2 - 8.3 � 19.4 - 6.1 � 17.4 - - - - 8.1 � 17.7
Segundo [37] 4007 - - 3.7 � 2.3 3.4 � 2.3 - 2.8 � 1.4 5.3 � 1.9 - - - - 4.1 � 1.9
Perakis [26] 975 5.6 � 3.1 5.8 � 3.4 4.2 � 2.2 4.4 � 2.5 - - 4.1 � 2.2 5.5 � 2.4 - - 4.9 � 3.7 - 5.0 � 2.7
Cruesot [27] 4007 5.9 � 3.1 6.0 � 3.0 4.3 � 2.4 4.3 � 2.0 4.2 � 2.0 3.4 � 2.0 4.8 � 3.6 5.5 � 3.5 4.2 � 3.2 5.5 � 3.9 7.3 � 7.4 3.7 � 3.1 5.0 � 3.3
Perakis [38] 975 4.7 5.4 4.0 4.1 3.7 4.3 - 4.1 - - 4.3

Sukno [64] 4007 4.7 � 2.7 4.6 � 2.7 3.5 � 1.7 3.6 � 1.7 2.5 � 1.6 2.3 � 1.7 2.6 � 1.4 3.9 � 2.8 3.3 � 1.8 4.6 � 3.4 4.9 � 3.5 2.7 � 1.1 3.5 � 2.4
Gilani [28] 4007 4.5 � 2.9 3.7 � 2.8 3.1 � 2.1 2.7 � 2.1 3.6 � 2.0 2.7 � 2.5 4.2 � 3.2 4.8 � 2.1 3.3 � 3.7 4.0 � 3.8 4.2 � 3.3 4.1 � 3.1 3.9 � 2.8
BFM [40] 4007 2.2 � 2.5 2.7 � 1.8 2.5 � 2.1 2.9 � 2.2 3.2 � 2.2 2.3 � 2.0 8.3 � 2.9 2.6 � 2.9 2.6 � 2.2 3.8 � 3.7 4.2 � 3.8 3.8 � 3.6 3.7 � 2.7
K3DM-NICP 4007 2.8 � 2.2 2.5 � 1.8 2.7 � 1.8 2.6 � 1.1 2.6 � 1.7 2.4 � 1.9 3.3 � 2.5 2.7 � 1.8 2.6 � 3.2 4.2 � 3.4 4.2 � 3.3 3.5 � 1.4 3.3 � 2.3
K3DMFR 4007 2.6 � 2.1 2.4 � 1.7 2.4 � 1.6 2.4 � 0.9 2.5 � 1.5 2.2 � 1.8 3.0 � 2.4 2.5 � 1.8 2.4 � 3.1 4.1 � 3.3 4.1 � 3.3 3.4 � 1.1 2.9 � 2.1
K3DMBU 4007 2.7 � 2.4 2.3 � 1.9 2.4 � 1.9 2.5 � 1.8 2.8 � 1.8 2.6 � 1.8 6.1 � 2.7 4.2 � 3.1 2.9 � 3.3 4.6 � 3.9 4.1 � 3.4 3.6 � 2.9 3.6 � 2.6
K3DMBO 4007 2.6 � 2.2 2.4 � 1.9 2.8 � 2.0 2.9 � 2.0 3.2 � 2.2 2.3 � 2.1 8.3 � 3.4 3.1 � 2.7 2.5 � 2.4 3.5 � 3.7 4.1 � 3.9 3.8 � 3.6 3.8 � 2.7

* Results have been averaged for left and right corners of nose and mouth.
A ‘-’ denotes that the authors have not detected this particular landmark. Ex/En-outer/inner eye corner, N-nosebridge saddle, Prn-nosetip, Ac-nose curvature, Ch-
mouth corner, Ls/Li upper/lower lip midpoint, Pg-chintip, Sn-nasal base.

TABLE 3
Comparison of Landmark Localization Results with the State-of-the-Art on Bosphorus Dataset

Mean of Localization Error (mm)

Author Images Ex(L) Ex( R ) En(L) En( R) N Prn Ac(L) Ac ( R) Ch( L) Ch( R) Ls Li Pg Sn Mean

E
x
p
re
ss
io
n Cruesot et al. [27] 2803 6.20 4.10 5.09 4.18 6.33 4.47 4.22 4.07 4.06 8.00 7.66 5.36 8.83 15.23 6.27

Sukno et al. [64] 2803 5.19 4.92 2.94 2.76 2.22 2.33 3.03 3.01 6.12 6.03 4.00 6.54 7.58 2.81 4.25
BFM [40] 2920 3.58 3.62 2.77 2.65 2.17 2.90 3.80 4.63 5.86 6.01 3.99 6.77 8.12 3.59 4.32
K3DMBO 2920 3.57 4.01 2.35 2.40 2.32 2.82 2.50 2.99 4.85 4.91 3.32 5.03 6.02 2.35 3.53

R
o
ta
ti
o
n

Cruesot et al. [27] 1155 5.42 4.12 5.18 3.65 5.17 4.89 3.52 3.43 4.05 4.29 3.84 3.81 4.68 9.47 4.68
Sukno et al. [64] 1155 4.48 4.95 2.97 3.23 3.40 4.36 3.36 3.37 3.76 3.75 3.47 5.01 7.77 4.19 4.15

BFM [40] 1365 4.63 4.96 5.30 5.16 3.81 5.08 4.81 5.49 4.49 5.28 5.43 6.40 7.10 3.14 5.08
K3DMBO 1365 4.84 5.09 3.31 3.85 2.68 3.19 2.73 3.20 4.53 4.91 4.13 5.84 6.22 3.80 4.14

O
cc
lu
si
o
n Cruesot et al. [27] 381 8.13 5.45 5.60 4.99 7.78 4.72 5.34 4.85 4.10 5.62 4.81 4.30 5.44 11.05 5.87

Sukno et al. [64] 381 6.63 6.28 3.82 3.87 4.12 3.83 4.40 4.67 4.75 5.07 3.61 4.81 7.63 3.76 4.80
BFM [40] 381 4.95 4.42 3.96 3.52 2.49 3.32 4.57 4.77 3.61 3.75 3.36 4.40 5.54 2.45 3.94
K3DMBO 381 4.64 4.51 3.10 2.95 2.69 3.18 2.55 3.01 4.36 4.22 2.89 4.14 5.00 2.90 3.58

All

Cruesot et al. [27] 4339 6.09 4.18 5.14 4.08 6.10 4.60 4.15 3.94 4.05 6.83 6.37 4.81 7.35 13.20 5.78
Sukno et al. [64] 4339 5.13 5.05 3.03 2.98 2.70 3.00 3.24 3.25 5.37 5.34 3.82 5.98 7.63 3.26 4.27

BFM [40] 4666 3.93 4.03 3.41 3.34 2.68 3.57 4.07 4.86 5.34 5.65 4.37 6.50 7.64 3.38 4.48
K3DMBO 4666 3.94 4.15 2.62 2.80 2.46 2.96 2.55 3.04 4.73 4.86 3.53 5.21 6.01 2.75 3.70
K3DMBU 4666 4.04 4.25 3.21 3.12 2.50 3.27 3.65 4.34 5.16 5.45 4.25 6.25 7.26 3.16 4.27
K3DMFR 4666 4.13 4.27 3.33 3.24 2.60 3.51 3.91 4.61 5.26 5.56 4.32 6.43 7.48 3.26 4.42

Standard Deviation of Localization Error (mm)

Images Ex(L) Ex( R ) En(L) En( R) N Prn Ac(L) Ac ( R) Ch( L) Ch( R) Ls Li Pg Sn Mean

All

Cruesot et al. [27] 4339 5.02 3.79 4.43 3.49 5.22 4.61 3.45 3.11 2.95 5.35 5.17 3.95 8.36 10.37 4.95
Sukno et al. [64] 4339 4.01 3.86 2.15 2.33 2.27 2.56 2.37 2.42 5.06 4.75 3.51 6.86 7.16 2.37 3.69

BFM [40] 4666 2.84 2.97 3.30 3.60 2.58 3.44 2.63 2.86 4.23 4.22 3.93 6.76 6.98 3.09 3.82
K3DMBO 4666 2.69 2.82 2.06 2.23 1.63 1.61 1.45 1.59 3.13 3.03 2.99 4.23 4.28 2.23 2.57
K3DMBU 4666 2.87 3.01 2.98 3.17 2.42 3.04 2.36 2.56 4.01 3.99 3.75 6.13 6.20 2.88 3.53
K3DMFR 4666 2.93 3.07 3.10 3.30 2.52 3.26 2.54 2.72 4.10 4.07 3.80 6.31 6.39 2.97 3.65

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1593



DB45R,DB60L andDB60R. This is a very challenging dataset
due to large yaw rotations, noisy scans and self occlusions.
The dense correspondence model is created from 200 neutral
expression scans of FRGCv2. Eight landmarks including the
two inner and outer eye corners, nose tip, mouth corners and
chin tip are annotated on the mean face of K3DMFR. The
mean and SD of landmark localization error for all 8 points is
comparedwith the state-of-the-art in Table 5.

6.2 Face Recognition

Imaging of faces is considered to be one of the most impor-
tant biometrics because it can be done passively and is
highly distinctive between individuals. 3D face recognition
has addressed many shortcomings of its counterpart in the
2D domain [67]. We consider this application apt to test the
quality of the presented algorithms. Note that our main aim
is to evaluate our proposed correspondence and model fit-
ting algorithms as opposed to presenting a face recognition
system per se.

FRGCv2 Dataset. We follow the FRGCv2 protocols [24] of
face recognition and include only one scan of each individ-
ual (466 in total) in the gallery. To demonstrate the effective-
ness of our K3DM Augmentation algorithm, we construct a
dense correspondence model on the first available neutral
scans of the first 200 identities of FRGCv2 dataset. The con-
structed model is then augmented with first available neu-
tral scans of the remaining 266 identities of the database
using Algorithm 2. The probe set consists of the remaining
(3,541) scans of all identities. Note that there is only one
scan per identity for 56 individuals in the dataset. All of
these identities appear only in the gallery. The complete
dataset was further classified into “neutral” and “non-
neutral” expression subclasses following the protocol out-
lined in [44] to evaluate the effects of expressions on
deformable model fitting and face recognition.

We employ a holistic and region based approach to
model fitting and face recognition. It is well known that
the generalization of a model can be increased by divid-
ing faces into independent subregions that are morphed

independently [29]. This technique has been used exten-
sively for face recognition [9], [67] and recently for match-
ing offsprings to their parents [68]. We also use this
approach and perform face recognition by morphing the
complete face as well as the eyes and nose regions We
define these regions on the mean face which is sufficient
to transfer the information to all the faces in the dense
correspondence model.

The full K3DMFR and the eyes and nose models are sepa-
rately morphed and fitted to each query face in the probe to
obtain model parameters (a-Step 12 in Algorithm 1). Next,
the parameters from the whole face and the regions are
concatenated to form the feature vector for face recognition.
We then perform feature selection using the GEFS algo-
rithm [69], [70] on the training data set of FRGCv2 contain-
ing 953 facial scans. Note that these scans are not used in
testing the face recognition algorithm. The selected features
of each query face are matched with those of the gallery
faces in the model. The query face is assigned the identity of
the gallery face with which it has the smallest distance

df ¼ cos �1 eaaTMeaaQkeaaT
M
k2keaaQk2, where eaaM are the selected features of

each face in K3DM and eaaQ are the selected features of the
query face.

Fig. 14 shows the process of model fitting in PCA space.
The dense correspondence model is iteratively fitted on the
query face, which in the figure is an extreme expression scan
of the first identity. The model fitting starts from the mean
face and in each iteration the fitted query model traverses
closer to its gallery face in the PCA space. Face recognition is
performed when the fitting residual error �f is less than 10

�5.
Figs. 13a and b show the resulting CMC and ROC curves.
Rank-1 identification rate for neutral probes is 99.85 percent
while 100 percent accuracy is achieved at Rank-8. In the
more difficult scenario of neutral versus non-neutral, the
Rank-1 identification rate is 96.3 percent. A similar trend is
observed in the verification rates at 0.1 percent FAR. Table 6

TABLE 4
Comparison of Landmark Localization Results (Mean � SD) with the State-of-the-Art on BU3DFE Dataset

Author Images Ex(L) En(L) N Ex( R ) En( R) Prn Ac Ch Ls Li Mean

Nair et al. [8] 2350 - 12.1 - - 11.9 8.8 - - - - 10.9
Segundo et al. [37] 2500 - 6.3 � 4.8 - - 6.3 � 5.0 1.9 � 1.1 6.6 � 3.4 - - - 4.4� 3.5
Salazar et al. [35] 350 9.6 � 6.1 6.8 � 4.5 - 8.5 � 5.8 6.1 � 4.2 5.9 � 2.7 6.8 � 3.2 - - - 5.9� 4.3
Gilani et al. [28] 2500 4.4 � 2.7 4.8 � 2.6 4.5 � 2.7 4.4 � 2.7 3.3 � 2.7 2.9 � 2.0 4.3 � 2.7 5.7 � 3.7 4.2 � 2.7 6.9 � 6.3 3.7� 3.1
K3DMBU 2500 3.8 � 2.2 2.2 � 1.5 2.9 � 2.1 3.3 � 2.2 2.4 � 1.6 2.5 � 1.7 2.3 � 1.6 4.6 � 3.3 3.6 � 2.3 6.4 � 6.1 2.8� 2.5
K3DMFR 2500 4.0 � 2.4 2.8 � 1.6 4.3 � 2.6 3.6 � 2.4 2.7 � 1.7 2.6 � 1.8 2.9 � 1.8 5.4 � 3.5 3.8 � 2.4 7.0 � 6.6 3.1� 2.7

TABLE 5
Comparative Landmark Localisation Results

(mm) on UND Side Pose Scans

Database DB45L DB45R DB60L DB60R

Yaw Est [26] �45� � 9� 44� � 8� �59� � 8� 57� � 7�
# Scans 118 118 87 87
Passalis et al. [18] 6.02 � 2.45 5.83 � 2.49 6.08 � 2.53 5.87 � 2.4
Perakis et al. [26] 4.75 � 1.91 5.03 �1.92 5.30 �2.49 4.95 � 1.80
K3DMFR 4.04 � 1.77 4.31 � 1.90 4.36 � 2.25 4.24 � 1.28

Fig. 13. (Left) ROC curves for identification and (Right) verification tasks
on FRGCv2 database using our dense correspondence model and fitting
algorithms.

1594 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018



compares our algorithm with the state-of-the-art. In most
cases, our results are better than the state-of-the-art depicting
the high quality of the dense correspondencemodel.

Bosphorus Dataset. Experiments are performed on the
more versatile Bosphorus dataset to demonstrate the
expression, pose and occlusion invariant face recognition
capabilities of our proposed model. K3DMBO is formed
from the first available neutral scan of each identity in the
dataset and a holistic approach to face recognition is
adopted. We follow the model fitting and parameter match-
ing technique as mentioned for FRGCv2. Comparative
results are given in Table 7. Our proposed technique signifi-
cantly outperforms the state-of-the-art in pose invariant
face recognition, while at the same time it handles expres-
sions and occlusions.

UND Ear Dataset. We perform face recognition experi-
ments on this dataset to demonstrate the ability of K3DM to
handle pose variations and self occlusions. The dataset is
divided into three subsets following the protocol set by [18].
UND00LR contains 466 subjects of FRGCv2 in the gallery.

Two 45 degree side scans each (left and right) for 39 sub-
jects and two 60 degree side scans each (left and right) for
32 subjects make the probe set. These subjects are common
between FRGCv2 and UND Ear databases. UND45LR is
composed of 45 degree side scans from 118 subjects. The
K3DMFR made from 200 scans is fitted on the left side scan
to get the gallery parameters and then fitted to the right
side scan to get the probe parameters. A similar protocol is
followed for UND60LR which contains 60 degree side
scans from 87 subjects. Comparative Rank-1 recognition
results are given in Table 8. Note that while Smeets
et al. [45] report > 98 percent face recognition results on
the side pose scans of this dataset, their performance on
pose variation in the Bosphorus dataset is significantly
low at 84.2 percent.

Cross Domain Face Recognition. To compare K3DM with
the state-of-the-art Basel Face Model(BFM) [40] we perform
cross domain face recognition experiments on FRGCv2 and
Bosphorus datasets as they include all the challenges of
expressions, occlusions and pose variation. For FRGCv2 we
use K3DMBO (created from 105 neutral scans of Bosphorus
database) and K3DMBU (created from 100 neutral and 100
angry level-1 scans) while for Bosphorus dataset we use
K3DMFR and K3DMBU . All three models are fitted to each
scan in FRGCv2 and Bosphorus datasets. The model param-
eters of the first neutral scan of each identity in each data-
base are used as gallery features. Table 9 details the Rank-1
recognition results from this experiment which show that
K3DM outperforms the BFM.

Fig. 14. Iterative model fitting. The 466 FRGCv2 identities are shown as
red stars in the first three PC space. The model is morphed iteratively
into the query face until the residual error is negligible. Notice how the fit-
ting process takes the query face through a non-linear path (inset image)
and removes the extreme facial expression to generate its equivalent
neutral expression model.

TABLE 6
Comparison of 3D Face Recognition Results with the
State-of-the-Art in Terms of Rank-1 Identification Rate

(I-Rate) and Verification Rate (V-Rate) at 0.1 Percent FAR

Author Neutral Non-neutral All

I-Rate V-Rate I-Rate V-Rate I-Rate V-Rate

Mian et al. [44] 99.4% 99.9% 92.1% 96.6% 96.1% 98.6%
Kakadiaris et al. [43] - 99.0% - 95.6% 97.0% 97.3%
Al-Osaimi et al. [71] 97.6% 98.4% 95.2% 97.8% 96.5% 98.1%
Queirolo et al. [72] - 99.5% - 94.8% 98.4% 96.6%
Drira et al. [73] 99.2% - 96.8% - 97.7% 97.1%
Smeets et al. [45] - - - - 89.6% 79.0%
Li et al. [47] - - - - 96.3% -
K3DMFR 99.9% 99.9% 96.9% 96.6% 98.5% 98.7%

TABLE 7
Comparison of Rank-1 Recognition Results (in Percentage) with the State-of-the-Art on Bosphorus Dataset

Author

Expressions Poses Occlusions

All 4,543AU Expr All YR< 90 YR90 PR CR All Eye Mouth Glasses Hair All

2,150 647 2,797 525 210 419 211 1,365 105 105 104 67 381

Alyz et al. [74] - - - - - - - - 93.6 93.6 97.8 89.6 93.6 -
Colombo et al. [75] - - - - - - - - 91.1 74.7 94.2 90.4 87.6 -
Drira et al. [73] - - - - - - - - 97.1 78.0 94.2 81.0 87.0 -
Berretti et al. [46] - - 95.7 81.6 45.7 98.3 93.4 88.6 - - - - 93.2 93.4
Smeetset al. [45] - - 97.7 - 24.3 - - 84.2 - - - - - 93.7
Li et al. [47] 99.2 96.6 98.8 84.1 47.1 99.5 99.1 91.1 100.0 100.0 100.0 95.5 99.2 96.6
K3DMBO 99.0 96.7 98.5 99.8 95.2 100.0 99.1 99.0 99.0 96.1 100.0 97.3 98.1 98.6

AU=Action Units; YR=Yaw Rotation; PR= Pitch Rotation; CR= Cross Rotation.

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1595



7 DISCUSSION AND CONCLUSIONS

We have proposed an algorithm that simultaneously estab-
lishes dense correspondences between a large number of 3D
faces. Based on the dense correspondences, a deformable
face model was constructed. We also proposed morphable
model fitting and updating algorithms that are useful for
landmark identification and face recognition. Thorough
experiments were performed on synthetic and real 3D faces.
Comparison with existing state-of-the-art shows that our
algorithm consistently achieves better or comparable perfor-
mance on both the tasks on all datasets. It is interesting to
note that while the face recognition algorithm proposed by
Li et al. [47] performs well on Bosphorus database, it does
not fare that well on FRGCv2. Similar trend can be observed
in case of Smeets et al. [45] for face recognition and
Sukno et al. [64] for landmark localization. To the best of our
knowledge this is the first paper that has reported consistent
comparable results on a variety of application on four
public datasets.

Although the dense correspondence model assumes
frontal and neutral pose scans, for landmark localization
and face recognition it demonstrates robustness to occlusion
as well as pose and expression variation during the fitting
process. Hence the three proposed algorithms present a uni-
fied solution to a variety of applications under expression,
occlusion and pose variation. The model can handle pose
variation up to �90�.

With regards to the computational complexity, it may be
noted that the model building process has to be done off-
line. The algorithm iterates over geodesic patches between
vertices for each image. Building a dense correspondence
model on 105 identities of Bosphorus database in approxi-
mately 30 iterations took over 48 hours on a Core–i7
machine with 8 GB RAM using MATLAB. However, the
model fitting process on an unseen face takes less than
seven seconds.

ACKNOWLEDGMENTS

This research was supported by ARC Discovery Grants
DP110102399 and DP160101458. I. Reid gratefully acknowl-
edges the financial support of the Australian Research
Council through grants CE140100016 and FL130100102.

REFERENCES
[1] O. Van Kaick , H. Zhang, G. Hamarneh, and D. Cohen-Or, “A

survey on shape correspondence,” Comput. Graph. Forum, vol. 30,
no. 6, pp. 1681–1707, 2011.

[2] V. Jain, H. Zhang, and O. van Kaick, “Non-rigid spectral corre-
spondence of triangle meshes,” Int. J. Shape Model., vol. 13 no. 1,
pp. 101–124, 2007.

[3] W. Chang and M. Zwicker, “Automatic registration for articu-
lated shapes,” Comput. Graph. Forum, vol. 27, no. 5, pp. 1459–
1468, 2008.

[4] H. Zhang, A. Sheffer, D. Cohen, Q. Zhou, O. van Kaick, and
A. Tagliasacchi, “Deformation-driven shape correspondence,”
Comput. Graph. Forum, vol. 27, no. 5, pp. 1431–1439, 2008.

[5] S. Z. Gilani, K. Rooney, F. Shafait, M. Walters, and A. Mian,
“Geometric facial gender scoring: Objectivity of perception,” PloS
One, vol. 9, no. 6, 2014, Art. no. e99483.

[6] P. Hammond, “The use of 3D face shape modelling in dys-
morphology,” Archives Disease Childhood, vol. 92, no. 12, pp. 1120–
1126, 2007.

[7] L. Farkas, “Anthropometry of the head and face in clinical
practice,” in Anthropometry of the Head and Face, 2nd ed. Ann
Arbor, MI, USA: Univ. Michigan, 1994, pp. 71–111.

[8] P. Nair and A. Cavallaro, “3-D face detection, landmark localiza-
tion, and registration using a point distribution model,” IEEE
Trans. Multimedia, vol. 11, no. 4, pp. 611–623, Jun. 2009.

[9] V. Blanz and T. Vetter, “Face recognition based on fitting a 3D
morphable model,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 25,
no. 9, pp. 1063–1074, Sep. 2003.

[10] S. Z. Gilani, A. Mian, and P. Eastwood, “Deep, dense and accu-
rate 3D face correspondence for generating population specific
deformable models,” Pattern Recognit., vol. 69, pp. 238–250,
2017.

[11] R. H. Davies, C. J. Twining, T. F. Cootes, J. C. Waterton, and
C. J. Taylor, “3D statistical shape models using direct optimisation
of description length,” inProc. Eur. Conf. Comput. Vis., 2002, pp. 3–20.

[12] T. Heimann and H.-P. Meinzer, “Statistical shape models for 3D
medical image segmentation: A review,”Med. Image Anal., vol. 13,
no. 4, pp. 543–563, 2009.

[13] M. Alexa, “Recent advances in mesh morphing,” Comput. Graph.
Forum, vol. 21, no. 2, pp. 173–198, 2002.

[14] D. Aiger, N. Mitra, and D. Cohen, “4-points congruent sets for
robust pairwise surface registration,” ACM Trans. Graph., vol. 27,
no. 3, 2008, Art. no. 85.

[15] B. Brown and S. Rusinkiewicz, “Global non-rigid alignment of 3-D
scans,” ACM Trans. Graph., vol. 26, no. 3, 2007, Art. no. 21.

[16] H.Mirzaalian, G.Hamarneh, and T. Lee, “A graph-based approach
to skin mole matching incorporating template-normalized coor-
dinates,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2009,
pp. 2152–2159.

[17] T. Funkhouser and P. Shilane, “Partial matching of 3D shapes
with priority-driven search,” in Proc. Eurographics Symp. Geometry
Process., 2006, vol. 256, pp. 131–142.

[18] G. Passalis, P. Perakis, T. Theoharis, and I. A. Kakadiaris, “Using
facial symmetry to handle pose variations in real-world 3D face
recognition,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 10,
pp. 1938–1951, Oct. 2011.

[19] U. Prabhu, J. Heo, andM. Savvides, “Unconstrained pose-invariant
face recognition using 3D generic elastic models,” IEEE Trans.
Pattern Anal. Mach. Intell., vol. 33, no. 10, pp. 1952–1961, Oct. 2011.

[20] V. Kraevoy and A. Sheffer, “Cross-parameterization and compati-
ble remeshing of 3D models,” ACM Trans. Graph., vol. 23, pp. 861–
869, 2004.

[21] H. Hochheiser, et al., “The facebase consortium: A comprehensive
program to facilitate craniofacial research,” Develop. Biology,
vol. 355, no. 2, pp. 175–182, 2011.

[22] A. J. Whitehouse, et al., “Prenatal testosterone exposure is related
to sexually dimorphic facial morphology in adulthood,” Proc. Roy.
Soc. B, vol. 282, no. 1816, 2015, Art. no. 7.

TABLE 8
Comparative of Rank-1 Recognition Results on

Partial Faces of UND Side Pose Scans

Database UND00LR UND45LR UND60LR

# Scans 608 236 174
Passalis et al. [18] 76.8% 86.4% 81.6%
Smeets et al. [45] - 98.3% 100.0%
K3DMFR 86.0% 95.8% 98.6%

TABLE 9
Comparison of Rank-1 Recognition on FRGCv2 and
Bosphorus Datasets Using Cross Domain Models

FRGCv2

Method Neutral Expressions Poses Occlusions All

BFM [40] 87.7% 65.6% - - 76.4%
K3DMBU 92.7% 69.0% - - 80.5%
K3DMBO 92.1% 62.9% - - 77.1%

Bosphorus

BFM [40] - 81.1% 86.1% 86.6% 82.7%
K3DMFR - 85.6% 86.5% 89.3% 85.8%
K3DMBU - 90.3% 92.8% 90.7% 90.7%

1596 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018



[23] S. Z. Gilani, et al., “Sexually dimorphic facial features vary accord-
ing to level of autistic-like traits in the general population,” J. Neu-
rodevelopmental Disorders, vol. 7, no. 1, 2015, Art. no. 14.

[24] P. Phillips, et al., “Overview of the face recognition grand
challenge,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern
Recognit., 2005, pp. 947–954.

[25] A. Savran, et al., “Bosphorus database for 3D face analysis,” in
Proc. Eur. Workshop Biometrics Identity Manage., 2008, pp. 47–56.

[26] P. Perakis, G. Passalis, T. Theoharis, and I. A. Kakadiaris, “3D
facial landmark detection under large yaw and expression var-
iations,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 7,
pp. 1552–1564, Jul. 2013.

[27] C. Creusot, N. Pears, and J. Austin, “A machine-learning
approach to keypoint detection and landmarking on 3D meshes,”
Int. J. Comput.Vis., vol. 102, no. 1–3, pp. 146–179, 2013.

[28] S. Z. Gilani, F. Shafait, and A. Mian, “Shape-based automatic
detection of a large number of 3D facial landmarks,” in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit., 2015, pp. 4639–4648.

[29] V. Blanz and T. Vetter, “A morphable model for the synthesis of
3D faces,” in Proc. ACM Conf. Comput. Graph. Interactive Techn.,
1999, pp. 187–194.

[30] Y. Sun, J. Paik, A. Koschan, D. Page, and M. Abidi, “Point fin-
gerprint: A new 3D object representation scheme,” IEEE Trans.
Syst. Man Cybern. Part B: Cybern., vol. 33, no. 4, pp. 712–717,
Aug. 2003.

[31] L. Yin, X. Wei, Y. Sun, J. Wang, and M. J. Rosato, “A 3D facial
expression database for facial behavior research,” in Proc. 7th Int.
Conf. Automatic Face Gesture Recognit., 2006, pp. 211–216.

[32] Y. Sun and M. A. Abidi, “Surface matching by 3D point’s finger-
print,” in Proc. 8th IEEE Int. Conf. Comput. Vis., 2001, pp. 263–269.

[33] S. Wang, Y. Wang, M. Jin, X. D. Gu, and D. Samaras, “Conformal
geometry and its applications on 3D shape matching, recognition,
and stitching,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 29,
no. 7, pp. 1209–1220, Jul. 2007.

[34] J. Novatnack and K. Nishino, “Scale-dependent/invariant local
3D shape descriptors for fully automatic registration of multiple
sets of range images,” in Proc. 10th Eur. Conf. Comput. Vis., 2008,
pp. 440–453.

[35] A. Salazar, S. Wuhrer, C. Shu, and F. Prieto, “Fully automatic
expression-invariant face correspondence,” Mach. Vis. Appl.,
vol. 25, no. 4, pp. 859–879, 2014.

[36] X. Lu and A. K. Jain, “Automatic feature extraction for multiview
3D face recognition,” in Proc. 7th Int. Conf. Automatic Face Gesture
Recognit., 2006, pp. 585–590.

[37] M. Segundo, L. Silva, P. Bellon, and C. C. Queirolo, “Automatic
face segmentation and facial landmark detection in range
images,” IEEE Trans. Syst. Man Cybern. Part B: Cybern., vol. 40,
no. 5, pp. 1319–1330, Oct. 2010.

[38] P. Perakis, T. Theoharis, and I. A. Kakadiaris, “Feature fusion for
facial landmark detection,” Pattern Recognit., vol. 47, no. 9,
pp. 2783–2793, 2014.

[39] V. Blanz, K. Scherbaum, and H.-P. Seidel, “Fitting a morphable
model to 3D scans of faces,” in Proc. 11th IEEE Int. Conf. Comput.
Vis., 2007, pp. 1–8.

[40] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter,
“A 3D face model for pose and illumination invariant face recog-
nition,” in Proc. Int. Conf. Adv. Video Signal Based Surveillance, 2009,
pp. 296–301.

[41] B. Amberg, S. Romdhani, and T. Vetter, “Optimal step nonrigid
ICP algorithms for surface registration,” in Proc. IEEE Conf. Com-
put. Vis. Pattern Recognit., 2007, pp. 1–8.

[42] G. Passalis, I. Kakadiaris, T. Theoharis, G. Toderici, and N. Mur-
tuza, “Evaluation of 3D face recognition in the presence of facial
expressions: An annotated deformable model approach,” in Proc.
IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Workshops,
2005, pp. 171–171.

[43] I. A. Kakadiaris, et al., “Three-dimensional face recognition in the
presence of facial expressions: An annotated deformable model
approach,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 4,
pp. 640–649, Apr. 2007.

[44] A. Mian, M. Bennamoun, and R. Owens, “Keypoint detection and
local feature matching for textured 3D face recognition,” Int.
J. Comput. Vis., vol. 79, no. 1, pp. 1–12, 2008.

[45] D. Smeets, J. Keustermans, D. Vandermeulen, and P. Suetens,
“MeshSIFT: Local surface features for 3D face recognition under
expression variations and partial data,” Comput. Vis. Image Under-
standing, vol. 117, no. 2, pp. 158–169, 2013.

[46] S. Berretti, N. Werghi, A. Del Bimbo , and P. Pala, “Matching 3D
face scans using interest points and local histogram descriptors,”
Comput. Graph., vol. 37, no. 5, pp. 509–525, 2013.

[47] H. Li, D. Huang, J.-M. Morvan, Y. Wang, and L. Chen, “Towards
3D face recognition in the real: A registration-free approach using
fine-grained matching of 3D keypoint descriptors,” Int. J. Comput.
Vis., vol. 113, no. 2, pp. 128–142, 2014.

[48] F. L. Bookstein, “Principal warps: Thin-plate splines and the
decomposition of deformations,” IEEE Trans. Pattern Anal. Mach.
Intell., vol. 11, no. 6, pp. 567–585, Jun. 1989.

[49] D. Rueckert, L. Sonoda, C. Hayes, D. L. G. Hill, M. O. Leach, and
D. J. Hawkes, “Nonrigid registration using free-form deforma-
tions: Application to breast MR images,” IEEE Trans. Med. Imag.,
vol. 18, no. 8, pp. 712–721, Aug. 1999.

[50] D.-J. Kroon, “Finite iterative closest point,” MATLAB Central
File Exchange, 2009, https://au.mathworks.com/matlabcentral/
fileexchange/24301-finite-iterative-closest-point

[51] Y. Guo, F. Sohel, M. Bennamoun, M. Lu, and J. Wan, “Rotational
projection statistics for 3D local surface description and object rec-
ognition,” Int. J. Comput. Vis., vol. 105, no. 1, pp. 63–86, 2013.

[52] F. Tombari, S. Salti, and L. Di Stefano, “Unique signatures of histo-
grams for local surface description,” in Proc. 11th Eur. Conf.
Comput. Vis., 2010, pp. 356–369.

[53] R. C. Gonzalez and R. E. Woods, Digital Image Processing, 2nd ed.
Upper Saddle River, NJ USA: Prentice Hall, 2002.

[54] A. Mian, M. Bennamoun, and R. Owens, “On the repeatability and
quality of keypoints for local feature-based 3D object retrieval
from cluttered scenes,” Int. J. Comput. Vis., vol. 89, no. 2/3,
pp. 348–361, 2010.

[55] B. C. Munsell, A. Teml, and S. Wang, “Fast multiple shape corre-
spondence by pre-organizing shape instances,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit., 2009, pp. 840–847.

[56] J. A. Sethian, “Evolution, implementation, and application of level
set and fast marching methods for advancing fronts,” J. Comput.
Physics, vol. 169, no. 2, pp. 503–555, 2001.

[57] G. Peyr�e, “The numerical tours of signal processing-advanced
computational signal and image processing,” IEEE Comput. Sci.
Eng., vol. 13, no. 4, pp. 94–97, Jul./Aug. 2011.

[58] J. L. Bentley, “Multidimensional binary search trees used for asso-
ciative searching,” Commun. ACM, vol. 18, no. 9, 1975, Art. no. 509.

[59] A. Todorov, S. Baron, and N. Oosterhof, “Evaluating face trust-
worthiness: A model based approach,” Social Cognitive Affect. Neu-
roscience, vol. 3, no. 2, pp. 119–127, 2008.

[60] N. N. Oosterhof and A. Todorov, “The functional basis of face
evaluation,” Proc. Nat. Acadmey Sci. United States America, vol. 105,
no. 32, pp. 11087–11092, 2008.

[61] P. Yan and K. Bowyer, “Empirical evaluation of advanced ear bio-
metrics,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Rec-
ognit. Workshops, 2005, pp. 41–41.

[62] P. Yan and K. W. Bowyer, “An automatic 3D ear recognition sys-
tem,” in Proc. 3rd Int. Symp. 3D Data Process. Vis. Transmiss., 2006,
vol. 6, pp. 326–333.

[63] B. C. Munsell, P. Dalal, and S. Wang, “Evaluating shape corre-
spondence for statistical shape analysis: A benchmark study,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 11, pp. 2023–
2039, Nov. 2008.

[64] F. M. Sukno, J. L. Waddington, and P. F. Whelan, “3-D facial land-
mark localization with asymmetry patterns and shape regression
from incomplete local features,” IEEE Trans. Cybern., vol. 45, no. 9,
pp. 1717–1730, Sep. 2015.

[65] S. Z. Gilani, F. Shafait, and A. Mian, “Biologically significant facial
landmarks: How significant are they for gender classification?” in
Proc. Int. Conf. Digit. Image Comput.: Techn. Appl., 2013, pp. 1–8.

[66] P. Szeptycki, M. Ardabilian, and L. Chen, “A coarse-to-fine curva-
ture analysis-based rotation invariant 3D face landmarking,” in
Proc. IEEE 3rd Int. Conf. Biometrics: Theory Appl. Syst., 2009, pp. 1–6.

[67] A. Mian, M. Bennamoun, and R. Owens, “An efficient multimodal
2D-3D hybrid approach to automatic face recognition,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 29, no. 11, pp. 1927–1943,
Nov. 2007.

[68] A. Dehghan, E. Ortiz, R. Villegas, and M. Shah, “Who do I look
like? Determining parent-offspring resemblance via gated
autoencoders,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.
Workshops, 2014, pp. 171–171.

[69] S. Z. Gilani and A. Mian, “Perceptual differences between men
and women: A 3D facial morphometric perspective,” in Proc. 22nd
Int. Conf. Pattern Recognit., 2014, pp. 2413–2418.

GILANI ETAL.: DENSE 3D FACE CORRESPONDENCE 1597

https://au.mathworks.com/matlabcentral/fileexchange/24301-finite-iterative-closest-point
https://au.mathworks.com/matlabcentral/fileexchange/24301-finite-iterative-closest-point


[70] S. Z. Gilani, F. Shafait, and A. Mian, “Gradient based efficient fea-
ture selection,” in Proc. IEEE Winter Conf. Appl. Comput. Vis., 2014,
pp. 191–197.

[71] F. Al-Osaimi, M. Bennamoun, and A. Mian, “An expression defor-
mation approach to non-rigid 3D face recognition,” Int. J. Comput.
Vis., vol. 81, no. 3, pp. 302–316, 2009.

[72] C. Queirolo, L. Silva, O. Bellon, andM. Segundo, “3D face recogni-
tion using simulated annealing and the surface interpenetration
measure,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 2,
pp. 206–219, Feb. 2010.

[73] H. Drira, B. Ben Amor, A. Srivastava, M. Daoudi, and R. Slama,
“3D face recognition under expressions, occlusions, and pose var-
iations,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 9,
pp. 2270–2283, Sep. 2013.

[74] N. Alyuz, B. Gokberk, and L. Akarun, “A 3D face recognition sys-
tem for expression and occlusion invariance,” in Proc. IEEE 2nd
Int. Conf. Biometrics: Theory Appl. Syst., 2008, pp. 1–7.

[75] A. Colombo, C. Cusano, and R. Schettini, “Three-dimensional
occlusion detection and restoration of partially occluded faces,”
J. Math. Imag. Vis., vol. 40, no. 1, pp. 105–119, 2011.

[76] J. D Erico, “Surface fitting using gridfit,” MATLAB Central
File Exchange, 2008, https://au.mathworks.com/matlabcentral/
fileexchange/8998-surface-fitting-using-gridfit

[77] J. Booth, A. Roussos, S. Zafeiriou and D. Dunaway, “A 3D morph-
able model learnt from 10,000 faces,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit., 2016, pp. 5543–5552.

[78] H. Li, R. W. Sumner, and M. Pauly, “Global correspondence opti-
mization for non-rigid registration of depth scans,” Comput.
Graph. Forum, vol. 27, no. 5, pp. 1421–1430, 2008,

[79] H. Li, B. Adams, L. J. Guibas, and M. Pauly, “Robust single-view
geometry and motion reconstruction,” ACM Trans. Graph., vol. 28,
no. 5, 2009, Art. no. 175.

[80] T. Bolkart and S. Wuhrer, “A robust multilinear model learning
framework for 3D faces,” in Proc. IEEE Conf. Comput. Vis. Pattern
Recognit., 2016, pp. 4911–4919.

[81] V. G. Kim, Y. Lipman, and T. Funkhouser, “Blended intrinsic
maps,” ACM Trans. Graph., vol. 30, no. 4, 2011, Art. no. 79.

Syed Zulqarnain Gilani received the MS degree
in EE from the National University of Sciences
and Technology (NUST), Pakistan, in 2009 and
secured the President’s Gold Medal. He received
the PhD degree from the University of Western
Australia. His research interests include 3D facial
morphometrics with applications to syndrome
delineation and machine learning.

Ajmal Mian is an Associate Professor of Com-
puter Science at The University of Western Aus-
tralia. He has received several awards including
the West Australian Early Career Scientist of the
Year Award, the Vice-chancellors Mid-career
Research Award and the Outstanding Young
Investigator Award. He has received two presti-
gious fellowships and seven major grants from
the Australian Research Council and the National
Health and Medical Research Council with total
funding of $3.0 Million. His research interests

include computer vision, machine learning, 3D face analysis, human
action recognition and remote sensing.

Faisal Shafait currently working as an Associate
Professor at School of Electrical Engineering
and Computer Science (SEECS), NUST, Paki-
stan. He is also an Adjunct Senior Lecturer at
the School of Computer Science at The Univer-
sity of Western Australia. Formerly, he was a
Senior Researcher at the German Research
Center for Artificial Intelligence (DFKI), Germany
and a visiting researcher at Google, California.
He received his Ph.D. in computer engineering
from Kaiserslautern University of Technology,

Germany in 2008. His research interests include machine learning and
pattern recognition.

Ian Reid received the DPhil degree from the Uni-
versity of Oxford, in 1991. He is a professor of
computer science with the University of Adelaide.
He has been employed in the Robotics Research
Group, conducting research in computer vision,
including holding an EPSRC Advanced Research
Fellowship (1997-2000), and he has been a Uni-
versity lecturer since 2000. In 2005, he was
awarded the title of reader and in 2010 the title of
professor.

" For more information on this or any other computing topic,
please visit our Digital Library at www.computer.org/publications/dlib.

1598 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 7, JULY 2018

https://au.mathworks.com/matlabcentral/fileexchange/8998-surface-fitting-using-gridfit
https://au.mathworks.com/matlabcentral/fileexchange/8998-surface-fitting-using-gridfit















<<
  /ASCII85EncodePages false
  /AllowTransparency false
  /AutoPositionEPSFiles true
  /AutoRotatePages /None
  /Binding /Left
  /CalGrayProfile (Gray Gamma 2.2)
  /CalRGBProfile (sRGB IEC61966-2.1)
  /CalCMYKProfile (U.S. Web Coated \050SWOP\051 v2)
  /sRGBProfile (sRGB IEC61966-2.1)
  /CannotEmbedFontPolicy /Warning
  /CompatibilityLevel 1.4
  /CompressObjects /Off
  /CompressPages true
  /ConvertImagesToIndexed true
  /PassThroughJPEGImages true
  /CreateJobTicket false
  /DefaultRenderingIntent /Default
  /DetectBlends true
  /DetectCurves 0.0000
  /ColorConversionStrategy /sRGB
  /DoThumbnails true
  /EmbedAllFonts true
  /EmbedOpenType false
  /ParseICCProfilesInComments true
  /EmbedJobOptions true
  /DSCReportingLevel 0
  /EmitDSCWarnings false
  /EndPage -1
  /ImageMemory 1048576
  /LockDistillerParams true
  /MaxSubsetPct 100
  /Optimize true
  /OPM 0
  /ParseDSCComments false
  /ParseDSCCommentsForDocInfo true
  /PreserveCopyPage true
  /PreserveDICMYKValues true
  /PreserveEPSInfo false
  /PreserveFlatness true
  /PreserveHalftoneInfo true
  /PreserveOPIComments false
  /PreserveOverprintSettings true
  /StartPage 1
  /SubsetFonts false
  /TransferFunctionInfo /Remove
  /UCRandBGInfo /Preserve
  /UsePrologue false
  /ColorSettingsFile ()
  /AlwaysEmbed [ true
    /Algerian
    /Arial-Black
    /Arial-BlackItalic
    /Arial-BoldItalicMT
    /Arial-BoldMT
    /Arial-ItalicMT
    /ArialMT
    /ArialNarrow
    /ArialNarrow-Bold
    /ArialNarrow-BoldItalic
    /ArialNarrow-Italic
    /ArialUnicodeMS
    /BaskOldFace
    /Batang
    /Bauhaus93
    /BellMT
    /BellMTBold
    /BellMTItalic
    /BerlinSansFB-Bold
    /BerlinSansFBDemi-Bold
    /BerlinSansFB-Reg
    /BernardMT-Condensed
    /BodoniMTPosterCompressed
    /BookAntiqua
    /BookAntiqua-Bold
    /BookAntiqua-BoldItalic
    /BookAntiqua-Italic
    /BookmanOldStyle
    /BookmanOldStyle-Bold
    /BookmanOldStyle-BoldItalic
    /BookmanOldStyle-Italic
    /BookshelfSymbolSeven
    /BritannicBold
    /Broadway
    /BrushScriptMT
    /CalifornianFB-Bold
    /CalifornianFB-Italic
    /CalifornianFB-Reg
    /Centaur
    /Century
    /CenturyGothic
    /CenturyGothic-Bold
    /CenturyGothic-BoldItalic
    /CenturyGothic-Italic
    /CenturySchoolbook
    /CenturySchoolbook-Bold
    /CenturySchoolbook-BoldItalic
    /CenturySchoolbook-Italic
    /Chiller-Regular
    /ColonnaMT
    /ComicSansMS
    /ComicSansMS-Bold
    /CooperBlack
    /CourierNewPS-BoldItalicMT
    /CourierNewPS-BoldMT
    /CourierNewPS-ItalicMT
    /CourierNewPSMT
    /EstrangeloEdessa
    /FootlightMTLight
    /FreestyleScript-Regular
    /Garamond
    /Garamond-Bold
    /Garamond-Italic
    /Georgia
    /Georgia-Bold
    /Georgia-BoldItalic
    /Georgia-Italic
    /Haettenschweiler
    /HarlowSolid
    /Harrington
    /HighTowerText-Italic
    /HighTowerText-Reg
    /Impact
    /InformalRoman-Regular
    /Jokerman-Regular
    /JuiceITC-Regular
    /KristenITC-Regular
    /KuenstlerScript-Black
    /KuenstlerScript-Medium
    /KuenstlerScript-TwoBold
    /KunstlerScript
    /LatinWide
    /LetterGothicMT
    /LetterGothicMT-Bold
    /LetterGothicMT-BoldOblique
    /LetterGothicMT-Oblique
    /LucidaBright
    /LucidaBright-Demi
    /LucidaBright-DemiItalic
    /LucidaBright-Italic
    /LucidaCalligraphy-Italic
    /LucidaConsole
    /LucidaFax
    /LucidaFax-Demi
    /LucidaFax-DemiItalic
    /LucidaFax-Italic
    /LucidaHandwriting-Italic
    /LucidaSansUnicode
    /Magneto-Bold
    /MaturaMTScriptCapitals
    /MediciScriptLTStd
    /MicrosoftSansSerif
    /Mistral
    /Modern-Regular
    /MonotypeCorsiva
    /MS-Mincho
    /MSReferenceSansSerif
    /MSReferenceSpecialty
    /NiagaraEngraved-Reg
    /NiagaraSolid-Reg
    /NuptialScript
    /OldEnglishTextMT
    /Onyx
    /PalatinoLinotype-Bold
    /PalatinoLinotype-BoldItalic
    /PalatinoLinotype-Italic
    /PalatinoLinotype-Roman
    /Parchment-Regular
    /Playbill
    /PMingLiU
    /PoorRichard-Regular
    /Ravie
    /ShowcardGothic-Reg
    /SimSun
    /SnapITC-Regular
    /Stencil
    /SymbolMT
    /Tahoma
    /Tahoma-Bold
    /TempusSansITC
    /TimesNewRomanMT-ExtraBold
    /TimesNewRomanMTStd
    /TimesNewRomanMTStd-Bold
    /TimesNewRomanMTStd-BoldCond
    /TimesNewRomanMTStd-BoldIt
    /TimesNewRomanMTStd-Cond
    /TimesNewRomanMTStd-CondIt
    /TimesNewRomanMTStd-Italic
    /TimesNewRomanPS-BoldItalicMT
    /TimesNewRomanPS-BoldMT
    /TimesNewRomanPS-ItalicMT
    /TimesNewRomanPSMT
    /Times-Roman
    /Trebuchet-BoldItalic
    /TrebuchetMS
    /TrebuchetMS-Bold
    /TrebuchetMS-Italic
    /Verdana
    /Verdana-Bold
    /Verdana-BoldItalic
    /Verdana-Italic
    /VinerHandITC
    /Vivaldii
    /VladimirScript
    /Webdings
    /Wingdings2
    /Wingdings3
    /Wingdings-Regular
    /ZapfChanceryStd-Demi
    /ZWAdobeF
  ]
  /NeverEmbed [ true
  ]
  /AntiAliasColorImages false
  /CropColorImages true
  /ColorImageMinResolution 150
  /ColorImageMinResolutionPolicy /OK
  /DownsampleColorImages true
  /ColorImageDownsampleType /Bicubic
  /ColorImageResolution 150
  /ColorImageDepth -1
  /ColorImageMinDownsampleDepth 1
  /ColorImageDownsampleThreshold 1.50000
  /EncodeColorImages true
  /ColorImageFilter /DCTEncode
  /AutoFilterColorImages false
  /ColorImageAutoFilterStrategy /JPEG
  /ColorACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /ColorImageDict <<
    /QFactor 0.40
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /JPEG2000ColorACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000ColorImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasGrayImages false
  /CropGrayImages true
  /GrayImageMinResolution 150
  /GrayImageMinResolutionPolicy /OK
  /DownsampleGrayImages true
  /GrayImageDownsampleType /Bicubic
  /GrayImageResolution 300
  /GrayImageDepth -1
  /GrayImageMinDownsampleDepth 2
  /GrayImageDownsampleThreshold 1.50000
  /EncodeGrayImages true
  /GrayImageFilter /DCTEncode
  /AutoFilterGrayImages false
  /GrayImageAutoFilterStrategy /JPEG
  /GrayACSImageDict <<
    /QFactor 0.76
    /HSamples [2 1 1 2] /VSamples [2 1 1 2]
  >>
  /GrayImageDict <<
    /QFactor 0.40
    /HSamples [1 1 1 1] /VSamples [1 1 1 1]
  >>
  /JPEG2000GrayACSImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /JPEG2000GrayImageDict <<
    /TileWidth 256
    /TileHeight 256
    /Quality 15
  >>
  /AntiAliasMonoImages false
  /CropMonoImages true
  /MonoImageMinResolution 1200
  /MonoImageMinResolutionPolicy /OK
  /DownsampleMonoImages true
  /MonoImageDownsampleType /Bicubic
  /MonoImageResolution 600
  /MonoImageDepth -1
  /MonoImageDownsampleThreshold 1.50000
  /EncodeMonoImages true
  /MonoImageFilter /CCITTFaxEncode
  /MonoImageDict <<
    /K -1
  >>
  /AllowPSXObjects false
  /CheckCompliance [
    /None
  ]
  /PDFX1aCheck false
  /PDFX3Check false
  /PDFXCompliantPDFOnly false
  /PDFXNoTrimBoxError true
  /PDFXTrimBoxToMediaBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXSetBleedBoxToMediaBox true
  /PDFXBleedBoxToTrimBoxOffset [
    0.00000
    0.00000
    0.00000
    0.00000
  ]
  /PDFXOutputIntentProfile (None)
  /PDFXOutputConditionIdentifier ()
  /PDFXOutputCondition ()
  /PDFXRegistryName ()
  /PDFXTrapped /False

  /CreateJDFFile false
  /Description <<
    /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>
    /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200035002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>
    /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200035002e00300020006f00670020006e0079006500720065002e>
    /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200035002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>
    /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200035002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>
    /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200035002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e>
    /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 5.0 e versioni successive.)
    /JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200035002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002>
    /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200035002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>
    /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 5.0 en hoger.)
    /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200035002e003000200065006c006c00650072002e>
    /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200035002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>
    /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200035002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>
    /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200035002e00300020006f00630068002000730065006e006100720065002e>
    /ENU (Use these settings to create PDFs that match the "Suggested"  settings for PDF Specification 4.0)
  >>
>> setdistillerparams
<<
  /HWResolution [600 600]
  /PageSize [612.000 792.000]
>> setpagedevice

