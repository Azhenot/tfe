





































66 1541-1672/16/$33.00 © 2016 IEEE Ieee INteLLIGeNt SYStemS
Published by the IEEE Computer Society

E X P E R T  O P I N I O N
Editor: Daniel Zeng, University of Arizona and Chinese Academy of Sciences, zengdaniel@gmail.com

Data Science: Nature 
and Pitfalls

Longbing Cao, University of Technology, Sydney

and innovation. This has resulted in signifi cant op-
portunities and prospects becoming available, and 
an overwhelming amount of fanfare has spread 
across domains, areas, and events.

A review of the related initiatives, progress, and 
status of data science, analytics, and big data4 and 
the diversifi ed discussions about the prospects, 
challenges, and directions5 makes clear the contro-
versy caused by the potential confl ict of these vari-
ous elements. There is a need for deep discussions 
about the nature and pitfalls of data science, clari-
fi cation of fundamental concepts and myths, and a 
demonstration of the intrinsic characteristics and 
opportunities of data science.

Thus, this article focuses on two fundamental 
issues—the nature and pitfalls of data science. I  
highlight the status, intrinsic factors, characteris-
tics, and features of the era of data science and ana-
lytics, as well as the challenges and opportunities in 
innovation, research, and disciplinary development. 
I also summarize common pitfalls about the concepts 
of data science, data volume, infrastructure, ana-
lytics, and capabilities and roles. Building on these 
discussions, I then present the concepts and possi-
ble future directions of data science.

Features of the Data Science Era
Identifying the features and characteristics of the 
data science era is critical and challenging. Let’s ex-
plore this from the perspective of the transformation 

and paradigm shift caused by data science and dis-
cuss the core driving forces and the status of several 
typical issues confronting the data science fi eld.

Transformation and Paradigm Shift
The emergence of the era of data science and ana-
lytics can be highlighted by three key indicators:

•	 a disciplinary paradigm shift, or the shifting of 
data-centric disciplinary paradigms from one to 
another;

•	 technological transformation, or the upgrading 
of data technology from one generation to an-
other; and

•	 innovative production, or the innovation of tech-
nical and practical data products.

We can defi ne the disciplinary paradigm shift of 
data-oriented and data-centric research, innova-
tion, and professions as moving from data analysis 
to data analytics, from descriptive analytics to 
deep analytics, and from data analytics to data 
science. The disciplinary paradigm shift promotes 
data-related technological transformation from 
large-scale data to big data, from business opera-
tional systems to business analytical systems, from 
the World Wide Web to the Wisdom Web, and 
from the Internet to the Internet of Everything (in-
cluding mobile and social networks and the Inter-
net of Things).

Innovative production in data and analytics can 
be represented by typical indicators—for exam-
ple, from a digital to a data economy, from closed 
to open government, from e-commerce to online 
business, from landlines to smartphones, and from 

The era of analytics,1 data science,2 and big data3 has driven substantial governmental, in-
dustrial, and disciplinary interest; goal and strategy 

transformation; and a paradigm shift in research



September/october 2016 www.computer.org/intelligent 67

the Internet to mobile and social 
networks.

Data-Centric Driving Forces
The transformation and paradigm  
shift of data-oriented discipline, tech-
nologies, and production are dri-
ven by core forces, including data-
enabled opportunities, data-related 
ubiquitous factors, and various com-
plexities and intelligences embed-
ded in data-oriented production and 
products.

Ubiquitous data-oriented factors in-
clude the following:

•	 data, which involves historical, real-
time, and future data;

•	 behavior, which bridges the gaps 
between the physical and data 
worlds;

•	 complexity, which differentiates 
one data system from another;

•	 intelligence, which is embedded in a 
data system;

•	 service, which is present in various 
forms and domains; and

•	 opportunities, because data enables 
enormous opportunities.

Data-enabled opportunities, also 
called X-opportunities, are overwhelm-
ing. They extend from research, inno-
vation, education, government, and the 
economy, and can include opportunities 
in the following areas:

•	 research, such as inventing data-
focused breakthrough theories and 
technologies;

•	 innovation, such as developing cut-
ting-edge data-based services, sys-
tems, and tools;

•	 education, such as innovating data-
oriented courses and training;

•	 government, such as enabling data-
driven government decision mak-
ing and objectives;

•	 economics, such as fostering data econ-
omy, services, and industrialization;

•	 lifestyle, such as promoting data-
enabled smarter living and smarter 
cities; and

•	 entertainment, such as creating 
data-driven entertainment activi-
ties, networks, and societies.

A data science problem is a complex 
system6,7 in which comprehensive sys-
tem complexities, also called X-
complexities,5 are embedded. These 
comprise complexities in the follow-
ing areas:

•	 data, including comprehensive data 
circumstances and characteristics;

•	 behavior, including individual and 
group activities, evolution, utility, 
impact, and change;

•	 the domain, including domain factors,  
processes, norms, policies, knowl-
edge, and domain expert engage-
ment in problem solving;

•	 social complexity, including social 
networking, community formation 
and divergence, sentiment, the dis-
semination of opinion and influ-
ence, and other social issues such 
as trust and security;

•	 the environment, including contex-
tual factors, interactions with sys-
tems, changes, and uncertainty;

•	 learning, including the development 
of appropriate methodologies, frame-
works, processes, models and algo-
rithms, and theoretical foundations 
and explanations; and

•	 decision making, including the meth-
ods and forms of deliverables, com-
munications, and decision-making 
actions.

In a complex data science problem, 
ubiquitous intelligence, also called X-
intelligence,5 is often demonstrated and 
must be incorporated and synergized7 in 
problem-solving processes and systems:

•	Data intelligence highlights the in-
teresting information, insights, and 

stories hidden in data about busi-
ness problems and driving forces.

•	 Behavioral intelligence demonstrates 
the insights of activities, processes, 
dynamics, impact, and trust of in-
dividual and group behaviors by hu-
mans and action-oriented organisms.

•	Domain intelligence includes do-
main values and insights that emerge 
from involving domain factors, 
knowledge, metaknowledge, and other 
domain-specific resources.

•	Human intelligence includes con-
tributions made by the empirical 
knowledge, beliefs, intentions, ex-
pectations, critical thinking, and 
imaginary thinking of human indi-
vidual and group actors.

•	Network intelligence results from 
the involvement of networks, the 
Web, and networking mechanisms 
in problem comprehension and prob-
lem solving.

•	Organizational intelligence includes 
insights and contributions created 
by the involvement of organization-
oriented factors, resources, com-
petency and capabilities, maturity, 
evaluation, and dynamics.

•	 Social intelligence includes contribu-
tions and values generated by the in-
clusion of social, cultural, and eco-
nomic factors, norms, and regulation.

•	 Environmental intelligence can be 
embodied through other intelligences 
specific to the underlying domain, 
organization, society, and actors.

All of these data-oriented and data-
driven factors, complexities, intelli-
gences, and opportunities constitute 
the nature and characteristics of data 
science and drive the evolution and 
dynamics of data science problems.

Data DNA
As a result of data quantification, data 
is everywhere, including the Internet; 
the IoT; sensor networks; sociocul-
tural, economic, and geographical  



68  www.computer.org/intelligent Ieee INteLLIGeNt SYStemS

repositories; and quantified personal-
ized sensors, including mobile, social, 
living, entertaining, and emotional 
sources. This forms the “datalogical” 
constituent, data DNA, which plays 
a critical role in data organisms and 
performs a similar function to biolog-
ical DNA in living organisms.

Definition 1. Data DNA is the 
datalogical “molecule” of data, 
consisting of fundamental and  
generic constituents: entity (E), 
property (P), and relationship (R). 
Here, “datalogical” means that 
data DNA plays a similar role in 
data organisms as biological DNA 
plays in living organisms. Entity 
can be an object, instance, human, 
organiza tion, system, or part of a 
subsystem. Property refers to the 
attributes that describe an entity. 
Relationship corresponds to entity 
interactions and property inter-
actions, including property value 
interactions.

Entity, property, and relationship 
present different characteristics in terms  
of quantity, type, hierarchy, structure,  
distribution, and organization. A data- 
intensive application or system often  
comprises many diverse entities, each 
of which has specific properties, and 
different relationships are embed-
ded within and between properties 
and entities. From the lowest to the 
highest levels, data DNA presents 
heterogeneity and hierarchical cou-
plings across levels. On each level, it 
maintains consistency (inheritance of 
properties and relationships) as well 
as variations (mutations) across en-
tities, properties, and relationships, 
while supporting personalized char-
acteristics for each individual entity, 
property, and relationship.

For a given data, its entities, proper-
ties, and relationships are instantiated 
into diverse and domain-specific forms, 

which carry most of the data’s ecologi-
cal and genetic information in data 
generation, development, functioning, 
reproduction, and evolution. In the 
data world, data DNA is embedded in 
the whole body of personal8 and non-
personal data organisms, and in the 
generation, development, functioning, 
management, analysis, and use of all 
data-based applications and systems.

Data DNA drives the evolution of a 
data-intensive organism. For example, 
university data DNA connects the data 
of students, lecturers, administrative 
systems, corporate services, and oper-
ations. The student data DNA further 
consists of academic, pathway, library 
access, online access, social media, 
mobile service, GPS, and Wi-Fi usage 
data. Such student data DNA is both 
fixed and evolving.

In complex data, data DNA is em-
bedded within various X-complexi-
ties and ubiquitous X-intelligence in 
a data organism.5,7 This makes data 
rich in content, characteristics, se-
mantics, and value, but challenging 
in acquisition, preparation, presenta-
tion, analysis, and interpretation.

Data Quality
Data science tasks involve roles and 
follow processes that differ from more 
generalized IT projects, because data 
science and analytics works tend to be 
creative, intelligent, exploratory, non-
standard, unautomated, and person-
alized, and they have the objective of 
discovering evidence and indicators 
for decision-making actions. They in-
evitably involve quality issues such as 
data validity, veracity, variability, and 
reliability, and social issues such as 
privacy, security, accountability, and 
trust, which must be considered in 
data science and analytics.

Data quality is a critical problem in 
data science and engineering. Given a 
data science problem, we should not as-
sume that the data available or given is 

perfect, the data always generates good 
outcomes, the outputs (findings) gener-
ated are always good and meaningful, 
or the outcomes can always inform bet-
ter decisions. These assumption myths 
involve the quality of the data (input), 
the model, and the outcomes (output)—
in particular, validity, veracity, variabil-
ity, and reliability.

Data and analytics validity deter-
mines whether a data model, concept, 
conclusion, or measurement is well-
founded and corresponds accurately to 
the data characteristics and real-world 
facts, making it capable of giving the 
right answer. Similarly, data and ana-
lytics veracity determines the correct-
ness and accuracy of data and analyt-
ics outcomes. Both validity and verac-
ity must be checked from the perspec-
tives of data content, representation, 
design, modeling, experiments, and 
evaluation.

Data and analytics variability is de-
termined by the changing and uncer-
tain nature of data, reflecting business 
dynamics (including the problem con-
text and problem-solving purposes), 
and thus requires the corresponding 
analytics to adapt to the data’s dynamic 
nature. Because of the changing nature 
of data, the need to check the valid-
ity, veracity, and reliability data used 
and analytics undertaken is very im-
portant. Data and analytics reliability 
refers to the consistency, redundancy, 
repeatability, and trust properties  
of the data used, the analytic models 
generated, and the outcomes delivered 
on the data. Reliable data and analyt-
ics are not necessarily static. Making 
data analytics adaptive to the evolv-
ing, streaming, and dynamic nature of 
data, business, and decision requests is 
a critical challenge in data science and 
analytics.

Social Issues
Domain-specific data and business 
are embedded in social contexts and 



September/october 2016 www.computer.org/intelligent 69

incorporated with social issues. Data 
science tasks typically involve such 
social issues as privacy, security, ac-
countability, and trust in data, mod-
eling, and deliverables.

Data and analytics privacy addresses 
the challenge of collecting, analyzing, 
disseminating, and sharing data and 
analytics while protecting personally 
identifiable or other sensitive infor-
mation and analytics from improper 
disclosure. Protection technology, 
regulation, and policies are required 
to balance protection and appropri-
ate disclosure in the process of data 
manipulation.

Data and analytics security pro-
tects target objects from both de-
structive forces and unauthorized us-
ers’ actions, such as improper use or 
disclosure. It addresses not only pri-
vacy issues but other aspects beyond 
privacy, such as software and hard-
ware backup and recovery. Data and 
analytics security also involves the 
development of regulating political or 
legal mechanisms and systems to ad-
dress such issues.

Data and analytics accountability 
refers to an obligation to comply with 
data privacy and security legislation 
and to report, explain, trace, and 
identify the data manipulated and 
analytics conducted to maintain the 
transparency, traceability, liability, 
and warranty of both the measure-
ment and results, as well as the effi-
cacy and verifiability of the analytics 
and protection.

Data and analytics trust refers 
to the belief in the reliability, truth, 
or ability of data and analytics to 
achieve relevant goals. This involves 
the development of appropriate tech-
nology, social norms, ethical rules, or 
legislation to ensure, measure, and 
protect trust in the data and analyt-
ics used and confidence in the corre-
sponding outcomes and evaluation of 
analytics.

The Extreme Challenge
Different types and levels of ana-
lytical problems trouble the existing 
knowledge base, and we are espe-
cially challenged by the problems in 
complex data and environments. Our 
focus on data science research and in-
novation concerns what we call an ex-
treme data challenge in data science 
and analytics. The extreme data chal-
lenge illustrated in Figure 1 seeks to 
discover and deliver complex knowl-
edge in complex data, taking into 
account complex behavior within a 
complex environment to achieve ac-
tionable insights that will inform and 
enable decision action-taking in com-
plex business problems that cannot 
be better handled by other methods.

The critical future directions of 
data science research and innovation 
in this case focus on the following:

•	 complex data with complex charac- 
teristics5,7;

•	 complex behaviors with complex 
relationships and dynamics5,7;

•	 complex environments in which 
complex data and behaviors are 
embedded and interacted with5,7;

•	 complex models to address the 
data and behavior complexities in a 
complex environment;

•	 complex findings to uncover hid-
den but technically interesting and 
business-friendly observations, in-
dicators or evidence, statements, or 
presentations; and

•	 actionable insights to demonstrate 
the next best or worst situation and 

inform the optimal strategies to 
support effective business decision 
making.9,10

Many real-life problems fall into this 
level of complexities and challenges, as 
the extreme data challenge shows, and 
they have not been addressed well. One 
example is understanding group be-
haviors by multiple actors when there 
are complex interactions and relation-
ships, such as in the manipulation 
of large-scale cross-capital markets 
pool by internationally collaborative  
investors,11 each of whom plays a 
role by connecting information from 
the underlying markets, social media, 
other financial markets, socioeconomic 
data, and policies.12 Another exam-
ple would be to predict local climate  
change and effect by connecting lo-
cal, regional, and global climate, geo-
graphical, and agricultural data and 
other information.13

Disciplinary Development of 
Data Science
I present a status summary of the 
disciplinary development of data 
science by reviewing the developmental 
gaps between the data’s potential 
and the state-of-the-art capabilities 
to fulfill such potential, the research 
map of data science, and the course 
framework of data science.

Data-to-Capability Development 
Gaps
The rapid increase in big data has led 
to significant gaps between what is in 

Figure 1. The extreme data challenge.

Complex
data 

Complex
behavior

Complex
models

Complex
findings

Actionable
insights

Complex environment/infrastructure



70  www.computer.org/intelligent Ieee INteLLIGeNt SYStemS

the data and how much we can under-
stand.9,10 Figure 2 shows empirically  
the data development gaps between 
the growth of data potentials and the 
state-of-the-art capabilities. Such gaps 
have increased in the past 10 years,  
especially recently, owing to the imbal-
ance between potential exponential  
increase and progressive state-of-the-
art capability development. Examples 
of such gaps could include the gaps 
between

•	 data availability and the currently 
understandable data level, scale, and  
degree;

•	 data complexities and the currently 
available analytics theories and tools;

•	 data complexities and the currently 
available technical capabilities;

•	possible values and impact and  
currently achievable outcomes and 
benefits;

•	 organizational needs and the cur-
rently available talent (that is, data 
scientists); and

•	 potential opportunities and the cur-
rent outcomes and benefits achievable.

Such growth gaps are driven by 
critical challenges for which there is 

a shortage of effective theories and 
tools. For example, a typical challenge  
in complex data concerns intrinsic  
complex coupling relationships and 
heterogeneity, forming data that is 
not independent and identically dis-
tributed (IID),12 which cannot be 
simplified in such a way that it can 
be handled by classic IID learning 
theories and systems. Other exam-
ples  include the real-time learning of 
large-scale online data, such as learn-
ing shopping manipulation and mak-
ing real-time recommendations on 
high-frequency data in the “11-11”  
shopping seasons launched by Alibaba,  
or identifying suspects in an imbal-
anced and multisource data and en-
vironment such as fraud detection in 
high-frequency market trading. Other 
challenges are high invisibility, high 
frequency, high uncertainty, high di-
mensionality, the dynamic nature, 
mixed sources, online learning at the 
Web scale, and the development of 
human-like thinking.

Data Science Research Map
The way to explore the fundamental 
challenges and innovative opportuni-
ties facing big data and data science is 

to conduct problem-, data-, and goal-
driven discovery:

•	 Problem-driven discovery requires 
understanding the problem’s intrinsic  
nature, characteristics, complexities,  
and boundaries and then analyz-
ing the gaps between the problem  
complexities and the existing capa-
bility set. This gap analysis is critical  
for original research and break-
throughs in scientific discovery.

•	Goal-driven discovery requires un-
derstanding the business, technical, 
and decision goals to be achieved by 
understanding the problem and then 
conducting gap analysis of what has 
been implemented and achieved and 
what is expected to be achieved.

•	Data-driven discovery requires un-
derstanding the data characteristics, 
complexities, and challenges and the 
gaps between the nature of a prob-
lem and the data capabilities. Be-
cause of the limitations of existing 
data systems, projection from the 
underlying physical world where the 
problem sits to the data world where 
the problem is “datafied” can be bi-
ased, dishonest, or manipulated. As 
a result, the data does not completely 
capture the problem and thus cannot 
create a full picture of it through any 
type of data exploration.

There are two ways to explore major 
research challenges: to summarize what 
concerns the relevant communities,  
and to scrutinize the potential issues 
arising from the intrinsic complexities 
and nature of data science problems 
as complex systems.7 With the first ap-
proach, we can grasp the main research 
challenges by summarizing the main 
topics and issues in the statistics com-
munities,14 informatics and computing 
communities,5,15 vendors,16 government 
initiatives,17,18 and research institu-
tions19,20 that focus on data science and 
analytics. The second approach is much 

Figure 2. Critical development gaps between data potential and state-of-the-art 
capabilities.

Potential:
- Data availability
- Data complexities
- Possible values and impact
- Organizational needs
- Potential opportunities
...

State of the art:
- Currently understandable level/degree
- Current analytics theories and tools
- Current technical capabilities
- Current talents/data scientists
- Current outcomes and benefits
...

In
di

ca
to

rs

Time

Critical challenges: 
- Couplings
- Heterogeneity
- Real time
- High invisibility
- High frequency
- High uncertainty
- High dimensionality
- Dynamic nature
- Mixed sources
- Web scale
- Human-like thinking
...

Objectives and possibilities:
- We do not know what we do not

know
- Blind people can recognize an

elephant
- Do the right thing in the right form at

the right time for the right people
- Make the impossible possible



September/october 2016 www.computer.org/intelligent 71

more challenging, because we must ex-
plore the unknown space of the com-
plexities and comprehensive intelligence 
in complex data problems.

We list some of the data science com-
munity’s main challenges in addressing 
the big data complexities represented 
by key topics in the Data A–Z diction-
ary. We categorize these as challenges 
in data/business understanding; math-
ematical and statistical foundations;  
X-analytics and data/knowledge engi-
neering; data quality and social issues; 
data value, impact, and usability; and 
data to decision and actions.

X-analytics and data/knowledge 
engineering encompass many specific 
research issues that have not been ad-
dressed properly. These include behav-
ior and event processing; data storage 
and management systems; data quality 
enhancement; data modeling, learning, 
and mining; deep analytics, learning,  
and discovery; simulation and experi-
mental design; high-performance pro-
cessing and analytics; analytics and 
computing architectures and infra-
structure; and networking, communi-
cation, and interoperation.

Data Science Course Framework
Data science and analytics education 
aims to train and generate the data and 
analytics knowledge and proficiency 
required to manage the capability and  
capacity gaps in the creation of a data 
science profession21,22 and to achieve 
the goals of data science innovation 
and the data economy. Accordingly, 
different levels of education and train-
ing are necessary, from attending public 
courses, corporate training, and under-
graduate courses to joining a master’s 
or PhD program in data science.

Public courses are designed for the 
general community to lift their under-
standing, skills, profession, and spe-
cialism in data science through multi-
level short courses. They range from 
basic courses to intermediate and ad-

vanced courses. The knowledge map 
consists of such components as data sci-
ence, data mining, machine learning, 
statistics, data management, comput-
ing, programming, system analysis and  
design, and modules related to case 
studies, hands-on practices, project 
management, communication, and de-
cision support.

Corporate training and workshops 
are customized to upgrade and foster 
corporate thinking, knowledge, capa-
bility, and practices for the entire enter-
prise’s innovation and productivity. 
This involves offering courses and 
workshops for the workforce, from se-
nior corporate executives to business 
owners, business analysts, data model-
ers, data scientists, data engineers, and 
deployment and enterprise strategists. 
Such courses cover topics such as data 
science, data engineering, analytics  
science, decision science, data and  
analytics software engineering, project 
management, communications, and case  
management.

Undergraduate courses can be offered 
on either a general data science basis, fo-
cusing on building data science founda-
tions and data and analytics computing,  
or specific areas such as data engineer-
ing, predictive modeling, and visualiza-
tion. Double degrees or majors might 
be offered to train professionals who 
will gain knowledge and abilities across 
disciplines such as business and analyt-
ics or statistics and computing.

A master of data science and ana-
lytics program aims to train special-
ists and foster the talent of those who 
can conduct a deep understanding of 
data and undertake analytics tasks in 
data mining, knowledge discovery, 
and machine learning-based advanced  
analytics. Interdisciplinary experts can 
be trained from those who have a solid 
foundation in statistics, business, so-
cial science, or other specific disciplines 
and can integrate data-driven explo-
ration technologies with disciplinary 

expertise and techniques. A critical 
area in which data science and analyt-
ics should be incorporated is in MBA 
courses. This is where the next genera-
tion of business leaders can be trained 
for the new economy and a global view 
of economic growth.

A PhD in data science and analytics 
program aims to train high-level tal-
ent and specialists who have indepen-
dent thinking, leadership, research, 
innovation, and better practices for 
theoretical innovation to manage the 
significant knowledge and capability 
gaps, and for substantial economic 
innovation and raising productivity. 
Interdisciplinary research is encour-
aged to train leaders who have a sys-
tematic and strategic understanding 
of the what, how, and why of data 
and economic innovation.

Figure 3 shows the level, objective, 
capability set, and outcomes of hier-
archical data science and analytics 
education and training.

Data Science as a  
New Science
So, what makes data science a new 
science? To address this question, we 
discuss Data A–Z, which can be used 
to capture every aspect of data science 
to form a data science ontological sys-
tem built on discussions about the fea-
tures, disciplinary development, and 
future of data science.

Data A–Z
The big data community often uses mul-
tiple “V”s to describe the characteris-
tics, challenges, and opportunities of big  
data. These include volume (size), veloc-
ity (speed), variety (diversity), veracity 
(quality and trust), value (insight), visual-
ization, and variability (formality).

In fact, these terms cannot com-
pletely describe big data or the field of 
data science. Therefore, it is valuable 
to build a Data A–Z dictionary to cap-
ture the intrinsic comprehensive but 



72  www.computer.org/intelligent Ieee INteLLIGeNt SYStemS

diverse aspects, characteristics, chal-
lenges, domains, tasks, processes, pur-
poses, applications, and outcomes of 
data. Figure 4 lists a sample sequence 
of data science keywords.

It is notable that such a Data A–Z 
ontology probably covers most of the 
topics of interest to major data science 
communities. The exercise of con-
structing Data A–Z can substantially 
deepen and broaden the understand-
ing of intrinsic data characteristics, 
complexities, challenges, prospects, 
and opportunities.23

What Is Data Science?
Generally speaking, data science is the 
science (or study) of data. We can define 
data science as being object focused, 
process based, or discipline oriented.5

Definition 2. From the process perspec-
tive, data science is a systematic ap-

proach to “thinking with wisdom,” 
“understanding domain,” “manag-
ing data,” “computing with data,” 
“mining on knowledge,” “commu-
nicating with stakeholders,” “delivering 
products,” and “acting on insights.”

In contrast, data analytics under-
stands data and its underlying busi-
ness, discovers knowledge, delivers 
actionable insights, and enables deci-
sion making. From this perspective, 
we can say that analytics is a key-
stone of data science.

From the disciplinary perspective, 
data science is a new interdisciplinary 
field in which to study data and its do-
main in terms of a data-to-knowledge-
to-wisdom thinking for generating  
data products.5 Data science integrates 
traditionally data-oriented disciplines 
such as statistics, informatics, and 
computing with traditionally data- 

independent fields such as communica-
tion, management, and sociology.

The Future of Data Science
It is difficult at this early stage of data sci-
ence to predict specific future data sci-
ence innovation and research; thus, 
next-generation data science will need 
to address the unknown space that is in-
visible to existing science and create new 
data products. We will need to deepen 
our understanding of data invisibility  
(that is, invisible data characteristics)  
in the hidden and blind spaces, under-
stand their X-complexities and X-intelli-
gences, and strengthen our capabilities.5 
We will need to invent data representa-
tion capabilities, including designs, struc-
tures, schemas, and algorithms, to make 
invisible data more visible and explicit. 
Another task will be creating analytical 
and learning capabilities, including origi-
nal theories, algorithms, and models, to 
disclose the unknown knowledge in un-
known spaces.5 New intelligent systems 
and services will need to be built, includ-
ing corporate and Internet-based collab-
orative platforms and services, to support 
collaborative and collective exploration 
of invisible and unknown challenges in 
fully unknown spaces.5 Finally, we will 
need to train a generation of qualified 
data science professionals in data literacy, 
thinking, competency, consciousness, 
and cognitive intelligence to work on this 
data science agenda.

Data Science Pitfalls
At this early stage, it is typical to see 
different and sometimes contradictory 
views appear in various communities. 
It is essential to share and discuss the  
myths and reality,24 memes,25 and pit-
falls to ensure the healthy  development of  
the field. From observations about the 
relevant communities, as well as expe-
riences and  lessons learned in conduct-
ing data science and analytics research, 
education, and services, several myths 
and pitfalls have emerged.

Public courses

Corporate training

Bachelor of DSA

Master of DSA

PhD of DSA

Individual know-what

Enterprise know-what

Know what and how

Know why

Invent new

Engineering
practices  

Enterprise engineering
practices

Technology
engineering

Research
specialist

Innovation
leadership

Technical skills
Practices

Technical skills
Practices

Management
Communication

Theoretical
foundation

Technical skills
Communication

Theoretical foundation
Technical skills
Communication

Management

Theoretical
breakthroughs

Level OutcomeObjective Capability

Figure 3. Data science course framework.

Figure 4. Sample sequence of data science keywords from the Data A–Z dictionary.

Actionability/Adaptation; Behavior/Boosting; Causality/Change; Dimensionality/Divergence;
Embedding/Ethics; Fusion/Forecasting; Governance/Generalization; Heterogeneity/Hashing;
Integrity/Inference; Join/Jungle; Kernelization/Knowledge; Linkage/Learning; Metrology/Migration;
Normalization/Novelty; Optimization/Outlier; Privacy/Provenance; Quality/Quantity;
Relation/Regularization; Scalability/Sparsity; Transformation/Transfer; Utility/Uncertainty;
Variation/Visibility; Wrangling/Weighting; X-analytics/X-informatics; Yield; Zettabyte.



September/october 2016 www.computer.org/intelligent 73

Pitfalls about Data Science 
Concepts
Typically, data science has been defined 
in terms of specific disciplinary founda-
tions, principles, goals, inputs, algo-
rithms and models, processes, tools,  
outputs, applications, and professions. 
Often, a fragmented statement can cause 
debate and result in the phenomenon of 
“How does a blind person recognize an 
elephant?” Here, we discuss some com-
mon arguments and observations.

Data science is statistics,26,27 so one 
might argue, “Why do we need data 
science when we’ve had statistics for 
centuries?”28 or “How does data sci-
ence really differ from statistics?”25 
Data science provides systematic, holis-
tic, and multidisciplinary solutions for 
learning explicit and implicit insights 
and intelligence from complex and 
large-scale data and generates evidence 
or indicators from data by undertaking 
diagnostic, descriptive, predictive, and 
prescriptive analytics, in addition to 
supporting other tasks on data, such as 
computing and management

Others might ask, “Why do we need 
data science when information science 
and data engineering have been explored 
for many years?” But consider the issues 
faced in related areas by the enormity 
of the task and the parallel example of 
enabling a blind person to recognize an 
animal as large as an elephant. Informa-
tion science and data engineering alone 
cannot achieve this. Other aspects may 
be learned from the discussion about 
greater or fewer statistics.14

Another object ion could come 
from those who have been doing data 
analysis for decades and who believe 
that data science has nothing new to 
offer them. We would counter that 
classic data analysis and technologies 
focus mostly on explicit observation 
analysis and hypothesis testing on 
small and simpler data.

Others might wonder whether data 
science is old wine in a new bottle, or 

what new grand challenges are fore-
grounded by data science. Analysis of 
the gaps between existing developments 
and data science’s potential (see Figure 2)  
shows that many opportunities exist to 
fill the theoretical gaps when data com-
plexities extend significantly beyond 
the level that can be handled by state-
of-the-art theories and systems. For ex-
ample, classic statistical and analytical 
theories and systems were not designed 
to handle non-IIDness12 in complex 
real-life systems.

It is also worth noting attention that 
the terms big data, data science, and 
advanced analytics are often overly 
used or improperly used. Most Google 
searches on these keywords return re-
sults that are irrelevant to their intrinsic 
semantics and scope, or simply repeat 
familiar arguments about the needs of 
data science and existing phenomena. 
In many such findings, big data is de-
scribed as being simple, data science 
has nothing to do with the science of 
data, and advanced analytics is the 
same as classic data analysis and infor-
mation processing. There is a lack of 
deep thinking and exploration of why, 
what, and how these new terms should 
be defined, developed, and applied.

These observations illustrate that 
data science is still young. They also 
justify the urgent need to develop sound 
terminology, standards, a code of con-
duct, statement and definitions, theo-
retical frameworks, and better practices 
that will exemplify typical data science 
professional practices and profiles.

Data Volume Pitfalls
Various pitfalls surround data volume. 
For example, what makes data “big”? It is 
usually not the volume but the complexi-
ties5,7 and large values that make data big. 
Why is the bigness of data important? 
The bigness (referring to data science com-
plexities) of data heralds new opportuni-
ties for theoretical, technological, practi-
cal, and economic developments.

Some assume that big data refers to 
massive volumes of data, but actually, 
it refers mainly to significant data 
complexities. From the volume per-
spective, a dataset is big when the size 
of the data itself becomes a quintes-
sential part of the problem.

One might argue, “I do not have 
big data, so I cannot do big data re-
search.” Most researchers and practi-
tioners do not have sizeable amounts 
of data and do not have access to big 
infrastructure, either. However, sig-
nificant research opportunities still 
exist to create fundamentally new the-
ories and tools to address respective 
X-complexities and X-intelligence.

Another idea we see is to collect data 
from all sources in order to conduct big 
data analytics. Actually, only relevant 
data is required to achieve a specific an-
alytical goal. A related pitfall is the idea 
that it is better to have too much data 
than too little. Although more data 
generally tends to present more op-
portunities, the data amount needs to 
be relevant to the data needed and the 
data manipulation goals. Whether big-
ger is better depends on many aspects.

Data Infrastructure Pitfalls
Two pitfalls are related to data infra-
structure. First, researchers who do 
not have big infrastructure might think 
they cannot do big data research. Al-
though big infrastructure is useful or 
necessary for some big data tasks, theo-
retical research on significant challenges  
might not require big infrastructure. 
Second, an organization will pur-
chase a high-performance computer to  
support big data analytics, when many 
big data analytics tasks can be done 
without one. We must also differentiate 
between distributed and parallel com-
puting and HPC.

Analytics Pitfalls
There are many pitfalls relating to ana-
lytics. For example, consider the idea 



74  www.computer.org/intelligent Ieee INteLLIGeNt SYStemS

that data-analytical thinking is crucial 
for data science. Actually, it is not only 
important for solving a specific problem, 
it is essential for obtaining a systematic 
solution. Converting an organization to 
think data analytically is a critical com-
petitive advantage in the data era.

Analysts might think that their task 
is to develop common task frame-
works and conduct inference29 from 
the particular to the general. In the 
real world, analytics is often specific. 
Focusing on certain common task 
frameworks could trigger incomplete 
or even misleading outcomes. As we 
discussed earlier, an analyst can take 
on other roles; predictive modeling is 
typically problem specific.

Some researchers might trust the 
quality of models built in commercial 
analytical tools alone. However, such 
tools can produce misleading or even 
incorrect outcomes if the assumption 
of their theoretical foundation does 
not fit the data.

Analysts often want to show “busi-
ness people” some of their statistically 
significant findings. But as domain-
driven data mining shows,10 many out-
comes are often statistically significant 
but are not actionable. An evaluation 
of those findings needs to be conducted 
to discover what business impact30 
might be generated if the findings they 
generate are operationalized. Analysts 
may also wonder why they cannot un-
derstand and interpret the outcomes. 
This could be because the problem has 
been misstated, the model is invalid for 
the data, or the data used is not rele-
vant or correct.

Another pitfall is when analytical 
reports comprise many figures and 
tables that summarize the data min-
ing outcomes, but the boss does not 
seem interested in them. Analytics is 
not just about producing meaning-
ful analytical outcomes and reports; 
rather, it concerns insights, recommen-
dations, and communication with  

upper management for decision mak-
ing and action.

Finally, there is the argument that data 
science and analytics projects are just 
other kinds of IT projects. Although data 
projects share many similar aspects to 
mainstream IT projects, certain distinc-
tive features in data, the manipulation 
process, delivery, and especially the ex-
ploratory nature of data science and ana-
lytics projects require different strategies, 
procedures, and treatments. Data science 
projects are more exploratory, ad hoc, 
decision oriented, and intelligence driven.

Pitfalls about Capabilities and Roles
Other pitfalls concern the individ-
ual—for example, “I am a data sci-
entist.” Lately, it seems that everyone 
has suddenly become a data scientist. 
Most data scientists simply conduct 
normal data engineering and descrip-
tive analytics. Do not expect omnipo-
tence from data scientists.

When an organization wants to do 
big data analytics, it might seek rec-
ommendations of PhD graduates. Al-
though data science and advanced 
analytics tasks usually benefit from 
the input of PhDs, an organization re-
quires different roles and competen-
cies according to the maturity level of 
the analytics and the organization.

Another organization might boast 
that its data science team comprises 
a group of data scientists. However, 
an effective data science team could 
consist of statisticians, programmers, 
physicists, artists, social scientists, deci-
sion makers, or even entrepreneurs.

Finally, there is the argument that a 
data scientist is a statistical program-
mer. Although this is true, in addition to 
the core skills of coding and statistics,  
a data scientist needs to handle many 
other matters.4

Other Matters
Some additional matters require care-
ful consideration in conducting data 

science and analytics. I list a few of 
the common remarks, with my com-
ments in parentheses:

•	Garbage in, garbage out. (The qual-
ity of data determines the quality of 
output.)

•	More complex data and a more ad-
vanced model lead to better outcomes.  
(Good data does not necessarily 
lead to good outcomes, nor does a 
good model.)

•	 Analytics aims to support decision-
making actions, not just to present 
outcomes about data understanding 
and analytical results. (This addresses 
the need for actionable knowledge de-
livery9 to recommend actions from 
data analytics for decision support.)

•	Many end users are investing in 
big data infrastructure without 
project management. (Do not rush 
into data infrastructure investment 
without a solid strategic plan of 
your data science initiatives, which 
requires the identification of busi-
ness needs and requirements, the 
definition of reasonable objectives, 
the specification of timelines, and 
the allocation of resources.)

•	 Pushing data science forward with-
out suitable talent. (On one hand, you 
should not simply wait for the right 
candidate to come along, but should 
actively plan and specify the skills 
needed for your organization’s initia-
tives and assemble a team according 
to the skillsets  required. On the other 
hand, getting the right people on board 
is critical, because data science is essen-
tially about intelligence and talent.)

•	Know nothing about the data be-
fore applying a model. (Under-
standing the data is a must-do step 
before a model is applied.)

•	 Do not assume the data you are given 
is perfect. (Data quality forms the basis 
of obtaining good models, outcomes, 
and decisions. Poor quality data, the 
same as poor quality models, can lead 



September/october 2016 www.computer.org/intelligent 75

to misleading or damaging decisions. 
Real-life data often contains imper-
fect features such as incompleteness, 
uncertainty, bias, rareness, imbalance, 
and non-IIDness.)

This is just a partial list of the re-
maining pitfalls in conducting data 
science and analytics.

As part of this comprehensive review  
of data science,4,5,31 I hope that the 
discussions about the nature and pit-
falls of data science in this article will 
stimulate deep and intrinsic discussions 
about what makes data science a new 
science and what makes it valuable for 
research, innovation, the economy, ser-
vices, and professionals. 

Acknowledgments
This work is partially sponsored by the 
Australian Research Council Discovery 
Grant (DP130102691).

References
 1. J.W. Tukey, “The Future of Data Analy-

sis,” Annals of Mathematical Statistics, 

vol. 33, no. 1, 1962, pp. 1–67.

 2. J.W. Tukey, Exploratory Data Analysis, 

Pearson, 1977.

 3. Big Data: The Next Frontier for Inno-

vation, Competition, and Productivity, 

McKinsey Global Inst., 2011.

 4. L. Cao, Data Science: A Comprehen-

sive Overview, tech. report, UTS Ad-

vanced Analytics Inst., 2016.

 5. L. Cao, Data Science: Intrinsic Chal-

lenges and Directions, tech. report, UTS 

Advanced Analytics Inst., 2016.

 6. M. Mitchell, Complexity: A Guided 

Tour, Oxford Univ. Press, 2011.

 7. L. Cao, Metasynthetic Computing 

and Engineering of Complex Systems, 

Springer, 2015.

 8. K. Schwab, The Global Competitive-

ness Report 2011–2012, report, World 

Economic Forum, 2011.

 9. L. Cao, “Domain Driven Data Min-

ing: Challenges and Prospects,” IEEE 

Trans. Knowledge and Data Eng., vol. 

22, no. 6, 2010, pp. 755–769.

 10. L. Cao et al., Domain Driven Data 

Mining, Springer, 2010.

 11. L. Cao, Y. Ou, and P.S. Yu, “Coupled 

Behavior Analysis with Applications,” 

IEEE Trans. Knowledge and Data Eng., 

vol. 24, no. 8, 2012, pp. 1378–1392.

 12. L. Cao, “Non-IIDness Learning in Be-

havioral and Social Data,” Computer 

J., vol. 57, no. 9, 2014, pp. 1358–1370.

 13. J.H. Faghmous and V. Kumar, “A Big Data 

Guide to Understanding Climate Change: 

The Case for Theory-Guided Data Sci-

ence,” Big Data, Sept. 2014, pp. 155–163.

 14. J.M. Chambers, “Greater or Lesser Sta-

tistics: A Choice for Future Research,” 

Statistics and Computing, vol. 3, no. 4, 

1993, pp. 182–184.

 15. C. Rudin, Discovery with Data: Lever-

aging Statistics with Computer Science 

to Transform Science and Society, Am. 

Statistical Assoc., 2014.

 16. M. Stonebraker, S. Madden, and P. 

Dubey, “Intel ‘Big Data’ Science and 

Technology Center Vision and Execu-

tion Plan,” SIGMOD Record, vol. 42, 

no. 1, 2013, pp. 44–49.

 17. US Big Data Research Initiative, Nat’l 

Science Foundation, 2012.

 18. United Nation Global Pulse Projects, 

2010; www.unglobalpulse.org/projects.

 19. Advanced Analytics Inst., Univ. of 

Technology Sydney, 2011; www.uts.

edu.au/research-and-teaching/our-

research/advanced-analytics-institute.

 20. Inst. for Advanced Analytics, North Caro-

lina State Univ., 2007; http://analytics.

ncsu.edu.

 21. M.A. Walker, “The Professionalization 

of Data Science,” Int’l J. Data Science, 

vol. 1, no. 1, 2015, pp. 7–16.

 22. A. Manieri et al., “Data Science Profes-

sional Uncovered: How the EDISON Proj-

ect Will Contribute to a Widely Accepted 

Profile for Data Scientists,” Proc. IEEE 

7th Int’l Conf. Cloud Computing Tech-

nology and Science, 2015, pp. 588–593.

 23. P. Geczy, “Big Data Characteristics,” 

Macrotheme Rev., vol. 3, no. 6, 2014, 

pp. 94–104.

 24. H.V. Jagadish, “Big Data and Science: 

Myths and Reality,” Big Data Re-

search, vol. 2, no. 2, 2015, pp. 49–52.

 25. D. Donoho, “50 Years of Data 

Science,” 2015; http://courses.

csail.mit.edu/18.337/2015/

docs/50YearsDataScience.pdf.

 26. K. Broman, “Data Science Is Statistics,” 

blog, 2013; http://kbroman.wordpress.

com/2013/04/05/data-science-is- 

statistics.

 27. P.J. Diggle, “Statistics: A Data Science 

for the 21st Century,” J. Royal Statisti-

cal Society: Series A, vol. 178, no. 4, 

2015, pp. 793–813.

 28. I. Wladawsky-Berger, “Why Do We 

Need Data Science When We’ve Had 

Statistics for Centuries?” Wall Street J., 

2014; http://on.wsj.com/1iRCO1w.

 29. L. Breiman, “Statistical Modeling: The 

Two Cultures,” Statistical Science, vol. 

16, no. 3, 2001, pp. 199–231.

 30. L. Cao, Y. Zhao, and C. Zhang, “Min-

ing Impact-Targeted Activity Patterns 

in Imbalanced Data,” IEEE Trans. 

Knowledge and Data Eng., vol. 20, no. 

8, 2008, pp. 1053–1066.

 31. L. Cao, Data Science: Profession and 

Education, tech. report, UTS Advanced 

Analytics Inst., 2016.

Longbing cao is a professor at the Ad-

vanced Analytics Institute at the Univer-

sity of Technology, Sydney. Contact him at  

longbing.cao@gmail.com.

Selected CS articles and columns 
are also available for free at 

http://ComputingNow.computer.org.


